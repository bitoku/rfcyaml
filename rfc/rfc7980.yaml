- title: __initial_text__
  contents:
  - ''
- title: Independent Submission                                      M. Behringer
  contents:
  - "Independent Submission                                      M. Behringer\n  \
    \            A Framework for Defining Network Complexity\n"
- title: Abstract
  contents:
  - "Abstract\n   Complexity is a widely used parameter in network design, yet there\
    \ is\n   no generally accepted definition of the term.  Complexity metrics\n \
    \  exist in a wide range of research papers, but most of these address\n   only\
    \ a particular aspect of a network, for example, the complexity of\n   a graph\
    \ or software.  While it may be impossible to define a metric\n   for overall\
    \ network complexity, there is a desire to better\n   understand the complexity\
    \ of a network as a whole, as deployed today\n   to provide Internet services.\
    \  This document provides a framework to\n   guide research on the topic of network\
    \ complexity as well as some\n   practical examples for trade-offs in networking.\n\
    \   This document summarizes the work of the IRTF's Network Complexity\n   Research\
    \ Group (NCRG) at the time of its closure.  It does not\n   present final results,\
    \ but a snapshot of an ongoing activity, as a\n   basis for future work.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This is a contribution to\
    \ the RFC Series, independently of any other\n   RFC stream.  The RFC Editor has\
    \ chosen to publish this document at\n   its discretion and makes no statement\
    \ about its value for\n   implementation or deployment.  Documents approved for\
    \ publication by\n   the RFC Editor are not a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7980.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2016 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   4\n   2.  General Considerations  . . . . . . . . . . . . . . .\
    \ . . . .   5\n     2.1.  The Behavior of a Complex Network . . . . . . . . .\
    \ . . .   5\n     2.2.  Complex versus Complicated  . . . . . . . . . . . . .\
    \ . .   5\n     2.3.  Robust Yet Fragile  . . . . . . . . . . . . . . . . . .\
    \ .   6\n     2.4.  The Complexity Cube . . . . . . . . . . . . . . . . . . .\
    \   6\n     2.5.  Related Concepts  . . . . . . . . . . . . . . . . . . . .  \
    \ 6\n     2.6.  Technical Debt  . . . . . . . . . . . . . . . . . . . . .   7\n\
    \     2.7.  Layering Considerations . . . . . . . . . . . . . . . . .   8\n  \
    \ 3.  Trade-Offs  . . . . . . . . . . . . . . . . . . . . . . . . .   8\n    \
    \ 3.1.  Control-Plane State versus Optimal Forwarding Paths\n           (Stretch)\
    \ . . . . . . . . . . . . . . . . . . . . . . . .   9\n     3.2.  Configuration\
    \ State versus Failure Domain Separation  . .  10\n     3.3.  Policy Centralization\
    \ versus Optimal Policy Application .  12\n     3.4.  Configuration State versus\
    \ Per-Hop Forwarding\n           Optimization  . . . . . . . . . . . . . . . .\
    \ . . . . . .  13\n     3.5.  Reactivity versus Stability . . . . . . . . . .\
    \ . . . . .  13\n   4.  Parameters  . . . . . . . . . . . . . . . . . . . . .\
    \ . . . .  15\n   5.  Elements of Complexity  . . . . . . . . . . . . . . . .\
    \ . . .  16\n     5.1.  The Physical Network (Hardware) . . . . . . . . . . .\
    \ . .  16\n     5.2.  Algorithms  . . . . . . . . . . . . . . . . . . . . . .\
    \ .  17\n     5.3.  State in the Network  . . . . . . . . . . . . . . . . . .\
    \  17\n     5.4.  Churn . . . . . . . . . . . . . . . . . . . . . . . . . .  17\n\
    \     5.5.  Knowledge . . . . . . . . . . . . . . . . . . . . . . . .  17\n  \
    \ 6.  Location of Complexity  . . . . . . . . . . . . . . . . . . .  17\n    \
    \ 6.1.  Topological Location  . . . . . . . . . . . . . . . . . .  17\n     6.2.\
    \  Logical Location  . . . . . . . . . . . . . . . . . . . .  18\n     6.3.  Layering\
    \ Considerations . . . . . . . . . . . . . . . . .  18\n   7.  Dependencies  .\
    \ . . . . . . . . . . . . . . . . . . . . . . .  18\n     7.1.  Local Dependencies\
    \  . . . . . . . . . . . . . . . . . . .  19\n     7.2.  Network-Wide Dependencies\
    \ . . . . . . . . . . . . . . . .  19\n     7.3.  Network-External Dependencies\
    \ . . . . . . . . . . . . . .  19\n   8.  Management Interactions . . . . . .\
    \ . . . . . . . . . . . . .  20\n     8.1.  Configuration Complexity  . . . .\
    \ . . . . . . . . . . . .  20\n     8.2.  Troubleshooting Complexity  . . . .\
    \ . . . . . . . . . . .  20\n     8.3.  Monitoring Complexity . . . . . . . .\
    \ . . . . . . . . . .  20\n     8.4.  Complexity of System Integration  . . .\
    \ . . . . . . . . .  21\n   9.  External Interactions . . . . . . . . . . . .\
    \ . . . . . . . .  21\n   10. Examples  . . . . . . . . . . . . . . . . . . .\
    \ . . . . . . .  22\n   11. Security Considerations . . . . . . . . . . . . .\
    \ . . . . . .  22\n   12. Informative References  . . . . . . . . . . . . . .\
    \ . . . . .  22\n   Acknowledgements  . . . . . . . . . . . . . . . . . . . .\
    \ . . . .  23\n   Authors' Addresses  . . . . . . . . . . . . . . . . . . . .\
    \ . . .  24\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Network design can be described as the art of finding the\
    \ simplest\n   solution to solve a given problem.  Complexity is thus assumed\
    \ in the\n   design process; engineers do not ask if there should be complexity,\n\
    \   but rather, how much complexity is required to solve the problem.\n   The\
    \ question of how much complexity assumes there is some way to\n   characterize\
    \ the amount of complexity present in a system.  The\n   reality is, however,\
    \ this is an area of research and experience\n   rather than a solved problem\
    \ within the network engineering space.\n   Today's design decisions are made\
    \ based on a rough estimation of the\n   network's complexity rather than a solid\
    \ understanding.\n   The document begins with general considerations, including\
    \ some\n   foundational definitions and concepts.  It then provides some\n   examples\
    \ for trade-offs that network engineers regularly make when\n   designing a network.\
    \  This section serves to demonstrate that there\n   is no single answer to complexity;\
    \ rather, it is a managed trade-off\n   between many parameters.  After this,\
    \ this document provides a set of\n   parameters engineers should consider when\
    \ attempting to either\n   measure complexity or build a framework around it.\
    \  This list makes\n   no claim to be complete, but it serves as a guide of known\
    \ existing\n   areas of investigation as well as a pointer to areas that still\
    \ need\n   to be investigated.\n   Two purposes are served here.  The first is\
    \ to guide researchers\n   working in the area of complexity in their work.  The\
    \ more\n   researchers are able to connect their work to the concerns of network\n\
    \   designers, the more useful their research will become.  This document\n  \
    \ may also guide research into areas not considered before.  The second\n   is\
    \ to help network engineers to build a better understanding of where\n   complexity\
    \ might be \"hiding\" in their networks and to be more fully\n   aware of how\
    \ complexity interacts with design and deployment.\n   The goal of the IRTF Network\
    \ Complexity Research Group (NCRG) [ncrg]\n   was to define a framework for network\
    \ complexity research while\n   recognizing that it may be impossible to define\
    \ metrics for overall\n   network complexity.  This document summarizes the work\
    \ of this group\n   at the time of its closure in 2014.  It does not present final\n\
    \   results, but rather a snapshot of an ongoing activity, as a basis for\n  \
    \ future work.\n   Many references to existing research in the area of network\n\
    \   complexity are listed on the Network Complexity Wiki [wiki].  This\n   wiki\
    \ also contains background information on previous meetings on the\n   subject,\
    \ previous research, etc.\n"
- title: 2.  General Considerations
  contents:
  - '2.  General Considerations

    '
- title: 2.1.  The Behavior of a Complex Network
  contents:
  - "2.1.  The Behavior of a Complex Network\n   While there is no generally accepted\
    \ definition of network\n   complexity, there is some understanding of the behavior\
    \ of a complex\n   network.  It has some or all of the following properties:\n\
    \   o  Self-Organization: A network runs some protocols and processes\n      without\
    \ external control; for example, a routing process, failover\n      mechanisms,\
    \ etc.  The interaction of those mechanisms can lead to\n      a complex behavior.\n\
    \   o  Unpredictability: In a complex network, the effect of a local\n      change\
    \ on the behavior of the global network may be unpredictable.\n   o  Emergence:\
    \ The behavior of the system as a whole is not reflected\n      in the behavior\
    \ of any individual component of the system.\n   o  Non-linearity: An input into\
    \ the network produces a non-linear\n      result.\n   o  Fragility: A small local\
    \ input can break the entire system.\n"
- title: 2.2.  Complex versus Complicated
  contents:
  - "2.2.  Complex versus Complicated\n   The two terms \"complex\" and \"complicated\"\
    \ are often used\n   interchangeably, yet they describe different but overlapping\n\
    \   properties.  The RG made the following statements about the two\n   terms,\
    \ but they would need further refinement to be considered formal\n   definitions:\n\
    \   o  A \"complicated\" system is a deterministic system that can be\n      understood\
    \ by an appropriate level of analysis.  It is often an\n      externally applied\
    \ attribute rather than an intrinsic property of\n      a system and is typically\
    \ associated with systems that require\n      deep or significant levels of analysis.\n\
    \   o  A \"complex\" system, by comparison, is an intrinsic property of a\n  \
    \    system and is typically associated with emergent behaviors such\n      that\
    \ the behavior of the system is not fully described by the sum\n      of the behavior\
    \ of each of the components of the system.  Complex\n      systems are often associated\
    \ with systems whose components exhibit\n      high levels of interaction and\
    \ feedback.\n"
- title: 2.3.  Robust Yet Fragile
  contents:
  - "2.3.  Robust Yet Fragile\n   Networks typically follow the \"robust yet fragile\"\
    \ paradigm: they are\n   designed to be robust against a set of failures, yet\
    \ they are very\n   vulnerable to other failures.  Doyle [Doyle] explains the\
    \ concept\n   with an example: the Internet is robust against single-component\n\
    \   failure but fragile to targeted attacks.  The \"robust yet fragile\"\n   property\
    \ also touches on the fact that all network designs are\n   necessarily making\
    \ trade-offs between different design goals.  The\n   simplest one is \"Good,\
    \ Fast, Cheap: Pick any two (you can't have all\n   three)\", as articulated in\
    \ \"The Twelve Networking Truths\" [RFC1925].\n   In real network design, trade-offs\
    \ between many aspects have to be\n   made, including, for example, issues of\
    \ scope, time, and cost in the\n   network cycle of planning, design, implementation,\
    \ and management of\n   a network platform.  Section 3 gives some examples of\
    \ trade-offs, and\n   parameters are discussed in Section 4.\n"
- title: 2.4.  The Complexity Cube
  contents:
  - "2.4.  The Complexity Cube\n   Complex tasks on a network can be done in different\
    \ components of the\n   network.  For example, routing can be controlled by central\n\
    \   algorithms and the result distributed (e.g., OpenFlow model); the\n   routing\
    \ algorithm can also run completely distributed (e.g., routing\n   protocols such\
    \ as OSPF or IS-IS), or a human operator could calculate\n   routing tables and\
    \ statically configure routing.  Behringer\n   [Behringer] defines these three\
    \ axes of complexity as a \"complexity\n   cube\" with the respective axes being\
    \ network elements, central\n   systems, and human operators.  Any function can\
    \ be implemented in any\n   of these three axes, and this choice likely has an\
    \ impact on the\n   overall complexity of the system.\n"
- title: 2.5.  Related Concepts
  contents:
  - "2.5.  Related Concepts\n   When discussing network complexity, a large number\
    \ of influencing\n   factors have to be taken into account to arrive at a full\
    \ picture,\n   for example:\n   o  State in the Network: Contains the network\
    \ elements, such as\n      routers, switches (with their OS, including protocols),\
    \ lines,\n      central systems, etc.  This also includes the number and\n   \
    \   algorithmic complexity of the protocols on network devices.\n   o  Human Operators:\
    \ Complexity manifests itself often by a network\n      that is not completely\
    \ understood by human operators.  Human error\n      is a primary source for catastrophic\
    \ failures and therefore must\n      be taken into account.\n   o  Classes/Templates:\
    \ Rather than counting the number of lines in a\n      configuration or the number\
    \ of hardware elements, more important\n      is the number of classes from which\
    \ those can be derived.  In\n      other words, it is probably less complex to\
    \ have 1000 interfaces\n      that are identically configured than 5 that are\
    \ configured\n      completely different.\n   o  Dependencies and Interactions:\
    \ The number of dependencies between\n      elements, as well as the interactions\
    \ between them, has influence\n      on the complexity of the network.\n   o \
    \ Total Cost of Ownership (TCO): TCO could be a good metric for\n      network\
    \ complexity if the TCO calculation takes into account all\n      influencing\
    \ factors, for example, training time for staff to be\n      able to maintain\
    \ a network.\n   o  Benchmark Unit Cost (BUC): BUC is a related metric that indicates\n\
    \      the cost of operating a certain component.  If calculated well, it\n  \
    \    reflects at least parts of the complexity of this component.\n      Therefore,\
    \ the way TCO or BUC is calculated can help to derive a\n      complexity metric.\n\
    \   o  Churn / Rate of Change: The change rate in a network itself can\n     \
    \ contribute to complexity, especially if a number of components of\n      the\
    \ overall network interact.\n   Networks differ in terms of their intended purpose\
    \ (such as is found\n   in differences between enterprise and public carriage\
    \ network\n   platforms) and differences in their intended roles (such as is found\n\
    \   in the differences between so-called \"access\" networks and \"core\"\n  \
    \ transit networks).  The differences in terms of role and purpose can\n   often\
    \ lead to differences in the tolerance for, and even the metrics\n   of, complexity\
    \ within such different network scenarios.  This is not\n   necessarily a space\
    \ where a single methodology for measuring\n   complexity, and defining a single\
    \ threshold value of acceptability of\n   complexity, is appropriate.\n"
- title: 2.6.  Technical Debt
  contents:
  - "2.6.  Technical Debt\n   Many changes in a network are made with a dependency\
    \ on the existing\n   network.  Often, a suboptimal decision is made because the\
    \ optimal\n   decision is hard or impossible to realize at the time.  Over time,\n\
    \   the number of suboptimal changes in themselves cause significant\n   complexity,\
    \ which would not have been there had the optimal solution\n   been implemented.\n\
    \   The term \"technical debt\" refers to the accumulated complexity of\n   suboptimal\
    \ changes over time.  As with financial debt, the idea is\n   that also technical\
    \ debt must be repaid one day by cleaning up the\n   network or software.\n"
- title: 2.7.  Layering Considerations
  contents:
  - "2.7.  Layering Considerations\n   In considering the larger space of applications,\
    \ transport services,\n   network services, and media services, it is feasible\
    \ to engineer\n   responses for certain types of desired applications responses\
    \ in many\n   different ways and involving different layers of the so-called\n\
    \   network protocol stack.  For example, Quality of Service (QoS) could\n   be\
    \ engineered at any of these layers or even in a number of\n   combinations of\
    \ different layers.\n   Considerations of complexity arise when mutually incompatible\n\
    \   measures are used in combination (such as error detection and\n   retransmission\
    \ at the media layer in conjunction with the use of TCP\n   transport protocol)\
    \ or when assumptions used in one layer are\n   violated by another layer.  This\
    \ results in surprising outcomes that\n   may result in complex interactions,\
    \ for example, oscillation, because\n   different layers use different timers\
    \ for retransmission.  These\n   issues have led to the perspective that increased\
    \ layering frequently\n   increases complexity [RFC3439].\n   While this research\
    \ work is focused on network complexity, the\n   interactions of the network with\
    \ the end-to-end transport protocols,\n   application layer protocols, and media\
    \ properties are relevant\n   considerations here.\n"
- title: 3.  Trade-Offs
  contents:
  - "3.  Trade-Offs\n   Network complexity is a system-level, rather than component-level,\n\
    \   problem; overall system complexity may be more than the sum of the\n   complexity\
    \ of the individual pieces.\n   There are two basic ways in which system-level\
    \ problems might be\n   addressed: interfaces and continuums.  In addressing a\
    \ system-level\n   problem through interfaces, we seek to treat each piece of\
    \ the system\n   as a \"black box\" and develop a complete understanding of the\n\
    \   interfaces between these black boxes.  In addressing a system-level\n   problem\
    \ as a continuum, we seek to understand the impact of a single\n   change or element\
    \ to the entire system as a set of trade-offs.\n   While network complexity can\
    \ profitably be approached from either of\n   these perspectives, in this document\
    \ we have chosen to approach the\n   system-level impact of network complexity\
    \ from the perspective of\n   continuums of trade-offs.  In theory, modifying\
    \ the network to\n   resolve one particular problem (or class of problems) will\
    \ add\n   complexity that results in the increased likelihood (or appearance)\n\
    \   of another class of problems.  Discovering these continuums of trade-\n  \
    \ offs, and then determining how to measure each one, become the key\n   steps\
    \ in understanding and measuring system-level complexity in this\n   view.\n \
    \  The following sections describe five such continuums; more may be\n   possible.\n\
    \   o  Control-Plane State versus Optimal Forwarding Paths (or its\n      opposite\
    \ measure, stretch)\n   o  Configuration State versus Failure Domain Separation\n\
    \   o  Policy Centralization versus Optimal Policy Application\n   o  Configuration\
    \ State versus Per-Hop Forwarding Optimization\n   o  Reactivity versus Stability\n"
- title: 3.1.  Control-Plane State versus Optimal Forwarding Paths (Stretch)
  contents:
  - "3.1.  Control-Plane State versus Optimal Forwarding Paths (Stretch)\n   Control-plane\
    \ state is the aggregate amount of information carried by\n   the control plane\
    \ through the network in order to produce the\n   forwarding table at each device.\
    \  Each additional piece of\n   information added to the control plane -- such\
    \ as more-specific\n   reachability information, policy information, additional\
    \ control\n   planes for virtualization and tunneling, or more precise topology\n\
    \   information -- adds to the complexity of the control plane.  This\n   added\
    \ complexity, in turn, adds to the burden of monitoring,\n   understanding, troubleshooting,\
    \ and managing the network.\n   Removing control-plane state, however, is not\
    \ always a net positive\n   gain for the network as a system; removing control-plane\
    \ state almost\n   always results in decreased optimality in the forwarding and\
    \ handling\n   of packets traveling through the network.  This decreased optimality\n\
    \   can be termed \"stretch\", which is defined as the difference between\n  \
    \ the absolute shortest (or best) path traffic could take through the\n   network\
    \ and the path the traffic actually takes.  Stretch is\n   expressed as the difference\
    \ between the optimal and actual path.  The\n   figure below provides an example\
    \ of this trade-off.\n                                +---R1---+\n           \
    \                     |        |\n        (aggregate: 192.0.2/24) R2       R3\
    \ (aggregate: 192.0.2/24)\n                                |        |\n      \
    \                          R4-------R5\n                                |\n  \
    \     (announce: 192.0.2.1/32) R6\n   Assume each link is of equal cost in this\
    \ figure and that R6 is\n   advertising 192.0.2.1/32.\n   For R1, the shortest\
    \ path to 192.0.2.1/32, advertised by R6, is along\n   the path [R1,R2,R4,R6].\n\
    \   Assume, however, the network administrator decides to aggregate\n   reachability\
    \ information at R2 and R3, advertising 192.0.2.0/24\n   towards R1 from both\
    \ of these points.  This reduces the overall\n   complexity of the control plane\
    \ by reducing the amount of information\n   carried past these two routers (at\
    \ R1 only in this case).\n   Aggregating reachability information at R2 and R3,\
    \ however, may have\n   the impact of making both routes towards 192.0.2.1/32\
    \ appear as equal\n   cost paths to R1; there is no particular reason R1 should\
    \ choose the\n   shortest path through R2 over the longer path through R3.  This,\
    \ in\n   effect, increases the stretch of the network.  The shortest path from\n\
    \   R1 to R6 is 3 hops, a path that will always be chosen before\n   aggregation\
    \ is configured.  Assuming half of the traffic will be\n   forwarded along the\
    \ path through R2 (3 hops), and half through R3 (4\n   hops), the network is stretched\
    \ by ((3+4)/2) - 3), or .5, a \"half a\n   hop\".\n   Traffic engineering through\
    \ various tunneling mechanisms is, at a\n   broad level, adding control-plane\
    \ state to provide more optimal\n   forwarding (or network utilization).  Optimizing\
    \ network utilization\n   may require detuning stretch (intentionally increasing\
    \ stretch) to\n   increase overall network utilization and efficiency; this is\
    \ simply\n   an alternate instance of control-plane state (and hence, complexity)\n\
    \   weighed against optimal forwarding through the network.\n"
- title: 3.2.  Configuration State versus Failure Domain Separation
  contents:
  - "3.2.  Configuration State versus Failure Domain Separation\n   A failure domain,\
    \ within the context of a network control plane, can\n   be defined as the set\
    \ of devices impacted by a change in the network\n   topology or configuration.\
    \  A network with larger failure domains is\n   more prone to cascading failures,\
    \ so smaller failure domains are\n   normally preferred over larger ones.\n  \
    \ The primary means used to limit the size of a failure domain within a\n   network's\
    \ control plane is information hiding; the two primary types\n   of information\
    \ hidden in a network control plane are reachability\n   information and topology\
    \ information.  An example of aggregating\n   reachability information is summarizing\
    \ the routes 192.0.2.1/32,\n   192.0.2.2/32, and 192.0.2.3/32 into the single\
    \ route 192.0.2.0/24,\n   along with the aggregation of the metric information\
    \ associated with\n   each of the component routes.  Note that aggregation is\
    \ a \"natural\"\n   part of IP networks, starting with the aggregation of individual\n\
    \   hosts into a subnet at the network edge.  An example of topology\n   aggregation\
    \ is the summarization of routes at a link-state flooding\n   domain boundary,\
    \ or the lack of topology information in a distance-\n   vector protocol.\n  \
    \ While limiting the size of failure domains appears to be an absolute\n   good\
    \ in terms of network complexity, there is a definite trade-off in\n   configuration\
    \ complexity.  The more failure domain edges created in a\n   network, the more\
    \ complex configuration will become.  This is\n   particularly true if redistribution\
    \ of routing information between\n   multiple control-plane processes is used\
    \ to create failure domain\n   boundaries; moving between different types of control\
    \ planes causes a\n   loss of the consistent metrics most control planes rely\
    \ on to build\n   loop-free paths.  Redistribution, in particular, opens the door\
    \ to\n   very destructive positive feedback loops within the control plane.\n\
    \   Examples of control-plane complexity caused by the creation of\n   failure\
    \ domain boundaries include route filters, routing aggregation\n   configuration,\
    \ and metric modifications to engineer traffic across\n   failure domain boundaries.\n\
    \   Returning to the network described in the previous section,\n   aggregating\
    \ routing information at R2 and R3 will divide the network\n   into two failure\
    \ domains: (R1, R2, R3) and (R2, R3, R4, R5).  A\n   failure at R5 should have\
    \ no impact on the forwarding information at\n   R1.\n   A false failure domain\
    \ separation occurs, however, when the metric of\n   the aggregate route advertised\
    \ by R2 and R3 is dependent on one of\n   the routes within the aggregate.  For\
    \ instance, if the metric of the\n   192.0.2.0/24 aggregate is derived from the\
    \ metric of the component\n   192.0.2.1/32, then a failure of this one component\
    \ will cause changes\n   in the forwarding table at R1 -- in this case, the control\
    \ plane has\n   not truly been separated into two distinct failure domains.  The\n\
    \   added complexity in the illustration network would be the management\n   of\
    \ the configuration required to aggregate the control-plane\n   information, and\
    \ the management of the metrics to ensure the control\n   plane is truly separated\
    \ into two distinct failure domains.\n   Replacing aggregation with redistribution\
    \ adds the complexity of\n   managing the feedback of routing information redistributed\
    \ between\n   the failure domains.  For instance, if R1, R2, and R3 were configured\n\
    \   to run one routing protocol while R2, R3, R4, R5, and R6 were\n   configured\
    \ to run another protocol, R2 and R3 could be configured to\n   redistribute reachability\
    \ information between these two control\n   planes.  This can split the control\
    \ plane into multiple failure\n   domains (depending on how, specifically, redistribution\
    \ is\n   configured) but at the cost of creating and managing the\n   redistribution\
    \ configuration.  Further, R3 must be configured to\n   block routing information\
    \ redistributed at R2 towards R1 from being\n   redistributed (again) towards\
    \ R4 and R5.\n"
- title: 3.3.  Policy Centralization versus Optimal Policy Application
  contents:
  - "3.3.  Policy Centralization versus Optimal Policy Application\n   Another broad\
    \ area where control-plane complexity interacts with\n   optimal network utilization\
    \ is QoS.  Two specific actions are\n   required to optimize the flow of traffic\
    \ through a network: marking\n   and Per Hop Behaviors (PHBs).  Rather than examining\
    \ each packet at\n   each forwarding device in a network, packets are often marked,\
    \ or\n   classified, in some way (typically through Type of Service bits) so\n\
    \   they can be handled consistently at all forwarding devices.\n   Packet-marking\
    \ policies must be configured on specific forwarding\n   devices throughout the\
    \ network.  Distributing marking closer to the\n   edge of the network necessarily\
    \ means configuring and managing more\n   devices, but it produces optimal forwarding\
    \ at a larger number of\n   network devices.  Moving marking towards the network\
    \ core means\n   packets are marked for proper handling across a smaller number\
    \ of\n   devices.  In the same way, each device through which a packet passes\n\
    \   with the correct PHBs configured represents an increase in the\n   consistency\
    \ in packet handling through the network as well as an\n   increase in the number\
    \ of devices that must be configured and managed\n   for the correct PHBs.  The\
    \ network below is used for an illustration\n   of this concept.\n           \
    \                   +----R1----+\n                              |          |\n\
    \                           +--R2--+   +--R3--+\n                           |\
    \      |   |      |\n                           R4     R5  R6     R7\n   In this\
    \ network, marking and PHB configuration may be configured on\n   any device,\
    \ R1 through R7.\n   Assume marking is configured at the network edge; in this\
    \ case, four\n   devices (R4, R5, R6, R7) must be configured, including ongoing\n\
    \   configuration management, to mark packets.  Moving packet marking to\n   R2\
    \ and R3 will halve the number of devices on which packet-marking\n   configuration\
    \ must be managed, but at the cost of inconsistent packet\n   handling at the\
    \ inbound interfaces of R2 and R3 themselves.\n   Thus, reducing the number of\
    \ devices that must have managed\n   configurations for packet marking will reduce\
    \ optimal packet flow\n   through the network.  Assuming packet marking is actually\
    \ configured\n   along the edge of this network, configuring PHBs on different\
    \ devices\n   has this same trade-off of managed configuration versus optimal\n\
    \   traffic flow.  If the correct PHBs are configured on R1, R2, and R3,\n   then\
    \ packets passing through the network will be handled correctly at\n   each hop.\
    \  The cost involved will be the management of PHB\n   configuration on three\
    \ devices.  Configuring a single device for the\n   correct PHBs (R1, for instance),\
    \ will decrease the amount of\n   configuration management required at the cost\
    \ of less than optimal\n   packet handling along the entire path.\n"
- title: 3.4.  Configuration State versus Per-Hop Forwarding Optimization
  contents:
  - "3.4.  Configuration State versus Per-Hop Forwarding Optimization\n   The number\
    \ of PHBs configured along a forwarding path exhibits the\n   same complexity\
    \ versus optimality trade-off described in the section\n   above.  The more classes\
    \ (or queues) traffic is divided into, the\n   more fine-grained traffic will\
    \ be managed as it passes through the\n   network.  At the same time, each class\
    \ of service must be managed,\n   both in terms of configuration and in its interaction\
    \ with other\n   classes of service configured in the network.\n"
- title: 3.5.  Reactivity versus Stability
  contents:
  - "3.5.  Reactivity versus Stability\n   The speed at which the network's control\
    \ plane can react to a change\n   in configuration or topology is an area of widespread\
    \ study.\n   Control-plane convergence can be broken down into four essential\n\
    \   parts:\n   o  Detecting the change\n   o  Propagating information about the\
    \ change\n   o  Determining the best path(s) through the network after the change\n\
    \   o  Changing the forwarding path at each network element along the\n      modified\
    \ paths\n   Each of these areas can be addressed in an effort to improve network\n\
    \   convergence speeds; some of these improvements come at the cost of\n   increased\
    \ complexity.\n   Changes in network topology can be detected much more quickly\
    \ through\n   faster echo (or hello) mechanisms, lower-layer physical detection,\n\
    \   and other methods.  Each of these mechanisms, however, can only be\n   used\
    \ at the cost of evaluating and managing false positives and high\n   rates of\
    \ topology change.\n   If the state of a link change can be detected in 10 ms,\
    \ for instance,\n   the link could theoretically change state 50 times in a second\
    \ -- it\n   would be impossible to tune a network control plane to react to\n\
    \   topology changes at this rate.  Injecting topology change information\n  \
    \ into the control plane at this rate can destabilize the control\n   plane, and\
    \ hence the network itself.  To counter this, most\n   techniques that quickly\
    \ detect link-down events include some form of\n   dampening mechanism; configuring\
    \ and managing these dampening\n   mechanisms increases complexity.\n   Changes\
    \ in network topology must also be propagated throughout the\n   network so each\
    \ device along the path can compute new forwarding\n   tables.  In high-speed\
    \ network environments, propagation of routing\n   information changes can take\
    \ place in tens of milliseconds, opening\n   the possibility of multiple changes\
    \ being propagated per second.\n   Injecting information at this rate into the\
    \ control plane creates the\n   risk of overloading the processes and devices\
    \ participating in the\n   control plane as well as creating destructive positive\
    \ feedback loops\n   in the network.  To avoid these consequences, most control-plane\n\
    \   protocols regulate the speed at which information about network\n   changes\
    \ can be transmitted by any individual device.  A recent\n   innovation in this\
    \ area is using exponential backoff techniques to\n   manage the rate at which\
    \ information is advertised into the control\n   plane; the first change is transmitted\
    \ quickly, while subsequent\n   changes are transmitted more slowly.  These techniques\
    \ all control\n   the destabilizing effects of rapid information flows through\
    \ the\n   control plane through the added complexity of configuring and\n   managing\
    \ the rate at which the control plane can propagate\n   information about network\
    \ changes.\n   All control planes require some form of algorithmic calculation\
    \ to\n   find the best path through the network to any given destination.\n  \
    \ These algorithms are often lightweight but they still require some\n   amount\
    \ of memory and computational power to execute.  Rapid changes\n   in the network\
    \ can overwhelm the devices on which these algorithms\n   run, particularly if\
    \ changes are presented more quickly than the\n   algorithm can run.  Once a device\
    \ running these algorithms becomes\n   processor or memory bound, it could experience\
    \ a computational\n   failure altogether, causing a more general network outage.\
    \  To\n   prevent computational overloading, control-plane protocols are\n   designed\
    \ with timers limiting how often they can compute the best\n   path through a\
    \ network; often these timers are exponential in nature\n   and thus allow the\
    \ first computation to run quickly while delaying\n   subsequent computations.\
    \  Configuring and managing these timers is\n   another source of complexity within\
    \ the network.\n   Another option to improve the speed at which the control plane\
    \ reacts\n   to changes in the network is to precompute alternate paths at each\n\
    \   device and possibly preinstall forwarding information into local\n   forwarding\
    \ tables.  Additional state is often needed to precompute\n   alternate paths,\
    \ and additional algorithms and techniques are often\n   configured and deployed.\
    \  This additional state, and these additional\n   algorithms, add some amount\
    \ of complexity to the configuration and\n   management of the network.\n   In\
    \ some situations (for some topologies), a tunnel is required to\n   pass traffic\
    \ around a network failure or topology change.  These\n   tunnels, while not manually\
    \ configured, represent additional\n   complexity at the forwarding and control\
    \ planes.\n"
- title: 4.  Parameters
  contents:
  - "4.  Parameters\n   In Section 3, we describe a set of trade-offs in network design\
    \ to\n   illustrate the practical choices network operators have to make.  The\n\
    \   amount of parameters to consider in such trade-off scenarios is very\n   large,\
    \ and thus a complete listing may not be possible.  Also, the\n   dependencies\
    \ between the various metrics themselves is very complex\n   and requires further\
    \ study.  This document attempts to define a\n   methodology and an overall high-level\
    \ structure.\n   To analyze trade-offs it is necessary to formalize them.  The\
    \ list of\n   parameters for such trade-offs is long, and the parameters can be\n\
    \   complex in themselves.  For example, \"cost\" can be a simple\n   unidimensional\
    \ metric, but \"extensibility\" and \"optimal forwarding\n   state\" are harder\
    \ to define in detail.\n   A list of parameters to trade off contains metrics\
    \ such as:\n   o  State: How much state needs to be held in the control plane,\n\
    \      forwarding plane, configuration, etc.?\n   o  Cost: How much does the network\
    \ cost to build and run (i.e.,\n      capital expenditure (capex) and operating\
    \ expenses (opex))?\n   o  Bandwidth/Delay/Jitter: Traffic characteristics between\
    \ two points\n      (average, max, etc.)\n   o  Configuration Complexity: How\
    \ hard is it to configure and maintain\n      the configuration?\n   o  Susceptibility\
    \ to Denial of Service: How easy is it to attack the\n      service?\n   o  Security\
    \ (Confidentiality/Integrity): How easy is it to\n      sniff/modify/insert the\
    \ data flow?\n   o  Scalability: To what size can I grow the network/service?\n\
    \   o  Stability: How stable is the network under the influence of local\n   \
    \   change?\n   o  Reactivity: How fast does the network converge or adapt to\
    \ new\n      situations?\n   o  Extensibility: Can I use the network for other\
    \ services in the\n      future?\n   o  Ease of Troubleshooting: Are failure domains\
    \ separated?  How hard\n      is it to find and correct problems?\n   o  Optimal\
    \ Per-Hop Forwarding Behavior\n   o  Predictability: If I change a parameter,\
    \ what will happen?\n   o  Clean Failure: When a problem arises, does the root\
    \ cause lead to\n      deterministic failure?\n"
- title: 5.  Elements of Complexity
  contents:
  - "5.  Elements of Complexity\n   Complexity can be found in various elements in\
    \ a networked system.\n   For example, the configuration of a network element\
    \ reflects some of\n   the complexity contained in this system, or an algorithm\
    \ used by a\n   protocol may be more or less complex.  When classifying complexity,\n\
    \   \"WHAT is complex?\" is the first question to ask.  This section offers\n\
    \   a method to answer this question.\n"
- title: 5.1.  The Physical Network (Hardware)
  contents:
  - "5.1.  The Physical Network (Hardware)\n   The set of network devices and wiring\
    \ contains a certain complexity.\n   For example, adding a redundant link between\
    \ two locations increases\n   the complexity of the network but provides more\
    \ redundancy.  Also,\n   network devices can be more or less modular, which has\
    \ impact on\n   complexity trading off against ease of maintenance, availability,\
    \ and\n   upgradability.\n"
- title: 5.2.  Algorithms
  contents:
  - "5.2.  Algorithms\n   The behavior of the physical network is not only defined\
    \ by the\n   hardware but also by algorithms that run on network elements and\
    \ in\n   central locations.  Every algorithm has a certain intrinsic\n   complexity,\
    \ which is the subject of research on software complexity.\n"
- title: 5.3.  State in the Network
  contents:
  - "5.3.  State in the Network\n   The way a network element treats traffic is defined\
    \ largely by the\n   state in the network, in form of configuration, routing state,\n\
    \   security measures, etc.  Section 3.1 shows an example where more\n   control-plane\
    \ state allows for a more precise forwarding.\n"
- title: 5.4.  Churn
  contents:
  - "5.4.  Churn\n   The rate of change itself is a parameter in complexity and needs\
    \ to\n   be weighed against other parameters.  Section 3.5 explains a trade-\n\
    \   off between the speed of communicating changes through the network\n   and\
    \ the stability of the network.\n"
- title: 5.5.  Knowledge
  contents:
  - "5.5.  Knowledge\n   Certain complexity parameters have a strong link to the human\
    \ aspect\n   of networking.  For example, the more options and parameters a\n\
    \   network protocol has, the harder it is to configure and troubleshoot.\n  \
    \ Therefore, there is a trade-off between the knowledge to be\n   maintained by\
    \ operational staff and desired functionality.  The\n   required knowledge of\
    \ network operators is therefore an important\n   part in complexity considerations.\n"
- title: 6.  Location of Complexity
  contents:
  - "6.  Location of Complexity\n   The previous section discussed in which form complexity\
    \ may be\n   perceived.  This section focuses on where this complexity is located\n\
    \   in a network.  For example, an algorithm can run centrally,\n   distributed,\
    \ or even in the head of a network administrator.  In\n   classifying the complexity\
    \ of a network, the location of a component\n   may have an impact on overall\
    \ complexity.  This section offers a\n   methodology to find WHERE the complex\
    \ component is located.\n"
- title: 6.1.  Topological Location
  contents:
  - "6.1.  Topological Location\n   An algorithm can run distributed; for example,\
    \ a routing protocol\n   like OSPF runs on all routers in a network.  But, it\
    \ can also be in a\n   central location such as the Network Operations Center\
    \ (NOC).  The\n   physical location has an impact on several other parameters,\
    \ such as\n   availability (local changes might be faster than going through a\n\
    \   remote NOC) and ease of operation, because it might be easier to\n   understand\
    \ and troubleshoot one central entity rather than many\n   remote ones.\n   The\
    \ example in Section 3.3 shows how the location of state (in this\n   case configuration)\
    \ impacts the precision of the policy enforcement\n   and the corresponding state\
    \ required.  Enforcement closer to the edge\n   requires more network-wide state\
    \ but is more precise.\n"
- title: 6.2.  Logical Location
  contents:
  - "6.2.  Logical Location\n   Independent of its physical location, the logical\
    \ location also may\n   make a difference to complexity.  A controller function,\
    \ for example,\n   can reside in a NOC and also on a network element.  Generally,\n\
    \   organizing a network in separate logical entities is considered\n   positive\
    \ because it eases the understanding of the network, thereby\n   making troubleshooting\
    \ and configuration easier.  For example, a BGP\n   route reflector is a separate\
    \ logical entity from a BGP speaker, but\n   it may reside on the same physical\
    \ node.\n"
- title: 6.3.  Layering Considerations
  contents:
  - "6.3.  Layering Considerations\n   Also, the layer of the TCP/IP stack in which\
    \ a function is\n   implemented can have an impact on the complexity of the overall\n\
    \   network.  Some functions are implemented in several layers in\n   slightly\
    \ different ways; this may lead to unexpected results.\n   As an example, a link\
    \ failure is detected on various layers: L1, L2,\n   the IGP, BGP, and potentially\
    \ more.  Since those have dependencies on\n   each other, different link failure\
    \ detection times can cause\n   undesired effects.  Dependencies are discussed\
    \ in more detail in the\n   next section.\n"
- title: 7.  Dependencies
  contents:
  - "7.  Dependencies\n   Dependencies are generally regarded as related to overall\
    \ complexity.\n   A system with less dependencies is generally considered less\
    \ complex.\n   This section proposes a way to analyze dependencies in a network.\n\
    \   For example, [Chun] states: \"We conjecture that the complexity\n   particular\
    \ to networked systems arises from the need to ensure state\n   is kept in sync\
    \ with its distributed dependencies.\"\n   In this document, we distinguish three\
    \ types of dependencies: local\n   dependencies, network-wide dependencies, and\
    \ network-external\n   dependencies.\n"
- title: 7.1.  Local Dependencies
  contents:
  - "7.1.  Local Dependencies\n   Local dependencies are relative to a single node\
    \ in the network.  For\n   example, an interface on a node may have an IP address;\
    \ this address\n   may be used in other parts of the configuration.  If the interface\n\
    \   address changes, the dependent configuration parts have to change as\n   well.\n\
    \   Similar dependencies exist for QoS policies, access-control lists,\n   names\
    \ and numbers of configuration parts, etc.\n"
- title: 7.2.  Network-Wide Dependencies
  contents:
  - "7.2.  Network-Wide Dependencies\n   Routing protocols, failover protocols, and\
    \ many others have\n   dependencies across the network.  If one node is affected\
    \ by a\n   problem, this may have a ripple effect through the network.  These\n\
    \   protocols are typically designed to deal with unexpected consequences\n  \
    \ and thus are unlikely to cause an issue on their own.  But,\n   occasionally\
    \ a number of complexity issues come together (for\n   example, different timers\
    \ on different layers), resulting in\n   unexpected behavior.\n"
- title: 7.3.  Network-External Dependencies
  contents:
  - "7.3.  Network-External Dependencies\n   Some dependencies are on elements outside\
    \ the actual network, for\n   example, on an external NTP clock source or an Authentication,\n\
    \   Authorization, and Accounting (AAA) server.  Again, a trade-off is\n   made:\
    \ in the example of AAA used for login authentication, we reduce\n   the configuration\
    \ (state) on each node (in particular, user-specific\n   configuration), but we\
    \ add an external dependency on a AAA server.\n   In networks with many administrators,\
    \ a AAA server is clearly the\n   only manageable way to track all administrators.\
    \  But, it comes at\n   the cost of this external dependency with the consequence\
    \ that admin\n   access may be lost for all devices at the same time when the\
    \ AAA\n   server is unavailable.\n   Even with the external dependency on a AAA\
    \ server, the advantage of\n   centralizing the user information (and logging)\
    \ still has significant\n   value over distributing user information across all\
    \ devices.  To\n   solve the problem of the central dependency not being available,\n\
    \   other solutions have been developed -- for example, a secondary\n   authentication\
    \ mode with a single root-level password in case the AAA\n   server is not available.\n"
- title: 8.  Management Interactions
  contents:
  - "8.  Management Interactions\n   A static network generally is relatively stable;\
    \ conversely, changes\n   introduce a degree of uncertainty and therefore need\
    \ to be examined\n   in detail.  Also, the troubleshooting of a network exposes\n\
    \   intuitively the complexity of the network.  This section proposes a\n   methodology\
    \ to classify management interactions with regard to their\n   relationship to\
    \ network complexity.\n"
- title: 8.1.  Configuration Complexity
  contents:
  - "8.1.  Configuration Complexity\n   Configuration can be seen as distributed state\
    \ across network devices\n   where the administrator has direct influence on the\
    \ operation of the\n   network.  Modifying the configuration can improve the network\n\
    \   behavior overall or negatively affect it.  In the worst case, a\n   single\
    \ misconfiguration could potentially bring down the entire\n   network.  Therefore,\
    \ it is important that a human administrator can\n   manage the complexity of\
    \ the configuration well.\n   The configuration reflects most of the local and\
    \ global dependencies\n   in the network, as explained in Section 7.  Tracking\
    \ those\n   dependencies in the configuration helps in understanding the overall\n\
    \   network complexity.\n"
- title: 8.2.  Troubleshooting Complexity
  contents:
  - "8.2.  Troubleshooting Complexity\n   Unexpected behavior can have a number of\
    \ sources: the configuration\n   may contain errors, the operating system (algorithms)\
    \ may have bugs,\n   and the hardware may be faulty, which includes anything from\
    \ broken\n   fibers to faulty line cards.  In serious problems, a combination\
    \ of\n   causes could result in a single visible condition.  Tracking the root\n\
    \   causes of an error condition may be extremely difficult, pointing to\n   the\
    \ complex nature of a network.\n   Being able to find the source of a problem\
    \ requires, therefore, a\n   solid understanding of the complexity of a network.\
    \  The\n   configuration complexity discussed in the previous section represents\n\
    \   only a part of the overall problem space.\n"
- title: 8.3.  Monitoring Complexity
  contents:
  - "8.3.  Monitoring Complexity\n   Even in the absence of error conditions, the\
    \ state of the network\n   should be monitored to detect error conditions ideally\
    \ before network\n   services are affected.  For example, a single link-down event\
    \ may not\n   cause a service disruption in a well-designed network, but the\n\
    \   problem needs to be resolved quickly to restore redundancy.\n   Monitoring\
    \ a network has itself a certain complexity.  Issues are in\n   scale; variations\
    \ of devices to be monitored; variations of methods\n   used to collect information;\
    \ the inevitable loss of information as\n   reporting is aggregated centrally;\
    \ and the knowledge required to\n   understand the network, the dependencies,\
    \ and the interactions with\n   users and other external inputs.\n"
- title: 8.4.  Complexity of System Integration
  contents:
  - "8.4.  Complexity of System Integration\n   A network doesn't just consist of\
    \ network devices but includes a vast\n   array of backend and support systems.\
    \  It also interfaces a large\n   variety of user devices, and a number of human\
    \ interfaces, both to\n   the user/customer as well as to administrators of the\
    \ network.  A\n   system integration job is required in order to make sure the\
    \ overall\n   network provides the overall service expected.\n   All those interactions\
    \ and systems have to be modeled to understand\n   the interdependencies and complexities\
    \ in the network.  This is a\n   large area of future research.\n"
- title: 9.  External Interactions
  contents:
  - "9.  External Interactions\n   A network is not a self-contained entity, but it\
    \ exists to provide\n   connectivity and services to users and other networks,\
    \ both of which\n   are outside the direct control of a network administrator.\
    \  The user\n   experience of a network also illustrates a form of interaction\
    \ with\n   its own complexity.\n   External interactions fall into the following\
    \ categories:\n   o  User Interactions: Users need a way to request a service,\
    \ to have\n      their problems resolved, and potentially to get billed for their\n\
    \      usage.  There are a number of human interfaces that need to be\n      considered,\
    \ which depend to some extent on the network, for\n      example, for troubleshooting\
    \ or monitoring usage.\n   o  Interactions with End Systems: The network also\
    \ interacts with the\n      devices that connect to it.  Typically, a device receives\
    \ an IP\n      address from the network and information on how to resolve domain\n\
    \      names, plus potentially other services.  While those interactions\n   \
    \   are relatively simple, the vast amount of end-device types makes\n      this\
    \ a complicated space to track.\n   o  Internetwork Interactions: Most networks\
    \ connect to other\n      networks.  Also, in this case, there are many interactions\
    \ between\n      networks, both technical (for example, running a routing protocol)\n\
    \      as well as non-technical (for example, tracing problems across\n      network\
    \ boundaries).\n   For a fully operational network providing services to users,\
    \ the\n   external interactions and dependencies also form an integral part of\n\
    \   the overall complexity of the network service.  A specific example\n   are\
    \ the root DNS servers, which are critical to the function of the\n   Internet.\
    \  Practically all Internet users have an implicit dependency\n   on the root\
    \ DNS servers, which explains why those are frequent\n   targets for attacks.\
    \  Understanding the overall complexity of a\n   network includes understanding\
    \ all those external dependencies.  Of\n   course, in the case of the root DNS\
    \ servers, there is little a\n   network operator can influence.\n"
- title: 10.  Examples
  contents:
  - "10.  Examples\n   In the foreseeable future, it is unlikely to define a single,\n\
    \   objective metric that includes all the relevant aspects of\n   complexity.\
    \  In the absence of such a global metric, a comparative\n   approach could be\
    \ easier.\n   For example, it is possible to compare the complexity of a\n   centralized\
    \ system where algorithms run centrally and the results are\n   distributed to\
    \ the network nodes with a distributed algorithm.  The\n   type of algorithm may\
    \ be similar, but the location is different, and\n   a different dependency graph\
    \ would result.  The supporting hardware\n   may be the same and thus could be\
    \ ignored for this exercise.  Also,\n   layering is likely to be the same.  The\
    \ management interactions,\n   though, would significantly differ in both cases.\n\
    \   The classification in this document also makes it easier to survey\n   existing\
    \ research with regards to which area of complexity is\n   covered.  This could\
    \ help in identifying open areas for research.\n"
- title: 11.  Security Considerations
  contents:
  - "11.  Security Considerations\n   This document does not discuss any specific\
    \ security considerations.\n"
- title: 12.  Informative References
  contents:
  - "12.  Informative References\n   [Behringer] Behringer, M., \"Classifying Network\
    \ Complexity\",\n               Proceedings of the 2009 Workshop on Re-architecting\
    \ the\n               Internet (Re-Arch '09), ACM, DOI 10.1145/1658978.1658983,\n\
    \               December 2009.\n   [Chun]      Chun, B-G., Ratnasamy, S., and\
    \ E. Eddie, \"NetComplex: A\n               Complexity Metric for Networked System\
    \ Designs\",\n               Proceedings of the 5th USENIX Symposium on Networked\n\
    \               Systems Design and Implementation (NSDI '08), pp.\n          \
    \     393-406, April 2008, <http://usenix.org/events/nsdi08/\n               tech/full_papers/chun/chun.pdf>.\n\
    \   [Doyle]     Doyle, J., Anderson, D., Li, L., Low, S., Roughnan, M.,\n    \
    \           Shalunov, S., Tanaka, R., and W. Willinger, \"The 'robust\n      \
    \         yet fragile' nature of the Internet\", Proceedings of the\n        \
    \       National Academy of Sciences of the United States of\n               America\
    \ (PNAS), Volume 102, Number 41,\n               DOI 10.1073/pnas.0501426102,\
    \ October 2005.\n   [ncrg]      IRTF, \"IRTF Network Complexity Research Group\
    \ (NCRG)\n               [CONCLUDED]\", <https://irtf.org/concluded/ncrg>.\n \
    \  [RFC1925]   Callon, R., \"The Twelve Networking Truths\", RFC 1925,\n     \
    \          DOI 10.17487/RFC1925, April 1996,\n               <http://www.rfc-editor.org/info/rfc1925>.\n\
    \   [RFC3439]   Bush, R. and D. Meyer, \"Some Internet Architectural\n       \
    \        Guidelines and Philosophy\", RFC 3439,\n               DOI 10.17487/RFC3439,\
    \ December 2002,\n               <http://www.rfc-editor.org/info/rfc3439>.\n \
    \  [wiki]      \"Network Complexity - The Wiki\",\n               <http://networkcomplexity.org/>.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   The motivations and framework of this overview of studies\
    \ into\n   network complexity are the result of many meetings and discussions\n\
    \   with too many people to provide a full list here.  However, key\n   contributions\
    \ have been made by John Doyle, Dave Meyer, Jon\n   Crowcroft, Mark Handley, Fred\
    \ Baker, Paul Vixie, Lars Eggert, Bob\n   Briscoe, Keith Jones, Bruno Klauser,\
    \ Stephen Youell, Joel Obstfeld,\n   and Philip Eardley.\n   The authors would\
    \ like to acknowledge the contributions of Rana\n   Sircar, Ken Carlberg, and\
    \ Luca Caviglione in the preparation of this\n   document.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Michael H. Behringer\n   Cisco Systems\n   Building D,\
    \ 45 Allee des Ormes\n   Mougins  06250\n   France\n   Email: mbehring@cisco.com\n\
    \   Alvaro Retana\n   Cisco Systems\n   7025 Kit Creek Rd.\n   Research Triangle\
    \ Park, NC  27709\n   United States of America\n   Email: aretana@cisco.com\n\
    \   Russ White\n   Ericsson\n   144 Warm Wood Lane\n   Apex, NC   27539\n   United\
    \ States of America\n   Email: russ@riw.us\n   URI:   http://www.ericsson.com\n\
    \   Geoff Huston\n   Asia Pacific Network Information Centre\n   6 Cordelia St\n\
    \   South Brisbane, QLD  4101\n   Australia\n   Email: gih@apnic.net\n   URI:\
    \   http://www.apnic.net\n"
