- title: __initial_text__
  contents:
  - '                     DNS Support for Load Balancing

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  This memo\n   does not specify an Internet standard of any kind.  Distribution\
    \ of\n   this memo is unlimited.\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   This RFC is meant to first chronicle a foray into the IETF\
    \ DNS\n   Working Group, discuss other possible alternatives to\n   provide/simulate\
    \ load balancing support for DNS, and to provide an\n   ultimate, flexible solution\
    \ for providing DNS support for balancing\n   loads of many types.\n"
- title: 2. History
  contents:
  - "2. History\n   The history of this probably dates back well before my own time\
    \ - so\n   undoubtedly some holes are here.  Hopefully they can be filled in by\n\
    \   other authors.\n   Initially; \"load balancing\" was intended to permit the\
    \ Domain Name\n   System (DNS) [1] agents to support the concept of \"clusters\"\
    \ (derived\n   from the VMS usage) of machines - where all machines were\n   functionally\
    \ similar or the same, and it didn't particularly matter\n   which machine was\
    \ picked - as long as the load of the processing was\n   reasonably well distributed\
    \ across a series of actual different\n   hosts.  Around 1986 a number of different\
    \ schemes started surfacing\n   as hacks to the Berkeley Internet Name Domain\
    \ server (BIND)\n   distribution.  Probably the most widely distributed of these\
    \ were the\n   \"Shuffle Address\" (SA) modifications by Bryan Beecher, or possibly\n\
    \   Marshall Rose's \"Round Robin\" code.\n   The SA records, however, did a round-robin\
    \ ordering of the Address\n   resource records, and didn't do much with regard\
    \ to the particular\n   loads on the target machines.  Matt Madison (of TGV) implemented\
    \ some\n   changes that used VMS facilities to review the system loads, and\n\
    \   return A RRs in the order of least-loaded to most loaded.\n   The problem\
    \ was with SAs was that load was not actually a factor, and\n   TGV's relied on\
    \ VMS specific facilities to order the records.  The SA\n   RRs required changes\
    \ to the DNS specification (in file syntax and in\n   record processing).  These\
    \ were both viewed as drawbacks and not as\n   general solutions.\n   Most of\
    \ the Internet waited in anticipation of an IETF approved\n   method for simulating\
    \ \"clusters\".\n   Through a few IETF DNS Working Group sessions (Chaired by\
    \ Rob Austein\n   of Epilogue), it was collectively agreed upon that a number\
    \ of\n   criteria must be met:\n       A) Backwards compatibility with the existing\
    \ DNS RFC.\n       B) Information changes frequently.\n       C) Multiple addresses\
    \ should be sent out.\n       D) Must interact with other RRs appropriately.\n\
    \       E) Must be able to represent many types of \"loads\"\n       F) Must be\
    \ fast.\n   (A) would ensure that the installed base of BIND and other DNS\n \
    \  implementations would continue to operate and interoperate properly.\n   (B)\
    \ would permit very fast update times - to enable modeling of\n   real-time data.\
    \  Five minutes was thought as a normal interval,\n   though changes as fast as\
    \ every sixty seconds could be imagined.\n   (C) would cover the possibility of\
    \ a host's address being advertised\n   as optimal, yet the machine crashed during\
    \ the period within the TTL\n   of the RR.  The second-most preferable address\
    \ would be advertised\n   second, the third-most preferable third, and so on.\
    \  This would allow\n   a reasonable stab at recovery during machine failures.\n\
    \   (D) would ensure correct handling of all ancillary information - such\n  \
    \ as MX, RP, and TXT information, as well as reverse lookup\n   information. \
    \ It needed to be ensured that such processes as mail\n   handling continued to\
    \ work in an unsurprising and predictable manner.\n   (E) would ensure the flexibility\
    \ that everyone wished.  A breadth of\n   \"loads\" were wished to be represented\
    \ by various members of the DNS\n   Working Group.  Some \"loads\" were fairly\
    \ eclectic - such as the\n   address ordering by the RTT to the host, some were\
    \ pragmatic - such\n   as balancing the CPU load evenly across a series of hosts.\
    \  All\n   represented valid concerns within their own context, and the idea of\n\
    \   having separate RR types for each was unthinkable (primarily; it\n   would\
    \ violate goal A).\n   (F) needed to ensure a few things.  Primarily that the\
    \ time to\n   calculate the information to order the addressing information did\
    \ not\n   exceed the TTL of the information distributed - i.e., that elements\n\
    \   with a TTL of five minutes didn't take six minutes to calculate.\n   Similarly;\
    \ it seems a fairly clear goal in the DNS RFC that clients\n   should not be kept\
    \ waiting - that request processing should continue\n   regardless of the state\
    \ of any other processing occurring.\n"
- title: 3. Possible Alternatives
  contents:
  - "3. Possible Alternatives\n   During various discussions with the DNS Working\
    \ Group and with the\n   Load Balancing Committee, it was noted that no existing\
    \ solution\n   dealt with all wishes appropriately.  One of the major successes\
    \ of\n   the DNS is its flexibility - and it was felt that this needed to be\n\
    \   retained in all aspects.  It was conceived that perhaps not only\n   address\
    \ information would need to be changed rapidly, but other\n   records may also\
    \ need to change rapidly (at least this could not be\n   ruled out - who knows\
    \ what technologies lurk in the future).\n   Of primary concern to many was the\
    \ ability to interact with older\n   implementations of DNS.  The DNS is implemented\
    \ widely now, and\n   changes to critical portions of the protocol could cause\
    \ havoc for\n   years.  It became rapidly apparent through conversations with\
    \ Jon\n   Postel and Dave Crocker (Area Director) that modifications to the\n\
    \   protocol would be viewed dimly.\n"
- title: 4. A Flexible Model
  contents:
  - "4. A Flexible Model\n   During many hours of discussions, it arose upon suggestion\
    \ from Rob\n   Austein that the changes could be implemented without changes to\
    \ the\n   protocol; if zone transfer behavior could be subtly changed, then the\n\
    \   zone transfer process could accommodate the changing of various RR\n   information.\
    \  What was needed was a smarter program to do the zone\n   transfers.  Pursuant\
    \ to this, changes were made to BIND that would\n   permit the specification of\
    \ the program to do the zone transfers for\n   particular zones.\n   There is\
    \ no specification that a secondary has to receive updates\n   from its primary\
    \ server in any specific manner - only that it needs\n   to check periodically,\
    \ and obtain new zone copies when changes have\n   been made.  Conceivably the\
    \ zone transfer agent could obtain the\n   information from any number of sources\
    \ (e.g., a load average daemon,\n   a round-robin sorter) and present the information\
    \ back to the\n   nameserver for distribution.\n   A number of questions arose\
    \ from this concept, and all seem to have\n   been dealt with accordingly.  Primarily,\
    \ the DNS protocol doesn't\n   guarantee ordering.  While the DNS protocol doesn't\
    \ guarantee\n   ordering, it is clear that the ordering is predictive - that\n\
    \   information read in twice in the same order will be presented twice\n   in\
    \ the same order to clients.  Clients, of course, may reorder this\n   information,\
    \ but that is deemed as a \"local issue\" as it is\n   configurable by the remote\
    \ systems administrators (e.g., sortlists,\n   etc).  The zone transfer agent\
    \ would have to account for any \"mis-\n   ordering\" that may occur locally,\
    \ but remote reordering (e.g., client\n   side sortlists) of RRs is is impossible\
    \ to predict.  Since local\n   mis-ordering is consistent, the zone transfer agents\
    \ could easily\n   account for this.\n   Secondarily, but perhaps more subtly,\
    \ the problem arises that zone\n   transfers aren't used by primary nameservers,\
    \ only by secondary\n   nameservers.  To clarify this, the idea of \"fast\" or\
    \ \"volatile\"\n   subzones must be dealt with.  In a volatile environment (where\n\
    \   address or other RR ordering changes rapidly), the refresh rate of a\n   zone\
    \ must be set very low, and the TTL of the RRs handed out must\n   similarly be\
    \ very low.  There is no use in handing out information\n   with TTLs of an hour,\
    \ when the conditions for ordering the RRs\n   changes minutely.  There must be\
    \ a relatively close relationship\n   between the refresh rates and TTLs of the\
    \ information.  Of course,\n   with very low refresh rates, zone transfers between\
    \ the primary and\n   secondary would have to occur frequently.  Given that primary\
    \ and\n   secondary nameservers should be topologically and geographically far\n\
    \   apart, moving that much data that frequently is seen as prohibitive.\n   Also;\
    \ the longer the propagation time between the primary and\n   secondary, the larger\
    \ the window in which circumstances can change -\n   thus invalidating the secondary's\
    \ information.  It is generally\n   thought that passing volatile information\
    \ on to a secondary is fairly\n   useless - if secondaries want accurate information,\
    \ then they should\n   calculate it themselves and not obtain it via zone transfers.\
    \  This\n   avoids the problem with secondaries losing contact with the primaries\n\
    \   (but access to the targets of the volatile domain are still\n   reachable),\
    \ but the secondary has information that is growing stale.\n   What is essentially\
    \ necessary is a secondary (with no primary) which\n   can calculate the necessary\
    \ ordering of the RR data for itself (which\n   also avoids the problem of different\
    \ versions of domain servers\n   predictively ordering RR information in different\
    \ predictive\n   fashions).  For a volatile zone, there is no primary DNS agent,\
    \ but\n   rather a series of autonomous secondary agents.  Each autonomous\n \
    \  secondary agent is, of course, capable of calculating the ordering or\n   content\
    \ of the volatile RRs itself.\n"
- title: 5. Implementation
  contents:
  - "5. Implementation\n   With some help from Masataka Ohta (Tokyo Institute of Technology),\
    \ I\n   implemented modifications to BIND to permit the specification of the\n\
    \   zone transfer program (zone transfer agent) for particular domains:\n    \
    \       transfer        <domain-name>       <program-name>\n   Currently I define\
    \ a separate subdomain that has a few hosts defined\n   in it - all volatile information.\
    \  The zone has a refresh rate of\n   300, and a minimum TTL of 300 indicated.\
    \  The configuration file is\n   indicated as \"volatile.hosts\".  Every 300 seconds\
    \ a program \"doAxfer\"\n   is run to do the zone transfer.  The program \"doAxfer\"\
    \ reads the file\n   \"volatile.hosts.template\" and the file \"volatile.hosts.list\"\
    .  The\n   addresses specified in volatile.hosts.list are rotated a random\n \
    \  number of times, and then substituted (in order) into\n   volatile.hosts.template\
    \ to generate the file volatile.hosts.  The\n   program \"doAxfer\" then exits\
    \ with a value of 1 - to indicate to the\n   nameserver that the zone transfer\
    \ was successful, and that the file\n   should be read in, and the information\
    \ distributed.  This results in\n   a host having multiple addresses, and the\
    \ addresses are randomized\n   every five minutes (300 seconds).\n   Two bugs\
    \ continue to plague us in this endeavor.  BIND currently\n   considers any TTL\
    \ under 300 seconds as \"irrational\", and substitutes\n   in the value of 300\
    \ instead.  This greatly hampers the functionality\n   of volatile zones.  In\
    \ the fastest of all cases - a 0 TTL -\n   information would be used once, and\
    \ then thrown away.  Presumably the\n   new RR information could be calculated\
    \ every 5 seconds, and the RRs\n   handed out with a TTL of 0.  It must be considered\
    \ that one\n   limitation of the speed of a zone is going to be the ability of\
    \ a\n   machine to calculate new information fast enough.\n   The other bug that\
    \ also effects this is that, as with TTLs, BIND\n   considers any zone refresh\
    \ rate under 15 minutes to be similarly\n   irrational.  Obviously zone refresh\
    \ rates of 15 minutes is\n   unacceptable for this sort of applications.\n   For\
    \ a work-around, the current code sets these same hard-coded values\n   to 60\
    \ seconds.  Sixty seconds is still large enough to avoid any\n   residual bugs\
    \ associated with small timer values, but is also short\n   enough to allow fast\
    \ subzones to be of use.\n   This version of BIND is currently in release within\
    \ Rutgers\n   University, operating in both \"fast\" and normal zones.\n"
- title: 6. Performance
  contents:
  - "6. Performance\n   While the performance of fast zones isn't exactly stellar,\
    \ it is not\n   much more than the normal CPU loads induced by BIND.  Testing\
    \ was\n   performed on a Sun Sparc-2 being used as a normal workstation, but no\n\
    \   resolvers were using the name server - essentially the nameserver was\n  \
    \ idle.  For a configuration with no fast subzones, BIND accrued 11 CPU\n   seconds\
    \ in 24 hours.  For a configuration with one fast zone, six\n   address records,\
    \ and being refreshed every 300 seconds (5 minutes),\n   BIND accrued 1 minute\
    \ 4 seconds CPU time.  For the same previous\n   configuration, but being refreshed\
    \ every sixty seconds, BIND accrued\n   5 minutes and 38 seconds of CPU time.\n\
    \   As is no great surprise, the CPU load on the serving machine was\n   linear\
    \ to the frequency of the refresh time.  The sixty second\n   refresh configuration\
    \ used approximately five times as much CPU time\n   as did the 300 second refresh\
    \ configuration.  One can easily\n   extrapolate that the overall CPU utilization\
    \ would be linear to the\n   number of zones and the frequency of the refresh\
    \ period.  All of this\n   is based on a shell script that always indicated that\
    \ a zone update\n   was necessary, a more intelligent program should realize when\
    \ the\n   reordering of the RRs was unnecessary and avoid such periodic zone\n\
    \   reloads.\n"
- title: 7. Acknowledgments
  contents:
  - "7. Acknowledgments\n   Most of the ideas in this document are the results of\
    \ conversations\n   and proposals from many, many people - including, but not\
    \ limited to,\n   Robert Austein, Stuart Vance, Masataka Ohta, Marshall Rose,\
    \ and the\n   members of the IETF DNS Working Group.\n"
- title: 8. References
  contents:
  - "8. References\n   [1] Mockapetris, P., \"Domain Names - Implementation and\n\
    \       Specification\", STD 13, RFC 1035, USC/Information Sciences\n       Institute,\
    \ November 1987.\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   Security issues are not discussed in this memo.\n"
- title: 10. Author's Address
  contents:
  - "10. Author's Address\n   Thomas P. Brisco\n   Associate Director for Network\
    \ Operations\n   Rutgers University\n   Computing Services, Telecommunications\
    \ Division\n   Hill Center for the Mathematical Sciences\n   Busch Campus\n  \
    \ Piscataway, New Jersey 08855-0879\n   USA\n   Phone: +1-908-445-2351\n   EMail:\
    \ brisco@rutgers.edu\n"
