- title: __initial_text__
  contents:
  - '       Address Resolution Problems in Large Data Center Networks

    '
- title: Abstract
  contents:
  - "Abstract\n   This document examines address resolution issues related to the\n\
    \   scaling of data centers with a very large number of hosts.  The scope\n  \
    \ of this document is relatively narrow, focusing on address resolution\n   (the\
    \ Address Resolution Protocol (ARP) in IPv4 and Neighbor Discovery\n   (ND) in\
    \ IPv6) within a data center.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc6820.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2013 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \   2. Terminology .....................................................3\n  \
    \ 3. Background ......................................................4\n   4.\
    \ Address Resolution in IPv4 ......................................6\n   5. Address\
    \ Resolution in IPv6 ......................................7\n   6. Generalized\
    \ Data Center Design ..................................7\n      6.1. Access Layer\
    \ ...............................................8\n      6.2. Aggregation Layer\
    \ ..........................................8\n      6.3. Core .......................................................9\n\
    \      6.4. L3/L2 Topological Variations ...............................9\n  \
    \         6.4.1. L3 to Access Switches ...............................9\n    \
    \       6.4.2. L3 to Aggregation Switches ..........................9\n      \
    \     6.4.3. L3 in the Core Only ................................10\n        \
    \   6.4.4. Overlays ...........................................10\n      6.5.\
    \ Factors That Affect Data Center Design ....................11\n           6.5.1.\
    \ Traffic Patterns ...................................11\n           6.5.2. Virtualization\
    \ .....................................11\n           6.5.3. Summary ............................................12\n\
    \   7. Problem Itemization ............................................12\n  \
    \    7.1. ARP Processing on Routers .................................12\n    \
    \  7.2. IPv6 Neighbor Discovery ...................................14\n      7.3.\
    \ MAC Address Table Size Limitations in Switches ............15\n   8. Summary\
    \ ........................................................15\n   9. Acknowledgments\
    \ ................................................16\n   10. Security Considerations\
    \ .......................................16\n   11. Informative References ........................................16\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document examines issues related to the scaling of\
    \ large data\n   centers.  Specifically, this document focuses on address resolution\n\
    \   (ARP in IPv4 and Neighbor Discovery in IPv6) within the data center.\n   Although\
    \ strictly speaking the scope of address resolution is\n   confined to a single\
    \ L2 broadcast domain (i.e., ARP runs at the L2\n   layer below IP), the issue\
    \ is complicated by routers having many\n   interfaces on which address resolution\
    \ must be performed or with the\n   presence of IEEE 802.1Q domains, where individual\
    \ VLANs effectively\n   form their own L2 broadcast domains.  Thus, the scope\
    \ of address\n   resolution spans both the L2 link and the devices attached to\
    \ those\n   links.\n   This document identifies potential issues associated with\
    \ address\n   resolution in data centers with a large number of hosts.  The scope\n\
    \   of this document is intentionally relatively narrow, as it mirrors\n   the\
    \ Address Resolution for Massive numbers of hosts in the Data\n   center (ARMD)\
    \ WG charter.  This document lists \"pain points\" that are\n   being experienced\
    \ in current data centers.  The goal of this document\n   is to focus on address\
    \ resolution issues and not other broader issues\n   that might arise in data\
    \ centers.\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   Address Resolution:  The process of determining the link-layer\n\
    \      address corresponding to a given IP address.  In IPv4, address\n      resolution\
    \ is performed by ARP [RFC0826]; in IPv6, it is provided\n      by Neighbor Discovery\
    \ (ND) [RFC4861].\n   Application:  Software that runs on either a physical or\
    \ virtual\n      machine, providing a service (e.g., web server, database server,\n\
    \      etc.).\n   L2 Broadcast Domain:  The set of all links, repeaters, and switches\n\
    \      that are traversed to reach all nodes that are members of a given\n   \
    \   L2 broadcast domain.  In IEEE 802.1Q networks, a broadcast domain\n      corresponds\
    \ to a single VLAN.\n   Host (or server):  A computer system on the network.\n\
    \   Hypervisor:  Software running on a host that allows multiple VMs to\n    \
    \  run on the same host.\n   Virtual Machine (VM):  A software implementation\
    \ of a physical\n      machine that runs programs as if they were executing on\
    \ a\n      physical, non-virtualized machine.  Applications (generally) do\n \
    \     not know they are running on a VM as opposed to running on a\n      \"bare\"\
    \ host or server, though some systems provide a\n      paravirtualization environment\
    \ that allows an operating system or\n      application to be aware of the presence\
    \ of virtualization for\n      optimization purposes.\n   ToR:  Top-of-Rack Switch.\
    \  A switch placed in a single rack to\n      aggregate network connectivity to\
    \ and from hosts in that rack.\n   EoR:  End-of-Row Switch.  A switch used to\
    \ aggregate network\n      connectivity from multiple racks.  EoR switches are\
    \ the next level\n      of switching above ToR switches.\n"
- title: 3.  Background
  contents:
  - "3.  Background\n   Large, flat L2 networks have long been known to have scaling\n\
    \   problems.  As the size of an L2 broadcast domain increases, the level\n  \
    \ of broadcast traffic from protocols like ARP increases.  Large\n   amounts of\
    \ broadcast traffic pose a particular burden because every\n   device (switch,\
    \ host, and router) must process and possibly act on\n   such traffic.  In extreme\
    \ cases, \"broadcast storms\" can occur where\n   the quantity of broadcast traffic\
    \ reaches a level that effectively\n   brings down part or all of a network. \
    \ For example, poor\n   implementations of loop detection and prevention or misconfiguration\n\
    \   errors can create conditions that lead to broadcast storms as network\n  \
    \ conditions change.  The conventional wisdom for addressing such\n   problems\
    \ has been to say \"don't do that\".  That is, split large L2\n   networks into\
    \ multiple smaller L2 networks, each operating as its own\n   L3/IP subnet.  Numerous\
    \ data center networks have been designed with\n   this principle, e.g., with\
    \ each rack placed within its own L3 IP\n   subnet.  By doing so, the broadcast\
    \ domain (and address resolution)\n   is confined to one ToR switch, which works\
    \ well from a scaling\n   perspective.  Unfortunately, this conflicts in some\
    \ ways with the\n   current trend towards dynamic workload shifting in data centers\
    \ and\n   increased virtualization, as discussed below.\n   Workload placement\
    \ has become a challenging task within data centers.\n   Ideally, it is desirable\
    \ to be able to dynamically reassign workloads\n   within a data center in order\
    \ to optimize server utilization, add\n   more servers in response to increased\
    \ demand, etc.  However, servers\n   are often pre-configured to run with a given\
    \ set of IP addresses.\n   Placement of such servers is then subject to constraints\
    \ of the IP\n   addressing restrictions of the data center.  For example, servers\n\
    \   configured with addresses from a particular subnet could only be\n   placed\
    \ where they connect to the IP subnet corresponding to their IP\n   addresses.\
    \  If each ToR switch is acting as a gateway for its own\n   subnet, a server\
    \ can only be connected to the one ToR switch.  This\n   gateway switch represents\
    \ the L2/L3 boundary.  A similar constraint\n   occurs in virtualized environments,\
    \ as discussed next.\n   Server virtualization is fast becoming the norm in data\
    \ centers.\n   With server virtualization, each physical server supports multiple\n\
    \   virtual machines, each running its own operating system, middleware,\n   and\
    \ applications.  Virtualization is a key enabler of workload\n   agility, i.e.,\
    \ allowing any server to host any application (on its\n   own VM) and providing\
    \ the flexibility of adding, shrinking, or moving\n   VMs within the physical\
    \ infrastructure.  Server virtualization\n   provides numerous benefits, including\
    \ higher utilization, increased\n   data security, reduced user downtime, and\
    \ even significant power\n   conservation, along with the promise of a more flexible\
    \ and dynamic\n   computing environment.\n   The discussion below focuses on VM\
    \ placement and migration.  Keep in\n   mind, however, that even in a non-virtualized\
    \ environment, many of\n   the same issues apply to individual workloads running\
    \ on standalone\n   machines.  For example, when increasing the number of servers\
    \ running\n   a particular workload to meet demand, placement of those workloads\n\
    \   may be constrained by IP subnet numbering considerations, as\n   discussed\
    \ earlier.\n   The greatest flexibility in VM and workload management occurs when\
    \ it\n   is possible to place a VM (or workload) anywhere in the data center\n\
    \   regardless of what IP addresses the VM uses and how the physical\n   network\
    \ is laid out.  In practice, movement of VMs within a data\n   center is easiest\
    \ when VM placement and movement do not conflict with\n   the IP subnet boundaries\
    \ of the data center's network, so that the\n   VM's IP address need not be changed\
    \ to reflect its actual point of\n   attachment on the network from an L3/IP perspective.\
    \  In contrast, if\n   a VM moves to a new IP subnet, its address must change,\
    \ and clients\n   will need to be made aware of that change.  From a VM management\n\
    \   perspective, management is simplified if all servers are on a single\n   large\
    \ L2 network.\n   With virtualization, it is not uncommon to have a single physical\n\
    \   server host ten or more VMs, each having its own IP (and Media Access\n  \
    \ Control (MAC)) addresses.  Consequently, the number of addresses per\n   machine\
    \ (and hence per subnet) is increasing, even when the number of\n   physical machines\
    \ stays constant.  In a few years, the numbers will\n   likely be even higher.\n\
    \   In the past, applications were static in the sense that they tended\n   to\
    \ stay in one physical place.  An application installed on a\n   physical machine\
    \ would stay on that machine because the cost of\n   moving an application elsewhere\
    \ was generally high.  Moreover,\n   physical servers hosting applications would\
    \ tend to be placed in such\n   a way as to facilitate communication locality.\
    \  That is, applications\n   running on servers would be physically located near\
    \ the servers\n   hosting the applications they communicated with most heavily.\
    \  The\n   network traffic patterns in such environments could thus be\n   optimized,\
    \ in some cases keeping significant traffic local to one\n   network segment.\
    \  In these more static and carefully managed\n   environments, it was possible\
    \ to build networks that approached\n   scaling limitations but did not actually\
    \ cross the threshold.\n   Today, with the proliferation of VMs, traffic patterns\
    \ are becoming\n   more diverse and less predictable.  In particular, there can\
    \ easily\n   be less locality of network traffic as VMs hosting applications are\n\
    \   moved for such reasons as reducing overall power usage (by\n   consolidating\
    \ VMs and powering off idle machines) or moving a VM to a\n   physical server\
    \ with more capacity or a lower load.  In today's\n   changing environments, it\
    \ is becoming more difficult to engineer\n   networks as traffic patterns continually\
    \ shift as VMs move around.\n   In summary, both the size and density of L2 networks\
    \ are increasing.\n   In addition, increasingly dynamic workloads and the increased\
    \ usage\n   of VMs are creating pressure for ever-larger L2 networks.  Today,\n\
    \   there are already data centers with over 100,000 physical machines\n   and\
    \ many times that number of VMs.  This number will only increase\n   going forward.\
    \  In addition, traffic patterns within a data center\n   are also constantly\
    \ changing.  Ultimately, the issues described in\n   this document might be observed\
    \ at any scale, depending on the\n   particular design of the data center.\n"
- title: 4.  Address Resolution in IPv4
  contents:
  - "4.  Address Resolution in IPv4\n   In IPv4 over Ethernet, ARP provides the function\
    \ of address\n   resolution.  To determine the link-layer address of a given IP\n\
    \   address, a node broadcasts an ARP Request.  The request is delivered\n   to\
    \ all portions of the L2 network, and the node with the requested IP\n   address\
    \ responds with an ARP Reply.  ARP is an old protocol and, by\n   current standards,\
    \ is sparsely documented.  For example, there are no\n   clear requirements for\
    \ retransmitting ARP Requests in the absence of\n   replies.  Consequently, implementations\
    \ vary in the details of what\n   they actually implement [RFC0826][RFC1122].\n\
    \   From a scaling perspective, there are a number of problems with ARP.\n   First,\
    \ it uses broadcast, and any network with a large number of\n   attached hosts\
    \ will see a correspondingly large amount of broadcast\n   ARP traffic.  The second\
    \ problem is that it is not feasible to change\n   host implementations of ARP\
    \ -- current implementations are too widely\n   entrenched, and any changes to\
    \ host implementations of ARP would take\n   years to become sufficiently deployed\
    \ to matter.  That said, it may\n   be possible to change ARP implementations\
    \ in hypervisors, L2/L3\n   boundary routers, and/or ToR access switches, to leverage\
    \ such\n   techniques as Proxy ARP.  Finally, ARP implementations need to take\n\
    \   steps to flush out stale or otherwise invalid entries.\n   Unfortunately,\
    \ existing standards do not provide clear implementation\n   guidelines for how\
    \ to do this.  Consequently, implementations vary\n   significantly, and some\
    \ implementations are \"chatty\" in that they\n   just periodically flush caches\
    \ every few minutes and send new ARP\n   queries.\n"
- title: 5.  Address Resolution in IPv6
  contents:
  - "5.  Address Resolution in IPv6\n   Broadly speaking, from the perspective of\
    \ address resolution, IPv6's\n   Neighbor Discovery (ND) behaves much like ARP,\
    \ with a few notable\n   differences.  First, ARP uses broadcast, whereas ND uses\
    \ multicast.\n   When querying for a target IP address, ND maps the target address\n\
    \   into an IPv6 Solicited Node multicast address.  Using multicast\n   rather\
    \ than broadcast has the benefit that the multicast frames do\n   not necessarily\
    \ need to be sent to all parts of the network, i.e.,\n   the frames can be sent\
    \ only to segments where listeners for the\n   Solicited Node multicast address\
    \ reside.  In the case where multicast\n   frames are delivered to all parts of\
    \ the network, sending to a\n   multicast address still has the advantage that\
    \ most (if not all)\n   nodes will filter out the (unwanted) multicast query via\
    \ filters\n   installed in the Network Interface Card (NIC) rather than burdening\n\
    \   host software with the need to process such packets.  Thus, whereas\n   all\
    \ nodes must process every ARP query, ND queries are processed only\n   by the\
    \ nodes to which they are intended.  In cases where multicast\n   filtering can't\
    \ effectively be implemented in the NIC (e.g., as on\n   hypervisors supporting\
    \ virtualization), filtering would need to be\n   done in software (e.g., in the\
    \ hypervisor's vSwitch).\n"
- title: 6.  Generalized Data Center Design
  contents:
  - "6.  Generalized Data Center Design\n   There are many different ways in which\
    \ data center networks might be\n   designed.  The designs are usually engineered\
    \ to suit the particular\n   workloads that are being deployed in the data center.\
    \  For example, a\n   large web server farm might be engineered in a very different\
    \ way\n   than a general-purpose multi-tenant cloud hosting service.  However,\n\
    \   in most cases the designs can be abstracted into a typical three-\n   layer\
    \ model consisting of an access layer, an aggregation layer, and\n   the Core.\
    \  The access layer generally refers to the switches that are\n   closest to the\
    \ physical or virtual servers; the aggregation layer\n   serves to interconnect\
    \ multiple access-layer devices.  The Core\n   switches connect the aggregation\
    \ switches to the larger network core.\n   Figure 1 shows a generalized data center\
    \ design, which captures the\n   essential elements of various alternatives.\n\
    \                  +-----+-----+     +-----+-----+\n                  |   Core0\
    \   |     |    Core1  |      Core\n                  +-----+-----+     +-----+-----+\n\
    \                        /    \\        /       /\n                       /  \
    \    \\----------\\   /\n                      /    /---------/    \\ /\n    \
    \                +-------+           +------+\n                  +/------+ | \
    \        +/-----+ |\n                  | Aggr11| + --------|AggrN1| +      Aggregation\
    \ Layer\n                  +---+---+/          +------+/\n                   \
    \ /     \\            /      \\\n                   /       \\          /    \
    \    \\\n                 +---+    +---+      +---+     +---+\n              \
    \   |T11|... |T1x|      |TN1|     |TNy|  Access Layer\n                 +---+\
    \    +---+      +---+     +---+\n                 |   |    |   |      |   |  \
    \   |   |\n                 +---+    +---+      +---+     +---+\n            \
    \     |   |... |   |      |   |     |   |\n                 +---+    +---+   \
    \   +---+     +---+  Server Racks\n                 |   |... |   |      |   |\
    \     |   |\n                 +---+    +---+      +---+     +---+\n          \
    \       |   |... |   |      |   |     |   |\n                 +---+    +---+ \
    \     +---+     +---+\n               Typical Layered Architecture in a Data Center\n\
    \                                 Figure 1\n"
- title: 6.1.  Access Layer
  contents:
  - "6.1.  Access Layer\n   The access switches provide connectivity directly to/from\
    \ physical\n   and virtual servers.  The access layer may be implemented by wiring\n\
    \   the servers within a rack to a ToR switch or, less commonly, the\n   servers\
    \ could be wired directly to an EoR switch.  A server rack may\n   have a single\
    \ uplink to one access switch or may have dual uplinks to\n   two different access\
    \ switches.\n"
- title: 6.2.  Aggregation Layer
  contents:
  - "6.2.  Aggregation Layer\n   In a typical data center, aggregation switches interconnect\
    \ many ToR\n   switches.  Usually, there are multiple parallel aggregation switches,\n\
    \   serving the same group of ToRs to achieve load sharing.  It is no\n   longer\
    \ uncommon to see aggregation switches interconnecting hundreds\n   of ToR switches\
    \ in large data centers.\n"
- title: 6.3.  Core
  contents:
  - "6.3.  Core\n   Core switches provide connectivity between aggregation switches\
    \ and\n   the main data center network.  Core switches interconnect different\n\
    \   sets of racks and provide connectivity to data center gateways\n   leading\
    \ to external networks.\n"
- title: 6.4.  L3/L2 Topological Variations
  contents:
  - '6.4.  L3/L2 Topological Variations

    '
- title: 6.4.1.  L3 to Access Switches
  contents:
  - "6.4.1.  L3 to Access Switches\n   In this scenario, the L3 domain is extended\
    \ all the way from the core\n   network to the access switches.  Each rack enclosure\
    \ consists of a\n   single L2 domain, which is confined to the rack.  In general,\
    \ there\n   are no significant ARP/ND scaling issues in this scenario, as the\
    \ L2\n   domain cannot grow very large.  Such a topology has benefits in\n   scenarios\
    \ where servers attached to a particular access switch\n   generally run VMs that\
    \ are confined to using a single subnet.  These\n   VMs and the applications they\
    \ host aren't moved (migrated) to other\n   racks that might be attached to different\
    \ access switches (and\n   different IP subnets).  A small server farm or very\
    \ static compute\n   cluster might be well served via this design.\n"
- title: 6.4.2.  L3 to Aggregation Switches
  contents:
  - "6.4.2.  L3 to Aggregation Switches\n   When the L3 domain extends only to aggregation\
    \ switches, hosts in any\n   of the IP subnets configured on the aggregation switches\
    \ can be\n   reachable via L2 through any access switches if access switches\n\
    \   enable all the VLANs.  Such a topology allows a greater level of\n   flexibility,\
    \ as servers attached to any access switch can run any VMs\n   that have been\
    \ provisioned with IP addresses configured on the\n   aggregation switches.  In\
    \ such an environment, VMs can migrate\n   between racks without IP address changes.\
    \  The drawback of this\n   design, however, is that multiple VLANs have to be\
    \ enabled on all\n   access switches and all access-facing ports on aggregation\
    \ switches.\n   Even though L2 traffic is still partitioned by VLANs, the fact\
    \ that\n   all VLANs are enabled on all ports can lead to broadcast traffic on\n\
    \   all VLANs that traverse all links and ports, which has the same\n   effect\
    \ as one big L2 domain on the access-facing side of the\n   aggregation switch.\
    \  In addition, the internal traffic itself might\n   have to cross different\
    \ L2 boundaries, resulting in significant\n   ARP/ND load at the aggregation switches.\
    \  This design provides a good\n   tradeoff between flexibility and L2 domain\
    \ size.  A moderate-sized\n   data center might utilize this approach to provide\
    \ high-availability\n   services at a single location.\n"
- title: 6.4.3.  L3 in the Core Only
  contents:
  - "6.4.3.  L3 in the Core Only\n   In some cases, where a wider range of VM mobility\
    \ is desired (i.e., a\n   greater number of racks among which VMs can move without\
    \ IP address\n   changes), the L3 routed domain might be terminated at the core\n\
    \   routers themselves.  In this case, VLANs can span multiple groups of\n   aggregation\
    \ switches, which allows hosts to be moved among a greater\n   number of server\
    \ racks without IP address changes.  This scenario\n   results in the largest\
    \ ARP/ND performance impact, as explained later.\n   A data center with very rapid\
    \ workload shifting may consider this\n   kind of design.\n"
- title: 6.4.4.  Overlays
  contents:
  - "6.4.4.  Overlays\n   There are several approaches where overlay networks can\
    \ be used to\n   build very large L2 networks to enable VM mobility.  Overlay\
    \ networks\n   using various L2 or L3 mechanisms allow interior switches/routers\
    \ to\n   mask host addresses.  In addition, L3 overlays can help the data\n  \
    \ center designer control the size of the L2 domain and also enhance\n   the ability\
    \ to provide multi-tenancy in data center networks.\n   However, the use of overlays\
    \ does not eliminate traffic associated\n   with address resolution; it simply\
    \ moves it to regular data traffic.\n   That is, address resolution is implemented\
    \ in the overlay and is not\n   directly visible to the switches of the data center\
    \ network.\n   A potential problem that arises in a large data center is that\
    \ when a\n   large number of hosts communicate with their peers in different\n\
    \   subnets, all these hosts send (and receive) data packets to their\n   respective\
    \ L2/L3 boundary nodes, as the traffic flows are generally\n   bidirectional.\
    \  This has the potential to further highlight any\n   scaling problems.  These\
    \ L2/L3 boundary nodes have to process ARP/ND\n   requests sent from originating\
    \ subnets and resolve physical (MAC)\n   addresses in the target subnets for what\
    \ are generally bidirectional\n   flows.  Therefore, for maximum flexibility in\
    \ managing the data\n   center workload, it is often desirable to use overlays\
    \ to place\n   related groups of hosts in the same topological subnet to avoid\
    \ the\n   L2/L3 boundary translation.  The use of overlays in the data center\n\
    \   network can be a useful design mechanism to help manage a potential\n   bottleneck\
    \ at the L2/L3 boundary by redefining where that boundary\n   exists.\n"
- title: 6.5.  Factors That Affect Data Center Design
  contents:
  - '6.5.  Factors That Affect Data Center Design

    '
- title: 6.5.1.  Traffic Patterns
  contents:
  - "6.5.1.  Traffic Patterns\n   Expected traffic patterns play an important role\
    \ in designing\n   appropriately sized access, aggregation, and core networks.\
    \  Traffic\n   patterns also vary based on the expected use of the data center.\n\
    \   Broadly speaking, it is desirable to keep as much traffic as possible\n  \
    \ on the access layer in order to minimize the bandwidth usage at the\n   aggregation\
    \ layer.  If the expected use of the data center is to\n   serve as a large web\
    \ server farm, where thousands of nodes are doing\n   similar things and the traffic\
    \ pattern is largely in and out of a\n   large data center, an access layer with\
    \ EoR switches might be used,\n   as it minimizes complexity, allows for servers\
    \ and databases to be\n   located in the same L2 domain, and provides for maximum\
    \ density.\n   A data center that is expected to host a multi-tenant cloud hosting\n\
    \   service might have some completely unique requirements.  In order to\n   isolate\
    \ inter-customer traffic, smaller L2 domains might be\n   preferred, and though\
    \ the size of the overall data center might be\n   comparable to the previous\
    \ example, the multi-tenant nature of the\n   cloud hosting application requires\
    \ a smaller and more\n   compartmentalized access layer.  A multi-tenant environment\
    \ might\n   also require the use of L3 all the way to the access-layer ToR\n \
    \  switch.\n   Yet another example of a workload with a unique traffic pattern\
    \ is a\n   high-performance compute cluster, where most of the traffic is\n  \
    \ expected to stay within the cluster but at the same time there is a\n   high\
    \ degree of crosstalk between the nodes.  This would once again\n   call for a\
    \ large access layer in order to minimize the requirements\n   at the aggregation\
    \ layer.\n"
- title: 6.5.2.  Virtualization
  contents:
  - "6.5.2.  Virtualization\n   Using virtualization in the data center further serves\
    \ to increase\n   the possible densities that can be achieved.  However, virtualization\n\
    \   also further complicates the requirements on the access layer, as\n   virtualization\
    \ restricts the scope of server placement in the event\n   of server failover\
    \ resulting from hardware failures or server\n   migration for load balancing\
    \ or other reasons.\n   Virtualization also can place additional requirements\
    \ on the\n   aggregation switches in terms of address resolution table size and\n\
    \   the scalability of any address-learning protocols that might be used\n   on\
    \ those switches.  The use of virtualization often also requires the\n   use of\
    \ additional VLANs for high-availability beaconing, which would\n   need to span\
    \ the entire virtualized infrastructure.  This would\n   require the access layer\
    \ to also span the entire virtualized\n   infrastructure.\n"
- title: 6.5.3.  Summary
  contents:
  - "6.5.3.  Summary\n   The designs described in this section have a number of tradeoffs.\n\
    \   The \"L3 to access switches\" design described in Section 6.4.1 is the\n \
    \  only design that constrains L2 domain size in a fashion that avoids\n   ARP/ND\
    \ scaling problems.  However, that design has limitations and\n   does not address\
    \ some of the other requirements that lead to\n   configurations that make use\
    \ of larger L2 domains.  Consequently,\n   ARP/ND scaling issues are a real problem\
    \ in practice.\n"
- title: 7.  Problem Itemization
  contents:
  - "7.  Problem Itemization\n   This section articulates some specific problems or\
    \ \"pain points\" that\n   are related to large data centers.\n"
- title: 7.1.  ARP Processing on Routers
  contents:
  - "7.1.  ARP Processing on Routers\n   One pain point with large L2 broadcast domains\
    \ is that the routers\n   connected to the L2 domain may need to process a significant\
    \ amount\n   of ARP traffic in some cases.  In particular, environments where\
    \ the\n   aggregate level of ARP traffic is very large may lead to a heavy ARP\n\
    \   load on routers.  Even though the vast majority of ARP traffic may\n   not\
    \ be aimed at that router, the router still has to process enough\n   of the ARP\
    \ Request to determine whether it can safely be ignored.\n   The ARP algorithm\
    \ specifies that a recipient must update its ARP\n   cache if it receives an ARP\
    \ query from a source for which it has an\n   entry [RFC0826].\n   ARP processing\
    \ in routers is commonly handled in a \"slow path\"\n   software processor, rather\
    \ than directly by a hardware Application-\n   Specific Integrated Circuit (ASIC)\
    \ as is the case when forwarding\n   packets.  Such a design significantly limits\
    \ the rate at which ARP\n   traffic can be processed compared to the rate at which\
    \ ASICs can\n   forward traffic.  Current implementations at the time of this\
    \ writing\n   can support ARP processing in the low thousands of ARP packets per\n\
    \   second.  In some deployments, limitations on the rate of ARP\n   processing\
    \ have been cited as being a problem.\n   To further reduce the ARP load, some\
    \ routers have implemented\n   additional optimizations in their forwarding ASIC\
    \ paths.  For\n   example, some routers can be configured to discard ARP Requests\
    \ for\n   target addresses other than those assigned to the router.  That way,\n\
    \   the router's software processor only receives ARP Requests for\n   addresses\
    \ it owns and must respond to.  This can significantly reduce\n   the number of\
    \ ARP Requests that must be processed by the router.\n   Another optimization\
    \ concerns reducing the number of ARP queries\n   targeted at routers, whether\
    \ for address resolution or to validate\n   existing cache entries.  Some routers\
    \ can be configured to broadcast\n   periodic gratuitous ARPs [RFC5227].  Upon\
    \ receipt of a gratuitous\n   ARP, implementations mark the associated entry as\
    \ \"fresh\", resetting\n   the aging timer to its maximum setting.  Consequently,\
    \ sending out\n   periodic gratuitous ARPs can effectively prevent nodes from\
    \ needing\n   to send ARP Requests intended to revalidate stale entries for a\n\
    \   router.  The net result is an overall reduction in the number of ARP\n   queries\
    \ routers receive.  Gratuitous ARPs, broadcast to all nodes in\n   the L2 broadcast\
    \ domain, may in some cases also pre-populate ARP\n   caches on neighboring devices,\
    \ further reducing ARP traffic.  But it\n   is not believed that pre-population\
    \ of ARP entries is supported by\n   most implementations, as the ARP specification\
    \ [RFC0826] recommends\n   only that pre-existing ARP entries be updated upon\
    \ receipt of ARP\n   messages; it does not call for the creation of new entries\
    \ when none\n   already exist.\n   Finally, another area concerns the overhead\
    \ of processing IP packets\n   for which no ARP entry exists.  Existing standards\
    \ specify that one\n   or more IP packets for which no ARP entries exist should\
    \ be queued\n   pending successful completion of the address resolution process\n\
    \   [RFC1122] [RFC1812].  Once an ARP query has been resolved, any queued\n  \
    \ packets can be forwarded on.  Again, the processing of such packets\n   is handled\
    \ in the \"slow path\", effectively limiting the rate at which\n   a router can\
    \ process ARP \"cache misses\", and is viewed as a problem\n   in some deployments\
    \ today.  Additionally, if no response is received,\n   the router may send the\
    \ ARP/ND query multiple times.  If no response\n   is received after a number\
    \ of ARP/ND requests, the router needs to\n   drop any queued data packets and\
    \ may send an ICMP destination\n   unreachable message as well [RFC0792].  This\
    \ entire process can be\n   CPU intensive.\n   Although address resolution traffic\
    \ remains local to one L2 network,\n   some data center designs terminate L2 domains\
    \ at individual\n   aggregation switches/routers (e.g., see Section 6.4.2).  Such\
    \ routers\n   can be connected to a large number of interfaces (e.g., 100 or more).\n\
    \   While the address resolution traffic on any one interface may be\n   manageable,\
    \ the aggregate address resolution traffic across all\n   interfaces can become\
    \ problematic.\n   Another variant of the above issue has individual routers servicing\
    \ a\n   relatively small number of interfaces, with the individual interfaces\n\
    \   themselves serving very large subnets.  Once again, it is the\n   aggregate\
    \ quantity of ARP traffic seen across all of the router's\n   interfaces that\
    \ can be problematic.  This pain point is essentially\n   the same as the one\
    \ discussed above, the only difference being\n   whether a given number of hosts\
    \ are spread across a few large IP\n   subnets or many smaller ones.\n   When\
    \ hosts in two different subnets under the same L2/L3 boundary\n   router need\
    \ to communicate with each other, the L2/L3 router not only\n   has to initiate\
    \ ARP/ND requests to the target's subnet, it also has\n   to process the ARP/ND\
    \ requests from the originating subnet.  This\n   process further adds to the\
    \ overall ARP processing load.\n"
- title: 7.2.  IPv6 Neighbor Discovery
  contents:
  - "7.2.  IPv6 Neighbor Discovery\n   Though IPv6's Neighbor Discovery behaves much\
    \ like ARP, there are\n   several notable differences that result in a different\
    \ set of\n   potential issues.  From an L2 perspective, an important difference\
    \ is\n   that ND address resolution requests are sent via multicast, which\n \
    \  results in ND queries only being processed by the nodes for which\n   they\
    \ are intended.  Compared with broadcast ARPs, this reduces the\n   total number\
    \ of ND packets that an implementation will receive.\n   Another key difference\
    \ concerns revalidating stale ND entries.  ND\n   requires that nodes periodically\
    \ revalidate any entries they are\n   using, to ensure that bad entries are timed\
    \ out quickly enough that\n   TCP does not terminate a connection.  Consequently,\
    \ some\n   implementations will send out \"probe\" ND queries to validate in-use\n\
    \   ND entries as frequently as every 35 seconds [RFC4861].  Such probes\n   are\
    \ sent via unicast (unlike in the case of ARP).  However, on larger\n   networks,\
    \ such probes can result in routers receiving many such\n   queries (i.e., many\
    \ more than with ARP, which does not specify such\n   behavior).  Unfortunately,\
    \ the IPv4 mitigation technique of sending\n   gratuitous ARPs (as described in\
    \ Section 7.1) does not work in IPv6.\n   The ND specification specifically states\
    \ that gratuitous ND \"updates\"\n   cannot cause an ND entry to be marked \"\
    valid\".  Rather, such entries\n   are marked \"probe\", which causes the receiving\
    \ node to (eventually)\n   generate a probe back to the sender, which in this\
    \ case is precisely\n   the behavior that the router is trying to prevent!\n \
    \  Routers implementing Neighbor Unreachability Discovery (NUD) (for\n   neighboring\
    \ destinations) will need to process neighbor cache state\n   changes such as\
    \ transitioning entries from REACHABLE to STALE.  How\n   this capability is implemented\
    \ may impact the scalability of ND on a\n   router.  For example, one possible\
    \ implementation is to have the\n   forwarding operation detect when an ND entry\
    \ is referenced that needs\n   to transition from REACHABLE to STALE, by signaling\
    \ an event that\n   would need to be processed by the software processor.  Such\
    \ an\n   implementation could increase the load on the service processor in\n\
    \   much the same way that high rates of ARP requests have led to\n   problems\
    \ on some routers.\n   It should be noted that ND does not require the sending\
    \ of probes in\n   all cases.  Section 7.3.1 of [RFC4861] describes a technique\
    \ whereby\n   hints from TCP can be used to verify that an existing ND entry is\n\
    \   working fine and does not need to be revalidated.\n   Finally, IPv6 and IPv4\
    \ are often run simultaneously and in parallel\n   on the same network, i.e.,\
    \ in dual-stack mode.  In such environments,\n   the IPv4 and IPv6 issues enumerated\
    \ above compound each other.\n"
- title: 7.3.  MAC Address Table Size Limitations in Switches
  contents:
  - "7.3.  MAC Address Table Size Limitations in Switches\n   L2 switches maintain\
    \ L2 MAC address forwarding tables for all sources\n   and destinations traversing\
    \ the switch.  These tables are populated\n   through learning and are used to\
    \ forward L2 frames to their correct\n   destination.  The larger the L2 domain,\
    \ the larger the tables have to\n   be.  While in theory a switch only needs to\
    \ keep track of addresses\n   it is actively using (sometimes called \"conversational\
    \ learning\"),\n   switches flood broadcast frames (e.g., from ARP), multicast\
    \ frames\n   (e.g., from Neighbor Discovery), and unicast frames to unknown\n\
    \   destinations.  Switches add entries for the source addresses of such\n   flooded\
    \ frames to their forwarding tables.  Consequently, MAC address\n   table size\
    \ can become a problem as the size of the L2 domain\n   increases.  The table\
    \ size problem is made worse with VMs, where a\n   single physical machine now\
    \ hosts many VMs (in the 10's today, but\n   growing rapidly as the number of\
    \ cores per CPU increases), since each\n   VM has its own MAC address that is\
    \ visible to switches.\n   When L3 extends all the way to access switches (see\
    \ Section 6.4.1),\n   the size of MAC address tables in switches is not generally\
    \ a\n   problem.  When L3 extends only to aggregation switches (see\n   Section\
    \ 6.4.2), however, MAC table size limitations can be a real\n   issue.\n"
- title: 8.  Summary
  contents:
  - "8.  Summary\n   This document has outlined a number of issues related to address\n\
    \   resolution in large data centers.  In particular, this document has\n   described\
    \ different scenarios where such issues might arise and what\n   these potential\
    \ issues are, along with outlining fundamental factors\n   that cause them.  It\
    \ is hoped that describing specific pain points\n   will facilitate a discussion\
    \ as to whether they should be addressed\n   and how best to address them.\n"
- title: 9.  Acknowledgments
  contents:
  - "9.  Acknowledgments\n   This document has been significantly improved by comments\
    \ from Manav\n   Bhatia, David Black, Stewart Bryant, Ralph Droms, Linda Dunbar,\n\
    \   Donald Eastlake, Wesley Eddy, Anoop Ghanwani, Joel Halpern, Sue\n   Hares,\
    \ Pete Resnick, Benson Schliesser, T. Sridhar, and Lucy Yong.\n   Igor Gashinsky\
    \ deserves additional credit for highlighting some of\n   the ARP-related pain\
    \ points and for clarifying the difference between\n   what the standards require\
    \ and what some router vendors have actually\n   implemented in response to operator\
    \ requests.\n"
- title: 10.  Security Considerations
  contents:
  - "10.  Security Considerations\n   This document does not create any security implications\
    \ nor does it\n   have any security implications.  The security vulnerabilities\
    \ in ARP\n   are well known, and this document does not change or mitigate them\
    \ in\n   any way.  Security considerations for Neighbor Discovery are\n   discussed\
    \ in [RFC4861] and [RFC6583].\n"
- title: 11.  Informative References
  contents:
  - "11.  Informative References\n   [RFC0792]  Postel, J., \"Internet Control Message\
    \ Protocol\", STD 5,\n              RFC 792, September 1981.\n   [RFC0826]  Plummer,\
    \ D., \"Ethernet Address Resolution Protocol: Or\n              converting network\
    \ protocol addresses to 48.bit Ethernet\n              address for transmission\
    \ on Ethernet hardware\", STD 37,\n              RFC 826, November 1982.\n   [RFC1122]\
    \  Braden, R., \"Requirements for Internet Hosts -\n              Communication\
    \ Layers\", STD 3, RFC 1122, October 1989.\n   [RFC1812]  Baker, F., \"Requirements\
    \ for IP Version 4 Routers\",\n              RFC 1812, June 1995.\n   [RFC4861]\
    \  Narten, T., Nordmark, E., Simpson, W., and H. Soliman,\n              \"Neighbor\
    \ Discovery for IP version 6 (IPv6)\", RFC 4861,\n              September 2007.\n\
    \   [RFC5227]  Cheshire, S., \"IPv4 Address Conflict Detection\", RFC 5227,\n\
    \              July 2008.\n   [RFC6583]  Gashinsky, I., Jaeggli, J., and W. Kumari,\
    \ \"Operational\n              Neighbor Discovery Problems\", RFC 6583, March\
    \ 2012.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Thomas Narten\n   IBM Corporation\n   3039 Cornwallis\
    \ Ave.\n   PO Box 12195\n   Research Triangle Park, NC  27709-2195\n   USA\n \
    \  EMail: narten@us.ibm.com\n   Manish Karir\n   Merit Network Inc.\n   EMail:\
    \ mkarir@merit.edu\n   Ian Foo\n   Huawei Technologies\n   EMail: Ian.Foo@huawei.com\n"
