- title: __initial_text__
  contents:
  - "                    Ford Aerospace and Communications Corporation\n         \
    \  Congestion Control in IP/TCP Internetworks\nThis memo discusses some aspects\
    \ of congestion control in  IP/TCP\nInternetworks.   It  is intended to stimulate\
    \ thought and further\ndiscussion of this topic.   While some specific  suggestions\
    \  are\nmade for improved congestion  control  implementation,  this memo\ndoes\
    \ not specify any standards.\n                          Introduction\nCongestion\
    \ control is a recognized problem in  complex  networks.\nWe have discovered that\
    \ the Department of Defense's Internet Pro-\ntocol (IP) , a pure datagram protocol,\
    \ and  Transmission  Control\nProtocol  (TCP),  a transport layer protocol, when\
    \ used together,\nare subject to unusual congestion problems caused by interactions\n\
    between  the  transport  and  datagram layers.  In particular, IP\ngateways are\
    \ vulnerable to a phenomenon we call  \"congestion col-\nlapse\",  especially\
    \ when such gateways connect networks of widely\ndifferent bandwidth.  We have\
    \ developed  solutions  that  prevent\ncongestion collapse.\nThese problems are\
    \ not generally recognized because these  proto-\ncols  are used most often on\
    \ networks built on top of ARPANET IMP\ntechnology.  ARPANET IMP based networks\
    \ traditionally  have  uni-\nform  bandwidth and identical switching nodes, and\
    \ are sized with\nsubstantial excess capacity.  This excess capacity, and the\
    \ abil-\nity  of the IMP system to throttle the transmissions of hosts has\nfor\
    \ most IP / TCP hosts and  networks  been  adequate  to  handle\ncongestion. \
    \ With the recent split of the ARPANET into two inter-\nconnected networks and\
    \ the growth of other networks with  differ-\ning properties connected to the\
    \ ARPANET, however, reliance on the\nbenign properties of the IMP system is no\
    \ longer enough to  allow\nhosts  to  communicate rapidly and reliably. Improved\
    \ handling of\ncongestion is now  mandatory  for  successful  network  operation\n\
    under load.\nFord Aerospace and Communications  Corporation,  and  its  parent\n\
    company,  Ford  Motor  Company,  operate  the only private IP/TCP\nlong-haul network\
    \ in existence today.  This network connects four\nfacilities  (one  in Michigan,\
    \ two in California, and one in Eng-\nland) some with extensive local networks.\
    \  This net is cross-tied\nto  the  ARPANET  but  uses  its  own long-haul circuits;\
    \ traffic\nbetween Ford  facilities  flows  over  private  leased  circuits,\n\
    including  a  leased  transatlantic  satellite  connection.   All\nswitching nodes\
    \ are pure IP datagram switches  with  no  node-to-\nnode  flow  control, and\
    \ all hosts run software either written or\nheavily modified by Ford or Ford Aerospace.\
    \  Bandwidth  of  links\nin  this  network varies widely, from 1200 to 10,000,000\
    \ bits per\nsecond.  In general, we have not been able to afford  the  luxury\n\
    Because of our pure datagram orientation, heavy loading, and wide\nvariation \
    \ in  bandwidth,  we have had to solve problems that the\nARPANET / MILNET community\
    \ is just beginning to  recognize.   Our\nnetwork is sensitive to suboptimal behavior\
    \ by host TCP implemen-\ntations, both on and off our own net.  We have devoted\
    \  consider-\nable  effort  to examining TCP behavior under various conditions,\n\
    and have solved some widely  prevalent  problems  with  TCP.   We\npresent  here\
    \  two problems and their solutions.  Many TCP imple-\nmentations have these problems;\
    \ if throughput is worse through an\nARPANET  /  MILNET  gateway  for  a given\
    \ TCP implementation than\nthroughput across a single net, there is a high probability\
    \  that\nthe TCP implementation has one or both of these problems.\n         \
    \              Congestion collapse\nBefore we proceed with a discussion of the\
    \ two specific  problems\nand  their  solutions,  a  description of what happens\
    \ when these\nproblems are not addressed is in order.  In heavily  loaded  pure\n\
    datagram  networks  with  end to end retransmission, as switching\nnodes become\
    \ congested, the  round  trip  time  through  the  net\nincreases  and  the  count\
    \ of datagrams in transit within the net\nalso increases.  This is normal behavior\
    \ under load.  As long  as\nthere is only one copy of each datagram in transit,\
    \ congestion is\nunder  control.   Once  retransmission  of  datagrams   not \
    \  yet\ndelivered begins, there is potential for serious trouble.\nHost TCP  implementations\
    \  are  expected  to  retransmit  packets\nseveral times at increasing time intervals\
    \ until some upper limit\non the retransmit interval is reached.  Normally, this\
    \  mechanism\nis  enough to prevent serious congestion problems.  Even with the\n\
    better adaptive host retransmission algorithms, though, a  sudden\nload on the\
    \ net can cause the round-trip time to rise faster than\nthe sending hosts measurements\
    \ of round-trip time can be updated.\nSuch  a  load  occurs  when  a  new  bulk\
    \  transfer,  such a file\ntransfer, begins and starts filling a large window.\
    \   Should  the\nround-trip  time  exceed  the maximum retransmission interval\
    \ for\nany host, that host will begin to introduce more and more  copies\nof \
    \ the same datagrams into the net.  The network is now in seri-\nous trouble.\
    \  Eventually all available buffers in  the  switching\nnodes  will  be full and\
    \ packets must be dropped.  The round-trip\ntime for packets that are delivered\
    \ is now at its maximum.  Hosts\nare  sending  each packet several times, and\
    \ eventually some copy\nof each packet arrives at its destination.   This  is\
    \  congestion\ncollapse.\nThis condition is stable.  Once the  saturation  point\
    \  has  been\nreached,  if the algorithm for selecting packets to be dropped is\n\
    fair, the network will continue to operate in a  degraded  condi-\ntion.   In\
    \  this  condition  every  packet  is  being transmitted\nseveral times and throughput\
    \ is reduced to a  small  fraction  of\nthe hosts involved time out.\nCongestion\
    \ collapse and pathological congestion are not  normally\nseen  in  the ARPANET\
    \ / MILNET system because these networks have\nsubstantial excess  capacity. \
    \  Where  connections  do  not  pass\nthrough IP gateways, the IMP-to host flow\
    \ control mechanisms usu-\nally prevent congestion collapse, especially since\
    \ TCP  implemen-\ntations  tend  to be well adjusted for the time constants associ-\n\
    ated with the pure ARPANET case.  However, other than ICMP Source\nQuench  messages,\
    \  nothing fundamentally prevents congestion col-\nlapse when TCP is run over\
    \ the ARPANET / MILNET and  packets  are\nbeing  dropped  at  gateways.  Worth\
    \  noting is that a few badly-\nbehaved hosts can by themselves congest the gateways\
    \ and  prevent\nother  hosts from passing traffic.  We have observed this problem\n\
    repeatedly with certain hosts (with whose administrators we  have\ncommunicated\
    \ privately) on the ARPANET.\nAdding additional memory to the gateways will not\
    \ solve the prob-\nlem.   The  more  memory  added, the longer round-trip times\
    \ must\nbecome before packets are dropped.  Thus, the onset of congestion\ncollapse\
    \  will be delayed but when collapse occurs an even larger\nfraction of the  packets\
    \  in  the  net  will  be  duplicates  and\nthroughput will be even worse.\n \
    \                       The two problems\nTwo key problems with the engineering\
    \ of TCP implementations have\nbeen  observed;  we  call  these the small-packet\
    \ problem and the\nsource-quench problem.  The second is being addressed by  several\n\
    implementors; the first is generally believed (incorrectly) to be\nsolved.  We\
    \ have discovered that once  the  small-packet  problem\nhas  been  solved,  the\
    \  source-quench  problem becomes much more\ntractable.  We thus present  the\
    \  small-packet  problem  and  our\nsolution to it first.\n                  \
    \  The small-packet problem\nThere is a special problem associated with small\
    \  packets.   When\nTCP  is  used  for  the transmission of single-character messages\n\
    originating at a keyboard, the typical result  is  that  41  byte\npackets  (one\
    \  byte  of data, 40 bytes of header) are transmitted\nfor each byte of useful\
    \ data.  This 4000%  overhead  is  annoying\nbut tolerable on lightly loaded networks.\
    \  On heavily loaded net-\nworks, however, the congestion resulting from this\
    \  overhead  can\nresult  in  lost datagrams and retransmissions, as well as exces-\n\
    sive propagation time caused by congestion in switching nodes and\ngateways. \
    \  In practice, throughput may drop so low that TCP con-\nnections are aborted.\n\
    This classic problem is well-known and was first addressed in the\nuntil a short\
    \ (200-500ms) time had elapsed, in hope that  another\ncharacter  or two would\
    \ become available for addition to the same\npacket before the  timer  ran  out.\
    \   An  additional  feature  to\nenhance  user  acceptability was to inhibit the\
    \ time delay when a\ncontrol character, such as a carriage return, was received.\n\
    This technique has been used in NCP Telnet, X.25  PADs,  and  TCP\nTelnet. It\
    \ has the advantage of being well-understood, and is not\ntoo difficult to implement.\
    \  Its flaw is that it is hard to  come\nup  with  a  time limit that will satisfy\
    \ everyone.  A time limit\nshort enough to provide highly responsive service over\
    \ a 10M bits\nper  second Ethernet will be too short to prevent congestion col-\n\
    lapse over a heavily loaded net with  a  five  second  round-trip\ntime;  and\
    \  conversely,  a  time  limit long enough to handle the\nheavily loaded net will\
    \ produce frustrated users on the Ethernet.\n            The solution to the small-packet\
    \ problem\nClearly an adaptive approach is desirable.  One  would  expect  a\n\
    proposal  for  an  adaptive  inter-packet time limit based on the\nround-trip\
    \ delay observed by TCP.  While such a  mechanism  could\ncertainly  be  implemented,\
    \  it  is  unnecessary.   A  simple and\nelegant solution has been discovered.\n\
    The solution is to inhibit the sending of new TCP  segments  when\nnew  outgoing\
    \  data  arrives  from  the  user  if  any previously\ntransmitted data on the\
    \ connection remains unacknowledged.   This\ninhibition  is  to be unconditional;\
    \ no timers, tests for size of\ndata received, or other conditions are required.\
    \   Implementation\ntypically requires one or two lines inside a TCP program.\n\
    At first glance, this solution seems to imply drastic changes  in\nthe  behavior\
    \ of TCP.  This is not so.  It all works out right in\nthe end.  Let us see why\
    \ this is so.\nWhen a user process writes to a TCP connection, TCP receives some\n\
    data.   It  may  hold  that data for future sending or may send a\npacket immediately.\
    \  If it refrains from  sending  now,  it  will\ntypically send the data later\
    \ when an incoming packet arrives and\nchanges the state of the system.  The state\
    \ changes in one of two\nways;  the incoming packet acknowledges old data the\
    \ distant host\nhas received, or announces the availability of  buffer  space\
    \  in\nthe  distant  host  for  new  data.  (This last is referred to as\n\"updating\
    \ the window\").    Each time data arrives  on  a  connec-\ntion,  TCP must reexamine\
    \ its current state and perhaps send some\npackets out.  Thus, when we omit sending\
    \ data on arrival from the\nuser,  we  are  simply  deferring its transmission\
    \ until the next\nmessage arrives from the distant host.   A  message  must  always\n\
    arrive soon unless the connection was previously idle or communi-\ncations with\
    \ the other end have been lost.  In  the  first  case,\nthe distant host has failed,\
    \ sending more data is futile  anyway.\nNote  that we have done nothing to inhibit\
    \ normal TCP retransmis-\nsion logic, so lost messages are not a problem.\nExamination\
    \ of the behavior of this scheme under  various  condi-\ntions  demonstrates \
    \ that the scheme does work in all cases.  The\nfirst case to examine is the one\
    \ we wanted to solve, that of  the\ncharacter-oriented  Telnet  connection.  \
    \ Let us suppose that the\nuser is sending TCP a new character every  200ms, \
    \ and  that  the\nconnection  is  via  an Ethernet with a round-trip time including\n\
    software processing of 50ms.  Without any  mechanism  to  prevent\nsmall-packet\
    \ congestion, one packet will be sent for each charac-\nter, and response will\
    \ be optimal.  Overhead will be  4000%,  but\nthis  is  acceptable  on  an Ethernet.\
    \  The classic timer scheme,\nwith a limit of 2 packets per second, will  cause\
    \  two  or  three\ncharacters to be sent per packet.  Response will thus be degraded\n\
    even though on a high-bandwidth  Ethernet  this  is  unnecessary.\nOverhead  will\
    \  drop  to  1500%, but on an Ethernet this is a bad\ntradeoff.  With our scheme,\
    \ every character the user  types  will\nfind  TCP with an idle connection, and\
    \ the character will be sent\nat once, just as in the no-control case.  The user\
    \  will  see  no\nvisible  delay.   Thus,  our  scheme  performs as well as the\
    \ no-\ncontrol scheme and provides better responsiveness than the  timer\nscheme.\n\
    The second case to examine is the same Telnet  test  but  over  a\nlong-haul \
    \ link  with  a  5-second  round trip time.  Without any\nmechanism to prevent\
    \  small-packet  congestion,  25  new  packets\nwould be sent in 5 seconds.* Overhead\
    \ here is  4000%.   With  the\nclassic timer scheme, and the same limit of 2 packets\
    \ per second,\nthere would still be 10 packets outstanding and  contributing \
    \ to\ncongestion.  Round-trip time will not be improved by sending many\npackets,\
    \ of course; in general it will be worse since the packets\nwill  contend  for\
    \ line time.  Overhead now drops to 1500%.  With\nour scheme, however, the first\
    \ character from the user would find\nan  idle  TCP connection and would be sent\
    \ immediately.  The next\n24 characters, arriving from the user at 200ms  intervals,\
    \  would\nbe  held  pending  a  message from the distant host.  When an ACK\n\
    arrived for the first packet at the end of 5  seconds,  a  single\npacket  with\
    \  the 24 queued characters would be sent.  Our scheme\nthus results in an overhead\
    \ reduction to 320% with no penalty  in\nresponse  time.   Response time will\
    \ usually be improved with our\nscheme because packet overhead is reduced, here\
    \ by  a  factor  of\n4.7 over the classic timer scheme.  Congestion will be reduced\
    \ by\nthis factor and round-trip delay will decrease sharply.  For this\n________\n\
    \  * This problem is not seen in the pure ARPANET case because the\n    IMPs will\
    \ block the host when the count of packets\n    outstanding becomes excessive,\
    \ but in the case where a pure\n    datagram local net (such as an Ethernet) or\
    \ a pure datagram\ncase, our scheme has a striking  advantage  over  either  of\
    \  the\nother approaches.\nWe use our scheme for all TCP connections, not just\
    \  Telnet  con-\nnections.   Let us see what happens for a file transfer data\
    \ con-\nnection using our technique. The two extreme cases will again  be\nconsidered.\n\
    As before, we first consider the Ethernet case.  The user is  now\nwriting data\
    \ to TCP in 512 byte blocks as fast as TCP will accept\nthem.  The user's first\
    \ write to TCP will start things going; our\nfirst  datagram  will  be  512+40\
    \  bytes  or 552 bytes long.  The\nuser's second write to TCP will not cause a\
    \ send but  will  cause\nthe  block  to  be buffered.  Assume that the user fills\
    \ up TCP's\noutgoing buffer area before the first ACK comes back.  Then  when\n\
    the  ACK  comes in, all queued data up to the window size will be\nsent.  From\
    \ then on, the window will be kept full,  as  each  ACK\ninitiates  a  sending\
    \  cycle  and queued data is sent out.  Thus,\nafter a one round-trip time initial\
    \ period when only one block is\nsent,  our  scheme  settles down into a maximum-throughput\
    \ condi-\ntion.  The delay in startup is only 50ms on the Ethernet, so  the\n\
    startup  transient  is  insignificant.  All three schemes provide\nequivalent\
    \ performance for this case.\nFinally, let us look at a file transfer over the\
    \  5-second  round\ntrip  time connection.  Again, only one packet will be sent\
    \ until\nthe first ACK comes back; the window will then be filled and kept\nfull.\
    \   Since the round-trip time is 5 seconds, only 512 bytes of\ndata are transmitted\
    \ in the first 5 seconds.  Assuming a 2K  win-\ndow,  once  the first ACK comes\
    \ in, 2K of data will be sent and a\nsteady rate of 2K per 5 seconds will  be\
    \  maintained  thereafter.\nOnly  for  this  case is our scheme inferior to the\
    \ timer scheme,\nand the difference is only in the startup transient; steady-state\n\
    throughput  is  identical.  The naive scheme and the timer scheme\nwould both\
    \ take 250 seconds to transmit a 100K  byte  file  under\nthe  above  conditions\
    \  and  our scheme would take 254 seconds, a\ndifference of 1.6%.\nThus, for all\
    \ cases examined, our scheme provides at least 98% of\nthe  performance  of  both\
    \ other schemes, and provides a dramatic\nimprovement in Telnet performance over\
    \ paths with long round trip\ntimes.   We  use  our  scheme  in  the  Ford  Aerospace\
    \  Software\nEngineering Network, and are able to run screen editors over Eth-\n\
    ernet and talk to distant TOPS-20 hosts with improved performance\nin both cases.\n\
    \                  Congestion control with ICMP\nHaving solved the small-packet\
    \ congestion problem and with it the\nproblem  of excessive small-packet congestion\
    \ within our own net-\nunder  the  IP standard was the ICMP Source Quench message.\
    \  With\ncareful handling,  we  find  this  adequate  to  prevent  serious\ncongestion\
    \ problems.  We do find it necessary to be careful about\nthe behavior of our\
    \ hosts and switching  nodes  regarding  Source\nQuench messages.\n          \
    \     When to send an ICMP Source Quench\nThe present ICMP standard* specifies\
    \ that an ICMP  Source  Quench\nmessage  should  be  sent whenever a packet is\
    \ dropped, and addi-\ntionally may be sent when a gateway finds itself  becoming\
    \  short\nof  resources.   There is some ambiguity here but clearly it is a\n\
    violation of the standard to drop a  packet  without  sending  an\nICMP message.\n\
    Our basic assumption is that packets ought not to be dropped dur-\ning  normal\
    \  network  operation.   We  therefore want to throttle\nsenders back before they\
    \ overload switching nodes  and  gateways.\nAll  our  switching  nodes  send ICMP\
    \ Source Quench messages well\nbefore buffer space is exhausted; they do not wait\
    \  until  it  is\nnecessary to drop a message before sending an ICMP Source Quench.\n\
    As demonstrated in our  analysis  of  the  small-packet  problem,\nmerely  providing\
    \  large  amounts of buffering is not a solution.\nIn general, our experience\
    \ is that Source Quench should  be  sent\nwhen  about  half  the  buffering space\
    \ is exhausted; this is not\nbased on extensive experimentation but appears to\
    \ be a reasonable\nengineering  decision.   One  could  argue for an adaptive\
    \ scheme\nthat adjusted the quench generation  threshold  based  on  recent\n\
    experience; we have not found this necessary as yet.\nThere exist other gateway\
    \ implementations  that  generate  Source\nQuenches  only after more than one\
    \ packet has been discarded.  We\nconsider this approach undesirable since any\
    \ system for  control-\nling congestion based on the discarding of packets is\
    \ wasteful of\nbandwidth and may be susceptible  to  congestion  collapse  under\n\
    heavy  load.   Our understanding is that the decision to generate\nSource Quenches\
    \ with great reluctance stems from a fear that ack-\nnowledge  traffic  will \
    \ be quenched and that this will result in\nconnection failure.  As will be shown\
    \ below, appropriate handling\nof  Source  Quench in host implementations eliminates\
    \ this possi-\nbility.\n        What to do when an ICMP Source Quench is received\n\
    We inform TCP or any other  protocol  at  that  layer  when  ICMP\nreceives  a\
    \ Source Quench.  The basic action of our TCP implemen-\ntations is to reduce\
    \ the amount of data  outstanding  on  connec-\ntions to the host mentioned in\
    \ the Source Quench. This control is\n________\n  * ARPANET RFC 792 is the present\
    \ standard.  We are advised by\napplied by causing the sending TCP to behave as\
    \  if  the  distant\nhost's  window  size  has been reduced.  Our first implementation\n\
    was simplistic but effective;  once  a  Source  Quench  has  been\nreceived  our\
    \  TCP behaves as if the window size is zero whenever\nthe window isn't  empty.\
    \   This  behavior  continues  until  some\nnumber  (at  present 10) of ACKs have\
    \ been received, at that time\nTCP returns to normal operation.* David Mills \
    \ of  Linkabit  Cor-\nporation  has  since  implemented  a  similar  but more\
    \ elaborate\nthrottle on the count of outstanding packets in his DCN  systems.\n\
    The  additional  sophistication seems to produce a modest gain in\nthroughput,\
    \ but we have not made formal tests.  Both  implementa-\ntions effectively prevent\
    \ congestion collapse in switching nodes.\nSource Quench thus has the effect of\
    \ limiting the connection to a\nlimited number (perhaps one) of outstanding messages.\
    \  Thus, com-\nmunication can continue but at a reduced rate,  that  is  exactly\n\
    the effect desired.\nThis scheme has the important property that Source Quench\
    \ doesn't\ninhibit  the  sending of acknowledges or retransmissions.  Imple-\n\
    mentations of Source Quench entirely within the IP layer are usu-\nally unsuccessful\
    \ because IP lacks enough information to throttle\na connection properly.  Holding\
    \ back acknowledges tends  to  pro-\nduce  retransmissions and thus unnecessary\
    \ traffic.  Holding back\nretransmissions may cause loss of a connection by  a\
    \  retransmis-\nsion  timeout.   Our  scheme  will  keep  connections alive under\n\
    severe overload but at reduced bandwidth per connection.\nOther protocols at the\
    \ same layer as TCP should also  be  respon-\nsive  to  Source  Quench.  In each\
    \ case we would suggest that new\ntraffic should be throttled but acknowledges\
    \  should  be  treated\nnormally.   The only serious problem comes from the User\
    \ Datagram\nProtocol, not normally a major traffic generator.   We  have  not\n\
    implemented  any  throttling  in  these protocols as yet; all are\npassed Source\
    \ Quench messages by ICMP but ignore them.\n                    Self-defense for\
    \ gateways\nAs we have shown, gateways are vulnerable to  host  mismanagement\n\
    of  congestion.  Host misbehavior by excessive traffic generation\ncan prevent\
    \ not only the host's own traffic from getting through,\nbut  can interfere with\
    \ other unrelated traffic.  The problem can\nbe dealt with at the host level but\
    \ since one malfunctioning host\ncan  interfere  with others, future gateways\
    \ should be capable of\ndefending themselves against such behavior by obnoxious\
    \ or  mali-\ncious hosts.  We offer some basic self-defense techniques.\nOn one\
    \ occasion in late 1983, a TCP bug in an ARPANET host caused\nthe  host  to  frantically\
    \  generate  retransmissions of the same\ndatagram as fast as the ARPANET would\
    \ accept them.   The  gateway\nthat connected our net with the ARPANET was saturated\
    \ and  little\nuseful  traffic  could  get  through,  since the gateway had more\n\
    bandwidth to the ARPANET than to our  net.   The  gateway  busily\nsent  ICMP\
    \  Source  Quench  messages  but the malfunctioning host\nignored them.  This\
    \ continued for several hours, until  the  mal-\nfunctioning  host  crashed. \
    \  During this period, our network was\neffectively disconnected from the ARPANET.\n\
    When a gateway is forced to  discard  a  packet,  the  packet  is\nselected  at\
    \  the  discretion of the gateway.  Classic techniques\nfor making  this  decision\
    \  are  to  discard  the  most  recently\nreceived packet, or the packet at the\
    \ end of the longest outgoing\nqueue.  We suggest that a worthwhile practical\
    \ measure is to dis-\ncard  the  latest  packet  from the host that originated\
    \ the most\npackets currently queued within the gateway.  This strategy  will\n\
    tend  to  balance throughput amongst the hosts using the gateway.\nWe have not\
    \ yet tried this strategy, but it  seems  a  reasonable\nstarting point for gateway\
    \ self-protection.\nAnother strategy is to discard a  newly  arrived  packet \
    \ if  the\npacket  duplicates  a  packet already in the queue.  The computa-\n\
    tional load for this check is not a problem if hashing techniques\nare  used.\
    \   This  check will not protect against malicious hosts\nbut will provide some\
    \ protection against TCP implementations with\npoor  retransmission  control.\
    \   Gateways between fast local net-\nworks and slower long-haul networks may\
    \ find this check  valuable\nif the local hosts are tuned to work well with the\
    \ local network.\nIdeally  the  gateway  should  detect  malfunctioning  hosts\
    \  and\nsquelch them; such detection is difficult in a pure datagram sys-\ntem.\
    \  Failure to  respond  to  an  ICMP  Source  Quench  message,\nthough,  should\
    \ be regarded as grounds for action by a gateway to\ndisconnect a host.  Detecting\
    \ such failure is non-trivial but  is\na worthwhile area for further research.\n\
    \                           Conclusion\nThe congestion control problems  associated\
    \  with  pure  datagram\nnetworks  are  difficult, but effective solutions exist.\
    \  If IP /\n"
