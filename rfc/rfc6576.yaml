- title: __initial_text__
  contents:
  - '       IP Performance Metrics (IPPM) Standard Advancement Testing

    '
- title: Abstract
  contents:
  - "Abstract\n   This document specifies tests to determine if multiple independent\n\
    \   instantiations of a performance-metric RFC have implemented the\n   specifications\
    \ in the same way.  This is the performance-metric\n   equivalent of interoperability,\
    \ required to advance RFCs along the\n   Standards Track.  Results from different\
    \ implementations of metric\n   RFCs will be collected under the same underlying\
    \ network conditions\n   and compared using statistical methods.  The goal is\
    \ an evaluation of\n   the metric RFC itself to determine whether its definitions\
    \ are clear\n   and unambiguous to implementors and therefore a candidate for\n\
    \   advancement on the IETF Standards Track.  This document is an\n   Internet\
    \ Best Current Practice.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo documents an Internet Best Current Practice.\n\
    \   This document is a product of the Internet Engineering Task Force\n   (IETF).\
    \  It represents the consensus of the IETF community.  It has\n   received public\
    \ review and has been approved for publication by the\n   Internet Engineering\
    \ Steering Group (IESG).  Further information on\n   BCPs is available in Section\
    \ 2 of RFC 5741.\n   Information about the current status of this document, any\
    \ errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc6576.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2012 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Requirements Language ......................................5\n  \
    \ 2. Basic Idea ......................................................5\n   3.\
    \ Verification of Conformance to a Metric Specification ...........7\n      3.1.\
    \ Tests of an Individual Implementation against a Metric\n           Specification\
    \ ..............................................8\n      3.2. Test Setup Resulting\
    \ in Identical Live Network\n           Testing Conditions .........................................9\n\
    \      3.3. Tests of Two or More Different Implementations\n           against\
    \ a Metric Specification ............................15\n      3.4. Clock Synchronization\
    \ .....................................16\n      3.5. Recommended Metric Verification\
    \ Measurement Process .......17\n      3.6. Proposal to Determine an Equivalence\
    \ Threshold for\n           Each Metric Evaluated .....................................20\n\
    \   4. Acknowledgements ...............................................21\n  \
    \ 5. Contributors ...................................................21\n   6.\
    \ Security Considerations ........................................21\n   7. References\
    \ .....................................................21\n      7.1. Normative\
    \ References ......................................21\n      7.2. Informative\
    \ References ....................................23\n   Appendix A.  An Example\
    \ on a One-Way Delay Metric Validation ......24\n     A.1.  Compliance to Metric\
    \ Specification Requirements ...........24\n     A.2.  Examples Related to Statistical\
    \ Tests for One-Way Delay ...25\n   Appendix B.  Anderson-Darling K-sample Reference\
    \ and 2 Sample\n                C++ Code .............................................27\n\
    \   Appendix C.  Glossary .............................................36\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The Internet Standards Process as updated by RFC 6410 [RFC6410]\n\
    \   specifies that widespread deployment and use is sufficient to show\n   interoperability\
    \ as a condition for advancement to Internet Standard.\n   The previous requirement\
    \ of interoperability tests prior to advancing\n   an RFC to the Standard maturity\
    \ level specified in RFC 2026 [RFC2026]\n   and RFC 5657 [RFC5657] has been removed.\
    \  While the modified\n   requirement is applicable to protocols, wide deployment\
    \ of different\n   measurement systems does not prove that the implementations\
    \ measure\n   metrics in a standard way.  Section 5.3 of RFC 5657 [RFC5657]\n\
    \   explicitly mentions the special case of Standards that are not \"on-\n   the-wire\"\
    \ protocols.  While this special case is not explicitly\n   mentioned by RFC 6410\
    \ [RFC6410], the four criteria in Section 2.2 of\n   RFC 6410 [RFC6410] are augmented\
    \ by this document for RFCs that\n   specify performance metrics.  This document\
    \ takes the position that\n   flexible metric definitions can be proven to be\
    \ clear and unambiguous\n   through tests that compare the results from independent\n\
    \   implementations.  It describes tests that infer whether metric\n   specifications\
    \ are sufficient using a definition of metric\n   \"interoperability\": measuring\
    \ equivalent results (in a statistical\n   sense) under the same network conditions.\
    \  The document expands on\n   this problem and its solution.\n   In the case\
    \ of a protocol specification, the notion of\n   \"interoperability\" is reasonably\
    \ intuitive -- the implementations\n   must successfully \"talk to each other\"\
    , while exercising all features\n   and options.  To achieve interoperability,\
    \ two implementors need to\n   interpret the protocol specifications in equivalent\
    \ ways.  In the\n   case of IP Performance Metrics (IPPM), this definition of\n\
    \   interoperability is only useful for test and control protocols like\n   the\
    \ One-Way Active Measurement Protocol (OWAMP) [RFC4656] and the\n   Two-Way Active\
    \ Measurement Protocol (TWAMP) [RFC5357].\n   A metric specification RFC describes\
    \ one or more metric definitions,\n   methods of measurement, and a way to report\
    \ the results of\n   measurement.  One example would be a way to test and report\
    \ the one-\n   way delay that data packets incur while being sent from one network\n\
    \   location to another, using the One-Way Delay Metric.\n   In the case of metric\
    \ specifications, the conditions that satisfy the\n   \"interoperability\" requirement\
    \ are less obvious, and there is a need\n   for IETF agreement on practices to\
    \ judge metric specification\n   \"interoperability\" in the context of the IETF\
    \ Standards Process.\n   This memo provides methods that should be suitable to\
    \ evaluate metric\n   specifications for Standards Track advancement.  The methods\
    \ proposed\n   here MAY be generally applicable to metric specification RFCs beyond\n\
    \   those developed under the IPPM Framework [RFC2330].\n   Since many implementations\
    \ of IP metrics are embedded in measurement\n   systems that do not interact with\
    \ one another (they were built before\n   OWAMP and TWAMP), the interoperability\
    \ evaluation called for in the\n   IETF Standards Process cannot be determined\
    \ by observing that\n   independent implementations interact properly for various\
    \ protocol\n   exchanges.  Instead, verifying that different implementations give\n\
    \   statistically equivalent results under controlled measurement\n   conditions\
    \ takes the place of interoperability observations.  Even\n   when evaluating\
    \ OWAMP and TWAMP RFCs for Standards Track advancement,\n   the methods described\
    \ here are useful to evaluate the measurement\n   results because their validity\
    \ would not be ascertained in protocol\n   interoperability testing.\n   The Standards\
    \ advancement process aims at producing confidence that\n   the metric definitions\
    \ and supporting material are clearly worded and\n   unambiguous, or reveals ways\
    \ in which the metric definitions can be\n   revised to achieve clarity.  The\
    \ process also permits identification\n   of options that were not implemented,\
    \ so that they can be removed\n   from the advancing specification.  Thus, the\
    \ product of this process\n   is information about the metric specification RFC\
    \ itself:\n   determination of the specifications or definitions that are clear\
    \ and\n   unambiguous and those that are not (as opposed to an evaluation of\n\
    \   the implementations that assist in the process).\n   This document defines\
    \ a process to verify that implementations (or\n   practically, measurement systems)\
    \ have interpreted the metric\n   specifications in equivalent ways and produce\
    \ equivalent results.\n   Testing for statistical equivalence requires ensuring\
    \ identical test\n   setups (or awareness of differences) to the best possible\
    \ extent.\n   Thus, producing identical test conditions is a core goal of this\n\
    \   memo.  Another important aspect of this process is to test individual\n  \
    \ implementations against specific requirements in the metric\n   specifications\
    \ using customized tests for each requirement.  These\n   tests can distinguish\
    \ equivalent interpretations of each specific\n   requirement.\n   Conclusions\
    \ on equivalence are reached by two measures.\n   First, implementations are compared\
    \ against individual metric\n   specifications to make sure that differences in\
    \ implementation are\n   minimized or at least known.\n   Second, a test setup\
    \ is proposed ensuring identical networking\n   conditions so that unknowns are\
    \ minimized and comparisons are\n   simplified.  The resulting separate data sets\
    \ may be seen as samples\n   taken from the same underlying distribution.  Using\
    \ statistical\n   methods, the equivalence of the results is verified.  To illustrate\n\
    \   application of the process and methods defined here, evaluation of\n   the\
    \ One-Way Delay Metric [RFC2679] is provided in Appendix A.  While\n   test setups\
    \ will vary with the metrics to be validated, the general\n   methodology of determining\
    \ equivalent results will not.  Documents\n   defining test setups to evaluate\
    \ other metrics should be developed\n   once the process proposed here has been\
    \ agreed and approved.\n   The metric RFC advancement process begins with a request\
    \ for protocol\n   action accompanied by a memo that documents the supporting\
    \ tests and\n   results.  The procedures of [RFC2026] are expanded in [RFC5657],\n\
    \   including sample implementation and interoperability reports.\n   [TESTPLAN]\
    \ can serve as a template for a metric RFC report that\n   accompanies the protocol\
    \ action request to the Area Director,\n   including a description of the test\
    \ setup, procedures, results for\n   each implementation, and conclusions.\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in RFC 2119 [RFC2119].\n"
- title: 2.  Basic Idea
  contents:
  - "2.  Basic Idea\n   The implementation of a standard compliant metric is expected\
    \ to meet\n   the requirements of the related metric specification.  So, before\n\
    \   comparing two metric implementations, each metric implementation is\n   individually\
    \ compared against the metric specification.\n   Most metric specifications leave\
    \ freedom to implementors on non-\n   fundamental aspects of an individual metric\
    \ (or options).  Comparing\n   different measurement results using a statistical\
    \ test with the\n   assumption of identical test path and testing conditions requires\n\
    \   knowledge of all differences in the overall test setup.  Metric\n   specification\
    \ options chosen by implementors have to be documented.\n   It is RECOMMENDED\
    \ to use identical metric options for any test\n   proposed here (an exception\
    \ would be if a variable parameter of the\n   metric definition is not configurable\
    \ in one or more\n   implementations).  Calibrations specified by metric standards\
    \ SHOULD\n   be performed to further identify (and possibly reduce) potential\n\
    \   sources of error in the test setup.\n   The IPPM Framework [RFC2330] expects\
    \ that a \"methodology for a metric\n   should have the property that it is repeatable:\
    \ if the methodology is\n   used multiple times under identical conditions, it\
    \ should result in\n   consistent measurements\".  This means an implementation\
    \ is expected\n   to repeatedly measure a metric with consistent results (repeatability\n\
    \   with the same result).  Small deviations in the test setup are\n   expected\
    \ to lead to small deviations in results only.  To\n   characterize statistical\
    \ equivalence in the case of small deviations,\n   [RFC2330] and [RFC2679] suggest\
    \ to apply a 95% confidence interval.\n   Quoting RFC 2679, \"95 percent was chosen\
    \ because ... a particular\n   confidence level should be specified so that the\
    \ results of\n   independent implementations can be compared\".\n   Two different\
    \ implementations are expected to produce statistically\n   equivalent results\
    \ if they both measure a metric under the same\n   networking conditions.  Formulating\
    \ in statistical terms: separate\n   metric implementations collect separate samples\
    \ from the same\n   underlying statistical process (the same network conditions).\
    \  The\n   statistical hypothesis to be tested is the expectation that both\n\
    \   samples do not expose statistically different properties.  This\n   requires\
    \ careful test design:\n   o  The measurement test setup must be self-consistent\
    \ to the largest\n      possible extent.  To minimize the influence of the test\
    \ and\n      measurement setup on the result, network conditions and paths MUST\n\
    \      be identical for the compared implementations to the largest\n      possible\
    \ degree.  This includes both the stability and non-\n      ambiguity of routes\
    \ taken by the measurement packets.  See\n      [RFC2330] for a discussion on\
    \ self-consistency.\n   o  To minimize the influence of implementation options\
    \ on the result,\n      metric implementations SHOULD use identical options and\
    \ parameters\n      for the metric under evaluation.\n   o  The sample size must\
    \ be large enough to minimize its influence on\n      the consistency of the test\
    \ results.  This consideration may be\n      especially important if two implementations\
    \ measure with different\n      average packet transmission rates.\n   o  The\
    \ implementation with the lowest average packet transmission\n      rate determines\
    \ the smallest temporal interval for which samples\n      can be compared.\n \
    \  o  Repeat comparisons with several independent metric samples to\n      avoid\
    \ random indications of compatibility (or the lack of it).\n   The metric specifications\
    \ themselves are the primary focus of\n   evaluation, rather than the implementations\
    \ of metrics.  The\n   documentation produced by the advancement process should\
    \ identify\n   which metric definitions and supporting material were found to\
    \ be\n   clearly worded and unambiguous, OR it should identify ways in which\n\
    \   the metric specification text should be revised to achieve clarity\n   and\
    \ unified interpretation.\n   The process should also permit identification of\
    \ options that were\n   not implemented, so that they can be removed from the\
    \ advancing\n   specification (this is an aspect more typical of protocol advancement\n\
    \   along the Standards Track).\n   Note that this document does not propose to\
    \ base interoperability\n   indications of performance-metric implementations\
    \ on comparisons of\n   individual singletons.  Individual singletons may be impacted\
    \ by many\n   statistical effects while they are measured.  Comparing two\n  \
    \ singletons of different implementations may result in failures with\n   higher\
    \ probability than comparing samples.\n"
- title: 3.  Verification of Conformance to a Metric Specification
  contents:
  - "3.  Verification of Conformance to a Metric Specification\n   This section specifies\
    \ how to verify compliance of two or more IPPM\n   implementations against a metric\
    \ specification.  This document only\n   proposes a general methodology.  Compliance\
    \ criteria to a specific\n   metric implementation need to be defined for each\
    \ individual metric\n   specification.  The only exception is the statistical\
    \ test comparing\n   two metric implementations that are simultaneously tested.\
    \  This test\n   is applicable without metric-specific decision criteria.\n  \
    \ Several testing options exist to compare two or more implementations:\n   o\
    \  Use a single test lab to compare the implementations and emulate\n      the\
    \ Internet with an impairment generator.\n   o  Use a single test lab to compare\
    \ the implementations and measure\n      across the Internet.\n   o  Use remotely\
    \ separated test labs to compare the implementations\n      and emulate the Internet\
    \ with two \"identically\" configured\n      impairment generators.\n   o  Use\
    \ remotely separated test labs to compare the implementations\n      and measure\
    \ across the Internet.\n   o  Use remotely separated test labs to compare the\
    \ implementations,\n      measure across the Internet, and include a single impairment\n\
    \      generator to impact all measurement flows in a non-discriminatory\n   \
    \   way.\n   The first two approaches work, but involve higher expenses than the\n\
    \   others (due to travel and/or shipping plus installation).  For the\n   third\
    \ option, ensuring two identically configured impairment\n   generators requires\
    \ well-defined test cases and possibly identical\n   hardware and software.\n\
    \   As documented in a test report [TESTPLAN], the last option was\n   required\
    \ to prove compatibility of two delay metric implementations.\n   An impairment\
    \ generator is probably required when testing\n   compatibility of most other\
    \ metrics, and it is therefore RECOMMENDED\n   to include an impairment generator\
    \ in metric test setups.\n"
- title: 3.1.  Tests of an Individual Implementation against a Metric
  contents:
  - "3.1.  Tests of an Individual Implementation against a Metric\n      Specification\n\
    \   A metric implementation is compliant with a metric specification if\n   it\
    \ supports the requirements classified as \"MUST\" and \"REQUIRED\" in\n   the\
    \ related metric specification.  An implementation that implements\n   all requirements\
    \ is fully compliant with the specification, and the\n   degree of compliance\
    \ SHOULD be noted in the conclusions of the\n   report.\n   Further, supported\
    \ options of a metric implementation SHOULD be\n   documented in sufficient detail\
    \ to evaluate whether the specification\n   was correctly interpreted.  The documentation\
    \ of chosen options\n   should minimize (and recognize) differences in the test\
    \ setup if two\n   metric implementations are compared.  Further, this documentation\
    \ is\n   used to validate or clarify the wording of the metric specification\n\
    \   option, to remove options that saw no implementation or that are\n   badly\
    \ specified from the metric specification.  This documentation\n   SHOULD be included\
    \ for all implementation-relevant specifications of\n   a metric picked for a\
    \ comparison, even those that are not explicitly\n   marked as \"MUST\" or \"\
    REQUIRED\" in the RFC text.  This applies for the\n   following sections of all\
    \ metric specifications:\n   o  Singleton Definition of the Metric.\n   o  Sample\
    \ Definition of the Metric.\n   o  Statistics Definition of the Metric.  As statistics\
    \ are compared\n      by the test specified here, this documentation is required\
    \ even in\n      the case that the metric specification does not contain a\n \
    \     Statistics Definition.\n   o  Timing- and Synchronization-related specification\
    \ (if relevant for\n      the Metric).\n   o  Any other technical part present\
    \ or missing in the metric\n      specification, which is relevant for the implementation\
    \ of the\n      Metric.\n   [RFC2330] and [RFC2679] emphasize precision as an\
    \ aim of IPPM metric\n   implementations.  A single IPPM-conforming implementation\
    \ should\n   under otherwise identical network conditions produce precise results\n\
    \   for repeated measurements of the same metric.\n   RFC 2330 prefers the \"\
    empirical distribution function\" (EDF) to\n   describe collections of measurements.\
    \  RFC 2330 determines, that\n   \"unless otherwise stated, IPPM goodness-of-fit\
    \ tests are done using\n   5% significance\".  The goodness-of-fit test determines\
    \ by which\n   precision two or more samples of a metric implementation belong\
    \ to\n   the same underlying distribution (of measured network performance\n \
    \  events).  The goodness-of-fit test suggested for the metric test is\n   the\
    \ Anderson-Darling K sample test (ADK sample test, K stands for the\n   number\
    \ of samples to be compared) [ADK].  Please note that RFC 2330\n   and RFC 2679\
    \ apply an Anderson-Darling goodness-of-fit test, too.\n   The results of a repeated\
    \ test with a single implementation MUST pass\n   an ADK sample test with a confidence\
    \ level of 95%.  The conditions\n   for which the ADK test has been passed with\
    \ the specified confidence\n   level MUST be documented.  To formulate this differently,\
    \ the\n   requirement is to document the set of parameters with the smallest\n\
    \   deviation at which the results of the tested metric implementation\n   pass\
    \ an ADK test with a confidence level of 95%.  The minimum\n   resolution available\
    \ in the reported results from each implementation\n   MUST be taken into account\
    \ in the ADK test.\n   The test conditions to be documented for a passed metric\
    \ test\n   include:\n   o  The metric resolution at which a test was passed (e.g.,\
    \ the\n      resolution of timestamps).\n   o  The parameters modified by an impairment\
    \ generator.\n   o  The impairment generator parameter settings.\n"
- title: 3.2.  Test Setup Resulting in Identical Live Network Testing Conditions
  contents:
  - "3.2.  Test Setup Resulting in Identical Live Network Testing Conditions\n   Two\
    \ major issues complicate tests for metric compliance across live\n   networks\
    \ under identical testing conditions.  One is the general\n   point that metric\
    \ definition implementations cannot be conveniently\n   examined in field measurement\
    \ scenarios.  The other one is more\n   broadly described as \"parallelism in\
    \ devices and networks\", including\n   mechanisms like those that achieve load\
    \ balancing (see [RFC4928]).\n   This section proposes two measures to deal with\
    \ both issues.\n   Tunneling mechanisms can be used to avoid parallel processing\
    \ of\n   different flows in the network.  Measuring by separate parallel probe\n\
    \   flows results in repeated collection of data.  If both measures are\n   combined,\
    \ Wide Area Network (WAN) conditions are identical for a\n   number of independent\
    \ measurement flows, no matter what the network\n   conditions are in detail.\n\
    \   Any measurement setup must be made to avoid the probing traffic\n   itself\
    \ to impede the metric measurement.  The created measurement\n   load must not\
    \ result in congestion at the access link connecting the\n   measurement implementation\
    \ to the WAN.  The created measurement load\n   must not overload the measurement\
    \ implementation itself, e.g., by\n   causing a high CPU load or by causing timestamp\
    \ imprecision due to\n   unwanted queuing while transmitting or receiving test\
    \ packets.\n   Tunneling multiple flows destined for a single physical port of\
    \ a\n   network element allows transmission of all packets via the same path.\n\
    \   Applying tunnels to avoid undesired influence of standard routing for\n  \
    \ measurement purposes is a concept known from literature, see e.g.,\n   GRE-encapsulated\
    \ multicast probing [GU-Duffield].  An existing\n   IP-in-IP tunnel protocol can\
    \ be applied to avoid Equal-Cost Multi-\n   Path (ECMP) routing of different measurement\
    \ streams if it meets the\n   following criteria:\n   o  Inner IP packets from\
    \ different measurement implementations are\n      mapped into a single tunnel\
    \ with a single outer IP origin and\n      destination address as well as origin\
    \ and destination port numbers\n      that are identical for all packets.\n  \
    \ o  An easily accessible tunneling protocol allows for carrying out a\n     \
    \ metric test from more test sites.\n   o  A low operational overhead may enable\
    \ a broader audience to set up\n      a metric test with the desired properties.\n\
    \   o  The tunneling protocol should be reliable and stable in setup and\n   \
    \   operation to avoid disturbances or influence on the test results.\n   o  The\
    \ tunneling protocol should not incur any extra cost for those\n      interested\
    \ in setting up a metric test.\n   An illustration of a test setup with two layer\
    \ 2 tunnels and two\n   flows between two linecards of one implementation is given\
    \ in\n   Figure 1.\n           Implementation                   ,---.       +--------+\n\
    \                               +~~~~~~~~~~~/     \\~~~~~~| Remote |\n       \
    \     +------->-----F2->-|          /       \\     |->---+  |\n            | +---------+\
    \      | Tunnel 1(         )    |     |  |\n            | | transmit|-F1->-| \
    \        (         )    |->+  |  |\n            | | LC1     |      +~~~~~~~~~|\
    \         |~~~~|  |  |  |\n            | | receive |-<--+           (        \
    \ )    | F1  F2 |\n            | +---------+    |           |Internet |    | \
    \ |  |  |\n            *-------<-----+  F2          |         |    |  |  |  |\n\
    \              +---------+ |  | +~~~~~~~~~|         |~~~~|  |  |  |\n        \
    \      | transmit|-*  *-|         |         |    |--+<-*  |\n              | LC2\
    \     |      | Tunnel 2(         )    |  |     |\n              | receive |-<-F1-|\
    \          \\       /     |<-*     |\n              +---------+      +~~~~~~~~~~~\\\
    \     /~~~~~~| Router |\n                                            `-+-'   \
    \    +--------+\n     For simplicity, only two linecards of one implementation\
    \ and two\n                      flows F between them are shown.\n      Figure\
    \ 1: Illustration of a Test Setup with Two Layer 2 Tunnels\n   Figure 2 shows\
    \ the network elements required to set up layer 2\n   tunnels as shown by Figure\
    \ 1.\n            Implementation\n            +-----+                   ,---.\n\
    \            | LC1 |                  /     \\\n            +-----+          \
    \       /       \\              +------+\n               |        +-------+  (\
    \         )  +-------+  |Remote|\n            +--------+  |       |  |       \
    \  |  |       |  |      |\n            |Ethernet|  | Tunnel|  |Internet |  | Tunnel|\
    \  |      |\n            |Switch  |--| Head  |--|         |--| Head  |--|    \
    \  |\n            +--------+  | Router|  |         |  | Router|  |      |\n  \
    \             |        |       |  (         )  |       |  |Router|\n         \
    \   +-----+     +-------+   \\       /   +-------+  +------+\n            | LC2\
    \ |                  \\     /\n            +-----+                   `-+-'\n \
    \  Figure 2: Illustration of a Hardware Setup to Realize the Test Setup\n    \
    \    Illustrated by Figure 1 with Layer 2 Tunnels or Pseudowires\n   The test\
    \ setup successfully used during a delay metric test\n   [TESTPLAN] is given as\
    \ an example in Figure 3.  Note that the shown\n   setup allows a metric test\
    \ between two remote sites.\n           +----+  +----+                       \
    \         +----+  +----+\n           |LC10|  |LC11|           ,---.          \
    \      |LC20|  |LC21|\n           +----+  +----+          /     \\    +-------+\
    \  +----+  +----+\n             | V10  | V11         /       \\   | Tunnel|  \
    \ | V20   |  V21\n             |      |            (         )  | Head  |   |\
    \       |\n            +--------+  +------+ |         |  | Router|__+----------+\n\
    \            |Ethernet|  |Tunnel| |Internet |  +---B---+  |Ethernet  |\n     \
    \       |Switch  |--|Head  |-|         |      |      |Switch    |\n          \
    \  +-+--+---+  |Router| |         |  +---+---+  +--+--+----+\n              |__|\
    \      +--A---+ (         )--|Option.|     |__|\n                            \
    \      \\       /   |Impair.|\n            Bridge                 \\     /   \
    \ |Gener. |     Bridge\n            V20 to V21              `-+-?     +-------+\
    \     V10 to V11\n     Figure 3: Example of Test Setup Successfully Used during\
    \ a Delay\n                                Metic Test\n   In Figure 3, LC10 identifies\
    \ measurement clients / linecards.  V10\n   and the others denote VLANs.  All\
    \ VLANs are using the same tunnel\n   from A to B and in the reverse direction.\
    \  The remote site VLANs are\n   U-bridged at the local site Ethernet switch.\
    \  The measurement packets\n   of site 1 travel tunnel A->B first, are U-bridged\
    \ at site 2, and\n   travel tunnel B->A second.  Measurement packets of site 2\
    \ travel\n   tunnel B->A first, are U-bridged at site 1, and travel tunnel A->B\n\
    \   second.  So, all measurement packets pass the same tunnel segments,\n   but\
    \ in different segment order.\n   If tunneling is applied, two tunnels MUST carry\
    \ all test traffic in\n   between the test site and the remote site.  For example,\
    \ if 802.1Q\n   Virtual LANs (VLANs) are applied and the measurement streams are\n\
    \   carried in different VLANs, the IP tunnel or pseudowires respectively\n  \
    \ are setup in physical port mode to avoid setup of pseudowires per\n   VLAN (which\
    \ may see different paths due to ECMP routing); see\n   [RFC4448].  The remote\
    \ router and the Ethernet switch shown in\n   Figure 3 have to support 802.1Q\
    \ in this setup.\n   The IP packet size of the metric implementation SHOULD be\
    \ chosen\n   small enough to avoid fragmentation due to the added Ethernet and\n\
    \   tunnel headers.  Otherwise, the impact of tunnel overhead on\n   fragmentation\
    \ and interface MTU size must be understood and taken\n   into account (see [RFC4459]).\n\
    \   An Ethernet port mode IP tunnel carrying several 802.1Q VLANs each\n   containing\
    \ measurement traffic of a single measurement system was\n   successfully applied\
    \ when testing compatibility of two metric\n   implementations [TESTPLAN].  Ethernet\
    \ over Layer 2 Tunneling Protocol\n   Version 3 (L2TPv3) [RFC4719] was picked\
    \ for this test.\n   The following headers may have to be accounted for when calculating\n\
    \   total packet length, if VLANs and Ethernet over L2TPv3 tunnels are\n   applied:\n\
    \   o  Ethernet 802.1Q: 22 bytes.\n   o  L2TPv3 Header: 4-16 bytes for L2TPv3\
    \ data messages over IP; 16-28\n      bytes for L2TPv3 data messages over UDP.\n\
    \   o  IPv4 Header (outer IP header): 20 bytes.\n   o  MPLS Labels may be added\
    \ by a carrier.  Each MPLS Label has a\n      length of 4 bytes.  At the time\
    \ of this writing, between 1 and 4\n      Labels seems to be a fair guess of what's\
    \ expected.\n   The applicability of one or more of the following tunneling protocols\n\
    \   may be investigated by interested parties if Ethernet over L2TPv3 is\n   felt\
    \ to be unsuitable: IP in IP [RFC2003] or Generic Routing\n   Encapsulation (GRE)\
    \ [RFC2784].  RFC 4928 [RFC4928] proposes measures\n   how to avoid ECMP treatment\
    \ in MPLS networks.\n   L2TP is a commodity tunneling protocol [RFC2661].  At\
    \ the time of\n   this writing, L2TPv3 [RFC3931] is the latest version of L2TP.\
    \  If\n   L2TPv3 is applied, software-based implementations of this protocol\n\
    \   are not suitable for the test setup, as such implementations may\n   cause\
    \ incalculable delay shifts.\n   Ethernet pseudowires may also be set up on MPLS\
    \ networks [RFC4448].\n   While there is no technical issue with this solution,\
    \ MPLS interfaces\n   are mostly found in the network provider domain.  Hence,\
    \ not all of\n   the above criteria for selecting a tunneling protocol are met.\n\
    \   Note that setting up a metric test environment is not a plug-and-play\n  \
    \ issue.  Skilled networking engineers should be consulted and involved\n   if\
    \ a setup between remote sites is preferred.\n   Passing or failing an ADK test\
    \ with 2 samples could be a random\n   result (note that [RFC2330] defines a sample\
    \ as a set of singleton\n   metric values produced by a measurement stream, and\
    \ we continue to\n   use this terminology here).  The error margin of a statistical\
    \ test\n   is higher if the number of samples it is based on is low (the number\n\
    \   of samples taken influences the so-called \"degree of freedom\" of a\n   statistical\
    \ test, and a higher degree of freedom produces more\n   reliable results).  To\
    \ pass an ADK with higher probability, the\n   number of samples collected per\
    \ implementation under identical\n   networking conditions SHOULD be greater than\
    \ 2.  Hardware and load\n   constraints may enforce an upper limit on the number\
    \ of simultaneous\n   measurement streams.  The ADK test allows one to combine\
    \ different\n   samples (see Section 9 of [ADK]) and then to run a 2-sample test\n\
    \   between combined samples.  At least 4 samples per implementation\n   captured\
    \ under identical networking conditions is RECOMMENDED when\n   comparing different\
    \ metric implementations by a statistical test.\n   It is RECOMMENDED that tests\
    \ be carried out by establishing N\n   different parallel measurement flows. \
    \ Two or three linecards per\n   implementation serving to send or receive measurement\
    \ flows should be\n   sufficient to create 4 or more parallel measurement flows.\
    \  Other\n   options are to separate flows by DiffServ marks (without deploying\n\
    \   any Quality of Service (QoS) in the inner or outer tunnel) or to use\n   a\
    \ single Constant Bitrate (CBR) flow and evaluate whether every n-th\n   singleton\
    \ belongs to a specific measurement flow.  Note that a\n   practical test indeed\
    \ showed that ADK passed with 4 samples even if a\n   2-sample test failed [TESTPLAN].\n\
    \   Some additional guidelines to calculate and compare samples to\n   perform\
    \ a metric test are:\n   o  Comparing different probes of a common underlying\
    \ distribution in\n      terms of metrics characterizing a communication network\
    \ requires\n      respecting the temporal nature for which the assumption of a\n\
    \      common underlying distribution may hold.  Any singletons or\n      samples\
    \ to be compared must be captured within the same time\n      interval.\n   o\
    \  If statistical events like rates are used to characterize measured\n      metrics\
    \ of a time interval, a minimum of 5 singletons of a\n      relevant metric should\
    \ be picked to ensure a minimum confidence\n      into the reported value.  The\
    \ error margin of the determined rate\n      depends on the number of singletons\
    \ (refer to statistical\n      textbooks on student's t-test).  As an example,\
    \ any packet loss\n      measurement interval to be compared with the results\
    \ of another\n      implementation contains at least five lost packets to have\
    \ some\n      confidence that the observed loss rate wasn't caused by a small\n\
    \      number of random packet drops.\n   o  The minimum number of singletons\
    \ or samples to be compared by an\n      Anderson-Darling test should be 100 per\
    \ tested metric\n      implementation.  Note that the Anderson-Darling test detects\
    \ small\n      differences in distributions fairly well and will fail for a high\n\
    \      number of compared results (RFC 2330 mentions an example with 8192\n  \
    \    measurements where an Anderson-Darling test always failed).\n   o  Generally,\
    \ the Anderson-Darling test is sensitive to differences\n      in the accuracy\
    \ or bias associated with varying implementations or\n      test conditions. \
    \ These dissimilarities may result in differing\n      averages of samples to\
    \ be compared.  An example may be different\n      packet sizes, resulting in\
    \ a constant delay difference between\n      compared samples.  Therefore, samples\
    \ to be compared by an\n      Anderson-Darling test MAY be calibrated by the difference\
    \ of the\n      average values of the samples.  Any calibration of this kind MUST\n\
    \      be documented in the test result.\n"
- title: 3.3.  Tests of Two or More Different Implementations against a Metric
  contents:
  - "3.3.  Tests of Two or More Different Implementations against a Metric\n     \
    \ Specification\n   [RFC2330] expects that \"a methodology for a given metric\
    \ exhibits\n   continuity if, for small variations in conditions, it results in\n\
    \   small variations in the resulting measurements.  Slightly more\n   precisely,\
    \ for every positive epsilon, there exists a positive delta,\n   such that if\
    \ two sets of conditions are within delta of each other,\n   then the resulting\
    \ measurements will be within epsilon of each\n   other\".  A small variation\
    \ in conditions in the context of the metric\n   test proposed here can be seen\
    \ as different implementations measuring\n   the same metric along the same path.\n\
    \   IPPM metric specifications, however, allow for implementor options to\n  \
    \ the largest possible degree.  It cannot be expected that two\n   implementors\
    \ allow 100% identical options in their implementations.\n   Testers SHOULD pick\
    \ the same metric measurement configurations for\n   their systems when comparing\
    \ their implementations by a metric test.\n   In some cases, a goodness-of-fit\
    \ test may not be possible or show\n   disappointing results.  To clarify the\
    \ difficulties arising from\n   different metric implementation options, the individual\
    \ options\n   picked for every compared metric implementation should be documented\n\
    \   as specified in Section 3.5.  If the cause of the failure is a lack\n   of\
    \ specification clarity or multiple legitimate interpretations of\n   the definition\
    \ text, the text should be modified and the resulting\n   memo proposed for consensus\
    \ and (possible) advancement to Internet\n   Standard.\n   The same statistical\
    \ test as applicable to quantify precision of a\n   single metric implementation\
    \ must be used to compare metric result\n   equivalence for different implementations.\
    \  To document\n   compatibility, the smallest measurement resolution at which\
    \ the\n   compared implementations passed the ADK sample test must be\n   documented.\n\
    \   For different implementations of the same metric, \"variations in\n   conditions\"\
    \ are reasonably expected.  The ADK test comparing samples\n   of the different\
    \ implementations may result in a lower precision than\n   the test for precision\
    \ in the same-implementation comparison.\n"
- title: 3.4.  Clock Synchronization
  contents:
  - "3.4.  Clock Synchronization\n   Clock synchronization effects require special\
    \ attention.  Accuracy of\n   one-way active delay measurements for any metric\
    \ implementation\n   depends on clock synchronization between the source and destination\n\
    \   of tests.  Ideally, one-way active delay measurement [RFC2679] test\n   endpoints\
    \ either have direct access to independent GPS or CDMA-based\n   time sources\
    \ or indirect access to nearby NTP primary (stratum 1)\n   time sources, equipped\
    \ with GPS receivers.  Access to these time\n   sources may not be available at\
    \ all test locations associated with\n   different Internet paths, for a variety\
    \ of reasons out of scope of\n   this document.\n   When secondary (stratum 2\
    \ and above) time sources are used with NTP\n   running across the same network,\
    \ whose metrics are subject to\n   comparative implementation tests, network impairments\
    \ can affect\n   clock synchronization and distort sample one-way values and their\n\
    \   interval statistics.  Discarding sample one-way delay values for any\n   implementation\
    \ is recommended when one of the following reliability\n   conditions is met:\n\
    \   o  Delay is measured and is finite in one direction but not the\n      other.\n\
    \   o  Absolute value of the difference between the sum of one-way\n      measurements\
    \ in both directions and the round-trip measurement is\n      greater than X%\
    \ of the latter value.\n   Examination of the second condition requires round-trip\
    \ time (RTT)\n   measurement for reference, e.g., based on TWAMP [RFC5357] in\n\
    \   conjunction with one-way delay measurement.\n   Specification of X% to strike\
    \ a balance between identification of\n   unreliable one-way delay samples and\
    \ misidentification of reliable\n   samples under a wide range of Internet path\
    \ RTTs requires further\n   study.\n   An IPPM-compliant metric implementation\
    \ of an RFC that requires\n   synchronized clocks is expected to provide precise\
    \ measurement\n   results.\n   IF an implementation publishes a specification\
    \ of its precision, such\n   as \"a precision of 1 ms (+/- 500 us) with a confidence\
    \ of 95%\", then\n   the specification should be met over a useful measurement\
    \ duration.\n   For example, if the metric is measured along an Internet path\
    \ that is\n   stable and not congested, then the precision specification should\
    \ be\n   met over durations of an hour or more.\n"
- title: 3.5.  Recommended Metric Verification Measurement Process
  contents:
  - "3.5.  Recommended Metric Verification Measurement Process\n   In order to meet\
    \ their obligations under the IETF Standards Process,\n   the IESG must be convinced\
    \ that each metric specification advanced to\n   Internet Standard status is clearly\
    \ written, that there are a\n   sufficient number of verified equivalent implementations,\
    \ and that\n   options that have been implemented are documented.\n   In the context\
    \ of this document, metrics are designed to measure some\n   characteristic of\
    \ a data network.  An aim of any metric definition\n   should be that it is specified\
    \ in a way that can reliably measure the\n   specific characteristic in a repeatable\
    \ way across multiple\n   independent implementations.\n   Each metric, statistic,\
    \ or option of those to be validated MUST be\n   compared against a reference\
    \ measurement or another implementation as\n   specified in this document.\n \
    \  Finally, the metric definitions, embodied in the text of the RFCs,\n   are\
    \ the objects that require evaluation and possible revision in\n   order to advance\
    \ to Internet Standard.\n   IF two (or more) implementations do not measure an\
    \ equivalent metric\n   as specified by this document,\n   AND sources of measurement\
    \ error do not adequately explain the lack\n   of agreement,\n   THEN the details\
    \ of each implementation should be audited along with\n   the exact definition\
    \ text to determine if there is a lack of clarity\n   that has caused the implementations\
    \ to vary in a way that affects the\n   correspondence of the results.\n   IF\
    \ there was a lack of clarity or multiple legitimate interpretations\n   of the\
    \ definition text,\n   THEN the text should be modified and the resulting memo\
    \ proposed for\n   consensus and (possible) advancement along the Standards Track.\n\
    \   Finally, all the findings MUST be documented in a report that can\n   support\
    \ advancement to Internet Standard, as described here (similar\n   to the reports\
    \ described in [RFC5657]).  The list of measurement\n   devices used in testing\
    \ satisfies the implementation requirement,\n   while the test results provide\
    \ information on the quality of each\n   specification in the metric RFC (the\
    \ surrogate for feature\n   interoperability).\n   The complete process of advancing\
    \ a metric specification to a\n   Standard as defined by this document is illustrated\
    \ in Figure 4.\n      ,---.\n     /     \\\n    ( Start )\n     \\     /    Implementations\n\
    \      `-+-'        +-------+\n        |         /|   1   `.\n    +---+----+ \
    \  / +-------+ `.-----------+     ,-------.\n    |  RFC   |  /             |Check\
    \ for  |   ,' was RFC `. YES\n    |        | /              |Equivalence.... \
    \ clause x   ------+\n    |        |/    +-------+  |under      |   `. clear?\
    \  ,'      |\n    | Metric \\.....|   2   ....relevant   |     `---+---'   +----+-----+\n\
    \    | Metric |\\    +-------+  |identical  |      No |       |Report    |\n \
    \   | Metric | \\              |network    |      +--+----+  |results + |\n  \
    \  |  ...   |  \\             |conditions |      |Modify |  |Advance   |\n   \
    \ |        |   \\ +-------+  |           |      |Spec   +--+RFC       |\n    +--------+\
    \    \\|   n   |.'+-----------+      +-------+  |request   |\n               \
    \    +-------+                                +----------+\n       Figure 4: Illustration\
    \ of the Metric Standardization Process\n   Any recommendation for the advancement\
    \ of a metric specification MUST\n   be accompanied by an implementation report.\
    \  The implementation\n   report needs to include the tests performed, the applied\
    \ test setup,\n   the specific metrics in the RFC, and reports of the tests performed\n\
    \   with two or more implementations.  The test plan needs to specify the\n  \
    \ precision reached for each measured metric and thus define the\n   meaning of\
    \ \"statistically equivalent\" for the specific metrics being\n   tested.\n  \
    \ Ideally, the test plan would co-evolve with the development of the\n   metric,\
    \ since that's when participants have the clearest context in\n   their minds\
    \ regarding the different subtleties that can arise.\n   In particular, the implementation\
    \ report MUST include the following\n   at minimum:\n   o  The metric compared\
    \ and the RFC specifying it.  This includes\n      statements as required by Section\
    \ 3.1 (\"Tests of an Individual\n      Implementation against a Metric Specification\"\
    ) of this document.\n   o  The measurement configuration and setup.\n   o  A complete\
    \ specification of the measurement stream (mean rate,\n      statistical distribution\
    \ of packets, packet size or mean packet\n      size, and their distribution),\
    \ Differentiated Services Code Point\n      (DSCP), and any other measurement\
    \ stream properties that could\n      result in deviating results.  Deviations\
    \ in results can also be\n      caused if chosen IP addresses and ports of different\n\
    \      implementations result in different layer 2 or layer 3 paths due\n    \
    \  to operation of Equal Cost Multi-Path routing in an operational\n      network.\n\
    \   o  The duration of each measurement to be used for a metric\n      validation,\
    \ the number of measurement points collected for each\n      metric during each\
    \ measurement interval (i.e., the probe size),\n      and the level of confidence\
    \ derived from this probe size for each\n      measurement interval.\n   o  The\
    \ result of the statistical tests performed for each metric\n      validation\
    \ as required by Section 3.3 (\"Tests of Two or More\n      Different Implementations\
    \ against a Metric Specification\") of this\n      document.\n   o  A parameterization\
    \ of laboratory conditions and applied traffic\n      and network conditions allowing\
    \ reproduction of these laboratory\n      conditions for readers of the implementation\
    \ report.\n   o  The documentation helping to improve metric specifications defined\n\
    \      by this section.\n   All of the tests for each set SHOULD be run in a test\
    \ setup as\n   specified in Section 3.2 (\"Test Setup Resulting in Identical Live\n\
    \   Network Testing Conditions\".\n   If a different test setup is chosen, it\
    \ is recommended to avoid\n   effects falsifying results of validation measurements\
    \ caused by real\n   data networks (like parallelism in devices and networks).\
    \  Data\n   networks may forward packets differently in the case of:\n   o  Different\
    \ packet sizes chosen for different metric\n      implementations.  A proposed\
    \ countermeasure is selecting the same\n      packet size when validating results\
    \ of two samples or a sample\n      against an original distribution.\n   o  Selection\
    \ of differing IP addresses and ports used by different\n      metric implementations\
    \ during metric validation tests.  If ECMP is\n      applied on the IP or MPLS\
    \ level, different paths can result (note\n      that it may be impossible to\
    \ detect an MPLS ECMP path from an IP\n      endpoint).  A proposed countermeasure\
    \ is to connect the\n      measurement equipment to be compared by a NAT device\
    \ or establish\n      a single tunnel to transport all measurement traffic.  The\
    \ aim is\n      to have the same IP addresses and port for all measurement packets\n\
    \      or to avoid ECMP-based local routing diversion by using a layer 2\n   \
    \   tunnel.\n   o  Different IP options.\n   o  Different DSCP.\n   o  If the\
    \ N measurements are captured using sequential measurements\n      instead of\
    \ simultaneous ones, then the following factors come into\n      play: time varying\
    \ paths and load conditions.\n"
- title: 3.6.  Proposal to Determine an Equivalence Threshold for Each Metric
  contents:
  - "3.6.  Proposal to Determine an Equivalence Threshold for Each Metric\n      Evaluated\n\
    \   This section describes a proposal for maximum error of equivalence,\n   based\
    \ on performance comparison of identical implementations.  This\n   comparison\
    \ may be useful for both ADK and non-ADK comparisons.\n   Each metric is tested\
    \ by two or more implementations (cross-\n   implementation testing).\n   Each\
    \ metric is also tested twice simultaneously by the *same*\n   implementation,\
    \ using different Src/Dst Address pairs and other\n   differences such that the\
    \ connectivity differences of the cross-\n   implementation tests are also experienced\
    \ and measured by the same\n   implementation.\n   Comparative results for the\
    \ same implementation represent a bound on\n   cross-implementation equivalence.\
    \  This should be particularly useful\n   when the metric does *not* produce a\
    \ continuous distribution of\n   singleton values, such as with a loss metric\
    \ or a duplication metric.\n   Appendix A indicates how the ADK will work for\
    \ one-way delay and\n   should be likewise applicable to distributions of delay\
    \ variation.\n   Appendix B discusses two possible ways to perform the ADK analysis:\n\
    \   the R statistical language [Rtool] with ADK package [Radk] and C++\n   code.\n\
    \   Conclusion: the implementation with the largest difference in\n   homogeneous\
    \ comparison results is the lower bound on the equivalence\n   threshold, noting\
    \ that there may be other systematic errors to\n   account for when comparing\
    \ implementations.\n   Thus, when evaluating equivalence in cross-implementation\
    \ results:\n   Maximum_Error = Same_Implementation_Error + Systematic_Error\n\
    \   and only the systematic error need be decided beforehand.\n   In the case\
    \ of ADK comparison, the largest same-implementation\n   resolution of distribution\
    \ equivalence can be used as a limit on\n   cross-implementation resolutions (at\
    \ the same confidence level).\n"
- title: 4.  Acknowledgements
  contents:
  - "4.  Acknowledgements\n   Gerhard Hasslinger commented a first draft version of\
    \ this document;\n   he suggested statistical tests and the evaluation of time\
    \ series\n   information.  Matthias Wieser's thesis on a metric test resulted\
    \ in\n   new input for this document.  Henk Uijterwaal and Lars Eggert have\n\
    \   encouraged and helped to organize this work.  Mike Hamilton, Scott\n   Bradner,\
    \ David Mcdysan, and Emile Stephan commented on this document.\n   Carol Davids\
    \ reviewed a version of the document before it became a WG\n   item.\n"
- title: 5.  Contributors
  contents:
  - "5.  Contributors\n   Scott Bradner, Vern Paxson, and Allison Mankin drafted [METRICTEST],\n\
    \   and major parts of it are included in this document.\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   This memo does not raise any specific security\
    \ issues.\n"
- title: 7.  References
  contents:
  - '7.  References

    '
- title: 7.1.  Normative References
  contents:
  - "7.1.  Normative References\n   [RFC2003]      Perkins, C., \"IP Encapsulation\
    \ within IP\", RFC 2003,\n                  October 1996.\n   [RFC2119]      Bradner,\
    \ S., \"Key words for use in RFCs to Indicate\n                  Requirement Levels\"\
    , BCP 14, RFC 2119, March 1997.\n   [RFC2330]      Paxson, V., Almes, G., Mahdavi,\
    \ J., and M. Mathis,\n                  \"Framework for IP Performance Metrics\"\
    , RFC 2330,\n                  May 1998.\n   [RFC2661]      Townsley, W., Valencia,\
    \ A., Rubens, A., Pall, G.,\n                  Zorn, G., and B. Palter, \"Layer\
    \ Two Tunneling Protocol\n                  \"L2TP\"\", RFC 2661, August 1999.\n\
    \   [RFC2679]      Almes, G., Kalidindi, S., and M. Zekauskas, \"A One-way\n \
    \                 Delay Metric for IPPM\", RFC 2679, September 1999.\n   [RFC2784]\
    \      Farinacci, D., Li, T., Hanks, S., Meyer, D., and P.\n                 \
    \ Traina, \"Generic Routing Encapsulation (GRE)\",\n                  RFC 2784,\
    \ March 2000.\n   [RFC3931]      Lau, J., Townsley, M., and I. Goyret, \"Layer\
    \ Two\n                  Tunneling Protocol - Version 3 (L2TPv3)\", RFC 3931,\n\
    \                  March 2005.\n   [RFC4448]      Martini, L., Rosen, E., El-Aawar,\
    \ N., and G. Heron,\n                  \"Encapsulation Methods for Transport of\
    \ Ethernet over\n                  MPLS Networks\", RFC 4448, April 2006.\n  \
    \ [RFC4656]      Shalunov, S., Teitelbaum, B., Karp, A., Boote, J., and\n    \
    \              M. Zekauskas, \"A One-way Active Measurement Protocol\n       \
    \           (OWAMP)\", RFC 4656, September 2006.\n   [RFC4719]      Aggarwal,\
    \ R., Townsley, M., and M. Dos Santos,\n                  \"Transport of Ethernet\
    \ Frames over Layer 2 Tunneling\n                  Protocol Version 3 (L2TPv3)\"\
    , RFC 4719, November 2006.\n   [RFC4928]      Swallow, G., Bryant, S., and L.\
    \ Andersson, \"Avoiding\n                  Equal Cost Multipath Treatment in MPLS\
    \ Networks\",\n                  BCP 128, RFC 4928, June 2007.\n   [RFC5657] \
    \     Dusseault, L. and R. Sparks, \"Guidance on\n                  Interoperation\
    \ and Implementation Reports for\n                  Advancement to Draft Standard\"\
    , BCP 9, RFC 5657,\n                  September 2009.\n   [RFC6410]      Housley,\
    \ R., Crocker, D., and E. Burger, \"Reducing the\n                  Standards\
    \ Track to Two Maturity Levels\", BCP 9,\n                  RFC 6410, October\
    \ 2011.\n"
- title: 7.2.  Informative References
  contents:
  - "7.2.  Informative References\n   [ADK]          Scholz, F. and M. Stephens, \"\
    K-sample Anderson-Darling\n                  Tests of Fit, for Continuous and\
    \ Discrete Cases\",\n                  University of Washington, Technical Report\
    \ No. 81,\n                  May 1986.\n   [GU-Duffield]  Gu, Y., Duffield, N.,\
    \ Breslau, L., and S. Sen, \"GRE\n                  Encapsulated Multicast Probing:\
    \ A Scalable Technique\n                  for Measuring One-Way Loss\", SIGMETRICS'07\
    \ San Diego,\n                  California, USA, June 2007.\n   [METRICTEST] \
    \  Bradner, S. and V. Paxson, \"Advancement of metrics\n                  specifications\
    \ on the IETF Standards Track\", Work\n                  in Progress, August 2007.\n\
    \   [RFC2026]      Bradner, S., \"The Internet Standards Process --\n        \
    \          Revision 3\", BCP 9, RFC 2026, October 1996.\n   [RFC4459]      Savola,\
    \ P., \"MTU and Fragmentation Issues with In-the-\n                  Network Tunneling\"\
    , RFC 4459, April 2006.\n   [RFC5357]      Hedayat, K., Krzanowski, R., Morton,\
    \ A., Yum, K., and\n                  J. Babiarz, \"A Two-Way Active Measurement\
    \ Protocol\n                  (TWAMP)\", RFC 5357, October 2008.\n   [Radk]  \
    \       Scholz, F., \"adk: Anderson-Darling K-Sample Test and\n              \
    \    Combinations of Such Tests.  R package version 1.0\",\n                 \
    \ 2008.\n   [Rtool]        R Development Core Team, \"R: A language and\n    \
    \              environment for statistical computing.  R Foundation\n        \
    \          for Statistical Computing, Vienna, Austria.  ISBN\n               \
    \   3-900051-07-0\", 2011, <http://www.R-project.org/>.\n   [TESTPLAN]     Ciavattone,\
    \ L., Geib, R., Morton, A., and M. Wieser,\n                  \"Test Plan and\
    \ Results for Advancing RFC 2679 on the\n                  Standards Track\",\
    \ Work in Progress, March 2012.\n"
- title: Appendix A.  An Example on a One-Way Delay Metric Validation
  contents:
  - "Appendix A.  An Example on a One-Way Delay Metric Validation\n   The text of\
    \ this appendix is not binding.  It is an example of what\n   parts of a One-Way\
    \ Delay Metric test could look like.\n"
- title: A.1.  Compliance to Metric Specification Requirements
  contents:
  - "A.1.  Compliance to Metric Specification Requirements\n   One-Way Delay, Loss\
    \ Threshold, RFC 2679\n   This test determines if implementations use the same\
    \ configured\n   maximum waiting time delay from one measurement to another under\n\
    \   different delay conditions and correctly declare packets arriving in\n   excess\
    \ of the waiting time threshold as lost.  See Sections 3.5 (3rd\n   bullet point)\
    \ and 3.8.2 of [RFC2679].\n   (1)  Configure a path with 1-second one-way constant\
    \ delay.\n   (2)  Measure one-way delay with 2 or more implementations, using\n\
    \        identical waiting time thresholds for loss set at 2 seconds.\n   (3)\
    \  Configure the path with 3-second one-way delay.\n   (4)  Repeat measurements.\n\
    \   (5)  Observe that the increase measured in step 4 caused all packets\n   \
    \     to be declared lost and that all packets that arrive\n        successfully\
    \ in step 2 are assigned a valid one-way delay.\n   One-Way Delay, First Bit to\
    \ Last Bit, RFC 2679\n   This test determines if implementations register the\
    \ same relative\n   increase in delay from one measurement to another under different\n\
    \   delay conditions.  This test tends to cancel the sources of error\n   that\
    \ may be present in an implementation.  See Section 3.7.2 of\n   [RFC2679] and\
    \ Section 10.2 of [RFC2330].\n   (1)  Configure a path with X ms one-way constant\
    \ delay and ideally\n        include a low-speed link.\n   (2)  Measure one-way\
    \ delay with 2 or more implementations, using\n        identical options and equal\
    \ size small packets (e.g., 100 octet\n        IP payload).\n   (3)  Maintain\
    \ the same path with X ms one-way delay.\n   (4)  Measure one-way delay with 2\
    \ or more implementations, using\n        identical options and equal size large\
    \ packets (e.g., 1500 octet\n        IP payload).\n   (5)  Observe that the increase\
    \ measured in steps 2 and 4 is\n        equivalent to the increase in ms expected\
    \ due to the larger\n        serialization time for each implementation.  Most\
    \ of the\n        measurement errors in each system should cancel, if they are\n\
    \        stationary.\n   One-Way Delay, RFC 2679\n   This test determines if implementations\
    \ register the same relative\n   increase in delay from one measurement to another\
    \ under different\n   delay conditions.  This test tends to cancel the sources\
    \ of error\n   that may be present in an implementation.  This test is intended\
    \ to\n   evaluate measurements in Sections 3 and 4 of [RFC2679].\n   (1)  Configure\
    \ a path with X ms one-way constant delay.\n   (2)  Measure one-way delay with\
    \ 2 or more implementations, using\n        identical options.\n   (3)  Configure\
    \ the path with X+Y ms one-way delay.\n   (4)  Repeat measurements.\n   (5)  Observe\
    \ that the increase measured in steps 2 and 4 is ~Y ms for\n        each implementation.\
    \  Most of the measurement errors in each\n        system should cancel, if they\
    \ are stationary.\n   Error Calibration, RFC 2679\n   This is a simple check to\
    \ determine if an implementation reports the\n   error calibration as required\
    \ in Section 4.8 of [RFC2679].  Note that\n   the context (Type-P) must also be\
    \ reported.\n"
- title: A.2.  Examples Related to Statistical Tests for One-Way Delay
  contents:
  - "A.2.  Examples Related to Statistical Tests for One-Way Delay\n   A one-way delay\
    \ measurement may pass an ADK test with a timestamp\n   result of 1 ms.  The same\
    \ test may fail if timestamps with a\n   resolution of 100 microseconds are evaluated.\
    \  The implementation is\n   then conforming to the metric specification up to\
    \ a timestamp\n   resolution of 1 ms.\n   Let's assume another one-way delay measurement\
    \ comparison between\n   implementation 1 probing with a frequency of 2 probes\
    \ per second and\n   implementation 2 probing at a rate of 2 probes every 3 minutes.\
    \  To\n   ensure reasonable confidence in results, sample metrics are\n   calculated\
    \ from at least 5 singletons per compared time interval.\n   This means that sample\
    \ delay values are calculated for each system\n   for identical 6-minute intervals\
    \ for the duration of the whole test.\n   Per 6-minute interval, the sample metric\
    \ is calculated from 720\n   singletons for implementation 1 and from 6 singletons\
    \ for\n   implementation 2.  Note that if outliers are not filtered, moving\n\
    \   averages are an option for an evaluation too.  The minimum move of an\n  \
    \ averaging interval is three minutes in this example.\n   The data in Table 1\
    \ may result from measuring one-way delay with\n   implementation 1 (see column\
    \ Implemnt_1) and implementation 2 (see\n   column Implemnt_2).  Each data point\
    \ in the table represents a\n   (rounded) average of the sampled delay values\
    \ per interval.  The\n   resolution of the clock is one micro-second.  The difference\
    \ in the\n   delay values may result, e.g., from different probe packet sizes.\n\
    \         +------------+------------+-----------------------------+\n        \
    \ | Implemnt_1 | Implemnt_2 | Implemnt_2 - Delta_Averages |\n         +------------+------------+-----------------------------+\n\
    \         |    5000    |    6549    |             4997            |\n        \
    \ |    5008    |    6555    |             5003            |\n         |    5012\
    \    |    6564    |             5012            |\n         |    5015    |   \
    \ 6565    |             5013            |\n         |    5019    |    6568   \
    \ |             5016            |\n         |    5022    |    6570    |      \
    \       5018            |\n         |    5024    |    6573    |             5021\
    \            |\n         |    5026    |    6575    |             5023        \
    \    |\n         |    5027    |    6577    |             5025            |\n \
    \        |    5029    |    6580    |             5028            |\n         |\
    \    5030    |    6585    |             5033            |\n         |    5032\
    \    |    6586    |             5034            |\n         |    5034    |   \
    \ 6587    |             5035            |\n         |    5036    |    6588   \
    \ |             5036            |\n         |    5038    |    6589    |      \
    \       5037            |\n         |    5039    |    6591    |             5039\
    \            |\n         |    5041    |    6592    |             5040        \
    \    |\n         |    5043    |    6599    |             5047            |\n \
    \        |    5046    |    6606    |             5054            |\n         |\
    \    5054    |    6612    |             5060            |\n         +------------+------------+-----------------------------+\n\
    \                                  Table 1\n   Average values of sample metrics\
    \ captured during identical time\n   intervals are compared.  This excludes random\
    \ differences caused by\n   differing probing intervals or differing temporal\
    \ distance of\n   singletons resulting from their Poisson-distributed sending\
    \ times.\n   In the example, 20 values have been picked (note that at least 100\n\
    \   values are recommended for a single run of a real test).  Data must\n   be\
    \ ordered by ascending rank.  The data of Implemnt_1 and Implemnt_2\n   as shown\
    \ in the first two columns of Table 1 clearly fails an ADK\n   test with 95% confidence.\n\
    \   The results of Implemnt_2 are now reduced by the difference of the\n   averages\
    \ of column 2 (rounded to 6581 us) and column 1 (rounded to\n   5029 us), which\
    \ is 1552 us.  The result may be found in column 3 of\n   Table 1.  Comparing\
    \ column 1 and column 3 of the table by an ADK test\n   shows that the data contained\
    \ in these columns passes an ADK test\n   with 95% confidence.\n   Comment: Extensive\
    \ averaging was used in this example because of the\n   vastly different sampling\
    \ frequencies.  As a result, the\n   distributions compared do not exactly align\
    \ with a metric in\n   [RFC2679] but illustrate the ADK process adequately.\n"
- title: Appendix B.  Anderson-Darling K-sample Reference and 2 Sample C++ Code
  contents:
  - "Appendix B.  Anderson-Darling K-sample Reference and 2 Sample C++ Code\n   There\
    \ are many statistical tools available, and this appendix\n   describes two that\
    \ are familiar to the authors.\n   The \"R tool\" is a language and command-line\
    \ environment for\n   statistical computing and plotting [Rtool].  With the optional\
    \ \"adk\"\n   package installed [Radk], it can perform individual and combined\n\
    \   sample ADK computations.  The user must consult the package\n   documentation\
    \ and the original paper [ADK] to interpret the results,\n   but this is as it\
    \ should be.\n   The C++ code below will perform an AD2-sample comparison when\n\
    \   compiled and presented with two column vectors in a file (using white\n  \
    \ space as separation).  This version contains modifications made by\n   Wes Eddy\
    \ in Sept 2011 to use the vectors and run as a stand-alone\n   module.  The status\
    \ of the comparison can be checked on the command\n   line with \"$ echo $?\"\
    \ or the last line can be replaced with a printf\n   statement for adk_result\
    \ instead.\n  /*\n      Copyright (c) 2012 IETF Trust and the persons identified\n\
    \      as authors of the code.  All rights reserved.\n      Redistribution and\
    \ use in source and binary forms, with\n      or without modification, is permitted\
    \ pursuant to, and subject\n      to the license terms contained in, the Simplified\
    \ BSD License\n      set forth in Section 4.c of the IETF Trust's Legal Provisions\n\
    \      Relating to IETF Documents (http://trustee.ietf.org/license-info).\n  */\n\
    \  /* Routines for computing the Anderson-Darling 2 sample\n  * test statistic.\n\
    \  *\n  * Implemented based on the description in\n  * \"Anderson-Darling K Sample\
    \ Test\" Heckert, Alan and\n  * Filliben, James, editors, Dataplot Reference Manual,\n\
    \  * Chapter 15 Auxiliary, NIST, 2004.\n  * Official Reference by 2010\n  * Heckert,\
    \ N. A. (2001).  Dataplot website at the\n  * National Institute of Standards\
    \ and Technology:\n  * http://www.itl.nist.gov/div898/software/dataplot.html/\n\
    \  * June 2001.\n */\n #include <iostream>\n #include <fstream>\n #include <vector>\n\
    \ #include <sstream>\n using namespace std;\n int main() {\n    vector<double>\
    \ vec1, vec2;\n    double adk_result;\n    static int k, val_st_z_samp1, val_st_z_samp2,\n\
    \               val_eq_z_samp1, val_eq_z_samp2,\n               j, n_total, n_sample1,\
    \ n_sample2, L,\n               max_number_samples, line, maxnumber_z;\n    static\
    \ int column_1, column_2;\n    static double adk, n_value, z, sum_adk_samp1,\n\
    \                  sum_adk_samp2, z_aux;\n    static double H_j, F1j, hj, F2j,\
    \ denom_1_aux, denom_2_aux;\n    static bool next_z_sample2, equal_z_both_samples;\n\
    \    static int stop_loop1, stop_loop2, stop_loop3,old_eq_line2,\n           \
    \    old_eq_line1;\n    static double adk_criterium = 1.993;\n    /* vec1 and\
    \ vec2 to be initialized with sample 1 and\n     * sample 2 values in ascending\
    \ order */\n    while (!cin.eof()) {\n       double f1, f2;\n       cin >> f1;\n\
    \       cin >> f2;\n       vec1.push_back(f1);\n       vec2.push_back(f2);\n \
    \   }\n    k = 2;\n    n_sample1 = vec1.size() - 1;\n    n_sample2 = vec2.size()\
    \ - 1;\n    // -1 because vec[0] is a dummy value\n    n_total = n_sample1 + n_sample2;\n\
    \    /* value equal to the line with a value = zj in sample 1.\n     * Here j=1,\
    \ so the line is 1.\n     */\n    val_eq_z_samp1 = 1;\n    /* value equal to the\
    \ line with a value = zj in sample 2.\n     * Here j=1, so the line is 1.\n  \
    \   */\n    val_eq_z_samp2 = 1;\n    /* value equal to the last line with a value\
    \ < zj\n     * in sample 1.  Here j=1, so the line is 0.\n     */\n    val_st_z_samp1\
    \ = 0;\n    /* value equal to the last line with a value < zj\n     * in sample\
    \ 1.  Here j=1, so the line is 0.\n     */\n    val_st_z_samp2 = 0;\n    sum_adk_samp1\
    \ = 0;\n    sum_adk_samp2 = 0;\n    j = 1;\n    // as mentioned above, j=1\n \
    \   equal_z_both_samples = false;\n    next_z_sample2 = false;\n    //assuming\
    \ the next z to be of sample 1\n    stop_loop1 = n_sample1 + 1;\n    // + 1 because\
    \ vec[0] is a dummy, see n_sample1 declaration\n    stop_loop2 = n_sample2 + 1;\n\
    \    stop_loop3 = n_total + 1;\n    /* The required z values are calculated until\
    \ all values\n     * of both samples have been taken into account.  See the\n\
    \     * lines above for the stoploop values.  Construct required\n     * to avoid\
    \ a mathematical operation in the while condition.\n     */\n    while (((stop_loop1\
    \ > val_eq_z_samp1)\n           || (stop_loop2 > val_eq_z_samp2)) && stop_loop3\
    \ > j)\n    {\n      if(val_eq_z_samp1 < n_sample1+1)\n      {\n     /* here,\
    \ a preliminary zj value is set.\n      * See below how to calculate the actual\
    \ zj.\n      */\n            z = vec1[val_eq_z_samp1];\n     /* this while sequence\
    \ calculates the number of values\n      * equal to z.\n      */\n           \
    \ while ((val_eq_z_samp1+1 < n_sample1)\n                    && z == vec1[val_eq_z_samp1+1]\
    \ )\n                    {\n                    val_eq_z_samp1++;\n          \
    \          }\n            }\n            else\n            {\n            val_eq_z_samp1\
    \ = 0;\n            val_st_z_samp1 = n_sample1;\n    // this should be val_eq_z_samp1\
    \ - 1 = n_sample1\n            }\n    if(val_eq_z_samp2 < n_sample2+1)\n     \
    \       {\n            z_aux = vec2[val_eq_z_samp2];;\n    /* this while sequence\
    \ calculates the number of values\n     * equal to z_aux\n     */\n          \
    \  while ((val_eq_z_samp2+1 < n_sample2)\n                    && z_aux == vec2[val_eq_z_samp2+1]\
    \ )\n                    {\n                    val_eq_z_samp2++;\n          \
    \          }\n    /* the smaller of the two actual data values is picked\n   \
    \  * as the next zj.\n     */\n        if(z > z_aux)\n                    {\n\
    \                    z = z_aux;\n                    next_z_sample2 = true;\n\
    \                    }\n             else\n                    {\n           \
    \         if (z == z_aux)\n                    {\n                    equal_z_both_samples\
    \ = true;\n                    }\n    /* This is the case if the last value of\
    \ column1 is\n     * smaller than the remaining values of column2.\n     */\n\
    \                   if (val_eq_z_samp1 == 0)\n                    {\n        \
    \            z = z_aux;\n                    next_z_sample2 = true;\n        \
    \            }\n                }\n            }\n           else\n          \
    \    {\n            val_eq_z_samp2 = 0;\n            val_st_z_samp2 = n_sample2;\n\
    \    // this should be val_eq_z_samp2 - 1 = n_sample2\n            }\n     /*\
    \ in the following, sum j = 1 to L is calculated for\n      * sample 1 and sample\
    \ 2.\n      */\n           if (equal_z_both_samples)\n              {\n      \
    \        /* hj is the number of values in the combined sample\n              \
    \ * equal to zj\n               */\n                   hj = val_eq_z_samp1 - val_st_z_samp1\n\
    \                  + val_eq_z_samp2 - val_st_z_samp2;\n              /* H_j is\
    \ the number of values in the combined sample\n               * smaller than zj\
    \ plus one half the number of\n               * values in the combined sample\
    \ equal to zj\n               * (that's hj/2).\n               */\n          \
    \        H_j = val_st_z_samp1 + val_st_z_samp2\n                         + hj\
    \ / 2;\n              /* F1j is the number of values in the 1st sample\n     \
    \          * that are less than zj plus one half the number\n               *\
    \ of values in this sample that are equal to zj.\n               */\n        \
    \          F1j = val_st_z_samp1 + (double)\n                      (val_eq_z_samp1\
    \ - val_st_z_samp1) / 2;\n              /* F2j is the number of values in the\
    \ 1st sample\n               * that are less than zj plus one half the number\n\
    \               * of values in this sample that are equal to zj.\n           \
    \    */\n                  F2j = val_st_z_samp2 + (double)\n                 \
    \    (val_eq_z_samp2 - val_st_z_samp2) / 2;\n              /* set the line of\
    \ values equal to zj to the\n               * actual line of the last value picked\
    \ for zj.\n               */\n                  val_st_z_samp1 = val_eq_z_samp1;\n\
    \              /* Set the line of values equal to zj to the actual\n         \
    \      * line of the last value picked for zj of each\n               * sample.\
    \  This is required as data smaller than zj\n               * is accounted differently\
    \ than values equal to zj.\n               */\n                  val_st_z_samp2\
    \ = val_eq_z_samp2;\n              /* next the lines of the next values z, i.e.,\
    \ zj+1\n               * are addressed.\n               */\n                val_eq_z_samp1++;\n\
    \              /* next the lines of the next values z, i.e.,\n               *\
    \ zj+1 are addressed\n               */\n                  val_eq_z_samp2++;\n\
    \                  }\n           else\n                  {\n              /* the\
    \ smaller z value was contained in sample 2;\n               * hence, this value\
    \ is the zj to base the following\n               * calculations on.\n       \
    \        */\n                            if (next_z_sample2)\n               \
    \             {\n              /* hj is the number of values in the combined\n\
    \               * sample equal to zj; in this case, these are\n              \
    \ * within sample 2 only.\n               */\n                            hj =\
    \ val_eq_z_samp2 - val_st_z_samp2;\n              /* H_j is the number of values\
    \ in the combined sample\n               * smaller than zj plus one half the number\
    \ of\n               * values in the combined sample equal to zj\n           \
    \    * (that's hj/2).\n               */\n                                H_j\
    \ = val_st_z_samp1 + val_st_z_samp2\n                              + hj / 2;\n\
    \              /* F1j is the number of values in the 1st sample that\n       \
    \        * are less than zj plus one half the number of values in\n          \
    \     * this sample that are equal to zj.\n               * As val_eq_z_samp2\
    \ < val_eq_z_samp1, these are the\n               * val_st_z_samp1 only.\n   \
    \            */\n                            F1j = val_st_z_samp1;\n         \
    \     /* F2j is the number of values in the 1st sample that\n               *\
    \ are less than zj plus one half the number of values in\n               * this\
    \ sample that are equal to zj.  The latter are from\n               * sample 2\
    \ only in this case.\n               */\n                    F2j = val_st_z_samp2\
    \ + (double)\n                         (val_eq_z_samp2 - val_st_z_samp2) / 2;\n\
    \              /* Set the line of values equal to zj to the actual line\n    \
    \           * of the last value picked for zj of sample 2 only in\n          \
    \     * this case.\n               */\n                                val_st_z_samp2\
    \ = val_eq_z_samp2;\n              /* next the line of the next value z, i.e.,\
    \ zj+1 is\n               * addressed.  Here, only sample 2 must be addressed.\n\
    \               */\n                    val_eq_z_samp2++;\n                  \
    \                  if (val_eq_z_samp1 == 0)\n                                \
    \    {\n                                    val_eq_z_samp1 = stop_loop1;\n   \
    \                                 }\n                            }\n    /* the\
    \ smaller z value was contained in sample 2;\n     * hence, this value is the\
    \ zj to base the following\n     * calculations on.\n     */\n               \
    \   else\n                  {\n    /* hj is the number of values in the combined\n\
    \     * sample equal to zj; in this case, these are\n     * within sample 1 only.\n\
    \     */\n                  hj = val_eq_z_samp1 - val_st_z_samp1;\n    /* H_j\
    \ is the number of values in the combined\n     * sample smaller than zj plus\
    \ one half the number\n     * of values in the combined sample equal to zj\n \
    \    * (that's hj/2).\n     */\n          H_j = val_st_z_samp1 + val_st_z_samp2\n\
    \                + hj / 2;\n    /* F1j is the number of values in the 1st sample\
    \ that\n     * are less than zj plus; in this case, these are within\n     * sample\
    \ 1 only one half the number of values in this\n     * sample that are equal to\
    \ zj.  The latter are from\n     * sample 1 only in this case.\n     */\n    \
    \      F1j = val_st_z_samp1 + (double)\n               (val_eq_z_samp1 - val_st_z_samp1)\
    \ / 2;\n    /* F2j is the number of values in the 1st sample that\n     * are\
    \ less than zj plus one half the number of values\n     * in this sample that\
    \ are equal to zj.  As\n     * val_eq_z_samp1 < val_eq_z_samp2, these are the\n\
    \     * val_st_z_samp2 only.\n     */\n                  F2j = val_st_z_samp2;\n\
    \    /* Set the line of values equal to zj to the actual line\n     * of the last\
    \ value picked for zj of sample 1 only in\n     * this case.\n     */\n      \
    \    val_st_z_samp1 = val_eq_z_samp1;\n    /* next the line of the next value\
    \ z, i.e., zj+1 is\n     * addressed.  Here, only sample 1 must be addressed.\n\
    \     */\n                  val_eq_z_samp1++;\n                  if (val_eq_z_samp2\
    \ == 0)\n                          {\n                          val_eq_z_samp2\
    \ = stop_loop2;\n                          }\n                  }\n          \
    \        }\n            denom_1_aux = n_total * F1j - n_sample1 * H_j;\n     \
    \       denom_2_aux = n_total * F2j - n_sample2 * H_j;\n            sum_adk_samp1\
    \ = sum_adk_samp1 + hj\n                    * (denom_1_aux * denom_1_aux) /\n\
    \                                       (H_j * (n_total - H_j)\n             \
    \       - n_total * hj / 4);\n            sum_adk_samp2 = sum_adk_samp2 + hj\n\
    \           * (denom_2_aux * denom_2_aux) /\n                               (H_j\
    \ * (n_total - H_j)\n          - n_total * hj / 4);\n            next_z_sample2\
    \ = false;\n            equal_z_both_samples = false;\n    /* index to count the\
    \ z.  It is only required to prevent\n     * the while slope to execute endless\n\
    \     */\n            j++;\n            }\n    // calculating the adk value is\
    \ the final step.\n    adk_result = (double) (n_total - 1) / (n_total\n      \
    \     * n_total * (k - 1))\n            * (sum_adk_samp1 / n_sample1\n       \
    \     + sum_adk_samp2 / n_sample2);\n    /* if(adk_result <= adk_criterium)\n\
    \     * adk_2_sample test is passed\n     */\n    return adk_result <= adk_criterium;\n\
    \ }\n"
- title: Appendix C.  Glossary
  contents:
  - "Appendix C.  Glossary\n   +-------------+-----------------------------------------------------+\n\
    \   | ADK         | Anderson-Darling K-Sample test, a test used to      |\n  \
    \ |             | check whether two samples have the same statistical |\n   |\
    \             | distribution.                                       |\n   | ECMP\
    \        | Equal Cost Multipath, a load-balancing mechanism    |\n   |       \
    \      | evaluating MPLS Labels stacks, IP addresses, and    |\n   |         \
    \    | ports.                                              |\n   | EDF       \
    \  | The \"empirical distribution function\" of a set of   |\n   |           \
    \  | scalar measurements is a function F(x), which for   |\n   |             |\
    \ any x gives the fractional proportion of the total  |\n   |             | measurements\
    \ that were smaller than or equal to x.  |\n   | Metric      | A measured quantity\
    \ related to the performance and  |\n   |             | reliability of the Internet,\
    \ expressed by a value.  |\n   |             | This could be a singleton (single\
    \ value), a sample  |\n   |             | of single values, or a statistic based\
    \ on a sample  |\n   |             | of singletons.                          \
    \            |\n   | OWAMP       | One-Way Active Measurement Protocol, a protocol\
    \ for |\n   |             | communication between IPPM measurement systems   \
    \   |\n   |             | specified by IPPM.                                 \
    \ |\n   | OWD         | One-Way Delay, a performance metric specified by    |\n\
    \   |             | IPPM.                                               |\n  \
    \ | Sample      | A sample metric is derived from a given singleton   |\n   |\
    \ metric      | metric by evaluating a number of distinct instances |\n   |  \
    \           | together.                                           |\n   | Singleton\
    \   | A singleton metric is, in a sense, one atomic       |\n   | metric     \
    \ | measurement of this metric.                         |\n   | Statistical |\
    \ A 'statistical' metric is derived from a given      |\n   | metric      | sample\
    \ metric by computing some statistic of the    |\n   |             | values defined\
    \ by the singleton metric on the       |\n   |             | sample.         \
    \                                    |\n   | TWAMP       | Two-way Active Measurement\
    \ Protocol, a protocol for |\n   |             | communication between IPPM measurement\
    \ systems      |\n   |             | specified by IPPM.                      \
    \            |\n   +-------------+-----------------------------------------------------+\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Ruediger Geib (editor)\n   Deutsche Telekom\n   Heinrich\
    \ Hertz Str. 3-7\n   Darmstadt  64295\n   Germany\n   Phone: +49 6151 58 12747\n\
    \   EMail: Ruediger.Geib@telekom.de\n   Al Morton\n   AT&T Labs\n   200 Laurel\
    \ Avenue South\n   Middletown, NJ  07748\n   USA\n   Phone: +1 732 420 1571\n\
    \   Fax:   +1 732 368 1192\n   EMail: acmorton@att.com\n   URI:   http://home.comcast.net/~acmacm/\n\
    \   Reza Fardid\n   Cariden Technologies\n   888 Villa Street, Suite 500\n   Mountain\
    \ View, CA  94041\n   USA\n   Phone:\n   EMail: rfardid@cariden.com\n   Alexander\
    \ Steinmitz\n   Deutsche Telekom\n   Memmelsdorfer Str. 209b\n   Bamberg  96052\n\
    \   Germany\n   Phone:\n   EMail: Alexander.Steinmitz@telekom.de\n"
