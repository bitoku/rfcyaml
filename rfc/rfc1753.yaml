- title: __initial_text__
  contents:
  - "                      IPng Technical Requirements\n           Of the Nimrod Routing\
    \ and Addressing Architecture\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  This memo\n   does not specify an Internet standard of any kind.  Distribution\
    \ of\n   this memo is unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document was submitted to the IETF IPng area in response to\
    \ RFC\n   1550.  Publication of this document does not imply acceptance by the\n\
    \   IPng area of any ideas expressed within.  Comments should be\n   submitted\
    \ to the big-internet@munnari.oz.au mailing list.\n   This document presents the\
    \ requirements that the Nimrod routing and\n   addressing architecture has upon\
    \ the internetwork layer protocol. To\n   be most useful to Nimrod, any protocol\
    \ selected as the IPng should\n   satisfy these requirements. Also presented is\
    \ some background\n   information, consisting of i) information about architectural\
    \ and\n   design principles which might apply to the design of a new\n   internetworking\
    \ layer, and ii) some details of the logic and\n   reasoning behind particular\
    \ requirements.\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   It is important to note that this document is not \"IPng\
    \ Requirements\n   for Routing\", as other proposed routing and addressing designs\
    \ may\n   need different support; this document is specific to Nimrod, and\n \
    \  doesn't claim to speak for other efforts.\n   However, although I don't wish\
    \ to assume that the particular designs\n   being worked on by the Nimrod WG will\
    \ be widely adopted by the\n   Internet (if for no other reason, they have not\
    \ yet been deployed and\n   tried and tested in practise, to see if they really\
    \ work, an\n   absolutely necessary hurdle for any protocol), there are reasons\
    \ to\n   believe that any routing architecture for a large, ubiquitous global\n\
    \   Internet will have many of the same basic fundamental principles as\n   the\
    \ Nimrod architecture, and the requirements that these generate.\n   While current\
    \ day routing technologies do not yet have the\n   characteristics and capabilities\
    \ that generate these requirements,\n   they also do not seem to be completely\
    \ suited to routing in the\n   next-generation Internet. As routing technology\
    \ moves towards what is\n   needed for the next generation Internet, the underlying\
    \ fundamental\n   laws and principles of routing will almost inevitably drive\
    \ the\n   design, and hence the requirements, toward things which look like the\n\
    \   material presented here.\n   Therefore, even if Nimrod is not the routing\
    \ architecture of the\n   next-generation Internet, the basic routing architecture\
    \ of that\n   Internet will have requirements that, while differing in detail,\
    \ will\n   almost inevitably be similar to these.\n   In a similar, but more general,\
    \ context, note that, by and large, the\n   general analysis of sections 3.1 (\"\
    Interaction Architectural Issues\")\n   and 3.2 (\"State and Flows in the Internetwork\
    \ Layer\") will apply to\n   other areas of a new internetwork layer, not just\
    \ routing.\n   I will tackle the internetwork packet format first (which is\n\
    \   simpler), and then the whole issue of the interaction with the rest\n   of\
    \ the internetwork layer (which is a much more subtle topic).\n"
- title: 2. Packet Format
  contents:
  - '2. Packet Format

    '
- title: 2.1 Packet Format Issues
  contents:
  - "2.1 Packet Format Issues\n   As a general rule, the design philosophy of Nimrod\
    \ is \"maximize the\n   lifetime (and flexibility) of the architecture\". Design\
    \ tradeoffs\n   (i.e., optimizations) that will adversely affect the flexibility,\n\
    \   adaptability and lifetime of the design are not not necessarily wise\n   choices;\
    \ they may cost more than they save. Such optimizations might\n   be the correct\
    \ choices in a stand-alone system, where the replacement\n   costs are relatively\
    \ small; in the global communication network, the\n   replacement costs are very\
    \ much higher.\n   Providing the Nimrod functionality requires the carrying of\
    \ certain\n   information in the packets. The design principle noted above has\
    \ a\n   number of corollaries in specifying the fields to contain that\n   information.\n\
    \   First, the design should be \"simple and straightforward\", which means\n\
    \   that various functions should be handled by completely separate\n   mechanisms,\
    \ and fields in the packets. It may seem that an\n   opportunity exists to save\
    \ space by overloading two functions onto\n   one mechanism or field, but general\
    \ experience is that, over time,\n   this attempt at optimization costs more,\
    \ by restricting flexibility\n   and adaptability.\n   Second, field lengths should\
    \ be specified to be somewhat larger than\n   can conceivably be used; the history\
    \ of system architecture is\n   replete with examples (processor address size\
    \ being the most\n   notorious) where fields became too short over the lifetime\
    \ of the\n   system. The document indicates what the smallest reasonable\n   \"\
    adequate\" lengths are, but this is more of a \"critical floor\" than a\n   recommendation.\
    \ A \"recommended\" length is also given, which is the\n   length which corresponds\
    \ to the application of this principle. The\n   wise designer would pick this\
    \ length.\n   It is important to now that this does *not* mean that implementations\n\
    \   must support the maximum value possible in a field of that size. I\n   imagine\
    \ that system-wide administrative limits will be placed on the\n   maximum values\
    \ which must be supported. Then, as the need arises, we\n   can increase the administrative\
    \ limit. This allows an easy, and\n   completely interoperable (with no special\
    \ mechanisms) path to upgrade\n   the capability of the network. If the maximum\
    \ supported value of a\n   field needs to be increased from M to N, an announcement\
    \ is made that\n   this is coming; during the interim period, the system continues\
    \ to\n   operate with M, but new implementations are deployed; while this is\n\
    \   happening, interoperation is automatic, with no transition mechanisms\n  \
    \ of any kind needed. When things are \"ready\" (i.e., the proportion of\n   old\
    \ equipment is small enough), use of the larger value commences.\n   Also, in\
    \ speaking of the packet format, you first need to distinguish\n   between the\
    \ host-router part of the path, and the router-router part;\n   a format that\
    \ works OK for one may not do for another.\n   The issue is complicated by the\
    \ fact that Nimrod can be made to work,\n   albeit not in optimal form, with information/fields\
    \ missing from the\n   packet in the host to \"first hop router\" section of the\
    \ packet's\n   path. The missing fields and information can then be added by the\n\
    \   first hop router. (This capability will be used to allow deployment\n   and\
    \ operation with unmodified IPv4 hosts, although similar techniques\n   could\
    \ be used with other internetworking protocols.) Access to the\n   full range\
    \ of Nimrod capabilities will require upgrading of hosts to\n   include the necessary\
    \ information in the packets they exchange with\n   the routers.\n   Second, Nimrod\
    \ currently has three planned forwarding modes (flows,\n   datagram, and source-routed\
    \ packets), and a format that works for one\n   may not work for another; some\
    \ modes use fields that are not used by\n   other modes.  The presence or absence\
    \ of these fields will make a\n   difference.\n"
- title: 2.2 Packet Format Fields
  contents:
  - "2.2 Packet Format Fields\n   What Nimrod would like to see in the internetworking\
    \ packet is:\n   - Source and destination endpoint identification. There are several\n\
    \     possibilities here.\n     One is \"UID\"s, which are \"shortish\", fixed\
    \ length fields which\n     appear in each packet, in the internetwork header,\
    \ which contain\n     globally unique, topologically insensitive identifiers for\
    \ either\n     i) endpoints (if you aren't familiar with endpoints, think of them\n\
    \     as hosts), or ii) multicast groups. (In the former instance, the\n     UID\
    \ is an EID; in the latter, a \"set ID\", or SID. An SID is an\n     identifier\
    \ which looks just like an EID, but it refers to a group\n     of endpoints. The\
    \ semantics of SID's are not completely defined.)\n     For each of these 48 bits\
    \ is adequate, but we would recommend 64\n     bits. (IPv4 will be able to operate\
    \ with smaller ones for a while,\n     but eventually either need a new packet\
    \ format, or the difficult\n     and not wholly satisfactory technique known as\
    \ Network Address\n     Translators, which allows the contents of these fields\
    \ to be only\n     locally unique.)\n     Another possibility is some shorter\
    \ field, named an \"endpoint\n     selector\", or ESEL, which contains a value\
    \ which is not globally\n     unique, but only unique in mapping tables on each\
    \ end, tables which\n     map from the small value to a globally unique value,\
    \ such as a DNS\n     name.\n     Finally, it is possible to conceive of overall\
    \ networking designs\n     which do not include any endpoint identification in\
    \ the packet at\n     all, but transfer it at the start of a communication, and\
    \ from then\n     on infer it.  This alternative would have to have some other\
    \ means\n     of telling which endpoint a given packet is for, if there are\n\
    \     several endpoints at a given destination. Some coordination on\n     allocation\
    \ of flow-ids, or higher level port numbers, etc., might\n     do this.\n   -\
    \ Flow identification. There are two basic approaches here, depending\n     on\
    \ whether flows are aggregated (in intermediate switches) or not.\n     It should\
    \ be emphasized at this point that it is not yet known\n     whether flow aggregation\
    \ will be needed. The only reason to do it\n     is to control the growth of state\
    \ in intermediate routers, but\n     there is no hard case made that either this\
    \ growth will be\n     unmanageable, or that aggregating flows will be feasible\n\
    \     practically.\n     For the non-aggregated case, a single \"flow-id\" field\
    \ will suffice.\n     This *must not* use one of the two previous UID fields,\
    \ as in\n     datagram mode (and probably source-routed mode as well) the flow-id\n\
    \     will be over-written during transit of the network. It could most\n    \
    \ easily be constructed by adding a UID to a locally unique flow-id,\n     which\
    \ will provide a globally unique flow-id. It is possible to use\n     non-globally\
    \ unique flow-ids, (which would allow a shorter length\n     to this field), although\
    \ this would mean that collisions would\n     result, and have to be dealt with.\
    \ An adequate length for the local\n     part of a globally unique flow-id would\
    \ be 12 bits (which would be\n     my \"out of thin air\" guess), but we recommend\
    \ 32. For a non-\n     globally unique flow-id, 24 bits would be adequate, but\
    \ I recommend\n     32.\n     For the aggregated case, three broad classes of\
    \ mechanism are\n     possible.\n      - Option 1: The packet contains a sequence\
    \ (sort of like a source\n        route) of flow-ids. Whenever you aggregate or\
    \ deaggregate, you\n        move along the list to the next one. This takes the\
    \ most space,\n        but is otherwise the least work for the routers.\n    \
    \  - Option 2: The packet contains a stack of flow-ids, with the\n        current\
    \ one on the top. When you aggregate, you push a new one\n        on; when you\
    \ de-aggregate, you take one off. This takes more\n        work, but less space\
    \ in the packet than the complete \"source-\n        route\". Encapsulating packets\
    \ to do aggregation does basically\n        this, but you're stacking entire headers,\
    \ not just flow-ids. The\n        clever way to do this flow-id stacking, without\
    \ doing\n        encapsulation, is to find out from flow-setup how deep the stack\n\
    \        will get, and allocate the space in the packet when it's\n        created.\
    \ That way, all you ever have to do is insert a new\n        flow-id, or \"remove\"\
    \ one; you never have to make room for more\n        flow-ids.\n      - Option\
    \ 3: The packet contains only the \"base\" flow-id (i.e., the\n        one with\
    \ the finest granularity), and the current flow-id. When\n        you aggregate,\
    \ you just bash the current flow-id. The tricky\n        part comes when you de-aggregate;\
    \ you have to put the right\n        value back. To do this, you have to have\
    \ state in the router at\n        the end of the aggregated flow, which tells\
    \ you what the de-\n        aggregated flow for each base flow is. The downside\
    \ here is\n        obvious: we get away without individual flow state for each\
    \ of\n        the constituent flows in all the routers along the path of that\n\
    \        aggregated, flow, *except* for the last one.\n        Other than encapsulation,\
    \ which has significant inefficiency in\n        space overhead fairly quickly,\
    \ after just a few layers of\n        aggregation, there appears to be no way\
    \ to do it with just one\n        flow-id in the packet header.  Even if you don't\
    \ touch the\n        packets, but do the aggregation by mapping some number of\
    \ \"base\"\n        flow-id's to a single aggregated flow in the routers along\
    \ the\n        path of the aggregated flow, the table that does the mapping is\n\
    \        still going to have to have a number of entries directly\n        proportional\
    \ to the number of base flows going through the\n        switch.\n   - A looping\
    \ packet detector. This is any mechanism that will detect a\n     packet which\
    \ is \"stuck\" in the network; a timeout value in packets,\n     together with\
    \ a check in routers, is an example. If this is a hop-\n     count, it has to\
    \ be more than 8 bits; 12 bits would be adequate,\n     and I recommend 16 (which\
    \ also makes it easy to update). This is\n     not to say that I think networks\
    \ with diameters larger than 256 are\n     good, or that we should design such\
    \ nets, but I think limiting the\n     maximum path through the network to 256\
    \ hops is likely to bite us\n     down the road the same way making \"infinity\"\
    \ 16 in RIP did (as it\n     did, eventually). When we hit that ceiling, it's\
    \ going to hurt, and\n     there won't be an easy fix. I will note in passing\
    \ that we are\n     already seeing paths lengths of over 30 hops.\n   - Optional\
    \ source and destination locators. These are structured,\n     variable length\
    \ items which are topologically sensitive identifiers\n     for the place in the\
    \ network from which the traffic originates or\n     to which the traffic is destined.\
    \ The locator will probably contain\n     internal separators which divide up\
    \ the fields, so that a\n     particular field can be enlarged without creating\
    \ a great deal of\n     upheaval. An adequate value for maximum length supported\
    \ would be\n     up to 32 bytes per locator, and longer would be even better;\
    \ I\n     would recommend up to 256 bytes per locator.\n   - Perhaps (paired with\
    \ the above), an optional pointer into the\n     locators.  This is optional \"\
    forwarding state\" (i.e., state in the\n     packet which records something about\
    \ its progress across the\n     network) which is used in the datagram forwarding\
    \ mode to help\n     ensure that the packet does not loop. It can also improve\
    \ the\n     forwarding processing efficiency. It is thus not absolutely\n    \
    \ essential, but is very desirable from a real-world engineering view\n     point.\
    \ It needs to be large enough to identify locations in either\n     locator; e.g.,\
    \ if locators can be up to 256 bytes, it would need to\n     be 9 bits.\n   -\
    \ An optional source route. This is used to support the \"source\n     routed\
    \ packet\" forwarding mode. Although not designed in detail\n     yet, we can\
    \ discuss two possible approaches.\n     In one, used with \"semi-strict\" source\
    \ routing (in which a\n     contiguous series of entities is named, albeit perhaps\
    \ at a high\n     layer of abstraction), the syntax will likely look much like\
    \ source\n     routes in PIP; in Nimrod they will be a sequence of Nimrod entity\n\
    \     identifiers (i.e., locator elements, not complete locators), along\n   \
    \  with clues as to the context in which each identifier is to be\n     interpreted\
    \ (e.g., up, down, across, etc.). Since those identifiers\n     themselves are\
    \ variable length (although probably most will be two\n     bytes or less, otherwise\
    \ the routing overhead inside the named\n     object would be excessive), and\
    \ the hop count above contemplates\n     the possibility of paths of over 256\
    \ hops, it would seem that these\n     might possibly some day exceed 512 bytes,\
    \ if a lengthy path was\n     specified in terms of the actual physical assets\
    \ used. An adequate\n     length would be 512 bytes; the recommended length would\
    \ be 2^16\n     bytes (although this length would probably not be supported in\n\
    \     practise; rather, the field length would allow it).\n     In the other,\
    \ used with classical \"loose\" source routes, the source\n     consists of a\
    \ number of locators. It is not yet clear if this mode\n     will be supported.\
    \ If so, the header would need to be able to store\n     a sequence of locators\
    \ (as described above). Space might be saved\n     by not repeating locator prefixes\
    \ that match that of the previous\n     locator in the sequence; Nimrod will probably\
    \ allow use of such\n     \"locally useful\" locators. It is hard to determine\
    \ what an adequate\n     length would be for this case; the recommended length\
    \ would be 2^16\n     bytes (again, with the previous caveat).\n   - Perhaps (paired\
    \ with the above), an optional pointer into the\n     source route. This is also\
    \ optional \"forwarding state\". It needs to\n     be large enough to identify\
    \ locations anywhere in the source route;\n     e.g., if the source router can\
    \ be up to 1024 bytes, it would need\n     to be 10 bits.\n   - An internetwork\
    \ header length. I mention this since the above\n     fields could easily exceed\
    \ 256 bytes, if they are to all be carried\n     in the internetwork header (see\
    \ comments below as to where to carry\n     all this information), the header\
    \ length field needs to be more\n     than 8 bits; 12 bits would be adequate,\
    \ and I recommend 16 bits.\n     The approach of putting some of the data items\
    \ above into an\n     interior header, to limit the size of the basic internetworking\n\
    \     header, does not really seem optimal, as this data is for use by\n     the\
    \ intermediate routers, and it needs to be easily accessible.\n   - Authentication\
    \ of some sort is needed. See the recent IAB document\n     which was produced\
    \ as a result of the IAB architecture retreat on\n     security (draft-iab-sec-arch-workshop-00.txt),\
    \ section 4, and\n     especially section 4.3. There is currently no set way of\
    \ doing\n     \"denial/theft of service\" in Nimrod, but this topic is well\n\
    \     explored in that document; Nimrod would use whatever mechanism(s)\n    \
    \ seem appropriate to those knowledgeable in this area.\n   - A version number.\
    \ Future forwarding mechanisms might need other\n     information (i.e., fields)\
    \ in the packet header; use a version\n     number would allow it to be modified\
    \ to contain what's needed.\n     (This would not necessarily be information that\
    \ is visible to the\n     hosts, so this does not necessarily mean that the hosts\
    \ would need\n     to know about this new format.) 4 bits is adequate; it's not\
    \ clear\n     if a larger value needs to be recommended.\n"
- title: 2.3 Field Requirements and Addition Methods
  contents:
  - "2.3 Field Requirements and Addition Methods\n   As noted above, it's possible\
    \ to use Nimrod in a limited mode where\n   needed information/fields are added\
    \ by the first-hop router. It's\n   thus useful to ask \"which of the fields must\
    \ be present in the host-\n   router header, and which could be added by the router?\"\
    \ The only ones\n   which are absolutely necessary in all packets are the endpoint\n\
    \   identification (provided that some means is available to map them\n   into\
    \ locators; this would obviously be most useful on UID's which are\n   EID's).\n\
    \   As to the others, if the user wishes to use flows, and wants to\n   guarantee\
    \ that their packets are assigned to the correct flows, the\n   flow-id field\
    \ is needed. If the user wishes efficient use of the\n   datagram mode, it's probably\
    \ necessary to include the locators in the\n   packet sent to the router.  If\
    \ the user wishes to specify the route\n   for the packets, and does not wish\
    \ to set up a flow, they need to\n   include the source route.\n   How would additional\
    \ information/fields be added to the packet, if\n   the packet is emitted from\
    \ the host in incomplete form? (By this, I\n   mean the simple question of how,\
    \ mechanically, not the more complex\n   issue of where any needed information\
    \ comes from.)\n   This question is complex, since all the IPng candidates (and\
    \ in fact,\n   any reasonable inter-networking protocol) are extensible protocols;\n\
    \   those extension mechanisms could be used. Also, it would possible to\n   carry\
    \ some of the required information as user data in the\n   internetworking packet,\
    \ with the original user's data encapsulated\n   further inside. Finally, a private\
    \ inter-router packet format could\n   be defined.\n   It's not clear which path\
    \ is best, but we can talk about which fields\n   the Nimrod routers need access\
    \ to, and how often; less used ones\n   could be placed in harder-to-get-to locations\
    \ (such as in an\n   encapsulated header). The fields to which the routers need\
    \ access on\n   every hop are the flow-id and the looping packet detector. The\n\
    \   locator/pointer fields are only needed at intervals (in what datagram\n  \
    \ forwarding mode calls \"active\" routers), as is the source route (the\n   latter\
    \ at every object which is named in the source route).\n   Depending on how access\
    \ control is done, and which forwarding mode is\n   used, the UID's and/or locators\
    \ might be examined for access control\n   purposes, wherever that function is\
    \ performed.\n   This is not a complete exploration of the topic, but should give\
    \ a\n   rough idea of what's going on.\n"
- title: 3. Architectural Issues
  contents:
  - '3. Architectural Issues

    '
- title: 3.1 Interaction Architectural Issues
  contents:
  - "3.1 Interaction Architectural Issues\n   The topic of the interaction with the\
    \ rest of the internetwork layer\n   is more complex. Nimrod springs in part from\
    \ a design vision which\n   sees the entire internetwork layer, distributed across\
    \ all the hosts\n   and routers of the internetwork, as a single system, albeit\
    \ a\n   distributed system.\n   Approached from that angle, one naturally falls\
    \ into a typical system\n   designer point of view, where you start to think of\
    \ the\n   modularization of the system; chosing the functional boundaries which\n\
    \   divide the system up into functional units, and defining the\n   interactions\
    \ between the functional units.  As we all know, that\n   modularization is the\
    \ key part of the system design process.\n   It's rare that a group of completely\
    \ independent modules form a\n   system; there's usually a fairly strong internal\
    \ interaction. Those\n   interactions have to be thought about and understood\
    \ as part of the\n   modularization process, since it effects the placement of\
    \ the\n   functional boundaries. Poor placement leads to complex interactions,\n\
    \   or desired interactions which cannot be realized.\n   These are all more important\
    \ issues with a system which is expected\n   to have a long lifetime; correct\
    \ placement of the functional\n   boundaries, so as to clearly and simply break\
    \ up the system into\n   truly fundamental units, is a necessity is the system\
    \ is to endure\n   and serve well.\n"
- title: 3.1.1 The Internetwork Layer Service Model
  contents:
  - "3.1.1 The Internetwork Layer Service Model\n   To return to the view of the internetwork\
    \ layer as a system, that\n   system provides certain services to its clients;\
    \ i.e., it\n   instantiates a service model. To begin with, lacking a shared view\
    \ of\n   the service model that the internetwork layer is supposed to provide,\n\
    \   it's reasonable to suppose that it will prove impossible to agree on\n   mechanisms\
    \ at the internetwork level to provide that service.\n   To answer the question\
    \ of what the service model ought to be, one can\n   view the internetwork layer\
    \ itself as a subsystem of an even large\n   system, the entire internetwork itself.\
    \ (That system is quite likely\n   the largest and most complex system we will\
    \ ever build, as it is the\n   largest system we can possibly build; it is the\
    \ system which will\n   inevitably contain almost all other systems.)\n   From\
    \ that point of view, the issue of the service model of the\n   internetwork layer\
    \ becomes a little clearer. The services provided by\n   the internetwork layer\
    \ are no longer purely abstract, but can be\n   thought about as the external\
    \ module interface of the internetwork\n   layer module. If agreement can be reached\
    \ on where to put the\n   functional boundaries of the internetwork layer, and\
    \ on what overall\n   service the internet as a whole should provide, the service\
    \ model of\n   the internetwork layer should be easier to agree on.\n   In general\
    \ terms, it seems that the unreliable packet ought to remain\n   the fundamental\
    \ building block of the internetwork layer. The design\n   principle that says\
    \ that we can take any packet and throw it away\n   with no warning or other action,\
    \ or take any router and turn it off\n   with no warning, and have the system\
    \ still work, seems very powerful.\n   The component design simplicity (since\
    \ routers don't have to stand on\n   their heads to retain a packet which they\
    \ have the only copy of), and\n   overall system robustness, resulting from these\
    \ two assumptions is\n   absolutely critical.\n   In detail, however, particularly\
    \ in areas which are still the subject\n   of research and experimentation (such\
    \ as resource allocation,\n   security, etc.), it seems difficult to provide a\
    \ finished definition\n   of exactly what the service model of the internetwork\
    \ layer ought to\n   be.\n"
- title: 3.1.2 The Subsystems of the Internetwork Layer
  contents:
  - "3.1.2 The Subsystems of the Internetwork Layer\n   In any event, by viewing the\
    \ internetwork layer as a large system,\n   one starts to think about what subsystems\
    \ are needed, and what the\n   interactions among them should look like. Nimrod\
    \ is simply a number\n   of the subsystems of this larger system, the internetwork\
    \ layer. It\n   is *not* intended to be a purely standalone set of subsystems,\
    \ but to\n   work together in close concert with the other subsystems of the\n\
    \   internetwork layer (resource allocation, security, charging, etc.) to\n  \
    \ provide the internetwork layer service model.\n   One reason that Nimrod is\
    \ not simply a monolithic subsystem is that\n   some of the interactions with\
    \ the other subsystems of the\n   internetwork layer, for instance the resource\
    \ allocation subsystem,\n   are much clearer and easier to manage if the routing\
    \ is broken up\n   into several subsystems, with the interactions between them\
    \ open.\n   It is important to realize that Nimrod was initially broken up into\n\
    \   separate subsystems for purely internal reasons. It so happens that,\n   considered\
    \ as a separate problem, the fundamental boundary lines for\n   dividing routing\
    \ up into subsystems are the same boundaries that make\n   interaction with other\
    \ subsystems cleaner; this provides added\n   evidence that these boundaries are\
    \ in fact the right ones.\n   The subsystems which comprise the functionality\
    \ covered by Nimrod are\n   i) routing information distribution (in the case of\
    \ Nimrod, topology\n   map distribution, along with the attributes [policy, QOS,\
    \ etc.] of\n   the topology elements), ii) route selection (strictly speaking,\
    \ not\n   part of the Nimrod spec per se, but functional examples will be\n  \
    \ produced), and iii) user traffic handling.\n   The former can fairly well be\
    \ defined without reference to other\n   subsystems, but the second and third\
    \ are necessarily more involved.\n   For instance, route selection might involve\
    \ finding out which links\n   have the resources available to handle some required\
    \ level of\n   service. For user traffic handling, if a particular application\
    \ needs\n   a resource reservation, getting that resource reservation to the\n\
    \   routers is as much a part of getting the routers ready as making sure\n  \
    \ they have the correct routing information, so here too, routing is\n   tied\
    \ in with other subsystems.\n   In any event, although we can talk about the relationship\
    \ between the\n   Nimrod subsystems, and the other functional subsystems of the\n\
    \   internetwork layer, until the service model of the internetwork layer\n  \
    \ is more clearly visible, along with the functional boundaries within\n   that\
    \ layer, such a discussion is necessarily rather nebulous.\n"
- title: 3.2 State and Flows in the Internetwork Layer
  contents:
  - "3.2 State and Flows in the Internetwork Layer\n   The internetwork layer as whole\
    \ contains a variety of information, of\n   varying lifetimes. This information\
    \ we can refer to as the\n   internetwork layer's \"state\". Some of this state\
    \ is stored in the\n   routers, and some is stored in the packets.\n   In the\
    \ packet, I distinguish between what I call \"forwarding state\",\n   which records\
    \ something about the progress of this individual packet\n   through the network\
    \ (such as the hop count, or the pointer into a\n   source route), and other state,\
    \ which is information about what\n   service the user wants from the network\
    \ (such as the destination of\n   the packet), etc.\n"
- title: 3.2.1 User and Service State
  contents:
  - "3.2.1 User and Service State\n   I call state which reflects the desires and\
    \ service requests of the\n   user \"user state\". This is information which could\
    \ be sent in each\n   packet, or which can be stored in the router and applied\
    \ to multiple\n   packets (depending on which makes the most engineering sense).\
    \ It is\n   still called user state, even when a copy is stored in the routers.\n\
    \   User state can be divided into two classes; \"critical\" (such as\n   destination\
    \ addresses), without which the packets cannot be forwarded\n   at all, and \"\
    non-critical\" (such as a resource allocation class),\n   without which the packets\
    \ can still be forwarded, just not quite in\n   the way the user would most prefer.\n\
    \   There are a range of possible mechanisms for getting this user state\n   to\
    \ the routers; it may be put in every packet, or placed there by a\n   setup.\
    \ In the latter case, you have a whole range of possibilities\n   for how to get\
    \ it back when you lose it, such as placing a copy in\n   every Nth packet.\n\
    \   However, other state is needed which cannot be stored in each packet;\n  \
    \ it's state about the longer-term (i.e., across the life of many\n   packets)\
    \ situation; i.e., state which is inherently associated with a\n   number of packets\
    \ over some time-frame (e.g., a resource allocation).\n   I call this state \"\
    server state\".\n   This apparently changes the \"stateless\" model of routers\
    \ somewhat,\n   but this change is more apparent than real. The routers already\n\
    \   contain state, such as routing table entries; state without which is\n   it\
    \ virtually impossible to handle user traffic. All that is being\n   changed is\
    \ the amount, granularity, and lifetime, of state in the\n   routers.\n   Some\
    \ of this service state may need to be installed in a fairly\n   reliable fashion;\
    \ e.g., if there is service state related to billing,\n   or allocation of resources\
    \ for a critical application, one more or\n   less needs to be guaranteed that\
    \ this service state has been\n   correctly installed.\n   To the extent that\
    \ you have state in the routers (either service\n   state, or user state), you\
    \ have to be able to associate that state\n   with the packets it goes with. The\
    \ fields in the packets that allow\n   you to do this are \"tags\".\n"
- title: 3.2.2 Flows
  contents:
  - "3.2.2 Flows\n   It is useful to step back for a bit here, and think about the\
    \ traffic\n   in the network. Some of it will be from applications with are\n\
    \   basically transactions; i.e., they require only a single packet, or a\n  \
    \ very small number.  (I tend to use the term \"datagram\" to refer to\n   such\
    \ applications, and use the term \"packet\" to describe the unit of\n   transmission\
    \ through the network.) However, other packets are part of\n   longer-lived communications,\
    \ which have been termed \"flows\".\n   A flow, from the user's point of view,\
    \ is a sequence of packets which\n   are associated, usually by being from a single\
    \ application instance.\n   In an internetwork layer which has a more complex\
    \ service model\n   (e.g., supports resource allocation, etc.), the flow would\
    \ have\n   service requirements to pass on to some or all of the subsystems\n\
    \   which provide those services.\n   To the internetworking layer, a flow is\
    \ a sequence of packets that\n   share all the attributes that the internetworking\
    \ layer cares about.\n   This includes, but is not limited to: source/destination,\
    \ path,\n   resource allocation, accounting/authorization,\n   authentication/security,\
    \ etc., etc.\n   There isn't necessarily a one-one mapping from flows to *anything*\n\
    \   else, be it a TCP connection, or an application instance, or\n   whatever.\
    \ A single flow might contain several TCP connections (e.g.,\n   with FTP, where\
    \ you have the control connection, and a number of data\n   connections), or a\
    \ single application might have several flows (e.g.,\n   multi-media conferencing,\
    \ where you'd have one flow for the audio,\n   another for a graphic window, etc.,\
    \ with different resource\n   requirements in terms of bandwidth, delay, etc.,\
    \ for each.)\n   Flows may also be multicast constructs, i.e., multiple sources\
    \ and\n   destinations; they are not inherently unicast. Multicast flows are\n\
    \   more complex than unicast (there is a large pool of state which must\n   be\
    \ made coherent), but the concepts are similar.\n   There's an interesting architectural\
    \ issue here. Let's assume we have\n   all these different internetwork level\
    \ subsystems (routing, resource\n   allocation, security/access-control, accounting),\
    \ etc. Now, we have\n   two choices.\n   First, we could allow each individual\
    \ subsystem which uses the\n   concept of flows to define itself what it thinks\
    \ a \"flow\" is, and\n   define which values in which fields in the packet define\
    \ a given\n   \"flow\" for it. Now, presumably, we have to allow 2 flows for\n\
    \   subsystem X to map onto 1 flow for subsystem Y to map onto 3 flows\n   for\
    \ subsystem Z; i.e., you can mix and match to your heart's content.\n   Second,\
    \ we could define a standard \"flow\" mechanism for the\n   internetwork layer,\
    \ along with a way of identifying the flow in the\n   packet, etc. Then, if you\
    \ have two things which wish to differ in\n   *any* subsystem, you have to have\
    \ a separate flow for each.\n   The former has the advantages that it's a little\
    \ easier to deploy\n   incrementally, since you don't have to agree on a common\
    \ flow\n   mechanism. It may save on replicated state (if I have 3 flows, and\n\
    \   they are the same for subsystem X, and different for Y, I only need\n   one\
    \ set of X state). It also has a lot more flexibility. The latter\n   is simple\
    \ and straightforward, and given the complexity of what is\n   being proposed,\
    \ it seems that any place we can make things simpler,\n   we should.\n   The choice\
    \ is not trivial; it all depends on things like \"what\n   percentage of flows\
    \ will want to share the same state in certain\n   subsystems with other flows\"\
    . I don't know how to quantify those, but\n   as an architect, I prefer simple,\
    \ straightforward things. This system\n   is pretty complex already, and I'm not\
    \ sure the benefits of being\n   able to mix and match are worth the added complexity.\
    \ So, for the\n   moment I'll assume a single, system-wide, definition of flows.\n\
    \   The packets which belong to a flow could be identified by a tag\n   consisting\
    \ of a number of fields (such as addresses, ports, etc.), as\n   opposed to a\
    \ specialized field. However, it may be more\n   straightforward, and foolproof,\
    \ to simply identify the flow a packet\n   belongs to with by means of a specialized\
    \ tag field (the \"flow-id\" )\n   in the internetwork header. Given that you\
    \ can always find situations\n   where the existing fields alone don't do the\
    \ job, and you *still*\n   need a separate field to do the job correctly, it seems\
    \ best to take\n   the simple, direct approach , and say \"the flow a packet belongs\
    \ to\n   is named by a flow-id in the packet header\".\n   The simplicity of globally-unique\
    \ flow-id's (or at least a flow-id\n   which unique along the path of the flow)\
    \ is also desirable; they take\n   more bits in the header, but then you don't\
    \ have to worry about all\n   the mechanism needed to remap locally-unique flow-id's,\
    \ etc., etc.\n   From the perspective of designing something with a long lifetime,\
    \ and\n   which is to be deployed widely, simplicity and directness is the only\n\
    \   way to go. For me, that translates into flows being named solely by\n   globally\
    \ unique flow-id's, rather than some complex semantics on\n   existing fields.\n\
    \   However, the issue of how to recognize which packets belong to flows\n   is\
    \ somewhat orthogonal to the issue of whether the internetwork level\n   recognizes\
    \ flows at all. Should it?\n"
- title: 3.2.3 Flows and State
  contents:
  - "3.2.3 Flows and State\n   To the extent that you have service state in the routers\
    \ you have to\n   be able to associate that state with the packets it goes with.\
    \ This\n   is a fundamental reason for flows. Access to service state is one\n\
    \   reason to explicitly recognize flows at the internetwork layer, but\n   it\
    \ is not the only one.\n   If the user has requirements in a number of areas (e.g.,\
    \ routing and\n   access control), they can theoretically communicate these to\
    \ the\n   routers by placing a copy of all the relevant information in each\n\
    \   packet (in the internetwork header). If many subsystems of the\n   internetwork\
    \ are involved, and the requirements are complex, this\n   could be a lot of bits.\n\
    \   (As a final aside, there's clearly no point in storing in the routers\n  \
    \ any user state about packets which are providing datagram service;\n   the datagram\
    \ service has usually come and gone in the same packet,\n   and this discussion\
    \ is all about state retention.)\n   There are two schools of thought as to how\
    \ to proceed. The first says\n   that for reasons of robustness and simplicity,\
    \ all user state ought\n   to be repeated in each packet. For efficiency reasons,\
    \ the routers\n   may cache such user state, probably along with precomputed data\n\
    \   derived from the user state.  (It makes sense to store such cached\n   user\
    \ state along with any applicable server state, of course.)\n   The second school\
    \ says that if something is going to generate lots of\n   packets, it makes engineering\
    \ sense to give all this information to\n   the routers once, and from then on\
    \ have a tag (the flow-id) in the\n   packet which tells the routers where to\
    \ find that information. It's\n   simply going to be too inefficient to carry\
    \ all the user state around\n   all the time. This is purely an engineering efficiency\
    \ reason, but\n   it's a significant one.\n   There is a slightly deeper argument,\
    \ which says that the routers will\n   inevitably come to contain more user state,\
    \ and it's simply a\n   question of whether that state is installed by an explicit\
    \ mechanism,\n   or whether the routers infer that state from watching the packets\n\
    \   which pass through them.  To the extent that it is inevitable anyway,\n  \
    \ there are obvious benefits to be gained from recognizing that, and an\n   explicit\
    \ design of the installation is more likely to give\n   satisfactory results (as\
    \ opposed to an ad-hoc mechanism).\n   It is worth noting that although the term\
    \ \"flow\" is often used to\n   refer to this state in the routers along the path\
    \ of the flow, it is\n   important to distinguish between i) a flow as a sequence\
    \ of packets\n   (i.e., the definition given in 3.2.2 above), and ii) a flow,\
    \ as the\n   thing which is set up in the routers. They are different, and\n \
    \  although the particular meaning is usually clear from the context,\n   they\
    \ are not the same thing at all.\n   I'm not sure how much use there is to any\
    \ intermediate position, in\n   which one subsystem installs user state in the\
    \ routers, and another\n   carries a copy of its user state in each packet.\n\
    \   (There are other intermediate positions. First, one flow might use a\n   given\
    \ technique for all its subsystems, and another flow might use a\n   different\
    \ technique for its; there is potentially some use to this,\n   although I'm not\
    \ sure the cost in complexity of supporting both\n   mechanisms is worth the benefits.\
    \ Second, one flow might use one\n   mechanism with one router along its path,\
    \ and another for a different\n   router. A number of different reasons exist\
    \ as to why one might do\n   this, including the fact that not all routers may\
    \ support the same\n   mechanisms simultaneously.)\n   It seems to me that to\
    \ have one internetwork layer subsystem (e.g.,\n   resource allocation) carry\
    \ user state in all the packets (perhaps\n   with use of a \"hint\" in the packets\
    \ to find potentially cached copies\n   in the router), and have a second (e.g.,\
    \ routing) use a direct\n   installation, and use a tag in the packets to find\
    \ it, makes little\n   sense. We should do one or the other, based on a consideration\
    \ of the\n   efficiency/robustness tradeoff.\n   Also, if there is a way of installing\
    \ such flow-associated state, it\n   makes sense to have only one, which all subsystems\
    \ use, instead of\n   building a separate one for each flow.\n   It's a little\
    \ difficult to make the choice between installation, and\n   carrying a copy in\
    \ each packet, without more information of exactly\n   how much user state the\
    \ network is likely to have in the future. (For\n   instance, we might wind up\
    \ with 500 byte headers if we include the\n   full source route, resource reservation,\
    \ etc., in every header.)\n   It's also difficult without consideration of the\
    \ actual mechanisms\n   involved. As a general principle, we wish to make recovery\
    \ from a\n   loss of state as local as possible, to limit the number of entities\n\
    \   which have to become involved. (For instance, when a router crashes,\n   traffic\
    \ is rerouted around it without needing to open a new TCP\n   connection.) The\
    \ option of the \"installation\" looks a lot more\n   attractive if it's simple,\
    \ and relatively cheap, to reinstall the\n   user state when a router crashes,\
    \ without otherwise causing a lot of\n   hassle.\n   However, given the likely\
    \ growth in user state, the necessity for\n   service state, the requirement for\
    \ reliable installation, and a\n   number of similar considerations, it seems\
    \ that direct installation\n   of user state, and explicit recognition of flows,\
    \ through a unified\n   definition and tag mechanism in the packets, is the way\
    \ to go, and\n   this is the path that Nimrod has chosen.\n"
- title: 3.3 Specific Interaction Issues
  contents:
  - "3.3 Specific Interaction Issues\n   Here is a very incomplete list of the things\
    \ which Nimrod would like\n   to see from the internetwork layer as a whole:\n\
    \   - A unified definition of flows in the internetwork layer, and a\n     unified\
    \ way of identifying, through a separate flow-id field, which\n     packets belong\
    \ to a given flow.\n   - A unified mechanism (potentially distributed) for installing\
    \ state\n     about flows (including multicast flows) in routers.\n   - A method\
    \ for getting information about whether a given resource\n     allocation request\
    \ has failed along a given path; this might be\n     part of the unified flow\
    \ setup mechanism.\n   - An interface to (potentially distributed) mechanism for\
    \ maintaining\n     the membership in a multi-cast group.\n   - Support for multiple\
    \ interfaces; i.e., multi-homing. Nimrod does\n     this by decoupling transport\
    \ identification (done via EID's) from\n     interface identification (done via\
    \ locators). E.g., a packet with\n     any valid destination locator should be\
    \ accepted by the TCP of an\n     endpoint, if the destination EID is the one\
    \ assigned to that\n     endpoint.\n   - Support for multiple locators (\"addresses\"\
    ) per network interface.\n     This is needed for a number of reasons, among them\
    \ to allow for\n     less painful transitions in the locator abstraction hierarchy\
    \ as\n     the topology changes.\n   - Support for multiple UID's (\"addresses\"\
    ) per endpoint (roughly, per\n     host). This would definitely include both multiple\
    \ multicast SID's,\n     and at least one unicast EID (the need for multiple unicast\
    \ EID's\n     per endpoint is not obvious).\n   - Support for distinction between\
    \ a multicast group as a named\n     entity, and a multicast flow which may not\
    \ reach all the members.\n   - A distributed, replicated, user name translation\
    \ system (DNS?) that\n     maps such user names into (EID, locator0, ... locatorN)\
    \ bindings.\n"
- title: Security Considerations
  contents:
  - "Security Considerations\n   Security issues are discussed in section 2.2.\n"
- title: Author's Address
  contents:
  - "Author's Address\n   J. Noel Chiappa\n   Phone: (804) 898-8183\n   EMail: jnc@lcs.mit.edu\n"
