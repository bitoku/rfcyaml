- title: __initial_text__
  contents:
  - "          MODULARITY AND EFFICIENCY IN PROTOCOL IMPLEMENTATION\n            \
    \                 David D. Clark\n                  MIT Laboratory for Computer\
    \ Science\n               Computer Systems and Communications Group\n        \
    \                       July, 1982\n     1.  Introduction\n     Many  protocol\
    \ implementers have made the unpleasant discovery that\ntheir packages do not\
    \ run quite as fast as they had hoped.    The  blame\nfor  this  widely  observed\
    \  problem has been attributed to a variety of\ncauses, ranging from details in\
    \  the  design  of  the  protocol  to  the\nunderlying  structure  of  the  host\
    \  operating  system.   This RFC will\ndiscuss  some  of  the  commonly  encountered\
    \   reasons   why   protocol\nimplementations seem to run slowly.\n     Experience\
    \  suggests  that  one  of  the  most important factors in\ndetermining the performance\
    \ of an implementation is the manner in  which\nthat   implementation  is  modularized\
    \  and  integrated  into  the  host\noperating system.  For this reason, it is\
    \ useful to discuss the question\nof how an implementation is structured at the\
    \ same time that we consider\nhow it will perform.  In fact, this RFC will argue\
    \  that  modularity  is\none  of  the chief villains in attempting to obtain good\
    \ performance, so\nthat the designer is faced  with  a  delicate  and  inevitable\
    \  tradeoff\nbetween good structure and good performance.  Further, the single\
    \ factor\n     2.  Efficiency Considerations\n     There  are  many aspects to\
    \ efficiency.  One aspect is sending data\nat minimum transmission cost, which\
    \  is  a  critical  aspect  of  common\ncarrier  communications,  if  not  in\
    \ local area network communications.\nAnother aspect is sending data at a high\
    \ rate, which may not be possible\nat all if the net is very slow, but which may\
    \ be the one central  design\nconstraint when taking advantage of a local net\
    \ with high raw bandwidth.\nThe  final  consideration is doing the above with\
    \ minimum expenditure of\ncomputer resources.  This last may be necessary to achieve\
    \  high  speed,\nbut  in  the  case  of  the  slow  net may be important only\
    \ in that the\nresources used up, for example  cpu  cycles,  are  costly  or \
    \ otherwise\nneeded.    It  is  worth  pointing  out that these different goals\
    \ often\nconflict; for example it is often possible to trade off efficient use\
    \ of\nthe computer against efficient use of the network.  Thus, there  may  be\n\
    no such thing as a successful general purpose protocol implementation.\n     The\
    \ simplest measure of performance is throughput, measured in bits\nper second.\
    \  It is worth doing a few simple computations in order to get\na  feeling for\
    \ the magnitude of the problems involved.  Assume that data\nis being sent from\
    \ one machine to another in packets of 576  bytes,  the\nmaximum  generally acceptable\
    \ internet packet size.  Allowing for header\noverhead, this packet size permits\
    \ 4288 bits  in  each  packet.    If  a\nuseful  throughput  of  10,000  bits\
    \  per second is desired, then a data\nbearing packet must leave the sending host\
    \ about every 430 milliseconds,\nthe packet must leave the host every 43 milliseconds,\
    \ and to achieve one\nmegabit  per  second,  which  is not at all unreasonable\
    \ on a high-speed\nlocal net, the packets must be spaced no more than 4.3 milliseconds.\n\
    \     These latter numbers are a slightly more alarming goal for which to\nset\
    \ one's sights.  Many operating systems take a substantial fraction of\na millisecond\
    \ just to service an interrupt.  If the  protocol  has  been\nstructured  as \
    \ a  process,  it  is  necessary  to  go through a process\nscheduling before\
    \ the protocol code can even begin to run.  If any piece\nof a protocol package\
    \ or its data must be fetched from disk,  real  time\ndelays  of  between  30\
    \  to  100  milliseconds  can be expected.  If the\nprotocol must compete for\
    \ cpu resources  with  other  processes  of  the\nsystem,  it  may  be  necessary\
    \  to wait a scheduling quantum before the\nprotocol can run.   Many  systems\
    \  have  a  scheduling  quantum  of  100\nmilliseconds  or  more.   Considering\
    \ these sorts of numbers, it becomes\nimmediately clear that the protocol must\
    \ be fitted  into  the  operating\nsystem  in  a  thorough  and  effective  manner\
    \  if  any like reasonable\nthroughput is to be achieved.\n     There is one obvious\
    \ conclusion immediately suggested by even  this\nsimple  analysis.    Except\
    \  in  very  special  circumstances, when many\npackets are being processed at\
    \ once, the cost of processing a packet  is\ndominated  by  factors, such as cpu\
    \ scheduling, which are independent of\nthe  packet  size.    This  suggests \
    \ two  general   rules   which   any\nimplementation  ought  to  obey.    First,\
    \  send  data in large packets.\nunneeded  packet.    Unneeded packets use up\
    \ just as many resources as a\npacket full of data, but perform no useful function.\
    \  RFC  813,  \"Window\nand  Acknowledgement  Strategy in TCP\", discusses one\
    \ aspect of reducing\nthe number of packets sent per useful data byte.    This\
    \  document  will\nmention other attacks on the same problem.\n     The  above\
    \  analysis  suggests that there are two main parts to the\nproblem of achieving\
    \ good protocol performance.  The  first  has  to  do\nwith  how  the  protocol\
    \  implementation  is  integrated  into  the host\noperating system.  The second\
    \ has to do with how  the  protocol  package\nitself  is  organized  internally.\
    \   This document will consider each of\nthese topics in turn.\n     3.  The Protocol\
    \ vs. the Operating System\n     There are normally three reasonable ways in which\
    \ to add a protocol\nto an operating system.  The protocol  can  be  in  a  process\
    \  that  is\nprovided by the operating system, or it can be part of the kernel\
    \ of the\noperating  system  itself, or it can be put in a separate communications\n\
    processor or front end machine.  This decision is strongly influenced by\ndetails\
    \ of hardware architecture and operating system  design;  each  of\nthese three\
    \ approaches has its own advantages and disadvantages.\n     The  \"process\"\
    \  is the abstraction which most operating systems use\nto provide the execution\
    \ environment for user programs.  A  very  simple\npath  for  implementing  a\
    \  protocol  is  to  obtain  a process from the\nmodifications  to  the  kernel\
    \  are not required, the job can be done by\nsomeone who is not an expert in the\
    \ kernel structure.  Since it is often\nimpossible to find somebody who is experienced\
    \ both in the structure  of\nthe  operating system and the structure of the protocol,\
    \ this path, from\na management point of view, is often extremely appealing. Unfortunately,\n\
    putting a protocol in a process has a number of  disadvantages,  related\nto \
    \ both  structure  and  performance.    First, as was discussed above,\nprocess\
    \ scheduling can be  a  significant  source  of  real-time  delay.\nThere  is\
    \  not  only the actual cost of going through the scheduler, but\nthe problem\
    \ that the operating system may not have  the  right  sort  of\npriority  tools\
    \  to  bring  the  process into execution quickly whenever\nthere is work to be\
    \ done.\n     Structurally, the difficulty with putting a protocol in  a  process\n\
    is  that  the protocol may be providing services, for example support of\ndata\
    \ streams, which are normally obtained by  going  to  special  kernel\nentry \
    \ points.   Depending on the generality of the operating system, it\nmay be impossible\
    \ to take a  program  which  is  accustomed  to  reading\nthrough  a kernel entry\
    \ point, and redirect it so it is reading the data\nfrom a process.  The most\
    \ extreme example of this  problem  occurs  when\nimplementing  server  telnet.\
    \  In almost all systems, the device handler\nfor the locally attached teletypes\
    \ is located  inside  the  kernel,  and\nprograms  read and write from their teletype\
    \ by making kernel calls.  If\nserver telnet is implemented in a process, it is\
    \ then necessary to  take\nthe  data  streams  provided  by server telnet and\
    \ somehow get them back\nmodification  is  necessary  to  achieve  this structure,\
    \ which somewhat\ndefeats the benefit of having removed the protocol from  the\
    \  kernel  in\nthe first place.\n     Clearly, then, there are advantages to putting\
    \ the protocol package\nin  the kernel.  Structurally, it is reasonable to view\
    \ the network as a\ndevice, and device drivers are traditionally contained  in\
    \  the  kernel.\nPresumably,  the  problems  associated  with  process  scheduling\
    \ can be\nsidesteped, at least to a certain extent, by placing the code inside\
    \ the\nkernel.  And it is obviously easier to make the server  telnet  channels\n\
    mimic  the local teletype channels if they are both realized in the same\nlevel\
    \ in the kernel.\n     However, implementation of protocols in the kernel has\
    \ its own  set\nof  pitfalls.    First, network protocols have a characteristic\
    \ which is\nshared by almost no other device:  they require rather  complex  actions\n\
    to  be  performed  as  a  result  of  a  timeout.  The problem with this\nrequirement\
    \ is that the kernel often has no facility by which a  program\ncan  be  brought\
    \ into execution as a result of the timer event.  What is\nreally needed, of course,\
    \ is  a  special  sort  of  process  inside  the\nkernel.    Most  systems  lack\
    \  this  mechanism.  Failing that, the only\nexecution mechanism available is\
    \ to run at interrupt time.\n     There are substantial drawbacks to implementing\
    \ a protocol  to  run\nat interrupt time.  First, the actions performed may be\
    \ somewhat complex\nand  time  consuming,  compared  to  the maximum amount of\
    \ time that the\nbad  when running as a result of a clock interrupt, which can\
    \ imply that\nthe clock interrupt is masked.  Second, the environment provided\
    \  by  an\ninterrupt  handler  is  usually  extremely  primitive  compared  to\
    \  the\nenvironment of a process.    There  are  usually  a  variety  of  system\n\
    facilities  which are unavailable while running in an interrupt handler.\nThe\
    \ most important of these is the ability to suspend execution  pending\nthe  arrival\
    \  of some event or message.  It is a cardinal rule of almost\nevery known operating\
    \ system that one  must  not  invoke  the  scheduler\nwhile  running  in  an \
    \ interrupt  handler.  Thus, the programmer who is\nforced to implement all or\
    \ part of his protocol package as an  interrupt\nhandler  must  be  the  best\
    \  sort  of  expert  in  the operating system\ninvolved, and must be prepared\
    \  for  development  sessions  filled  with\nobscure  bugs  which  crash not just\
    \ the protocol package but the entire\noperating system.\n     A final problem\
    \ with processing  at  interrupt  time  is  that  the\nsystem  scheduler has no\
    \ control over the percentage of system time used\nby the protocol handler.  If\
    \ a large number of packets  arrive,  from  a\nforeign  host that is either malfunctioning\
    \ or fast, all of the time may\nbe spent in the interrupt handler, effectively\
    \ killing the system.\n     There are other problems associated with putting protocols\
    \ into  an\noperating system kernel.  The simplest problem often encountered is\
    \ that\nthe  kernel  address space is simply too small to hold the piece of code\n\
    in question.  This is a rather artificial sort of problem, but it  is  a\nfor\
    \  every  byte  of new feature put in one must find some other byte of\nold feature\
    \ to throw out.  It is hopeless to  expect  an  effective  and\ngeneral  implementation\
    \  under this kind of constraint.  Another problem\nis that the protocol package,\
    \ once it  is  thoroughly  entwined  in  the\noperating  system, may need to be\
    \ redone every time the operating system\nchanges.  If the protocol and the operating\
    \ system are not maintained by\nthe same group,  this  makes  maintenance  of\
    \  the  protocol  package  a\nperpetual headache.\n     The  third  option  for\
    \  protocol  implementation  is  to  take the\nprotocol package and move it outside\
    \  the  machine  entirely,  on  to  a\nseparate  processor  dedicated  to this\
    \ kind of task.  Such a machine is\noften described as a communications processor\
    \ or a front-end  processor.\nThere  are  several  advantages  to this approach.\
    \  First, the operating\nsystem on the communications processor can  be  tailored\
    \  for  precisely\nthis  kind  of  task.  This makes the job of implementation\
    \ much easier.\nSecond, one does not need to redo the task for every  machine\
    \  to  which\nthe  protocol  is  to  be  added.   It may be possible to reuse\
    \ the same\nfront-end machine on different host computers.  Since the task need\
    \  not\nbe  done as many times, one might hope that more attention could be paid\n\
    to doing it right.  Given a careful  implementation  in  an  environment\nwhich\
    \  is  optimized for this kind of task, the resulting package should\nturn out\
    \ to be very efficient.  Unfortunately, there are  also  problems\nwith this approach.\
    \  There is, of course, a financial problem associated\nwith  buying  an  additional\
    \  computer.    In  many cases, this is not a\nfundamentally, the communications\
    \ processor approach does not completely\nsidestep  any  of  the  problems  raised\
    \  above.  The reason is that the\ncommunications processor, since  it  is  a\
    \  separate  machine,  must  be\nattached  to  the mainframe by some mechanism.\
    \  Whatever that mechanism,\ncode is required in the mainframe to deal with it.\
    \   It  can  be  argued\nthat  the  program  to deal with the communications processor\
    \ is simpler\nthan the program to implement the entire protocol package.  Even\
    \ if that\nis so,  the  communications  processor  interface  package  is  still\
    \  a\nprotocol in nature, with all of the same structural problems.  Thus, all\n\
    of  the  issues  raised above must still be faced.  In addition to those\nproblems,\
    \ there are some other, more subtle problems associated with  an\noutboard implementation\
    \ of a protocol.  We will return to these problems\nlater.\n     There  is  a\
    \  way  of  attaching  a  communications  processor to a\nmainframe host which\
    \  sidesteps  all  of  the  mainframe  implementation\nproblems, which is to use\
    \ some preexisting interface on the host machine\nas  the  port  by  which  a\
    \  communications processor is attached.  This\nstrategy is often used as a last\
    \ stage of desperation when the  software\non  the host computer is so intractable\
    \ that it cannot be changed in any\nway.  Unfortunately, it is almost inevitably\
    \ the case that  all  of  the\navailable  interfaces  are  totally  unsuitable\
    \ for this purpose, so the\nresult is unsatisfactory at best.  The most common\
    \  way  in  which  this\nform  of attachment occurs is when a network connection\
    \ is being used to\nmimic local teletypes.  In this case, the  front-end  processor\
    \  can  be\nplugged  into teletype ports on the mainframe computer.  (Because\
    \ of the\nappearance  of  the  physical  configuration  which  results  from \
    \ this\narrangement,  Michael  Padlipsky  has  described  this  as  the \"milking\n\
    machine\" approach to computer networking.)   This  strategy  solves  the\nimmediate\
    \  problem  of  providing  remote  access  to  a host, but it is\nextremely inflexible.\
    \  The channels  being  provided  to  the  host  are\nrestricted  by  the host\
    \ software to one purpose only, remote login.  It\nis impossible to use them for\
    \ any other purpose, such as  file  transfer\nor  sending mail, so the host is\
    \ integrated into the network environment\nin an extremely limited and inflexible\
    \ manner.  If this is the best that\ncan be done, then it  should  be  tolerated.\
    \    Otherwise,  implementors\nshould be strongly encouraged to take a more flexible\
    \ approach.\n     4.  Protocol Layering\n     The  previous  discussion suggested\
    \ that there was a decision to be\nmade as to where a protocol ought to  be  implemented.\
    \    In  fact,  the\ndecision  is  much  more  complicated  than that, for the\
    \ goal is not to\nimplement a single protocol, but to implement a whole family\
    \ of protocol\nlayers, starting with a device driver or local  network  driver\
    \  at  the\nbottom,  then  IP  and  TCP,  and  eventually  reaching  the application\n\
    specific protocol, such as Telnet, FTP and SMTP on the  top.    Clearly,\nthe\
    \ bottommost of these layers is somewhere within the kernel, since the\nphysical\
    \  device  driver for the net is almost inevitably located there.\nEqually clearly,\
    \ the top layers of this package, which provide the  user\nwhether  the  protocol\
    \ family shall be inside or outside the kernel, but\nhow it shall be sliced in\
    \ two between that part  inside  and  that  part\noutside.\n     Since  protocols\
    \  come  nicely layered, an obvious proposal is that\none of the layer interfaces\
    \ should be the point at which the inside  and\noutside components are sliced\
    \ apart.  Most systems have been implemented\nin  this  way,  and  many have been\
    \ made to work quite effectively.  One\nobvious place to slice is at the upper\
    \ interface  of  TCP.    Since  TCP\nprovides  a  bidirectional byte stream, which\
    \ is somewhat similar to the\nI/O facility provided by most operating systems,\
    \ it is possible to  make\nthe  interface  to  TCP  almost  mimic  the  interface\
    \ to other existing\ndevices.  Except in the matter of opening a connection, and\
    \ dealing with\npeculiar failures, the software using TCP need not know  that\
    \  it  is  a\nnetwork connection, rather than a local I/O stream that is providing\
    \ the\ncommunications  function.  This approach does put TCP inside the kernel,\n\
    which raises all the problems addressed  above.    It  also  raises  the\nproblem\
    \ that the interface to the IP layer can, if the programmer is not\ncareful, \
    \ become  excessively  buried  inside  the  kernel.   It must be\nremembered that\
    \ things other than TCP are expected to run on top of  IP.\nThe  IP interface\
    \ must be made accessible, even if TCP sits on top of it\ninside the kernel.\n\
    \     Another obvious place to slice is above Telnet.  The  advantage  of\nslicing\
    \  above  Telnet  is  that  it solves the problem of having remote\nbeen  included\
    \  there  is  getting  remarkably  large.    In  some early\nimplementations,\
    \ the size of the  network  package,  when  one  includes\nprotocols  at  the\
    \  level  of Telnet, rivals the size of the rest of the\nsupervisor.  This leads\
    \ to vague feelings that all is not right.\n     Any attempt to slice through\
    \ a lower layer  boundary,  for  example\nbetween  internet  and  TCP,  reveals\
    \  one fundamental problem.  The TCP\nlayer, as well as the IP layer, performs\
    \ a  demultiplexing  function  on\nincoming  datagrams.   Until the TCP header\
    \ has been examined, it is not\npossible to know for which  user  the  packet\
    \  is  ultimately  destined.\nTherefore,  if  TCP,  as  a  whole,  is  moved outside\
    \ the kernel, it is\nnecessary to create one separate process called the TCP \
    \ process,  which\nperforms  the TCP multiplexing function, and probably all of\
    \ the rest of\nTCP processing as well.  This means that incoming data  destined\
    \  for  a\nuser  process  involves  not  just a scheduling of the user process,\
    \ but\nscheduling the TCP process first.\n     This suggests an  alternative \
    \ structuring  strategy  which  slices\nthrough  the  protocols,  not  along \
    \ an established layer boundary, but\nalong a functional boundary having to do\
    \ with demultiplexing.   In  this\napproach, certain parts of IP and certain parts\
    \ of TCP are placed in the\nkernel.    The amount of code placed there is sufficient\
    \ so that when an\nincoming datagram arrives, it is possible to know for which\
    \ process that\ndatagram is ultimately destined.  The datagram is then  routed\
    \  directly\nto  the  final  process,  where  additional  IP  and  TCP  processing\
    \ is\nuser.    This  structure  has  the  additional advantage of reducing the\n\
    amount of code required in the  kernel,  so  that  it  is  suitable  for\nsystems\
    \ where kernel space is at a premium.  The RFC 814, titled \"Names,\nAddresses,\
    \  Ports, and Routes,\" discusses this rather orthogonal slicing\nstrategy in\
    \ more detail.\n     A related discussion of protocol layering and multiplexing\
    \  can  be\nfound in Cohen and Postel [1].\n     5.  Breaking Down the Barriers\n\
    \     In  fact, the implementor should be sensitive to the possibility of\neven\
    \ more  peculiar  slicing  strategies  in  dividing  up  the  various\nprotocol\
    \  layers  between the kernel and the one or more user processes.\nThe result\
    \ of the strategy proposed above was that part  of  TCP  should\nexecute  in \
    \ the process of the user.  In other words, instead of having\none TCP process\
    \ for the system, there is one TCP process per connection.\nGiven this architecture,\
    \ it is not longer necessary to imagine that  all\nof  the  TCPs  are  identical.\
    \    One  TCP  could  be optimized for high\nthroughput applications, such as\
    \ file transfer.  Another  TCP  could  be\noptimized  for small low delay applications\
    \ such as Telnet.  In fact, it\nwould be possible to produce a TCP which was \
    \ somewhat  integrated  with\nthe  Telnet  or  FTP  on  top  of  it.  Such an\
    \ integration is extremely\nimportant,  for  it  can  lead  to  a  kind  of  efficiency\
    \  which  more\ntraditional  structures are incapable of producing.  Earlier,\
    \ this paper\npointed out that one of the important rules to achieving efficiency\
    \  was\ngoal,  because  independent  layers  have  independent  ideas about when\n\
    packets should be sent, and unless these layers can somehow  be  brought\ninto\
    \  cooperation,  additional  packets  will flow.  The best example of\nthis is\
    \ the operation of server telnet in a character at a  time  remote\necho  mode\
    \  on top of TCP.  When a packet containing a character arrives\nat a server host,\
    \ each layer has a different response  to  that  packet.\nTCP  has  an obligation\
    \ to acknowledge the packet.  Either server telnet\nor the application layer above\
    \ has an obligation to echo  the  character\nreceived  in the packet.  If the\
    \ character is a Telnet control sequence,\nthen Telnet has additional actions\
    \ which it must perform in response  to\nthe  packet.    The  result  of  this,\
    \  in most implementations, is that\nseveral packets are sent back in response\
    \ to the  one  arriving  packet.\nCombining  all of these return messages into\
    \ one packet is important for\nseveral reasons.  First, of course, it reduces\
    \  the  number  of  packets\nbeing sent over the net, which directly reduces the\
    \ charges incurred for\nmany common carrier tariff structures.  Second, it reduces\
    \ the number of\nscheduling  actions  which  will  occur inside both hosts, which,\
    \ as was\ndiscussed above, is extremely important in improving throughput.\n \
    \    The way to achieve this goal of packet sharing is to break down the\nbarrier\
    \ between the layers of the protocols, in a  very  restrained  and\ncareful  manner,\
    \ so that a limited amount of information can leak across\nthe barrier to enable\
    \ one layer to optimize its behavior with respect to\nthe desires of the layers\
    \ above and below it.   For  example,  it  would\nrepresent  an  improvement \
    \ if TCP, when it received a packet, could ask\nupper  layer  would  have  any\
    \  outgoing  data to send.  Dallying before\nsending  the  acknowledgement  produces\
    \  precisely  the  right  sort  of\noptimization  if  the client of TCP is server\
    \ Telnet.  However, dallying\nbefore sending an acknowledgement is absolutely\
    \ unacceptable if  TCP  is\nbeing used for file transfer, for in file transfer\
    \ there is almost never\ndata  flowing  in  the  reverse  direction, and the delay\
    \ in sending the\nacknowledgement probably translates directly into a delay  in\
    \  obtaining\nthe  next  packets.  Thus, TCP must know a little about the layers\
    \ above\nit to adjust its performance as needed.\n     It would be possible to\
    \ imagine a general  purpose  TCP  which  was\nequipped  with  all  sorts of special\
    \ mechanisms by which it would query\nthe layer above and modify its behavior\
    \ accordingly.  In the  structures\nsuggested above, in which there is not one\
    \ but several TCPs, the TCP can\nsimply  be modified so that it produces the correct\
    \ behavior as a matter\nof course.  This structure has  the  disadvantage  that\
    \  there  will  be\nseveral  implementations  of TCP existing on a single machine,\
    \ which can\nmean more maintenance headaches if a problem is found where TCP needs\
    \ to\nbe changed.  However, it is probably the case that each of the TCPs will\n\
    be substantially simpler  than  the  general  purpose  TCP  which  would\notherwise\
    \  have  been  built.    There  are  some  experimental projects\ncurrently under\
    \ way which suggest that this approach may make  designing\nof  a  TCP, or almost\
    \ any other layer, substantially easier, so that the\ntotal effort involved in\
    \ bringing up a complete package is actually less\n     The  general conclusion\
    \ to be drawn from this sort of consideration\nis that a layer boundary has both\
    \ a benefit and a penalty.    A  visible\nlayer  boundary,  with  a  well  specified\
    \ interface, provides a form of\nisolation between two layers which allows one\
    \ to  be  changed  with  the\nconfidence  that  the  other  one  will  not  stop\
    \  working as a result.\nHowever, a firm layer boundary almost inevitably  leads\
    \  to  inefficient\noperation.    This  can  easily be seen by analogy with other\
    \ aspects of\noperating systems.  Consider, for example,  file  systems.    A\
    \  typical\noperating  system  provides  a file system, which is a highly abstracted\n\
    representation of a disk.   The  interface  is  highly  formalized,  and\npresumed\
    \  to  be highly stable.  This makes it very easy for naive users\nto have access\
    \ to  disks  without  having  to  write  a  great  deal  of\nsoftware.  The existence\
    \ of a file system is clearly beneficial.  On the\nother  hand,  it is clear that\
    \ the restricted interface to a file system\nalmost inevitably leads to inefficiency.\
    \  If the interface is  organized\nas  a  sequential read and write of bytes,\
    \ then there will be people who\nwish to do high throughput transfers who cannot\
    \ achieve their goal.   If\nthe  interface  is  a  virtual  memory  interface,\
    \ then other users will\nregret the necessity of building a byte stream interface\
    \ on top  of  the\nmemory  mapped file.  The most objectionable inefficiency results\
    \ when a\nhighly sophisticated package, such as a data  base  management  package,\n\
    must  be  built  on  top  of  an  existing  operating  system.    Almost\ninevitably,\
    \ the implementors of the database system  attempt  to  reject\nthe  file  system\
    \  and  obtain  direct  access  to the disks.  They have\nsacrificed modularity\
    \ for efficiency.\nThe concept of a protocol is still unknown and frightening\
    \ to most naive\nprogrammers.   The idea that they might have to implement a protocol,\
    \ or\neven part of a protocol, as part  of  some  application  package,  is  a\n\
    dreadful thought.  And thus there is great pressure to hide the function\nof \
    \ the  net behind a very hard barrier.  On the other hand, the kind of\ninefficiency\
    \ which results from this is a particularly undesirable  sort\nof  inefficiency,\
    \ for it shows up, among other things, in increasing the\ncost of the communications\
    \ resource used up to achieve  the  application\ngoal.   In cases where one must\
    \ pay for one's communications costs, they\nusually turn out to be the dominant\
    \ cost within the system.  Thus, doing\nan excessively good job of packaging up\
    \ the protocols in  an  inflexible\nmanner  has  a  direct  impact  on  increasing\
    \  the cost of the critical\nresource within the system.  This is a dilemma which\
    \ will probably  only\nbe solved when programmers become somewhat less alarmed\
    \ about protocols,\nso that they are willing to weave a certain amount of protocol\
    \ structure\ninto their application program, much as application programs today\
    \ weave\nparts  of  database  management  systems  into  the  structure  of their\n\
    application program.\n     An extreme example of putting the protocol package\
    \  behind  a  firm\nlayer boundary occurs when the protocol package is relegated\
    \ to a front-\nend processor.  In this case the interface to the protocol is some\
    \ other\nprotocol.    It  is  difficult to imagine how to build close cooperation\n\
    between layers when they are that far separated.  Realistically, one  of\nthe\
    \ prices which must be associated with an implementation so physically\nthe  mainframe\
    \  architecture, with interprocessor co-ordination signals,\nshared memory, and\
    \ similar features.  Such a physical  modularity  might\nwork  very  well,  but\
    \  there  is little documented experience with this\nclosely coupled architecture\
    \ for protocol support.\n     6.  Efficiency of Protocol Processing\n     To this\
    \ point, this document has considered how a protocol  package\nshould  be  broken\
    \  into  modules,  and  how  those  modules  should  be\ndistributed between free\
    \ standing machines, the operating system kernel,\nand one or more user processes.\
    \  It is now time to  consider  the  other\nhalf  of the efficiency question,\
    \ which is what can be done to speed the\nexecution of those programs that actually\
    \ implement the protocols.    We\nwill make some specific observations about TCP\
    \ and IP, and then conclude\nwith a few generalities.\n     IP  is a simple protocol,\
    \ especially with respect to the processing\nof  normal  packets,  so  it  should\
    \  be  easy  to  get  it  to  perform\nefficiently.    The only area of any complexity\
    \ related to actual packet\nprocessing has to do with fragmentation and reassembly.\
    \  The  reader  is\nreferred  to  RFC  815,  titled \"IP Datagram Reassembly Algorithms\"\
    , for\nspecific consideration of this point.\n     Most costs in the IP layer\
    \ come from table look  up  functions,  as\nopposed to packet processing functions.\
    \  An outgoing packet requires two\ntranslation  functions  to  be  performed.\
    \  The internet address must be\nnetwork).    It  is easy to build a simple implementation\
    \ of these table\nlook up functions that in fact performs very  poorly.    The\
    \  programmer\nshould  keep  in  mind  that  there may be as many as a thousand\
    \ network\nnumbers in a typical configuration.   Linear  searching  of  a  thousand\n\
    entry table on every packet is extremely unsuitable.  In fact, it may be\nworth\
    \  asking  TCP  to  cache  a  hint for each connection, which can be\nhanded down\
    \ to IP each time a packet  is  sent,  to  try  to  avoid  the\noverhead of a\
    \ table look up.\n     TCP   is   a   more   complex  protocol,  and  presents\
    \  many  more\nopportunities for getting things wrong.  There  is  one  area \
    \ which  is\ngenerally  accepted  as  causing  noticeable and substantial overhead\
    \ as\npart of TCP processing.  This is computation of the checksum.  It  would\n\
    be  nice  if this cost could be avoided somehow, but the idea of an end-\nto-end\
    \ checksum is absolutely central to the functioning  of  TCP.    No\nhost  implementor\
    \  should think of omitting the validation of a checksum\non incoming data.\n\
    \     Various clever tricks have been used to try to minimize the cost of\ncomputing\
    \ the checksum.  If it is possible to add additional  microcoded\ninstructions\
    \  to the machine, a checksum instruction is the most obvious\ncandidate.  Since\
    \ computing the checksum involves picking up every  byte\nof the segment and examining\
    \ it, it is possible to combine the operation\nof computing the checksum with\
    \ the operation of copying the segment from\none  location  to  another.   Since\
    \ a number of data copies are probably\nthe  modularity  of  the  program.  Finally,\
    \ computation of the checksum\nseems to be one place where careful attention \
    \ to  the  details  of  the\nalgorithm  used  can  make a drastic difference in\
    \ the throughput of the\nprogram.  The Multics system provides one of the best\
    \  case  studies  of\nthis,  since  Multics  is  about  as  poorly  organized\
    \  to perform this\nfunction as any machine implementing TCP.   Multics  is  a\
    \  36-bit  word\nmachine,  with  four 9-bit bytes per word.  The eight-bit bytes\
    \ of a TCP\nsegment are laid down packed in memory, ignoring word boundaries.\
    \   This\nmeans  that  when it is necessary to pick up the data as a set of 16-bit\n\
    units for the purpose of adding  them  to  compute  checksums,  horrible\nmasking\
    \  and  shifting  is  required  for  each  16-bit value.  An early\nversion of\
    \ a program using this  strategy  required  6  milliseconds  to\nchecksum  a \
    \ 576-byte  segment.    Obviously,  at  this  point, checksum\ncomputation was\
    \ becoming the central bottleneck to throughput.   A  more\ncareful  recoding\
    \ of this algorithm reduced the checksum processing time\nto less than one millisecond.\
    \  The strategy used  was  extremely  dirty.\nIt  involved adding up carefully\
    \ selected words of the area in which the\ndata lay, knowing that for those particular\
    \  words,  the  16-bit  values\nwere  properly  aligned  inside  the words.  Only\
    \ after the addition had\nbeen done were the various sums shifted, and finally\
    \  added  to  produce\nthe  eventual  checksum.  This kind of highly specialized\
    \ programming is\nprobably not acceptable if used everywhere within an  operating\
    \  system.\nIt is clearly appropriate for one highly localized function which\
    \ can be\nclearly identified as an extreme performance bottleneck.\noccur in each\
    \ incoming packet.  One paper, by Bunch and Day [2], asserts\nthat  the  overhead\
    \ of packet header processing is actually an important\nlimiting  factor  in \
    \ throughput  computation.    Not  all   measurement\nexperiments  have  tended\
    \ to support this result.  To whatever extent it\nis true, however, there is an\
    \ obvious  strategy  which  the  implementor\nought  to  use in designing his\
    \ program.  He should build his program to\noptimize the expected case.  It is\
    \ easy, especially when first designing\na program, to pay equal attention to\
    \ all of  the  possible  outcomes  of\nevery test.  In practice, however, few\
    \ of these will ever happen.  A TCP\nshould  be  built  on the assumption that\
    \ the next packet to arrive will\nhave absolutely nothing special about it,  and\
    \  will  be  the  next  one\nexpected  in  the  sequence  space.   One or two\
    \ tests are sufficient to\ndetermine that the expected set of control flags are\
    \ on.  (The ACK  flag\nshould be on; the Push flag may or may not be on.  No other\
    \ flags should\nbe on.)  One test is sufficient to determine that the sequence\
    \ number of\nthe  incoming  packet  is  one  greater  than  the  last sequence\
    \ number\nreceived.  In almost every case, that will be the actual result.  Again,\n\
    using the Multics system as an example, failure to optimize the case  of\nreceiving\
    \  the  expected  sequence number had a detectable effect on the\nperformance\
    \ of the system.  The particular problem arose when  a  number\nof  packets  arrived\
    \  at  once.    TCP attempted to process all of these\npackets before awaking\
    \ the user.  As a result,  by  the  time  the  last\npacket  arrived,  there was\
    \ a threaded list of packets which had several\nitems on it.  When a new packet\
    \ arrived, the list was searched  to  find\nnumber,  because  one is expecting\
    \ to receive a packet which comes after\nthose already received.  By mistake,\
    \ the list was searched from front to\nback, starting with the packets with the\
    \ lowest sequence  number.    The\namount of time spent searching this list backwards\
    \ was easily detectable\nin the metering measurements.\n     Other data structures\
    \ can be organized to optimize the action which\nis  normally  taken  on  them.\
    \  For example, the retransmission queue is\nvery seldom actually used  for  retransmission,\
    \  so  it  should  not  be\norganized  to  optimize that action.  In fact, it\
    \ should be organized to\noptimized the discarding of things  from  it  when \
    \ the  acknowledgement\narrives.    In many cases, the easiest way to do this\
    \ is not to save the\npacket  at  all,  but  to  reconstruct  it  only  if  it\
    \  needs  to   be\nretransmitted,  starting  from the data as it was originally\
    \ buffered by\nthe user.\n     There is another generality, at least as  important\
    \  as  optimizing\nthe  common  case,  which  is  to avoid copying data any more\
    \ times than\nnecessary.  One more result from the Multics TCP may prove  enlightening\n\
    here.    Multics takes between two and three milliseconds within the TCP\nlayer\
    \ to process an incoming packet, depending on its size.  For a  576-\nbyte packet,\
    \ the three milliseconds is used up approximately as follows.\nOne   millisecond\
    \   is   used  computing  the  checksum.    Six  hundred\nmicroseconds is spent\
    \ copying the data.  (The data is copied  twice,  at\n.3  milliseconds  a copy.)\
    \  One of those copy operations could correctly\nHowever,  the  copy also performs\
    \ another necessary transfer at the same\ntime.  Header processing and packet\
    \ resequencing takes .7  milliseconds.\nThe  rest  of  the  time  is  used  in\
    \ miscellaneous processing, such as\nremoving packets from the retransmission\
    \ queue which are acknowledged by\nthis packet.  Data copying is the second most\
    \ expensive single operation\nafter data checksuming.   Some  implementations,\
    \  often  because  of  an\nexcessively  layered  modularity, end up copying the\
    \ data around a great\ndeal.  Other implementations end up copying the data because\
    \ there is no\nshared memory between processes, and the data must be moved from\
    \ process\nto process via a kernel operation.  Unless the amount of  this  activity\n\
    is  kept  strictly  under  control,  it  will  quickly  become the major\nperformance\
    \ bottleneck.\n     7.  Conclusions\n     This document has addressed two aspects\
    \  of  obtaining  performance\nfrom a protocol implementation, the way in which\
    \ the protocol is layered\nand  integrated  into  the  operating  system,  and\
    \ the way in which the\ndetailed handling of the packet is optimized.  It would\
    \ be nice  if  one\nor  the  other  of these costs would completely dominate,\
    \ so that all of\none's attention could be concentrated there.  Regrettably, this\
    \  is  not\nso.    Depending  on  the particular sort of traffic one is getting,\
    \ for\nexample, whether Telnet one-byte packets or file transfer  maximum  size\n\
    packets  at  maximum  speed, one can expect to see one or the other cost\nbeing\
    \ the major bottleneck to throughput.  Most  implementors  who  have\nequally\
    \  to  all parts of their program.  With the possible exception of\nchecksum \
    \ processing,  very  few  people  have  ever  found  that  their\nperformance\
    \  problems  were  due  to a single, horrible bottleneck which\nthey could fix\
    \ by a single stroke of inventive programming.  Rather, the\nperformance was something\
    \ which was improved by  painstaking  tuning  of\nthe entire program.\n     Most\
    \  discussions  of protocols begin by introducing the concept of\nlayering, which\
    \ tends  to  suggest  that  layering  is  a  fundamentally\nwonderful  idea  which\
    \  should  be  a  part  of  every  consideration of\nprotocols.  In fact, layering\
    \ is a mixed blessing.    Clearly,  a  layer\ninterface  is  necessary  whenever\
    \  more than one client of a particular\nlayer is to be allowed to use  that \
    \ same  layer.    But  an  interface,\nprecisely  because  it  is fixed, inevitably\
    \ leads to a lack of complete\nunderstanding as to what one layer wishes to obtain\
    \ from another.   This\nhas to lead to inefficiency.  Furthermore, layering is\
    \ a potential snare\nin  that  one  is  tempted  to think that a layer boundary,\
    \ which was an\nartifact of the specification procedure, is in fact the proper\
    \  boundary\nto  use in modularizing the implementation.  Again, in certain cases,\
    \ an\narchitected layer must correspond to an implemented layer, precisely  so\n\
    that  several  clients  can  have  access  to that layer in a reasonably\nstraightforward\
    \ manner.  In other cases, cunning  rearrangement  of  the\nimplemented  module\
    \  boundaries to match with various functions, such as\nthe demultiplexing of\
    \ incoming packets, or the sending  of  asynchronous\noutgoing  packets,  can\
    \  lead  to  unexpected  performance  improvements\nprogram.   Since performance\
    \ is influenced, not just by the fine detail,\nbut by the gross structure, it\
    \ is sometimes the case that  in  order  to\nobtain  a  substantial  performance\
    \  improvement,  it  is  necessary  to\ncompletely redo the program from  the\
    \  bottom  up.    This  is  a  great\ndisappointment   to  programmers,  especially\
    \  those  doing  a  protocol\nimplementation for  the  first  time.    Programmers\
    \  who  are  somewhat\ninexperienced  and  unfamiliar with protocols are sufficiently\
    \ concerned\nwith getting their program logically correct that they do not  have\
    \  the\ncapacity  to  think  at  the  same  time  about  the  performance of the\n\
    structure they are building.  Only after they have achieved a  logically\ncorrect\
    \  program  do they discover that they have done so in a way which\nhas precluded\
    \ real performance.  Clearly, it is more difficult to design\na program thinking\
    \ from the start about  both  logical  correctness  and\nperformance.  With time,\
    \ as implementors as a group learn more about the\nappropriate  structures  to\
    \  use  for  building  protocols,  it  will be\npossible  to  proceed  with  an\
    \  implementation  project   having   more\nconfidence  that  the structure is\
    \ rational, that the program will work,\nand that the program will work well.\
    \    Those  of  us  now  implementing\nprotocols  have the privilege of being\
    \ on the forefront of this learning\nCitations\n     [1]  Cohen  and  Postel,\
    \  \"On  Protocol  Multiplexing\",  Sixth Data\nCommunications Symposium, ACM/IEEE,\
    \ November 1979.\n     [2] Bunch and Day, \"Control Structure Overhead in TCP\"\
    , Trends  and\n"
