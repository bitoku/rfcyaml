- title: __initial_text__
  contents:
  - '                  Framework for TCP Throughput Testing

    '
- title: Abstract
  contents:
  - "Abstract\n   This framework describes a practical methodology for measuring end-\n\
    \   to-end TCP Throughput in a managed IP network.  The goal is to\n   provide\
    \ a better indication in regard to user experience.  In this\n   framework, TCP\
    \ and IP parameters are specified to optimize TCP\n   Throughput.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc6349.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2011 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Requirements Language ......................................4\n  \
    \    1.2. Terminology ................................................5\n    \
    \  1.3. TCP Equilibrium ............................................6\n   2. Scope\
    \ and Goals .................................................7\n   3. Methodology\
    \ .....................................................8\n      3.1. Path MTU\
    \ ..................................................10\n      3.2. Round-Trip\
    \ Time (RTT) and Bottleneck Bandwidth (BB) .......11\n           3.2.1. Measuring\
    \ RTT ......................................11\n           3.2.2. Measuring BB\
    \ .......................................12\n      3.3. Measuring TCP Throughput\
    \ ..................................12\n           3.3.1. Minimum TCP RWND ...................................13\n\
    \   4. TCP Metrics ....................................................16\n  \
    \    4.1. Transfer Time Ratio .......................................16\n    \
    \       4.1.1. Maximum Achievable TCP Throughput Calculation ......17\n      \
    \     4.1.2. TCP Transfer Time and Transfer Time Ratio\n                  Calculation\
    \ ........................................19\n      4.2. TCP Efficiency ............................................20\n\
    \           4.2.1. TCP Efficiency Percentage Calculation ..............20\n  \
    \    4.3. Buffer Delay ..............................................20\n    \
    \       4.3.1. Buffer Delay Percentage Calculation ................21\n   5. Conducting\
    \ TCP Throughput Tests ................................21\n      5.1. Single versus\
    \ Multiple TCP Connections ....................21\n      5.2. Results Interpretation\
    \ ....................................22\n   6. Security Considerations ........................................25\n\
    \      6.1. Denial-of-Service Attacks .................................25\n  \
    \    6.2. User Data Confidentiality .................................25\n    \
    \  6.3. Interference with Metrics .................................25\n   7. Acknowledgments\
    \ ................................................26\n   8. Normative References\
    \ ...........................................26\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   In the network industry, the SLA (Service Level Agreement)\
    \ provided\n   to business-class customers is generally based upon Layer 2/3\n\
    \   criteria such as bandwidth, latency, packet loss, and delay\n   variations\
    \ (jitter).  Network providers are coming to the realization\n   that Layer 2/3\
    \ testing is not enough to adequately ensure end-users'\n   satisfaction.  In\
    \ addition to Layer 2/3 testing, this framework\n   recommends a methodology for\
    \ measuring TCP Throughput in order to\n   provide meaningful results with respect\
    \ to user experience.\n   Additionally, business-class customers seek to conduct\
    \ repeatable TCP\n   Throughput tests between locations.  Since these organizations\
    \ rely\n   on the networks of the providers, a common test methodology with\n\
    \   predefined metrics would benefit both parties.\n   Note that the primary focus\
    \ of this methodology is managed business-\n   class IP networks, e.g., those\
    \ Ethernet-terminated services for which\n   organizations are provided an SLA\
    \ from the network provider.  Because\n   of the SLA, the expectation is that\
    \ the TCP Throughput should achieve\n   the guaranteed bandwidth.  End-users with\
    \ \"best effort\" access could\n   use this methodology, but this framework and\
    \ its metrics are intended\n   to be used in a predictable managed IP network.\
    \  No end-to-end\n   performance can be guaranteed when only the access portion\
    \ is being\n   provisioned to a specific bandwidth capacity.\n   The intent behind\
    \ this document is to define a methodology for\n   testing sustained TCP Layer\
    \ performance.  In this document, the\n   achievable TCP Throughput is that amount\
    \ of data per unit of time\n   that TCP transports when in the TCP Equilibrium\
    \ state.  (See\n   Section 1.3 for the TCP Equilibrium definition).  Throughout\
    \ this\n   document, \"maximum achievable throughput\" refers to the theoretical\n\
    \   achievable throughput when TCP is in the Equilibrium state.\n   TCP is connection\
    \ oriented, and at the transmitting side, it uses a\n   congestion window (TCP\
    \ CWND).  At the receiving end, TCP uses a\n   receive window (TCP RWND) to inform\
    \ the transmitting end on how many\n   Bytes it is capable of accepting at a given\
    \ time.\n   Derived from Round-Trip Time (RTT) and network Bottleneck Bandwidth\n\
    \   (BB), the Bandwidth-Delay Product (BDP) determines the Send and\n   Received\
    \ Socket buffer sizes required to achieve the maximum TCP\n   Throughput.  Then,\
    \ with the help of slow start and congestion\n   avoidance algorithms, a TCP CWND\
    \ is calculated based on the IP\n   network path loss rate.  Finally, the minimum\
    \ value between the\n   calculated TCP CWND and the TCP RWND advertised by the\
    \ opposite end\n   will determine how many Bytes can actually be sent by the\n\
    \   transmitting side at a given time.\n   Both TCP Window sizes (RWND and CWND)\
    \ may vary during any given TCP\n   session, although up to bandwidth limits,\
    \ larger RWND and larger CWND\n   will achieve higher throughputs by permitting\
    \ more in-flight Bytes.\n   At both ends of the TCP connection and for each socket,\
    \ there are\n   default buffer sizes.  There are also kernel-enforced maximum\
    \ buffer\n   sizes.  These buffer sizes can be adjusted at both ends (transmitting\n\
    \   and receiving).  Some TCP/IP stack implementations use Receive Window\n  \
    \ Auto-Tuning, although, in order to obtain the maximum throughput, it\n   is\
    \ critical to use large enough TCP Send and Receive Socket Buffer\n   sizes. \
    \ In fact, they SHOULD be equal to or greater than BDP.\n   Many variables are\
    \ involved in TCP Throughput performance, but this\n   methodology focuses on\
    \ the following:\n   - BB (Bottleneck Bandwidth)\n   - RTT (Round-Trip Time)\n\
    \   - Send and Receive Socket Buffers\n   - Minimum TCP RWND\n   - Path MTU (Maximum\
    \ Transmission Unit)\n   This methodology proposes TCP testing that SHOULD be\
    \ performed in\n   addition to traditional tests of the Layer 2/3 type.  In fact,\
    \ Layer\n   2/3 tests are REQUIRED to verify the integrity of the network before\n\
    \   conducting TCP tests.  Examples include \"iperf\" (UDP mode) and manual\n\
    \   packet-layer test techniques where packet throughput, loss, and delay\n  \
    \ measurements are conducted.  When available, standardized testing\n   similar\
    \ to [RFC2544], but adapted for use in operational networks,\n   MAY be used.\n\
    \   Note: [RFC2544] was never meant to be used outside a lab environment.\n  \
    \ Sections 2 and 3 of this document provide a general overview of the\n   proposed\
    \ methodology.  Section 4 defines the metrics, while Section 5\n   explains how\
    \ to conduct the tests and interpret the results.\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in RFC 2119 [RFC2119].\n"
- title: 1.2.  Terminology
  contents:
  - "1.2.  Terminology\n   The common definitions used in this methodology are as\
    \ follows:\n   - TCP Throughput Test Device (TCP TTD) refers to a compliant TCP\
    \ host\n     that generates traffic and measures metrics as defined in this\n\
    \     methodology, i.e., a dedicated communications test instrument.\n   - Customer\
    \ Provided Equipment (CPE) refers to customer-owned\n     equipment (routers,\
    \ switches, computers, etc.).\n   - Customer Edge (CE) refers to a provider-owned\
    \ demarcation device.\n   - Provider Edge (PE) refers to a provider's distribution\
    \ equipment.\n   - Bottleneck Bandwidth (BB) refers to the lowest bandwidth along\
    \ the\n     complete path.  \"Bottleneck Bandwidth\" and \"Bandwidth\" are used\n\
    \     synonymously in this document.  Most of the time, the Bottleneck\n     Bandwidth\
    \ is in the access portion of the wide-area network\n     (CE - PE).\n   - Provider\
    \ (P) refers to provider core network equipment.\n   - Network Under Test (NUT)\
    \ refers to the tested IP network path.\n   - Round-Trip Time (RTT) is the elapsed\
    \ time between the clocking in\n     of the first bit of a TCP segment sent and\
    \ the receipt of the last\n     bit of the corresponding TCP Acknowledgment.\n\
    \   - Bandwidth-Delay Product (BDP) refers to the product of a data\n     link's\
    \ capacity (in bits per second) and its end-to-end delay (in\n     seconds).\n\
    \   +---+ +----+ +----+  +----+ +---+  +---+ +----+  +----+ +----+ +---+\n   |TCP|-|\
    \ CPE|-| CE |--| PE |-| P |--| P |-| PE |--| CE |-| CPE|-|TCP|\n   |TTD| |   \
    \ | |    |BB|    | |   |  |   | |    |BB|    | |    | |TTD|\n   +---+ +----+ +----+\
    \  +----+ +---+  +---+ +----+  +----+ +----+ +---+\n         <------------------------\
    \ NUT ------------------------->\n     R >-----------------------------------------------------------|\n\
    \     T                                                             |\n     T\
    \ <-----------------------------------------------------------|\n            \
    \      Figure 1.2.  Devices, Links, and Paths\n   Note that the NUT may be built\
    \ with a variety of devices including,\n   but not limited to, load balancers,\
    \ proxy servers, or WAN\n   acceleration appliances.  The detailed topology of\
    \ the NUT SHOULD be\n   well-known when conducting the TCP Throughput tests, although\
    \ this\n   methodology makes no attempt to characterize specific network\n   architectures.\n"
- title: 1.3.  TCP Equilibrium
  contents:
  - "1.3.  TCP Equilibrium\n   TCP connections have three (3) fundamental congestion\
    \ window phases,\n   which are depicted in Figure 1.3.\n   1. The Slow Start phase,\
    \ which occurs at the beginning of a TCP\n      transmission or after a retransmission\
    \ Time-Out.\n   2. The Congestion Avoidance phase, during which TCP ramps up to\n\
    \      establish the maximum achievable throughput.  It is important to\n    \
    \  note that retransmissions are a natural by-product of the TCP\n      congestion\
    \ avoidance algorithm as it seeks to achieve maximum\n      throughput.\n   3.\
    \ The Loss Recovery phase, which could include Fast Retransmit\n      (Tahoe)\
    \ or Fast Recovery (Reno and New Reno).  When packet loss\n      occurs, the Congestion\
    \ Avoidance phase transitions either to Fast\n      Retransmission or Fast Recovery,\
    \ depending upon the TCP\n      implementation.  If a Time-Out occurs, TCP transitions\
    \ back to the\n      Slow Start phase.\n    /\\  |\n    /\\  |High ssthresh  TCP\
    \ CWND                         TCP\n    /\\  |Loss Event *   halving    3-Loss\
    \ Recovery       Equilibrium\n     T  |          * \\  upon loss\n     h  |  \
    \        *  \\    /  \\        Time-Out            Adjusted\n     r  |       \
    \   *   \\  /    \\      +--------+         * ssthresh\n   T o  |          * \
    \   \\/      \\    / Multiple|        *\n   C u  |          * 2-Congestion\\ \
    \ /  Loss    |        *\n   P g  |         *    Avoidance  \\/   Event   |   \
    \    *\n     h  |        *              Half           |     *\n     p  |    \
    \  *                TCP CWND       | * 1-Slow Start\n     u  | * 1-Slow Start\
    \                      Min TCP CWND after T-O\n     t  +-----------------------------------------------------------\n\
    \          Time > > > > > > > > > > > > > > > > > > > > > > > > > >\n      Note:\
    \ ssthresh = Slow Start threshold.\n                       Figure 1.3.  TCP CWND\
    \ Phases\n   A well-tuned and well-managed IP network with appropriate TCP\n \
    \  adjustments in the IP hosts and applications should perform very\n   close\
    \ to the BB when TCP is in the Equilibrium state.\n   This TCP methodology provides\
    \ guidelines to measure the maximum\n   achievable TCP Throughput when TCP is\
    \ in the Equilibrium state.  All\n   maximum achievable TCP Throughputs specified\
    \ in Section 3.3 are with\n   respect to this condition.\n   It is important to\
    \ clarify the interaction between the sender's Send\n   Socket Buffer and the\
    \ receiver's advertised TCP RWND size.  TCP test\n   programs such as \"iperf\"\
    , \"ttcp\", etc. allow the sender to control\n   the quantity of TCP Bytes transmitted\
    \ and unacknowledged (in-flight),\n   commonly referred to as the Send Socket\
    \ Buffer.  This is done\n   independently of the TCP RWND size advertised by the\
    \ receiver.\n"
- title: 2.  Scope and Goals
  contents:
  - "2.  Scope and Goals\n   Before defining the goals, it is important to clearly\
    \ define the\n   areas that are out of scope.\n   - This methodology is not intended\
    \ to predict the TCP Throughput\n     during the transient stages of a TCP connection,\
    \ such as during the\n     Slow Start phase.\n   - This methodology is not intended\
    \ to definitively benchmark TCP\n     implementations of one OS to another, although\
    \ some users may find\n     value in conducting qualitative experiments.\n   -\
    \ This methodology is not intended to provide detailed diagnosis of\n     problems\
    \ within endpoints or within the network itself as related\n     to non-optimal\
    \ TCP performance, although results interpretation for\n     each test step may\
    \ provide insights to potential issues.\n   - This methodology does not propose\
    \ to operate permanently with high\n     measurement loads.  TCP performance and\
    \ optimization within\n     operational networks MAY be captured and evaluated\
    \ by using data\n     from the \"TCP Extended Statistics MIB\" [RFC4898].\n  \
    \ In contrast to the above exclusions, the primary goal is to define a\n   method\
    \ to conduct a practical end-to-end assessment of sustained TCP\n   performance\
    \ within a managed business-class IP network.  Another key\n   goal is to establish\
    \ a set of \"best practices\" that a non-TCP expert\n   SHOULD apply when validating\
    \ the ability of a managed IP network to\n   carry end-user TCP applications.\n\
    \   Specific goals are to:\n   - Provide a practical test approach that specifies\
    \ tunable parameters\n     (such as MTU (Maximum Transmission Unit) and Socket\
    \ Buffer sizes)\n     and how these affect the outcome of TCP performance over\
    \ an IP\n     network.\n   - Provide specific test conditions such as link speed,\
    \ RTT, MTU,\n     Socket Buffer sizes, and achievable TCP Throughput when TCP\
    \ is in\n     the Equilibrium state.  For guideline purposes, provide examples\
    \ of\n     test conditions and their maximum achievable TCP Throughput.\n    \
    \ Section 1.3 provides specific details concerning the definition of\n     TCP\
    \ Equilibrium within this methodology, while Section 3 provides\n     specific\
    \ test conditions with examples.\n   - Define three (3) basic metrics to compare\
    \ the performance of TCP\n     connections under various network conditions. \
    \ See Section 4.\n   - Provide some areas within the end host or the network that\
    \ SHOULD\n     be considered for investigation in test situations where the\n\
    \     recommended procedure does not yield the maximum achievable TCP\n     Throughput.\
    \  However, this methodology is not intended to provide\n     detailed diagnosis\
    \ on these issues.  See Section 5.2.\n"
- title: 3.  Methodology
  contents:
  - "3.  Methodology\n   This methodology is intended for operational and managed\
    \ IP networks.\n   A multitude of network architectures and topologies can be\
    \ tested.\n   The diagram in Figure 1.2 is very general and is only provided to\n\
    \   illustrate typical segmentation within end-user and network provider\n   domains.\n\
    \   Also, as stated in Section 1, it is considered best practice to\n   verify\
    \ the integrity of the network by conducting Layer 2/3 tests\n   such as [RFC2544]\
    \ or other methods of network stress tests; although\n   it is important to mention\
    \ here that [RFC2544] was never meant to be\n   used outside a lab environment.\n\
    \   It is not possible to make an accurate TCP Throughput measurement\n   when\
    \ the network is dysfunctional.  In particular, if the network is\n   exhibiting\
    \ high packet loss and/or high jitter, then TCP Layer\n   Throughput testing will\
    \ not be meaningful.  As a guideline, 5% packet\n   loss and/or 150 ms of jitter\
    \ may be considered too high for an\n   accurate measurement.\n   TCP Throughput\
    \ testing may require cooperation between the end-user\n   customer and the network\
    \ provider.  As an example, in an MPLS\n   (Multiprotocol Label Switching) network\
    \ architecture, the testing\n   SHOULD be conducted either on the CPE or on the\
    \ CE device and not on\n   the PE (Provider Edge) router.\n   The following represents\
    \ the sequential order of steps for this\n   testing methodology:\n   1. Identify\
    \ the Path MTU.  Packetization Layer Path MTU Discovery\n      (PLPMTUD) [RFC4821]\
    \ SHOULD be conducted.  It is important to\n      identify the path MTU so that\
    \ the TCP TTD is configured properly\n      to avoid fragmentation.\n   2. Baseline\
    \ Round-Trip Time and Bandwidth.  This step establishes the\n      inherent, non-congested\
    \ Round-Trip Time (RTT) and the Bottleneck\n      Bandwidth (BB) of the end-to-end\
    \ network path.  These measurements\n      are used to provide estimates of the\
    \ TCP RWND and Send Socket\n      Buffer sizes that SHOULD be used during subsequent\
    \ test steps.\n   3. TCP Connection Throughput Tests.  With baseline measurements\
    \ of\n      Round-Trip Time and Bottleneck Bandwidth, single- and multiple-\n\
    \      TCP-connection throughput tests SHOULD be conducted to baseline\n     \
    \ network performance.\n   These three (3) steps are detailed in Sections 3.1\
    \ to 3.3.\n   Important to note are some of the key characteristics and\n   considerations\
    \ for the TCP test instrument.  The test host MAY be a\n   standard computer or\
    \ a dedicated communications test instrument.  In\n   both cases, it MUST be capable\
    \ of emulating both a client and a\n   server.\n   The following criteria SHOULD\
    \ be considered when selecting whether\n   the TCP test host can be a standard\
    \ computer or has to be a dedicated\n   communications test instrument:\n   -\
    \ TCP implementation used by the test host, OS version (e.g., LINUX\n     OS kernel\
    \ using TCP New Reno), TCP options supported, etc. will\n     obviously be more\
    \ important when using dedicated communications\n     test instruments where the\
    \ TCP implementation may be customized or\n     tuned to run in higher-performance\
    \ hardware.  When a compliant TCP\n     TTD is used, the TCP implementation SHOULD\
    \ be identified in the\n     test results.  The compliant TCP TTD SHOULD be usable\
    \ for complete\n     end-to-end testing through network security elements and\
    \ SHOULD\n     also be usable for testing network sections.\n   - More importantly,\
    \ the TCP test host MUST be capable of generating\n     and receiving stateful\
    \ TCP test traffic at the full BB of the NUT.\n     Stateful TCP test traffic\
    \ means that the test host MUST fully\n     implement a TCP/IP stack; this is\
    \ generally a comment aimed at\n     dedicated communications test equipment that\
    \ sometimes \"blasts\"\n     packets with TCP headers.  At the time of this publication,\
    \ testing\n     TCP Throughput at rates greater than 100 Mbps may require high-\n\
    \     performance server hardware or dedicated hardware-based test tools.\n  \
    \ - A compliant TCP Throughput Test Device MUST allow adjusting both\n     Send\
    \ and Receive Socket Buffer sizes.  The Socket Buffers MUST be\n     large enough\
    \ to fill the BDP.\n   - Measuring RTT and retransmissions per connection will\
    \ generally\n     require a dedicated communications test instrument.  In the\
    \ absence\n     of dedicated hardware-based test tools, these measurements may\
    \ need\n     to be conducted with packet capture tools, i.e., conduct TCP\n  \
    \   Throughput tests and analyze RTT and retransmissions in packet\n     captures.\
    \  Another option MAY be to use the \"TCP Extended\n     Statistics MIB\" [RFC4898].\n\
    \   - The [RFC4821] PLPMTUD test SHOULD be conducted with a dedicated\n     tester\
    \ that exposes the ability to run the PLPMTUD algorithm\n     independently from\
    \ the OS stack.\n"
- title: 3.1.  Path MTU
  contents:
  - "3.1.  Path MTU\n   TCP implementations should use Path MTU Discovery techniques\
    \ (PMTUD).\n   PMTUD relies on ICMP 'need to frag' messages to learn the path\
    \ MTU.\n   When a device has a packet to send that has the Don't Fragment (DF)\n\
    \   bit in the IP header set and the packet is larger than the MTU of the\n  \
    \ next hop, the packet is dropped, and the device sends an ICMP 'need\n   to frag'\
    \ message back to the host that originated the packet.  The\n   ICMP 'need to\
    \ frag' message includes the next-hop MTU, which PMTUD\n   uses to adjust itself.\
    \  Unfortunately, because many network managers\n   completely disable ICMP, this\
    \ technique does not always prove\n   reliable.\n   Packetization Layer Path MTU\
    \ Discovery (PLPMTUD) [RFC4821] MUST then\n   be conducted to verify the network\
    \ path MTU.  PLPMTUD can be used\n   with or without ICMP.  [RFC4821] specifies\
    \ search_high and search_low\n   parameters for the MTU, and we recommend using\
    \ those parameters.  The\n   goal is to avoid fragmentation during all subsequent\
    \ tests.\n"
- title: 3.2.  Round-Trip Time (RTT) and Bottleneck Bandwidth (BB)
  contents:
  - "3.2.  Round-Trip Time (RTT) and Bottleneck Bandwidth (BB)\n   Before stateful\
    \ TCP testing can begin, it is important to determine\n   the baseline RTT (i.e.,\
    \ non-congested inherent delay) and BB of the\n   end-to-end network to be tested.\
    \  These measurements are used to\n   calculate the BDP and to provide estimates\
    \ of the TCP RWND and Send\n   Socket Buffer sizes that SHOULD be used in subsequent\
    \ test steps.\n"
- title: 3.2.1.  Measuring RTT
  contents:
  - "3.2.1.  Measuring RTT\n   As previously defined in Section 1.2, RTT is the elapsed\
    \ time between\n   the clocking in of the first bit of a TCP segment sent and\
    \ the\n   receipt of the last bit of the corresponding TCP Acknowledgment.\n \
    \  The RTT SHOULD be baselined during off-peak hours in order to obtain\n   a\
    \ reliable figure of the inherent network latency.  Otherwise,\n   additional\
    \ delay caused by network buffering can occur.  Also, when\n   sampling RTT values\
    \ over a given test interval, the minimum measured\n   value SHOULD be used as\
    \ the baseline RTT.  This will most closely\n   estimate the real inherent RTT.\
    \  This value is also used to determine\n   the Buffer Delay Percentage metric\
    \ defined in Section 4.3.\n   The following list is not meant to be exhaustive,\
    \ although it\n   summarizes some of the most common ways to determine Round-Trip\
    \ Time.\n   The desired measurement precision (i.e., ms versus us) may dictate\n\
    \   whether the RTT measurement can be achieved with ICMP pings or by a\n   dedicated\
    \ communications test instrument with precision timers.  The\n   objective of\
    \ this section is to list several techniques in order of\n   decreasing accuracy.\n\
    \   - Use test equipment on each end of the network, \"looping\" the far-\n  \
    \   end tester so that a packet stream can be measured back and forth\n     from\
    \ end to end.  This RTT measurement may be compatible with delay\n     measurement\
    \ protocols specified in [RFC5357].\n   - Conduct packet captures of TCP test\
    \ sessions using \"iperf\" or FTP,\n     or other TCP test applications.  By running\
    \ multiple experiments,\n     packet captures can then be analyzed to estimate\
    \ RTT.  It is\n     important to note that results based upon the SYN -> SYN-ACK\
    \ at the\n     beginning of TCP sessions SHOULD be avoided, since Firewalls might\n\
    \     slow down 3-way handshakes.  Also, at the sender's side,\n     Ostermann's\
    \ LINUX TCPTRACE utility with -l -r arguments can be used\n     to extract the\
    \ RTT results directly from the packet captures.\n   - Obtain RTT statistics available\
    \ from MIBs defined in [RFC4898].\n   - ICMP pings may also be adequate to provide\
    \ Round-Trip Time\n     estimates, provided that the packet size is factored into\
    \ the\n     estimates (i.e., pings with different packet sizes might be\n    \
    \ required).  Some limitations with ICMP ping may include ms\n     resolution\
    \ and whether or not the network elements are responding\n     to pings.  Also,\
    \ ICMP is often rate-limited or segregated into\n     different buffer queues.\
    \  ICMP might not work if QoS (Quality of\n     Service) reclassification is done\
    \ at any hop.  ICMP is not as\n     reliable and accurate as in-band measurements.\n"
- title: 3.2.2.  Measuring BB
  contents:
  - "3.2.2.  Measuring BB\n   Before any TCP Throughput test can be conducted, bandwidth\n\
    \   measurement tests SHOULD be run with stateless IP streams (i.e., not\n   stateful\
    \ TCP) in order to determine the BB of the NUT.  These\n   measurements SHOULD\
    \ be conducted in both directions, especially in\n   asymmetrical access networks\
    \ (e.g., Asymmetric Bit-Rate DSL (ADSL)\n   access).  These tests SHOULD be performed\
    \ at various intervals\n   throughout a business day or even across a week.\n\
    \   Testing at various time intervals would provide a better\n   characterization\
    \ of TCP Throughput and better diagnosis insight (for\n   cases where there are\
    \ TCP performance issues).  The bandwidth tests\n   SHOULD produce logged outputs\
    \ of the achieved bandwidths across the\n   complete test duration.\n   There\
    \ are many well-established techniques available to provide\n   estimated measures\
    \ of bandwidth over a network.  It is a common\n   practice for network providers\
    \ to conduct Layer 2/3 bandwidth\n   capacity tests using [RFC2544], although\
    \ it is understood that\n   [RFC2544] was never meant to be used outside a lab\
    \ environment.\n   These bandwidth measurements SHOULD use network capacity techniques\n\
    \   as defined in [RFC5136].\n"
- title: 3.3.  Measuring TCP Throughput
  contents:
  - "3.3.  Measuring TCP Throughput\n   This methodology specifically defines TCP\
    \ Throughput measurement\n   techniques to verify maximum achievable TCP performance\
    \ in a managed\n   business-class IP network.\n   With baseline measurements of\
    \ RTT and BB from Section 3.2, a series\n   of single- and/or multiple-TCP-connection\
    \ throughput tests SHOULD be\n   conducted.\n   The number of trials and the choice\
    \ between single or multiple TCP\n   connections will be based on the intention\
    \ of the test.  A single-\n   TCP-connection test might be enough to measure the\
    \ achievable\n   throughput of Metro Ethernet connectivity.  However, it is important\n\
    \   to note that various traffic management techniques can be used in an\n   IP\
    \ network and that some of those techniques can only be tested with\n   multiple\
    \ connections.  As an example, multiple TCP sessions might be\n   required to\
    \ detect traffic shaping versus policing.  Multiple\n   sessions might also be\
    \ needed to measure Active Queue Management\n   performance.  However, traffic\
    \ management testing is not within the\n   scope of this test methodology.\n \
    \  In all circumstances, it is RECOMMENDED to run the tests in each\n   direction\
    \ independently first and then to run them in both directions\n   simultaneously.\
    \  It is also RECOMMENDED to run the tests at different\n   times of the day.\n\
    \   In each case, the TCP Transfer Time Ratio, the TCP Efficiency\n   Percentage,\
    \ and the Buffer Delay Percentage MUST be measured in each\n   direction.  These\
    \ 3 metrics are defined in Section 4.\n"
- title: 3.3.1.  Minimum TCP RWND
  contents:
  - "3.3.1.  Minimum TCP RWND\n   The TCP TTD MUST allow the Send Socket Buffer and\
    \ Receive Window\n   sizes to be set higher than the BDP; otherwise, TCP performance\
    \ will\n   be limited.  In the business customer environment, these settings are\n\
    \   not generally adjustable by the average user.  These settings are\n   either\
    \ hard-coded in the application or configured within the OS as\n   part of a corporate\
    \ image.  In many cases, the user's host Send\n   Socket Buffer and Receive Window\
    \ size settings are not optimal.\n   This section provides derivations of BDPs\
    \ under various network\n   conditions.  It also provides examples of achievable\
    \ TCP Throughput\n   with various TCP RWND sizes.  This provides important guidelines\n\
    \   showing what can be achieved with settings higher than the BDP,\n   versus\
    \ what would be achieved in a variety of real-world conditions.\n   The minimum\
    \ required TCP RWND size can be calculated from the\n   Bandwidth-Delay Product\
    \ (BDP), which is as follows:\n      BDP (bits) = RTT (sec) X BB (bps)\n   Note\
    \ that the RTT is being used as the \"Delay\" variable for the BDP.\n   Then,\
    \ by dividing the BDP by 8, we obtain the minimum required TCP\n   RWND size in\
    \ Bytes.  For optimal results, the Send Socket Buffer MUST\n   be adjusted to\
    \ the same value at each end of the network.\n      Minimum required TCP RWND\
    \ = BDP / 8\n   As an example, on a T3 link with 25-ms RTT, the BDP would equal\n\
    \   ~1,105,000 bits, and the minimum required TCP RWND would be ~138 KB.\n   Note\
    \ that separate calculations are REQUIRED on asymmetrical paths.\n   An asymmetrical-path\
    \ example would be a 90-ms RTT ADSL line with 5\n   Mbps downstream and 640 Kbps\
    \ upstream.  The downstream BDP would\n   equal ~450,000 bits, while the upstream\
    \ one would be only\n   ~57,600 bits.\n   The following table provides some representative\
    \ network link speeds,\n   RTT, BDP, and their associated minimum required TCP\
    \ RWND sizes.\n       Link                                        Minimum Required\n\
    \       Speed*        RTT              BDP             TCP RWND\n       (Mbps)\
    \        (ms)            (bits)           (KBytes)\n   --------------------------------------------------------------------\n\
    \        1.536        20.00           30,720              3.84\n        1.536\
    \        50.00           76,800              9.60\n        1.536       100.00\
    \          153,600             19.20\n       44.210        10.00          442,100\
    \             55.26\n       44.210        15.00          663,150             82.89\n\
    \       44.210        25.00        1,105,250            138.16\n      100.000\
    \         1.00          100,000             12.50\n      100.000         2.00\
    \          200,000             25.00\n      100.000         5.00          500,000\
    \             62.50\n    1,000.000         0.10          100,000             12.50\n\
    \    1,000.000         0.50          500,000             62.50\n    1,000.000\
    \         1.00        1,000,000            125.00\n   10,000.000         0.05\
    \          500,000             62.50\n   10,000.000         0.30        3,000,000\
    \            375.00\n   * Note that link speed is the BB for the NUT\n    Table\
    \ 3.3.1. Link Speed, RTT, Calculated BDP, and Minimum TCP RWND\n   In the above\
    \ table, the following serial link speeds are used:\n      - T1 = 1.536 Mbps (for\
    \ a B8ZS line encoding facility)\n      - T3 = 44.21 Mbps (for a C-Bit framing\
    \ facility)\n   The previous table illustrates the minimum required TCP RWND.\
    \  If a\n   smaller TCP RWND size is used, then the TCP Throughput cannot be\n\
    \   optimal.  To calculate the TCP Throughput, the following formula is\n   used:\n\
    \      TCP Throughput = TCP RWND X 8 / RTT\n   An example could be a 100-Mbps\
    \ IP path with 5-ms RTT and a TCP RWND\n   of 16 KB; then:\n      TCP Throughput\
    \ = 16 KBytes X 8 bits / 5 ms\n      TCP Throughput = 128,000 bits / 0.005 sec\n\
    \      TCP Throughput = 25.6 Mbps\n   Another example, for a T3 using the same\
    \ calculation formula, is\n   illustrated in Figure 3.3.1a:\n      TCP Throughput\
    \ = 16 KBytes X 8 bits / 10 ms\n      TCP Throughput = 128,000 bits / 0.01 sec\n\
    \      TCP Throughput = 12.8 Mbps*\n   When the TCP RWND size exceeds the BDP\
    \ (T3 link and 64-KByte TCP RWND\n   on a 10-ms RTT path), the maximum Frames\
    \ Per Second (FPS) limit of\n   3664 is reached, and then the formula is:\n  \
    \    TCP Throughput = max FPS X (MTU - 40) X 8\n      TCP Throughput = 3664 FPS\
    \ X 1460 Bytes X 8 bits\n      TCP Throughput = 42.8 Mbps**\n   The following\
    \ diagram compares achievable TCP Throughputs on a T3\n   with Send Socket Buffer\
    \ and TCP RWND sizes of 16 KB versus 64 KB.\n             45|\n              \
    \ |           _______**42.8\n             40|           |64KB |\n    TCP     \
    \   |           |     |\n   Through-  35|           |     |\n    put        |\
    \           |     |          +-----+34.1\n   (Mbps)    30|           |     | \
    \         |64KB |\n               |           |     |          |     |\n     \
    \        25|           |     |          |     |\n               |           |\
    \     |          |     |\n             20|           |     |          |     |\
    \          _______20.5\n               |           |     |          |     |  \
    \        |64KB |\n             15|           |     |          |     |        \
    \  |     |\n               |*12.8+-----|     |          |     |          |   \
    \  |\n             10|     |16KB |     |          |     |          |     |\n \
    \              |     |     |     |8.5 +-----|     |          |     |\n       \
    \       5|     |     |     |    |16KB |     |5.1 +-----|     |\n             \
    \  |_____|_____|_____|____|_____|_____|____|16KB |_____|____\n               \
    \           10               15               25\n                           \
    \       RTT (milliseconds)\n         Figure 3.3.1a.  TCP Throughputs on a T3 at\
    \ Different RTTs\n   The following diagram shows the achievable TCP Throughput\
    \ on a 25-ms\n   T3 when Send Socket Buffer and TCP RWND sizes are increased.\n\
    \             45|\n               |\n             40|                        \
    \                    +-----+40.9\n    TCP        |                           \
    \                 |     |\n   Through-  35|                                  \
    \          |     |\n    put        |                                         \
    \   |     |\n   (Mbps)    30|                                            |   \
    \  |\n               |                                            |     |\n  \
    \           25|                                            |     |\n         \
    \      |                                            |     |\n             20|\
    \                               +-----+20.5  |     |\n               |       \
    \                        |     |      |     |\n             15|              \
    \                 |     |      |     |\n               |                     \
    \          |     |      |     |\n             10|                  +-----+10.2\
    \  |     |      |     |\n               |                  |     |      |    \
    \ |      |     |\n              5|     +-----+5.1   |     |      |     |     \
    \ |     |\n               |_____|_____|______|_____|______|_____|______|_____|_____\n\
    \                       16           32           64            128*\n       \
    \                     TCP RWND Size (KBytes)\n      * Note that 128 KB requires\
    \ the [RFC1323] TCP Window Scale option.\n      Figure 3.3.1b.  TCP Throughputs\
    \ on a T3 with Different TCP RWND\n"
- title: 4.  TCP Metrics
  contents:
  - "4.  TCP Metrics\n   This methodology focuses on a TCP Throughput and provides\
    \ 3 basic\n   metrics that can be used for better understanding of the results.\
    \  It\n   is recognized that the complexity and unpredictability of TCP makes\n\
    \   it very difficult to develop a complete set of metrics that accounts\n   for\
    \ the myriad of variables (i.e., RTT variations, loss conditions,\n   TCP implementations,\
    \ etc.).  However, these 3 metrics facilitate TCP\n   Throughput comparisons under\
    \ varying network conditions and host\n   buffer size/RWND settings.\n"
- title: 4.1.  Transfer Time Ratio
  contents:
  - "4.1.  Transfer Time Ratio\n   The first metric is the TCP Transfer Time Ratio,\
    \ which is simply the\n   ratio between the Actual TCP Transfer Time versus the\
    \ Ideal TCP\n   Transfer Time.\n   The Actual TCP Transfer Time is simply the\
    \ time it takes to transfer\n   a block of data across TCP connection(s).\n  \
    \ The Ideal TCP Transfer Time is the predicted time for which a block\n   of data\
    \ SHOULD transfer across TCP connection(s), considering the BB\n   of the NUT.\n\
    \                                 Actual TCP Transfer Time\n      TCP Transfer\
    \ Time Ratio =  -------------------------\n                                 Ideal\
    \ TCP Transfer Time\n   The Ideal TCP Transfer Time is derived from the Maximum\
    \ Achievable\n   TCP Throughput, which is related to the BB and Layer 1/2/3/4\n\
    \   overheads associated with the network path.  The following sections\n   provide\
    \ derivations for the Maximum Achievable TCP Throughput and\n   example calculations\
    \ for the TCP Transfer Time Ratio.\n"
- title: 4.1.1.  Maximum Achievable TCP Throughput Calculation
  contents:
  - "4.1.1.  Maximum Achievable TCP Throughput Calculation\n   This section provides\
    \ formulas to calculate the Maximum Achievable\n   TCP Throughput, with examples\
    \ for T3 (44.21 Mbps) and Ethernet.\n   All calculations are based on IP version\
    \ 4 with TCP/IP headers of 20\n   Bytes each (20 for TCP + 20 for IP) within an\
    \ MTU of 1500 Bytes.\n   First, the maximum achievable Layer 2 throughput of a\
    \ T3 interface is\n   limited by the maximum quantity of Frames Per Second (FPS)\
    \ permitted\n   by the actual physical layer (Layer 1) speed.\n   The calculation\
    \ formula is:\n      FPS = T3 Physical Speed / ((MTU + PPP + Flags + CRC16) X\
    \ 8)\n      FPS = (44.21 Mbps /\n                 ((1500 Bytes + 4 Bytes + 2 Bytes\
    \ + 2 Bytes) X 8 )))\n      FPS = (44.21 Mbps / (1508 Bytes X 8))\n      FPS =\
    \ 44.21 Mbps / 12064 bits\n      FPS = 3664\n   Then, to obtain the Maximum Achievable\
    \ TCP Throughput (Layer 4), we\n   simply use:\n      (MTU - 40) in Bytes X 8\
    \ bits X max FPS\n   For a T3, the maximum TCP Throughput =\n      1460 Bytes\
    \ X 8 bits X 3664 FPS\n      Maximum TCP Throughput = 11680 bits X 3664 FPS\n\
    \      Maximum TCP Throughput = 42.8 Mbps\n   On Ethernet, the maximum achievable\
    \ Layer 2 throughput is limited by\n   the maximum Frames Per Second permitted\
    \ by the IEEE802.3 standard.\n   The maximum FPS for 100-Mbps Ethernet is 8127,\
    \ and the calculation\n   formula is:\n      FPS = (100 Mbps / (1538 Bytes X 8\
    \ bits))\n   The maximum FPS for GigE is 81274, and the calculation formula is:\n\
    \      FPS = (1 Gbps / (1538 Bytes X 8 bits))\n   The maximum FPS for 10GigE is\
    \ 812743, and the calculation formula is:\n      FPS = (10 Gbps / (1538 Bytes\
    \ X 8 bits))\n   The 1538 Bytes equates to:\n      MTU + Ethernet + CRC32 + IFG\
    \ + Preamble + SFD\n           (IFG = Inter-Frame Gap and SFD = Start of Frame\
    \ Delimiter)\n   where MTU is 1500 Bytes, Ethernet is 14 Bytes, CRC32 is 4 Bytes,\
    \ IFG\n   is 12 Bytes, Preamble is 7 Bytes, and SFD is 1 Byte.\n   Then, to obtain\
    \ the Maximum Achievable TCP Throughput (Layer 4), we\n   simply use:\n      (MTU\
    \ - 40) in Bytes X 8 bits X max FPS\n   For 100-Mbps Ethernet, the maximum TCP\
    \ Throughput =\n      1460 Bytes X 8 bits X 8127 FPS\n      Maximum TCP Throughput\
    \ = 11680 bits X 8127 FPS\n      Maximum TCP Throughput = 94.9 Mbps\n   It is\
    \ important to note that better results could be obtained with\n   jumbo frames\
    \ on Gigabit and 10-Gigabit Ethernet interfaces.\n"
- title: 4.1.2.  TCP Transfer Time and Transfer Time Ratio Calculation
  contents:
  - "4.1.2.  TCP Transfer Time and Transfer Time Ratio Calculation\n   The following\
    \ table illustrates the Ideal TCP Transfer Time of a\n   single TCP connection\
    \ when its TCP RWND and Send Socket Buffer sizes\n   equal or exceed the BDP.\n\
    \       Link                             Maximum            Ideal TCP\n      \
    \ Speed                   BDP      Achievable TCP     Transfer Time\n       (Mbps)\
    \     RTT (ms)   (KBytes)   Throughput(Mbps)   (seconds)*\n   --------------------------------------------------------------------\n\
    \         1.536    50.00         9.6            1.4             571.0\n      \
    \  44.210    25.00       138.2           42.8              18.0\n       100.000\
    \     2.00        25.0           94.9               9.0\n     1,000.000     1.00\
    \       125.0          949.2               1.0\n    10,000.000     0.05      \
    \  62.5        9,492.0               0.1\n    * Transfer times are rounded for\
    \ simplicity.\n          Table 4.1.2.  Link Speed, RTT, BDP, TCP Throughput, and\n\
    \                 Ideal TCP Transfer Time for a 100-MB File\n   For a 100-MB file\
    \ (100 X 8 = 800 Mbits), the Ideal TCP Transfer Time\n   is derived as follows:\n\
    \                                          800 Mbits\n      Ideal TCP Transfer\
    \ Time = -----------------------------------\n                               \
    \  Maximum Achievable TCP Throughput\n   To illustrate the TCP Transfer Time Ratio,\
    \ an example would be the\n   bulk transfer of 100 MB over 5 simultaneous TCP\
    \ connections  (each\n   connection transferring 100 MB).  In this example, the\
    \ Ethernet\n   service provides a Committed Access Rate (CAR) of 500 Mbps.  Each\n\
    \   connection may achieve different throughputs during a test, and the\n   overall\
    \ throughput rate is not always easy to determine (especially\n   as the number\
    \ of connections increases).\n   The Ideal TCP Transfer Time would be ~8 seconds,\
    \ but in this example,\n   the Actual TCP Transfer Time was 12 seconds.  The TCP\
    \ Transfer Time\n   Ratio would then be 12/8 = 1.5, which indicates that the transfer\n\
    \   across all connections took 1.5 times longer than the ideal.\n"
- title: 4.2.  TCP Efficiency
  contents:
  - "4.2.  TCP Efficiency\n   The second metric represents the percentage of Bytes\
    \ that were not\n   retransmitted.\n                          Transmitted Bytes\
    \ - Retransmitted Bytes\n      TCP Efficiency % =  ---------------------------------------\
    \  X 100\n                                   Transmitted Bytes\n   Transmitted\
    \ Bytes are the total number of TCP Bytes to be\n   transmitted, including the\
    \ original and the retransmitted Bytes.\n"
- title: 4.2.1.  TCP Efficiency Percentage Calculation
  contents:
  - "4.2.1.  TCP Efficiency Percentage Calculation\n   As an example, if 100,000 Bytes\
    \ were sent and 2,000 had to be\n   retransmitted, the TCP Efficiency Percentage\
    \ would be calculated as:\n                           102,000 - 2,000\n      TCP\
    \ Efficiency % =  -----------------  X 100 = 98.03%\n                        \
    \     102,000\n   Note that the Retransmitted Bytes may have occurred more than\
    \ once;\n   if so, then these multiple retransmissions are added to the\n   Retransmitted\
    \ Bytes and to the Transmitted Bytes counts.\n"
- title: 4.3.  Buffer Delay
  contents:
  - "4.3.  Buffer Delay\n   The third metric is the Buffer Delay Percentage, which\
    \ represents the\n   increase in RTT during a TCP Throughput test versus the inherent\
    \ or\n   baseline RTT.  The baseline RTT is the Round-Trip Time inherent to\n\
    \   the network path under non-congested conditions as defined in\n   Section\
    \ 3.2.1.  The average RTT is derived from the total of all\n   measured RTTs during\
    \ the actual test at every second divided by the\n   test duration in seconds.\n\
    \                                      Total RTTs during transfer\n      Average\
    \ RTT during transfer = -----------------------------\n                      \
    \               Transfer duration in seconds\n                       Average RTT\
    \ during transfer - Baseline RTT\n      Buffer Delay % = ------------------------------------------\
    \ X 100\n                                   Baseline RTT\n"
- title: 4.3.1.  Buffer Delay Percentage Calculation
  contents:
  - "4.3.1.  Buffer Delay Percentage Calculation\n   As an example, consider a network\
    \ path with a baseline RTT of 25 ms.\n   During the course of a TCP transfer,\
    \ the average RTT across the\n   entire transfer increases to 32 ms.  Then, the\
    \ Buffer Delay\n   Percentage would be calculated as:\n                      \
    \ 32 - 25\n      Buffer Delay % = ------- X 100 = 28%\n                      \
    \   25\n   Note that the TCP Transfer Time Ratio, TCP Efficiency Percentage, and\n\
    \   the Buffer Delay Percentage MUST all be measured during each\n   throughput\
    \ test.  A poor TCP Transfer Time Ratio (i.e., Actual TCP\n   Transfer Time greater\
    \ than the Ideal TCP Transfer Time) may be\n   diagnosed by correlating with sub-optimal\
    \ TCP Efficiency Percentage\n   and/or Buffer Delay Percentage metrics.\n"
- title: 5.  Conducting TCP Throughput Tests
  contents:
  - "5.  Conducting TCP Throughput Tests\n   Several TCP tools are currently used\
    \ in the network world, and one of\n   the most common is \"iperf\".  With this\
    \ tool, hosts are installed at\n   each end of the network path; one acts as a\
    \ client and the other as a\n   server.  The Send Socket Buffer and the TCP RWND\
    \ sizes of both client\n   and server can be manually set.  The achieved throughput\
    \ can then be\n   measured, either uni-directionally or bi-directionally.  For\
    \ higher-\n   BDP situations in lossy networks (Long Fat Networks (LFNs) or\n\
    \   satellite links, etc.), TCP options such as Selective Acknowledgment\n   SHOULD\
    \ become part of the window size/throughput characterization.\n   Host hardware\
    \ performance must be well understood before conducting\n   the tests described\
    \ in the following sections.  A dedicated\n   communications test instrument will\
    \ generally be REQUIRED, especially\n   for line rates of GigE and 10 GigE.  A\
    \ compliant TCP TTD SHOULD\n   provide a warning message when the expected test\
    \ throughput will\n   exceed the subscribed customer SLA.  If the throughput test\
    \ is\n   expected to exceed the subscribed customer SLA, then the test SHOULD\n\
    \   be coordinated with the network provider.\n   The TCP Throughput test SHOULD\
    \ be run over a long enough duration to\n   properly exercise network buffers\
    \ (i.e., greater than 30 seconds) and\n   SHOULD also characterize performance\
    \ at different times of the day.\n"
- title: 5.1.  Single versus Multiple TCP Connections
  contents:
  - "5.1.  Single versus Multiple TCP Connections\n   The decision whether to conduct\
    \ single- or multiple-TCP-connection\n   tests depends upon the size of the BDP\
    \ in relation to the TCP RWND\n   configured in the end-user environment.  For\
    \ example, if the BDP for\n   a Long Fat Network (LFN) turns out to be 2 MB, then\
    \ it is probably\n   more realistic to test this network path with multiple connections.\n\
    \   Assuming typical host TCP RWND sizes of 64 KB (e.g., Windows XP),\n   using\
    \ 32 TCP connections would emulate a small-office scenario.\n   The following\
    \ table is provided to illustrate the relationship\n   between the TCP RWND and\
    \ the number of TCP connections required to\n   fill the available capacity of\
    \ a given BDP.  For this example, the\n   network bandwidth is 500 Mbps and the\
    \ RTT is 5 ms; then, the BDP\n   equates to 312.5 KBytes.\n                  \
    \            Number of TCP Connections\n                  TCP RWND   to fill available\
    \ bandwidth\n                  --------------------------------------\n      \
    \              16 KB             20\n                    32 KB             10\n\
    \                    64 KB              5\n                   128 KB         \
    \     3\n           Table 5.1.  Number of TCP Connections versus TCP RWND\n  \
    \ The TCP Transfer Time Ratio metric is useful when conducting\n   multiple-connection\
    \ tests.  Each connection SHOULD be configured to\n   transfer payloads of the\
    \ same size (e.g., 100 MB); then, the TCP\n   Transfer Time Ratio provides a simple\
    \ metric to verify the actual\n   versus expected results.\n   Note that the TCP\
    \ transfer time is the time required for each\n   connection to complete the transfer\
    \ of the predetermined payload\n   size.  From the previous table, the 64-KB window\
    \ is considered.  Each\n   of the 5 TCP connections would be configured to transfer\
    \ 100 MB, and\n   each one should obtain a maximum of 100 Mbps.  So for this example,\n\
    \   the 100-MB payload should be transferred across the connections in\n   approximately\
    \ 8 seconds (which would be the Ideal TCP Transfer Time\n   under these conditions).\n\
    \   Additionally, the TCP Efficiency Percentage metric MUST be computed\n   for\
    \ each connection as defined in Section 4.2.\n"
- title: 5.2.  Results Interpretation
  contents:
  - "5.2.  Results Interpretation\n   At the end, a TCP Throughput Test Device (TCP\
    \ TTD) SHOULD generate a\n   report with the calculated BDP and a set of Window\
    \ size experiments.\n   Window size refers to the minimum of the Send Socket Buffer\
    \ and TCP\n   RWND.  The report SHOULD include TCP Throughput results for each\
    \ TCP\n   Window size tested.  The goal is to provide achievable versus actual\n\
    \   TCP Throughput results with respect to the TCP Window size when no\n   fragmentation\
    \ occurs.  The report SHOULD also include the results for\n   the 3 metrics defined\
    \ in Section 4.  The goal is to provide a clear\n   relationship between these\
    \ 3 metrics and user experience.  As an\n   example, for the same results in regard\
    \ to Transfer Time Ratio, a\n   better TCP Efficiency could be obtained at the\
    \ cost of higher Buffer\n   Delays.\n   For cases where the test results are not\
    \ equal to the ideal values,\n   some possible causes are as follows:\n   - Network\
    \ congestion causing packet loss, which may be inferred from\n     a poor TCP\
    \ Efficiency % (i.e., higher TCP Efficiency % = less\n     packet loss).\n   -\
    \ Network congestion causing an increase in RTT, which may be\n     inferred from\
    \ the Buffer Delay Percentage (i.e., 0% = no increase\n     in RTT over baseline).\n\
    \   - Intermediate network devices that actively regenerate the TCP\n     connection\
    \ and can alter TCP RWND size, MTU, etc.\n   - Rate limiting by policing instead\
    \ of shaping.\n   - Maximum TCP Buffer Space.  All operating systems have a global\n\
    \     mechanism to limit the quantity of system memory to be used by TCP\n   \
    \  connections.  On some systems, each connection is subject to a\n     memory\
    \ limit that is applied to the total memory used for input\n     data, output\
    \ data, and controls.  On other systems, there are\n     separate limits for input\
    \ and output buffer spaces per connection.\n     Client/server IP hosts might\
    \ be configured with Maximum TCP Buffer\n     Space limits that are far too small\
    \ for high-performance networks.\n   - Socket Buffer sizes.  Most operating systems\
    \ support separate\n     per-connection send and receive buffer limits that can\
    \ be adjusted\n     as long as they stay within the maximum memory limits.  These\n\
    \     socket buffers MUST be large enough to hold a full BDP of TCP Bytes\n  \
    \   plus some overhead.  There are several methods that can be used to\n     adjust\
    \ Socket Buffer sizes, but TCP Auto-Tuning automatically\n     adjusts these as\
    \ needed to optimally balance TCP performance and\n     memory usage.\n     It\
    \ is important to note that Auto-Tuning is enabled by default in\n     LINUX since\
    \ kernel release 2.6.6 and in UNIX since FreeBSD 7.0.  It\n     is also enabled\
    \ by default in Windows since Vista and in Mac since\n     OS X version 10.5 (Leopard).\
    \  Over-buffering can cause some\n     applications to behave poorly, typically\
    \ causing sluggish\n     interactive response and introducing the risk of running\
    \ the system\n     out of memory.  Large default socket buffers have to be considered\n\
    \     carefully on multi-user systems.\n   - TCP Window Scale option [RFC1323].\
    \  This option enables TCP to\n     support large BDP paths.  It provides a scale\
    \ factor that is\n     required for TCP to support window sizes larger than 64\
    \ KB.  Most\n     systems automatically request WSCALE under some conditions,\
    \ such as\n     when the Receive Socket Buffer is larger than 64 KB or when the\n\
    \     other end of the TCP connection requests it first.  WSCALE can only\n  \
    \   be negotiated during the 3-way handshake.  If either end fails to\n     request\
    \ WSCALE or requests an insufficient value, it cannot be\n     renegotiated. \
    \ Different systems use different algorithms to select\n     WSCALE, but it is\
    \ very important to have large enough buffer sizes.\n     Note that under these\
    \ constraints, a client application wishing to\n     send data at high rates may\
    \ need to set its own receive buffer to\n     something larger than 64 KBytes\
    \ before it opens the connection, to\n     ensure that the server properly negotiates\
    \ WSCALE.  A system\n     administrator might have to explicitly enable [RFC1323]\
    \ extensions.\n     Otherwise, the client/server IP host would not support TCP\
    \ Window\n     sizes (BDP) larger than 64 KB.  Most of the time, performance gains\n\
    \     will be obtained by enabling this option in LFNs.\n   - TCP Timestamps option\
    \ [RFC1323].  This feature provides better\n     measurements of the Round-Trip\
    \ Time and protects TCP from data\n     corruption that might occur if packets\
    \ are delivered so late that\n     the sequence numbers wrap before they are delivered.\
    \  Wrapped\n     sequence numbers do not pose a serious risk below 100 Mbps, but\
    \ the\n     risk increases at higher data rates.  Most of the time, performance\n\
    \     gains will be obtained by enabling this option in Gigabit-bandwidth\n  \
    \   networks.\n   - TCP Selective Acknowledgments (SACK) option [RFC2018].  This\
    \ allows\n     a TCP receiver to inform the sender about exactly which data\n\
    \     segment is missing and needs to be retransmitted.  Without SACK,\n     TCP\
    \ has to estimate which data segment is missing, which works just\n     fine if\
    \ all losses are isolated (i.e., only one loss in any given\n     round trip).\
    \  Without SACK, TCP takes a very long time to recover\n     after multiple and\
    \ consecutive losses.  SACK is now supported by\n     most operating systems,\
    \ but it may have to be explicitly enabled by\n     the system administrator.\
    \  In networks with unknown load and error\n     patterns, TCP SACK will improve\
    \ throughput performance.  On the\n     other hand, security appliance vendors\
    \ might have implemented TCP\n     randomization without considering TCP SACK,\
    \ and under such\n     circumstances, SACK might need to be disabled in the client/server\n\
    \     IP hosts until the vendor corrects the issue.  Also, poorly\n     implemented\
    \ SACK algorithms might cause extreme CPU loads and might\n     need to be disabled.\n\
    \   - Path MTU.  The client/server IP host system SHOULD use the largest\n   \
    \  possible MTU for the path.  This may require enabling Path MTU\n     Discovery\
    \ [RFC1191] and [RFC4821].  Since [RFC1191] is flawed, Path\n     MTU Discovery\
    \ is sometimes not enabled by default and may need to\n     be explicitly enabled\
    \ by the system administrator.  [RFC4821]\n     describes a new, more robust algorithm\
    \ for MTU discovery and ICMP\n     black hole recovery.\n   - TOE (TCP Offload\
    \ Engine).  Some recent Network Interface Cards\n     (NICs) are equipped with\
    \ drivers that can do part or all of the\n     TCP/IP protocol processing.  TOE\
    \ implementations require additional\n     work (i.e., hardware-specific socket\
    \ manipulation) to set up and\n     tear down connections.  Because TOE NIC configuration\
    \ parameters\n     are vendor-specific and not necessarily RFC-compliant, they\
    \ are\n     poorly integrated with UNIX and LINUX.  Occasionally, TOE might\n\
    \     need to be disabled in a server because its NIC does not have\n     enough\
    \ memory resources to buffer thousands of connections.\n   Note that both ends\
    \ of a TCP connection MUST be properly tuned.\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   Measuring TCP network performance raises security\
    \ concerns.  Metrics\n   produced within this framework may create security issues.\n"
- title: 6.1.  Denial-of-Service Attacks
  contents:
  - "6.1.  Denial-of-Service Attacks\n   TCP network performance metrics, as defined\
    \ in this document, attempt\n   to fill the NUT with a stateful connection.  However,\
    \ since the test\n   MAY use stateless IP streams as specified in Section 3.2.2,\
    \ it might\n   appear to network operators to be a denial-of-service attack. \
    \ Thus,\n   as mentioned at the beginning of Section 3, TCP Throughput testing\n\
    \   may require cooperation between the end-user customer and the network\n  \
    \ provider.\n"
- title: 6.2.  User Data Confidentiality
  contents:
  - "6.2.  User Data Confidentiality\n   Metrics within this framework generate packets\
    \ from a sample, rather\n   than taking samples based on user data.  Thus, our\
    \ framework does not\n   threaten user data confidentiality.\n"
- title: 6.3.  Interference with Metrics
  contents:
  - "6.3.  Interference with Metrics\n   The security considerations that apply to\
    \ any active measurement of\n   live networks are relevant here as well.  See\
    \ [RFC4656] and\n   [RFC5357].\n"
- title: 7.  Acknowledgments
  contents:
  - "7.  Acknowledgments\n   Thanks to Lars Eggert, Al Morton, Matt Mathis, Matt Zekauskas,\
    \ Yaakov\n   Stein, and Loki Jorgenson for many good comments and for pointing\
    \ us\n   to great sources of information pertaining to past works in the TCP\n\
    \   capacity area.\n"
- title: 8.  Normative References
  contents:
  - "8.  Normative References\n   [RFC1191]   Mogul, J. and S. Deering, \"Path MTU\
    \ discovery\", RFC 1191,\n               November 1990.\n   [RFC1323]   Jacobson,\
    \ V., Braden, R., and D. Borman, \"TCP Extensions\n               for High Performance\"\
    , RFC 1323, May 1992.\n   [RFC2018]   Mathis, M., Mahdavi, J., Floyd, S., and\
    \ A. Romanow, \"TCP\n               Selective Acknowledgment Options\", RFC 2018,\n\
    \               October 1996.\n   [RFC2119]   Bradner, S., \"Key words for use\
    \ in RFCs to Indicate\n               Requirement Levels\", BCP 14, RFC 2119,\
    \ March 1997.\n   [RFC2544]   Bradner, S. and J. McQuaid, \"Benchmarking Methodology\
    \ for\n               Network Interconnect Devices\", RFC 2544, March 1999.\n\
    \   [RFC4656]   Shalunov, S., Teitelbaum, B., Karp, A., Boote, J., and M.\n  \
    \             Zekauskas, \"A One-way Active Measurement Protocol\n           \
    \    (OWAMP)\", RFC 4656, September 2006.\n   [RFC4821]   Mathis, M. and J. Heffner,\
    \ \"Packetization Layer Path MTU\n               Discovery\", RFC 4821, March\
    \ 2007.\n   [RFC4898]   Mathis, M., Heffner, J., and R. Raghunarayan, \"TCP\n\
    \               Extended Statistics MIB\", RFC 4898, May 2007.\n   [RFC5136] \
    \  Chimento, P. and J. Ishac, \"Defining Network Capacity\",\n               RFC\
    \ 5136, February 2008.\n   [RFC5357]   Hedayat, K., Krzanowski, R., Morton, A.,\
    \ Yum, K., and J.\n               Babiarz, \"A Two-Way Active Measurement Protocol\
    \ (TWAMP)\",\n               RFC 5357, October 2008.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Barry Constantine\n   JDSU, Test and Measurement Division\n\
    \   One Milesone Center Court\n   Germantown, MD  20876-7100\n   USA\n   Phone:\
    \ +1 240 404 2227\n   EMail: barry.constantine@jdsu.com\n   Gilles Forget\n  \
    \ Independent Consultant to Bell Canada\n   308, rue de Monaco, St-Eustache\n\
    \   Qc. J7P-4T5  CANADA\n   Phone: (514) 895-8212\n   EMail: gilles.forget@sympatico.ca\n\
    \   Ruediger Geib\n   Heinrich-Hertz-Strasse 3-7\n   Darmstadt, 64295  Germany\n\
    \   Phone: +49 6151 5812747\n   EMail: Ruediger.Geib@telekom.de\n   Reinhard Schrage\n\
    \   Osterende 7\n   Seelze, 30926\n   Germany\n   Schrage Consulting\n   Phone:\
    \ +49 (0) 5137 909540\n   EMail: reinhard@schrageconsult.com\n"
