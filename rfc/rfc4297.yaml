- title: __initial_text__
  contents:
  - '      Remote Direct Memory Access (RDMA) over IP Problem Statement

    '
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2005).\n"
- title: Abstract
  contents:
  - "Abstract\n   Overhead due to the movement of user data in the end-system network\n\
    \   I/O processing path at high speeds is significant, and has limited\n   the\
    \ use of Internet protocols in interconnection networks, and the\n   Internet\
    \ itself -- especially where high bandwidth, low latency,\n   and/or low overhead\
    \ are required by the hosted application.\n   This document examines this overhead,\
    \ and addresses an architectural,\n   IP-based \"copy avoidance\" solution for\
    \ its elimination, by enabling\n   Remote Direct Memory Access (RDMA).\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................2\n\
    \   2. The High Cost of Data Movement Operations in Network I/O ........4\n  \
    \    2.1. Copy avoidance improves processing overhead. ...............5\n   3.\
    \ Memory bandwidth is the root cause of the problem. ..............6\n   4. High\
    \ copy overhead is problematic for many key Internet\n      applications. ...................................................8\n\
    \   5. Copy Avoidance Techniques ......................................10\n  \
    \    5.1. A Conceptual Framework: DDP and RDMA ......................11\n   6.\
    \ Conclusions ....................................................12\n   7. Security\
    \ Considerations ........................................12\n   8. Terminology\
    \ ....................................................14\n   9. Acknowledgements\
    \ ...............................................14\n   10. Informative References\
    \ ........................................15\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document considers the problem of high host processing\
    \ overhead\n   associated with the movement of user data to and from the network\n\
    \   interface under high speed conditions.  This problem is often\n   referred\
    \ to as the \"I/O bottleneck\" [CT90].  More specifically, the\n   source of high\
    \ overhead that is of interest here is data movement\n   operations, i.e., copying.\
    \  The throughput of a system may therefore\n   be limited by the overhead of\
    \ this copying.  This issue is not to be\n   confused with TCP offload, which\
    \ is not addressed here.  High speed\n   refers to conditions where the network\
    \ link speed is high, relative\n   to the bandwidths of the host CPU and memory.\
    \  With today's computer\n   systems, one Gigabit per second (Gbits/s) and over\
    \ is considered high\n   speed.\n   High costs associated with copying are an\
    \ issue primarily for large\n   scale systems.  Although smaller systems such\
    \ as rack-mounted PCs and\n   small workstations would benefit from a reduction\
    \ in copying\n   overhead, the benefit to smaller machines will be primarily in\
    \ the\n   next few years as they scale the amount of bandwidth they handle.\n\
    \   Today, it is large system machines with high bandwidth feeds, usually\n  \
    \ multiprocessors and clusters, that are adversely affected by copying\n   overhead.\
    \  Examples of such machines include all varieties of\n   servers: database servers,\
    \ storage servers, application servers for\n   transaction processing, for e-commerce,\
    \ and web serving, content\n   distribution, video distribution, backups, data\
    \ mining and decision\n   support, and scientific computing.\n   Note that such\
    \ servers almost exclusively service many concurrent\n   sessions (transport connections),\
    \ which, in aggregate, are\n   responsible for > 1 Gbits/s of communication. \
    \ Nonetheless, the cost\n   of copying overhead for a particular load is the same\
    \ whether from\n   few or many sessions.\n   The I/O bottleneck, and the role\
    \ of data movement operations, have\n   been widely studied in research and industry\
    \ over the last\n   approximately 14 years, and we draw freely on these results.\n\
    \   Historically, the I/O bottleneck has received attention whenever new\n   networking\
    \ technology has substantially increased line rates: 100\n   Megabit per second\
    \ (Mbits/s) Fast Ethernet and Fibre Distributed Data\n   Interface [FDDI], 155\
    \ Mbits/s Asynchronous Transfer Mode [ATM], 1\n   Gbits/s Ethernet.  In earlier\
    \ speed transitions, the availability of\n   memory bandwidth allowed the I/O\
    \ bottleneck issue to be deferred.\n   Now however, this is no longer the case.\
    \  While the I/O problem is\n   significant at 1 Gbits/s, it is the introduction\
    \ of 10 Gbits/s\n   Ethernet which is motivating an upsurge of activity in industry\
    \ and\n   research [IB, VI, CGY01, Ma02, MAF+02].\n   Because of high overhead\
    \ of end-host processing in current\n   implementations, the TCP/IP protocol stack\
    \ is not used for high speed\n   transfer.  Instead, special purpose network fabrics,\
    \ using a\n   technology generally known as Remote Direct Memory Access (RDMA),\n\
    \   have been developed and are widely used.  RDMA is a set of mechanisms\n  \
    \ that allow the network adapter, under control of the application, to\n   steer\
    \ data directly into and out of application buffers.  Examples of\n   such interconnection\
    \ fabrics include Fibre Channel [FIBRE] for block\n   storage transfer, Virtual\
    \ Interface Architecture [VI] for database\n   clusters, and Infiniband [IB],\
    \ Compaq Servernet [SRVNET], and\n   Quadrics [QUAD] for System Area Networks.\
    \  These link level\n   technologies limit application scaling in both distance\
    \ and size,\n   meaning that the number of nodes cannot be arbitrarily large.\n\
    \   This problem statement substantiates the claim that in network I/O\n   processing,\
    \ high overhead results from data movement operations,\n   specifically copying;\
    \ and that copy avoidance significantly decreases\n   this processing overhead.\
    \  It describes when and why the high\n   processing overheads occur, explains\
    \ why the overhead is problematic,\n   and points out which applications are most\
    \ affected.\n   The document goes on to discuss why the problem is relevant to\
    \ the\n   Internet and to Internet-based applications.  Applications that\n  \
    \ store, manage, and distribute the information of the Internet are\n   well suited\
    \ to applying the copy avoidance solution.  They will\n   benefit by avoiding\
    \ high processing overheads, which removes limits\n   to the available scaling\
    \ of tiered end-systems.  Copy avoidance also\n   eliminates latency for these\
    \ systems, which can further benefit\n   effective distributed processing.\n \
    \  In addition, this document introduces an architectural approach to\n   solving\
    \ the problem, which is developed in detail in [BT05].  It also\n   discusses\
    \ how the proposed technology may introduce security concerns\n   and how they\
    \ should be addressed.\n   Finally, this document includes a Terminology section\
    \ to aid as a\n   reference for several new terms introduced by RDMA.\n"
- title: 2.  The High Cost of Data Movement Operations in Network I/O
  contents:
  - "2.  The High Cost of Data Movement Operations in Network I/O\n   A wealth of\
    \ data from research and industry shows that copying is\n   responsible for substantial\
    \ amounts of processing overhead.  It\n   further shows that even in carefully\
    \ implemented systems, eliminating\n   copies significantly reduces the overhead,\
    \ as referenced below.\n   Clark et al. [CJRS89] in 1989 shows that TCP [Po81]\
    \ overhead\n   processing is attributable to both operating system costs (such\
    \ as\n   interrupts, context switches, process management, buffer management,\n\
    \   timer management) and the costs associated with processing individual\n  \
    \ bytes (specifically, computing the checksum and moving data in\n   memory).\
    \  They found that moving data in memory is the more important\n   of the costs,\
    \ and their experiments show that memory bandwidth is the\n   greatest source\
    \ of limitation.  In the data presented [CJRS89], 64%\n   of the measured microsecond\
    \ overhead was attributable to data\n   touching operations, and 48% was accounted\
    \ for by copying.  The\n   system measured Berkeley TCP on a Sun-3/60 using 1460\
    \ Byte Ethernet\n   packets.\n   In a well-implemented system, copying can occur\
    \ between the network\n   interface and the kernel, and between the kernel and\
    \ application\n   buffers; there are two copies, each of which are two memory\
    \ bus\n   crossings, for read and write.  Although in certain circumstances it\n\
    \   is possible to do better, usually two copies are required on receive.\n  \
    \ Subsequent work has consistently shown the same phenomenon as the\n   earlier\
    \ Clark study.  A number of studies report results that data-\n   touching operations,\
    \ checksumming and data movement, dominate the\n   processing costs for messages\
    \ longer than 128 Bytes [BS96, CGY01,\n   Ch96, CJRS89, DAPP93, KP96].  For smaller\
    \ sized messages, per-packet\n   overheads dominate [KP96, CGY01].\n   The percentage\
    \ of overhead due to data-touching operations increases\n   with packet size,\
    \ since time spent on per-byte operations scales\n   linearly with message size\
    \ [KP96].  For example, Chu [Ch96] reported\n   substantial per-byte latency costs\
    \ as a percentage of total\n   networking software costs for an MTU size packet\
    \ on a SPARCstation/20\n   running memory-to-memory TCP tests over networks with\
    \ 3 different MTU\n   sizes.  The percentage of total software costs attributable\
    \ to\n   per-byte operations were:\n      1500 Byte Ethernet 18-25%\n      4352\
    \ Byte FDDI     35-50%\n      9180 Byte ATM      55-65%\n   Although many studies\
    \ report results for data-touching operations,\n   including checksumming and\
    \ data movement together, much work has\n   focused just on copying [BS96, Br99,\
    \ Ch96, TK95].  For example,\n   [KP96] reports results that separate processing\
    \ times for checksum\n   from data movement operations.  For the 1500 Byte Ethernet\
    \ size, 20%\n   of total processing overhead time is attributable to copying.\
    \  The\n   study used 2 DECstations 5000/200 connected by an FDDI network.  (In\n\
    \   this study, checksum accounts for 30% of the processing time.)\n"
- title: 2.1.  Copy avoidance improves processing overhead.
  contents:
  - "2.1.  Copy avoidance improves processing overhead.\n   A number of studies show\
    \ that eliminating copies substantially\n   reduces overhead.  For example, results\
    \ from copy-avoidance in the\n   IO-Lite system [PDZ99], which aimed at improving\
    \ web server\n   performance, show a throughput increase of 43% over an optimized\
    \ web\n   server, and 137% improvement over an Apache server.  The system was\n\
    \   implemented in a 4.4BSD-derived UNIX kernel, and the experiments used\n  \
    \ a server system based on a 333MHz Pentium II PC connected to a\n   switched\
    \ 100 Mbits/s Fast Ethernet.\n   There are many other examples where elimination\
    \ of copying using a\n   variety of different approaches showed significant improvement\
    \ in\n   system performance [CFF+94, DP93, EBBV95, KSZ95, TK95, Wa97].  We\n \
    \  will discuss the results of one of these studies in detail in order\n   to\
    \ clarify the significant degree of improvement produced by copy\n   avoidance\
    \ [Ch02].\n   Recent work by Chase et al. [CGY01], measuring CPU utilization,\
    \ shows\n   that avoiding copies reduces CPU time spent on data access from 24%\n\
    \   to 15% at 370 Mbits/s for a 32 KBytes MTU using an AlphaStation\n   XP1000\
    \ and a Myrinet adapter [BCF+95].  This is an absolute\n   improvement of 9% due\
    \ to copy avoidance.\n   The total CPU utilization was 35%, with data access accounting\
    \ for\n   24%.  Thus, the relative importance of reducing copies is 26%.  At\n\
    \   370 Mbits/s, the system is not very heavily loaded.  The relative\n   improvement\
    \ in achievable bandwidth is 34%.  This is the improvement\n   we would see if\
    \ copy avoidance were added when the machine was\n   saturated by network I/O.\n\
    \   Note that improvement from the optimization becomes more important if\n  \
    \ the overhead it targets is a larger share of the total cost.  This is\n   what\
    \ happens if other sources of overhead, such as checksumming, are\n   eliminated.\
    \  In [CGY01], after removing checksum overhead, copy\n   avoidance reduces CPU\
    \ utilization from 26% to 10%.  This is a 16%\n   absolute reduction, a 61% relative\
    \ reduction, and a 160% relative\n   improvement in achievable bandwidth.\n  \
    \ In fact, today's network interface hardware commonly offloads the\n   checksum,\
    \ which removes the other source of per-byte overhead.  They\n   also coalesce\
    \ interrupts to reduce per-packet costs.  Thus, today\n   copying costs account\
    \ for a relatively larger part of CPU utilization\n   than previously, and therefore\
    \ relatively more benefit is to be\n   gained in reducing them.  (Of course this\
    \ argument would be specious\n   if the amount of overhead were insignificant,\
    \ but it has been shown\n   to be substantial.  [BS96, Br99, Ch96, KP96, TK95])\n"
- title: 3.  Memory bandwidth is the root cause of the problem.
  contents:
  - "3.  Memory bandwidth is the root cause of the problem.\n   Data movement operations\
    \ are expensive because memory bandwidth is\n   scarce relative to network bandwidth\
    \ and CPU bandwidth [PAC+97].\n   This trend existed in the past and is expected\
    \ to continue into the\n   future [HP97, STREAM], especially in large multiprocessor\
    \ systems.\n   With copies crossing the bus twice per copy, network processing\n\
    \   overhead is high whenever network bandwidth is large in comparison to\n  \
    \ CPU and memory bandwidths.  Generally, with today's end-systems, the\n   effects\
    \ are observable at network speeds over 1 Gbits/s.  In fact,\n   with multiple\
    \ bus crossings it is possible to see the bus bandwidth\n   being the limiting\
    \ factor for throughput.  This prevents such an\n   end-system from simultaneously\
    \ achieving full network bandwidth and\n   full application performance.\n   A\
    \ common question is whether an increase in CPU processing power\n   alleviates\
    \ the problem of high processing costs of network I/O.  The\n   answer is no,\
    \ it is the memory bandwidth that is the issue.  Faster\n   CPUs do not help if\
    \ the CPU spends most of its time waiting for\n   memory [CGY01].\n   The widening\
    \ gap between microprocessor performance and memory\n   performance has long been\
    \ a widely recognized and well-understood\n   problem [PAC+97].  Hennessy [HP97]\
    \ shows microprocessor performance\n   grew from 1980-1998 at 60% per year, while\
    \ the access time to DRAM\n   improved at 10% per year, giving rise to an increasing\
    \ \"processor-\n   memory performance gap\".\n   Another source of relevant data\
    \ is the STREAM Benchmark Reference\n   Information website, which provides information\
    \ on the STREAM\n   benchmark [STREAM].  The benchmark is a simple synthetic benchmark\n\
    \   program that measures sustainable memory bandwidth (in MBytes/s) and\n   the\
    \ corresponding computation rate for simple vector kernels measured\n   in MFLOPS.\
    \  The website tracks information on sustainable memory\n   bandwidth for hundreds\
    \ of machines and all major vendors.\n   Results show measured system performance\
    \ statistics.  Processing\n   performance from 1985-2001 increased at 50% per\
    \ year on average, and\n   sustainable memory bandwidth from 1975 to 2001 increased\
    \ at 35% per\n   year, on average, over all the systems measured.  A similar 15%\
    \ per\n   year lead of processing bandwidth over memory bandwidth shows up in\n\
    \   another statistic, machine balance [Mc95], a measure of the relative\n   rate\
    \ of CPU to memory bandwidth (FLOPS/cycle) / (sustained memory\n   ops/cycle)\
    \ [STREAM].\n   Network bandwidth has been increasing about 10-fold roughly every\
    \ 8\n   years, which is a 40% per year growth rate.\n   A typical example illustrates\
    \ that the memory bandwidth compares\n   unfavorably with link speed.  The STREAM\
    \ benchmark shows that a\n   modern uniprocessor PC, for example the 1.2 GHz Athlon\
    \ in 2001, will\n   move the data 3 times in doing a receive operation: once for\
    \ the\n   network interface to deposit the data in memory, and twice for the\n\
    \   CPU to copy the data.  With 1 GBytes/s of memory bandwidth, meaning\n   one\
    \ read or one write, the machine could handle approximately 2.67\n   Gbits/s of\
    \ network bandwidth, one third the copy bandwidth.  But this\n   assumes 100%\
    \ utilization, which is not possible, and more importantly\n   the machine would\
    \ be totally consumed!  (A rule of thumb for\n   databases is that 20% of the\
    \ machine should be required to service\n   I/O, leaving 80% for the database\
    \ application.  And, the less, the\n   better.)\n   In 2001, 1 Gbits/s links were\
    \ common.  An application server may\n   typically have two 1 Gbits/s connections:\
    \ one connection backend to a\n   storage server and one front-end, say for serving\
    \ HTTP [FGM+99].\n   Thus, the communications could use 2 Gbits/s.  In our typical\n\
    \   example, the machine could handle 2.7 Gbits/s at its theoretical\n   maximum\
    \ while doing nothing else.  This means that the machine\n   basically could not\
    \ keep up with the communication demands in 2001;\n   with the relative growth\
    \ trends, the situation only gets worse.\n"
- title: 4.  High copy overhead is problematic for many key Internet
  contents:
  - "4.  High copy overhead is problematic for many key Internet\n    applications.\n\
    \   If a significant portion of resources on an application machine is\n   consumed\
    \ in network I/O rather than in application processing, it\n   makes it difficult\
    \ for the application to scale, i.e., to handle more\n   clients, to offer more\
    \ services.\n   Several years ago the most affected applications were streaming\n\
    \   multimedia, parallel file systems, and supercomputing on clusters\n   [BS96].\
    \  In addition, today the applications that suffer from copying\n   overhead are\
    \ more central in Internet computing -- they store,\n   manage, and distribute\
    \ the information of the Internet and the\n   enterprise.  They include database\
    \ applications doing transaction\n   processing, e-commerce, web serving, decision\
    \ support, content\n   distribution, video distribution, and backups.  Clusters\
    \ are\n   typically used for this category of application, since they have\n \
    \  advantages of availability and scalability.\n   Today these applications, which\
    \ provide and manage Internet and\n   corporate information, are typically run\
    \ in data centers that are\n   organized into three logical tiers.  One tier is\
    \ typically a set of\n   web servers connecting to the WAN.  The second tier is\
    \ a set of\n   application servers that run the specific applications usually\
    \ on\n   more powerful machines, and the third tier is backend databases.\n  \
    \ Physically, the first two tiers -- web server and application server\n   --\
    \ are usually combined [Pi01].  For example, an e-commerce server\n   communicates\
    \ with a database server and with a customer site, or a\n   content distribution\
    \ server connects to a server farm, or an OLTP\n   server connects to a database\
    \ and a customer site.\n   When network I/O uses too much memory bandwidth, performance\
    \ on\n   network paths between tiers can suffer.  (There might also be\n   performance\
    \ issues on Storage Area Network paths used either by the\n   database tier or\
    \ the application tier.)  The high overhead from\n   network-related memory copies\
    \ diverts system resources from other\n   application processing.  It also can\
    \ create bottlenecks that limit\n   total system performance.\n   There is high\
    \ motivation to maximize the processing capacity of each\n   CPU because scaling\
    \ by adding CPUs, one way or another, has\n   drawbacks.  For example, adding\
    \ CPUs to a multiprocessor will not\n   necessarily help because a multiprocessor\
    \ improves performance only\n   when the memory bus has additional bandwidth to\
    \ spare.  Clustering\n   can add additional complexity to handling the applications.\n\
    \   In order to scale a cluster or multiprocessor system, one must\n   proportionately\
    \ scale the interconnect bandwidth.  Interconnect\n   bandwidth governs the performance\
    \ of communication-intensive parallel\n   applications; if this (often expressed\
    \ in terms of \"bisection\n   bandwidth\") is too low, adding additional processors\
    \ cannot improve\n   system throughput.  Interconnect latency can also limit the\n\
    \   performance of applications that frequently share data between\n   processors.\n\
    \   So, excessive overheads on network paths in a \"scalable\" system both\n \
    \  can require the use of more processors than optimal, and can reduce\n   the\
    \ marginal utility of those additional processors.\n   Copy avoidance scales a\
    \ machine upwards by removing at least two-\n   thirds of the bus bandwidth load\
    \ from the \"very best\" 1-copy (on\n   receive) implementations, and removes\
    \ at least 80% of the bandwidth\n   overhead from the 2-copy implementations.\n\
    \   The removal of bus bandwidth requirements, in turn, removes\n   bottlenecks\
    \ from the network processing path and increases the\n   throughput of the machine.\
    \  On a machine with limited bus bandwidth,\n   the advantages of removing this\
    \ load is immediately evident, as the\n   host can attain full network bandwidth.\
    \  Even on a machine with bus\n   bandwidth adequate to sustain full network bandwidth,\
    \ removal of bus\n   bandwidth load serves to increase the availability of the\
    \ machine for\n   the processing of user applications, in some cases dramatically.\n\
    \   An example showing poor performance with copies and improved scaling\n   with\
    \ copy avoidance is illustrative.  The IO-Lite work [PDZ99] shows\n   higher server\
    \ throughput servicing more clients using a zero-copy\n   system.  In an experiment\
    \ designed to mimic real world web conditions\n   by simulating the effect of\
    \ TCP WAN connections on the server, the\n   performance of 3 servers was compared.\
    \  One server was Apache,\n   another was an optimized server called Flash, and\
    \ the third was the\n   Flash server running IO-Lite, called Flash-Lite with zero\
    \ copy.  The\n   measurement was of throughput in requests/second as a function\
    \ of the\n   number of slow background clients that could be served.  As the table\n\
    \   shows, Flash-Lite has better throughput, especially as the number of\n   clients\
    \ increases.\n              Apache              Flash         Flash-Lite\n   \
    \           ------              -----         ----------\n   #Clients   Throughput\
    \ reqs/s   Throughput    Throughput\n   0          520                 610   \
    \        890\n   16         390                 490           890\n   32     \
    \    360                 490           850\n   64         360                \
    \ 490           890\n   128        310                 450           880\n   256\
    \        310                 440           820\n   Traditional Web servers (which\
    \ mostly send data and can keep most of\n   their content in the file cache) are\
    \ not the worst case for copy\n   overhead.  Web proxies (which often receive\
    \ as much data as they\n   send) and complex Web servers based on System Area\
    \ Networks or\n   multi-tier systems will suffer more from copy overheads than\
    \ in the\n   example above.\n"
- title: 5.  Copy Avoidance Techniques
  contents:
  - "5.  Copy Avoidance Techniques\n   There have been extensive research investigation\
    \ and industry\n   experience with two main alternative approaches to eliminating\
    \ data\n   movement overhead, often along with improving other Operating System\n\
    \   processing costs.  In one approach, hardware and/or software changes\n   within\
    \ a single host reduce processing costs.  In another approach,\n   memory-to-memory\
    \ networking [MAF+02], the exchange of explicit data\n   placement information\
    \ between hosts allows them to reduce processing\n   costs.\n   The single host\
    \ approaches range from new hardware and software\n   architectures [KSZ95, Wa97,\
    \ DWB+93] to new or modified software\n   systems [BS96, Ch96, TK95, DP93, PDZ99].\
    \  In the approach based on\n   using a networking protocol to exchange information,\
    \ the network\n   adapter, under control of the application, places data directly\
    \ into\n   and out of application buffers, reducing the need for data movement.\n\
    \   Commonly this approach is called RDMA, Remote Direct Memory Access.\n   As\
    \ discussed below, research and industry experience has shown that\n   copy avoidance\
    \ techniques within the receiver processing path alone\n   have proven to be problematic.\
    \  The research special purpose host\n   adapter systems had good performance\
    \ and can be seen as precursors\n   for the commercial RDMA-based adapters [KSZ95,\
    \ DWB+93].  In software,\n   many implementations have successfully achieved zero-copy\
    \ transmit,\n   but few have accomplished zero-copy receive.  And those that have\n\
    \   done so make strict alignment and no-touch requirements on the\n   application,\
    \ greatly reducing the portability and usefulness of the\n   implementation.\n\
    \   In contrast, experience has proven satisfactory with memory-to-memory\n  \
    \ systems that permit RDMA; performance has been good and there have\n   not been\
    \ system or networking difficulties.  RDMA is a single\n   solution.  Once implemented,\
    \ it can be used with any OS and machine\n   architecture, and it does not need\
    \ to be revised when either of these\n   are changed.\n   In early work, one goal\
    \ of the software approaches was to show that\n   TCP could go faster with appropriate\
    \ OS support [CJRS89, CFF+94].\n   While this goal was achieved, further investigation\
    \ and experience\n   showed that, though possible to craft software solutions,\
    \ specific\n   system optimizations have been complex, fragile, extremely\n  \
    \ interdependent with other system parameters in complex ways, and\n   often of\
    \ only marginal improvement [CFF+94, CGY01, Ch96, DAPP93,\n   KSZ95, PDZ99]. \
    \ The network I/O system interacts with other aspects\n   of the Operating System\
    \ such as machine architecture and file I/O,\n   and disk I/O [Br99, Ch96, DP93].\n\
    \   For example, the Solaris Zero-Copy TCP work [Ch96], which relies on\n   page\
    \ remapping, shows that the results are highly interdependent with\n   other systems,\
    \ such as the file system, and that the particular\n   optimizations are specific\
    \ for particular architectures, meaning that\n   for each variation in architecture,\
    \ optimizations must be re-crafted\n   [Ch96].\n   With RDMA, application I/O\
    \ buffers are mapped directly, and the\n   authorized peer may access it without\
    \ incurring additional processing\n   overhead.  When RDMA is implemented in hardware,\
    \ arbitrary data\n   movement can be performed without involving the host CPU\
    \ at all.\n   A number of research projects and industry products have been based\n\
    \   on the memory-to-memory approach to copy avoidance.  These include\n   U-Net\
    \ [EBBV95], SHRIMP [BLA+94], Hamlyn [BJM+96], Infiniband [IB],\n   Winsock Direct\
    \ [Pi01].  Several memory-to-memory systems have been\n   widely used and have\
    \ generally been found to be robust, to have good\n   performance, and to be relatively\
    \ simple to implement.  These include\n   VI [VI], Myrinet [BCF+95], Quadrics\
    \ [QUAD], Compaq/Tandem Servernet\n   [SRVNET].  Networks based on these memory-to-memory\
    \ architectures\n   have been used widely in scientific applications and in data\
    \ centers\n   for block storage, file system access, and transaction processing.\n\
    \   By exporting direct memory access \"across the wire\", applications may\n\
    \   direct the network stack to manage all data directly from application\n  \
    \ buffers.  A large and growing class that takes advantage of such\n   capabilities\
    \ of applications has already emerged.  It includes all\n   the major databases,\
    \ as well as network protocols such as Sockets\n   Direct [SDP].\n"
- title: '5.1.  A Conceptual Framework: DDP and RDMA'
  contents:
  - "5.1.  A Conceptual Framework: DDP and RDMA\n   An RDMA solution can be usefully\
    \ viewed as being comprised of two\n   distinct components: \"direct data placement\
    \ (DDP)\" and \"remote direct\n   memory access (RDMA) semantics\".  They are\
    \ distinct in purpose and\n   also in practice -- they may be implemented as separate\
    \ protocols.\n   The more fundamental of the two is the direct data placement\n\
    \   facility.  This is the means by which memory is exposed to the remote\n  \
    \ peer in an appropriate fashion, and the means by which the peer may\n   access\
    \ it, for instance, reading and writing.\n   The RDMA control functions are semantically\
    \ layered atop direct data\n   placement.  Included are operations that provide\
    \ \"control\" features,\n   such as connection and termination, and the ordering\
    \ of operations\n   and signaling their completions.  A \"send\" facility is provided.\n\
    \   While the functions (and potentially protocols) are distinct,\n   historically\
    \ both aspects taken together have been referred to as\n   \"RDMA\".  The facilities\
    \ of direct data placement are useful in and of\n   themselves, and may be employed\
    \ by other upper layer protocols to\n   facilitate data transfer.  Therefore,\
    \ it is often useful to refer to\n   DDP as the data placement functionality and\
    \ RDMA as the control\n   aspect.\n   [BT05] develops an architecture for DDP\
    \ and RDMA atop the Internet\n   Protocol Suite, and is a companion document to\
    \ this problem\n   statement.\n"
- title: 6.  Conclusions
  contents:
  - "6.  Conclusions\n   This Problem Statement concludes that an IP-based, general\
    \ solution\n   for reducing processing overhead in end-hosts is desirable.\n \
    \  It has shown that high overhead of the processing of network data\n   leads\
    \ to end-host bottlenecks.  These bottlenecks are in large part\n   attributable\
    \ to the copying of data.  The bus bandwidth of machines\n   has historically\
    \ been limited, and the bandwidth of high-speed\n   interconnects taxes it heavily.\n\
    \   An architectural solution to alleviate these bottlenecks best\n   satisfies\
    \ the issue.  Further, the high speed of today's\n   interconnects and the deployment\
    \ of these hosts on Internet\n   Protocol-based networks leads to the desirability\
    \ of layering such a\n   solution on the Internet Protocol Suite.  The architecture\
    \ described\n   in [BT05] is such a proposal.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   Solutions to the problem of reducing copying\
    \ overhead in high\n   bandwidth transfers may introduce new security concerns.\
    \  Any\n   proposed solution must be analyzed for security vulnerabilities and\n\
    \   any such vulnerabilities addressed.  Potential security weaknesses --\n  \
    \ due to resource issues that might lead to denial-of-service attacks,\n   overwrites\
    \ and other concurrent operations, the ordering of\n   completions as required\
    \ by the RDMA protocol, the granularity of\n   transfer, and any other identified\
    \ vulnerabilities -- need to be\n   examined, described, and an adequate resolution\
    \ to them found.\n   Layered atop Internet transport protocols, the RDMA protocols\
    \ will\n   gain leverage from and must permit integration with Internet security\n\
    \   standards, such as IPsec and TLS [IPSEC, TLS].  However, there may be\n  \
    \ implementation ramifications for certain security approaches with\n   respect\
    \ to RDMA, due to its copy avoidance.\n   IPsec, operating to secure the connection\
    \ on a packet-by-packet\n   basis, seems to be a natural fit to securing RDMA\
    \ placement, which\n   operates in conjunction with transport.  Because RDMA enables\
    \ an\n   implementation to avoid buffering, it is preferable to perform all\n\
    \   applicable security protection prior to processing of each segment by\n  \
    \ the transport and RDMA layers.  Such a layering enables the most\n   efficient\
    \ secure RDMA implementation.\n   The TLS record protocol, on the other hand,\
    \ is layered on top of\n   reliable transports and cannot provide such security\
    \ assurance until\n   an entire record is available, which may require the buffering\
    \ and/or\n   assembly of several distinct messages prior to TLS processing.  This\n\
    \   defers RDMA processing and introduces overheads that RDMA is designed\n  \
    \ to avoid.  Therefore, TLS is viewed as potentially a less natural fit\n   for\
    \ protecting the RDMA protocols.\n   It is necessary to guarantee properties such\
    \ as confidentiality,\n   integrity, and authentication on an RDMA communications\
    \ channel.\n   However, these properties cannot defend against all attacks from\n\
    \   properly authenticated peers, which might be malicious, compromised,\n   or\
    \ buggy.  Therefore, the RDMA design must address protection against\n   such\
    \ attacks.  For example, an RDMA peer should not be able to read\n   or write\
    \ memory regions without prior consent.\n   Further, it must not be possible to\
    \ evade memory consistency checks\n   at the recipient.  The RDMA design must\
    \ allow the recipient to rely\n   on its consistent memory contents by explicitly\
    \ controlling peer\n   access to memory regions at appropriate times.\n   Peer\
    \ connections that do not pass authentication and authorization\n   checks by\
    \ upper layers must not be permitted to begin processing in\n   RDMA mode with\
    \ an inappropriate endpoint.  Once associated, peer\n   accesses to memory regions\
    \ must be authenticated and made subject to\n   authorization checks in the context\
    \ of the association and connection\n   on which they are to be performed, prior\
    \ to any transfer operation or\n   data being accessed.\n   The RDMA protocols\
    \ must ensure that these region protections be under\n   strict application control.\
    \  Remote access to local memory by a\n   network peer is particularly important\
    \ in the Internet context, where\n   such access can be exported globally.\n"
- title: 8.  Terminology
  contents:
  - "8.  Terminology\n   This section contains general terminology definitions for\
    \ this\n   document and for Remote Direct Memory Access in general.\n   Remote\
    \ Direct Memory Access (RDMA)\n        A method of accessing memory on a remote\
    \ system in which the\n        local system specifies the location of the data\
    \ to be\n        transferred.\n   RDMA Protocol\n        A protocol that supports\
    \ RDMA Operations to transfer data\n        between systems.\n   Fabric\n    \
    \    The collection of links, switches, and routers that connect a\n        set\
    \ of systems.\n   Storage Area Network (SAN)\n        A network where disks, tapes,\
    \ and other storage devices are made\n        available to one or more end-systems\
    \ via a fabric.\n   System Area Network\n        A network where clustered systems\
    \ share services, such as\n        storage and interprocess communication, via\
    \ a fabric.\n   Fibre Channel (FC)\n        An ANSI standard link layer with associated\
    \ protocols, typically\n        used to implement Storage Area Networks. [FIBRE]\n\
    \   Virtual Interface Architecture (VI, VIA)\n        An RDMA interface definition\
    \ developed by an industry group and\n        implemented with a variety of differing\
    \ wire protocols. [VI]\n   Infiniband (IB)\n        An RDMA interface, protocol\
    \ suite and link layer specification\n        defined by an industry trade association.\
    \ [IB]\n"
- title: 9.  Acknowledgements
  contents:
  - "9.  Acknowledgements\n   Jeff Chase generously provided many useful insights\
    \ and information.\n   Thanks to Jim Pinkerton for many helpful discussions.\n"
- title: 10.  Informative References
  contents:
  - "10.  Informative References\n   [ATM]      The ATM Forum, \"Asynchronous Transfer\
    \ Mode Physical Layer\n              Specification\" af-phy-0015.000, etc.  available\
    \ from\n              http://www.atmforum.com/standards/approved.html.\n   [BCF+95]\
    \   N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C.\n              L.\
    \ Seitz, J. N. Seizovic, and W. Su. \"Myrinet - A\n              gigabit-per-second\
    \ local-area network\", IEEE Micro,\n              February 1995.\n   [BJM+96]\
    \   G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, J.\n              Wilkes,\
    \ \"An implementation of the Hamlyn send-managed\n              interface architecture\"\
    , in Proceedings of the Second\n              Symposium on Operating Systems Design\
    \ and Implementation,\n              USENIX Assoc., October 1996.\n   [BLA+94]\
    \   M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W.\n              Felten,\
    \ \"A virtual memory mapped network interface for the\n              SHRIMP multicomputer\"\
    , in Proceedings of the 21st Annual\n              Symposium on Computer Architecture,\
    \ April 1994, pp. 142-\n              153.\n   [Br99]     J. C. Brustoloni, \"\
    Interoperation of copy avoidance in\n              network and file I/O\", Proceedings\
    \ of IEEE Infocom, 1999,\n              pp. 534-542.\n   [BS96]     J. C. Brustoloni,\
    \ P. Steenkiste, \"Effects of buffering\n              semantics on I/O performance\"\
    , Proceedings OSDI'96,\n              USENIX, Seattle, WA October 1996, pp. 277-291.\n\
    \   [BT05]     Bailey, S. and T. Talpey, \"The Architecture of Direct Data\n \
    \             Placement (DDP) And Remote Direct Memory Access (RDMA) On\n    \
    \          Internet Protocols\", RFC 4296, December 2005.\n   [CFF+94]   C-H Chang,\
    \ D. Flower, J. Forecast, H. Gray, B. Hawe, A.\n              Nadkarni, K. K.\
    \ Ramakrishnan, U. Shikarpur, K. Wilde,\n              \"High-performance TCP/IP\
    \ and UDP/IP networking in DEC\n              OSF/1 for Alpha AXP\",  Proceedings\
    \ of the 3rd IEEE\n              Symposium on High Performance Distributed Computing,\n\
    \              August 1994, pp. 36-42.\n   [CGY01]    J. S. Chase, A. J. Gallatin,\
    \ and K. G. Yocum, \"End system\n              optimizations for high-speed TCP\"\
    , IEEE Communications\n              Magazine, Volume: 39, Issue: 4 , April 2001,\
    \ pp 68-74.\n              http://www.cs.duke.edu/ari/publications/end-\n    \
    \          system.{ps,pdf}.\n   [Ch96]     H.K. Chu, \"Zero-copy TCP in Solaris\"\
    , Proc. of the USENIX\n              1996 Annual Technical Conference, San Diego,\
    \ CA, January\n              1996.\n   [Ch02]     Jeffrey Chase, Personal communication.\n\
    \   [CJRS89]   D. D. Clark, V. Jacobson, J. Romkey, H. Salwen, \"An\n        \
    \      analysis of TCP processing overhead\", IEEE Communications\n          \
    \    Magazine, volume:  27, Issue: 6, June 1989, pp 23-29.\n   [CT90]     D. D.\
    \ Clark, D. Tennenhouse, \"Architectural considerations\n              for a new\
    \ generation of protocols\", Proceedings of the ACM\n              SIGCOMM Conference,\
    \ 1990.\n   [DAPP93]   P. Druschel, M. B. Abbott, M. A. Pagels, L. L. Peterson,\n\
    \              \"Network subsystem design\", IEEE Network, July 1993, pp.\n  \
    \            8-17.\n   [DP93]     P. Druschel, L. L. Peterson, \"Fbufs: a high-bandwidth\n\
    \              cross-domain transfer facility\", Proceedings of the 14th\n   \
    \           ACM Symposium of Operating Systems Principles, December\n        \
    \      1993.\n   [DWB+93]   C. Dalton, G. Watson, D. Banks, C. Calamvokis, A.\
    \ Edwards,\n              J. Lumley, \"Afterburner: architectural support for\
    \ high-\n              performance protocols\", Technical Report, HP Laboratories\n\
    \              Bristol, HPL-93-46, July 1993.\n   [EBBV95]   T. von Eicken, A.\
    \ Basu, V. Buch, and W. Vogels, \"U-Net: A\n              user-level network interface\
    \ for parallel and distributed\n              computing\", Proc. of the 15th ACM\
    \ Symposium on Operating\n              Systems Principles, Copper Mountain, Colorado,\
    \ December\n              3-6, 1995.\n   [FDDI]     International Standards Organization,\
    \ \"Fibre Distributed\n              Data Interface\", ISO/IEC 9314, committee\
    \ drafts available\n              from http://www.iso.org.\n   [FGM+99]   Fielding,\
    \  R., Gettys, J., Mogul, J., Frystyk, H.,\n              Masinter, L., Leach,\
    \ P., and T. Berners-Lee, \"Hypertext\n              Transfer Protocol -- HTTP/1.1\"\
    , RFC 2616, June 1999.\n   [FIBRE]    ANSI Technical Committee T10, \"Fibre Channel\
    \ Protocol\n              (FCP)\" (and as revised and updated), ANSI X3.269:1996\n\
    \              [R2001], committee draft available from\n              http://www.t10.org/drafts.htm#FibreChannel\n\
    \   [HP97]     J. L. Hennessy, D. A. Patterson, Computer Organization and\n  \
    \            Design, 2nd Edition, San Francisco: Morgan Kaufmann\n           \
    \   Publishers, 1997.\n   [IB]       InfiniBand Trade Association, \"InfiniBand\
    \ Architecture\n              Specification, Volumes 1 and 2\", Release 1.1, November\n\
    \              2002, available from http://www.infinibandta.org/specs.\n   [IPSEC]\
    \    Kent, S. and R. Atkinson, \"Security Architecture for the\n             \
    \ Internet Protocol\", RFC 2401, November 1998.\n   [KP96]     J. Kay, J. Pasquale,\
    \ \"Profiling and reducing processing\n              overheads in TCP/IP\", IEEE/ACM\
    \ Transactions on Networking,\n              Vol 4, No. 6, pp.817-828, December\
    \ 1996.\n   [KSZ95]    K. Kleinpaste, P. Steenkiste, B. Zill, \"Software support\n\
    \              for outboard buffering and checksumming\", SIGCOMM'95.\n   [Ma02]\
    \     K. Magoutis, \"Design and Implementation of a Direct Access\n          \
    \    File System (DAFS) Kernel Server for FreeBSD\", in\n              Proceedings\
    \ of USENIX BSDCon 2002 Conference, San\n              Francisco, CA, February\
    \ 11-14, 2002.\n   [MAF+02]   K. Magoutis, S. Addetia, A. Fedorova, M.  I. Seltzer,\
    \ J.\n              S. Chase, D. Gallatin, R. Kisley, R. Wickremesinghe, E.\n\
    \              Gabber, \"Structure and Performance of the Direct Access\n    \
    \          File System (DAFS)\", in Proceedings of the 2002 USENIX\n         \
    \     Annual Technical Conference, Monterey, CA, June 9-14,\n              2002.\n\
    \   [Mc95]     J. D. McCalpin, \"A Survey of memory bandwidth and machine\n  \
    \            balance in current high performance computers\", IEEE TCCA\n    \
    \          Newsletter, December 1995.\n   [PAC+97]   D. Patterson, T. Anderson,\
    \ N. Cardwell, R. Fromm, K.\n              Keeton, C. Kozyrakis, R. Thomas, K.\
    \ Yelick , \"A case for\n              intelligient RAM: IRAM\", IEEE Micro, April\
    \ 1997.\n   [PDZ99]    V. S. Pai, P. Druschel, W. Zwaenepoel, \"IO-Lite: a unified\n\
    \              I/O buffering and caching system\", Proc. of the 3rd\n        \
    \      Symposium on Operating Systems Design and Implementation,\n           \
    \   New Orleans, LA, February 1999.\n   [Pi01]     J. Pinkerton, \"Winsock Direct:\
    \ The Value of System Area\n              Networks\", May 2001, available from\n\
    \              http://www.microsoft.com/windows2000/techinfo/\n              howitworks/communications/winsock.asp.\n\
    \   [Po81]     Postel, J., \"Transmission Control Protocol\", STD 7, RFC\n   \
    \           793, September 1981.\n   [QUAD]     Quadrics Ltd., Quadrics QSNet\
    \ product information,\n              available from\n              http://www.quadrics.com/website/pages/02qsn.html.\n\
    \   [SDP]      InfiniBand Trade Association, \"Sockets Direct Protocol\n     \
    \         v1.0\", Annex A of InfiniBand Architecture Specification\n         \
    \     Volume 1, Release 1.1, November 2002, available from\n              http://www.infinibandta.org/specs.\n\
    \   [SRVNET]   R. Horst, \"TNet: A reliable system area network\", IEEE\n    \
    \          Micro, pp. 37-45, February 1995.\n   [STREAM]   J. D. McAlpin, The\
    \ STREAM Benchmark Reference Information,\n              http://www.cs.virginia.edu/stream/.\n\
    \   [TK95]     M. N. Thadani, Y. A. Khalidi, \"An efficient zero-copy I/O\n  \
    \            framework for UNIX\", Technical Report, SMLI TR-95-39, May\n    \
    \          1995.\n   [TLS]      Dierks, T. and C. Allen, \"The TLS Protocol Version\
    \ 1.0\",\n              RFC 2246, January 1999.\n   [VI]       D. Cameron and\
    \ G. Regnier, \"The Virtual Interface\n              Architecture\", ISBN 0971288704,\
    \ Intel Press, April 2002,\n              more info at http://www.intel.com/intelpress/via/.\n\
    \   [Wa97]     J. R. Walsh, \"DART: Fast application-level networking via\n  \
    \            data-copy avoidance\", IEEE Network, July/August 1997, pp.\n    \
    \          28-38.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Stephen Bailey\n   Sandburst Corporation\n   600 Federal\
    \ Street\n   Andover, MA  01810 USA\n   Phone: +1 978 689 1614\n   EMail: steph@sandburst.com\n\
    \   Jeffrey C. Mogul\n   HP Labs\n   Hewlett-Packard Company\n   1501 Page Mill\
    \ Road, MS 1117\n   Palo Alto, CA  94304 USA\n   Phone: +1 650 857 2206 (EMail\
    \ preferred)\n   EMail: JeffMogul@acm.org\n   Allyn Romanow\n   Cisco Systems,\
    \ Inc.\n   170 W. Tasman Drive\n   San Jose, CA  95134 USA\n   Phone: +1 408 525\
    \ 8836\n   EMail: allyn@cisco.com\n   Tom Talpey\n   Network Appliance\n   1601\
    \ Trapelo Road\n   Waltham, MA  02451 USA\n   Phone: +1 781 768 5329\n   EMail:\
    \ thomas.talpey@netapp.com\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The Internet Society (2005).\n   This\
    \ document is subject to the rights, licenses and restrictions\n   contained in\
    \ BCP 78, and except as set forth therein, the authors\n   retain all their rights.\n\
    \   This document and the information contained herein are provided on an\n  \
    \ \"AS IS\" basis and THE CONTRIBUTOR, THE ORGANIZATION HE/SHE REPRESENTS\n  \
    \ OR IS SPONSORED BY (IF ANY), THE INTERNET SOCIETY AND THE INTERNET\n   ENGINEERING\
    \ TASK FORCE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED,\n   INCLUDING BUT NOT\
    \ LIMITED TO ANY WARRANTY THAT THE USE OF THE\n   INFORMATION HEREIN WILL NOT\
    \ INFRINGE ANY RIGHTS OR ANY IMPLIED\n   WARRANTIES OF MERCHANTABILITY OR FITNESS\
    \ FOR A PARTICULAR PURPOSE.\n"
- title: Intellectual Property
  contents:
  - "Intellectual Property\n   The IETF takes no position regarding the validity or\
    \ scope of any\n   Intellectual Property Rights or other rights that might be\
    \ claimed to\n   pertain to the implementation or use of the technology described\
    \ in\n   this document or the extent to which any license under such rights\n\
    \   might or might not be available; nor does it represent that it has\n   made\
    \ any independent effort to identify any such rights.  Information\n   on the\
    \ procedures with respect to rights in RFC documents can be\n   found in BCP 78\
    \ and BCP 79.\n   Copies of IPR disclosures made to the IETF Secretariat and any\n\
    \   assurances of licenses to be made available, or the result of an\n   attempt\
    \ made to obtain a general license or permission for the use of\n   such proprietary\
    \ rights by implementers or users of this\n   specification can be obtained from\
    \ the IETF on-line IPR repository at\n   http://www.ietf.org/ipr.\n   The IETF\
    \ invites any interested party to bring to its attention any\n   copyrights, patents\
    \ or patent applications, or other proprietary\n   rights that may cover technology\
    \ that may be required to implement\n   this standard.  Please address the information\
    \ to the IETF at ietf-\n   ipr@ietf.org.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
