- title: __initial_text__
  contents:
  - "     Network File System (NFS) Remote Direct Memory Access (RDMA)\n         \
    \                  Problem Statement\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2009 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\
    \ in effect on the date of\n   publication of this document (http://trustee.ietf.org/license-info).\n\
    \   Please review these documents carefully, as they describe your rights\n  \
    \ and restrictions with respect to this document.\n   This document may contain\
    \ material from IETF Documents or IETF\n   Contributions published or made publicly\
    \ available before November\n   10, 2008.  The person(s) controlling the copyright\
    \ in some of this\n   material may not have granted the IETF Trust the right to\
    \ allow\n   modifications of such material outside the IETF Standards Process.\n\
    \   Without obtaining an adequate license from the person(s) controlling\n   the\
    \ copyright in such materials, this document may not be modified\n   outside the\
    \ IETF Standards Process, and derivative works of it may\n   not be created outside\
    \ the IETF Standards Process, except to format\n   it for publication as an RFC\
    \ or to translate it into languages other\n   than English.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document addresses enabling the use of Remote Direct Memory\n\
    \   Access (RDMA) by the Network File System (NFS) protocols.  NFS\n   implementations\
    \ historically incur significant overhead due to data\n   copies on end-host systems,\
    \ as well as other processing overhead.\n   This document explores the potential\
    \ benefits of RDMA to these\n   implementations and evaluates the reasons why\
    \ RDMA is especially\n   well-suited to NFS and network file protocols in general.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................2\n\
    \      1.1. Background .................................................3\n  \
    \ 2. Problem Statement ...............................................4\n   3.\
    \ File Protocol Architecture ......................................5\n   4. Sources\
    \ of Overhead .............................................7\n      4.1. Savings\
    \ from TOE ...........................................8\n      4.2. Savings from\
    \ RDMA ..........................................9\n   5. Application of RDMA\
    \ to NFS .....................................10\n   6. Conclusions ....................................................10\n\
    \   7. Security Considerations ........................................11\n  \
    \ 8. Acknowledgments ................................................12\n   9.\
    \ References .....................................................12\n      9.1.\
    \ Normative References ......................................12\n      9.2. Informative\
    \ References ....................................13\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The Network File System (NFS) protocol (as described in\
    \ [RFC1094],\n   [RFC1813], and [RFC3530]) is one of several remote file access\n\
    \   protocols used in the class of processing architecture sometimes\n   called\
    \ Network-Attached Storage (NAS).\n   Historically, remote file access has proven\
    \ to be a convenient,\n   cost-effective way to share information over a network,\
    \ a concept\n   proven over time by the popularity of the NFS protocol.  However,\n\
    \   there are issues in such a deployment.\n   As compared to a local (direct-attached)\
    \ file access architecture,\n   NFS removes the overhead of managing the local\
    \ on-disk file system\n   state and its metadata, but interposes at least a transport\
    \ network\n   and two network endpoints between an application process and the\n\
    \   files it is accessing.  To date, this trade-off has usually resulted\n   in\
    \ a net performance loss as a result of reduced bandwidth, increased\n   application\
    \ server CPU utilization, and other overheads.\n   Several classes of applications,\
    \ including those directly supporting\n   enterprise activities in high-performance\
    \ domains such as database\n   applications and shared clusters, have therefore\
    \ encountered issues\n   with moving to NFS architectures.  While this has been\
    \ due\n   principally to the performance costs of NFS versus direct-attached\n\
    \   files, other reasons are relevant, such as the lack of strong\n   consistency\
    \ guarantees being provided by NFS implementations.\n   Replication of local file\
    \ access performance on NAS using traditional\n   network protocol stacks has\
    \ proven difficult, not because of protocol\n   processing overheads, but because\
    \ of data copy costs in the network\n   endpoints.  This is especially true since\
    \ host buses are now often\n   the main bottleneck in NAS architectures [MOG03]\
    \ [CHA+01].\n   The External Data Representation [RFC4506] employed beneath NFS\
    \ and\n   the Remote Procedure Call (RPC) [RFC5531] can add more data copies,\n\
    \   exacerbating the problem.\n   Data copy-avoidance designs have not been widely\
    \ adopted for a\n   variety of reasons.  [BRU99] points out that \"many copy avoidance\n\
    \   techniques for network I/O are not applicable or may even backfire if\n  \
    \ applied to file I/O\".  Other designs that eliminate unnecessary\n   copies,\
    \ such as [PAI+00], are incompatible with existing APIs and\n   therefore force\
    \ application changes.\n   In recent years, an effort to standardize a set of\
    \ protocols for\n   Remote Direct Memory Access (RDMA) over the standard Internet\n\
    \   Protocol Suite has been chartered [RDDP].  A complete IP-based RDMA\n   protocol\
    \ suite is available in the published Standards Track\n   specifications.\n  \
    \ RDMA is a general solution to the problem of CPU overhead incurred\n   due to\
    \ data copies, primarily at the receiver.  Substantial research\n   has addressed\
    \ this and has borne out the efficacy of the approach.\n   An overview of this\
    \ is the \"Remote Direct Memory Access (RDMA) over\n   IP Problem Statement\"\
    \ [RFC4297].\n   In addition to the per-byte savings of offloading data copies,\
    \ RDMA-\n   enabled NICs (RNICS) offload the underlying protocol layers as well\n\
    \   (e.g., TCP), further reducing CPU overhead due to NAS processing.\n"
- title: 1.1.  Background
  contents:
  - "1.1.  Background\n   The RDDP Problem Statement [RFC4297] asserts:\n      High\
    \ costs associated with copying are an issue primarily for\n      large scale\
    \ systems ... with high bandwidth feeds, usually\n      multiprocessors and clusters,\
    \ that are adversely affected by\n      copying overhead.  Examples of such machines\
    \ include all varieties\n      of servers: database servers, storage servers,\
    \ application servers\n      for transaction processing, for e-commerce, and web\
    \ serving,\n      content distribution, video distribution, backups, data mining\
    \ and\n      decision support, and scientific computing.\n      Note that such\
    \ servers almost exclusively service many concurrent\n      sessions (transport\
    \ connections), which, in aggregate, are\n      responsible for > 1 Gbits/s of\
    \ communication.  Nonetheless, the\n      cost of copying overhead for a particular\
    \ load is the same whether\n      from few or many sessions.\n   Note that each\
    \ of the servers listed above could be accessing their\n   file data as an NFS\
    \ client, or as NFS serving the data to such\n   clients, or acting as both.\n\
    \   The CPU overhead of the NFS and TCP/IP protocol stacks (including\n   data\
    \ copies or reduced copy workarounds) becomes a significant matter\n   in these\
    \ clients and servers.  File access using locally attached\n   disks imposes relatively\
    \ low overhead due to the highly optimized I/O\n   path and direct memory access\
    \ afforded to the storage controller.\n   This is not the case with NFS, which\
    \ must pass data to, and\n   especially from, the network and network processing\
    \ stack to the NFS\n   stack.  Frequently, data copies are imposed on this transfer;\
    \ in some\n   cases, several such copies are imposed in each direction.\n   Copies\
    \ are potentially encountered in an NFS implementation\n   exchanging data to\
    \ and from user address spaces, within kernel buffer\n   caches, in eXternal Data\
    \ Representation (XDR) marshalling and\n   unmarshalling, and within network stacks\
    \ and network drivers.  Other\n   overheads such as serialization among multiple\
    \ threads of execution\n   sharing a single NFS mount point and transport connection\
    \ are\n   additionally encountered.\n   Numerous upper-layer protocols achieve\
    \ extremely high bandwidth and\n   low overhead through the use of RDMA.  [MAF+02]\
    \ shows that the RDMA-\n   based Direct Access File System (with a user-level\
    \ implementation of\n   the file system client) can outperform even a zero-copy\n\
    \   implementation of NFS [CHA+01] [CHA+99] [GAL+99] [KM02].  Also, file\n   data\
    \ access implies the use of large Unequal Loss Protection (ULP)\n   messages.\
    \  These large messages tend to amortize any increase in\n   per-message costs\
    \ due to the offload of protocol processing incurred\n   when using RNICs while\
    \ gaining the benefits of reduced per-byte\n   costs.  Finally, the direct memory\
    \ addressing afforded by RDMA avoids\n   many sources of contention on network\
    \ resources.\n"
- title: 2.  Problem Statement
  contents:
  - "2.  Problem Statement\n   The principal performance problem encountered by NFS\
    \ implementations\n   is the CPU overhead required to implement the protocol.\
    \  Primary\n   among the sources of this overhead is the movement of data from\
    \ NFS\n   protocol messages to its eventual destination in user buffers or\n \
    \  aligned kernel buffers.  Due to the nature of the RPC and XDR\n   protocols,\
    \ the NFS data payload arrives at arbitrary alignment,\n   necessitating a copy\
    \ at the receiver, and the NFS requests are\n   completed in an arbitrary sequence.\n\
    \   The data copies consume system bus bandwidth and CPU time, reducing\n   the\
    \ available system capacity for applications [RFC4297].  To date,\n   achieving\
    \ zero-copy with NFS has required sophisticated, version-\n   specific \"header\
    \ cracking\" hardware and/or extensive platform-\n   specific virtual memory mapping\
    \ tricks.  Such approaches become even\n   more difficult for NFS version 4 due\
    \ to the existence of the COMPOUND\n   operation and presence of Kerberos and\
    \ other security information,\n   which further reduce alignment and greatly complicate\
    \ ULP offload.\n   Furthermore, NFS is challenged by high-speed network fabrics\
    \ such as\n   10 Gbits/s Ethernet.  Performing even raw network I/O such as TCP\
    \ is\n   an issue at such speeds with today's hardware.  The problem is\n   fundamental\
    \ in nature and has led the IETF to explore RDMA [RFC4297].\n   Zero-copy techniques\
    \ benefit file protocols extensively, as they\n   enable direct user I/O, reduce\
    \ the overhead of protocol stacks,\n   provide perfect alignment into caches,\
    \ etc.  Many studies have\n   already shown the performance benefits of such techniques\
    \ [SKE+01]\n   [DCK+03] [FJNFS] [FJDAFS] [KM02] [MAF+02].\n   RDMA is compelling\
    \ here for another reason; hardware-offloaded\n   networking support in itself\
    \ does not avoid data copies, without\n   resorting to implementing part of the\
    \ NFS protocol in the Network\n   Interface Card (NIC).  Support of RDMA by NFS\
    \ enables the highest\n   performance at the architecture level rather than by\
    \ implementation;\n   this enables ubiquitous and interoperable solutions.\n \
    \  By providing file access performance equivalent to that of local file\n   systems,\
    \ NFS over RDMA will enable applications running on a set of\n   client machines\
    \ to interact through an NFS file system, just as\n   applications running on\
    \ a single machine might interact through a\n   local file system.\n"
- title: 3.  File Protocol Architecture
  contents:
  - "3.  File Protocol Architecture\n   NFS runs as an Open Network Computing (ONC)\
    \ RPC [RFC5531]\n   application.  Being a file access protocol, NFS is very \"\
    rich\" in\n   data content (versus control information).\n   NFS messages can\
    \ range from very small (under 100 bytes) to very\n   large (from many kilobytes\
    \ to a megabyte or more).  They are all\n   contained within an RPC message and\
    \ follow a variable-length RPC\n   header.  This layout provides an alignment\
    \ challenge for the data\n   items contained in an NFS call (request) or reply\
    \ (response) message.\n   In addition to the control information in each NFS call\
    \ or reply\n   message, sometimes there are large \"chunks\" of application file\
    \ data,\n   for example, read and write requests.  With NFS version 4 (due to\
    \ the\n   existence of the COMPOUND operation), there can be several of these\n\
    \   data chunks interspersed with control information.\n   ONC RPC is a remote\
    \ procedure call protocol that has been run over a\n   variety of transports.\
    \  Most implementations today use UDP or TCP.\n   RPC messages are defined in\
    \ terms of an eXternal Data Representation\n   (XDR) [RFC4506], which provides\
    \ a canonical data representation\n   across a variety of host architectures.\
    \  An XDR data stream is\n   conveyed differently on each type of transport. \
    \ On UDP, RPC messages\n   are encapsulated inside datagrams, while on a TCP byte\
    \ stream, RPC\n   messages are delineated by a record-marking protocol.  An RDMA\n\
    \   transport also conveys RPC messages in a unique fashion that must be\n   fully\
    \ described if client and server implementations are to\n   interoperate.\n  \
    \ The RPC transport is responsible for conveying an RPC message from a\n   sender\
    \ to a receiver.  An RPC message is either an RPC call from a\n   client to a\
    \ server, or an RPC reply from the server back to the\n   client.  An RPC message\
    \ contains an RPC call header followed by\n   arguments if the message is an RPC\
    \ call, or an RPC reply header\n   followed by results if the message is an RPC\
    \ reply.  The call header\n   contains a transaction ID (XID) followed by the\
    \ program and procedure\n   number as well as a security credential.  An RPC reply\
    \ header begins\n   with an XID that matches that of the RPC call message, followed\
    \ by a\n   security verifier and results.  All data in an RPC message is XDR\n\
    \   encoded.\n   The encoding of XDR data into transport buffers is referred to\
    \ as\n   \"marshalling\", and the decoding of XDR data contained within\n   transport\
    \ buffers and into destination RPC procedure result buffers,\n   is referred to\
    \ as \"unmarshalling\".  Therefore, the process of\n   marshalling takes place\
    \ at the sender of any particular message, be\n   it an RPC request or an RPC\
    \ response.  Unmarshalling, of course,\n   takes place at the receiver.\n   Normally,\
    \ any bulk data is moved (copied) as a result of the\n   unmarshalling process,\
    \ because the destination address is not known\n   until the RPC code receives\
    \ control and subsequently invokes the XDR\n   unmarshalling routine.  In other\
    \ words, XDR-encoded data is not\n   self-describing, and it carries no placement\
    \ information.  This\n   results in a data copy in most NFS implementations.\n\
    \   One mechanism by which the RPC layer may overcome this is for each\n   request\
    \ to include placement information, to be used for direct\n   placement during\
    \ XDR encode.  This \"write chunk\" can avoid sending\n   bulk data inline in\
    \ an RPC message and generally results in one or\n   more RDMA Write operations.\n\
    \   Similarly, a \"read chunk\", where placement information referring to\n  \
    \ bulk data that may be directly fetched via one or more RDMA Read\n   operations\
    \ during XDR decode, may be conveyed.  The \"read chunk\" will\n   therefore be\
    \ useful in both RPC calls and replies, while the \"write\n   chunk\" is used\
    \ solely in replies.\n   These \"chunks\" are the key concept in an existing proposal\
    \ [RPCRDMA].\n   They convey what are effectively pointers to remote memory across\
    \ the\n   network.  They allow cooperating peers to exchange data outside of\n\
    \   XDR encodings but still use XDR for describing the data to be\n   transferred.\
    \  And, finally, through use of XDR they maintain a large\n   degree of on-the-wire\
    \ compatibility.\n   The central concept of the RDMA transport is to provide the\n\
    \   additional encoding conventions to convey this placement information\n   in\
    \ transport-specific encoding, and to modify the XDR handling of\n   bulk data.\n\
    \                           Block Diagram\n   +------------------------+-----------------------------------+\n\
    \   |         NFS            |            NFS + RDMA             |\n   +------------------------+----------------------+------------+\n\
    \   |           Operations / Procedures             |            |\n   +-----------------------------------------------+\
    \            |\n   |                   RPC/XDR                     |         \
    \   |\n   +--------------------------------+--------------+            |\n   |\
    \       Stream Transport         |      RDMA Transport       |\n   +--------------------------------+---------------------------+\n"
- title: 4.  Sources of Overhead
  contents:
  - "4.  Sources of Overhead\n   Network and file protocol costs can be categorized\
    \ as follows:\n   o  per-byte costs - data touching costs such as checksum or\
    \ data\n      copy.  Today's network interface hardware commonly offloads the\n\
    \      checksum, which leaves the other major source of per-byte\n      overhead,\
    \ data copy.\n   o  per-packet costs - interrupts and lower-layer processing (LLP).\n\
    \      Today's network interface hardware also commonly coalesce\n      interrupts\
    \ to reduce per-packet costs.\n   o  per-message (request or response) costs -\
    \ LLP and ULP processing.\n   Improvement from optimization becomes more important\
    \ if the overhead\n   it targets is a larger share of the total cost.  As other\
    \ sources of\n   overhead, such as the checksumming and interrupt handling above\
    \ are\n   eliminated, the remaining overheads (primarily data copy) loom\n   larger.\n\
    \   With copies crossing the bus twice per copy, network processing\n   overhead\
    \ is high whenever network bandwidth is large in comparison to\n   CPU and memory\
    \ bandwidths.  Generally, with today's end-systems, the\n   effects are observable\
    \ at network speeds at or above 1 Gbit/s.\n   A common question is whether an\
    \ increase in CPU processing power\n   alleviates the problem of high processing\
    \ costs of network I/O.  The\n   answer is no, it is the memory bandwidth that\
    \ is the issue.  Faster\n   CPUs do not help if the CPU spends most of its time\
    \ waiting for\n   memory [RFC4297].\n   TCP offload engine (TOE) technology aims\
    \ to offload the CPU by moving\n   TCP/IP protocol processing to the NIC.  However,\
    \ TOE technology by\n   itself does nothing to avoid necessary data copies within\
    \ upper-layer\n   protocols.  [MOG03] provides a description of the role TOE can\
    \ play\n   in reducing per-packet and per-message costs.  Beyond the offloads\n\
    \   commonly provided by today's network interface hardware, TOE alone\n   (without\
    \ RDMA) helps in protocol header processing, but this has been\n   shown to be\
    \ a minority component of the total protocol processing\n   overhead. [CHA+01]\n\
    \   Numerous software approaches to the optimization of network\n   throughput\
    \ have been made.  Experience has shown that network I/O\n   interacts with other\
    \ aspects of system processing such as file I/O\n   and disk I/O [BRU99] [CHU96].\
    \  Zero-copy optimizations based on page\n   remapping [CHU96] can be dependent\
    \ upon machine architecture, and are\n   not scalable to multi-processor architectures.\
    \  Correct buffer\n   alignment and sizing together are needed to optimize the\
    \ performance\n   of zero-copy movement mechanisms [SKE+01].  The NFS message\
    \ layout\n   described above does not facilitate the splitting of headers from\n\
    \   data nor does it facilitate providing correct data buffer alignment.\n"
- title: 4.1.  Savings from TOE
  contents:
  - "4.1.  Savings from TOE\n   The expected improvement of TOE specifically for NFS\
    \ protocol\n   processing can be quantified and shown to be fundamentally limited.\n\
    \   [SHI+03] presents a set of \"LAWS\" parameters that serve to illustrate\n\
    \   the issues.  In the TOE case, the copy cost can be viewed as part of\n   the\
    \ application processing \"a\".  Application processing increases the\n   LAWS\
    \ \"gamma\", which is shown by the paper to result in a diminished\n   benefit\
    \ for TOE.\n   For example, if the overhead is 20% TCP/IP, 30% copy, and 50% real\n\
    \   application work, then gamma is 80/20 or 4, which means the maximum\n   benefit\
    \ of TOE is 1/gamma, or only 25%.\n   For RDMA (with embedded TOE) and the same\
    \ example, the \"overhead\" (o)\n   offloaded or eliminated is 50% (20% + 30%).\
    \  Therefore, in the RDMA\n   case, gamma is 50/50 or 1, and the inverse gives\
    \ the potential\n   benefit of 1 (100%), a factor of two.\n                  \
    \  CPU Overhead Reduction Factor\n               No Offload   TCP Offload   RDMA\
    \ Offload\n               -----------+-------------+-------------\n          \
    \        1.00x        1.25x         2.00x\n   The analysis in the paper shows\
    \ that RDMA could improve throughput by\n   the same factor of two, even when\
    \ the host is (just) powerful enough\n   to drive the full network bandwidth without\
    \ RDMA.  It can also be\n   shown that the speedup may be higher if network bandwidth\
    \ grows\n   faster than Moore's Law, although the higher benefits will apply to\
    \ a\n   narrow range of applications.\n"
- title: 4.2.  Savings from RDMA
  contents:
  - "4.2.  Savings from RDMA\n   Performance measurements directly comparing an NFS-over-RDMA\n\
    \   prototype with conventional network-based NFS processing are\n   described\
    \ in [CAL+03].  Comparisons of Read throughput and CPU\n   overhead were performed\
    \ on two types of Gigabit Ethernet adapters,\n   one type being a conventional\
    \ adapter, and another type with RDMA\n   capability.  The prototype RDMA protocol\
    \ performed all transfers via\n   RDMA Read.  The NFS layer in the study was measured\
    \ while performing\n   read transfers, varying the transfer size and readahead\
    \ depth across\n   ranges used by typical NFS deployments.\n   In these results,\
    \ conventional network-based throughput was severely\n   limited by the client's\
    \ CPU being saturated at 100% for all\n   transfers.  Read throughput reached\
    \ no more than 60 MBytes/s.\n      I/O Type        Size     Read Throughput  \
    \   CPU Utilization\n      Conventional    2 KB         20 MB/s              100%\n\
    \      Conventional   16 KB         40 MB/s              100%\n      Conventional\
    \  256 KB         60 MB/s              100%\n   However, over RDMA, throughput\
    \ rose to the theoretical maximum\n   throughput of the platform, while saturating\
    \ the single-CPU system\n   only at maximum throughput.\n       I/O Type     \
    \  Size     Read Throughput     CPU Utilization\n      RDMA            2 KB  \
    \       10 MB/s               45%\n      RDMA           16 KB         40 MB/s\
    \               70%\n      RDMA          256 KB        100 MB/s              100%\n\
    \   The lower relative throughput of the RDMA prototype at the small\n   blocksize\
    \ may be attributable to the RDMA Read imposed by the\n   prototype protocol,\
    \ which reduced the operation rate since it\n   introduces additional latency.\
    \  As well, it may reflect the relative\n   increase of per-packet setup costs\
    \ within the DMA portion of the\n   transfer.\n"
- title: 5.  Application of RDMA to NFS
  contents:
  - "5.  Application of RDMA to NFS\n   Efficient file protocols require efficient\
    \ data positioning and\n   movement.  The client system knows the client memory\
    \ address where\n   the application has data to be written or wants read data\
    \ deposited.\n   The server system knows the server memory address where the local\n\
    \   file system will accept write data or has data to be read.  Neither\n   peer\
    \ however is aware of the others' data destination in the current\n   NFS, RPC,\
    \ or XDR protocols.  Existing NFS implementations have\n   struggled with the\
    \ performance costs of data copies when using\n   traditional Ethernet transports.\n\
    \   With the onset of faster networks, the network I/O bottleneck will\n   worsen.\
    \  Fortunately, new transports that support RDMA have emerged.\n   RDMA excels\
    \ at bulk transfer efficiency; it is an efficient way to\n   deliver direct data\
    \ placement and remove a major part of the problem:\n   data copies.  RDMA also\
    \ addresses other overheads, e.g., underlying\n   protocol offload, and offers\
    \ separation of control information from\n   data.\n   The current NFS message\
    \ layout provides the performance-enhancing\n   opportunity for an NFS-over-RDMA\
    \ protocol that separates the control\n   information from data chunks while meeting\
    \ the alignment needs of\n   both.  The data chunks can be copied \"directly\"\
    \ between the client\n   and server memory addresses above (with a single occurrence\
    \ on each\n   memory bus) while the control information can be passed \"inline\"\
    .\n   [RPCRDMA] describes such a protocol.\n"
- title: 6.  Conclusions
  contents:
  - "6.  Conclusions\n   NFS version 4 [RFC3530] has been granted \"Proposed Standard\"\
    \ status.\n   The NFSv4 protocol was developed along several design points,\n\
    \   important among them: effective operation over wide-area networks,\n   including\
    \ the Internet itself; strong security integrated into the\n   protocol; extensive\
    \ cross-platform interoperability including\n   integrated locking semantics compatible\
    \ with multiple operating\n   systems; and (this is key), protocol extension.\n\
    \   NFS version 4 is an excellent base on which to add the needed\n   performance\
    \ enhancements and improved semantics described above.  The\n   minor versioning\
    \ support defined in NFS version 4 was designed to\n   support protocol improvements\
    \ without disruption to the installed\n   base [NFSv4.1].  Evolutionary improvement\
    \ of the protocol via minor\n   versioning is a conservative and cautious approach\
    \ to current and\n   future problems and shortcomings.\n   Many arguments can\
    \ be made as to the efficacy of the file abstraction\n   in meeting the future\
    \ needs of enterprise data service and the\n   Internet.  Fine grained Quality\
    \ of Service (QoS) policies (e.g., data\n   delivery, retention, availability,\
    \ security, etc.) are high among\n   them.\n   It is vital that the NFS protocol\
    \ continue to provide these benefits\n   to a wide range of applications, without\
    \ its usefulness being\n   compromised by concerns about performance and semantic\
    \ inadequacies.\n   This can reasonably be addressed in the existing NFS protocol\n\
    \   framework.  A cautious evolutionary improvement of performance and\n   semantics\
    \ allows building on the value already present in the NFS\n   protocol, while\
    \ addressing new requirements that have arisen from the\n   application of networking\
    \ technology.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   The NFS protocol, in conjunction with its layering\
    \ on RPC, provides a\n   rich and widely interoperable security model to applications\
    \ and\n   systems.  Any layering of NFS-over-RDMA transports must address the\n\
    \   NFS security requirements, and additionally must ensure that no new\n   vulnerabilities\
    \ are introduced.  For RDMA, the integrity, and any\n   privacy, of the data stream\
    \ are of particular importance.\n   The core goals of an NFS-to-RDMA binding are\
    \ to reduce overhead and\n   to enable high performance.  To support these goals\
    \ while maintaining\n   required NFS security protection presents a special challenge.\n\
    \   Historically, the provision of integrity and privacy have been\n   implemented\
    \ within the RPC layer, and their operation requires local\n   processing of messages\
    \ exchanged with the RPC peer.  This processing\n   imposes memory and processing\
    \ overhead on a per-message basis,\n   exactly the overhead that RDMA is designed\
    \ to avoid.\n   Therefore, it is a requirement that the RDMA transport binding\n\
    \   provide a means to delegate the integrity and privacy processing to\n   the\
    \ RDMA hardware, in order to maintain the high level of performance\n   desired\
    \ from the approach, while simultaneously providing the\n   existing highest levels\
    \ of security required by the NFS protocol.\n   This in turn requires a means\
    \ by which the RPC layer may invoke these\n   services from the RDMA provider,\
    \ and for the NFS layer to negotiate\n   their use end-to-end.\n   The \"Channel\
    \ Binding\" concept [RFC5056] together with \"IPsec Channel\n   Connection Latching\"\
    \ [BTNSLATCH] provide a means by which the RPC and\n   NFS layers may delegate\
    \ their session protection to the lower RDMA\n   layers.  An extension to the\
    \ RPCSEC_GSS protocol [RFC5403] may be\n   employed to negotiate the use of these\
    \ bindings, and to establish the\n   shared secrets necessary to protect the sessions.\n\
    \   The protocol described in [RPCRDMA] specifies the use of these\n   mechanisms,\
    \ and they are required to implement the protocol.\n   An additional consideration\
    \ is protection of the integrity and\n   privacy of local memory by the RDMA transport\
    \ itself.  The use of\n   RDMA by NFS must not introduce any vulnerabilities to\
    \ system memory\n   contents, or to memory owned by user processes.  These protections\n\
    \   are provided by the RDMA layer specifications, and specifically their\n  \
    \ security models.  It is required that any RDMA provider used for NFS\n   transport\
    \ be conformant to the requirements of [RFC5042] in order to\n   satisfy these\
    \ protections.\n"
- title: 8.  Acknowledgments
  contents:
  - "8.  Acknowledgments\n   The authors wish to thank Jeff Chase who provided many\
    \ useful\n   suggestions.\n"
- title: 9.  References
  contents:
  - '9.  References

    '
- title: 9.1.  Normative References
  contents:
  - "9.1.  Normative References\n   [RFC3530]   Shepler, S., Callaghan, B., Robinson,\
    \ D., Thurlow, R.,\n               Beame, C., Eisler, M., and D. Noveck, \"Network\
    \ File\n               System (NFS) version 4 Protocol\", RFC 3530, April 2003.\n\
    \   [RFC5531]   Thurlow, R., \"RPC: Remote Procedure Call Protocol\n         \
    \      Specification Version 2\", RFC 5531, May 2009.\n   [RFC4506]   Eisler,\
    \ M., Ed., \"XDR: External Data Representation\n               Standard\", STD\
    \ 67, RFC 4506, May 2006.\n   [RFC1813]   Callaghan, B., Pawlowski, B., and P.\
    \ Staubach, \"NFS\n               Version 3 Protocol Specification\", RFC 1813,\
    \ June 1995.\n   [RFC5403]   Eisler, M., \"RPCSEC_GSS Version 2\", RFC 5403, February\n\
    \               2009.\n   [RFC5056]   Williams, N., \"On the Use of Channel Bindings\
    \ to Secure\n               Channels\", RFC 5056, November 2007.\n   [RFC5042]\
    \   Pinkerton, J. and E. Deleganes, \"Direct Data Placement\n               Protocol\
    \ (DDP) / Remote Direct Memory Access Protocol\n               (RDMAP) Security\"\
    , RFC 5042, October 2007.\n"
- title: 9.2.  Informative References
  contents:
  - "9.2.  Informative References\n   [BRU99]     J. Brustoloni, \"Interoperation\
    \ of copy avoidance in\n               network and file I/O\", in Proc. INFOCOM\
    \ '99, pages 534-\n               542, New York, NY, Mar. 1999., IEEE.  Also available\
    \ from\n               http://www.cs.pitt.edu/~jcb/publs.html.\n   [BTNSLATCH]\
    \ Williams, N., \"IPsec Channels: Connection Latching\", Work\n              \
    \ in Progress, November 2008.\n   [CAL+03]    B. Callaghan, T. Lingutla-Raj, A.\
    \ Chiu, P. Staubach, O.\n               Asad, \"NFS over RDMA\", in Proceedings\
    \ of ACM SIGCOMM\n               Summer 2003 NICELI Workshop.\n   [CHA+01]   \
    \ J. S. Chase, A. J. Gallatin, K. G. Yocum, \"Endsystem\n               optimizations\
    \ for high-speed TCP\", IEEE Communications,\n               39(4):68-74, April\
    \ 2001.\n   [CHA+99]    J. S. Chase, D. C. Anderson, A. J. Gallatin, A. R.\n \
    \              Lebeck, K.  G. Yocum, \"Network I/O with Trapeze\", in 1999\n \
    \              Hot Interconnects Symposium, August 1999.\n   [CHU96]     H.K.\
    \ Chu, \"Zero-copy TCP in Solaris\", Proc. of the USENIX\n               1996\
    \ Annual Technical Conference, San Diego, CA, January\n               1996.\n\
    \   [DCK+03]    M. DeBergalis, P. Corbett, S. Kleiman, A. Lent, D.\n         \
    \      Noveck, T.  Talpey, M. Wittle, \"The Direct Access File\n             \
    \  System\", in Proceedings of 2nd USENIX Conference on File\n               and\
    \ Storage Technologies (FAST '03), San Francisco, CA,\n               March 31\
    \ - April 2, 2003.\n   [FJDAFS]    Fujitsu Prime Software Technologies, \"Meet\
    \ the DAFS\n               Performance with DAFS/VI Kernel Implementation using\n\
    \               cLAN\", available from\n               http://www.pst.fujitsu.com/english/dafsdemo/index.html,\n\
    \               2001.\n   [FJNFS]     Fujitsu Prime Software Technologies, \"\
    An Adaptation of\n               VIA to NFS on Linux\", available from\n     \
    \          http://www.pst.fujitsu.com/english/nfs/index.html, 2000.\n   [GAL+99]\
    \    A. Gallatin, J. Chase, K. Yocum, \"Trapeze/IP: TCP/IP at\n              \
    \ Near-Gigabit Speeds\", 1999 USENIX Technical Conference\n               (Freenix\
    \ Track), June 1999.\n   [KM02]      K. Magoutis, \"Design and Implementation\
    \ of a Direct\n               Access File System (DAFS) Kernel Server for FreeBSD\"\
    , in\n               Proceedings of USENIX BSDCon 2002 Conference, San\n     \
    \          Francisco, CA, February 11-14, 2002.\n   [MAF+02]    K. Magoutis, S.\
    \ Addetia, A. Fedorova, M. Seltzer, J.\n               Chase, D.  Gallatin, R.\
    \ Kisley, R. Wickremesinghe, E.\n               Gabber, \"Structure and Performance\
    \ of the Direct Access\n               File System (DAFS)\", in Proceedings of\
    \ 2002 USENIX Annual\n               Technical Conference, Monterey, CA, June\
    \ 9-14, 2002.\n   [MOG03]     J. Mogul, \"TCP offload is a dumb idea whose time\
    \ has\n               come\", 9th Workshop on Hot Topics in Operating Systems\n\
    \               (HotOS IX), Lihue, HI, May 2003. USENIX.\n   [NFSv4.1]   Shepler,\
    \ S., Eisler, M., and D. Noveck, \"NFSv4 Minor\n               Version 1\", Work\
    \ in Progress, September 2008.\n   [PAI+00]    V. S. Pai, P. Druschel, W. Zwaenepoel,\
    \ \"IO-Lite: a\n               unified I/O buffering and caching system\", ACM\
    \ Trans.\n               Computer Systems, 18(1):37-66, Feb. 2000.\n   [RDDP]\
    \      RDDP Working Group charter,\n               http://www.ietf.org/html.charters/rddpcharter.html.\n\
    \   [RFC4297]   Romanow, A., Mogul, J., Talpey, T., and S. Bailey,\n         \
    \      \"Remote Direct Memory Access (RDMA) over IP Problem\n               Statement\"\
    , RFC 4297, December 2005.\n   [RFC1094]   Sun Microsystems, \"NFS: Network File\
    \ System Protocol\n               specification\", RFC 1094, March 1989.\n   [RPCRDMA]\
    \   Talpey, T. and B. Callaghan, \"Remote Direct Memory Access\n             \
    \  Transport for Remote Procedure Call\", Work in Progress,\n               April\
    \ 2008.\n   [SHI+03]    P. Shivam, J. Chase, \"On the Elusive Benefits of Protocol\n\
    \               Offload\", Proceedings of ACM SIGCOMM Summer 2003 NICELI\n   \
    \            Workshop, also available from\n               http://issg.cs.duke.edu/publications/niceli03.pdf.\n\
    \   [SKE+01]    K.-A. Skevik, T. Plagemann, V. Goebel, P. Halvorsen,\n       \
    \        \"Evaluation of a Zero-Copy Protocol Implementation\", in\n         \
    \      Proceedings of the 27th Euromicro Conference - Multimedia\n           \
    \    and Telecommunications Track (MTT'2001), Warsaw, Poland,\n              \
    \ September 2001.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Tom Talpey\n   170 Whitman St.\n   Stow, MA 01775 USA\n\
    \   Phone: +1 978 821-8577\n   EMail: tmtalpey@gmail.com\n   Chet Juszczak\n \
    \  P.O. Box 1467\n   Merrimack, NH 03054\n   Phone: +1 603 253-6602\n   EMail:\
    \ chetnh@earthlink.net\n"
