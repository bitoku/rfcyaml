- title: __initial_text__
  contents:
  - '                Parallel NFS (pNFS) Block/Volume Layout

    '
- title: Abstract
  contents:
  - "Abstract\n   Parallel NFS (pNFS) extends Network File Sharing version 4 (NFSv4)\
    \ to\n   allow clients to directly access file data on the storage used by the\n\
    \   NFSv4 server.  This ability to bypass the server for data access can\n   increase\
    \ both performance and parallelism, but requires additional\n   client functionality\
    \ for data access, some of which is dependent on\n   the class of storage used.\
    \  The main pNFS operations document\n   specifies storage-class-independent extensions\
    \ to NFS; this document\n   specifies the additional extensions (primarily data\
    \ structures) for\n   use of pNFS with block- and volume-based storage.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This\
    \ document is a product of the Internet Engineering Task Force\n   (IETF).  It\
    \ represents the consensus of the IETF community.  It has\n   received public\
    \ review and has been approved for publication by the\n   Internet Engineering\
    \ Steering Group (IESG).  Further information on\n   Internet Standards is available\
    \ in Section 2 of RFC 5741.\n   Information about the current status of this document,\
    \ any errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc5663.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2010 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................4\n\
    \      1.1. Conventions Used in This Document ..........................4\n  \
    \    1.2. General Definitions ........................................5\n    \
    \  1.3. Code Components Licensing Notice ...........................5\n      1.4.\
    \ XDR Description ............................................5\n   2. Block Layout\
    \ Description ........................................7\n      2.1. Background\
    \ and Architecture ................................7\n      2.2. GETDEVICELIST\
    \ and GETDEVICEINFO ............................9\n           2.2.1. Volume Identification\
    \ ...............................9\n           2.2.2. Volume Topology ....................................10\n\
    \           2.2.3. GETDEVICELIST and GETDEVICEINFO deviceid4 ..........12\n  \
    \    2.3. Data Structures: Extents and Extent Lists .................12\n    \
    \       2.3.1. Layout Requests and Extent Lists ...................15\n      \
    \     2.3.2. Layout Commits .....................................16\n        \
    \   2.3.3. Layout Returns .....................................16\n          \
    \ 2.3.4. Client Copy-on-Write Processing ....................17\n           2.3.5.\
    \ Extents are Permissions ............................18\n           2.3.6. End-of-file\
    \ Processing .............................20\n           2.3.7. Layout Hints .......................................20\n\
    \           2.3.8. Client Fencing .....................................21\n  \
    \    2.4. Crash Recovery Issues .....................................23\n    \
    \  2.5. Recalling Resources: CB_RECALL_ANY ........................23\n      2.6.\
    \ Transient and Permanent Errors ............................24\n   3. Security\
    \ Considerations ........................................24\n   4. Conclusions\
    \ ....................................................26\n   5. IANA Considerations\
    \ ............................................26\n   6. Acknowledgments ................................................26\n\
    \   7. References .....................................................27\n  \
    \    7.1. Normative References ......................................27\n    \
    \  7.2. Informative References ....................................27\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Figure 1 shows the overall architecture of a Parallel NFS\
    \ (pNFS)\n   system:\n      +-----------+\n      |+-----------+              \
    \                   +-----------+\n      ||+-----------+                     \
    \           |           |\n      |||           |       NFSv4.1 + pNFS        \
    \   |           |\n      +||  Clients  |<------------------------------>|   Server\
    \  |\n       +|           |                                |           |\n   \
    \     +-----------+                                |           |\n           \
    \  |||                                     +-----------+\n             |||   \
    \                                        |\n             |||                 \
    \                          |\n             ||| Storage        +-----------+  \
    \            |\n             ||| Protocol       |+-----------+             |\n\
    \             ||+----------------||+-----------+  Control   |\n             |+-----------------|||\
    \           |    Protocol|\n             +------------------+||  Storage  |------------+\n\
    \                                 +|  Systems  |\n                           \
    \       +-----------+\n                         Figure 1: pNFS Architecture\n\
    \   The overall approach is that pNFS-enhanced clients obtain sufficient\n   information\
    \ from the server to enable them to access the underlying\n   storage (on the\
    \ storage systems) directly.  See the pNFS portion of\n   [NFSv4.1] for more details.\
    \  This document is concerned with access\n   from pNFS clients to storage systems\
    \ over storage protocols based on\n   blocks and volumes, such as the Small Computer\
    \ System Interface\n   (SCSI) protocol family (e.g., parallel SCSI, Fibre Channel\
    \ Protocol\n   (FCP) for Fibre Channel, Internet SCSI (iSCSI), Serial Attached\
    \ SCSI\n   (SAS), and Fibre Channel over Ethernet (FCoE)).  This class of\n  \
    \ storage is referred to as block/volume storage.  While the Server to\n   Storage\
    \ System protocol, called the \"Control Protocol\", is not of\n   concern for\
    \ interoperability here, it will typically also be a\n   block/volume protocol\
    \ when clients use block/ volume protocols.\n"
- title: 1.1.  Conventions Used in This Document
  contents:
  - "1.1.  Conventions Used in This Document\n   The key words \"MUST\", \"MUST NOT\"\
    , \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\"\
    , \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in RFC 2119 [RFC2119].\n"
- title: 1.2.  General Definitions
  contents:
  - "1.2.  General Definitions\n   The following definitions are provided for the\
    \ purpose of providing\n   an appropriate context for the reader.\n   Byte\n \
    \     This document defines a byte as an octet, i.e., a datum exactly 8\n    \
    \  bits in length.\n   Client\n      The \"client\" is the entity that accesses\
    \ the NFS server's\n      resources.  The client may be an application that contains\
    \ the\n      logic to access the NFS server directly.  The client may also be\n\
    \      the traditional operating system client that provides remote file\n   \
    \   system services for a set of applications.\n   Server\n      The \"server\"\
    \ is the entity responsible for coordinating client\n      access to a set of\
    \ file systems and is identified by a server\n      owner.\n"
- title: 1.3.  Code Components Licensing Notice
  contents:
  - "1.3.  Code Components Licensing Notice\n   The external data representation (XDR)\
    \ description and scripts for\n   extracting the XDR description are Code Components\
    \ as described in\n   Section 4 of \"Legal Provisions Relating to IETF Documents\"\
    \ [LEGAL].\n   These Code Components are licensed according to the terms of Section\n\
    \   4 of \"Legal Provisions Relating to IETF Documents\".\n"
- title: 1.4.  XDR Description
  contents:
  - "1.4.  XDR Description\n   This document contains the XDR ([XDR]) description\
    \ of the NFSv4.1\n   block layout protocol.  The XDR description is embedded in\
    \ this\n   document in a way that makes it simple for the reader to extract into\n\
    \   a ready-to-compile form.  The reader can feed this document into the\n   following\
    \ shell script to produce the machine readable XDR\n   description of the NFSv4.1\
    \ block layout:\n   #!/bin/sh\n   grep '^ *///' $* | sed 's?^ */// ??' | sed 's?^\
    \  *///$??'\n   That is, if the above script is stored in a file called \"extract.sh\"\
    ,\n   and this document is in a file called \"spec.txt\", then the reader can\n\
    \   do:\n      sh extract.sh < spec.txt > nfs4_block_layout_spec.x\n   The effect\
    \ of the script is to remove both leading white space and a\n   sentinel sequence\
    \ of \"///\" from each matching line.\n   The embedded XDR file header follows,\
    \ with subsequent pieces embedded\n   throughout the document:\n   /// /*\n  \
    \ ///  * This code was derived from RFC 5663.\n   ///  * Please reproduce this\
    \ note if possible.\n   ///  */\n   /// /*\n   ///  * Copyright (c) 2010 IETF\
    \ Trust and the persons identified\n   ///  * as the document authors.  All rights\
    \ reserved.\n   ///  *\n   ///  * Redistribution and use in source and binary\
    \ forms, with\n   ///  * or without modification, are permitted provided that\
    \ the\n   ///  * following conditions are met:\n   ///  *\n   ///  * - Redistributions\
    \ of source code must retain the above\n   ///  *   copyright notice, this list\
    \ of conditions and the\n   ///  *   following disclaimer.\n   ///  *\n   ///\
    \  * - Redistributions in binary form must reproduce the above\n   ///  *   copyright\
    \ notice, this list of conditions and the\n   ///  *   following disclaimer in\
    \ the documentation and/or other\n   ///  *   materials provided with the distribution.\n\
    \   ///  *\n   ///  * - Neither the name of Internet Society, IETF or IETF\n \
    \  ///  *   Trust, nor the names of specific contributors, may be\n   ///  * \
    \  used to endorse or promote products derived from this\n   ///  *   software\
    \ without specific prior written permission.\n   ///  *\n   ///  *   THIS SOFTWARE\
    \ IS PROVIDED BY THE COPYRIGHT HOLDERS\n   ///  *   AND CONTRIBUTORS \"AS IS\"\
    \ AND ANY EXPRESS OR IMPLIED\n   ///  *   WARRANTIES, INCLUDING, BUT NOT LIMITED\
    \ TO, THE\n   ///  *   IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n   ///\
    \  *   FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n   ///  *   EVENT SHALL\
    \ THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n   ///  *   LIABLE FOR ANY DIRECT, INDIRECT,\
    \ INCIDENTAL, SPECIAL,\n   ///  *   EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\
    \ BUT\n   ///  *   NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n   ///\
    \  *   SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n   ///  *   INTERRUPTION)\
    \ HOWEVER CAUSED AND ON ANY THEORY OF\n   ///  *   LIABILITY, WHETHER IN CONTRACT,\
    \ STRICT LIABILITY,\n   ///  *   OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING\n\
    \   ///  *   IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n   ///  *  \
    \ ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n   ///  */\n   ///\n   /// /*\n\
    \   ///  *      nfs4_block_layout_prot.x\n   ///  */\n   ///\n   /// %#include\
    \ \"nfsv41.h\"\n   ///\n   The XDR code contained in this document depends on\
    \ types from the\n   nfsv41.x file.  This includes both nfs types that end with\
    \ a 4, such\n   as offset4, length4, etc., as well as more generic types such\
    \ as\n   uint32_t and uint64_t.\n"
- title: 2.  Block Layout Description
  contents:
  - '2.  Block Layout Description

    '
- title: 2.1.  Background and Architecture
  contents:
  - "2.1.  Background and Architecture\n   The fundamental storage abstraction supported\
    \ by block/volume storage\n   is a storage volume consisting of a sequential series\
    \ of fixed-size\n   blocks.  This can be thought of as a logical disk; it may\
    \ be realized\n   by the storage system as a physical disk, a portion of a physical\n\
    \   disk, or something more complex (e.g., concatenation, striping, RAID,\n  \
    \ and combinations thereof) involving multiple physical disks or\n   portions\
    \ thereof.\n   A pNFS layout for this block/volume class of storage is responsible\n\
    \   for mapping from an NFS file (or portion of a file) to the blocks of\n   storage\
    \ volumes that contain the file.  The blocks are expressed as\n   extents with\
    \ 64-bit offsets and lengths using the existing NFSv4\n   offset4 and length4\
    \ types.  Clients must be able to perform I/O to\n   the block extents without\
    \ affecting additional areas of storage\n   (especially important for writes);\
    \ therefore, extents MUST be aligned\n   to 512-byte boundaries, and writable\
    \ extents MUST be aligned to the\n   block size used by the NFSv4 server in managing\
    \ the actual file\n   system (4 kilobytes and 8 kilobytes are common block sizes).\
    \  This\n   block size is available as the NFSv4.1 layout_blksize attribute.\n\
    \   [NFSv4.1].  Readable extents SHOULD be aligned to the block size used\n  \
    \ by the NFSv4 server, but in order to support legacy file systems with\n   fragments,\
    \ alignment to 512-byte boundaries is acceptable.\n   The pNFS operation for requesting\
    \ a layout (LAYOUTGET) includes the\n   \"layoutiomode4 loga_iomode\" argument,\
    \ which indicates whether the\n   requested layout is for read-only use or read-write\
    \ use.  A read-only\n   layout may contain holes that are read as zero, whereas\
    \ a read-write\n   layout will contain allocated, but un-initialized storage in\
    \ those\n   holes (read as zero, can be written by client).  This document also\n\
    \   supports client participation in copy-on-write (e.g., for file\n   systems\
    \ with snapshots) by providing both read-only and un-\n   initialized storage\
    \ for the same range in a layout.  Reads are\n   initially performed on the read-only\
    \ storage, with writes going to\n   the un-initialized storage.  After the first\
    \ write that initializes\n   the un-initialized storage, all reads are performed\
    \ to that now-\n   initialized writable storage, and the corresponding read-only\
    \ storage\n   is no longer used.\n   The block/volume layout solution expands\
    \ the security\n   responsibilities of the pNFS clients, and there are a number\
    \ of\n   environments where the mandatory to implement security properties for\n\
    \   NFS cannot be satisfied.  The additional security responsibilities of\n  \
    \ the client follow, and a full discussion is present in Section 3,\n   \"Security\
    \ Considerations\".\n   o  Typically, storage area network (SAN) disk arrays and\
    \ SAN\n      protocols provide access control mechanisms (e.g., Logical Unit\n\
    \      Number (LUN) mapping and/or masking), which operate at the\n      granularity\
    \ of individual hosts, not individual blocks.  For this\n      reason, block-based\
    \ protection must be provided by the client\n      software.\n   o  Similarly,\
    \ SAN disk arrays and SAN protocols typically are not\n      able to validate\
    \ NFS locks that apply to file regions.  For\n      instance, if a file is covered\
    \ by a mandatory read-only lock, the\n      server can ensure that only readable\
    \ layouts for the file are\n      granted to pNFS clients.  However, it is up\
    \ to each pNFS client to\n      ensure that the readable layout is used only to\
    \ service read\n      requests, and not to allow writes to the existing parts\
    \ of the\n      file.\n   Since block/volume storage systems are generally not\
    \ capable of\n   enforcing such file-based security, in environments where pNFS\n\
    \   clients cannot be trusted to enforce such policies, pNFS block/volume\n  \
    \ storage layouts SHOULD NOT be used.\n"
- title: 2.2.  GETDEVICELIST and GETDEVICEINFO
  contents:
  - '2.2.  GETDEVICELIST and GETDEVICEINFO

    '
- title: 2.2.1.  Volume Identification
  contents:
  - "2.2.1.  Volume Identification\n   Storage systems such as storage arrays can\
    \ have multiple physical\n   network ports that need not be connected to a common\
    \ network,\n   resulting in a pNFS client having simultaneous multipath access\
    \ to\n   the same storage volumes via different ports on different networks.\n\
    \   The networks may not even be the same technology -- for example,\n   access\
    \ to the same volume via both iSCSI and Fibre Channel is\n   possible, hence network\
    \ addresses are difficult to use for volume\n   identification.  For this reason,\
    \ this pNFS block layout identifies\n   storage volumes by content, for example\
    \ providing the means to match\n   (unique portions of) labels used by volume\
    \ managers.  Volume\n   identification is performed by matching one or more opaque\
    \ byte\n   sequences to specific parts of the stored data.  Any block pNFS\n \
    \  system using this layout MUST support a means of content-based unique\n   volume\
    \ identification that can be employed via the data structure\n   given here.\n\
    \   /// struct pnfs_block_sig_component4 { /* disk signature component */\n  \
    \ ///     int64_t bsc_sig_offset;        /* byte offset of component\n   /// \
    \                                      on volume*/\n   ///     opaque  bsc_contents<>;\
    \        /* contents of this component\n   ///                               \
    \        of the signature */\n   /// };\n   ///\n   Note that the opaque \"bsc_contents\"\
    \ field in the\n   \"pnfs_block_sig_component4\" structure MUST NOT be interpreted\
    \ as a\n   zero-terminated string, as it may contain embedded zero-valued bytes.\n\
    \   There are no restrictions on alignment (e.g., neither bsc_sig_offset\n   nor\
    \ the length are required to be multiples of 4).  The\n   bsc_sig_offset is a\
    \ signed quantity, which, when positive, represents\n   an byte offset from the\
    \ start of the volume, and when negative\n   represents an byte offset from the\
    \ end of the volume.\n   Negative offsets are permitted in order to simplify the\
    \ client\n   implementation on systems where the device label is found at a fixed\n\
    \   offset from the end of the volume.  If the server uses negative\n   offsets\
    \ to describe the signature, then the client and server MUST\n   NOT see different\
    \ volume sizes.  Negative offsets SHOULD NOT be used\n   in systems that dynamically\
    \ resize volumes unless care is taken to\n   ensure that the device label is always\
    \ present at the offset from the\n   end of the volume as seen by the clients.\n\
    \   A signature is an array of up to \"PNFS_BLOCK_MAX_SIG_COMP\" (defined\n  \
    \ below) signature components.  The client MUST NOT assume that all\n   signature\
    \ components are co-located within a single sector on a block\n   device.\n  \
    \ The pNFS client block layout driver uses this volume identification\n   to map\
    \ pnfs_block_volume_type4 PNFS_BLOCK_VOLUME_SIMPLE deviceid4s to\n   its local\
    \ view of a LUN.\n"
- title: 2.2.2.  Volume Topology
  contents:
  - "2.2.2.  Volume Topology\n   The pNFS block server volume topology is expressed\
    \ as an arbitrary\n   combination of base volume types enumerated in the following\
    \ data\n   structures.  The individual components of the topology are contained\n\
    \   in an array and components may refer to other components by using\n   array\
    \ indices.\n   /// enum pnfs_block_volume_type4 {\n   ///     PNFS_BLOCK_VOLUME_SIMPLE\
    \ = 0,  /* volume maps to a single\n   ///                                   \
    \    LU */\n   ///     PNFS_BLOCK_VOLUME_SLICE  = 1,  /* volume is a slice of\n\
    \   ///                                       another volume */\n   ///     PNFS_BLOCK_VOLUME_CONCAT\
    \ = 2,  /* volume is a\n   ///                                       concatenation\
    \ of\n   ///                                       multiple volumes */\n   ///\
    \     PNFS_BLOCK_VOLUME_STRIPE = 3   /* volume is striped across\n   ///     \
    \                                  multiple volumes */\n   /// };\n   ///\n  \
    \ /// const PNFS_BLOCK_MAX_SIG_COMP = 16;/* maximum components per\n   ///   \
    \                                    signature */\n   /// struct pnfs_block_simple_volume_info4\
    \ {\n   ///     pnfs_block_sig_component4 bsv_ds<PNFS_BLOCK_MAX_SIG_COMP>;\n \
    \  ///                                    /* disk signature */\n   /// };\n  \
    \ ///\n   ///\n   /// struct pnfs_block_slice_volume_info4 {\n   ///     offset4\
    \  bsv_start;            /* offset of the start of the\n   ///               \
    \                        slice in bytes */\n   ///     length4  bsv_length;  \
    \         /* length of slice in bytes */\n   ///     uint32_t bsv_volume;    \
    \       /* array index of sliced\n   ///                                     \
    \  volume */\n   /// };\n   ///\n   /// struct pnfs_block_concat_volume_info4\
    \ {\n   ///     uint32_t  bcv_volumes<>;       /* array indices of volumes\n \
    \  ///                                       which are concatenated */\n   ///\
    \ };\n   ///\n   /// struct pnfs_block_stripe_volume_info4 {\n   ///     length4\
    \  bsv_stripe_unit;      /* size of stripe in bytes */\n   ///     uint32_t bsv_volumes<>;\
    \        /* array indices of volumes\n   ///                                 \
    \      which are striped across --\n   ///                                   \
    \    MUST be same size */\n   /// };\n   ///\n   /// union pnfs_block_volume4\
    \ switch (pnfs_block_volume_type4 type) {\n   ///     case PNFS_BLOCK_VOLUME_SIMPLE:\n\
    \   ///         pnfs_block_simple_volume_info4 bv_simple_info;\n   ///     case\
    \ PNFS_BLOCK_VOLUME_SLICE:\n   ///         pnfs_block_slice_volume_info4 bv_slice_info;\n\
    \   ///     case PNFS_BLOCK_VOLUME_CONCAT:\n   ///         pnfs_block_concat_volume_info4\
    \ bv_concat_info;\n   ///     case PNFS_BLOCK_VOLUME_STRIPE:\n   ///         pnfs_block_stripe_volume_info4\
    \ bv_stripe_info;\n   /// };\n   ///\n   /// /* block layout specific type for\
    \ da_addr_body */\n   /// struct pnfs_block_deviceaddr4 {\n   ///     pnfs_block_volume4\
    \ bda_volumes<>; /* array of volumes */\n   /// };\n   ///\n   The \"pnfs_block_deviceaddr4\"\
    \ data structure is a structure that\n   allows arbitrarily complex nested volume\
    \ structures to be encoded.\n   The types of aggregations that are allowed are\
    \ stripes,\n   concatenations, and slices.  Note that the volume topology expressed\n\
    \   in the pnfs_block_deviceaddr4 data structure will always resolve to a\n  \
    \ set of pnfs_block_volume_type4 PNFS_BLOCK_VOLUME_SIMPLE.  The array\n   of volumes\
    \ is ordered such that the root of the volume hierarchy is\n   the last element\
    \ of the array.  Concat, slice, and stripe volumes\n   MUST refer to volumes defined\
    \ by lower indexed elements of the array.\n   The \"pnfs_block_device_addr4\"\
    \ data structure is returned by the\n   server as the storage-protocol-specific\
    \ opaque field da_addr_body in\n   the \"device_addr4\" structure by a successful\
    \ GETDEVICEINFO operation\n   [NFSv4.1].\n   As noted above, all device_addr4\
    \ structures eventually resolve to a\n   set of volumes of type PNFS_BLOCK_VOLUME_SIMPLE.\
    \  These volumes are\n   each uniquely identified by a set of signature components.\n\
    \   Complicated volume hierarchies may be composed of dozens of volumes\n   each\
    \ with several signature components; thus, the device address may\n   require\
    \ several kilobytes.  The client SHOULD be prepared to allocate\n   a large buffer\
    \ to contain the result.  In the case of the server\n   returning NFS4ERR_TOOSMALL,\
    \ the client SHOULD allocate a buffer of at\n   least gdir_mincount_bytes to contain\
    \ the expected result and retry\n   the GETDEVICEINFO request.\n"
- title: 2.2.3.  GETDEVICELIST and GETDEVICEINFO deviceid4
  contents:
  - "2.2.3.  GETDEVICELIST and GETDEVICEINFO deviceid4\n   The server in response\
    \ to a GETDEVICELIST request typically will\n   return a single \"deviceid4\"\
    \ in the gdlr_deviceid_list array.  This is\n   because the deviceid4 when passed\
    \ to GETDEVICEINFO will return a\n   \"device_addr4\", which encodes the entire\
    \ volume hierarchy.  In the\n   case of copy-on-write file systems, the \"gdlr_deviceid_list\"\
    \ array\n   may contain two deviceid4's, one referencing the read-only volume\n\
    \   hierarchy, and one referencing the writable volume hierarchy.  There\n   is\
    \ no required ordering of the readable and writable IDs in the array\n   as the\
    \ volumes are uniquely identified by their deviceid4, and are\n   referred to\
    \ by layouts using the deviceid4.  Another example of the\n   server returning\
    \ multiple device items occurs when the file handle\n   represents the root of\
    \ a namespace spanning multiple physical file\n   systems on the server, each\
    \ with a different volume hierarchy.  In\n   this example, a server implementation\
    \ may return either a list of\n   device IDs used by each of the physical file\
    \ systems, or it may\n   return an empty list.\n   Each deviceid4 returned by\
    \ a successful GETDEVICELIST operation is a\n   shorthand id used to reference\
    \ the whole volume topology.  These\n   device IDs, as well as device IDs returned\
    \ in extents of a LAYOUTGET\n   operation, can be used as input to the GETDEVICEINFO\
    \ operation.\n   Decoding the \"pnfs_block_deviceaddr4\" results in a flat ordering\
    \ of\n   data blocks mapped to PNFS_BLOCK_VOLUME_SIMPLE volumes.  Combined\n \
    \  with the mapping to a client LUN described in Section 2.2.1 \"Volume\n   Identification\"\
    , a logical volume offset can be mapped to a block on\n   a pNFS client LUN [NFSv4.1].\n"
- title: '2.3.  Data Structures: Extents and Extent Lists'
  contents:
  - "2.3.  Data Structures: Extents and Extent Lists\n   A pNFS block layout is a\
    \ list of extents within a flat array of data\n   blocks in a logical volume.\
    \  The details of the volume topology can\n   be determined by using the GETDEVICEINFO\
    \ operation (see discussion of\n   volume identification, Section 2.2 above).\
    \  The block layout\n   describes the individual block extents on the volume that\
    \ make up the\n   file.  The offsets and length contained in an extent are specified\
    \ in\n   units of bytes.\n   /// enum pnfs_block_extent_state4 {\n   ///     PNFS_BLOCK_READ_WRITE_DATA\
    \ = 0,/* the data located by this\n   ///                                    \
    \   extent is valid\n   ///                                       for reading\
    \ and writing. */\n   ///     PNFS_BLOCK_READ_DATA      = 1, /* the data located\
    \ by this\n   ///                                       extent is valid for reading\n\
    \   ///                                       only; it may not be\n   ///    \
    \                                   written. */\n   ///     PNFS_BLOCK_INVALID_DATA\
    \   = 2, /* the location is valid; the\n   ///                               \
    \        data is invalid.  It is a\n   ///                                   \
    \    newly (pre-) allocated\n   ///                                       extent.\
    \  There is physical\n   ///                                       space on the\
    \ volume. */\n   ///     PNFS_BLOCK_NONE_DATA      = 3  /* the location is invalid.\n\
    \   ///                                       It is a hole in the file.\n   ///\
    \                                       There is no physical space\n   ///   \
    \                                    on the volume. */\n   /// };\n   ///\n  \
    \ /// struct pnfs_block_extent4 {\n   ///     deviceid4    bex_vol_id;       /*\
    \ id of logical volume on\n   ///                                       which\
    \ extent of file is\n   ///                                       stored. */\n\
    \   ///     offset4      bex_file_offset;  /* the starting byte offset in\n  \
    \ ///                                       the file */\n   ///     length4  \
    \    bex_length;       /* the size in bytes of the\n   ///                   \
    \                    extent */\n   ///     offset4      bex_storage_offset;  /*\
    \ the starting byte offset\n   ///                                       in the\
    \ volume */\n   ///     pnfs_block_extent_state4 bex_state;\n   ///          \
    \                          /* the state of this extent */\n   /// };\n   ///\n\
    \   /// /* block layout specific type for loc_body */\n   /// struct pnfs_block_layout4\
    \ {\n   ///     pnfs_block_extent4 blo_extents<>;\n   ///                    \
    \                /* extents which make up this\n   ///                       \
    \                layout. */\n   /// };\n   ///\n   The block layout consists of\
    \ a list of extents that map the logical\n   regions of the file to physical locations\
    \ on a volume.  The\n   \"bex_storage_offset\" field within each extent identifies\
    \ a location\n   on the logical volume specified by the \"bex_vol_id\" field in\
    \ the\n   extent.  The bex_vol_id itself is shorthand for the whole topology of\n\
    \   the logical volume on which the file is stored.  The client is\n   responsible\
    \ for translating this logical offset into an offset on the\n   appropriate underlying\
    \ SAN logical unit.  In most cases, all extents\n   in a layout will reside on\
    \ the same volume and thus have the same\n   bex_vol_id.  In the case of copy-on-write\
    \ file systems, the\n   PNFS_BLOCK_READ_DATA extents may have a different bex_vol_id\
    \ from the\n   writable extents.\n   Each extent maps a logical region of the\
    \ file onto a portion of the\n   specified logical volume.  The bex_file_offset,\
    \ bex_length, and\n   bex_state fields for an extent returned from the server\
    \ are valid for\n   all extents.  In contrast, the interpretation of the\n   bex_storage_offset\
    \ field depends on the value of bex_state as follows\n   (in increasing order):\n\
    \   o  PNFS_BLOCK_READ_WRITE_DATA means that bex_storage_offset is valid,\n  \
    \    and points to valid/initialized data that can be read and written.\n   o\
    \  PNFS_BLOCK_READ_DATA means that bex_storage_offset is valid and\n      points\
    \ to valid/ initialized data that can only be read.  Write\n      operations are\
    \ prohibited; the client may need to request a\n      read-write layout.\n   o\
    \  PNFS_BLOCK_INVALID_DATA means that bex_storage_offset is valid,\n      but\
    \ points to invalid un-initialized data.  This data must not be\n      physically\
    \ read from the disk until it has been initialized.  A\n      read request for\
    \ a PNFS_BLOCK_INVALID_DATA extent must fill the\n      user buffer with zeros,\
    \ unless the extent is covered by a\n      PNFS_BLOCK_READ_DATA extent of a copy-on-write\
    \ file system.  Write\n      requests must write whole server-sized blocks to\
    \ the disk; bytes\n      not initialized by the user must be set to zero.  Any\
    \ write to\n      storage in a PNFS_BLOCK_INVALID_DATA extent changes the written\n\
    \      portion of the extent to PNFS_BLOCK_READ_WRITE_DATA; the pNFS\n      client\
    \ is responsible for reporting this change via LAYOUTCOMMIT.\n   o  PNFS_BLOCK_NONE_DATA\
    \ means that bex_storage_offset is not valid,\n      and this extent may not be\
    \ used to satisfy write requests.  Read\n      requests may be satisfied by zero-filling\
    \ as for\n      PNFS_BLOCK_INVALID_DATA.  PNFS_BLOCK_NONE_DATA extents may be\n\
    \      returned by requests for readable extents; they are never returned\n  \
    \    if the request was for a writable extent.\n   An extent list contains all\
    \ relevant extents in increasing order of\n   the bex_file_offset of each extent;\
    \ any ties are broken by increasing\n   order of the extent state (bex_state).\n"
- title: 2.3.1.  Layout Requests and Extent Lists
  contents:
  - "2.3.1.  Layout Requests and Extent Lists\n   Each request for a layout specifies\
    \ at least three parameters: file\n   offset, desired size, and minimum size.\
    \  If the status of a request\n   indicates success, the extent list returned\
    \ must meet the following\n   criteria:\n   o  A request for a readable (but not\
    \ writable) layout returns only\n      PNFS_BLOCK_READ_DATA or PNFS_BLOCK_NONE_DATA\
    \ extents (but not\n      PNFS_BLOCK_INVALID_DATA or PNFS_BLOCK_READ_WRITE_DATA\
    \ extents).\n   o  A request for a writable layout returns PNFS_BLOCK_READ_WRITE_DATA\n\
    \      or PNFS_BLOCK_INVALID_DATA extents (but not PNFS_BLOCK_NONE_DATA\n    \
    \  extents).  It may also return PNFS_BLOCK_READ_DATA extents only\n      when\
    \ the offset ranges in those extents are also covered by\n      PNFS_BLOCK_INVALID_DATA\
    \ extents to permit writes.\n   o  The first extent in the list MUST contain the\
    \ requested starting\n      offset.\n   o  The total size of extents within the\
    \ requested range MUST cover at\n      least the minimum size.  One exception\
    \ is allowed: the total size\n      MAY be smaller if only readable extents were\
    \ requested and EOF is\n      encountered.\n   o  Extents in the extent list MUST\
    \ be logically contiguous for a\n      read-only layout.  For a read-write layout,\
    \ the set of writable\n      extents (i.e., excluding PNFS_BLOCK_READ_DATA extents)\
    \ MUST be\n      logically contiguous.  Every PNFS_BLOCK_READ_DATA extent in a\n\
    \      read-write layout MUST be covered by one or more\n      PNFS_BLOCK_INVALID_DATA\
    \ extents.  This overlap of\n      PNFS_BLOCK_READ_DATA and PNFS_BLOCK_INVALID_DATA\
    \ extents is the\n      only permitted extent overlap.\n   o  Extents MUST be\
    \ ordered in the list by starting offset, with\n      PNFS_BLOCK_READ_DATA extents\
    \ preceding PNFS_BLOCK_INVALID_DATA\n      extents in the case of equal bex_file_offsets.\n\
    \   If the minimum requested size, loga_minlength, is zero, this is an\n   indication\
    \ to the metadata server that the client desires any layout\n   at offset loga_offset\
    \ or less that the metadata server has \"readily\n   available\".  Readily is\
    \ subjective, and depends on the layout type\n   and the pNFS server implementation.\
    \  For block layout servers,\n   readily available SHOULD be interpreted such\
    \ that readable layouts\n   are always available, even if some extents are in\
    \ the\n   PNFS_BLOCK_NONE_DATA state.  When processing requests for writable\n\
    \   layouts, a layout is readily available if extents can be returned in\n   the\
    \ PNFS_BLOCK_READ_WRITE_DATA state.\n"
- title: 2.3.2.  Layout Commits
  contents:
  - "2.3.2.  Layout Commits\n   /// /* block layout specific type for lou_body */\n\
    \   /// struct pnfs_block_layoutupdate4 {\n   ///     pnfs_block_extent4 blu_commit_list<>;\n\
    \   ///                                    /* list of extents which\n   ///  \
    \                                   * now contain valid data.\n   ///        \
    \                             */\n   /// };\n   ///\n   The \"pnfs_block_layoutupdate4\"\
    \ structure is used by the client as the\n   block-protocol specific argument\
    \ in a LAYOUTCOMMIT operation.  The\n   \"blu_commit_list\" field is an extent\
    \ list covering regions of the\n   file layout that were previously in the PNFS_BLOCK_INVALID_DATA\n\
    \   state, but have been written by the client and should now be\n   considered\
    \ in the PNFS_BLOCK_READ_WRITE_DATA state.  The bex_state\n   field of each extent\
    \ in the blu_commit_list MUST be set to\n   PNFS_BLOCK_READ_WRITE_DATA.  The extents\
    \ in the commit list MUST be\n   disjoint and MUST be sorted by bex_file_offset.\
    \  The\n   bex_storage_offset field is unused.  Implementors should be aware\n\
    \   that a server may be unable to commit regions at a granularity\n   smaller\
    \ than a file-system block (typically 4 KB or 8 KB).  As noted\n   above, the\
    \ block-size that the server uses is available as an NFSv4\n   attribute, and\
    \ any extents included in the \"blu_commit_list\" MUST be\n   aligned to this\
    \ granularity and have a size that is a multiple of\n   this granularity.  If\
    \ the client believes that its actions have moved\n   the end-of-file into the\
    \ middle of a block being committed, the\n   client MUST write zeroes from the\
    \ end-of-file to the end of that\n   block before committing the block.  Failure\
    \ to do so may result in\n   junk (un-initialized data) appearing in that area\
    \ if the file is\n   subsequently extended by moving the end-of-file.\n"
- title: 2.3.3.  Layout Returns
  contents:
  - "2.3.3.  Layout Returns\n   The LAYOUTRETURN operation is done without any block\
    \ layout specific\n   data.  When the LAYOUTRETURN operation specifies a\n   LAYOUTRETURN4_FILE_return\
    \ type, then the layoutreturn_file4 data\n   structure specifies the region of\
    \ the file layout that is no longer\n   needed by the client.  The opaque \"lrf_body\"\
    \ field of the\n   \"layoutreturn_file4\" data structure MUST have length zero.\
    \  A\n   LAYOUTRETURN operation represents an explicit release of resources by\n\
    \   the client, usually done for the purpose of avoiding unnecessary\n   CB_LAYOUTRECALL\
    \ operations in the future.  The client may return\n   disjoint regions of the\
    \ file by using multiple LAYOUTRETURN\n   operations within a single COMPOUND\
    \ operation.\n   Note that the block/volume layout supports unilateral layout\n\
    \   revocation.  When a layout is unilaterally revoked by the server,\n   usually\
    \ due to the client's lease time expiring, or a delegation\n   being recalled,\
    \ or the client failing to return a layout in a timely\n   manner, it is important\
    \ for the sake of correctness that any in-\n   flight I/Os that the client issued\
    \ before the layout was revoked are\n   rejected at the storage.  For the block/volume\
    \ protocol, this is\n   possible by fencing a client with an expired layout timer\
    \ from the\n   physical storage.  Note, however, that the granularity of this\n\
    \   operation can only be at the host/logical-unit level.  Thus, if one\n   of\
    \ a client's layouts is unilaterally revoked by the server, it will\n   effectively\
    \ render useless *all* of the client's layouts for files\n   located on the storage\
    \ units comprising the logical volume.  This may\n   render useless the client's\
    \ layouts for files in other file systems.\n"
- title: 2.3.4.  Client Copy-on-Write Processing
  contents:
  - "2.3.4.  Client Copy-on-Write Processing\n   Copy-on-write is a mechanism used\
    \ to support file and/or file system\n   snapshots.  When writing to unaligned\
    \ regions, or to regions smaller\n   than a file system block, the writer must\
    \ copy the portions of the\n   original file data to a new location on disk. \
    \ This behavior can\n   either be implemented on the client or the server.  The\
    \ paragraphs\n   below describe how a pNFS block layout client implements access\
    \ to a\n   file that requires copy-on-write semantics.\n   Distinguishing the\
    \ PNFS_BLOCK_READ_WRITE_DATA and\n   PNFS_BLOCK_READ_DATA extent types in combination\
    \ with the allowed\n   overlap of PNFS_BLOCK_READ_DATA extents with PNFS_BLOCK_INVALID_DATA\n\
    \   extents allows copy-on-write processing to be done by pNFS clients.\n   In\
    \ classic NFS, this operation would be done by the server.  Since\n   pNFS enables\
    \ clients to do direct block access, it is useful for\n   clients to participate\
    \ in copy-on-write operations.  All block/volume\n   pNFS clients MUST support\
    \ this copy-on-write processing.\n   When a client wishes to write data covered\
    \ by a PNFS_BLOCK_READ_DATA\n   extent, it MUST have requested a writable layout\
    \ from the server;\n   that layout will contain PNFS_BLOCK_INVALID_DATA extents\
    \ to cover all\n   the data ranges of that layout's PNFS_BLOCK_READ_DATA extents.\
    \  More\n   precisely, for any bex_file_offset range covered by one or more\n\
    \   PNFS_BLOCK_READ_DATA extents in a writable layout, the server MUST\n   include\
    \ one or more PNFS_BLOCK_INVALID_DATA extents in the layout\n   that cover the\
    \ same bex_file_offset range.  When performing a write\n   to such an area of\
    \ a layout, the client MUST effectively copy the\n   data from the PNFS_BLOCK_READ_DATA\
    \ extent for any partial blocks of\n   bex_file_offset and range, merge in the\
    \ changes to be written, and\n   write the result to the PNFS_BLOCK_INVALID_DATA\
    \ extent for the blocks\n   for that bex_file_offset and range.  That is, if entire\
    \ blocks of\n   data are to be overwritten by an operation, the corresponding\n\
    \   PNFS_BLOCK_READ_DATA blocks need not be fetched, but any partial-\n   block\
    \ writes must be merged with data fetched via\n   PNFS_BLOCK_READ_DATA extents\
    \ before storing the result via\n   PNFS_BLOCK_INVALID_DATA extents.  For the\
    \ purposes of this\n   discussion, \"entire blocks\" and \"partial blocks\" refer\
    \ to the\n   server's file-system block size.  Storing of data in a\n   PNFS_BLOCK_INVALID_DATA\
    \ extent converts the written portion of the\n   PNFS_BLOCK_INVALID_DATA extent\
    \ to a PNFS_BLOCK_READ_WRITE_DATA\n   extent; all subsequent reads MUST be performed\
    \ from this extent; the\n   corresponding portion of the PNFS_BLOCK_READ_DATA\
    \ extent MUST NOT be\n   used after storing data in a PNFS_BLOCK_INVALID_DATA\
    \ extent.  If a\n   client writes only a portion of an extent, the extent may\
    \ be split at\n   block aligned boundaries.\n   When a client wishes to write\
    \ data to a PNFS_BLOCK_INVALID_DATA\n   extent that is not covered by a PNFS_BLOCK_READ_DATA\
    \ extent, it MUST\n   treat this write identically to a write to a file not involved\
    \ with\n   copy-on-write semantics.  Thus, data must be written in at least\n\
    \   block-sized increments, aligned to multiples of block-sized offsets,\n   and\
    \ unwritten portions of blocks must be zero filled.\n   In the LAYOUTCOMMIT operation\
    \ that normally sends updated layout\n   information back to the server, for writable\
    \ data, some\n   PNFS_BLOCK_INVALID_DATA extents may be committed as\n   PNFS_BLOCK_READ_WRITE_DATA\
    \ extents, signifying that the storage at\n   the corresponding bex_storage_offset\
    \ values has been stored into and\n   is now to be considered as valid data to\
    \ be read.\n   PNFS_BLOCK_READ_DATA extents are not committed to the server. \
    \ For\n   extents that the client receives via LAYOUTGET as\n   PNFS_BLOCK_INVALID_DATA\
    \ and returns via LAYOUTCOMMIT as\n   PNFS_BLOCK_READ_WRITE_DATA, the server will\
    \ understand that the\n   PNFS_BLOCK_READ_DATA mapping for that extent is no longer\
    \ valid or\n   necessary for that file.\n"
- title: 2.3.5.  Extents are Permissions
  contents:
  - "2.3.5.  Extents are Permissions\n   Layout extents returned to pNFS clients grant\
    \ permission to read or\n   write; PNFS_BLOCK_READ_DATA and PNFS_BLOCK_NONE_DATA\
    \ are read-only\n   (PNFS_BLOCK_NONE_DATA reads as zeroes), PNFS_BLOCK_READ_WRITE_DATA\n\
    \   and PNFS_BLOCK_INVALID_DATA are read/write, (PNFS_BLOCK_INVALID_DATA\n   reads\
    \ as zeros, any write converts it to PNFS_BLOCK_READ_WRITE_DATA).\n   This is\
    \ the only means a client has of obtaining permission to\n   perform direct I/O\
    \ to storage devices; a pNFS client MUST NOT perform\n   direct I/O operations\
    \ that are not permitted by an extent held by the\n   client.  Client adherence\
    \ to this rule places the pNFS server in\n   control of potentially conflicting\
    \ storage device operations,\n   enabling the server to determine what does conflict\
    \ and how to avoid\n   conflicts by granting and recalling extents to/from clients.\n\
    \   Block/volume class storage devices are not required to perform read\n   and\
    \ write operations atomically.  Overlapping concurrent read and\n   write operations\
    \ to the same data may cause the read to return a\n   mixture of before-write\
    \ and after-write data.  Overlapping write\n   operations can be worse, as the\
    \ result could be a mixture of data\n   from the two write operations; data corruption\
    \ can occur if the\n   underlying storage is striped and the operations complete\
    \ in\n   different orders on different stripes.  When there are multiple\n   clients\
    \ who wish to access the same data, a pNFS server can avoid\n   these conflicts\
    \ by implementing a concurrency control policy of\n   single writer XOR multiple\
    \ readers.  This policy MUST be implemented\n   when storage devices do not provide\
    \ atomicity for concurrent\n   read/write and write/write operations to the same\
    \ data.\n   If a client makes a layout request that conflicts with an existing\n\
    \   layout delegation, the request will be rejected with the error\n   NFS4ERR_LAYOUTTRYLATER.\
    \  This client is then expected to retry the\n   request after a short interval.\
    \  During this interval, the server\n   SHOULD recall the conflicting portion\
    \ of the layout delegation from\n   the client that currently holds it.  This\
    \ reject-and-retry approach\n   does not prevent client starvation when there\
    \ is contention for the\n   layout of a particular file.  For this reason, a pNFS\
    \ server SHOULD\n   implement a mechanism to prevent starvation.  One possibility\
    \ is that\n   the server can maintain a queue of rejected layout requests.  Each\n\
    \   new layout request can be checked to see if it conflicts with a\n   previous\
    \ rejected request, and if so, the newer request can be\n   rejected.  Once the\
    \ original requesting client retries its request,\n   its entry in the rejected\
    \ request queue can be cleared, or the entry\n   in the rejected request queue\
    \ can be removed when it reaches a\n   certain age.\n   NFSv4 supports mandatory\
    \ locks and share reservations.  These are\n   mechanisms that clients can use\
    \ to restrict the set of I/O operations\n   that are permissible to other clients.\
    \  Since all I/O operations\n   ultimately arrive at the NFSv4 server for processing,\
    \ the server is\n   in a position to enforce these restrictions.  However, with\
    \ pNFS\n   layouts, I/Os will be issued from the clients that hold the layouts\n\
    \   directly to the storage devices that host the data.  These devices\n   have\
    \ no knowledge of files, mandatory locks, or share reservations,\n   and are not\
    \ in a position to enforce such restrictions.  For this\n   reason the NFSv4 server\
    \ MUST NOT grant layouts that conflict with\n   mandatory locks or share reservations.\
    \  Further, if a conflicting\n   mandatory lock request or a conflicting open\
    \ request arrives at the\n   server, the server MUST recall the part of the layout\
    \ in conflict\n   with the request before granting the request.\n"
- title: 2.3.6.  End-of-file Processing
  contents:
  - "2.3.6.  End-of-file Processing\n   The end-of-file location can be changed in\
    \ two ways: implicitly as\n   the result of a WRITE or LAYOUTCOMMIT beyond the\
    \ current end-of-file,\n   or explicitly as the result of a SETATTR request. \
    \ Typically, when a\n   file is truncated by an NFSv4 client via the SETATTR call,\
    \ the server\n   frees any disk blocks belonging to the file that are beyond the\
    \ new\n   end-of-file byte, and MUST write zeros to the portion of the new\n \
    \  end-of-file block beyond the new end-of-file byte.  These actions\n   render\
    \ any pNFS layouts that refer to the blocks that are freed or\n   written semantically\
    \ invalid.  Therefore, the server MUST recall from\n   clients the portions of\
    \ any pNFS layouts that refer to blocks that\n   will be freed or written by the\
    \ server before processing the truncate\n   request.  These recalls may take time\
    \ to complete; as explained in\n   [NFSv4.1], if the server cannot respond to\
    \ the client SETATTR request\n   in a reasonable amount of time, it SHOULD reply\
    \ to the client with\n   the error NFS4ERR_DELAY.\n   Blocks in the PNFS_BLOCK_INVALID_DATA\
    \ state that lie beyond the new\n   end-of-file block present a special case.\
    \  The server has reserved\n   these blocks for use by a pNFS client with a writable\
    \ layout for the\n   file, but the client has yet to commit the blocks, and they\
    \ are not\n   yet a part of the file mapping on disk.  The server MAY free these\n\
    \   blocks while processing the SETATTR request.  If so, the server MUST\n   recall\
    \ any layouts from pNFS clients that refer to the blocks before\n   processing\
    \ the truncate.  If the server does not free the\n   PNFS_BLOCK_INVALID_DATA blocks\
    \ while processing the SETATTR request,\n   it need not recall layouts that refer\
    \ only to the PNFS_BLOCK_INVALID\n   DATA blocks.\n   When a file is extended\
    \ implicitly by a WRITE or LAYOUTCOMMIT beyond\n   the current end-of-file, or\
    \ extended explicitly by a SETATTR request,\n   the server need not recall any\
    \ portions of any pNFS layouts.\n"
- title: 2.3.7.  Layout Hints
  contents:
  - "2.3.7.  Layout Hints\n   The SETATTR operation supports a layout hint attribute\
    \ [NFSv4.1].\n   When the client sets a layout hint (data type layouthint4) with\
    \ a\n   layout type of LAYOUT4_BLOCK_VOLUME (the loh_type field), the\n   loh_body\
    \ field contains a value of data type pnfs_block_layouthint4.\n   /// /* block\
    \ layout specific type for loh_body */\n   /// struct pnfs_block_layouthint4 {\n\
    \   ///     uint64_t blh_maximum_io_time;  /* maximum i/o time in seconds\n  \
    \ ///                                       */\n   /// };\n   ///\n   The block\
    \ layout client uses the layout hint data structure to\n   communicate to the\
    \ server the maximum time that it may take an I/O to\n   execute on the client.\
    \  Clients using block layouts MUST set the\n   layout hint attribute before using\
    \ LAYOUTGET operations.\n"
- title: 2.3.8.  Client Fencing
  contents:
  - "2.3.8.  Client Fencing\n   The pNFS block protocol must handle situations in\
    \ which a system\n   failure, typically a network connectivity issue, requires\
    \ the server\n   to unilaterally revoke extents from one client in order to transfer\n\
    \   the extents to another client.  The pNFS server implementation MUST\n   ensure\
    \ that when resources are transferred to another client, they\n   are not used\
    \ by the client originally owning them, and this must be\n   ensured against any\
    \ possible combination of partitions and delays\n   among all of the participants\
    \ to the protocol (server, storage and\n   client).  Two approaches to guaranteeing\
    \ this isolation are possible\n   and are discussed below.\n   One implementation\
    \ choice for fencing the block client from the block\n   storage is the use of\
    \ LUN masking or mapping at the storage systems\n   or storage area network to\
    \ disable access by the client to be\n   isolated.  This requires server access\
    \ to a management interface for\n   the storage system and authorization to perform\
    \ LUN masking and\n   management operations.  For example, the Storage Management\n\
    \   Initiative Specification (SMI-S) [SMIS] provides a means to discover\n   and\
    \ mask LUNs, including a means of associating clients with the\n   necessary World\
    \ Wide Names or Initiator names to be masked.\n   In the absence of support for\
    \ LUN masking, the server has to rely on\n   the clients to implement a timed-lease\
    \ I/O fencing mechanism.\n   Because clients do not know if the server is using\
    \ LUN masking, in\n   all cases, the client MUST implement timed-lease fencing.\
    \  In timed-\n   lease fencing, we define two time periods, the first, \"lease_time\"\
    \ is\n   the length of a lease as defined by the server's lease_time attribute\n\
    \   (see [NFSv4.1]), and the second, \"blh_maximum_io_time\" is the maximum\n\
    \   time it can take for a client I/O to the storage system to either\n   complete\
    \ or fail; this value is often 30 seconds or 60 seconds, but\n   may be longer\
    \ in some environments.  If the maximum client I/O time\n   cannot be bounded,\
    \ the client MUST use a value of all 1s as the\n   blh_maximum_io_time.\n   After\
    \ a new client ID is established, the client MUST use SETATTR\n   with a layout\
    \ hint of type LAYOUT4_BLOCK_VOLUME to inform the server\n   of its maximum I/O\
    \ time prior to issuing the first LAYOUTGET\n   operation.  While the maximum\
    \ I/O time hint is a per-file attribute,\n   it is actually a per-client characteristic.\
    \  Thus, the server MUST\n   maintain the last maximum I/O time hint sent separately\
    \ for each\n   client.  Each time the maximum I/O time changes, the server MUST\n\
    \   apply it to all files for which the client has a layout.  If the\n   client\
    \ does not specify this attribute on a file for which a block\n   layout is requested,\
    \ the server SHOULD use the most recent value\n   provided by the same client\
    \ for any file; if that client has not\n   provided a value for this attribute,\
    \ the server SHOULD reject the\n   layout request with the error NFS4ERR_LAYOUTUNAVAILABLE.\
    \  The client\n   SHOULD NOT send a SETATTR of the layout hint with every LAYOUTGET.\
    \  A\n   server that implements fencing via LUN masking SHOULD accept any\n  \
    \ maximum I/O time value from a client.  A server that does not\n   implement\
    \ fencing may return an error NFS4ERR_INVAL to the SETATTR\n   operation.  Such\
    \ a server SHOULD return NFS4ERR_INVAL when a client\n   sends an unbounded maximum\
    \ I/O time (all 1s), or when the maximum I/O\n   time is significantly greater\
    \ than that of other clients using block\n   layouts with pNFS.\n   When a client\
    \ receives the error NFS4ERR_INVAL in response to the\n   SETATTR operation for\
    \ a layout hint, the client MUST NOT use the\n   LAYOUTGET operation.  After responding\
    \ with NFS4ERR_INVAL to the\n   SETATTR for layout hint, the server MUST return\
    \ the error\n   NFS4ERR_LAYOUTUNAVAILABLE to all subsequent LAYOUTGET operations\
    \ from\n   that client.  Thus, the server, by returning either NFS4ERR_INVAL or\n\
    \   NFS4_OK determines whether or not a client with a large, or an\n   unbounded-maximum\
    \ I/O time may use pNFS.\n   Using the lease time and the maximum I/O time values,\
    \ we specify the\n   behavior of the client and server as follows.\n   When a\
    \ client receives layout information via a LAYOUTGET operation,\n   those layouts\
    \ are valid for at most \"lease_time\" seconds from when\n   the server granted\
    \ them.  A layout is renewed by any successful\n   SEQUENCE operation, or whenever\
    \ a new stateid is created or updated\n   (see the section \"Lease Renewal\" of\
    \ [NFSv4.1]).  If the layout lease\n   is not renewed prior to expiration, the\
    \ client MUST cease to use the\n   layout after \"lease_time\" seconds from when\
    \ it either sent the\n   original LAYOUTGET command or sent the last operation\
    \ renewing the\n   lease.  In other words, the client may not issue any I/O to\
    \ blocks\n   specified by an expired layout.  In the presence of large\n   communication\
    \ delays between the client and server, it is even\n   possible for the lease\
    \ to expire prior to the server response\n   arriving at the client.  In such\
    \ a situation, the client MUST NOT use\n   the expired layouts, and SHOULD revert\
    \ to using standard NFSv41 READ\n   and WRITE operations.  Furthermore, the client\
    \ must be configured\n   such that I/O operations complete within the \"blh_maximum_io_time\"\
    \n   even in the presence of multipath drivers that will retry I/Os via\n   multiple\
    \ paths.\n   As stated in the \"Dealing with Lease Expiration on the Client\"\n\
    \   section of [NFSv4.1], if any SEQUENCE operation is successful, but\n   sr_status_flag\
    \ has SEQ4_STATUS_EXPIRED_ALL_STATE_REVOKED,\n   SEQ4_STATUS_EXPIRED_SOME_STATE_REVOKED,\
    \ or\n   SEQ4_STATUS_ADMIN_STATE_REVOKED is set, the client MUST immediately\n\
    \   cease to use all layouts and device ID to device address mappings\n   associated\
    \ with the corresponding server.\n   In the absence of known two-way communication\
    \ between the client and\n   the server on the fore channel, the server must wait\
    \ for at least the\n   time period \"lease_time\" plus \"blh_maximum_io_time\"\
    \ before\n   transferring layouts from the original client to any other client.\n\
    \   The server, like the client, must take a conservative approach, and\n   start\
    \ the lease expiration timer from the time that it received the\n   operation\
    \ that last renewed the lease.\n"
- title: 2.4.  Crash Recovery Issues
  contents:
  - "2.4.  Crash Recovery Issues\n   A critical requirement in crash recovery is that\
    \ both the client and\n   the server know when the other has failed.  Additionally,\
    \ it is\n   required that a client sees a consistent view of data across server\n\
    \   restarts.  These requirements and a full discussion of crash recovery\n  \
    \ issues are covered in the \"Crash Recovery\" section of the NFSv41\n   specification\
    \ [NFSv4.1].  This document contains additional crash\n   recovery material specific\
    \ only to the block/volume layout.\n   When the server crashes while the client\
    \ holds a writable layout, and\n   the client has written data to blocks covered\
    \ by the layout, and the\n   blocks are still in the PNFS_BLOCK_INVALID_DATA state,\
    \ the client has\n   two options for recovery.  If the data that has been written\
    \ to these\n   blocks is still cached by the client, the client can simply re-write\n\
    \   the data via NFSv4, once the server has come back online.  However,\n   if\
    \ the data is no longer in the client's cache, the client MUST NOT\n   attempt\
    \ to source the data from the data servers.  Instead, it should\n   attempt to\
    \ commit the blocks in question to the server during the\n   server's recovery\
    \ grace period, by sending a LAYOUTCOMMIT with the\n   \"loca_reclaim\" flag set\
    \ to true.  This process is described in detail\n   in Section 18.42.4 of [NFSv4.1].\n"
- title: '2.5.  Recalling Resources: CB_RECALL_ANY'
  contents:
  - "2.5.  Recalling Resources: CB_RECALL_ANY\n   The server may decide that it cannot\
    \ hold all of the state for\n   layouts without running out of resources.  In\
    \ such a case, it is free\n   to recall individual layouts using CB_LAYOUTRECALL\
    \ to reduce the\n   load, or it may choose to request that the client return any\
    \ layout.\n   The NFSv4.1 spec [NFSv4.1] defines the following types:\n   const\
    \ RCA4_TYPE_MASK_BLK_LAYOUT = 4;\n   struct CB_RECALL_ANY4args {\n          uint32_t\
    \      craa_objects_to_keep;\n          bitmap4       craa_type_mask;\n   };\n\
    \   When the server sends a CB_RECALL_ANY request to a client specifying\n   the\
    \ RCA4_TYPE_MASK_BLK_LAYOUT bit in craa_type_mask, the client\n   should immediately\
    \ respond with NFS4_OK, and then asynchronously\n   return complete file layouts\
    \ until the number of files with layouts\n   cached on the client is less than\
    \ craa_object_to_keep.\n"
- title: 2.6.  Transient and Permanent Errors
  contents:
  - "2.6.  Transient and Permanent Errors\n   The server may respond to LAYOUTGET\
    \ with a variety of error statuses.\n   These errors can convey transient conditions\
    \ or more permanent\n   conditions that are unlikely to be resolved soon.\n  \
    \ The transient errors, NFS4ERR_RECALLCONFLICT and NFS4ERR_TRYLATER,\n   are used\
    \ to indicate that the server cannot immediately grant the\n   layout to the client.\
    \  In the former case, this is because the server\n   has recently issued a CB_LAYOUTRECALL\
    \ to the requesting client,\n   whereas in the case of NFS4ERR_TRYLATER, the server\
    \ cannot grant the\n   request possibly due to sharing conflicts with other clients.\
    \  In\n   either case, a reasonable approach for the client is to wait several\n\
    \   milliseconds and retry the request.  The client SHOULD track the\n   number\
    \ of retries, and if forward progress is not made, the client\n   SHOULD send\
    \ the READ or WRITE operation directly to the server.\n   The error NFS4ERR_LAYOUTUNAVAILABLE\
    \ may be returned by the server if\n   layouts are not supported for the requested\
    \ file or its containing\n   file system.  The server may also return this error\
    \ code if the\n   server is the progress of migrating the file from secondary\
    \ storage,\n   or for any other reason that causes the server to be unable to\
    \ supply\n   the layout.  As a result of receiving NFS4ERR_LAYOUTUNAVAILABLE,\
    \ the\n   client SHOULD send future READ and WRITE requests directly to the\n\
    \   server.  It is expected that a client will not cache the file's\n   layoutunavailable\
    \ state forever, particular if the file is closed,\n   and thus eventually, the\
    \ client MAY reissue a LAYOUTGET operation.\n"
- title: 3.  Security Considerations
  contents:
  - "3.  Security Considerations\n   Typically, SAN disk arrays and SAN protocols\
    \ provide access control\n   mechanisms (e.g., LUN mapping and/or masking) that\
    \ operate at the\n   granularity of individual hosts.  The functionality provided\
    \ by such\n   mechanisms makes it possible for the server to \"fence\" individual\n\
    \   client machines from certain physical disks -- that is to say, to\n   prevent\
    \ individual client machines from reading or writing to certain\n   physical disks.\
    \  Finer-grained access control methods are not\n   generally available.  For\
    \ this reason, certain security\n   responsibilities are delegated to pNFS clients\
    \ for block/volume\n   layouts.  Block/volume storage systems generally control\
    \ access at a\n   volume granularity, and hence pNFS clients have to be trusted\
    \ to only\n   perform accesses allowed by the layout extents they currently hold\n\
    \   (e.g., and not access storage for files on which a layout extent is\n   not\
    \ held).  In general, the server will not be able to prevent a\n   client that\
    \ holds a layout for a file from accessing parts of the\n   physical disk not\
    \ covered by the layout.  Similarly, the server will\n   not be able to prevent\
    \ a client from accessing blocks covered by a\n   layout that it has already returned.\
    \  This block-based level of\n   protection must be provided by the client software.\n\
    \   An alternative method of block/volume protocol use is for the storage\n  \
    \ devices to export virtualized block addresses, which do reflect the\n   files\
    \ to which blocks belong.  These virtual block addresses are\n   exported to pNFS\
    \ clients via layouts.  This allows the storage device\n   to make appropriate\
    \ access checks, while mapping virtual block\n   addresses to physical block addresses.\
    \  In environments where the\n   security requirements are such that client-side\
    \ protection from\n   access to storage outside of the authorized layout extents\
    \ is not\n   sufficient, pNFS block/volume storage layouts SHOULD NOT be used\n\
    \   unless the storage device is able to implement the appropriate access\n  \
    \ checks, via use of virtualized block addresses or other means.  In\n   contrast,\
    \ an environment where client-side protection may suffice\n   consists of co-located\
    \ clients, server and storage systems in a data\n   center with a physically isolated\
    \ SAN under control of a single\n   system administrator or small group of system\
    \ administrators.\n   This also has implications for some NFSv4 functionality\
    \ outside pNFS.\n   For instance, if a file is covered by a mandatory read-only\
    \ lock, the\n   server can ensure that only readable layouts for the file are\
    \ granted\n   to pNFS clients.  However, it is up to each pNFS client to ensure\n\
    \   that the readable layout is used only to service read requests, and\n   not\
    \ to allow writes to the existing parts of the file.  Similarly,\n   block/volume\
    \ storage devices are unable to validate NFS Access\n   Control Lists (ACLs) and\
    \ file open modes, so the client must enforce\n   the policies before sending\
    \ a READ or WRITE request to the storage\n   device.  Since block/volume storage\
    \ systems are generally not capable\n   of enforcing such file-based security,\
    \ in environments where pNFS\n   clients cannot be trusted to enforce such policies,\
    \ pNFS block/volume\n   storage layouts SHOULD NOT be used.\n   Access to block/volume\
    \ storage is logically at a lower layer of the\n   I/O stack than NFSv4, and hence\
    \ NFSv4 security is not directly\n   applicable to protocols that access such\
    \ storage directly.  Depending\n   on the protocol, some of the security mechanisms\
    \ provided by NFSv4\n   (e.g., encryption, cryptographic integrity) may not be\
    \ available or\n   may be provided via different means.  At one extreme, pNFS\
    \ with\n   block/volume storage can be used with storage access protocols (e.g.,\n\
    \   parallel SCSI) that provide essentially no security functionality.\n   At\
    \ the other extreme, pNFS may be used with storage protocols such as\n   iSCSI\
    \ that can provide significant security functionality.  It is the\n   responsibility\
    \ of those administering and deploying pNFS with a\n   block/volume storage access\
    \ protocol to ensure that appropriate\n   protection is provided to that protocol\
    \ (physical security is a\n   common means for protocols not based on IP).  In\
    \ environments where\n   the security requirements for the storage protocol cannot\
    \ be met,\n   pNFS block/volume storage layouts SHOULD NOT be used.\n   When security\
    \ is available for a storage protocol, it is generally at\n   a different granularity\
    \ and with a different notion of identity than\n   NFSv4 (e.g., NFSv4 controls\
    \ user access to files, iSCSI controls\n   initiator access to volumes).  The\
    \ responsibility for enforcing\n   appropriate correspondences between these security\
    \ layers is placed\n   upon the pNFS client.  As with the issues in the first\
    \ paragraph of\n   this section, in environments where the security requirements\
    \ are\n   such that client-side protection from access to storage outside of\n\
    \   the layout is not sufficient, pNFS block/volume storage layouts\n   SHOULD\
    \ NOT be used.\n"
- title: 4.  Conclusions
  contents:
  - "4.  Conclusions\n   This document specifies the block/volume layout type for\
    \ pNFS and\n   associated functionality.\n"
- title: 5.  IANA Considerations
  contents:
  - "5.  IANA Considerations\n   There are no IANA considerations in this document.\
    \  All pNFS IANA\n   Considerations are covered in [NFSv4.1].\n"
- title: 6.  Acknowledgments
  contents:
  - "6.  Acknowledgments\n   This document draws extensively on the authors' familiarity\
    \ with the\n   mapping functionality and protocol in EMC's Multi-Path File System\n\
    \   (MPFS) (previously named HighRoad) system [MPFS].  The protocol used\n   by\
    \ MPFS is called FMP (File Mapping Protocol); it is an add-on\n   protocol that\
    \ runs in parallel with file system protocols such as\n   NFSv3 to provide pNFS-like\
    \ functionality for block/volume storage.\n   While drawing on FMP, the data structures\
    \ and functional\n   considerations in this document differ in significant ways,\
    \ based on\n   lessons learned and the opportunity to take advantage of NFSv4\n\
    \   features such as COMPOUND operations.  The design to support pNFS\n   client\
    \ participation in copy-on-write is based on text and ideas\n   contributed by\
    \ Craig Everhart.\n   Andy Adamson, Ben Campbell, Richard Chandler, Benny Halevy,\
    \ Fredric\n   Isaman, and Mario Wurzl all helped to review versions of this\n\
    \   specification.\n"
- title: 7.  References
  contents:
  - '7.  References

    '
- title: 7.1.  Normative References
  contents:
  - "7.1.  Normative References\n   [LEGAL]   IETF Trust, \"Legal Provisions Relating\
    \ to IETF Documents\",\n             http://trustee.ietf.org/docs/IETF-Trust-License-Policy.pdf,\n\
    \             November 2008.\n   [RFC2119] Bradner, S., \"Key words for use in\
    \ RFCs to Indicate\n             Requirement Levels\", BCP 14, RFC 2119, March\
    \ 1997.\n   [NFSv4.1] Shepler, S., Ed., Eisler, M., Ed., and D. Noveck, Ed.,\n\
    \             \"Network File System (NFS) Version 4 Minor Version 1\n        \
    \     Protocol\", RFC 5661, January 2010.\n   [XDR]     Eisler, M., Ed., \"XDR:\
    \ External Data Representation\n             Standard\", STD 67, RFC 4506, May\
    \ 2006.\n"
- title: 7.2.  Informative References
  contents:
  - "7.2.  Informative References\n   [MPFS]    EMC Corporation, \"EMC Celerra Multi-Path\
    \ File System\n             (MPFS)\", EMC Data Sheet,\n             http://www.emc.com/collateral/software/data-sheet/\n\
    \             h2006-celerra-mpfs-mpfsi.pdf.\n   [SMIS]    SNIA, \"Storage Management\
    \ Initiative Specification (SMI-S)\n             v1.4\", http://www.snia.org/tech_activities/standards/\n\
    \             curr_standards/smi/SMI-S_Technical_Position_v1.4.0r4.zip.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   David L. Black\n   EMC Corporation\n   176 South Street\n\
    \   Hopkinton, MA 01748\n   Phone: +1 (508) 293-7953\n   EMail: black_david@emc.com\n\
    \   Stephen Fridella\n   Nasuni Inc\n   313 Speen St\n   Natick MA 01760\n   EMail:\
    \ stevef@nasuni.com\n   Jason Glasgow\n   Google\n   5 Cambridge Center\n   Cambridge,\
    \ MA  02142\n   Phone: +1 (617) 575 1599\n   EMail: jglasgow@aya.yale.edu\n"
