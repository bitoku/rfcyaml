- title: __initial_text__
  contents:
  - " Analysis of Generalized Multi-Protocol Label Switching (GMPLS)-based\n     \
    \ Recovery Mechanisms (including Protection and Restoration)\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2006).\n"
- title: Abstract
  contents:
  - "Abstract\n   This document provides an analysis grid to evaluate, compare, and\n\
    \   contrast the Generalized Multi-Protocol Label Switching (GMPLS)\n   protocol\
    \ suite capabilities with the recovery mechanisms currently\n   proposed at the\
    \ IETF CCAMP Working Group.  A detailed analysis of\n   each of the recovery phases\
    \ is provided using the terminology defined\n   in RFC 4427.  This document focuses\
    \ on transport plane survivability\n   and recovery issues and not on control\
    \ plane resilience and related\n   aspects.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n      5.2. Technology-Independent and Technology-Dependent\n\
    \      5.3. Specific Aspects of Control Plane-Based Recovery\n           Mechanisms\
    \ ................................................14\n           5.3.1. In-Band\
    \ vs. Out-Of-Band Signaling ..................14\n           5.3.2. Uni- vs. Bi-Directional\
    \ Failures ...................15\n           5.3.3. Partial vs. Full Span Recovery\
    \ .....................17\n           5.3.4. Difference between LSP, LSP Segment\
    \ and\n                  Span Recovery ......................................18\n\
    \      5.4. Difference between Recovery Type and Scheme ...............19\n  \
    \    5.5. LSP Recovery Mechanisms ...................................21\n    \
    \       5.5.1. Classification .....................................21\n      \
    \     5.5.2. LSP Restoration ....................................23\n        \
    \   5.5.3. Pre-Planned LSP Restoration ........................24\n          \
    \ 5.5.4. LSP Segment Restoration ............................25\n   6. Reversion\
    \ ......................................................26\n      6.1. Wait-To-Restore\
    \ (WTR) .....................................26\n      6.2. Revertive Mode Operation\
    \ ..................................26\n      6.3. Orphans ...................................................27\n\
    \   7. Hierarchies ....................................................27\n  \
    \    7.1. Horizontal Hierarchy (Partitioning) .......................28\n    \
    \  7.2. Vertical Hierarchy (Layers) ...............................28\n      \
    \     7.2.1. Recovery Granularity ...............................30\n      7.3.\
    \ Escalation Strategies .....................................30\n      7.4. Disjointness\
    \ ..............................................31\n           7.4.1. SRLG Disjointness\
    \ ..................................32\n   8. Recovery Mechanisms Analysis ...................................33\n\
    \      8.1. Fast Convergence (Detection/Correlation and\n           Hold-off Time)\
    \ ............................................34\n      8.2. Efficiency (Recovery\
    \ Switching Time) ......................34\n      8.3. Robustness ................................................35\n\
    \      8.4. Resource Optimization .....................................36\n  \
    \         8.4.1. Recovery Resource Sharing ..........................37\n    \
    \       8.4.2. Recovery Resource Sharing and SRLG Recovery ........39\n      \
    \     8.4.3. Recovery Resource Sharing, SRLG\n                  Disjointness and\
    \ Admission Control .................40\n   9. Summary and Conclusions ........................................42\n\
    \   10. Security Considerations .......................................43\n  \
    \ 11. Acknowledgements ..............................................43\n   12.\
    \ References ....................................................44\n      12.1.\
    \ Normative References .....................................44\n      12.2. Informative\
    \ References ...................................44\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document provides an analysis grid to evaluate, compare,\
    \ and\n   contrast the Generalized MPLS (GMPLS) protocol suite capabilities\n\
    \   with the recovery mechanisms proposed at the IETF CCAMP Working\n   Group.\
    \  The focus is on transport plane survivability and recovery\n   issues and not\
    \ on control-plane-resilience-related aspects.  Although\n   the recovery mechanisms\
    \ described in this document impose different\n   requirements on GMPLS-based\
    \ recovery protocols, the protocols'\n   specifications will not be covered in\
    \ this document.  Though the\n   concepts discussed are technology independent,\
    \ this document\n   implicitly focuses on SONET [T1.105]/SDH [G.707], Optical\
    \ Transport\n   Networks (OTN) [G.709], and pre-OTN technologies, except when\n\
    \   specific details need to be considered (for instance, in the case of\n   failure\
    \ detection).\n   A detailed analysis is provided for each of the recovery phases\
    \ as\n   identified in [RFC4427].  These phases define the sequence of generic\n\
    \   operations that need to be performed when a LSP/Span failure (or any\n   other\
    \ event generating such failures) occurs:\n      - Phase 1: Failure Detection\n\
    \      - Phase 2: Failure Localization (and Isolation)\n      - Phase 3: Failure\
    \ Notification\n      - Phase 4: Recovery (Protection or Restoration)\n      -\
    \ Phase 5: Reversion (Normalization)\n   Together, failure detection, localization,\
    \ and notification phases\n   are referred to as \"fault management\".  Within\
    \ a recovery domain, the\n   entities involved during the recovery operations\
    \ are defined in\n   [RFC4427]; these entities include ingress, egress, and intermediate\n\
    \   nodes.  The term \"recovery mechanism\" is used to cover both\n   protection\
    \ and restoration mechanisms.  Specific terms such as\n   \"protection\" and \"\
    restoration\" are used only when differentiation is\n   required.  Likewise the\
    \ term \"failure\" is used to represent both\n   signal failure and signal degradation.\n\
    \   In addition, when analyzing the different hierarchical recovery\n   mechanisms\
    \ including disjointness-related issues, a clear distinction\n   is made between\
    \ partitioning (horizontal hierarchy) and layering\n   (vertical hierarchy). \
    \ In order to assess the current GMPLS protocol\n   capabilities and the potential\
    \ need for further extensions, the\n   dimensions for analyzing each of the recovery\
    \ mechanisms detailed in\n   this document are introduced.  This document concludes\
    \ by detailing\n   the applicability of the current GMPLS protocol building blocks\
    \ for\n   recovery purposes.\n"
- title: 2.  Contributors
  contents:
  - "2.  Contributors\n   This document is the result of the CCAMP Working Group Protection\
    \ and\n   Restoration design team joint effort.  Besides the editors, the\n  \
    \ following are the authors that contributed to the present memo:\n   Deborah\
    \ Brungard (AT&T)\n   200 S. Laurel Ave.\n   Middletown, NJ 07748, USA\n   EMail:\
    \ dbrungard@att.com\n   Sudheer Dharanikota\n   EMail: sudheer@ieee.org\n   Jonathan\
    \ P. Lang (Sonos)\n   506 Chapala Street\n   Santa Barbara, CA 93101, USA\n  \
    \ EMail: jplang@ieee.org\n   Guangzhi Li (AT&T)\n   180 Park Avenue,\n   Florham\
    \ Park, NJ 07932, USA\n   EMail: gli@research.att.com\n   Eric Mannie\n   Perceval\n\
    \   Rue Tenbosch, 9\n   1000 Brussels\n   Belgium\n   Phone: +32-2-6409194\n \
    \  EMail: eric.mannie@perceval.net\n   Dimitri Papadimitriou (Alcatel)\n   Francis\
    \ Wellesplein, 1\n   B-2018 Antwerpen, Belgium\n   EMail: dimitri.papadimitriou@alcatel.be\n\
    \   Bala Rajagopalan\n   Microsoft India Development Center\n   Hyderabad, India\n\
    \   EMail: balar@microsoft.com\n   Yakov Rekhter (Juniper)\n   1194 N. Mathilda\
    \ Avenue\n   Sunnyvale, CA 94089, USA\n   EMail: yakov@juniper.net\n"
- title: 3.  Conventions Used in this Document
  contents:
  - "3.  Conventions Used in this Document\n   The key words \"MUST\", \"MUST NOT\"\
    , \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\"\
    ,  \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in [RFC2119].\n   Any other recovery-related terminology used in this document\
    \ conforms\n   to that defined in [RFC4427].  The reader is also assumed to be\n\
    \   familiar with the terminology developed in [RFC3945], [RFC3471],\n   [RFC3473],\
    \ [RFC4202], and [RFC4204].\n"
- title: 4.  Fault Management
  contents:
  - '4.  Fault Management

    '
- title: 4.1.  Failure Detection
  contents:
  - "4.1.  Failure Detection\n   Transport failure detection is the only phase that\
    \ cannot be achieved\n   by the control plane alone because the latter needs a\
    \ hook to the\n   transport plane in order to collect the related information.\
    \  It has\n   to be emphasized that even if failure events themselves are detected\n\
    \   by the transport plane, the latter, upon a failure condition, must\n   trigger\
    \ the control plane for subsequent actions through the use of\n   GMPLS signaling\
    \ capabilities (see [RFC3471] and [RFC3473]) or Link\n   Management Protocol capabilities\
    \ (see [RFC4204], Section 6).\n   Therefore, by definition, transport failure\
    \ detection is transport\n   technology dependent (and so exceptionally, we keep\
    \ here the\n   \"transport plane\" terminology).  In transport fault management,\n\
    \   distinction is made between a defect and a failure.  Here, the\n   discussion\
    \ addresses failure detection (persistent fault cause).  In\n   the technology-dependent\
    \ descriptions, a more precise specification\n   will be provided.\n   As an example,\
    \ SONET/SDH (see [G.707], [G.783], and [G.806]) provides\n   supervision capabilities\
    \ covering:\n   - Continuity: SONET/SDH monitors the integrity of the continuity\
    \ of a\n     trail (i.e., section or path).  This operation is performed by\n\
    \     monitoring the presence/absence of the signal.  Examples are Loss\n    \
    \ of Signal (LOS) detection for the physical layer, Unequipped (UNEQ)\n     Signal\
    \ detection for the path layer, Server Signal Fail Detection\n     (e.g., AIS)\
    \ at the client layer.\n   - Connectivity: SONET/SDH monitors the integrity of\
    \ the routing of\n     the signal between end-points.  Connectivity monitoring\
    \ is needed\n     if the layer provides flexible connectivity, either automatically\n\
    \     (e.g., cross-connects) or manually (e.g., fiber distribution\n     frame).\
    \  An example is the Trail (i.e., section or path) Trace\n     Identifier used\
    \ at the different layers and the corresponding Trail\n     Trace Identifier Mismatch\
    \ detection.\n   - Alignment: SONET/SDH checks that the client and server layer\
    \ frame\n     start can be correctly recovered from the detection of loss of\n\
    \     alignment.  The specific processes depend on the signal/frame\n     structure\
    \ and may include: (multi-)frame alignment, pointer\n     processing, and alignment\
    \ of several independent frames to a common\n     frame start in case of inverse\
    \ multiplexing.  Loss of alignment is\n     a generic term.  Examples are loss\
    \ of frame, loss of multi-frame,\n     or loss of pointer.\n   - Payload type:\
    \ SONET/SDH checks that compatible adaptation functions\n     are used at the\
    \ source and the destination.  Normally, this is done\n     by adding a payload\
    \ type identifier (referred to as the \"signal\n     label\") at the source adaptation\
    \ function and comparing it with the\n     expected identifier at the destination.\
    \  For instance, the payload\n     type identifier is compared with the corresponding\
    \ mismatch\n     detection.\n   - Signal Quality: SONET/SDH monitors the performance\
    \ of a signal.\n     For instance, if the performance falls below a certain threshold,\
    \ a\n     defect -- excessive errors (EXC) or degraded signal (DEG) -- is\n  \
    \   detected.\n   The most important point is that the supervision processes and\
    \ the\n   corresponding failure detection (used to initiate the recovery\n   phase(s))\
    \ result in either:\n   - Signal Degrade (SD): A signal indicating that the associated\
    \ data\n     has degraded in the sense that a degraded defect condition is\n \
    \    active (for instance, a dDEG declared when the Bit Error Rate\n     exceeds\
    \ a preset threshold).  Or\n   - Signal Fail (SF): A signal indicating that the\
    \ associated data has\n     failed in the sense that a signal interrupting near-end\
    \ defect\n     condition is active (as opposed to the degraded defect).\n   In\
    \ Optical Transport Networks (OTN), equivalent supervision\n   capabilities are\
    \ provided at the optical/digital section layers\n   (i.e., Optical Transmission\
    \ Section (OTS), Optical Multiplex Section\n   (OMS) and Optical channel Transport\
    \ Unit (OTU)) and at the\n   optical/digital path layers (i.e., Optical Channel\
    \ (OCh) and Optical\n   channel Data Unit (ODU)).  Interested readers are referred\
    \ to the\n   ITU-T Recommendations [G.798] and [G.709] for more details.\n   The\
    \ above are examples that illustrate cases where the failure\n   detection and\
    \ reporting entities (see [RFC4427]) are co-located.  The\n   following example\
    \ illustrates the scenario where the failure\n   detecting and reporting entities\
    \ (see [RFC4427]) are not co-located.\n   In pre-OTN networks, a failure may be\
    \ masked by intermediate O-E-O\n   based Optical Line System (OLS), preventing\
    \ a Photonic Cross-Connect\n   (PXC) from detecting upstream failures.  In such\
    \ cases, failure\n   detection may be assisted by an out-of-band communication\
    \ channel,\n   and failure condition may be reported to the PXC control plane.\
    \  This\n   can be provided by using [RFC4209] extensions that deliver IP\n  \
    \ message-based communication between the PXC and the OLS control\n   plane. \
    \ Also, since PXCs are independent of the framing format,\n   failure conditions\
    \ can only be triggered either by detecting the\n   absence of the optical signal\
    \ or by measuring its quality.  These\n   mechanisms are generally less reliable\
    \ than electrical (digital)\n   ones.  Both types of detection mechanisms are\
    \ outside the scope of\n   this document.  If the intermediate OLS supports electrical\
    \ (digital)\n   mechanisms, using the LMP communication channel, these failure\n\
    \   conditions are reported to\n   the PXC and subsequent recovery actions are\
    \ performed as described in\n   Section 5.  As such, from the control plane viewpoint,\
    \ this mechanism\n   turns the OLS-PXC-composed system into a single logical entity,\
    \ thus\n   having the same failure management mechanisms as any other O-E-O\n\
    \   capable device.\n   More generally, the following are typical failure conditions\
    \ in\n   SONET/SDH and pre-OTN networks:\n   - Loss of Light (LOL)/Loss of Signal\
    \ (LOS): Signal Failure (SF)\n     condition where the optical signal is not detected\
    \ any longer on\n     the receiver of a given interface.\n   - Signal Degrade\
    \ (SD): detection of the signal degradation over\n     a specific period of time.\n\
    \   - For SONET/SDH payloads, all of the above-mentioned supervision\n     capabilities\
    \ can be used, resulting in SD or SF conditions.\n   In summary, the following\
    \ cases apply when considering the\n   communication between the detecting and\
    \ reporting entities:\n   - Co-located detecting and reporting entities: both\
    \ the detecting and\n     reporting entities are on the same node (e.g., SONET/SDH\
    \ equipment,\n     Opaque cross-connects, and, with some limitations, Transparent\n\
    \     cross-connects, etc.)\n   - Non-co-located detecting and reporting entities:\n\
    \     o with in-band communication between entities: entities are\n       physically\
    \ separated, but the transport plane provides in-band\n       communication between\
    \ them (e.g., Server Signal Failures such as\n       Alarm Indication Signal (AIS),\
    \ etc.)\n     o with out-of-band communication between entities: entities are\n\
    \       physically separated, but an out-of-band communication channel is\n  \
    \     provided between them (e.g., using [RFCF4204]).\n"
- title: 4.2.  Failure Localization and Isolation
  contents:
  - "4.2.  Failure Localization and Isolation\n   Failure localization provides information\
    \ to the deciding entity\n   about the location (and so the identity) of the transport\
    \ plane\n   entity that detects the LSP(s)/span(s) failure.  The deciding entity\n\
    \   can then make an accurate decision to achieve finer grained recovery\n   switching\
    \ action(s).  Note that this information can also be included\n   as part of the\
    \ failure notification (see Section 4.3).\n   In some cases, this accurate failure\
    \ localization information may be\n   less urgent to determine if it requires\
    \ performing more time-\n   consuming failure isolation (see also Section 4.4).\
    \  This is\n   particularly the case when edge-to-edge LSP recovery is performed\n\
    \   based on a simple failure notification (including the identification\n   of\
    \ the working LSPs under failure condition).  Note that \"edge\"\n   refers to\
    \ a sub-network end-node, for instance.  In this case, a more\n   accurate localization\
    \ and isolation can be performed after recovery\n   of these LSPs.\n   Failure\
    \ localization should be triggered immediately after the fault\n   detection phase.\
    \  This operation can be performed at the transport\n   plane and/or (if the operation\
    \ is unavailable via the transport\n   plane) the control plane level where dedicated\
    \ signaling messages can\n   be used.  When performed at the control plane level,\
    \ a protocol such\n   as LMP (see [RFC4204], Section 6) can be used for failure\n\
    \   localization purposes.\n"
- title: 4.3.  Failure Notification
  contents:
  - "4.3.  Failure Notification\n   Failure notification is used 1) to inform intermediate\
    \ nodes that an\n   LSP/span failure has occurred and has been detected and 2)\
    \ to inform\n   the deciding entities (which can correspond to any intermediate\
    \ or\n   end-point of the failed LSP/span) that the corresponding service is\n\
    \   not available.  In general, these deciding entities will be the ones\n   making\
    \ the appropriate recovery decision.  When co-located with the\n   recovering\
    \ entity, these entities will also perform the corresponding\n   recovery action(s).\n\
    \   Failure notification can be provided either by the transport or by\n   the\
    \ control plane.  As an example, let us first briefly describe the\n   failure\
    \ notification mechanism defined at the SONET/SDH transport\n   plane level (also\
    \ referred to as maintenance signal supervision):\n   - AIS (Alarm Indication\
    \ Signal) occurs as a result of a failure\n     condition such as Loss of Signal\
    \ and is used to notify downstream\n     nodes (of the appropriate layer processing)\
    \ that a failure has\n     occurred.  AIS performs two functions: 1) inform the\
    \ intermediate\n     nodes (with the appropriate layer monitoring capability)\
    \ that a\n     failure has been detected and 2) notify the connection end-point\n\
    \     that the service is no longer available.\n   For a distributed control plane\
    \ supporting one (or more) failure\n   notification mechanism(s), regardless of\
    \ the mechanism's actual\n   implementation, the same capabilities are needed\
    \ with more (or less)\n   information provided about the LSPs/spans under failure\
    \ condition,\n   their detailed statuses, etc.\n   The most important difference\
    \ between these mechanisms is related to\n   the fact that transport plane notifications\
    \ (as defined today) would\n   directly initiate either a certain type of protection\
    \ switching (such\n   as those described in [RFC4427]) via the transport plane\
    \ or\n   restoration actions via the management plane.\n   On the other hand,\
    \ using a failure notification mechanism through the\n   control plane would provide\
    \ the possibility of triggering either a\n   protection or a restoration action\
    \ via the control plane.  This has\n   the advantage that a control-plane-recovery-responsible\
    \ entity does\n   not necessarily have to be co-located with a transport\n   maintenance/recovery\
    \ domain.  A control plane recovery domain can be\n   defined at entities not\
    \ supporting a transport plane recovery.\n   Moreover, as specified in [RFC3473],\
    \ notification message exchanges\n   through a GMPLS control plane may not follow\
    \ the same path as the\n   LSP/spans for which these messages carry the status.\
    \  In turn, this\n   ensures a fast, reliable (through acknowledgement and the\
    \ use of\n   either a dedicated control plane network or disjoint control\n  \
    \ channels), and efficient (through the aggregation of several LSP/span\n   statuses\
    \ within the same message) failure notification mechanism.\n   The other important\
    \ properties to be met by the failure notification\n   mechanism are mainly the\
    \ following:\n   - Notification messages must provide enough information such\
    \ that the\n     most efficient subsequent recovery action will be taken at the\n\
    \     recovering entities (in most of the recovery types and schemes this\n  \
    \   action is even deterministic).  Remember here that these entities\n     can\
    \ be either intermediate or end-points through which normal\n     traffic flows.\
    \  Based on local policy, intermediate nodes may not\n     use this information\
    \ for subsequent recovery actions (see for\n     instance the APS protocol phases\
    \ as described in [RFC4427]).  In\n     addition, since fast notification is a\
    \ mechanism running in\n     collaboration with the existing GMPLS signaling (see\
    \ [RFC3473])\n     that also allows intermediate nodes to stay informed about\
    \ the\n     status of the working LSP/spans under failure condition.\n     The\
    \ trade-off here arises when defining what information the\n     LSP/span end-points\
    \ (more precisely, the deciding entities) need in\n     order for the recovering\
    \ entity to take the best recovery action:\n     If not enough information is\
    \ provided, the decision cannot be\n     optimal (note that in this eventuality,\
    \ the important issue is to\n     quantify the level of sub-optimality).  If too\
    \ much information is\n     provided, the control plane may be overloaded with\
    \ unnecessary\n     information and the aggregation/correlation of this notification\n\
    \     information will be more complex and time-consuming to achieve.\n     Note\
    \ that a more detailed quantification of the amount of\n     information to be\
    \ exchanged and processed is strongly dependent on\n     the failure notification\
    \ protocol.\n   - If the failure localization and isolation are not performed\
    \ by one\n     of the LSP/span end-points or some intermediate points, the points\n\
    \     should receive enough information from the notification message in\n   \
    \  order to locate the failure.  Otherwise, they would need to (re-)\n     initiate\
    \ a failure localization and isolation action.\n   - Avoiding so-called notification\
    \ storms implies that 1) the failure\n     detection output is correlated (i.e.,\
    \ alarm correlation) and\n     aggregated at the node detecting the failure(s),\
    \ 2) the failure\n     notifications are directed to a restricted set of destinations\
    \ (in\n     general the end-points), and 3) failure notification suppression\n\
    \     (i.e., alarm suppression) is provided in order to limit flooding in\n  \
    \   case of multiple and/or correlated failures detected at several\n     locations\
    \ in the network.\n   - Alarm correlation and aggregation (at the failure-detecting\
    \ node)\n     implies a consistent decision based on the conditions for which\
    \ a\n     trade-off between fast convergence (at detecting node) and fast\n  \
    \   notification (implying that correlation and aggregation occurs at\n     receiving\
    \ end-points) can be found.\n"
- title: 4.4.  Failure Correlation
  contents:
  - "4.4.  Failure Correlation\n   A single failure event (such as a span failure)\
    \ can cause multiple\n   failure (such as individual LSP failures) conditions\
    \ to be reported.\n   These can be grouped (i.e., correlated) to reduce the number\
    \ of\n   failure conditions communicated on the reporting channel, for both\n\
    \   in-band and out-of-band failure reporting.\n   In such a scenario, it can\
    \ be important to wait for a certain period\n   of time, typically called failure\
    \ correlation time, and gather all\n   the failures to report them as a group\
    \ of failures (or simply group\n   failure).  For instance, this approach can\
    \ be provided using LMP-WDM\n   for pre-OTN networks (see [RFC4209]) or when using\
    \ Signal\n   Failure/Degrade Group in the SONET/SDH context.\n   Note that a default\
    \ average time interval during which failure\n   correlation operation can be\
    \ performed is difficult to provide since\n   it is strongly dependent on the\
    \ underlying network topology.\n   Therefore, providing a per-node configurable\
    \ failure correlation time\n   can be advisable.  The detailed selection criteria\
    \ for this time\n   interval are outside of the scope of this document.\n   When\
    \ failure correlation is not provided, multiple failure\n   notification messages\
    \ may be sent out in response to a single failure\n   (for instance, a fiber cut).\
    \  Each failure notification message\n   contains a set of information on the\
    \ failed working resources (for\n   instance, the individual lambda LSP flowing\
    \ through this fiber).\n   This allows for a more prompt response, but can potentially\
    \ overload\n   the control plane due to a large amount of failure notifications.\n"
- title: 5.  Recovery Mechanisms
  contents:
  - '5.  Recovery Mechanisms

    '
- title: 5.1.  Transport vs. Control Plane Responsibilities
  contents:
  - "5.1.  Transport vs. Control Plane Responsibilities\n   When applicable, recovery\
    \ resources are provisioned, for both\n   protection and restoration, using GMPLS\
    \ signaling capabilities.\n   Thus, these are control plane-driven actions (topological\
    \ and\n   resource-constrained) that are always performed in this context.\n \
    \  The following tables give an overview of the responsibilities taken\n   by\
    \ the control plane in case of LSP/span recovery:\n   1. LSP/span Protection\n\
    \   - Phase 1: Failure Detection                  Transport plane\n   - Phase\
    \ 2: Failure Localization/Isolation     Transport/Control plane\n   - Phase 3:\
    \ Failure Notification               Transport/Control plane\n   - Phase 4: Protection\
    \ Switching               Transport/Control plane\n   - Phase 5: Reversion (Normalization)\
    \          Transport/Control plane\n   Note: in the context of LSP/span protection,\
    \ control plane actions\n   can be performed either for operational purposes and/or\n\
    \   synchronization purposes (vertical synchronization between transport\n   and\
    \ control plane) and/or notification purposes (horizontal\n   synchronization\
    \ between end-nodes at control plane level).  This\n   suggests the selection\
    \ of the responsible plane (in particular for\n   protection switching) during\
    \ the provisioning phase of the\n   protected/protection LSP.\n   2. LSP/span\
    \ Restoration\n   - Phase 1: Failure Detection                  Transport plane\n\
    \   - Phase 2: Failure Localization/Isolation     Transport/Control plane\n  \
    \ - Phase 3: Failure Notification               Control plane\n   - Phase 4: Recovery\
    \ Switching                 Control plane\n   - Phase 5: Reversion (Normalization)\
    \          Control plane\n   Therefore, this document primarily focuses on provisioning\
    \ of LSP\n   recovery resources, failure notification mechanisms, recovery\n \
    \  switching, and reversion operations.  Moreover, some additional\n   considerations\
    \ can be dedicated to the mechanisms associated to the\n   failure localization/isolation\
    \ phase.\n"
- title: 5.2.  Technology-Independent and Technology-Dependent Mechanisms
  contents:
  - "5.2.  Technology-Independent and Technology-Dependent Mechanisms\n   The present\
    \ recovery mechanisms analysis applies to any circuit-\n   oriented data plane\
    \ technology with discrete bandwidth increments\n   (like SONET/SDH, G.709 OTN,\
    \ etc.) being controlled by a GMPLS-based\n   distributed control plane.\n   The\
    \ following sub-sections are not intended to favor one technology\n   versus another.\
    \  They list pro and cons for each technology in order\n   to determine the mechanisms\
    \ that GMPLS-based recovery must deliver to\n   overcome their cons and make use\
    \ of their pros in their respective\n   applicability context.\n"
- title: 5.2.1.  OTN Recovery
  contents:
  - "5.2.1.  OTN Recovery\n   OTN recovery specifics are left for further consideration.\n"
- title: 5.2.2.  Pre-OTN Recovery
  contents:
  - "5.2.2.  Pre-OTN Recovery\n   Pre-OTN recovery specifics (also referred to as\
    \ \"lambda switching\")\n   present mainly the following advantages:\n   - They\
    \ benefit from a simpler architecture, making it more suitable\n     for mesh-based\
    \ recovery types and schemes (on a per-channel basis).\n   - Failure suppression\
    \ at intermediate node transponders, e.g., use of\n     squelching, implies that\
    \ failures (such as LoL) will propagate to\n     edge nodes.  Thus, edge nodes\
    \ will have the possibility to initiate\n     recovery actions driven by upper\
    \ layers (vs. use of non-standard\n     masking of upstream failures).\n   The\
    \ main disadvantage is the lack of interworking due to the large\n   amount of\
    \ failure management (in particular failure notification\n   protocols) and recovery\
    \ mechanisms currently available.\n   Note also, that for all-optical networks,\
    \ combination of recovery\n   with optical physical impairments is left for a\
    \ future release of\n   this document because corresponding detection technologies\
    \ are under\n   specification.\n"
- title: 5.2.3.  SONET/SDH Recovery
  contents:
  - "5.2.3.  SONET/SDH Recovery\n   Some of the advantages of SONET [T1.105]/SDH [G.707],\
    \ and more\n   generically any Time Division Multiplexing (TDM) transport plane\n\
    \   recovery, are that they provide:\n   - Protection types operating at the data\
    \ plane level that are\n     standardized (see [G.841]) and can operate across\
    \ protected domains\n     and interwork (see [G.842]).\n   - Failure detection,\
    \ notification, and path/section Automatic\n     Protection Switching (APS) mechanisms.\n\
    \   - Greater control over the granularity of the TDM LSPs/links that can\n  \
    \   be recovered with respect to coarser optical channel (or whole\n     fiber\
    \ content) recovery switching\n   Some of the limitations of the SONET/SDH recovery\
    \ are:\n   - Limited topological scope: Inherently the use of ring topologies,\n\
    \     typically, dedicated Sub-Network Connection Protection (SNCP) or\n     shared\
    \ protection rings, has reduced flexibility and resource\n     efficiency with\
    \ respect to the (somewhat more complex) meshed\n     recovery.\n   - Inefficient\
    \ use of spare capacity: SONET/SDH protection is largely\n     applied to ring\
    \ topologies, where spare capacity often remains\n     idle, making the efficiency\
    \ of bandwidth usage a real issue.\n   - Support of meshed recovery requires intensive\
    \ network management\n     development, and the functionality is limited by both\
    \ the network\n     elements and the capabilities of the element management systems\n\
    \     (thus justifying the development of GMPLS-based distributed\n     recovery\
    \ mechanisms.)\n"
- title: 5.3.  Specific Aspects of Control Plane-Based Recovery Mechanisms
  contents:
  - '5.3.  Specific Aspects of Control Plane-Based Recovery Mechanisms

    '
- title: 5.3.1.  In-Band vs. Out-Of-Band Signaling
  contents:
  - "5.3.1.  In-Band vs. Out-Of-Band Signaling\n   The nodes communicate through the\
    \ use of IP terminating control\n   channels defining the control plane (transport)\
    \ topology.  In this\n   context, two classes of transport mechanisms can be considered\
    \ here:\n   in-fiber or out-of-fiber (through a dedicated physically diverse\n\
    \   control network referred to as the Data Communication Network or\n   DCN).\
    \  The potential impact of the usage of an in-fiber (signaling)\n   transport\
    \ mechanism is briefly considered here.\n   In-fiber transport mechanisms can\
    \ be further subdivided into in-band\n   and out-of-band.  As such, the distinction\
    \ between in-fiber in-band\n   and in-fiber out-of-band signaling reduces to the\
    \ consideration of a\n   logically- versus physically-embedded control plane topology\
    \ with\n   respect to the transport plane topology.  In the scope of this\n  \
    \ document, it is assumed that at least one IP control channel between\n   each\
    \ pair of adjacent nodes is continuously available to enable the\n   exchange\
    \ of recovery-related information and messages.  Thus, in\n   either case (i.e.,\
    \ in-band or out-of-band) at least one logical or\n   physical control channel\
    \ between each pair of nodes is always\n   expected to be available.\n   Therefore,\
    \ the key issue when using in-fiber signaling is whether one\n   can assume independence\
    \ between the fault-tolerance capabilities of\n   control plane and the failures\
    \ affecting the transport plane\n   (including the nodes).  Note also that existing\
    \ specifications like\n   the OTN provide a limited form of independence for in-fiber\
    \ signaling\n   by dedicating a separate optical supervisory channel (OSC, see\n\
    \   [G.709] and [G.874]) to transport the overhead and other control\n   traffic.\
    \  For OTNs, failure of the OSC does not result in failing the\n   optical channels.\
    \  Similarly, loss of the control channel must not\n   result in failing the data\
    \ channels (transport plane).\n"
- title: 5.3.2.  Uni- vs. Bi-Directional Failures
  contents:
  - "5.3.2.  Uni- vs. Bi-Directional Failures\n   The failure detection, correlation,\
    \ and notification mechanisms\n   (described in Section 4) can be triggered when\
    \ either a uni-\n   directional or a bi-directional LSP/Span failure occurs (or\
    \ a\n   combination of both).  As illustrated in Figures 1 and 2, two\n   alternatives\
    \ can be considered here:\n   1. Uni-directional failure detection: the failure\
    \ is detected on the\n      receiver side, i.e., it is detected by only the downstream\
    \ node to\n      the failure (or by the upstream node depending on the failure\n\
    \      propagation direction, respectively).\n   2. Bi-directional failure detection:\
    \ the failure is detected on the\n      receiver side of both downstream node\
    \ AND upstream node to the\n      failure.\n   Notice that after the failure detection\
    \ time, if only control-plane-\n   based failure management is provided, the peering\
    \ node is unaware of\n   the failure detection status of its neighbor.\n    -------\
    \             -------           -------             -------\n   |       |    \
    \       |       |Tx     Rx|       |           |       |\n   | NodeA |----...----|\
    \ NodeB |xxxxxxxxx| NodeC |----...----| NodeD |\n   |       |----...----|    \
    \   |---------|       |----...----|       |\n    -------             ------- \
    \          -------             -------\n   t0                                >>>>>>>\
    \ F\n   t1                      x <---------------x\n                        \
    \       Notification\n   t2  <--------...--------x                 x--------...-------->\n\
    \          Up Notification                      Down Notification\n          \
    \    Figure 1: Uni-directional failure detection\n    -------             -------\
    \           -------             -------\n   |       |           |       |Tx  \
    \   Rx|       |           |       |\n   | NodeA |----...----| NodeB |xxxxxxxxx|\
    \ NodeC |----...----| NodeD |\n   |       |----...----|       |xxxxxxxxx|    \
    \   |----...----|       |\n    -------             -------           ------- \
    \            -------\n   t0                      F <<<<<<< >>>>>>> F\n   t1  \
    \                    x <-------------> x\n                               Notification\n\
    \   t2  <--------...--------x                 x--------...-------->\n        \
    \  Up Notification                      Down Notification\n               Figure\
    \ 2: Bi-directional failure detection\n   After failure detection, the following\
    \ failure management operations\n   can be subsequently considered:\n   - Each\
    \ detecting entity sends a notification message to the\n     corresponding transmitting\
    \ entity.  For instance, in Figure 1, node\n     C sends a notification message\
    \ to node B.  In Figure 2, node C\n     sends a notification message to node B\
    \ while node B sends a\n     notification message to node C.  To ensure reliable\
    \ failure\n     notification, a dedicated acknowledgement message can be returned\n\
    \     back to the sender node.\n   - Next, within a certain (and pre-determined)\
    \ time window, nodes\n     impacted by the failure occurrences may perform their\
    \ correlation.\n     In case of uni-directional failure, node B only receives\
    \ the\n     notification message from node C, and thus the time for this\n   \
    \  operation is negligible.  In case of bi-directional failure, node B\n     has\
    \ to correlate the received notification message from node C with\n     the corresponding\
    \ locally detected information (and node C has to\n     do the same with the message\
    \ from node B).\n   - After some (pre-determined) period of time, referred to\
    \ as the\n     hold-off time, if the local recovery actions (see Section 5.3.4)\n\
    \     were not successful, the following occurs.  In case of uni-\n     directional\
    \ failure and depending on the directionality of the LSP,\n     node B should\
    \ send an upstream notification message (see [RFC3473])\n     to the ingress node\
    \ A.  Node C may send a downstream notification\n     message (see [RFC3473])\
    \ to the egress node D.  However, in that\n     case, only node A would initiate\
    \ an edge to edge recovery action.\n     Node A is referred to as the \"master\"\
    , and node D is referred to as\n     the \"slave\", per [RFC4427].  Note that\
    \ the other LSP end-node (node\n     D in this case) may be optionally notified\
    \ using a downstream\n     notification message (see [RFC3473]).\n     In case\
    \ of bi-directional failure, node B should send an upstream\n     notification\
    \ message (see [RFC3473]) to the ingress node A.  Node C\n     may send a downstream\
    \ notification message (see [RFC3473]) to the\n     egress node D.  However, due\
    \ to the dependence on the LSP\n     directionality, only ingress node A would\
    \ initiate an edge-to-edge\n     recovery action.  Note that the other LSP end-node\
    \ (node D in this\n     case) should also be notified of this event using a downstream\n\
    \     notification message (see [RFC3473]).  For instance, if an LSP\n     directed\
    \ from D to A is under failure condition, only the\n     notification message\
    \ sent from node C to D would initiate a\n     recovery action.  In this case,\
    \ per [RFC4427], the deciding and\n     recovering node D is referred to as the\
    \ \"master\", while node A is\n     referred to as the \"slave\" (i.e., recovering\
    \ only entity).\n     Note: The determination of the master and the slave may\
    \ be based\n     either on configured information or dedicated protocol capability.\n\
    \   In the above scenarios, the path followed by the upstream and\n   downstream\
    \ notification messages does not have to be the same as the\n   one followed by\
    \ the failed LSP (see [RFC3473] for more details on the\n   notification message\
    \ exchange).  The important point concerning this\n   mechanism is that either\
    \ the detecting/reporting entity (i.e., nodes\n   B and C) is also the deciding/recovery\
    \ entity or the\n   detecting/reporting entity is simply an intermediate node\
    \ in the\n   subsequent recovery process.  One refers to local recovery in the\n\
    \   former case, and to edge-to-edge recovery in the latter one (see also\n  \
    \ Section 5.3.4).\n"
- title: 5.3.3.  Partial vs. Full Span Recovery
  contents:
  - "5.3.3.  Partial vs. Full Span Recovery\n   When a given span carries more than\
    \ one LSP or LSP segment, an\n   additional aspect must be considered.  In case\
    \ of span failure, the\n   LSPs it carries can be recovered individually, as a\
    \ group (aka bulk\n   LSP recovery), or as independent sub-groups.  When correlation\
    \ time\n   windows are used and simultaneous recovery of several LSPs can be\n\
    \   performed using a single request, the selection of this mechanism\n   would\
    \ be triggered independently of the failure notification\n   granularity.  Moreover,\
    \ criteria for forming such sub-groups are\n   outside of the scope of this document.\n\
    \   Additional complexity arises in the case of (sub-)group LSP recovery.\n  \
    \ Between a given pair of nodes, the LSPs that a given (sub-)group\n   contains\
    \ may have been created from different source nodes (i.e.,\n   initiator) and\
    \ directed toward different destination nodes.\n   Consequently the failure notification\
    \ messages following a bi-\n   directional span failure that affects several LSPs\
    \ (or the whole\n   group of LSPs it carries) are not necessarily directed toward\
    \ the\n   same initiator nodes.  In particular, these messages may be directed\n\
    \   to both the upstream and downstream nodes to the failure.  Therefore,\n  \
    \ such span failure may trigger recovery actions to be performed from\n   both\
    \ sides (i.e., from both the upstream and the downstream nodes to\n   the failure).\
    \  In order to facilitate the definition of the\n   corresponding recovery mechanisms\
    \ (and their sequence), one assumes\n   here as well that, per [RFC4427], the\
    \ deciding (and recovering)\n   entity (referred to as the \"master\") is the\
    \ only initiator of the\n   recovery of the whole LSP (sub-)group.\n"
- title: 5.3.4.  Difference between LSP, LSP Segment and Span Recovery
  contents:
  - "5.3.4.  Difference between LSP, LSP Segment and Span Recovery\n   The recovery\
    \ definitions given in [RFC4427] are quite generic and\n   apply for link (or\
    \ local span) and LSP recovery.  The major\n   difference between LSP, LSP Segment\
    \ and span recovery is related to\n   the number of intermediate nodes that the\
    \ signaling messages have to\n   travel.  Since nodes are not necessarily adjacent\
    \ in the case of LSP\n   (or LSP Segment) recovery, signaling message exchanges\
    \ from the\n   reporting to the deciding/recovery entity may have to cross several\n\
    \   intermediate nodes.  In particular, this applies to the notification\n   messages\
    \ due to the number of hops separating the location of a\n   failure occurrence\
    \ from its destination.  This results in an\n   additional propagation and forwarding\
    \ delay.  Note that the former\n   delay may in certain circumstances be non-negligible;\
    \ e.g., in a\n   copper out-of-band network, the delay is approximately 1 ms per\n\
    \   200km.\n   Moreover, the recovery mechanisms applicable to end-to-end LSPs\
    \ and\n   to the segments that may compose an end-to-end LSP (i.e., edge-to-\n\
    \   edge recovery) can be exactly the same.  However, one expects in the\n   latter\
    \ case, that the destination of the failure notification message\n   will be the\
    \ ingress/egress of each of these segments.  Therefore,\n   using the mechanisms\
    \ described in Section 5.3.2, failure notification\n   messages can be exchanged\
    \ first between terminating points of the LSP\n   segment, and after expiration\
    \ of the hold-off time, between\n   terminating points of the end-to-end LSP.\n\
    \   Note: Several studies provide quantitative analysis of the relative\n   performance\
    \ of LSP/span recovery techniques. [WANG] for instance,\n   provides an analysis\
    \ grid for these techniques showing that dynamic\n   LSP restoration (see Section\
    \ 5.5.2) performs well under medium\n   network loads, but suffers performance\
    \ degradations at higher loads\n   due to greater contention for recovery resources.\
    \  LSP restoration\n   upon span failure, as defined in [WANG], degrades at higher\
    \ loads\n   because paths around failed links tend to increase the hop count of\n\
    \   the affected LSPs and thus consume additional network resources.\n   Also,\
    \ performance of LSP restoration can be enhanced by a failed\n   working LSP's\
    \ source node that initiates a new recovery attempt if an\n   initial attempt\
    \ fails.  A single retry attempt is sufficient to\n   produce large increases\
    \ in the restoration success rate and ability\n   to initiate successful LSP restoration\
    \ attempts, especially at high\n   loads, while not adding significantly to the\
    \ long-term average\n   recovery time.  Allowing additional attempts produces\
    \ only small\n   additional gains in performance.  This suggests using additional\n\
    \   (intermediate) crankback signaling when using dynamic LSP restoration\n  \
    \ (described in Section 5.5.2 - case 2).  Details on crankback\n   signaling are\
    \ outside the scope of this document.\n"
- title: 5.4.  Difference between Recovery Type and Scheme
  contents:
  - "5.4.  Difference between Recovery Type and Scheme\n   [RFC4427] defines the basic\
    \ LSP/span recovery types.  This section\n   describes the recovery schemes that\
    \ can be built using these recovery\n   types.  In brief, a recovery scheme is\
    \ defined as the combination of\n   several ingress-egress node pairs supporting\
    \ a given recovery type\n   (from the set of the recovery types they allow). \
    \ Several examples\n   are provided here to illustrate the difference between\
    \ recovery types\n   such as 1:1 or M:N, and recovery schemes such as (1:1)^n\
    \ or (M:N)^n\n   (referred to as shared-mesh recovery).\n   1. (1:1)^n with recovery\
    \ resource sharing\n   The exponent, n, indicates the number of times a 1:1 recovery\
    \ type is\n   applied between at most n different ingress-egress node pairs. \
    \ Here,\n   at most n pairs of disjoint working and recovery LSPs/spans share\
    \ a\n   common resource at most n times.  Since the working LSPs/spans are\n \
    \  mutually disjoint, simultaneous requests for use of the shared\n   (common)\
    \ resource will only occur in case of simultaneous failures,\n   which are less\
    \ likely to happen.\n   For instance, in the common (1:1)^2 case, if the 2 recovery\
    \ LSPs in\n   the group overlap the same common resource, then it can handle only\n\
    \   single failures; any multiple working LSP failures will cause at\n   least\
    \ one working LSP to be denied automatic recovery.  Consider for\n   instance\
    \ the following topology with the working LSPs A-B-C and F-G-H\n   and their respective\
    \ recovery LSPs A-D-E-C and F-D-E-H that share a\n   common D-E link resource.\n\
    \                          A---------B---------C\n                           \\\
    \                 /\n                            \\               /\n        \
    \                     D-------------E\n                            /         \
    \      \\\n                           /                 \\\n                 \
    \         F---------G---------H\n   2. (M:N)^n with recovery resource sharing\n\
    \   The (M:N)^n scheme is documented here for the sake of completeness\n   only\
    \ (i.e., it is not mandated that GMPLS capabilities support this\n   scheme).\
    \  The exponent, n, indicates the number of times an M:N\n   recovery type is\
    \ applied between at most n different ingress-egress\n   node pairs.  So the interpretation\
    \ follows from the previous case,\n   except that here disjointness applies to\
    \ the N working LSPs/spans and\n   to the M recovery LSPs/spans while sharing\
    \ at most n times M common\n   resources.\n   In both schemes, it results in a\
    \ \"group\" of sum{n=1}^N N{n} working\n   LSPs and a pool of shared recovery\
    \ resources, not all of which are\n   available to any given working LSP.  In\
    \ such conditions, defining a\n   metric that describes the amount of overlap\
    \ among the recovery LSPs\n   would give some indication of the group's ability\
    \ to handle\n   simultaneous failures of multiple LSPs.\n   For instance, in the\
    \ simple (1:1)^n case, if n recovery LSPs in a\n   (1:1)^n group overlap, then\
    \ the group can handle only single\n   failures; any simultaneous failure of multiple\
    \ working LSPs will\n   cause at least one working LSP to be denied automatic\
    \ recovery.  But\n   if one considers, for instance, a (2:2)^2 group in which\
    \ there are\n   two pairs of overlapping recovery LSPs, then two LSPs (belonging\
    \ to\n   the same pair) can be simultaneously recovered.  The latter case can\n\
    \   be illustrated by the following topology with 2 pairs of working LSPs\n  \
    \ A-B-C and F-G-H and their respective recovery LSPs A-D-E-C and\n   F-D-E-H that\
    \ share two common D-E link resources.\n                           A========B========C\n\
    \                           \\\\               //\n                          \
    \  \\\\             //\n                             D =========== E\n       \
    \                     //             \\\\\n                           //     \
    \          \\\\\n                           F========G========H\n   Moreover,\
    \ in all these schemes, (working) path disjointness can be\n   enforced by exchanging\
    \ information related to working LSPs during the\n   recovery LSP signaling. \
    \ Specific issues related to the combination\n   of shared (discrete) bandwidth\
    \ and disjointness for recovery schemes\n   are described in Section 8.4.2.\n"
- title: 5.5.  LSP Recovery Mechanisms
  contents:
  - '5.5.  LSP Recovery Mechanisms

    '
- title: 5.5.1.  Classification
  contents:
  - "5.5.1.  Classification\n   The recovery time and ratio of LSPs/spans depend on\
    \ proper recovery\n   LSP provisioning (meaning pre-provisioning when performed\
    \ before\n   failure occurrence) and the level of overbooking of recovery\n  \
    \ resources (i.e., over-provisioning).  A proper balance of these two\n   operations\
    \ will result in the desired LSP/span recovery time and\n   ratio when single\
    \ or multiple failures occur.  Note also that these\n   operations are mostly\
    \ performed during the network planning phases.\n   The different options for\
    \ LSP (pre-)provisioning and overbooking are\n   classified below to structure\
    \ the analysis of the different recovery\n   mechanisms.\n   1. Pre-Provisioning\n\
    \   Proper recovery LSP pre-provisioning will help to alleviate the\n   failure\
    \ of the working LSPs (due to the failure of the resources that\n   carry these\
    \ LSPs).  As an example, one may compute and establish the\n   recovery LSP either\
    \ end-to-end or segment-per-segment, to protect a\n   working LSP from multiple\
    \ failure events affecting link(s), node(s)\n   and/or SRLG(s).  The recovery\
    \ LSP pre-provisioning options are\n   classified as follows in the figure below:\n\
    \   (1) The recovery path can be either pre-computed or computed on-\n       demand.\n\
    \   (2) When the recovery path is pre-computed, it can be either pre-\n      \
    \ signaled (implying recovery resource reservation) or signaled\n       on-demand.\n\
    \   (3) When the recovery resources are pre-signaled, they can be either\n   \
    \    pre-selected or selected on-demand.\n   Recovery LSP provisioning phases:\n\
    \   (1) Path Computation --> On-demand\n           |\n           |\n         \
    \   --> Pre-Computed\n                    |\n                    |\n         \
    \          (2) Signaling --> On-demand\n                           |\n       \
    \                    |\n                            --> Pre-Signaled\n       \
    \                             |\n                                    |\n     \
    \                              (3) Resource Selection --> On-demand\n        \
    \                                        |\n                                 \
    \               |\n                                                 --> Pre-Selected\n\
    \   Note that these different options lead to different LSP/span recovery\n  \
    \ times.  The following sections will consider the above-mentioned\n   pre-provisioning\
    \ options when analyzing the different recovery\n   mechanisms.\n   2. Overbooking\n\
    \   There are many mechanisms available that allow the overbooking of the\n  \
    \ recovery resources.  This overbooking can be done per LSP (as in the\n   example\
    \ mentioned above), per link (such as span protection), or even\n   per domain.\
    \  In all these cases, the level of overbooking, as shown\n   in the below figure,\
    \ can be classified as dedicated (such as 1+1 and\n   1:1), shared (such as 1:N\
    \ and M:N), or unprotected (and thus\n   restorable, if enough recovery resources\
    \ are available).\n   Overbooking levels:\n                    +----- Dedicated\
    \ (for instance: 1+1, 1:1, etc.)\n                    |\n                    |\n\
    \                    +----- Shared (for instance: 1:N, M:N, etc.)\n          \
    \          |\n   Level of         |\n   Overbooking -----+----- Unprotected (for\
    \ instance: 0:1, 0:N)\n   Also, when using shared recovery, one may support preemptible\
    \ extra-\n   traffic; the recovery mechanism is then expected to allow preemption\n\
    \   of this low priority traffic in case of recovery resource contention\n   during\
    \ recovery operations.  The following sections will consider the\n   above-mentioned\
    \ overbooking options when analyzing the different\n   recovery mechanisms.\n"
- title: 5.5.2.  LSP Restoration
  contents:
  - "5.5.2.  LSP Restoration\n   The following times are defined to provide a quantitative\
    \ estimation\n   about the time performance of the different LSP restoration\n\
    \   mechanisms (also referred to as LSP re-routing):\n   - Path Computation Time:\
    \ Tc\n   - Path Selection Time: Ts\n   - End-to-end LSP Resource Reservation Time:\
    \ Tr (a delta for resource\n     selection is also considered, the corresponding\
    \ total time is then\n     referred to as Trs)\n   - End-to-end LSP Resource Activation\
    \ Time: Ta (a delta for\n     resource selection is also considered, the corresponding\
    \ total\n     time is then referred to as Tas)\n   The Path Selection Time (Ts)\
    \ is considered when a pool of recovery\n   LSP paths between a given pair of\
    \ source/destination end-points is\n   pre-computed, and after a failure occurrence\
    \ one of these paths is\n   selected for the recovery of the LSP under failure\
    \ condition.\n   Note: failure management operations such as failure detection,\n\
    \   correlation, and notification are considered (for a given failure\n   event)\
    \ as equally time-consuming for all the mechanisms described\n   below:\n   1.\
    \ With Route Pre-computation (or LSP re-provisioning)\n   An end-to-end restoration\
    \ LSP is established after the failure(s)\n   occur(s) based on a pre-computed\
    \ path.  As such, one can define this\n   as an \"LSP re-provisioning\" mechanism.\
    \  Here, one or more (disjoint)\n   paths for the restoration LSP are computed\
    \ (and optionally pre-\n   selected) before a failure occurs.\n   No reservation\
    \ or selection of resources is performed along the\n   restoration path before\
    \ failure occurrence.  As a result, there is no\n   guarantee that a restoration\
    \ LSP is available when a failure occurs.\n   The expected total restoration time\
    \ T is thus equal to Ts + Trs or to\n   Trs when a dedicated computation is performed\
    \ for each working LSP.\n   2. Without Route Pre-computation (or Full LSP re-routing)\n\
    \   An end-to-end restoration LSP is dynamically established after the\n   failure(s)\
    \ occur(s).  After failure occurrence, one or more\n   (disjoint) paths for the\
    \ restoration LSP are dynamically computed and\n   one is selected.  As such,\
    \ one can define this as a complete \"LSP\n   re-routing\" mechanism.\n   No reservation\
    \ or selection of resources is performed along the\n   restoration path before\
    \ failure occurrence.  As a result, there is no\n   guarantee that a restoration\
    \ LSP is available when a failure occurs.\n   The expected total restoration time\
    \ T is thus equal to Tc (+ Ts) +\n   Trs.  Therefore, time performance between\
    \ these two approaches\n   differs by the time required for route computation\
    \ Tc (and its\n   potential selection time, Ts).\n"
- title: 5.5.3.  Pre-Planned LSP Restoration
  contents:
  - "5.5.3.  Pre-Planned LSP Restoration\n   Pre-planned LSP restoration (also referred\
    \ to as pre-planned LSP re-\n   routing) implies that the restoration LSP is pre-signaled.\
    \  This in\n   turn implies the reservation of recovery resources along the\n\
    \   restoration path.  Two cases can be defined based on whether the\n   recovery\
    \ resources are pre-selected.\n   1. With resource reservation and without resource\
    \ pre-selection\n   Before failure occurrence, an end-to-end restoration path\
    \ is pre-\n   selected from a set of pre-computed (disjoint) paths.  The\n   restoration\
    \ LSP is signaled along this pre-selected path to reserve\n   resources at each\
    \ node, but these resources are not selected.\n   In this case, the resources\
    \ reserved for each restoration LSP may be\n   dedicated or shared between multiple\
    \ restoration LSPs whose working\n   LSPs are not expected to fail simultaneously.\
    \  Local node policies\n   can be applied to define the degree to which these\
    \ resources can be\n   shared across independent failures.  Also, since a restoration\
    \ scheme\n   is considered, resource sharing should not be limited to restoration\n\
    \   LSPs that start and end at the same ingress and egress nodes.\n   Therefore,\
    \ each node participating in this scheme is expected to\n   receive some feedback\
    \ information on the sharing degree of the\n   recovery resource(s) that this\
    \ scheme involves.\n   Upon failure detection/notification message reception,\
    \ signaling is\n   initiated along the restoration path to select the resources,\
    \ and to\n   perform the appropriate operation at each node crossed by the\n \
    \  restoration LSP (e.g., cross-connections).  If lower priority LSPs\n   were\
    \ established using the restoration resources, they must be\n   preempted when\
    \ the restoration LSP is activated.\n   Thus, the expected total restoration time\
    \ T is equal to Tas (post-\n   failure activation), while operations performed\
    \ before failure\n   occurrence take Tc + Ts + Tr.\n   2. With both resource reservation\
    \ and resource pre-selection\n   Before failure occurrence, an end-to-end restoration\
    \ path is pre-\n   selected from a set of pre-computed (disjoint) paths.  The\n\
    \   restoration LSP is signaled along this pre-selected path to reserve\n   AND\
    \ select resources at each node, but these resources are not\n   committed at\
    \ the data plane level.  So that the selection of the\n   recovery resources is\
    \ committed at the control plane level only, no\n   cross-connections are performed\
    \ along the restoration path.\n   In this case, the resources reserved and selected\
    \ for each\n   restoration LSP may be dedicated or even shared between multiple\n\
    \   restoration LSPs whose associated working LSPs are not expected to\n   fail\
    \ simultaneously.  Local node policies can be applied to define\n   the degree\
    \ to which these resources can be shared across independent\n   failures.  Also,\
    \ because a restoration scheme is considered, resource\n   sharing should not\
    \ be limited to restoration LSPs that start and end\n   at the same ingress and\
    \ egress nodes.  Therefore, each node\n   participating in this scheme is expected\
    \ to receive some feedback\n   information on the sharing degree of the recovery\
    \ resource(s) that\n   this scheme involves.\n   Upon failure detection/notification\
    \ message reception, signaling is\n   initiated along the restoration path to\
    \ activate the reserved and\n   selected resources, and to perform the appropriate\
    \ operation at each\n   node crossed by the restoration LSP (e.g., cross-connections).\
    \  If\n   lower priority LSPs were established using the restoration resources,\n\
    \   they must be preempted when the restoration LSP is activated.\n   Thus, the\
    \ expected total restoration time T is equal to Ta (post-\n   failure activation),\
    \ while operations performed before failure\n   occurrence take Tc + Ts + Trs.\
    \  Therefore, time performance between\n   these two approaches differs only by\
    \ the time required for resource\n   selection during the activation of the recovery\
    \ LSP (i.e., Tas - Ta).\n"
- title: 5.5.4.  LSP Segment Restoration
  contents:
  - "5.5.4.  LSP Segment Restoration\n   The above approaches can be applied on an\
    \ edge-to-edge LSP basis\n   rather than end-to-end LSP basis (i.e., to reduce\
    \ the global recovery\n   time) by allowing the recovery of the individual LSP\
    \ segments\n   constituting the end-to-end LSP.\n   Also, by using the horizontal\
    \ hierarchy approach described in Section\n   7.1, an end-to-end LSP can be recovered\
    \ by multiple recovery\n   mechanisms applied on an LSP segment basis (e.g., 1:1\
    \ edge-to-edge\n   LSP protection in a metro network, and M:N edge-to-edge protection\
    \ in\n   the core).  These mechanisms are ideally independent and may even use\n\
    \   different failure localization and notification mechanisms.\n"
- title: 6.  Reversion
  contents:
  - "6.  Reversion\n   Reversion (a.k.a. normalization) is defined as the mechanism\
    \ allowing\n   switching of normal traffic from the recovery LSP/span to the working\n\
    \   LSP/span previously under failure condition.  Use of normalization is\n  \
    \ at the discretion of the recovery domain policy.  Normalization may\n   impact\
    \ the normal traffic (a second hit) depending on the\n   normalization mechanism\
    \ used.\n   If normalization is supported, then 1) the LSP/span must be returned\n\
    \   to the working LSP/span when the failure condition clears and 2) the\n   capability\
    \ to de-activate (turn-off) the use of reversion should be\n   provided.  De-activation\
    \ of reversion should not impact the normal\n   traffic, regardless of whether\
    \ it is currently using the working or\n   recovery LSP/span.\n   Note: during\
    \ the failure, the reuse of any non-failed resources\n   (e.g., LSP and/or spans)\
    \ belonging to the working LSP/span is under\n   the discretion of recovery domain\
    \ policy.\n"
- title: 6.1.  Wait-To-Restore (WTR)
  contents:
  - "6.1.  Wait-To-Restore (WTR)\n   A specific mechanism (Wait-To-Restore) is used\
    \ to prevent frequent\n   recovery switching operations due to an intermittent\
    \ defect (e.g.,\n   Bit Error Rate (BER) fluctuating around the SD threshold).\n\
    \   First, an LSP/span under failure condition must become fault-free,\n   e.g.,\
    \ a BER less than a certain recovery threshold.  After the\n   recovered LSP/span\
    \ (i.e., the previously working LSP/span) meets this\n   criterion, a fixed period\
    \ of time shall elapse before normal traffic\n   uses the corresponding resources\
    \ again.  This duration called Wait-\n   To-Restore (WTR) period or timer is generally\
    \ on the order of a few\n   minutes (for instance, 5 minutes) and should be capable\
    \ of being set.\n   The WTR timer may be either a fixed period, or provide for\n\
    \   incrementally longer periods before retrying.  An SF or SD condition\n   on\
    \ the previously working LSP/span will override the WTR timer value\n   (i.e.,\
    \ the WTR cancels and the WTR timer will restart).\n"
- title: 6.2.  Revertive Mode Operation
  contents:
  - "6.2.  Revertive Mode Operation\n   In revertive mode of operation, when the recovery\
    \ LSP/span is no\n   longer required, i.e., the failed working LSP/span is no\
    \ longer in SD\n   or SF condition, a local Wait-to-Restore (WTR) state will be\n\
    \   activated before switching the normal traffic back to the recovered\n   working\
    \ LSP/span.\n   During the reversion operation, since this state becomes the highest\n\
    \   in priority, signaling must maintain the normal traffic on the\n   recovery\
    \ LSP/span from the previously failed working LSP/span.\n   Moreover, during this\
    \ WTR state, any null traffic or extra traffic\n   (if applicable) request is\
    \ rejected.\n   However, deactivation (cancellation) of the wait-to-restore timer\
    \ may\n   occur if there are higher priority request attempts.  That is, the\n\
    \   recovery LSP/span usage by the normal traffic may be preempted if a\n   higher\
    \ priority request for this recovery LSP/span is attempted.\n"
- title: 6.3.  Orphans
  contents:
  - "6.3.  Orphans\n   When a reversion operation is requested, normal traffic must\
    \ be\n   switched from the recovery to the recovered working LSP/span.  A\n  \
    \ particular situation occurs when the previously working LSP/span\n   cannot\
    \ be recovered, so normal traffic cannot be switched back.  In\n   that case,\
    \ the LSP/span under failure condition (also referred to as\n   \"orphan\") must\
    \ be cleared (i.e., removed) from the pool of resources\n   allocated for normal\
    \ traffic.  Otherwise, potential de-\n   synchronization between the control and\
    \ transport plane resource\n   usage can appear.  Depending on the signaling protocol\
    \ capabilities\n   and behavior, different mechanisms are expected here.\n   Therefore,\
    \ any reserved or allocated resources for the LSP/span under\n   failure condition\
    \ must be unreserved/de-allocated.  Several ways can\n   be used for that purpose:\
    \ wait for the clear-out time interval to\n   elapse, initiate a deletion from\
    \ the ingress or the egress node, or\n   trigger the initiation of deletion from\
    \ an entity (such as an EMS or\n   NMS) capable of reacting upon reception of\
    \ an appropriate\n   notification message.\n"
- title: 7.  Hierarchies
  contents:
  - "7.  Hierarchies\n   Recovery mechanisms are being made available at multiple\
    \ (if not all)\n   transport layers within so-called \"IP/MPLS-over-optical\"\
    \ networks.\n   However, each layer has certain recovery features, and one needs\
    \ to\n   determine the exact impact of the interaction between the recovery\n\
    \   mechanisms provided by these layers.\n   Hierarchies are used to build scalable\
    \ complex systems.  By hiding\n   the internal details, abstraction is used as\
    \ a mechanism to build\n   large networks or as a technique for enforcing technology,\n\
    \   topological, or administrative boundaries.  The same hierarchical\n   concept\
    \ can be applied to control the network survivability.  Network\n   survivability\
    \ is the set of capabilities that allow a network to\n   restore affected traffic\
    \ in the event of a failure.  Network\n   survivability is defined further in\
    \ [RFC4427].  In general, it is\n   expected that the recovery action is taken\
    \ by the recoverable\n   LSP/span closest to the failure in order to avoid the\
    \ multiplication\n   of recovery actions.  Moreover, recovery hierarchies also\
    \ can be\n   bound to control plane logical partitions (e.g., administrative or\n\
    \   topological boundaries).  Each logical partition may apply different\n   recovery\
    \ mechanisms.\n   In brief, it is commonly accepted that the lower layers can\
    \ provide\n   coarse but faster recovery while the higher layers can provide finer\n\
    \   but slower recovery.  Moreover, it is also desirable to avoid similar\n  \
    \ layers with functional overlaps in order to optimize network resource\n   utilization\
    \ and processing overhead, since repeating the same\n   capabilities at each layer\
    \ does not create any added value for the\n   network as a whole.  In addition,\
    \ even if a lower layer recovery\n   mechanism is enabled, it does not prevent\
    \ the additional provision of\n   a recovery mechanism at the upper layer.  The\
    \ inverse statement does\n   not necessarily hold; that is, enabling an upper\
    \ layer recovery\n   mechanism may prevent the use of a lower layer recovery mechanism.\n\
    \   In this context, this section analyzes these hierarchical aspects\n   including\
    \ the physical (passive) layer(s).\n"
- title: 7.1.  Horizontal Hierarchy (Partitioning)
  contents:
  - "7.1.  Horizontal Hierarchy (Partitioning)\n   A horizontal hierarchy is defined\
    \ when partitioning a single-layer\n   network (and its control plane) into several\
    \ recovery domains.\n   Within a domain, the recovery scope may extend over a\
    \ link (or span),\n   LSP segment, or even an end-to-end LSP.  Moreover, an administrative\n\
    \   domain may consist of a single recovery domain or can be partitioned\n   into\
    \ several smaller recovery domains.  The operator can partition\n   the network\
    \ into recovery domains based on physical network topology,\n   control plane\
    \ capabilities, or various traffic engineering\n   constraints.\n   An example\
    \ often addressed in the literature is the metro-core-metro\n   application (sometimes\
    \ extended to a metro-metro/core-core) within a\n   single transport layer (see\
    \ Section 7.2).  For such a case, an end-\n   to-end LSP is defined between the\
    \ ingress and egress metro nodes,\n   while LSP segments may be defined within\
    \ the metro or core sub-\n   networks.  Each of these topological structures determines\
    \ a so-\n   called \"recovery domain\" since each of the LSPs they carry can have\n\
    \   its own recovery type (or even scheme).  The support of multiple\n   recovery\
    \ types and schemes within a sub-network is referred to as a\n   \"multi-recovery\
    \ capable domain\" or simply \"multi-recovery domain\".\n"
- title: 7.2.  Vertical Hierarchy (Layers)
  contents:
  - "7.2.  Vertical Hierarchy (Layers)\n   It is very challenging to combine the different\
    \ recovery capabilities\n   available across the path (i.e., switching capable)\
    \ and section\n   layers to ensure that certain network survivability objectives\
    \ are\n   met for the network-supported services.\n   As a first analysis step,\
    \ one can draw the following guidelines for\n   a vertical coordination of the\
    \ recovery mechanisms:\n   - The lower the layer, the faster the notification\
    \ and switching.\n   - The higher the layer, the finer the granularity of the\
    \ recoverable\n     entity and therefore the granularity of the recovery resource.\n\
    \   Moreover, in the context of this analysis, a vertical hierarchy\n   consists\
    \ of multiple layered transport planes providing different:\n   - Discrete bandwidth\
    \ granularities for non-packet LSPs such as OCh,\n     ODUk, STS_SPE/HOVC, and\
    \ VT_SPE/LOVC LSPs and continuous bandwidth\n     granularities for packet LSPs.\n\
    \   - Potential recovery capabilities with different temporal\n     granularities:\
    \ ranging from milliseconds to tens of seconds\n   Note: based on the bandwidth\
    \ granularity, we can determine four\n   classes of vertical hierarchies: (1)\
    \ packet over packet, (2) packet\n   over circuit, (3) circuit over packet, and\
    \ (4) circuit over circuit.\n   Below we briefly expand on (4) only. (2) is covered\
    \ in [RFC3386]. (1)\n   is extensively covered by the MPLS Working Group, and\
    \ (3) by the PWE3\n   Working Group.\n   In SONET/SDH environments, one typically\
    \ considers the VT_SPE/LOVC\n   and STS SPE/HOVC as independent layers (for example,\
    \ VT_SPE/LOVC LSP\n   uses the underlying STS_SPE/HOVC LSPs as links).  In OTN,\
    \ the ODUk\n   path layers will lie on the OCh path layer, i.e., the ODUk LSPs\
    \ use\n   the underlying OCh LSPs as OTUk links.  Note here that lower layer\n\
    \   LSPs may simply be provisioned and not necessarily dynamically\n   triggered\
    \ or established (control driven approach).  In this context,\n   an LSP at the\
    \ path layer (i.e., established using GMPLS signaling),\n   such as an optical\
    \ channel LSP, appears at the OTUk layer as a link,\n   controlled by a link management\
    \ protocol such as LMP.\n   The first key issue with multi-layer recovery is that\
    \ achieving\n   individual or bulk LSP recovery will be as efficient as the\n\
    \   underlying link (local span) recovery.  In such a case, the span can\n   be\
    \ either protected or unprotected, but the LSP it carries must be\n   (at least\
    \ locally) recoverable.  Therefore, the span recovery process\n   can be either\
    \ independent when protected (or restorable), or\n   triggered by the upper LSP\
    \ recovery process.  The former case\n   requires coordination to achieve subsequent\
    \ LSP recovery.  Therefore,\n   in order to achieve robustness and fast convergence,\
    \ multi-layer\n   recovery requires a fine-tuned coordination mechanism.\n   Moreover,\
    \ in the absence of adequate recovery mechanism coordination\n   (for instance,\
    \ a pre-determined coordination when using a hold-off\n   timer), a failure notification\
    \ may propagate from one layer to the\n   next one within a recovery hierarchy.\
    \  This can cause \"collisions\"\n   and trigger simultaneous recovery actions\
    \ that may lead to race\n   conditions and, in turn, reduce the optimization of\
    \ the resource\n   utilization and/or generate global instabilities in the network\
    \ (see\n   [MANCHESTER]).  Therefore, a consistent and efficient escalation\n\
    \   strategy is needed to coordinate recovery across several layers.\n   One can\
    \ expect that the definition of the recovery mechanisms and\n   protocol(s) is\
    \ technology-independent so that they can be\n   consistently implemented at different\
    \ layers; this would in turn\n   simplify their global coordination.  Moreover,\
    \ as mentioned in\n   [RFC3386], some looser form of coordination and communication\
    \ between\n   (vertical) layers such as a consistent hold-off timer configuration\n\
    \   (and setup through signaling during the working LSP establishment)\n   can\
    \ be considered, thereby allowing the synchronization between\n   recovery actions\
    \ performed across these layers.\n"
- title: 7.2.1.  Recovery Granularity
  contents:
  - "7.2.1.  Recovery Granularity\n   In most environments, the design of the network\
    \ and the vertical\n   distribution of the LSP bandwidth are such that the recovery\n\
    \   granularity is finer at higher layers.  The OTN and SONET/SDH layers\n   can\
    \ recover only the whole section or the individual connections they\n   transports\
    \ whereas the IP/MPLS control plane can recover individual\n   packet LSPs or\
    \ groups of packet LSPs independently of their\n   granularity.  On the other\
    \ side, the recovery granularity at the\n   sub-wavelength level (i.e., SONET/SDH)\
    \ can be provided only when the\n   network includes devices switching at the\
    \ same granularity (and thus\n   not with optical channel level).  Therefore,\
    \ the network layer can\n   deliver control-plane-driven recovery mechanisms on\
    \ a per-LSP basis\n   if and only if these LSPs have their corresponding switching\n\
    \   granularity supported at the transport plane level.\n"
- title: 7.3.  Escalation Strategies
  contents:
  - "7.3.  Escalation Strategies\n   There are two types of escalation strategies\
    \ (see [DEMEESTER]):\n   bottom-up and top-down.\n   The bottom-up approach assumes\
    \ that lower layer recovery types and\n   schemes are more expedient and faster\
    \ than upper layer ones.\n   Therefore, we can inhibit or hold off higher layer\
    \ recovery.\n   However, this assumption is not entirely true.  Consider for instance\n\
    \   a SONET/SDH based protection mechanism (with a protection switching\n   time\
    \ of less than 50 ms) lying on top of an OTN restoration mechanism\n   (with a\
    \ restoration time of less than 200 ms).  Therefore, this\n   assumption should\
    \ be (at least) clarified as: the lower layer\n   recovery mechanism is expected\
    \ to be faster than the upper level one,\n   if the same type of recovery mechanism\
    \ is used at each layer.\n   Consequently, taking into account the recovery actions\
    \ at the\n   different layers in a bottom-up approach: if lower layer recovery\n\
    \   mechanisms are provided and sequentially activated in conjunction\n   with\
    \ higher layer ones, the lower layers must have an opportunity to\n   recover\
    \ normal traffic before the higher layers do.  However, if\n   lower layer recovery\
    \ is slower than higher layer recovery, the lower\n   layer must either communicate\
    \ the failure-related information to the\n   higher layer(s) (and allow it to\
    \ perform recovery), or use a hold-off\n   timer in order to temporarily set the\
    \ higher layer recovery action in\n   a \"standby mode\".  Note that the a priori\
    \ information exchange\n   between layers concerning their efficiency is not within\
    \ the current\n   scope of this document.  Nevertheless, the coordination functionality\n\
    \   between layers must be configurable and tunable.\n   For example, coordination\
    \ between the optical and packet layer\n   control plane enables the optical layer\
    \ to perform the failure\n   management operations (in particular, failure detection\
    \ and\n   notification) while giving to the packet layer control plane the\n \
    \  authority to decide and perform the recovery actions.  If the packet\n   layer\
    \ recovery action is unsuccessful, fallback at the optical layer\n   can be performed\
    \ subsequently.\n   The top-down approach attempts service recovery at the higher\
    \ layers\n   before invoking lower layer recovery.  Higher layer recovery is\n\
    \   service selective, and permits \"per-CoS\" or \"per-connection\" re-\n   routing.\
    \  With this approach, the most important aspect is that the\n   upper layer should\
    \ provide its own reliable and independent failure\n   detection mechanism from\
    \ the lower layer.\n   [DEMEESTER] also suggests recovery mechanisms incorporating\
    \ a\n   coordinated effort shared by two adjacent layers with periodic status\n\
    \   updates.  Moreover, some of these recovery operations can be pre-\n   assigned\
    \ (on a per-link basis) to a certain layer, e.g., a given link\n   will be recovered\
    \ at the packet layer while another will be recovered\n   at the optical layer.\n"
- title: 7.4.  Disjointness
  contents:
  - "7.4.  Disjointness\n   Having link and node diverse working and recovery LSPs/spans\
    \ does not\n   guarantee their complete disjointness.  Due to the common physical\n\
    \   layer topology (passive), additional hierarchical concepts, such as\n   the\
    \ Shared Risk Link Group (SRLG), and mechanisms, such as SRLG\n   diverse path\
    \ computation, must be developed to provide complete\n   working and recovery\
    \ LSP/span disjointness (see [IPO-IMP] and\n   [RFC4202]).  Otherwise, a failure\
    \ affecting the working LSP/span\n   would also potentially affect the recovery\
    \ LSP/span; one refers to\n   such an event as \"common failure\".\n"
- title: 7.4.1.  SRLG Disjointness
  contents:
  - "7.4.1.  SRLG Disjointness\n   A Shared Risk Link Group (SRLG) is defined as the\
    \ set of links\n   sharing a common risk (such as a common physical resource such\
    \ as a\n   fiber link or a fiber cable).  For instance, a set of links L belongs\n\
    \   to the same SRLG s, if they are provisioned over the same fiber link\n   f.\n\
    \   The SRLG properties can be summarized as follows:\n   1) A link belongs to\
    \ more than one SRLG if and only if it crosses one\n      of the resources covered\
    \ by each of them.\n   2) Two links belonging to the same SRLG can belong individually\
    \ to\n      (one or more) other SRLGs.\n   3) The SRLG set S of an LSP is defined\
    \ as the union of the individual\n      SRLG s of the individual links composing\
    \ this LSP.\n   SRLG disjointness is also applicable to LSPs:\n      The LSP SRLG\
    \ disjointness concept is based on the following\n      postulate: an LSP (i.e.,\
    \ a sequence of links and nodes) covers an\n      SRLG if and only if it crosses\
    \ one of the links or nodes belonging\n      to that SRLG.\n      Therefore, the\
    \ SRLG disjointness for LSPs, can be defined as\n      follows: two LSPs are disjoint\
    \ with respect to an SRLG s if and\n      only if they do not cover simultaneously\
    \ this SRLG s.\n      Whilst the SRLG disjointness for LSPs with respect to a\
    \ set S of\n      SRLGs, is defined as follows: two LSPs are disjoint with respect\n\
    \      to a set of SRLGs S if and only if the set of SRLGs that are\n      common\
    \ to both LSPs is disjoint from set S.\n   The impact on recovery is noticeable:\
    \ SRLG disjointness is a\n   necessary (but not a sufficient) condition to ensure\
    \ network\n   survivability.  With respect to the physical network resources,\
    \ a\n   working-recovery LSP/span pair must be SRLG-disjoint in case of\n   dedicated\
    \ recovery type.  On the other hand, in case of shared\n   recovery, a group of\
    \ working LSP/spans must be mutually SRLG-disjoint\n   in order to allow for a\
    \ (single and common) shared recovery LSP that\n   is itself SRLG-disjoint from\
    \ each of the working LSPs/spans.\n"
- title: 8.  Recovery Mechanisms Analysis
  contents:
  - "8.  Recovery Mechanisms Analysis\n   In order to provide a structured analysis\
    \ of the recovery mechanisms\n   detailed in the previous sections, the following\
    \ dimensions can be\n   considered:\n   1. Fast convergence (performance): provide\
    \ a mechanism that\n      aggregates multiple failures (implying fast failure\
    \ detection and\n      correlation mechanisms) and fast recovery decision independently\n\
    \      of the number of failures occurring in the optical network (also\n    \
    \  implying a fast failure notification).\n   2. Efficiency (scalability): minimize\
    \ the switching time required for\n      LSP/span recovery independently of the\
    \ number of LSPs/spans being\n      recovered (this implies efficient failure\
    \ correlation, fast\n      failure notification, and time-efficient recovery mechanisms).\n\
    \   3. Robustness (availability): minimize the LSP/span downtime\n      independently\
    \ of the underlying topology of the transport plane\n      (this implies a highly\
    \ responsive recovery mechanism).\n   4. Resource optimization (optimality): minimize\
    \ the resource\n      capacity, including LSPs/spans and nodes (switching capacity),\n\
    \      required for recovery purposes; this dimension can also be\n      referred\
    \ to as optimizing the sharing degree of the recovery\n      resources.\n   5.\
    \ Cost optimization: provide a cost-effective recovery type/scheme.\n   However,\
    \ these dimensions are either outside the scope of this\n   document (such as\
    \ cost optimization and recovery path computational\n   aspects) or mutually conflicting.\
    \  For instance, it is obvious that\n   providing a 1+1 LSP protection minimizes\
    \ the LSP downtime (in case of\n   failure) while being non-scalable and consuming\
    \ recovery resource\n   without enabling any extra-traffic.\n   The following\
    \ sections analyze the recovery phases and mechanisms\n   detailed in the previous\
    \ sections with respect to the dimensions\n   described above in order to assess\
    \ the GMPLS protocol suite\n   capabilities and applicability.  In turn, this\
    \ allows the evaluation\n   of the potential need for further GMPLS signaling\
    \ and routing\n   extensions.\n"
- title: 8.1.  Fast Convergence (Detection/Correlation and Hold-off Time)
  contents:
  - "8.1.  Fast Convergence (Detection/Correlation and Hold-off Time)\n   Fast convergence\
    \ is related to the failure management operations.  It\n   refers to the time\
    \ elapsed between failure detection/correlation and\n   hold-off time, the point\
    \ at which the recovery switching actions are\n   initiated.  This point has been\
    \ detailed in Section 4.\n"
- title: 8.2.  Efficiency (Recovery Switching Time)
  contents:
  - "8.2.  Efficiency (Recovery Switching Time)\n   In general, the more pre-assignment/pre-planning\
    \ of the recovery\n   LSP/span, the more rapid the recovery is.  Because protection\
    \ implies\n   pre-assignment (and cross-connection) of the protection resources,\
    \ in\n   general, protection recovers faster than restoration.\n   Span restoration\
    \ is likely to be slower than most span protection\n   types; however this greatly\
    \ depends on the efficiency of the span\n   restoration signaling.  LSP restoration\
    \ with pre-signaled and pre-\n   selected recovery resources is likely to be faster\
    \ than fully dynamic\n   LSP restoration, especially because of the elimination\
    \ of any\n   potential crankback during the recovery LSP establishment.\n   If\
    \ one excludes the crankback issue, the difference between dynamic\n   and pre-planned\
    \ restoration depends on the restoration path\n   computation and selection time.\
    \  Since computational considerations\n   are outside the scope of this document,\
    \ it is up to the vendor to\n   determine the average and maximum path computation\
    \ time in different\n   scenarios and to the operator to decide whether or not\
    \ dynamic\n   restoration is advantageous over pre-planned schemes that depend\
    \ on\n   the network environment.  This difference also depends on the\n   flexibility\
    \ provided by pre-planned restoration versus dynamic\n   restoration.  Pre-planned\
    \ restoration implies a somewhat limited\n   number of failure scenarios (that\
    \ can be due, for instance, to local\n   storage capacity limitation).  Dynamic\
    \ restoration enables on-demand\n   path computation based on the information\
    \ received through failure\n   notification message, and as such, it is more robust\
    \ with respect to\n   the failure scenario scope.\n   Moreover, LSP segment restoration,\
    \ in particular, dynamic restoration\n   (i.e., no path pre-computation, so none\
    \ of the recovery resource is\n   pre-reserved) will generally be faster than\
    \ end-to-end LSP\n   restoration.  However, local LSP restoration assumes that\
    \ each LSP\n   segment end-point has enough computational capacity to perform\
    \ this\n   operation while end-to-end LSP restoration requires only that LSP\n\
    \   end-points provide this path computation capability.\n   Recovery time objectives\
    \ for SONET/SDH protection switching (not\n   including time to detect failure)\
    \ are specified in [G.841] at 50 ms,\n   taking into account constraints on distance,\
    \ number of connections\n   involved, and in the case of ring enhanced protection,\
    \ number of\n   nodes in the ring.  Recovery time objectives for restoration\n\
    \   mechanisms have been proposed through a separate effort [RFC3386].\n"
- title: 8.3.  Robustness
  contents:
  - "8.3.  Robustness\n   In general, the less pre-assignment (protection)/pre-planning\n\
    \   (restoration) of the recovery LSP/span, the more robust the recovery\n   type\
    \ or scheme is to a variety of single failures, provided that\n   adequate resources\
    \ are available.  Moreover, the pre-selection of the\n   recovery resources gives\
    \ (in the case of multiple failure scenarios)\n   less flexibility than no recovery\
    \ resource pre-selection.  For\n   instance, if failures occur that affect two\
    \ LSPs sharing a common\n   link along their restoration paths, then only one\
    \ of these LSPs can\n   be recovered.  This occurs unless the restoration path\
    \ of at least\n   one of these LSPs is re-computed, or the local resource assignment\
    \ is\n   modified on the fly.\n   In addition, recovery types and schemes with\
    \ pre-planned recovery\n   resources (in particular, LSP/spans for protection\
    \ and LSPs for\n   restoration purposes) will not be able to recover from failures\
    \ that\n   simultaneously affect both the working and recovery LSP/span.  Thus,\n\
    \   the recovery resources should ideally be as disjoint as possible\n   (with\
    \ respect to link, node, and SRLG) from the working ones, so that\n   any single\
    \ failure event will not affect both working and recovery\n   LSP/span.  In brief,\
    \ working and recovery resources must be fully\n   diverse in order to guarantee\
    \ that a given failure will not affect\n   simultaneously the working and the\
    \ recovery LSP/span.  Also, the risk\n   of simultaneous failure of the working\
    \ and the recovery LSPs can be\n   reduced.  It is reduced by computing a new\
    \ recovery path whenever a\n   failure occurs along one of the recovery LSPs or\
    \ by computing a new\n   recovery path and provision the corresponding LSP whenever\
    \ a failure\n   occurs along a working LSP/span.  Both methods enable the network\
    \ to\n   maintain the number of available recovery path constant.\n   The robustness\
    \ of a recovery scheme is also determined by the amount\n   of pre-reserved (i.e.,\
    \ signaled) recovery resources within a given\n   shared resource pool: as the\
    \ sharing degree of recovery resources\n   increases, the recovery scheme becomes\
    \ less robust to multiple\n   LSP/span failure occurrences.  Recovery schemes,\
    \ in particular\n   restoration, with pre-signaled resource reservation (with\
    \ or without\n   pre-selection) should be capable of reserving an adequate amount\
    \ of\n   resource to ensure recovery from any specific set of failure events,\n\
    \   such as any single SRLG failure, any two SRLG failures, etc.\n"
- title: 8.4.  Resource Optimization
  contents:
  - "8.4.  Resource Optimization\n   It is commonly admitted that sharing recovery\
    \ resources provides\n   network resource optimization.  Therefore, from a resource\n\
    \   utilization perspective, protection schemes are often classified with\n  \
    \ respect to their degree of sharing recovery resources with the\n   working entities.\
    \  Moreover, non-permanent bridging protection types\n   allow (under normal conditions)\
    \ for extra-traffic over the recovery\n   resources.\n   From this perspective,\
    \ the following statements are true:\n   1) 1+1 LSP/Span protection is the most\
    \ resource-consuming protection\n      type because it does not allow for any\
    \ extra traffic.\n   2) 1:1 LSP/span recovery requires dedicated recovery LSP/span\n\
    \      allowing for extra traffic.\n   3) 1:N and M:N LSP/span recovery require\
    \ 1 (and M, respectively)\n      recovery LSP/span (shared between the N working\
    \ LSP/span) allowing\n      for extra traffic.\n   Obviously, 1+1 protection precludes,\
    \ and 1:1 recovery does not allow\n   for any recovery LSP/span sharing, whereas\
    \ 1:N and M:N recovery do\n   allow sharing of 1 (M, respectively) recovery LSP/spans\
    \ between N\n   working LSP/spans.  However, despite the fact that 1:1 LSP recovery\n\
    \   precludes the sharing of the recovery LSP, the recovery schemes that\n   can\
    \ be built from it (e.g., (1:1)^n, see Section 5.4) do allow\n   sharing of its\
    \ recovery resources.  In addition, the flexibility in\n   the usage of shared\
    \ recovery resources (in particular, shared links)\n   may be limited because\
    \ of network topology restrictions, e.g., fixed\n   ring topology for traditional\
    \ enhanced protection schemes.\n   On the other hand, when using LSP restoration\
    \ with pre-signaled\n   resource reservation, the amount of reserved restoration\
    \ capacity is\n   determined by the local bandwidth reservation policies.  In\
    \ LSP\n   restoration schemes with re-provisioning, a pool of spare resources\n\
    \   can be defined from which all resources are selected after failure\n   occurrence\
    \ for the purpose of restoration path computation.  The\n   degree to which restoration\
    \ schemes allow sharing amongst multiple\n   independent failures is then directly\
    \ inferred from the size of the\n   resource pool.  Moreover, in all restoration\
    \ schemes, spare resources\n   can be used to carry preemptible traffic (thus\
    \ over preemptible\n   LSP/span) when the corresponding resources have not been\
    \ committed\n   for LSP/span recovery purposes.\n   From this, it clearly follows\
    \ that less recovery resources (i.e.,\n   LSP/spans and switching capacity) have\
    \ to be allocated to a shared\n   recovery resource pool if a greater sharing\
    \ degree is allowed.  Thus,\n   the network survivability level is determined\
    \ by the policy that\n   defines the amount of shared recovery resources and by\
    \ the maximum\n   sharing degree allowed for these recovery resources.\n"
- title: 8.4.1.  Recovery Resource Sharing
  contents:
  - "8.4.1.  Recovery Resource Sharing\n   When recovery resources are shared over\
    \ several LSP/Spans, the use of\n   the Maximum Reservable Bandwidth, the Unreserved\
    \ Bandwidth, and the\n   Maximum LSP Bandwidth (see [RFC4202]) provides the information\
    \ needed\n   to obtain the optimization of the network resources allocated for\n\
    \   shared recovery purposes.\n   The Maximum Reservable Bandwidth is defined\
    \ as the Maximum Link\n   Bandwidth but it may be greater in case of link over-subscription.\n\
    \   The Unreserved Bandwidth (at priority p) is defined as the bandwidth\n   not\
    \ yet reserved on a given TE link (its initial value for each\n   priority p corresponds\
    \ to the Maximum Reservable Bandwidth).  Last,\n   the Maximum LSP Bandwidth (at\
    \ priority p) is defined as the smaller\n   of Unreserved Bandwidth (at priority\
    \ p) and Maximum Link Bandwidth.\n   Here, one generally considers a recovery\
    \ resource sharing degree (or\n   ratio) to globally optimize the shared recovery\
    \ resource usage.  The\n   distribution of the bandwidth utilization per TE link\
    \ can be inferred\n   from the per-priority bandwidth pre-allocation.  By using\
    \ the Maximum\n   LSP Bandwidth and the Maximum Reservable Bandwidth, the amount\
    \ of\n   (over-provisioned) resources that can be used for shared recovery\n \
    \  purposes is known from the IGP.\n   In order to analyze this behavior, we define\
    \ the difference between\n   the Maximum Reservable Bandwidth (in the present\
    \ case, this value is\n   greater than the Maximum Link Bandwidth) and the Maximum\
    \ LSP\n   Bandwidth per TE link i as the Maximum Shareable Bandwidth or\n   max_R[i].\
    \  Within this quantity, the amount of bandwidth currently\n   allocated for shared\
    \ recovery per TE link i is defined as R[i].  Both\n   quantities are expressed\
    \ in terms of discrete bandwidth units (and\n   thus, the Minimum LSP Bandwidth\
    \ is of one bandwidth unit).\n   The knowledge of this information available per\
    \ TE link can be\n   exploited in order to optimize the usage of the resources\
    \ allocated\n   per TE link for shared recovery.  If one refers to r[i] as the\
    \ actual\n   bandwidth per TE link i (in terms of discrete bandwidth units)\n\
    \   committed for shared recovery, then the following quantity must be\n   maximized\
    \ over the potential TE link candidates:\n        sum {i=1}^N [(R{i} - r{i})/(t{i}\
    \ - b{i})]\n        or equivalently: sum {i=1}^N [(R{i} - r{i})/r{i}]\n      \
    \  with R{i} >= 1 and r{i} >= 1 (in terms of per component\n        bandwidth\
    \ unit)\n   In this formula, N is the total number of links traversed by a given\n\
    \   LSP, t[i] the Maximum Link Bandwidth per TE link i, and b[i] the sum\n   per\
    \ TE link i of the bandwidth committed for working LSPs and other\n   recovery\
    \ LSPs (thus except \"shared bandwidth\" LSPs).  The quantity\n   [(R{i} - r{i})/r{i}]\
    \ is defined as the Shared (Recovery) Bandwidth\n   Ratio per TE link i.  In addition,\
    \ TE links for which R[i] reaches\n   max_R[i] or for which r[i] = 0 are pruned\
    \ during shared recovery path\n   computation as well as TE links for which max_R[i]\
    \ = r[i] that can\n   simply not be shared.\n   More generally, one can draw the\
    \ following mapping between the\n   available bandwidth at the transport and control\
    \ plane level:\n                                 - ---------- Max Reservable Bandwidth\n\
    \                                |  -----  ^\n                               \
    \ |R -----  |\n                                |  -----  |\n                 \
    \                - -----  |max_R\n                                   -----  |\n\
    \   --------  TE link Capacity    - ------ | - Maximum TE Link Bandwidth\n   -----\
    \                        |r -----  v\n   -----     <------ b ------>   - ----------\
    \ Maximum LSP Bandwidth\n   -----                           -----\n   -----  \
    \                         -----\n   -----                           -----\n  \
    \ -----                           -----\n   -----                           -----\
    \ <--- Minimum LSP Bandwidth\n   -------- 0                      ---------- 0\n\
    \   Note that the above approach does not require the flooding of any per\n  \
    \ LSP information or any detailed distribution of the bandwidth\n   allocation\
    \ per component link or individual ports or even any per-\n   priority shareable\
    \ recovery bandwidth information (using a dedicated\n   sub-TLV).  The latter\
    \ would provide the same capability as the\n   already defined Maximum LSP bandwidth\
    \ per-priority information.  This\n   approach is referred to as a Partial (or\
    \ Aggregated) Information\n   Routing as described in [KODIALAM1] and [KODIALAM2].\
    \  They show that\n   the difference obtained with a Full (or Complete) Information\
    \ Routing\n   approach (where for the whole set of working and recovery LSPs,\
    \ the\n   amount of bandwidth units they use per-link is known at each node and\n\
    \   for each link) is clearly negligible.  The Full Information Routing\n   approach\
    \ is detailed in [GLI].  Note also that both approaches rely\n   on the deterministic\
    \ knowledge (at different degrees) of the network\n   topology and resource usage\
    \ status.\n   Moreover, extending the GMPLS signaling capabilities can enhance\
    \ the\n   Partial Information Routing approach.  It is enhanced by allowing\n\
    \   working-LSP-related information and, in particular, its path\n   (including\
    \ link and node identifiers) to be exchanged with the\n   recovery LSP request.\
    \  This enables more efficient admission control\n   at upstream nodes of shared\
    \ recovery resources, and in particular,\n   links (see Section 8.4.3).\n"
- title: 8.4.2.  Recovery Resource Sharing and SRLG Recovery
  contents:
  - "8.4.2.  Recovery Resource Sharing and SRLG Recovery\n   Resource shareability\
    \ can also be maximized with respect to the\n   number of times each SRLG is protected\
    \ by a recovery resource (in\n   particular, a shared TE link) and methods can\
    \ be considered for\n   avoiding contention of the shared recovery resources in\
    \ case of\n   single SRLG failure.  These methods enable the sharing of recovery\n\
    \   resources between two (or more) recovery LSPs, if their respective\n   working\
    \ LSPs are mutually disjoint with respect to link, node, and\n   SRLGs.  Then,\
    \ a single failure does not simultaneously disrupt\n   several (or at least two)\
    \ working LSPs.\n   For instance, [BOUILLET] shows that the Partial Information\
    \ Routing\n   approach can be extended to cover recovery resource shareability\
    \ with\n   respect to SRLG recoverability (i.e., the number of times each SRLG\n\
    \   is recoverable).  By flooding this aggregated information per TE\n   link,\
    \ path computation and selection of SRLG-diverse recovery LSPs\n   can be optimized\
    \ with respect to the sharing of recovery resource\n   reserved on each TE link.\
    \  This yields a performance difference of\n   less than 5%, which is negligible\
    \ compared to the corresponding Full\n   Information Flooding approach (see [GLI]).\n\
    \   For this purpose, additional extensions to [RFC4202] in support of\n   path\
    \ computation for shared mesh recovery have been often considered\n   in the literature.\
    \  TE link attributes would include, among others,\n   the current number of recovery\
    \ LSPs sharing the recovery resources\n   reserved on the TE link, and the current\
    \ number of SRLGs recoverable\n   by this amount of (shared) recovery resources\
    \ reserved on the TE\n   link.  The latter is equivalent to the current number\
    \ of SRLGs that\n   will be recovered by the recovery LSPs sharing the recovery\
    \ resource\n   reserved on the TE link.  Then, if explicit SRLG recoverability\
    \ is\n   considered, a TE link attribute would be added that includes the\n  \
    \ explicit list of SRLGs (recoverable by the shared recovery resource\n   reserved\
    \ on the TE link) and their respective shareable recovery\n   bandwidths.  The\
    \ latter information is equivalent to the shareable\n   recovery bandwidth per\
    \ SRLG (or per group of SRLGs), which implies\n   that the amount of shareable\
    \ bandwidth and the number of listed SRLGs\n   will decrease over time.\n   Compared\
    \ to the case of recovery resource sharing only (regardless of\n   SRLG recoverability,\
    \ as described in Section 8.4.1), these additional\n   TE link attributes would\
    \ potentially deliver better path computation\n   and selection (at a distinct\
    \ ingress node) for shared mesh recovery\n   purposes.  However, due to the lack\
    \ of evidence of better efficiency\n   and due to the complexity that such extensions\
    \ would generate, they\n   are not further considered in the scope of the present\
    \ analysis.  For\n   instance, a per-SRLG group minimum/maximum shareable recovery\n\
    \   bandwidth is restricted by the length that the corresponding (sub-)\n   TLV\
    \ may take and thus the number of SRLGs that it can include.\n   Therefore, the\
    \ corresponding parameter should not be translated into\n   GMPLS routing (or\
    \ even signaling) protocol extensions in the form of\n   TE link sub-TLV.\n"
- title: 8.4.3.  Recovery Resource Sharing, SRLG Disjointness and Admission
  contents:
  - "8.4.3.  Recovery Resource Sharing, SRLG Disjointness and Admission\n        Control\n\
    \   Admission control is a strict requirement to be fulfilled by nodes\n   giving\
    \ access to shared links.  This can be illustrated using the\n   following network\
    \ topology:\n      A ------ C ====== D\n      |        |        |\n      |   \
    \     |        |\n      |        B        |\n      |        |        |\n     \
    \ |        |        |\n       ------- E ------ F\n   Node A creates a working\
    \ LSP to D (A-C-D), B creates simultaneously a\n   working LSP to D (B-C-D) and\
    \ a recovery LSP (B-E-F-D) to the same\n   destination.  Then, A decides to create\
    \ a recovery LSP to D (A-E-F-\n   D), but since the C-D span carries both working\
    \ LSPs, node E should\n   either assign a dedicated resource for this recovery\
    \ LSP or reject\n   this request if the C-D span has already reached its maximum\
    \ recovery\n   bandwidth sharing ratio.  In the latter case, C-D span failure\
    \ would\n   imply that one of the working LSP would not be recoverable.\n   Consequently,\
    \ node E must have the required information to perform\n   admission control for\
    \ the recovery LSP requests it processes\n   (implying for instance, that the\
    \ path followed by the working LSP is\n   carried with the corresponding recovery\
    \ LSP request).  If node E can\n   guarantee that the working LSPs (A-C-D and\
    \ B-C-D) are SRLG disjoint\n   over the C-D span, it may securely accept the incoming\
    \ recovery LSP\n   request and assign to the recovery LSPs (A-E-F-D and B-E-F-D)\
    \ the\n   same resources on the link E-F.  This may occur if the link E-F has\n\
    \   not yet reached its maximum recovery bandwidth sharing ratio.  In\n   this\
    \ example, one assumes that the node failure probability is\n   negligible compared\
    \ to the link failure probability.\n   To achieve this, the path followed by the\
    \ working LSP is transported\n   with the recovery LSP request and examined at\
    \ each upstream node of\n   potentially shareable links.  Admission control is\
    \ performed using\n   the interface identifiers (included in the path) to retrieve\
    \ in the\n   TE DataBase the list of SRLG IDs associated to each of the working\n\
    \   LSP links.  If the working LSPs (A-C-D and B-C-D) have one or more\n   link\
    \ or SRLG ID in common (in this example, one or more SRLG id in\n   common over\
    \ the span C-D), node E should not assign the same resource\n   over link E-F\
    \ to the recovery LSPs (A-E-F-D and B-E-F-D).  Otherwise,\n   one of these working\
    \ LSPs would not be recoverable if C-D span\n   failure occurred.\n   There are\
    \ some issues related to this method; the major one is the\n   number of SRLG\
    \ IDs that a single link can cover (more than 100, in\n   complex environments).\
    \  Moreover, when using link bundles, this\n   approach may generate the rejection\
    \ of some recovery LSP requests.\n   This occurs when the SRLG sub-TLV corresponding\
    \ to a link bundle\n   includes the union of the SRLG id list of all the component\
    \ links\n   belonging to this bundle (see [RFC4202] and [RFC4201]).\n   In order\
    \ to overcome this specific issue, an additional mechanism may\n   consist of\
    \ querying the nodes where the information would be\n   available (in this case,\
    \ node E would query C).  The main drawback of\n   this method is that (in addition\
    \ to the dedicated mechanism(s) it\n   requires) it may become complex when several\
    \ common nodes are\n   traversed by the working LSPs.  Therefore, when using link\
    \ bundles,\n   solving this issue is closely related to the sequence of the recovery\n\
    \   operations.  Per-component flooding of SRLG identifiers would deeply\n   impact\
    \ the scalability of the link state routing protocol.\n   Therefore, one may rely\
    \ on the usage of an on-line accessible network\n   management system.\n"
- title: 9.  Summary and Conclusions
  contents:
  - "9.  Summary and Conclusions\n   The following table summarizes the different\
    \ recovery types and\n   schemes analyzed throughout this document.\n   --------------------------------------------------------------------\n\
    \              |       Path Search (computation and selection)\n   --------------------------------------------------------------------\n\
    \              |       Pre-planned (a)      |         Dynamic (b)\n   --------------------------------------------------------------------\n\
    \          |   | faster recovery            | Does not apply\n          |   |\
    \ less flexible              |\n          | 1 | less robust                |\n\
    \          |   | most resource-consuming    |\n   Path   |   |               \
    \             |\n   Setup   ------------------------------------------------------------\n\
    \          |   | relatively fast recovery   | Does not apply\n          |   |\
    \ relatively flexible        |\n          | 2 | relatively robust          |\n\
    \          |   | resource consumption       |\n          |   |  depends on sharing\
    \ degree |\n           ------------------------------------------------------------\n\
    \          |   | relatively fast recovery   | less faster (computation)\n    \
    \      |   | more flexible              | most flexible\n          | 3 | relatively\
    \ robust          | most robust\n          |   | less resource-consuming    |\
    \ least resource-consuming\n          |   |  depends on sharing degree |\n   --------------------------------------------------------------------\n\
    \   1a. Recovery LSP setup (before failure occurrence) with resource\n       reservation\
    \ (i.e., signaling) and selection is referred to as LSP\n       protection.\n\
    \   2a. Recovery LSP setup (before failure occurrence) with resource\n       reservation\
    \ (i.e., signaling) and with resource pre-selection is\n       referred to as\
    \ pre-planned LSP re-routing with resource pre-\n       selection.  This implies\
    \ only recovery LSP activation after\n       failure occurrence.\n   3a. Recovery\
    \ LSP setup (before failure occurrence) with resource\n       reservation (i.e.,\
    \ signaling) and without resource selection is\n       referred to as pre-planned\
    \ LSP re-routing without resource pre-\n       selection.  This implies recovery\
    \ LSP activation and resource\n       (i.e., label) selection after failure occurrence.\n\
    \   3b. Recovery LSP setup after failure occurrence is referred to as to\n   \
    \    as LSP re-routing, which is full when recovery LSP path\n       computation\
    \ occurs after failure occurrence.\n   Thus, the term pre-planned refers to recovery\
    \ LSP path pre-\n   computation, signaling (reservation), and a priori resource\
    \ selection\n   (optional), but not cross-connection.  Also, the shared-mesh recovery\n\
    \   scheme can be viewed as a particular case of 2a) and 3a), using the\n   additional\
    \ constraint described in Section 8.4.3.\n   The implementation of these recovery\
    \ mechanisms requires only\n   considering extensions to GMPLS signaling protocols\
    \ (i.e., [RFC3471]\n   and [RFC3473]).  These GMPLS signaling extensions should\
    \ mainly focus\n   in delivering (1) recovery LSP pre-provisioning for the cases\
    \ 1a, 2a,\n   and 3a, (2) LSP failure notification, (3) recovery LSP switching\n\
    \   action(s), and (4) reversion mechanisms.\n   Moreover, the present analysis\
    \ (see Section 8) shows that no GMPLS\n   routing extensions are expected to efficiently\
    \ implement any of these\n   recovery types and schemes.\n"
- title: 10.  Security Considerations
  contents:
  - "10.  Security Considerations\n   This document does not introduce any additional\
    \ security issue or\n   imply any specific security consideration from [RFC3945]\
    \ to the\n   current RSVP-TE GMPLS signaling, routing protocols (OSPF-TE, IS-IS-\n\
    \   TE) or network management protocols.\n   However, the authorization of requests\
    \ for resources by GMPLS-capable\n   nodes should determine whether a given party,\
    \ presumably already\n   authenticated, has a right to access the requested resources.\
    \  This\n   determination is typically a matter of local policy control, for\n\
    \   example, by setting limits on the total bandwidth made available to\n   some\
    \ party in the presence of resource contention.  Such policies may\n   become\
    \ quite complex as the number of users, types of resources, and\n   sophistication\
    \ of authorization rules increases.  This is\n   particularly the case for recovery\
    \ schemes that assume pre-planned\n   sharing of recovery resources, or contention\
    \ for resources in case of\n   dynamic re-routing.\n   Therefore, control elements\
    \ should match the requests against the\n   local authorization policy.  These\
    \ control elements must be capable\n   of making decisions based on the identity\
    \ of the requester, as\n   verified cryptographically and/or topologically.\n"
- title: 11.  Acknowledgements
  contents:
  - "11.  Acknowledgements\n   The authors would like to thank Fabrice Poppe (Alcatel)\
    \ and Bart\n   Rousseau (Alcatel) for their revision effort, and Richard Rabbat\n\
    \   (Fujitsu Labs), David Griffith (NIST), and Lyndon Ong (Ciena) for\n   their\
    \ useful comments.\n   Thanks also to Adrian Farrel for the thorough review of\
    \ the document.\n"
- title: 12.  References
  contents:
  - '12.  References

    '
- title: 12.1.  Normative References
  contents:
  - "12.1.  Normative References\n   [RFC2119]    Bradner, S., \"Key words for use\
    \ in RFCs to Indicate\n                Requirement Levels\", BCP 14, RFC 2119,\
    \ March 1997.\n   [RFC3471]    Berger, L., \"Generalized Multi-Protocol Label\
    \ Switching\n                (GMPLS) Signaling Functional Description\", RFC 3471,\n\
    \                January 2003.\n   [RFC3473]    Berger, L., \"Generalized Multi-Protocol\
    \ Label Switching\n                (GMPLS) Signaling Resource ReserVation Protocol-Traffic\n\
    \                Engineering (RSVP-TE) Extensions\", RFC 3473, January\n     \
    \           2003.\n   [RFC3945]    Mannie, E., \"Generalized Multi-Protocol Label\
    \ Switching\n                (GMPLS) Architecture\", RFC 3945, October 2004.\n\
    \   [RFC4201]    Kompella, K., Rekhter, Y., and L. Berger, \"Link Bundling\n \
    \               in MPLS Traffic Engineering (TE)\", RFC 4201, October\n      \
    \          2005.\n   [RFC4202]    Kompella, K., Ed. and Y. Rekhter, Ed., \"Routing\n\
    \                Extensions in Support of Generalized Multi-Protocol\n       \
    \         Label Switching (GMPLS)\", RFC 4202, October 2005.\n   [RFC4204]   \
    \ Lang, J., Ed., \"Link Management Protocol (LMP)\", RFC\n                4204,\
    \ October 2005.\n   [RFC4209]    Fredette, A., Ed. and J. Lang, Ed., \"Link Management\n\
    \                Protocol (LMP) for Dense Wavelength Division\n              \
    \  Multiplexing (DWDM) Optical Line Systems\", RFC 4209,\n                October\
    \ 2005.\n   [RFC4427]    Mannie E., Ed. and D. Papadimitriou, Ed., \"Recovery\n\
    \                (Protection and Restoration) Terminology for Generalized\n  \
    \              Multi-Protocol Label Switching (GMPLS)\", RFC 4427, March\n   \
    \             2006.\n"
- title: 12.2.  Informative References
  contents:
  - "12.2.  Informative References\n   [BOUILLET]   E. Bouillet, et al., \"Stochastic\
    \ Approaches to Compute\n                Shared Meshed Restored Lightpaths in\
    \ Optical Network\n                Architectures,\" IEEE Infocom 2002, New York\
    \ City, June\n                2002.\n   [DEMEESTER]  P. Demeester, et al., \"\
    Resilience in Multilayer\n                Networks,\" IEEE Communications Magazine,\
    \ Vol. 37, No. 8,\n                pp. 70-76, August 1998.\n   [GLI]        G.\
    \ Li, et al., \"Efficient Distributed Path Selection for\n                Shared\
    \ Restoration Connections,\" IEEE Infocom 2002, New\n                York City,\
    \ June 2002.\n   [IPO-IMP]    Strand, J. and A. Chiu, \"Impairments and Other\n\
    \                Constraints on Optical Layer Routing\", RFC 4054, May\n     \
    \           2005.\n   [KODIALAM1]  M. Kodialam and T.V. Lakshman, \"Restorable\
    \ Dynamic\n                Quality of Service Routing,\" IEEE Communications\n\
    \                Magazine, pp. 72-81, June 2002.\n   [KODIALAM2]  M. Kodialam\
    \ and T.V. Lakshman, \"Dynamic Routing of\n                Restorable Bandwidth-Guaranteed\
    \ Tunnels using Aggregated\n                Network Resource Usage Information,\"\
    \ IEEE/ ACM\n                Transactions on Networking, pp. 399-410, June 2003.\n\
    \   [MANCHESTER] J. Manchester, P. Bonenfant and C. Newton, \"The\n          \
    \      Evolution of Transport Network Survivability,\" IEEE\n                Communications\
    \ Magazine, August 1999.\n   [RFC3386]    Lai, W. and D. McDysan, \"Network Hierarchy\
    \ and\n                Multilayer Survivability\", RFC 3386, November 2002.\n\
    \   [T1.105]     ANSI, \"Synchronous Optical Network (SONET): Basic\n        \
    \        Description Including Multiplex Structure, Rates, and\n             \
    \   Formats,\" ANSI T1.105, January 2001.\n   [WANG]       J. Wang, L. Sahasrabuddhe,\
    \ and B. Mukherjee, \"Path vs.\n                Subpath vs. Link Restoration for\
    \ Fault Management in\n                IP-over-WDM Networks: Performance Comparisons\
    \ Using\n                GMPLS Control Signaling,\" IEEE Communications Magazine,\n\
    \                pp. 80-87, November 2002.\n   For information on the availability\
    \ of the following documents,\n   please see http://www.itu.int\n   [G.707]  \
    \    ITU-T, \"Network Node Interface for the Synchronous\n                Digital\
    \ Hierarchy (SDH),\" Recommendation G.707, October\n                2000.\n  \
    \ [G.709]      ITU-T, \"Network Node Interface for the Optical Transport\n   \
    \             Network (OTN),\" Recommendation G.709, February 2001 (and\n    \
    \            Amendment no.1, October 2001).\n   [G.783]      ITU-T, \"Characteristics\
    \ of Synchronous Digital Hierarchy\n                (SDH) Equipment Functional\
    \ Blocks,\" Recommendation\n                G.783, October 2000.\n   [G.798] \
    \     ITU-T, \"Characteristics of optical transport network\n                hierarchy\
    \ equipment functional block,\" Recommendation\n                G.798, June 2004.\n\
    \   [G.806]      ITU-T, \"Characteristics of Transport Equipment -\n         \
    \       Description Methodology and Generic Functionality\",\n               \
    \ Recommendation G.806, October 2000.\n   [G.841]      ITU-T, \"Types and Characteristics\
    \ of SDH Network\n                Protection Architectures,\" Recommendation G.841,\
    \ October\n                1998.\n   [G.842]      ITU-T, \"Interworking of SDH\
    \ network protection\n                architectures,\" Recommendation G.842, October\
    \ 1998.\n   [G.874]      ITU-T, \"Management aspects of the optical transport\n\
    \                network element,\" Recommendation G.874, November 2001.\n"
- title: Editors' Addresses
  contents:
  - "Editors' Addresses\n   Dimitri Papadimitriou\n   Alcatel\n   Francis Wellesplein,\
    \ 1\n   B-2018 Antwerpen, Belgium\n   Phone:  +32 3 240-8491\n   EMail: dimitri.papadimitriou@alcatel.be\n\
    \   Eric Mannie\n   Perceval\n   Rue Tenbosch, 9\n   1000 Brussels\n   Belgium\n\
    \   Phone: +32-2-6409194\n   EMail: eric.mannie@perceval.net\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The Internet Society (2006).\n   This\
    \ document is subject to the rights, licenses and restrictions\n   contained in\
    \ BCP 78, and except as set forth therein, the authors\n   retain all their rights.\n\
    \   This document and the information contained herein are provided on an\n  \
    \ \"AS IS\" basis and THE CONTRIBUTOR, THE ORGANIZATION HE/SHE REPRESENTS\n  \
    \ OR IS SPONSORED BY (IF ANY), THE INTERNET SOCIETY AND THE INTERNET\n   ENGINEERING\
    \ TASK FORCE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED,\n   INCLUDING BUT NOT\
    \ LIMITED TO ANY WARRANTY THAT THE USE OF THE\n   INFORMATION HEREIN WILL NOT\
    \ INFRINGE ANY RIGHTS OR ANY IMPLIED\n   WARRANTIES OF MERCHANTABILITY OR FITNESS\
    \ FOR A PARTICULAR PURPOSE.\n"
- title: Intellectual Property
  contents:
  - "Intellectual Property\n   The IETF takes no position regarding the validity or\
    \ scope of any\n   Intellectual Property Rights or other rights that might be\
    \ claimed to\n   pertain to the implementation or use of the technology described\
    \ in\n   this document or the extent to which any license under such rights\n\
    \   might or might not be available; nor does it represent that it has\n   made\
    \ any independent effort to identify any such rights.  Information\n   on the\
    \ procedures with respect to rights in RFC documents can be\n   found in BCP 78\
    \ and BCP 79.\n   Copies of IPR disclosures made to the IETF Secretariat and any\n\
    \   assurances of licenses to be made available, or the result of an\n   attempt\
    \ made to obtain a general license or permission for the use of\n   such proprietary\
    \ rights by implementers or users of this\n   specification can be obtained from\
    \ the IETF on-line IPR repository at\n   http://www.ietf.org/ipr.\n   The IETF\
    \ invites any interested party to bring to its attention any\n   copyrights, patents\
    \ or patent applications, or other proprietary\n   rights that may cover technology\
    \ that may be required to implement\n   this standard.  Please address the information\
    \ to the IETF at\n   ietf-ipr@ietf.org.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is provided by the IETF\n\
    \   Administrative Support Activity (IASA).\n"
