- title: __initial_text__
  contents:
  - '      A Queuing Algorithm to Provide Type-of-Service for IP Links

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo is intended to explore how Type-of-Service\
    \ might be\n   implemented in the Internet.  The proposal describes a method of\n\
    \   queuing which can provide the different classes of service.  The\n   technique\
    \ also prohibits one class of service from consuming\n   excessive resources or\
    \ excluding other classes of service.  This is\n   an \"idea paper\" and discussion\
    \ is strongly encouraged.  Distribution\n   of this memo is unlimited.\n"
- title: Introduction
  contents:
  - "Introduction\n   The Type-of-Service (TOS) field in IP headers allows one to\
    \ chose\n   from none to all the following service types; low delay, high\n  \
    \ throughput, and high reliability.  It also has a portion allowing a\n   priority\
    \ selection from 0-7.  To date, there is nothing describing\n   what should be\
    \ done with these parameters.  This discussion proposes\n   an approach to providing\
    \ the different classes of service and\n   priorities requestable in the TOS field.\n"
- title: Desired Attributes
  contents:
  - "Desired Attributes\n   We should first consider how we want these services to\
    \ perform.  We\n   must first assume that there is a demand for service that exceeds\n\
    \   current capabilities.  If not, significant queues do not form and\n   queuing\
    \ algorithms become superfluous.\n   The low delay class of service should have\
    \ the ability to pass data\n   through the net faster than regular data.  If a\
    \ request is for low\n   delay class of service only, not high throughput or high\
    \ reliability,\n   the Internet should provide low delay for relatively less throughput,\n\
    \   with less than high reliability.  The requester is more concerned\n   with\
    \ promptness of delivery than guaranteed delivery.  The Internet\n   should provide\
    \ a Maximum Guaranteed Delay (MGD) per node, or better,\n   if the datagram successfully\
    \ traverses the Internet.  In the worst\n   case, a datagram's arrival will be\
    \ MGD times the number of nodes\n   traversed.  A node is any packet switching\
    \ element, including IP\n   gateways and ARPANET IMP's.  The MGD bound will not\
    \ be affected by\n   the amount of traffic in the net.  During non-busy hours,\
    \ the delay\n   provided should be better than the guarantee.  If the delay a\n\
    \   satellite link introduces is less than the MGD, that link should be\n   considered\
    \ in the route.  If however, the MGD is less than the\n   satellite link can provide,\
    \ it should not be used.  For this\n   discussion it is assumed that delay for\
    \ individual links are low\n   enough that a sending node can provide the MGD\
    \ service.\n   Low delay class of service is not the same as low Round Trip Time\n\
    \   (RTT).  Class of service is unidirectional.  The datagrams responding\n  \
    \ to low delay traffic (i.e., Acking the data) might be sent with a\n   high reliability\
    \ class of service, but not low delay.\n   The performance of TCP might be significantly\
    \ improved with an\n   accurate estimate of the round trip time and the retransmission\n\
    \   timeout.  The TCP retransmission timeout could be set to the maximum\n   delay\
    \ for the current route (if the current route could be\n   determined).  The timeout\
    \ value would have to be redetermined when\n   the number of hops in the route\
    \ changes.\n   High throughput class of service should get a large volume of data\n\
    \   through the Internet.  Requesters of this class are less concerned\n   with\
    \ the delay the datagrams have crossing the Internet and the\n   reliability of\
    \ their delivery.  This type of traffic might be served\n   well by a satellite\
    \ link, especially if the bandwidth is high.\n   Another attribute this class\
    \ might have is consistent one way\n   traversal time for a given burst of datagrams.\
    \  This class of service\n   will have its traversal times affected by the amount\
    \ of Internet\n   load.  As the Internet load goes up, the throughput for each\
    \ source\n   will go down.\n   High reliability class of service should see most\
    \ of its datagrams\n   delivered if the Internet is not too heavily loaded.  Source\
    \ Quenches\n   (SQ) should not be sent only when datagrams are discarded.  SQs\n\
    \   should be sent well before the queues become full, to advise the\n   sender\
    \ of the rate that can be currently supported.\n   Priority service should allow\
    \ data that has a higher priority to be\n   queued ahead of other lower priority\
    \ data.  It is important to limit\n   the amount of priority data.  The amount\
    \ of preemption a lower\n   priority datagram suffers must also be limited.\n\
    \   It is assumed that a queuing algorithm provides these classes of\n   service.\
    \  For one facility to be used over another, that is, making\n   different routing\
    \ decisions based upon the TOS, requires a more\n   sophisticated routing algorithm\
    \ and larger routing database.  These\n   issues are not discussed in this document.\n"
- title: Applications for Class of Service
  contents:
  - "Applications for Class of Service\n   The following are examples of how classes\
    \ of service might be used.\n   They do not necessarily represent the best choices,\
    \ but are presented\n   only to illustrate how the different classes of service\
    \ might be used\n   to advantage.\n   Interactive timesharing access using a line-at-a-time\
    \ or character-\n   at-a-time terminal (TTY) type of access is typically low volume\n\
    \   typing speed input with low or high volume output.  Some Internet\n   applications\
    \ use echoplex or character by character echoing of user\n   input by the destination\
    \ host.  PC devices also have local files that\n   may be uploaded to remote hosts\
    \ in a streaming mode.  Supporting such\n   traffic can require several types\
    \ of service.  User keyboard input\n   should be forwarded with low delay.  If\
    \ echoplex is used, all user\n   characters sent and echoed should be low delay\
    \ to minimize the\n   echoing delay.  The computer responses should be regular\
    \ or high\n   throughput depending upon the volume of data sent and the speed\
    \ of\n   the output device.  If the computer response is a single datagram of\n\
    \   data, the user should get low delay for the response, to minimize the\n  \
    \ human/computer interaction time.  If however the output takes a while\n   to\
    \ read and digest, low delay computer responses are a waste of\n   Internet resources.\
    \  When streaming input is being sent the data\n   should be sent requesting high\
    \ throughput or regular class of\n   service.\n   The IBM 3270 class of terminals\
    \ typically have traffic volumes\n   greater than TTY access.  Echoplex is not\
    \ needed.  The output devices\n   usually handle higher speed output streams and\
    \ most sites do not have\n   the ability to stream input.  Input is typically\
    \ a screen at a time,\n   but some PC implementations of 3270 use a variation\
    \ of the protocol\n   to effectively stream in volumes of data.  Low delay for\
    \ low volume\n   input and output is appropriate.  High throughput is appropriate\
    \ for\n   the higher volume traffic.\n   Applications that transfer high volumes\
    \ of data are typically\n   streaming in one direction only, with acks for the\
    \ data, on the\n   return path.  The data transfer should be high throughput and\
    \ the\n   acks should probably be regular class of service.  Transfer\n   initiation\
    \ and termination might be served best with low delay class\n   of service.\n\
    \   Requests to, and responses from a time service might use low delay\n   class\
    \ of service effectively.\n   These suggestions for class of service usage implies\
    \ that the\n   application sets the service based on the knowledge it has during\
    \ the\n   session.  Thus, the application should have control of this setting\n\
    \   dynamically for each send data request, not just on a per\n   session/conversation/transaction\
    \ basis.  It would be possible for the\n   transport level protocol to guess (i.e.,\
    \ TCP), but it would be sub-\n   optimal.\n"
- title: Algorithm
  contents:
  - "Algorithm\n   When we provide class of service queuing, one class may be more\n\
    \   desirable than the others.  We must limit the amount of resources\n   each\
    \ class consumes when there is contention, so the other classes\n   may also operate\
    \ effectively.  To be fair, the algorithm provides the\n   requested service by\
    \ reducing the other service attributes.  A\n   request for multiple classes of\
    \ service is an OR type of request not\n   an AND request.  For example, one can\
    \ not get low delay and high\n   throughput unless there is no contention for\
    \ the available resources.\n"
- title: Low Delay Queuing
  contents:
  - "Low Delay Queuing\n   To support low delay, use a limited queue so requests will\
    \ not wait\n   longer than the MGD on the queue.  The low delay queue should be\n\
    \   serviced at a lower rate than other classes of service, so low delay\n   requests\
    \ will not consume excessive resources.  If the number of low\n   delay datagrams\
    \ exceeds the queue limit, discard the datagrams.  The\n   service rate should\
    \ be low enough so that other data can still get\n   through. (See discussion\
    \ of service rates below.)  Make the queue\n   limit small enough so that, if\
    \ the datagram is queued, it will have a\n   guaranteed transit time (MGD).  It\
    \ seems unlikely that Source Quench\n   flow control mechanisms will be an effective\
    \ method of flow control\n   because of the small size of the queue.  It should\
    \ not be done for\n   this class of service.  Instead, datagrams should just be\
    \ discarded\n   as required.  If the bandwidth or percentage allocated to low\
    \ delay\n   is such that a large queue is possible (see formula below), SQs\n\
    \   should be reconsidered.\n   The maximum delay a datagram with low delay class\
    \ of service will\n   experience (MGD), can be determined with the following information:\n\
    \      N = Queue size for low delay queue\n      P = Percentage of link resources\
    \ allocated to low delay\n      R = Link rate (in datagrams/sec.)\n          \
    \            N\n      Max Delay =   -----\n                    P * R\n   If Max\
    \ Delay is held fixed, then as P and R go up, so does N.  It is\n   probable that\
    \ low delay service datagrams will prove to be, on the\n   average, smaller than\
    \ other traffic.  This means that the number of\n   datagrams that can be sent\
    \ in the allocated bandwidth can be larger.\n"
- title: High Reliability Queuing
  contents:
  - "High Reliability Queuing\n   To support high reliability class of service, use\
    \ a queue that is\n   longer than normal (longer queue means higher potential\
    \ delay).  Send\n   SQ earlier (smaller percentage of max queue length) and don't\
    \ discard\n   datagrams until the queue is full.  This queue should have a lower\n\
    \   service rate than high throughput class of service.\n   Users of this class\
    \ of service should specify a Time-to-Live (TTL)\n   which is made appropriately\
    \ longer so that it will survive longer\n   queueing times for this class of service.\n\
    \   This queuing procedure will only be effective for Internet\n   unreliability\
    \ due to congestion.  Other Internet unreliability\n   problems such as high error\
    \ rate links or reliability features such\n   as forward error correcting modems\
    \ must be dealt with by more\n   sophisticated routing algorithms.\n"
- title: High Throughput Queuing
  contents:
  - "High Throughput Queuing\n   To support high throughput class of service have\
    \ a queue that is\n   treated like current IP queuing.  It should have the highest\
    \ service\n   rate.  It will experience higher average through node delay than\
    \ low\n   delay because of the larger queue size.\n   Another thing that might\
    \ be done, is to keep datagrams of the same\n   burst together when possible.\
    \  This must be done in a way that will\n   not block other traffic.  The idea\
    \ is to deliver all the data to the\n   other end in a contiguous burst.  This\
    \ could be an advantage by\n   allowing piggybacking acks for the whole burst\
    \ at one time.  This\n   makes some assumptions about the overlying protocol which\
    \ may be\n   inappropriate.\n"
- title: Regular Service Queuing
  contents:
  - "Regular Service Queuing\n   For datagrams which request none of the three classes\
    \ of service,\n   queue the datagrams on the queue representing the least delay\
    \ between\n   the two queues, the high throughput queue or the high reliability\n\
    \   queue.  If one queue becomes full, queue on the other.  If both\n   queues\
    \ are full, follow the source quench procedure for regular class\n   of service\
    \ (see RFC-1016), not the procedure for the queue the\n   datagram failed to attain.\n\
    \   In the discussion of service rates described below, it is proposed\n   that\
    \ the high throughput queue get service three times for every two\n   times for\
    \ the high reliability queue.  Therefore, the queue length of\n   the high reliability\
    \ queue should be increased by 50% (in this\n   example) to compare the lengths\
    \ of the two queues more accurately.  A\n   simplification to this method is to\
    \ just queue new data on the queue\n   that is the shortest.  The slower service\
    \ rate queue will quickly\n   exceed the size of the faster service rate queue\
    \ and new data will go\n   on the proper queue.  This however, would lead to more\
    \ packet\n   reordering than the first method.\n"
- title: Service Rates
  contents:
  - "Service Rates\n   In this discussion, a higher service rate means that a queue,\
    \ when\n   non-empty, will consume a larger percentage of the available\n   bandwidth\
    \ than a lower service rate queue.  It will not block a lower\n   service rate\
    \ queue even if it is always full.\n   For example, the service pattern could\
    \ be; send low delay 17% of the\n   time, high throughput 50% of the time, and\
    \ high reliability 33% of\n   the time.  Throughput requires the most bandwidth\
    \ and high\n   reliability requires medium bandwidth.  One could achieve this\
    \ split\n   using a pattern of L, R,R, T,T,T, where low delay is \"L\", high\n\
    \   reliability is \"R\", and high throughput is \"T'.  We want to keep the\n\
    \   high throughput datagrams together.  We therefore send all of the\n   high\
    \ throughput data at one time, that is, not interspersed with the\n   other classes\
    \ of service.  By keeping all of the high throughput data\n   together, we may\
    \ help higher level protocols, such as TCP, as\n   described above.  This would\
    \ still be done in a way to not exceed the\n   allowed service rate of the available\
    \ bandwidth.\n   These service rates are suggestions.  Some simplifications can\
    \ be\n   considered, such as having only two routing classes; low delay, and\n\
    \   other.\n"
- title: Priority
  contents:
  - "Priority\n   There is the ability to select 8 levels of priority 0-7, in addition\n\
    \   to the class of service selected.  To provide this without blocking\n   the\
    \ least priority requests, we must give preempted datagrams\n   frustration points\
    \ every time a higher priority request cuts in line\n   in front of it.  Thus\
    \ if a datagram with low priority waits, it will\n   always get through even when\
    \ competing against the highest priority\n   requests.  This assumes the TTL (Time-to-Live)\
    \ field does not expire.\n   When a datagram with priority arrives at a node,\
    \ the node will queue\n   the datagram on the appropriate queue ahead of all datagrams\
    \ with\n   lower priority.  Each datagram that was preempted gets its priority\n\
    \   raised (locally).  The priority data will not bump a lower priority\n   datagram\
    \ off its queue, discarding the data.  If the queue is full,\n   the newest data\
    \ (priority or not) will be discarded.  The priority\n   preemption will preempt\
    \ only within the class of service queue to\n   which the priority data is targeted.\
    \  A request specifying regular\n   class of service, will contend on the queue\
    \ where it is placed, high\n   throughput or high reliability.\n   An implementation\
    \ strategy is to multiply the requested priority by 2\n   or 4, then store the\
    \ value in a buffer overhead area.  Each time the\n   datagram is preempted, increment\
    \ the value by one.  Looking at an\n   example, assume we use a multiplier of\
    \ 2.  A priority 6 buffer will\n   have an initial local value of 12.  A new priority\
    \ 7 datagram would\n   have a local value of 14.  If 2 priority 7 datagrams arrive,\n\
    \   preempting the priority 6 datagram, its local value is incremented to\n  \
    \ 14.  It can no longer be preempted.  After that, it has the same\n   local value\
    \ as a priority 7 datagram and will no longer be preempted\n   within this node.\
    \  In our example, this means that a priority 0\n   datagram can be preempted\
    \ by no more than 14 higher priority\n   datagrams.  The priority is raised only\
    \ locally in the node.  The\n   datagram could again be preempted in the next\
    \ node on the route.\n   Priority queuing changes the effects we were obtaining\
    \ with the low\n   delay queuing described above.  Once a buffer was queued, the\
    \ delay\n   that a datagram would see could be determined.  When we accepted low\n\
    \   delay data, we could guarantee a certain maximum delay.  With this\n   addition,\
    \ if the datagram requesting low delay does not also request\n   high priority,\
    \ the guaranteed delay can vary a lot more.  It could be\n   1 up to 28 times\
    \ as much as without priority queuing.\n"
- title: Discussion and Details
  contents:
  - "Discussion and Details\n   If a low delay queue is for a satellite link (or any\
    \ high delay\n   link), the max queue size should be reduced by the number of\n\
    \   datagrams that can be forwarded from the queue during the one way\n   delay\
    \ for the link.  That is, if the service rate for the low delay\n   queue is L\
    \ datagrams per second, the delay added by the high delay\n   link is D seconds\
    \ and M is the max delay per node allowed (MGD) in\n   seconds, then the maximum\
    \ queue size should be:\n         Max Queue Size = L ( M - D),  M > D\n      \
    \                  = 0         ,  M <= D\n   If the result is negative (M is less\
    \ than the delay introduced by the\n   link), then the maximum queue size should\
    \ be zero because the link\n   could never provide a delay less than the guaranteed\
    \ M value.  If the\n   bandwidth is high (as in T1 links), the delay introduced\
    \ by a\n   terrestrial link and the terminating equipment could be significant\n\
    \   and greater than the average service time for a single datagram on\n   the\
    \ low delay queue.  If so, this formula should be used to reduce\n   the queue\
    \ size as well.  Note that this is reducing the queue size\n   and is not the\
    \ same as the allocated bandwidth.  Even though the\n   queue size is reduced,\
    \ the chit scheme described below will give low\n   delay requesters a chance\
    \ to use the allocated bandwidth.\n   If a datagram requests multiple classes\
    \ of service, only one class\n   can be provided.  For example, when both low\
    \ delay and high\n   reliability classes are requested, and if the low delay queue\
    \ is\n   full, queue the data on the high reliability queue instead.  If we\n\
    \   are able to queue the data on the low delay queue, then the datagram\n   gets\
    \ part of the high reliability service it also requested, because,\n   once data\
    \ is queued, data will not be discarded.  However, the\n   datagram will be routed\
    \ as a low delay request.  The same scheme is\n   used for any other combinations\
    \ of service requested.  The order of\n   selection for classes of service when\
    \ more than one is requested\n   would be low delay, high throughput, then high\
    \ reliability.  If a\n   block of datagrams request multiple classes of service,\
    \ it is quite\n   possible that datagram reordering will occur.  If one queue\
    \ is full\n   causing the other queue to be used for some of the data, data will\
    \ be\n   forwarded at different service rates.  Requesting multiple classes of\n\
    \   service gives the data a better chance of making it through the net\n   because\
    \ they have multiple chances of getting on a service queue.\n   However, the datagrams\
    \ pay the penalty of possible reordering and\n   more variability in the one way\
    \ transmission times.\n   Besides total buffer consumption, individual class of\
    \ service queue\n   sizes should be used to SQ those asking for service except\
    \ as noted\n   above.\n   A request for regular class of service is handled by\
    \ queuing to the\n   high reliability or high throughput queues evenly (proportional\
    \ to\n   the service rates of queue).  The low delay queue should only receive\n\
    \   data with the low delay service type.  Its queue is too small to\n   accept\
    \ other traffic.\n   Because of the small queue size for low delay suggested above,\
    \ it is\n   difficult for low delay service requests to consume the bandwidth\n\
    \   allocated.  To do so, low delay users must keep the small queue\n   continuously\
    \ non-empty.  This is hard to do with a small queue.\n   Traffic flow has been\
    \ shown to be bursty in nature.  In order for the\n   low delay queue to be able\
    \ to consume the allocated bandwidth, a\n   count of the various types being forwarded\
    \ should be kept.  The\n   service rate should increase if the actual percentage\
    \ falls too low\n   for the low delay queue.  The measure of service rates would\
    \ have to\n   be smoothed over time.\n   While this does sound complicated, a\
    \ reasonably efficient way can be\n   described.  Every Q seconds, where Q is\
    \ less than or equal to the\n   MGD, each class gets N M P chits proportional\
    \ to their allowed\n   percentage.  Send data for the low delay queue up to the\
    \ number of\n   chits it receives decrementing the chits as datagrams are sent.\
    \  Next\n   send from the high reliability queue as many as it has chits for.\n\
    \   Finally, send from the high throughput queue.  At this point, each\n   queue\
    \ gets N M P chits again.  If the low delay queue does not\n   consume all of\
    \ its chits, when a low delay datagram arrives, before\n   chit replenishment,\
    \ send from the low delay queue immediately.  This\n   provides some smoothing\
    \ of the actual bandwidth made available for\n   low delay traffic.  If operational\
    \ experience shows that low delay\n   requests are experiencing excessive congestion\
    \ loss but still not\n   consuming the classes allocated bandwidth, adjustments\
    \ should be\n   made.  The service rates should be made larger and the queue sizes\n\
    \   adjusted accordingly.  This is more important on lower speed links\n   where\
    \ the above formula makes the queue small.\n   What we should see during the Q\
    \ seconds is that low delay data will\n   be sent as soon as possible (as long\
    \ as the volume is below the\n   allowed percentage).  Also, the tendency will\
    \ be to send all the high\n   throughput datagrams contiguously.  This will give\
    \ a more regular\n   measured round trip time for bursts of datagrams.  Classes\
    \ of service\n   will tend to be grouped together at each intermediate node in\
    \ the\n   route.  If all of the queues with datagrams have consumed all of\n \
    \  their allocated chits, but one or more classes with empty queues have\n   unused\
    \ chits then a percentage of these left over chits should be\n   carried over.\
    \  Divide the remaining chit counts by two (with round\n   down), then add in\
    \ the refresh chit counts.  This allows a 50% carry\n   over for the next interval.\
    \  The carry over is self limiting to less\n   than or equal to the refresh chit\
    \ count.  This prevents excessive\n   build up.  It provides some smoothing of\
    \ the percentage allocation\n   over time but will not allow an unused queue to\
    \ build up chits\n   indefinitely.  No timer is required.\n   If only a simple\
    \ subset of the described algorithm is to be\n   implemented, then low delay queuing\
    \ would be the best choice.  One\n   should use a small queue.  Service the queue\
    \ with a high service rate\n   but restrict the bandwidth to a small reasonable\
    \ percentage of the\n   available bandwidth.  Currently, wide area networks with\
    \ high traffic\n   volumes do not provide low delay service unless low delay requests\n\
    \   are able to preempt other traffic.\n"
- title: Applicability
  contents:
  - "Applicability\n   When the output speed and volume match the input speed and\
    \ volume,\n   queues don't get large.  If the queues never grow large enough to\n\
    \   exceed the guaranteed low delay performance, no queuing algorithm\n   other\
    \ than first in, first out, should be used.\n   The algorithm could be turned\
    \ on when the main queue size exceeds a\n   certain threshold.  The routing node\
    \ can periodically check for queue\n   build up.  This queuing algorithm can be\
    \ turned on when the maximum\n   delays will exceed the allowed nodal delay for\
    \ low delay class of\n   service.  It can also be turned off when queue sizes\
    \ are no longer a\n   problem.\n"
- title: Issues
  contents:
  - "Issues\n   Several issues need to be addressed before type of service queuing\
    \ as\n   described should be implemented.  What percentage of the bandwidth\n\
    \   should each class of service consume assuming an infinite supply of\n   each\
    \ class of service datagrams?  What maximum delay (MGD) should be\n   guaranteed\
    \ per node for low delay datagrams?\n   It is possible to provide a more optimal\
    \ route if the queue sizes for\n   each class of service are considered in the\
    \ routing decision.  This,\n   however, adds additional overhead and complexity\
    \ to each routing\n   node.  This may be an unacceptable additional complexity.\n\
    \   How are we going to limit the use of more desirable classes of\n   service\
    \ and higher priorities?  The algorithm limits use of the\n   various classes\
    \ by restricting queue sizes especially the low delay\n   queue size.  This helps\
    \ but it seems likely we will want to\n   instrument the number of datagrams requesting\
    \ each Type-of-Service\n   and priority.  When a datagram requests multiple classes\
    \ of service,\n   increment the instrumentation count once based upon the queue\n\
    \   actually used, selecting, low delay, high throughput, high\n   reliability,\
    \ then regular.  If instrumentation reveals an excessive\n   imbalance, Internet\
    \ operations can give this to administrators to\n   handle.  This instrumentation\
    \ will show the distribution for types of\n   service requested by the Internet\
    \ users.  This information can be\n   used to tune the Internet to service the\
    \ user demands.\n   Will the routing algorithms in use today have problems when\
    \ routing\n   data with this algorithm?  Simulation tests need to be done to model\n\
    \   how the Internet will react.  If, for example, an application\n   requests\
    \ multiple classes of service, round trip times may fluctuate\n   significantly.\
    \  Would TCP have to be more sophisticated in its round\n   trip time estimator?\n\
    \   An objection to this type of queuing algorithm is that it is making\n   the\
    \ routing and queuing more complicated.  There is current interest\n   in high\
    \ speed packet switches which have very little protocol\n   overhead when handling/routing\
    \ packets.  This algorithm complicates\n   not simplifies the protocol.  The bandwidth\
    \ being made available is\n   increasing.  More T1 (1.5 Mbps) and higher speed\
    \ links are being used\n   all the time.  However, in the history of communications,\
    \ it seems\n   that the demand for bandwidth has always exceeded the supply. \
    \ When\n   there is wide spread use of optical fiber we may temporarily\n   experience\
    \ a glut of capacity.  As soon as 1 gigabit optical fiber\n   link becomes reasonably\
    \ priced, new applications will be created to\n   consume it all.  A single full\
    \ motion high resolution color image\n   system can consume, as an upper limit,\
    \ nearly a gigabit per second\n   channel (30 fps X 24 b/pixel X 1024 X 1024 pixels).\n\
    \   In the study of one gateway, Dave Clark discovered that the per\n   datagram\
    \ processing of the IP header constituted about 20% of the\n   processing time.\
    \  Much of the time per datagram was spent on\n   restarting input, starting output\
    \ and queuing datagrams.  He thought\n   that a small additional amount of processing\
    \ to support Type-of-\n   Service would be reasonable.  He suggests that even\
    \ if the code does\n   slow the gateway down, we need to see if TOS is good for\
    \ anything, so\n   this experiment is valuable.  To support the new high speed\n\
    \   communications of the near future, Dave wants to see switches which\n   will\
    \ run one to two orders of magnitude faster.  This can not be done\n   by trimming\
    \ a few instructions here or there.\n   From a practical perspective, the problem\
    \ this algorithm is trying to\n   solve is the lack of low delay service through\
    \ the Internet today.\n   Implementing only the low delay queuing portion of this\
    \ algorithm\n   would allow the Internet to provide a class of service it otherwise\n\
    \   could not provide.  Requesters of this class of service would not get\n  \
    \ it for free.  Low delay class of datagram streams get low delay at\n   the cost\
    \ of reliability and throughput.\n"
