- title: __initial_text__
  contents:
  - '                     Speedup of Host-IMP Interface

    '
- title: I. Introduction
  contents:
  - "I. Introduction\n   In order to make the full performance capabilities of the\
    \ subnet\n   available for interprocess communication, the host's IMP interface\n\
    \   and the IMP's host interface should operate at the highest speed\n   obtainable.\n\
    \   First, this high throughput will minimize the latency observed when\n   RFNM's,\
    \ control messages, and NVT (network virtual terminal)\n   characters are queued\
    \ behind full sized messages.  A full-sized\n   message currently ties up a 100\
    \ kb interface for almost 100 Msec.\n   delaying short messages behind it by 100\
    \ Msec.  Speeding up the host\n   interface to 300 kilobaud will shrink this latency\
    \ to 30 Msec.\n   Secondly, this high-speed operation minimizes the time that\
    \ the IMP\n   buffer and the host core buffer are locked down during message\n\
    \   transfer. (One being emptied, one being filled).  Being able to\n   dispose\
    \ of buffers far faster means that many fewer of them will\n   suffice to carry\
    \ the communications traffic; each buffer can be\n   reused far more often.\n\
    \   Third, high-speed operation makes it possible to improve error\n   control:\
    \  currently, a destination IMP returns an RFNM after\n   transmitting the first\
    \ packet of a multipacket message to the\n   destination host.  If an error occurs\
    \ during the transmission of the\n   (up to seven) other packets into the destination\
    \ host, the source\n   host will not be informed of the error: it has already\
    \ been given a\n   positive message acknowledgement in the RFNM.  The alternative,\n\
    \   holding off the RFNM until all packets have been transmitted into the\n  \
    \ destination host, would add another 80 Msec. to the round trip\n   message -\
    \ RFNM time with the current 100 kilobaud interface.  A\n   higher speed interface\
    \ will reduce this delayed - RFNM cost to a more\n   acceptable value, making\
    \ it practical to eliminate this source of\n   undetected message transmission\
    \ errors.\n   Fourth, a high speed interface will permit greater host\n   communications\
    \ bandwidth. (Currently limited to 100 kilobaud).  This\n   increase in bandwidth\
    \ will be essential for communications between\n   hosts at a \"network-structured\"\
    \ site, where different hosts on the\n   same IMP are specialized to perform different\
    \ parts of a computation.\n   Clearly, any new or retrofitted host interfaces\
    \ should be very high\n   speed, and existing host interfaces should be adjusted\
    \ to operate at\n   their maximum speed, which is in excess of 300 kilobaud.\n"
- title: II.  Experimental Results
  contents:
  - "II.  Experimental Results\n   In support of the above predictions, the BBN TENEX\
    \ staff performed an\n   experiment in cooperation with the BBN IMP group to determine\
    \ how\n   fast the System A (BBN-TENEX) and System B (BBNB) distant interfaces\n\
    \   would operate.\n   Results are as follows:\n   The Host-to-IMP connection\
    \ is synchronized by a two-way handshake\n   which has an available burst bandwidth\
    \ of 1 bit/(2225 nsec + 3\n   nsec/ft.*<cable length>ft) For our cable length,\
    \ this results in a\n   bandwidth of 310 kilobaud.\n   The IMP-to-Host connection\
    \ is synchronized by a four-way handshake\n   which has an available burst bandwidth\
    \ of 1 bit/(1350 nsec + 6\n   nsec/ft.*<cable length>ft.)  which results in a\
    \ bandwidth of 290\n   kilobaud for our installation.\n   Both System A and System\
    \ B are now operating at this higher interface\n   speed.\n   Since the propogation\
    \ delay time through a distant host driver-\n   receiver pair amounts to 250 nsec,\
    \ it is expected that local host\n   interfaces (<30ft) can be operated at speeds\
    \ substantially faster\n   than our 300 kilobaud.\n   In addition to the above\
    \ measurements of hardware speed, new results\n   were obtained in measurements\
    \ of file transfer performance, i.e. the\n   CPU time and real time used per megabit\
    \ of information transmitted\n   over the network.\n   This experiment involved\
    \ the movement of one-megabit data files to\n   and from an FTP User process in\
    \ System B communicating with the FTP\n   Server Process in System A.  The results\
    \ are summarized in the\n   followiing table:\n   Operation  Byte Size    Type\
    \        Bandwidth       User CPU seconds/\n                                 \
    \                          megabit\n   Get           8         ASCII       47Kbaud\
    \              7.9\n   Send          8         ASCII       50Kbaud           \
    \   7.9\n   Get           32        LocalByte   43Kbaud              1.80\n  \
    \ Send          32        LocalByte   38Kbaud              1.70\n   Get      \
    \     36        Image       79Kbaud              1.85\n   Send          36   \
    \     Image       85Kbaud               .95\n   The 36-bit bandwidth of around\
    \ 80Kbaud is a great improvement from\n   the (typically 25Kbaud measured before\
    \ the speedup of the interface\n   hardware.  The CPU time use has also decreased\
    \ somewhat from that\n   reported in RFC #557 by Barry Wessler: this demonstrates\
    \ continued\n   improvement of system efficiency between the TENEX version 1.31\
    \ and\n   TENEX version 1.32.\n   In conclusion, the BBN-TENEX staff recommends\
    \ that all host-IMP\n   interfaces in the network be speeded up to the fastest\
    \ operation\n   obtainable.\n          [This RFC was put into machine readable\
    \ form for entry]\n           [into the online RFC archives by Alan Whinery, 1/02]\n"
