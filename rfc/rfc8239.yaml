- title: __initial_text__
  contents:
  - '                  Data Center Benchmarking Methodology

    '
- title: Abstract
  contents:
  - "Abstract\n   The purpose of this informational document is to establish test\
    \ and\n   evaluation methodology and measurement techniques for physical\n   network\
    \ equipment in the data center.  RFC 8238 is a prerequisite for\n   this document,\
    \ as it contains terminology that is considered\n   normative.  Many of these\
    \ terms and methods may be applicable beyond\n   the scope of this document as\
    \ the technologies originally applied in\n   the data center are deployed elsewhere.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc8239.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2017 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Requirements Language ......................................4\n  \
    \    1.2. Methodology Format and Repeatability Recommendation ........4\n   2.\
    \ Line-Rate Testing ...............................................4\n      2.1.\
    \ Objective ..................................................4\n      2.2. Methodology\
    \ ................................................4\n      2.3. Reporting Format\
    \ ...........................................5\n   3. Buffering Testing ...............................................6\n\
    \      3.1. Objective ..................................................6\n  \
    \    3.2. Methodology ................................................7\n    \
    \  3.3. Reporting Format ...........................................9\n   4. Microburst\
    \ Testing .............................................10\n      4.1. Objective\
    \ .................................................10\n      4.2. Methodology\
    \ ...............................................10\n      4.3. Reporting Format\
    \ ..........................................11\n   5. Head-of-Line Blocking ..........................................12\n\
    \      5.1. Objective .................................................12\n  \
    \    5.2. Methodology ...............................................12\n    \
    \  5.3. Reporting Format ..........................................14\n   6. Incast\
    \ Stateful and Stateless Traffic ..........................15\n      6.1. Objective\
    \ .................................................15\n      6.2. Methodology\
    \ ...............................................15\n      6.3. Reporting Format\
    \ ..........................................17\n   7. Security Considerations\
    \ ........................................17\n   8. IANA Considerations ............................................17\n\
    \   9. References .....................................................18\n  \
    \    9.1. Normative References ......................................18\n    \
    \  9.2. Informative References ....................................18\n   Acknowledgments\
    \ ...................................................19\n   Authors' Addresses\
    \ ................................................19\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Traffic patterns in the data center are not uniform and\
    \ are\n   constantly changing.  They are dictated by the nature and variety of\n\
    \   applications utilized in the data center.  They can be largely\n   east-west\
    \ traffic flows (server to server inside the data center) in\n   one data center\
    \ and north-south (from the outside of the data center\n   to the server) in another,\
    \ while others may combine both.  Traffic\n   patterns can be bursty in nature\
    \ and contain many-to-one,\n   many-to-many, or one-to-many flows.  Each flow\
    \ may also be small and\n   latency sensitive or large and throughput sensitive\
    \ while containing\n   a mix of UDP and TCP traffic.  All of these can coexist\
    \ in a single\n   cluster and flow through a single network device simultaneously.\n\
    \   Benchmarking tests for network devices have long used [RFC1242],\n   [RFC2432],\
    \ [RFC2544], [RFC2889], and [RFC3918], which have largely\n   been focused around\
    \ various latency attributes and throughput\n   [RFC2889] of the Device Under\
    \ Test (DUT) being benchmarked.  These\n   standards are good at measuring theoretical\
    \ throughput, forwarding\n   rates, and latency under testing conditions; however,\
    \ they do not\n   represent real traffic patterns that may affect these networking\n\
    \   devices.\n   Currently, typical data center networking devices are\n   characterized\
    \ by:\n   -  High port density (48 ports or more).\n   -  High speed (currently,\
    \ up to 100 GB/s per port).\n   -  High throughput (line rate on all ports for\
    \ Layer 2 and/or\n      Layer 3).\n   -  Low latency (in the microsecond or nanosecond\
    \ range).\n   -  Low amount of buffer (in the MB range per networking device).\n\
    \   -  Layer 2 and Layer 3 forwarding capability (Layer 3 not mandatory).\n  \
    \ This document provides a methodology for benchmarking data center\n   physical\
    \ network equipment DUTs, including congestion scenarios,\n   switch buffer analysis,\
    \ microburst, and head-of-line blocking, while\n   also using a wide mix of traffic\
    \ conditions.  [RFC8238] is a\n   prerequisite for this document, as it contains\
    \ terminology that is\n   considered normative.\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    NOT RECOMMENDED\", \"MAY\", and\n   \"OPTIONAL\" in this document are to be interpreted\
    \ as described in\n   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear\
    \ in all\n   capitals, as shown here.\n"
- title: 1.2.  Methodology Format and Repeatability Recommendation
  contents:
  - "1.2.  Methodology Format and Repeatability Recommendation\n   The following format\
    \ is used in Sections 2 through 6 of this\n   document:\n   -  Objective\n   -\
    \  Methodology\n   -  Reporting Format\n   For each test methodology described\
    \ in this document, it is critical\n   that repeatability of the results be obtained.\
    \  The recommendation is\n   to perform enough iterations of the given test and\
    \ to make sure that\n   the result is consistent.  This is especially important\
    \ in the\n   context of the tests described in Section 3, as the buffering testing\n\
    \   has historically been the least reliable.  The number of iterations\n   SHOULD\
    \ be explicitly reported.  The relative standard deviation\n   SHOULD be below\
    \ 10%.\n"
- title: 2.  Line-Rate Testing
  contents:
  - '2.  Line-Rate Testing

    '
- title: 2.1.  Objective
  contents:
  - "2.1.  Objective\n   The objective of this test is to provide a \"maximum rate\"\
    \ test for\n   the performance values for throughput, latency, and jitter.  It\
    \ is\n   meant to provide (1) the tests to perform and (2) methodology for\n \
    \  verifying that a DUT is capable of forwarding packets at line rate\n   under\
    \ non-congested conditions.\n"
- title: 2.2.  Methodology
  contents:
  - "2.2.  Methodology\n   A traffic generator SHOULD be connected to all ports on\
    \ the DUT.  Two\n   tests MUST be conducted: (1) a port-pair test [RFC2544] [RFC3918]\
    \ and\n   (2) a test using a full-mesh DUT [RFC2889] [RFC3918].\n   For all tests,\
    \ the traffic generator's sending rate MUST be less than\n   or equal to 99.98%\
    \ of the nominal value of the line rate (with no\n   further Parts Per Million\
    \ (PPM) adjustment to account for interface\n   clock tolerances), to ensure stressing\
    \ of the DUT in reasonable\n   worst-case conditions (see [RFC8238], Section 5\
    \ for more details).\n   Test results at a lower rate MAY be provided for better\
    \ understanding\n   of performance increase in terms of latency and jitter when\
    \ the rate\n   is lower than 99.98%.  The receiving rate of the traffic SHOULD\
    \ be\n   captured during this test as a percentage of line rate.\n   The test\
    \ MUST provide the statistics of minimum, average, and\n   maximum of the latency\
    \ distribution, for the exact same iteration of\n   the test.\n   The test MUST\
    \ provide the statistics of minimum, average, and maximum\n   of the jitter distribution,\
    \ for the exact same iteration of the test.\n   Alternatively, when a traffic\
    \ generator cannot be connected to all\n   ports on the DUT, a snake test MUST\
    \ be used for line-rate testing,\n   excluding latency and jitter, as those would\
    \ become irrelevant.  The\n   snake test is performed as follows:\n   -  Connect\
    \ the first and last port of the DUT to a traffic generator.\n   -  Connect, back\
    \ to back and sequentially, all the ports in between:\n      port 2 to port 3,\
    \ port 4 to port 5, etc., to port N-2 to port N-1,\n      where N is the total\
    \ number of ports of the DUT.\n   -  Configure port 1 and port 2 in the same VLAN\
    \ X, port 3 and port 4\n      in the same VLAN Y, etc., and port N-1 and port\
    \ N in the same\n      VLAN Z.\n   This snake test provides the capability to\
    \ test line rate for Layer 2\n   and Layer 3 [RFC2544] [RFC3918] in instances\
    \ where a traffic\n   generator with only two ports is available.  Latency and\
    \ jitter are\n   not to be considered for this test.\n"
- title: 2.3.  Reporting Format
  contents:
  - "2.3.  Reporting Format\n   The report MUST include the following:\n   -  Physical-layer\
    \ calibration information, as defined in [RFC8238],\n      Section 4.\n   -  Number\
    \ of ports used.\n   -  Reading for \"throughput received as a percentage of bandwidth\"\
    ,\n      while sending 99.98% of the nominal value of the line rate on each\n\
    \      port, for each packet size from 64 bytes to 9216 bytes.  As\n      guidance,\
    \ with a packet-size increment of 64 bytes between each\n      iteration being\
    \ ideal, 256-byte and 512-byte packets are also\n      often used.  The most common\
    \ packet-size ordering for the report\n      is 64 bytes, 128 bytes, 256 bytes,\
    \ 512 bytes, 1024 bytes,\n      1518 bytes, 4096 bytes, 8000 bytes, and 9216 bytes.\n\
    \      The pattern for testing can be expressed using [RFC6985].\n   -  Throughput\
    \ needs to be expressed as a percentage of total\n      transmitted frames.\n\
    \   -  Packet drops MUST be expressed as a count of packets and SHOULD be\n  \
    \    expressed as a percentage of line rate.\n   -  For latency and jitter, values\
    \ are expressed in units of time\n      (usually microseconds or nanoseconds),\
    \ reading across packet sizes\n      from 64 bytes to 9216 bytes.\n   -  For latency\
    \ and jitter, provide minimum, average, and maximum\n      values.  If different\
    \ iterations are done to gather the minimum,\n      average, and maximum values,\
    \ this SHOULD be specified in the\n      report, along with a justification for\
    \ why the information could\n      not have been gathered in the same test iteration.\n\
    \   -  For jitter, a histogram describing the population of packets\n      measured\
    \ per latency or latency buckets is RECOMMENDED.\n   -  The tests for throughput,\
    \ latency, and jitter MAY be conducted as\n      individual independent trials,\
    \ with proper documentation provided\n      in the report, but SHOULD be conducted\
    \ at the same time.\n   -  The methodology assumes that the DUT has at least nine\
    \ ports, as\n      certain methodologies require nine or more ports.\n"
- title: 3.  Buffering Testing
  contents:
  - '3.  Buffering Testing

    '
- title: 3.1.  Objective
  contents:
  - "3.1.  Objective\n   The objective of this test is to measure the size of the\
    \ buffer of a\n   DUT under typical/many/multiple conditions.  Buffer architectures\n\
    \   between multiple DUTs can differ and include egress buffering, shared\n  \
    \ egress buffering SoC (Switch-on-Chip), ingress buffering, or a\n   combination\
    \ thereof.  The test methodology covers the buffer\n   measurement, regardless\
    \ of buffer architecture used in the DUT.\n"
- title: 3.2.  Methodology
  contents:
  - "3.2.  Methodology\n   A traffic generator MUST be connected to all ports on the\
    \ DUT.  The\n   methodology for measuring buffering for a data center switch is\
    \ based\n   on using known congestion of known fixed packet size, along with\n\
    \   maximum latency value measurements.  The maximum latency will\n   increase\
    \ until the first packet drop occurs.  At this point, the\n   maximum latency\
    \ value will remain constant.  This is the point of\n   inflection of this maximum\
    \ latency change to a constant value.  There\n   MUST be multiple ingress ports\
    \ receiving a known amount of frames at\n   a known fixed size, destined for the\
    \ same egress port in order to\n   create a known congestion condition.  The total\
    \ amount of packets\n   sent from the oversubscribed port minus one, multiplied\
    \ by the packet\n   size, represents the maximum port buffer size at the measured\n\
    \   inflection point.\n   Note that the tests described in procedures 1), 2),\
    \ 3), and 4) in\n   this section have iterations called \"first iteration\", \"\
    second\n   iteration\", and \"last iteration\".  The idea is to show the first\n\
    \   two iterations so the reader understands the logic of how to keep\n   incrementing\
    \ the iterations.  The last iteration shows the end state\n   of the variables.\n\
    \   1) Measure the highest buffer efficiency.\n      o  First iteration: Ingress\
    \ port 1 sending 64-byte packets at line\n         rate to egress port 2, while\
    \ port 3 is sending a known low\n         amount of oversubscription traffic (1%\
    \ recommended) with the\n         same packet size of 64 bytes to egress port\
    \ 2.  Measure the\n         buffer size value of the number of frames sent from\
    \ the port\n         sending the oversubscribed traffic up to the inflection point\n\
    \         multiplied by the frame size.\n      o  Second iteration: Ingress port\
    \ 1 sending 65-byte packets at\n         line rate to egress port 2, while port\
    \ 3 is sending a known low\n         amount of oversubscription traffic (1% recommended)\
    \ with the\n         same packet size of 65 bytes to egress port 2.  Measure the\n\
    \         buffer size value of the number of frames sent from the port\n     \
    \    sending the oversubscribed traffic up to the inflection point\n         multiplied\
    \ by the frame size.\n      o  Last iteration: Ingress port 1 sending packets\
    \ of size B bytes\n         at line rate to egress port 2, while port 3 is sending\
    \ a known\n         low amount of oversubscription traffic (1% recommended) with\n\
    \         the same packet size of B bytes to egress port 2.  Measure the\n   \
    \      buffer size value of the number of frames sent from the port\n        \
    \ sending the oversubscribed traffic up to the inflection point\n         multiplied\
    \ by the frame size.\n      When the B value is found to provide the largest buffer\
    \ size, then\n      size B allows the highest buffer efficiency.\n   2) Measure\
    \ maximum port buffer size.\n      At fixed packet size B as determined in procedure\
    \ 1), for a fixed\n      default Differentiated Services Code Point (DSCP) / Class\
    \ of\n      Service (CoS) value of 0 and for unicast traffic, proceed with the\n\
    \      following:\n      o  First iteration: Ingress port 1 sending line rate\
    \ to egress\n         port 2, while port 3 is sending a known low amount of\n\
    \         oversubscription traffic (1% recommended) with the same packet\n   \
    \      size to egress port 2.  Measure the buffer size value by\n         multiplying\
    \ the number of extra frames sent by the frame size.\n      o  Second iteration:\
    \ Ingress port 2 sending line rate to egress\n         port 3, while port 4 is\
    \ sending a known low amount of\n         oversubscription traffic (1% recommended)\
    \ with the same packet\n         size to egress port 3.  Measure the buffer size\
    \ value by\n         multiplying the number of extra frames sent by the frame\
    \ size.\n      o  Last iteration: Ingress port N-2 sending line rate to egress\n\
    \         port N-1, while port N is sending a known low amount of\n         oversubscription\
    \ traffic (1% recommended) with the same packet\n         size to egress port\
    \ N.  Measure the buffer size value by\n         multiplying the number of extra\
    \ frames sent by the frame size.\n      This test series MAY be repeated using\
    \ all different DSCP/CoS\n      values of traffic, and then using multicast traffic,\
    \ in order to\n      find out if there is any DSCP/CoS impact on the buffer size.\n\
    \   3) Measure maximum port pair buffer sizes.\n      o  First iteration: Ingress\
    \ port 1 sending line rate to egress\n         port 2, ingress port 3 sending\
    \ line rate to egress port 4, etc.\n         Ingress port N-1 and port N will\
    \ oversubscribe, at 1% of line\n         rate, egress port 2 and port 3, respectively.\
    \  Measure the\n         buffer size value by multiplying the number of extra\
    \ frames\n         sent by the frame size for each egress port.\n      o  Second\
    \ iteration: Ingress port 1 sending line rate to egress\n         port 2, ingress\
    \ port 3 sending line rate to egress port 4, etc.\n         Ingress port N-1 and\
    \ port N will oversubscribe, at 1% of line\n         rate, egress port 4 and port\
    \ 5, respectively.  Measure the\n         buffer size value by multiplying the\
    \ number of extra frames\n         sent by the frame size for each egress port.\n\
    \      o  Last iteration: Ingress port 1 sending line rate to egress\n       \
    \  port 2, ingress port 3 sending line rate to egress port 4, etc.\n         Ingress\
    \ port N-1 and port N will oversubscribe, at 1% of line\n         rate, egress\
    \ port N-3 and port N-2, respectively.  Measure the\n         buffer size value\
    \ by multiplying the number of extra frames\n         sent by the frame size for\
    \ each egress port.\n      This test series MAY be repeated using all different\
    \ DSCP/CoS\n      values of traffic and then using multicast traffic.\n   4) Measure\
    \ maximum DUT buffer size with many-to-one ports.\n      o  First iteration: Ingress\
    \ ports 1,2,... N-1 each sending\n         [(1/[N-1])*99.98]+[1/[N-1]] % of line\
    \ rate per port to egress\n         port N.\n      o  Second iteration: Ingress\
    \ ports 2,... N each sending\n         [(1/[N-1])*99.98]+[1/[N-1]] % of line rate\
    \ per port to egress\n         port 1.\n      o  Last iteration: Ingress ports\
    \ N,1,2...N-2 each sending\n         [(1/[N-1])*99.98]+[1/[N-1]] % of line rate\
    \ per port to egress\n         port N-1.\n   This test series MAY be repeated\
    \ using all different CoS values of\n   traffic and then using multicast traffic.\n\
    \   Unicast traffic, and then multicast traffic, SHOULD be used in order\n   to\
    \ determine the proportion of buffer for the documented selection of\n   tests.\
    \  Also, the CoS value for the packets SHOULD be provided for\n   each test iteration,\
    \ as the buffer allocation size MAY differ per CoS\n   value.  It is RECOMMENDED\
    \ that the ingress and egress ports be varied\n   in a random but documented fashion\
    \ in multiple tests in order to\n   measure the buffer size for each port of the\
    \ DUT.\n"
- title: 3.3.  Reporting Format
  contents:
  - "3.3.  Reporting Format\n   The report MUST include the following:\n   -  The\
    \ packet size used for the most efficient buffer used,\n      along with the DSCP/CoS\
    \ value.\n   -  The maximum port buffer size for each port.\n   -  The maximum\
    \ DUT buffer size.\n   -  The packet size used in the test.\n   -  The amount\
    \ of oversubscription, if different than 1%.\n   -  The number of ingress and\
    \ egress ports, along with their location\n      on the DUT.\n   -  The repeatability\
    \ of the test needs to be indicated: the number of\n      iterations of the same\
    \ test and the percentage of variation\n      between results for each of the\
    \ tests (min, max, avg).\n   The percentage of variation is a metric providing\
    \ a sense of how big\n   the difference is between the measured value and the\
    \ previous values.\n   For example, for a latency test where the minimum latency\
    \ is\n   measured, the percentage of variation (PV) of the minimum latency\n \
    \  will indicate by how much this value has varied between the current\n   test\
    \ executed and the previous one.\n   PV = ((x2-x1)/x1)*100, where x2 is the minimum\
    \ latency value in the\n   current test and x1 is the minimum latency value obtained\
    \ in the\n   previous test.\n   The same formula is used for maximum and average\
    \ variations measured.\n"
- title: 4.  Microburst Testing
  contents:
  - '4.  Microburst Testing

    '
- title: 4.1.  Objective
  contents:
  - "4.1.  Objective\n   The objective of this test is to find the maximum amount\
    \ of packet\n   bursts that a DUT can sustain under various configurations.\n\
    \   This test provides additional methodology that supplements the tests\n   described\
    \ in [RFC1242], [RFC2432], [RFC2544], [RFC2889], and\n   [RFC3918].\n   -  All\
    \ bursts should be sent with 100% intensity.  Note: \"Intensity\"\n      is defined\
    \ in [RFC8238], Section 6.1.1.\n   -  All ports of the DUT must be used for this\
    \ test.\n   -  It is recommended that all ports be tested simultaneously.\n"
- title: 4.2.  Methodology
  contents:
  - "4.2.  Methodology\n   A traffic generator MUST be connected to all ports on the\
    \ DUT.  In\n   order to cause congestion, two or more ingress ports MUST send\
    \ bursts\n   of packets destined for the same egress port.  The simplest of the\n\
    \   setups would be two ingress ports and one egress port (2 to 1).\n   The burst\
    \ MUST be sent with an intensity (as defined in [RFC8238],\n   Section 6.1.1)\
    \ of 100%, meaning that the burst of packets will be\n   sent with a minimum interpacket\
    \ gap.  The amount of packets contained\n   in the burst will be trial variable\
    \ and increase until there is a\n   non-zero packet loss measured.  The aggregate\
    \ amount of packets from\n   all the senders will be used to calculate the maximum\
    \ microburst\n   amount that the DUT can sustain.\n   It is RECOMMENDED that the\
    \ ingress and egress ports be varied in\n   multiple tests in order to measure\
    \ the maximum microburst capacity.\n   The intensity of a microburst (see [RFC8238],\
    \ Section 6.1.1) MAY be\n   varied in order to obtain the microburst capacity\
    \ at various\n   ingress rates.\n   It is RECOMMENDED that all ports on the DUT\
    \ be tested simultaneously,\n   and in various configurations, in order to understand\
    \ all the\n   combinations of ingress ports, egress ports, and intensities.\n\
    \   An example would be:\n   o  First iteration: N-1 ingress ports sending to\
    \ one egress port.\n   o  Second iteration: N-2 ingress ports sending to two egress\
    \ ports.\n   o  Last iteration: Two ingress ports sending to N-2 egress ports.\n"
- title: 4.3.  Reporting Format
  contents:
  - "4.3.  Reporting Format\n   The report MUST include the following:\n   -  The\
    \ maximum number of packets received per ingress port with the\n      maximum\
    \ burst size obtained with zero packet loss.\n   -  The packet size used in the\
    \ test.\n   -  The number of ingress and egress ports, along with their location\n\
    \      on the DUT.\n   -  The repeatability of the test needs to be indicated:\
    \ the number of\n      iterations of the same test and the percentage of variation\n\
    \      between results (min, max, avg).\n"
- title: 5.  Head-of-Line Blocking
  contents:
  - '5.  Head-of-Line Blocking

    '
- title: 5.1.  Objective
  contents:
  - "5.1.  Objective\n   Head-of-line blocking (HOLB) is a performance-limiting phenomenon\n\
    \   that occurs when packets are held up by the first packet ahead\n   waiting\
    \ to be transmitted to a different output port.  This is\n   defined in RFC 2889,\
    \ Section 5.5 (\"Congestion Control\").  This\n   section expands on RFC 2889\
    \ in the context of data center\n   benchmarking.\n   The objective of this test\
    \ is to understand the DUT's behavior in the\n   HOLB scenario and measure the\
    \ packet loss.\n   The differences between this HOLB test and RFC 2889 are as\
    \ follows:\n   -  This HOLB test starts with eight ports in two groups of four\
    \ ports\n      each, instead of four ports (as compared with Section 5.5 of\n\
    \      RFC 2889).\n   -  This HOLB test shifts all the port numbers by one in\
    \ a second\n      iteration of the test; this is new, as compared to the HOLB\
    \ test\n      described in RFC 2889.  The shifting port numbers continue until\n\
    \      all ports are the first in the group; the purpose of this is to\n     \
    \ make sure that all permutations are tested in order to cover\n      differences\
    \ in behavior in the SoC of the DUT.\n   -  Another test within this HOLB test\
    \ expands the group of ports,\n      such that traffic is divided among four ports\
    \ instead of two\n      (25% instead of 50% per port).\n   -  Section 5.3 lists\
    \ requirements that supplement the requirements\n      listed in RFC 2889, Section\
    \ 5.5.\n"
- title: 5.2.  Methodology
  contents:
  - "5.2.  Methodology\n   In order to cause congestion in the form of HOLB, groups\
    \ of\n   four ports are used.  A group has two ingress ports and two\n   egress\
    \ ports.  The first ingress port MUST have two flows configured,\n   each going\
    \ to a different egress port.  The second ingress port will\n   congest the second\
    \ egress port by sending line rate.  The goal is to\n   measure if there is loss\
    \ on the flow for the first egress port, which\n   is not oversubscribed.\n  \
    \ A traffic generator MUST be connected to at least eight ports on the\n   DUT\
    \ and SHOULD be connected using all the DUT ports.\n   Note that the tests described\
    \ in procedures 1) and 2) in this section\n   have iterations called \"first iteration\"\
    , \"second iteration\", and\n   \"last iteration\".  The idea is to show the first\
    \ two iterations so\n   the reader understands the logic of how to keep incrementing\
    \ the\n   iterations.  The last iteration shows the end state of the variables.\n\
    \   1) Measure two groups with eight DUT ports.\n      o  First iteration: Measure\
    \ the packet loss for two groups with\n         consecutive ports.\n         The\
    \ composition of the first group is as follows:\n         Ingress port 1 sending\
    \ 50% of traffic to egress port 3\n         and ingress port 1 sending 50% of\
    \ traffic to egress port 4.\n         Ingress port 2 sending line rate to egress\
    \ port 4.\n         Measure the amount of traffic loss for the traffic from ingress\n\
    \         port 1 to egress port 3.\n         The composition of the second group\
    \ is as follows:\n         Ingress port 5 sending 50% of traffic to egress port\
    \ 7\n         and ingress port 5 sending 50% of traffic to egress port 8.\n  \
    \       Ingress port 6 sending line rate to egress port 8.\n         Measure the\
    \ amount of traffic loss for the traffic from ingress\n         port 5 to egress\
    \ port 7.\n      o  Second iteration: Repeat the first iteration by shifting all\n\
    \         the ports from N to N+1.\n         The composition of the first group\
    \ is as follows:\n         Ingress port 2 sending 50% of traffic to egress port\
    \ 4\n         and ingress port 2 sending 50% of traffic to egress port 5.\n  \
    \       Ingress port 3 sending line rate to egress port 5.\n         Measure the\
    \ amount of traffic loss for the traffic from ingress\n         port 2 to egress\
    \ port 4.\n         The composition of the second group is as follows:\n     \
    \    Ingress port 6 sending 50% of traffic to egress port 8\n         and ingress\
    \ port 6 sending 50% of traffic to egress port 9.\n         Ingress port 7 sending\
    \ line rate to egress port 9.\n         Measure the amount of traffic loss for\
    \ the traffic from ingress\n         port 6 to egress port 8.\n      o  Last iteration:\
    \ When the first port of the first group is\n         connected to the last DUT\
    \ port and the last port of the second\n         group is connected to the seventh\
    \ port of the DUT.\n      Measure the amount of traffic loss for the traffic from\
    \ ingress\n      port N to egress port 2 and from ingress port 4 to egress port\
    \ 6.\n   2) Measure with N/4 groups with N DUT ports.\n      The traffic from\
    \ the ingress port is split across four egress\n      ports (100/4 = 25%).\n \
    \     o  First iteration: Expand to fully utilize all the DUT ports in\n     \
    \    increments of four.  Repeat the methodology of procedure 1)\n         with\
    \ all the groups of ports possible to achieve on the device,\n         and measure\
    \ the amount of traffic loss for each port group.\n      o  Second iteration:\
    \ Shift by +1 the start of each consecutive\n         port of the port groups.\n\
    \      o  Last iteration: Shift by N-1 the start of each consecutive port\n  \
    \       of the port groups, and measure the amount of traffic loss for\n     \
    \    each port group.\n"
- title: 5.3.  Reporting Format
  contents:
  - "5.3.  Reporting Format\n   For each test, the report MUST include the following:\n\
    \   -  The port configuration, including the number and location of\n      ingress\
    \ and egress ports located on the DUT.\n   -  If HOLB was observed in accordance\
    \ with the HOLB test described in\n      Section 5.\n   -  Percent of traffic\
    \ loss.\n   -  The repeatability of the test needs to be indicated: the number\
    \ of\n      iterations of the same test and the percentage of variation\n    \
    \  between results (min, max, avg).\n"
- title: 6.  Incast Stateful and Stateless Traffic
  contents:
  - '6.  Incast Stateful and Stateless Traffic

    '
- title: 6.1.  Objective
  contents:
  - "6.1.  Objective\n   The objective of this test is to measure the values for TCP\
    \ Goodput\n   [TCP-INCAST] and latency with a mix of large and small flows.  The\n\
    \   test is designed to simulate a mixed environment of stateful flows\n   that\
    \ require high rates of goodput and stateless flows that require\n   low latency.\
    \  Stateful flows are created by generating TCP traffic,\n   and stateless flows\
    \ are created using UDP traffic.\n"
- title: 6.2.  Methodology
  contents:
  - "6.2.  Methodology\n   In order to simulate the effects of stateless and stateful\
    \ traffic on\n   the DUT, there MUST be multiple ingress ports receiving traffic\n\
    \   destined for the same egress port.  There also MAY be a mix of\n   stateful\
    \ and stateless traffic arriving on a single ingress port.\n   The simplest setup\
    \ would be two ingress ports receiving traffic\n   destined to the same egress\
    \ port.\n   One ingress port MUST maintain a TCP connection through the ingress\n\
    \   port to a receiver connected to an egress port.  Traffic in the TCP\n   stream\
    \ MUST be sent at the maximum rate allowed by the traffic\n   generator.  At the\
    \ same time, the TCP traffic is flowing through the\n   DUT, and the stateless\
    \ traffic is sent destined to a receiver on the\n   same egress port.  The stateless\
    \ traffic MUST be a microburst of\n   100% intensity.\n   It is RECOMMENDED that\
    \ the ingress and egress ports be varied in\n   multiple tests in order to measure\
    \ the maximum microburst capacity.\n   The intensity of a microburst MAY be varied\
    \ in order to obtain the\n   microburst capacity at various ingress rates.\n \
    \  It is RECOMMENDED that all ports on the DUT be used in the test.\n   The tests\
    \ described below have iterations called \"first iteration\",\n   \"second iteration\"\
    , and \"last iteration\".  The idea is to show the\n   first two iterations so\
    \ the reader understands the logic of how to\n   keep incrementing the iterations.\
    \  The last iteration shows the\n   end state of the variables.\n   For example:\n\
    \   Stateful traffic port variation (TCP traffic):\n   TCP traffic needs to be\
    \ generated for this test.  During the\n   iterations, the number of egress ports\
    \ MAY vary as well.\n   o  First iteration: One ingress port receiving stateful\
    \ TCP traffic\n      and one ingress port receiving stateless traffic destined\
    \ to\n      one egress port.\n   o  Second iteration: Two ingress ports receiving\
    \ stateful TCP traffic\n      and one ingress port receiving stateless traffic\
    \ destined to\n      one egress port.\n   o  Last iteration: N-2 ingress ports\
    \ receiving stateful TCP traffic\n      and one ingress port receiving stateless\
    \ traffic destined to\n      one egress port.\n   Stateless traffic port variation\
    \ (UDP traffic):\n   UDP traffic needs to be generated for this test.  During\
    \ the\n   iterations, the number of egress ports MAY vary as well.\n   o  First\
    \ iteration: One ingress port receiving stateful TCP traffic\n      and one ingress\
    \ port receiving stateless traffic destined to\n      one egress port.\n   o \
    \ Second iteration: One ingress port receiving stateful TCP traffic\n      and\
    \ two ingress ports receiving stateless traffic destined to\n      one egress\
    \ port.\n   o  Last iteration: One ingress port receiving stateful TCP traffic\n\
    \      and N-2 ingress ports receiving stateless traffic destined to\n      one\
    \ egress port.\n"
- title: 6.3.  Reporting Format
  contents:
  - "6.3.  Reporting Format\n   The report MUST include the following:\n   -  Number\
    \ of ingress and egress ports, along with designation of\n      stateful or stateless\
    \ flow assignment.\n   -  Stateful flow goodput.\n   -  Stateless flow latency.\n\
    \   -  The repeatability of the test needs to be indicated: the number of\n  \
    \    iterations of the same test and the percentage of variation\n      between\
    \ results (min, max, avg).\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   Benchmarking activities as described in this\
    \ memo are limited to\n   technology characterization using controlled stimuli\
    \ in a laboratory\n   environment, with dedicated address space and the constraints\n\
    \   specified in the sections above.\n   The benchmarking network topology will\
    \ be an independent test setup\n   and MUST NOT be connected to devices that may\
    \ forward the test\n   traffic into a production network or misroute traffic to\
    \ the test\n   management network.\n   Further, benchmarking is performed on a\
    \ \"black-box\" basis, relying\n   solely on measurements observable external\
    \ to the DUT.\n   Special capabilities SHOULD NOT exist in the DUT specifically\
    \ for\n   benchmarking purposes.  Any implications for network security arising\n\
    \   from the DUT SHOULD be identical in the lab and in production\n   networks.\n"
- title: 8.  IANA Considerations
  contents:
  - "8.  IANA Considerations\n   This document does not require any IANA actions.\n"
- title: 9.  References
  contents:
  - '9.  References

    '
- title: 9.1.  Normative References
  contents:
  - "9.1.  Normative References\n   [RFC1242]  Bradner, S., \"Benchmarking Terminology\
    \ for Network\n              Interconnection Devices\", RFC 1242, DOI 10.17487/RFC1242,\n\
    \              July 1991, <https://www.rfc-editor.org/info/rfc1242>.\n   [RFC2119]\
    \  Bradner, S., \"Key words for use in RFCs to Indicate\n              Requirement\
    \ Levels\", BCP 14, RFC 2119,\n              DOI 10.17487/RFC2119, March 1997,\n\
    \              <https://www.rfc-editor.org/info/rfc2119>.\n   [RFC2544]  Bradner,\
    \ S. and J. McQuaid, \"Benchmarking Methodology for\n              Network Interconnect\
    \ Devices\", RFC 2544,\n              DOI 10.17487/RFC2544, March 1999,\n    \
    \          <https://www.rfc-editor.org/info/rfc2544>.\n   [RFC8174]  Leiba, B.,\
    \ \"Ambiguity of Uppercase vs Lowercase in\n              RFC 2119 Key Words\"\
    , BCP 14, RFC 8174,\n              DOI 10.17487/RFC8174, May 2017,\n         \
    \     <https://www.rfc-editor.org/info/rfc8174>.\n   [RFC8238]  Avramov, L. and\
    \ J. Rapp, \"Data Center Benchmarking\n              Terminology\", RFC 8238,\
    \ DOI 10.17487/RFC8238, August 2017,\n              <https://www.rfc-editor.org/info/rfc8238>.\n"
- title: 9.2.  Informative References
  contents:
  - "9.2.  Informative References\n   [RFC2432]  Dubray, K., \"Terminology for IP\
    \ Multicast Benchmarking\",\n              RFC 2432, DOI 10.17487/RFC2432, October\
    \ 1998,\n              <https://www.rfc-editor.org/info/rfc2432>.\n   [RFC2889]\
    \  Mandeville, R. and J. Perser, \"Benchmarking Methodology\n              for\
    \ LAN Switching Devices\", RFC 2889,\n              DOI 10.17487/RFC2889, August\
    \ 2000,\n              <https://www.rfc-editor.org/info/rfc2889>.\n   [RFC3918]\
    \  Stopp, D. and B. Hickman, \"Methodology for IP Multicast\n              Benchmarking\"\
    , RFC 3918, DOI 10.17487/RFC3918,\n              October 2004, <https://www.rfc-editor.org/info/rfc3918>.\n\
    \   [RFC6985]  Morton, A., \"IMIX Genome: Specification of Variable Packet\n \
    \             Sizes for Additional Testing\", RFC 6985,\n              DOI 10.17487/RFC6985,\
    \ July 2013,\n              <https://www.rfc-editor.org/info/rfc6985>.\n   [TCP-INCAST]\n\
    \              Chen, Y., Griffith, R., Zats, D., Joseph, A., and R. Katz,\n  \
    \            \"Understanding TCP Incast and Its Implications for Big\n       \
    \       Data Workloads\", April 2012, <http://yanpeichen.com/\n              professional/usenixLoginIncastReady.pdf>.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   The authors would like to thank Al Morton and Scott Bradner\
    \ for their\n   reviews and feedback.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Lucien Avramov\n   Google\n   1600 Amphitheatre Parkway\n\
    \   Mountain View, CA  94043\n   United States of America\n   Email: lucien.avramov@gmail.com\n\
    \   Jacob Rapp\n   VMware\n   3401 Hillview Ave.\n   Palo Alto, CA  94304\n  \
    \ United States of America\n   Email: jhrapp@gmail.com\n"
