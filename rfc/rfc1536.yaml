- title: __initial_text__
  contents:
  - '          Common DNS Implementation Errors and Suggested Fixes

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard.  Distribution of this memo is\n\
    \   unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   This memo describes common errors seen in DNS implementations and\n\
    \   suggests some fixes. Where applicable, violations of recommendations\n   from\
    \ STD 13, RFC 1034 and STD 13, RFC 1035 are mentioned. The memo\n   also describes,\
    \ where relevant, the algorithms followed in BIND\n   (versions 4.8.3 and 4.9\
    \ which the authors referred to) to serve as an\n   example.\n"
- title: Introduction
  contents:
  - "Introduction\n   The last few years have seen, virtually, an explosion of DNS\
    \ traffic\n   on the NSFnet backbone. Various DNS implementations and various\n\
    \   versions of these implementations interact with each other, producing\n  \
    \ huge amounts of unnecessary traffic. Attempts are being made by\n   researchers\
    \ all over the internet, to document the nature of these\n   interactions, the\
    \ symptomatic traffic patterns and to devise remedies\n   for the sick pieces\
    \ of software.\n   This draft is an attempt to document fixes for known DNS problems\
    \ so\n   people know what problems to watch out for and how to repair broken\n\
    \   software.\n"
- title: 1. Fast Retransmissions
  contents:
  - "1. Fast Retransmissions\n   DNS implements the classic request-response scheme\
    \ of client-server\n   interaction. UDP is, therefore, the chosen protocol for\
    \ communication\n   though TCP is used for zone transfers. The onus of requerying\
    \ in case\n   no response is seen in a \"reasonable\" period of time, lies with\
    \ the\n   client. Although RFC 1034 and 1035 do not recommend any\n   retransmission\
    \ policy, RFC 1035 does recommend that the resolvers\n   should cycle through\
    \ a list of servers. Both name servers and stub\n   resolvers should, therefore,\
    \ implement some kind of a retransmission\n   policy based on round trip time\
    \ estimates of the name servers. The\n   client should back-off exponentially,\
    \ probably to a maximum timeout\n   value.\n   However, clients might not implement\
    \ either of the two. They might\n   not wait a sufficient amount of time before\
    \ retransmitting or they\n   might not back-off their inter-query times sufficiently.\n\
    \   Thus, what the server would see will be a series of queries from the\n   same\
    \ querying entity, spaced very close together. Of course, a\n   correctly implemented\
    \ server discards all duplicate queries but the\n   queries contribute to wide-area\
    \ traffic, nevertheless.\n   We classify a retransmission of a query as a pure\
    \ Fast retry timeout\n   problem when a series of query packets meet the following\
    \ conditions.\n      a. Query packets are seen within a time less than a \"reasonable\n\
    \         waiting period\" of each other.\n      b. No response to the original\
    \ query was seen i.e., we see two or\n         more queries, back to back.\n \
    \     c. The query packets share the same query identifier.\n      d. The server\
    \ eventually responds to the query.\n"
- title: 'A GOOD IMPLEMENTATION:'
  contents:
  - "A GOOD IMPLEMENTATION:\n   BIND (we looked at versions 4.8.3 and 4.9) implements\
    \ a good\n   retransmission algorithm which solves or limits all of these\n  \
    \ problems.  The Berkeley stub-resolver queries servers at an interval\n   that\
    \ starts at the greater of 4 seconds and 5 seconds divided by the\n   number of\
    \ servers the resolver queries. The resolver cycles through\n   servers and at\
    \ the end of a cycle, backs off the time out\n   exponentially.\n   The Berkeley\
    \ full-service resolver (built in with the program\n   \"named\") starts with\
    \ a time-out equal to the greater of 4 seconds and\n   two times the round-trip\
    \ time estimate of the server.  The time-out\n   is backed off with each cycle,\
    \ exponentially, to a ceiling value of\n   45 seconds.\n"
- title: 'FIXES:'
  contents:
  - "FIXES:\n      a. Estimate round-trip times or set a reasonably high initial\n\
    \         time-out.\n      b. Back-off timeout periods exponentially.\n      c.\
    \ Yet another fundamental though difficult fix is to send the\n         client\
    \ an acknowledgement of a query, with a round-trip time\n         estimate.\n\
    \   Since UDP is used, no response is expected by the client until the\n   query\
    \ is complete.  Thus, it is less likely to have information about\n   previous\
    \ packets on which to estimate its back-off time.  Unless, you\n   maintain state\
    \ across queries, so subsequent queries to the same\n   server use information\
    \ from previous queries.  Unfortunately, such\n   estimates are likely to be inaccurate\
    \ for chained requests since the\n   variance is likely to be high.\n   The fix\
    \ chosen in the ARDP library used by Prospero is that the\n   server will send\
    \ an initial acknowledgement to the client in those\n   cases where the server\
    \ expects the query to take a long time (as\n   might be the case for chained\
    \ queries).  This initial acknowledgement\n   can include an expected time to\
    \ wait before retrying.\n   This fix is more difficult since it requires that\
    \ the client software\n   also be trained to expect the acknowledgement packet.\
    \ This, in an\n   internet of millions of hosts is at best a hard problem.\n"
- title: 2. Recursion Bugs
  contents:
  - "2. Recursion Bugs\n   When a server receives a client request, it first looks\
    \ up its zone\n   data and the cache to check if the query can be answered. If\
    \ the\n   answer is unavailable in either place, the server seeks names of\n \
    \  servers that are more likely to have the information, in its cache or\n   zone\
    \ data. It then does one of two things. If the client desires the\n   server to\
    \ recurse and the server architecture allows recursion, the\n   server chains\
    \ this request to these known servers closest to the\n   queried name. If the\
    \ client doesn't seek recursion or if the server\n   cannot handle recursion,\
    \ it returns the list of name servers to the\n   client assuming the client knows\
    \ what to do with these records.\n   The client queries this new list of name\
    \ servers to get either the\n   answer, or names of another set of name servers\
    \ to query. This\n   process repeats until the client is satisfied. Servers might\
    \ also go\n   through this chaining process if the server returns a CNAME record\n\
    \   for the queried name. Some servers reprocess this name to try and get\n  \
    \ the desired record type.\n   However, in certain cases, this chain of events\
    \ may not be good. For\n   example, a broken or malicious name server might list\
    \ itself as one\n   of the name servers to query again. The unsuspecting client\
    \ resends\n   the same query to the same server.\n   In another situation, more\
    \ difficult to detect, a set of servers\n   might form a loop wherein A refers\
    \ to B and B refers to A. This loop\n   might involve more than two servers.\n\
    \   Yet another error is where the client does not know how to process\n   the\
    \ list of name servers returned, and requeries the same server\n   since that\
    \ is one (of the few) servers it knows.\n   We, therefore, classify recursion\
    \ bugs into three distinct\n   categories:\n      a. Ignored referral: Client\
    \ did not know how to handle NS records\n         in the AUTHORITY section.\n\
    \      b. Too many referrals: Client called on a server too many times,\n    \
    \     beyond a \"reasonable\" number, with same query. This is\n         different\
    \ from a Fast retransmission problem and a Server\n         Failure detection\
    \ problem in that a response is seen for every\n         query.  Also, the identifiers\
    \ are always different. It implies\n         client is in a loop and should have\
    \ detected that and broken\n         it.  (RFC 1035 mentions that client should\
    \ not recurse beyond\n         a certain depth.)\n      c. Malicious Server: a\
    \ server refers to itself in the authority\n         section. If a server does\
    \ not have an answer now, it is very\n         unlikely it will be any better\
    \ the next time you query it,\n         specially when it claims to be authoritative\
    \ over a domain.\n      RFC 1034 warns against such situations, on page 35.\n\
    \      \"Bound the amount of work (packets sent, parallel processes\n       started)\
    \ so that a request can't get into an infinite loop or\n       start off a chain\
    \ reaction of requests or queries with other\n       implementations EVEN IF SOMEONE\
    \ HAS INCORRECTLY CONFIGURED\n       SOME DATA.\"\n"
- title: 'A GOOD IMPLEMENTATION:'
  contents:
  - "A GOOD IMPLEMENTATION:\n   BIND fixes at least one of these problems. It places\
    \ an upper limit\n   on the number of recursive queries it will make, to answer\
    \ a\n   question.  It chases a maximum of 20 referral links and 8 canonical\n\
    \   name translations.\n"
- title: 'FIXES:'
  contents:
  - "FIXES:\n      a. Set an upper limit on the number of referral links and CNAME\n\
    \         links you are willing to chase.\n         Note that this is not guaranteed\
    \ to break only recursion loops.\n         It could, in a rare case, prune off\
    \ a very long search path,\n         prematurely.  We know, however, with high\
    \ probability, that if\n         the number of links cross a certain metric (two\
    \ times the depth\n         of the DNS tree), it is a recursion problem.\n   \
    \   b. Watch out for self-referring servers. Avoid them whenever\n         possible.\n\
    \      c. Make sure you never pass off an authority NS record with your\n    \
    \     own name on it!\n      d. Fix clients to accept iterative answers from servers\
    \ not built\n         to provide recursion. Such clients should either be happy\
    \ with\n         the non-authoritative answer or be willing to chase the\n   \
    \      referral links themselves.\n"
- title: '3. Zero Answer Bugs:'
  contents:
  - "3. Zero Answer Bugs:\n   Name servers sometimes return an authoritative NOERROR\
    \ with no\n   ANSWER, AUTHORITY or ADDITIONAL records. This happens when the\n\
    \   queried name is valid but it does not have a record of the desired\n   type.\
    \ Of course, the server has authority over the domain.\n   However, once again,\
    \ some implementations of resolvers do not\n   interpret this kind of a response\
    \ reasonably. They always expect an\n   answer record when they see an authoritative\
    \ NOERROR. These entities\n   continue to resend their queries, possibly endlessly.\n"
- title: A GOOD IMPLEMENTATION
  contents:
  - "A GOOD IMPLEMENTATION\n   BIND resolver code does not query a server more than\
    \ 3 times. If it\n   is unable to get an answer from 4 servers, querying them\
    \ three times\n   each, it returns error.\n   Of course, it treats a zero-answer\
    \ response the way it should be\n   treated; with respect!\n"
- title: 'FIXES:'
  contents:
  - "FIXES:\n      a. Set an upper limit on the number of retransmissions for a given\n\
    \         query, at the very least.\n      b. Fix resolvers to interpret such\
    \ a response as an authoritative\n         statement of non-existence of the record\
    \ type for the given\n         name.\n"
- title: '4. Inability to detect server failure:'
  contents:
  - "4. Inability to detect server failure:\n   Servers in the internet are not very\
    \ reliable (they go down every\n   once in a while) and resolvers are expected\
    \ to adapt to the changed\n   scenario by not querying the server for a while.\
    \ Thus, when a server\n   does not respond to a query, resolvers should try another\
    \ server.\n   Also, non-stub resolvers should update their round trip time estimate\n\
    \   for the server to a large value so that server is not tried again\n   before\
    \ other, faster servers.\n   Stub resolvers, however, cycle through a fixed set\
    \ of servers and if,\n   unfortunately, a server is down while others do not respond\
    \ for other\n   reasons (high load, recursive resolution of query is taking more\
    \ time\n   than the resolver's time-out, ....), the resolver queries the dead\n\
    \   server again! In fact, some resolvers might not set an upper limit on\n  \
    \ the number of query retransmissions they will send and continue to\n   query\
    \ dead servers indefinitely.\n   Name servers running system or chained queries\
    \ might also suffer from\n   the same problem. They store names of servers they\
    \ should query for a\n   given domain. They cycle through these names and in case\
    \ none of them\n   answers, hit each one more than one. It is, once again, important\n\
    \   that there be an upper limit on the number of retransmissions, to\n   prevent\
    \ network overload.\n   This behavior is clearly in violation of the dictum in\
    \ RFC 1035 (page\n   46)\n      \"If a resolver gets a server error or other bizarre\
    \ response\n       from a name server, it should remove it from SLIST, and may\n\
    \       wish to schedule an immediate transmission to the next\n       candidate\
    \ server address.\"\n   Removal from SLIST implies that the server is not queried\
    \ again for\n   some time.\n   Correctly implemented full-service resolvers should,\
    \ as pointed out\n   before, update round trip time values for servers that do\
    \ not respond\n   and query them only after other, good servers. Full-service\
    \ resolvers\n   might, however, not follow any of these common sense directives.\
    \ They\n   query dead servers, and they query them endlessly.\n"
- title: 'A GOOD IMPLEMENTATION:'
  contents:
  - "A GOOD IMPLEMENTATION:\n   BIND places an upper limit on the number of times\
    \ it queries a\n   server.  Both the stub-resolver and the full-service resolver\
    \ code do\n   this.  Also, since the full-service resolver estimates round-trip\n\
    \   times and sorts name server addresses by these estimates, it does not\n  \
    \ query a dead server again, until and unless all the other servers in\n   the\
    \ list are dead too!  Further, BIND implements exponential back-off\n   too.\n"
- title: 'FIXES:'
  contents:
  - "FIXES:\n      a. Set an upper limit on number of retransmissions.\n      b. Measure\
    \ round-trip time from servers (some estimate is better\n         than none).\
    \ Treat no response as a \"very large\" round-trip\n         time.\n      c. Maintain\
    \ a weighted rtt estimate and decay the \"large\" value\n         slowly, with\
    \ time, so that the server is eventually tested\n         again, but not after\
    \ an indefinitely long period.\n      d. Follow an exponential back-off scheme\
    \ so that even if you do\n         not restrict the number of queries, you do\
    \ not overload the\n         net excessively.\n"
- title: '5. Cache Leaks:'
  contents:
  - "5. Cache Leaks:\n   Every resource record returned by a server is cached for\
    \ TTL seconds,\n   where the TTL value is returned with the RR. Full-service (or\
    \ stub)\n   resolvers cache the RR and answer any queries based on this cached\n\
    \   information, in the future, until the TTL expires. After that, one\n   more\
    \ query to the wide-area network gets the RR in cache again.\n   Full-service\
    \ resolvers might not implement this caching mechanism\n   well. They might impose\
    \ a limit on the cache size or might not\n   interpret the TTL value correctly.\
    \ In either case, queries repeated\n   within a TTL period of a RR constitute\
    \ a cache leak.\n"
- title: 'A GOOD/BAD IMPLEMENTATION:'
  contents:
  - "A GOOD/BAD IMPLEMENTATION:\n   BIND has no restriction on the cache size and\
    \ the size is governed by\n   the limits on the virtual address space of the machine\
    \ it is running\n   on. BIND caches RRs for the duration of the TTL returned with\
    \ each\n   record.\n   It does, however, not follow the RFCs with respect to interpretation\n\
    \   of a 0 TTL value. If a record has a TTL value of 0 seconds, BIND uses\n  \
    \ the minimum TTL value, for that zone, from the SOA record and caches\n   it\
    \ for that duration. This, though it saves some traffic on the\n   wide-area network,\
    \ is not correct behavior.\n"
- title: 'FIXES:'
  contents:
  - "FIXES:\n      a. Look over your caching mechanism to ensure TTLs are interpreted\n\
    \         correctly.\n      b. Do not restrict cache sizes (come on, memory is\
    \ cheap!).\n         Expired entries are reclaimed periodically, anyway. Of course,\n\
    \         the cache size is bound to have some physical limit. But, when\n   \
    \      possible, this limit should be large (run your name server on\n       \
    \  a machine with a large amount of physical memory).\n      c. Possibly, a mechanism\
    \ is needed to flush the cache, when it is\n         known or even suspected that\
    \ the information has changed.\n"
- title: '6. Name Error Bugs:'
  contents:
  - "6. Name Error Bugs:\n   This bug is very similar to the Zero Answer bug. A server\
    \ returns an\n   authoritative NXDOMAIN when the queried name is known to be bad,\
    \ by\n   the server authoritative for the domain, in the absence of negative\n\
    \   caching. This authoritative NXDOMAIN response is usually accompanied\n   by\
    \ the SOA record for the domain, in the authority section.\n   Resolvers should\
    \ recognize that the name they queried for was a bad\n   name and should stop\
    \ querying further.\n   Some resolvers might, however, not interpret this correctly\
    \ and\n   continue to query servers, expecting an answer record.\n   Some applications,\
    \ in fact, prompt NXDOMAIN answers! When given a\n   perfectly good name to resolve,\
    \ they append the local domain to it\n   e.g., an application in the domain \"\
    foo.bar.com\", when trying to\n   resolve the name \"usc.edu\" first tries \"\
    usc.edu.foo.bar.com\", then\n   \"usc.edu.bar.com\" and finally the good name\
    \ \"usc.edu\". This causes at\n   least two queries that return NXDOMAIN, for\
    \ every good query. The\n   problem is aggravated since the negative answers from\
    \ the previous\n   queries are not cached.  When the same name is sought again,\
    \ the\n   process repeats.\n   Some DNS resolver implementations suffer from this\
    \ problem, too. They\n   append successive sub-parts of the local domain using\
    \ an implicit\n   searchlist mechanism, when certain conditions are satisfied\
    \ and try\n   the original name, only when this first set of iterations fails.\
    \ This\n   behavior recently caused pandemonium in the Internet when the domain\n\
    \   \"edu.com\" was registered and a wildcard \"CNAME\" record placed at the\n\
    \   top level. All machines from \"com\" domains trying to connect to hosts\n\
    \   in the \"edu\" domain ended up with connections to the local machine in\n\
    \   the \"edu.com\" domain!\n"
- title: 'GOOD/BAD IMPLEMENTATIONS:'
  contents:
  - "GOOD/BAD IMPLEMENTATIONS:\n   Some local versions of BIND already implement negative\
    \ caching. They\n   typically cache negative answers with a very small TTL, sufficient\
    \ to\n   answer a burst of queries spaced close together, as is typically\n  \
    \ seen.\n   The next official public release of BIND (4.9.2) will have negative\n\
    \   caching as an ifdef'd feature.\n   The BIND resolver appends local domain\
    \ to the given name, when one of\n   two conditions is met:\n      i.  The name\
    \ has no periods and the flag RES_DEFNAME is set.\n      ii. There is no trailing\
    \ period and the flag RES_DNSRCH is set.\n   The flags RES_DEFNAME and RES_DNSRCH\
    \ are default resolver options, in\n   BIND, but can be changed at compile time.\n\
    \   Only if the name, so generated, returns an NXDOMAIN is the original\n   name\
    \ tried as a Fully Qualified Domain Name. And only if it contains\n   at least\
    \ one period.\n"
- title: 'FIXES:'
  contents:
  - "FIXES:\n      a. Fix the resolver code.\n      b. Negative Caching. Negative\
    \ caching servers will restrict the\n         traffic seen on the wide-area network,\
    \ even if not curb it\n         altogether.\n      c. Applications and resolvers\
    \ should not append the local domain to\n         names they seek to resolve,\
    \ as far as possible. Names\n         interspersed with periods should be treated\
    \ as Fully Qualified\n         Domain Names.\n         In other words, Use searchlists\
    \ only when explicitly specified.\n         No implicit searchlists should be\
    \ used. A name that contains\n         any dots should first be tried as a FQDN\
    \ and if that fails, with\n         the local domain name (or searchlist if specified)\
    \ appended. A\n         name containing no dots can be appended with the searchlist\
    \ right\n         away, but once again, no implicit searchlists should be used.\n\
    \   Associated with the name error bug is another problem where a server\n   might\
    \ return an authoritative NXDOMAIN, although the name is valid. A\n   secondary\
    \ server, on start-up, reads the zone information from the\n   primary, through\
    \ a zone transfer. While it is in the process of\n   loading the zones, it does\
    \ not have information about them, although\n   it is authoritative for them.\
    \  Thus, any query for a name in that\n   domain is answered with an NXDOMAIN\
    \ response code. This problem might\n   not be disastrous were it not for negative\
    \ caching servers that cache\n   this answer and so propagate incorrect information\
    \ over the internet.\n"
- title: 'BAD IMPLEMENTATION:'
  contents:
  - "BAD IMPLEMENTATION:\n   BIND apparently suffers from this problem.\n   Also,\
    \ a new name added to the primary database will take a while to\n   propagate\
    \ to the secondaries. Until that time, they will return\n   NXDOMAIN answers for\
    \ a good name. Negative caching servers store this\n   answer, too and aggravate\
    \ this problem further. This is probably a\n   more general DNS problem but is\
    \ apparently more harmful in this\n   situation.\n"
- title: 'FIX:'
  contents:
  - "FIX:\n      a. Servers should start answering only after loading all the zone\n\
    \         data. A failed server is better than a server handing out\n        \
    \ incorrect information.\n      b. Negative cache records for a very small time,\
    \ sufficient only\n         to ward off a burst of requests for the same bad name.\
    \ This\n         could be related to the round-trip time of the server from\n\
    \         which the negative answer was received. Alternatively, a\n         statistical\
    \ measure of the amount of time for which queries\n         for such names are\
    \ received could be used. Minimum TTL value\n         from the SOA record is not\
    \ advisable since they tend to be\n         pretty large.\n      c. A \"PUSH\"\
    \ (or, at least, a \"NOTIFY\") mechanism should be allowed\n         and implemented,\
    \ to allow the primary server to inform\n         secondaries that the database\
    \ has been modified since it last\n         transferred zone data.  To alleviate\
    \ the problem of \"too many\n         zone transfers\" that this might cause,\
    \ Incremental Zone\n         Transfers should also be part of DNS.  Also, the\
    \ primary should\n         not NOTIFY/PUSH with every update but bunch a good\
    \ number\n         together.\n"
- title: '7. Format Errors:'
  contents:
  - "7. Format Errors:\n   Some resolvers issue query packets that do not necessarily\
    \ conform to\n   standards as laid out in the relevant RFCs. This unnecessarily\n\
    \   increases net traffic and wastes server time.\n"
- title: 'FIXES:'
  contents:
  - "FIXES:\n      a. Fix resolvers.\n      b. Each resolver verify format of packets\
    \ before sending them out,\n         using a mechanism outside of the resolver.\
    \ This is, obviously,\n         needed only if step 1 cannot be followed.\n"
- title: References
  contents:
  - "References\n   [1] Mockapetris, P., \"Domain Names Concepts and Facilities\"\
    , STD 13,\n       RFC 1034, USC/Information Sciences Institute, November 1987.\n\
    \   [2] Mockapetris, P., \"Domain Names Implementation and Specification\",\n\
    \       STD 13, RFC 1035, USC/Information Sciences Institute, November\n     \
    \  1987.\n   [3] Partridge, C., \"Mail Routing and the Domain System\", STD 14,\
    \ RFC\n       974, CSNET CIC BBN, January 1986.\n   [4] Gavron, E., \"A Security\
    \ Problem and Proposed Correction With\n       Widely Deployed DNS Software\"\
    , RFC 1535, ACES Research Inc.,\n       October 1993.\n   [5] Beertema, P., \"\
    Common DNS Data File Configuration Errors\", RFC\n       1537, CWI, October 1993.\n"
- title: Security Considerations
  contents:
  - "Security Considerations\n   Security issues are not discussed in this memo.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Anant Kumar\n   USC Information Sciences Institute\n \
    \  4676 Admiralty Way\n   Marina Del Rey CA 90292-6695\n   Phone:(310) 822-1511\n\
    \   FAX:  (310) 823-6741\n   EMail: anant@isi.edu\n   Jon Postel\n   USC Information\
    \ Sciences Institute\n   4676 Admiralty Way\n   Marina Del Rey CA 90292-6695\n\
    \   Phone:(310) 822-1511\n   FAX:  (310) 823-6714\n   EMail: postel@isi.edu\n\
    \   Cliff Neuman\n   USC Information Sciences Institute\n   4676 Admiralty Way\n\
    \   Marina Del Rey CA 90292-6695\n   Phone:(310) 822-1511\n   FAX:  (310) 823-6714\n\
    \   EMail: bcn@isi.edu\n   Peter Danzig\n   Computer Science Department\n   University\
    \ of Southern California\n   University Park\n   EMail: danzig@caldera.usc.edu\n\
    \   Steve Miller\n   Computer Science Department\n   University of Southern California\n\
    \   University Park\n   Los Angeles CA 90089\n   EMail: smiller@caldera.usc.edu\n"
