- title: __initial_text__
  contents:
  - "   Performance Evaluation of Explicit Congestion Notification (ECN)\n       \
    \                      in IP Networks\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2000).  All Rights Reserved.\n"
- title: Abstract
  contents:
  - "Abstract\n   This memo presents a performance study of the Explicit Congestion\n\
    \   Notification (ECN) mechanism in the TCP/IP protocol using our\n   implementation\
    \ on the Linux Operating System. ECN is an end-to-end\n   congestion avoidance\
    \ mechanism proposed by [6] and incorporated into\n   RFC 2481[7]. We study the\
    \ behavior of ECN for both bulk and\n   transactional transfers. Our experiments\
    \ show that there is\n   improvement in throughput over NON ECN (TCP employing\
    \ any of Reno,\n   SACK/FACK or NewReno congestion control) in the case of bulk\n\
    \   transfers and substantial improvement for transactional transfers.\n   A more\
    \ complete pdf version of this document is available at:\n   http://www7.nortel.com:8080/CTL/ecnperf.pdf\n\
    \   This memo in its current revision is missing a lot of the visual\n   representations\
    \ and experimental results found in the pdf version.\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   In current IP networks, congestion management is left to\
    \ the\n   protocols running on top of IP. An IP router when congested simply\n\
    \   drops packets.  TCP is the dominant transport protocol today [26].\n   TCP\
    \ infers that there is congestion in the network by detecting\n   packet drops\
    \ (RFC 2581). Congestion control algorithms [11] [15] [21]\n   are then invoked\
    \ to alleviate congestion.  TCP initially sends at a\n   higher rate (slow start)\
    \ until it detects a packet loss. A packet\n   loss is inferred by the receipt\
    \ of 3 duplicate ACKs or detected by a\n   timeout. The sending TCP then moves\
    \ into a congestion avoidance state\n   where it carefully probes the network\
    \ by sending at a slower rate\n   (which goes up until another packet loss is\
    \ detected).  Traditionally\n   a router reacts to congestion by dropping a packet\
    \ in the absence of\n   buffer space. This is referred to as Tail Drop. This method\
    \ has a\n   number of drawbacks (outlined in Section 2). These drawbacks coupled\n\
    \   with the limitations of end-to-end congestion control have led to\n   interest\
    \ in introducing smarter congestion control mechanisms in\n   routers.  One such\
    \ mechanism is Random Early Detection (RED) [9]\n   which detects incipient congestion\
    \ and implicitly signals the\n   oversubscribing flow to slow down by dropping\
    \ its packets. A RED-\n   enabled router detects congestion before the buffer\
    \ overflows, based\n   on a running average queue size, and drops packets probabilistically\n\
    \   before the queue actually fills up. The probability of dropping a new\n  \
    \ arriving packet increases as the average queue size increases above a\n   low\
    \ water mark minth, towards higher water mark maxth. When the\n   average queue\
    \ size exceeds maxth all arriving packets are dropped.\n   An extension to RED\
    \ is to mark the IP header instead of dropping\n   packets (when the average queue\
    \ size is between minth and maxth;\n   above maxth arriving packets are dropped\
    \ as before). Cooperating end\n   systems would then use this as a signal that\
    \ the network is congested\n   and slow down. This is known as Explicit Congestion\
    \ Notification\n   (ECN).  In this paper we study an ECN implementation on Linux\
    \ for\n   both the router and the end systems in a live network.  The memo is\n\
    \   organized as follows. In Section 2 we give an overview of queue\n   management\
    \ in routers. Section 3 gives an overview of ECN and the\n   changes required\
    \ at the router and the end hosts to support ECN.\n   Section 4 defines the experimental\
    \ testbed and the terminologies used\n   throughout this memo. Section 5 introduces\
    \ the experiments that are\n   carried out, outlines the results and presents\
    \ an analysis of the\n   results obtained.  Section 6 concludes the paper.\n"
- title: 2. Queue Management in routers
  contents:
  - "2. Queue Management in routers\n   TCP's congestion control and avoidance algorithms\
    \ are necessary and\n   powerful but are not enough to provide good service in\
    \ all\n   circumstances since they treat the network as a black box. Some sort\n\
    \   of control is required from the routers to complement the end system\n   congestion\
    \ control mechanisms. More detailed analysis is contained in\n   [19].  Queue\
    \ management algorithms traditionally manage the length of\n   packet queues in\
    \ the router by dropping packets only when the buffer\n   overflows.  A maximum\
    \ length for each queue is configured. The router\n   will accept packets till\
    \ this maximum size is exceeded, at which\n   point it will drop incoming packets.\
    \ New packets are accepted when\n   buffer space allows. This technique is known\
    \ as Tail Drop. This\n   method has served the Internet well for years, but has\
    \ the several\n   drawbacks.  Since all arriving packets (from all flows) are\
    \ dropped\n   when the buffer overflows, this interacts badly with the congestion\n\
    \   control mechanism of TCP. A cycle is formed with a burst of drops\n   after\
    \ the maximum queue size is exceeded, followed by a period of\n   underutilization\
    \ at the router as end systems back off. End systems\n   then increase their windows\
    \ simultaneously up to a point where a\n   burst of drops happens again. This\
    \ phenomenon is called Global\n   Synchronization. It leads to poor link utilization\
    \ and lower overall\n   throughput [19] Another problem with Tail Drop is that\
    \ a single\n   connection or a few flows could monopolize the queue space, in\
    \ some\n   circumstances. This results in a lock out phenomenon leading to\n \
    \  synchronization or other timing effects [19].  Lastly, one of the\n   major\
    \ drawbacks of Tail Drop is that queues remain full for long\n   periods of time.\
    \ One of the major goals of queue management is to\n   reduce the steady state\
    \ queue size[19].  Other queue management\n   techniques include random drop on\
    \ full and drop front on full [13].\n"
- title: 2.1. Active Queue Management
  contents:
  - "2.1. Active Queue Management\n   Active queue management mechanisms detect congestion\
    \ before the queue\n   overflows and provide an indication of this congestion\
    \ to the end\n   nodes [7]. With this approach TCP does not have to rely only\
    \ on\n   buffer overflow as the indication of congestion since notification\n\
    \   happens before serious congestion occurs. One such active management\n   technique\
    \ is RED.\n"
- title: 2.1.1. Random Early Detection
  contents:
  - "2.1.1. Random Early Detection\n   Random Early Detection (RED) [9] is a congestion\
    \ avoidance mechanism\n   implemented in routers which works on the basis of active\
    \ queue\n   management. RED addresses the shortcomings of Tail Drop.  A RED\n\
    \   router signals incipient congestion to TCP by dropping packets\n   probabilistically\
    \ before the queue runs out of buffer space. This\n   drop probability is dependent\
    \ on a running average queue size to\n   avoid any bias against bursty traffic.\
    \ A RED router randomly drops\n   arriving packets, with the result that the probability\
    \ of dropping a\n   packet belonging to a particular flow is approximately proportional\n\
    \   to the flow's share of bandwidth. Thus, if the sender is using\n   relatively\
    \ more bandwidth it gets penalized by having more of its\n   packets dropped.\
    \  RED operates by maintaining two levels of\n   thresholds minimum (minth) and\
    \ maximum (maxth). It drops a packet\n   probabilistically if and only if the\
    \ average queue size lies between\n   the minth and maxth thresholds. If the average\
    \ queue size is above\n   the maximum threshold, the arriving packet is always\
    \ dropped. When\n   the average queue size is between the minimum and the maximum\n\
    \   threshold, each arriving packet is dropped with probability pa, where\n  \
    \ pa is a function of the average queue size. As the average queue\n   length\
    \ varies between minth and maxth, pa increases linearly towards\n   a configured\
    \ maximum drop probability, maxp. Beyond maxth, the drop\n   probability is 100%.\
    \  Dropping packets in this way ensures that when\n   some subset of the source\
    \ TCP packets get dropped and they invoke\n   congestion avoidance algorithms\
    \ that will ease the congestion at the\n   gateway. Since the dropping is distributed\
    \ across flows, the problem\n   of global synchronization is avoided.\n"
- title: 3. Explicit Congestion Notification
  contents:
  - "3. Explicit Congestion Notification\n   Explicit Congestion Notification is an\
    \ extension proposed to RED\n   which marks a packet instead of dropping it when\
    \ the average queue\n   size is between minth and maxth [7]. Since ECN marks packets\
    \ before\n   congestion actually occurs, this is useful for protocols like TCP\n\
    \   that are sensitive to even a single packet loss. Upon receipt of a\n   congestion\
    \ marked packet, the TCP receiver informs the sender (in the\n   subsequent ACK)\
    \ about incipient congestion which will in turn trigger\n   the congestion avoidance\
    \ algorithm at the sender.  ECN requires\n   support from both the router as well\
    \ as the end hosts, i.e.  the end\n   hosts TCP stack needs to be modified. Packets\
    \ from flows that are not\n   ECN capable will continue to be dropped by RED (as\
    \ was the case\n   before ECN).\n"
- title: 3.1. Changes at the router
  contents:
  - "3.1. Changes at the router\n   Router side support for ECN can be added by modifying\
    \ current RED\n   implementations. For packets from ECN capable hosts, the router\
    \ marks\n   the packets rather than dropping them (if the average queue size is\n\
    \   between minth and maxth).  It is necessary that the router identifies\n  \
    \ that a packet is ECN capable, and should only mark packets that are\n   from\
    \ ECN capable hosts. This uses two bits in the IP header.  The ECN\n   Capable\
    \ Transport (ECT) bit is set by the sender end system if both\n   the end systems\
    \ are ECN capable (for a unicast transport, only if\n   both end systems are ECN-capable).\
    \ In TCP this is confirmed in the\n   pre-negotiation during the connection setup\
    \ phase (explained in\n   Section 3.2).  Packets encountering congestion are marked\
    \ by the\n   router using the Congestion Experienced (CE) (if the average queue\n\
    \   size is between minth and maxth) on their way to the receiver end\n   system\
    \ (from the sender end system), with a probability proportional\n   to the average\
    \ queue size following the procedure used in RED\n   (RFC2309) routers.  Bits\
    \ 10 and 11 in the IPV6 header are proposed\n   respectively for the ECT and CE\
    \ bits. Bits 6 and 7 of the IPV4 header\n   DSCP field are also specified for\
    \ experimental purposes for the ECT\n   and CE bits respectively.\n"
- title: 3.2. Changes at the TCP Host side
  contents:
  - "3.2. Changes at the TCP Host side\n   The proposal to add ECN to TCP specifies\
    \ two new flags in the\n   reserved field of the TCP header. Bit 9 in the reserved\
    \ field of the\n   TCP header is designated as the ECN-Echo (ECE) flag and Bit\
    \ 8 is\n   designated as the Congestion Window Reduced (CWR) flag.  These two\n\
    \   bits are used both for the initializing phase in which the sender and\n  \
    \ the receiver negotiate the capability and the desire to use ECN, as\n   well\
    \ as for the subsequent actions to be taken in case there is\n   congestion experienced\
    \ in the network during the established state.\n   There are two main changes\
    \ that need to be made to add ECN to TCP to\n   an end system and one extension\
    \ to a router running RED.\n   1. In the connection setup phase, the source and\
    \ destination TCPs\n   have to exchange information about their desire and/or\
    \ capability to\n   use ECN. This is done by setting both the ECN-Echo flag and\
    \ the CWR\n   flag in the SYN packet of the initial connection phase by the sender;\n\
    \   on receipt of this SYN packet, the receiver will set the ECN-Echo\n   flag\
    \ in the SYN-ACK response. Once this agreement has been reached,\n   the sender\
    \ will thereon set the ECT bit in the IP header of data\n   packets for that flow,\
    \ to indicate to the network that it is capable\n   and willing to participate\
    \ in ECN. The ECT bit is set on all packets\n   other than pure ACK's.\n   2.\
    \ When a router has decided from its active queue management\n   mechanism, to\
    \ drop or mark a packet, it checks the IP-ECT bit in the\n   packet header. It\
    \ sets the CE bit in the IP header if the IP-ECT bit\n   is set. When such a packet\
    \ reaches the receiver, the receiver\n   responds by setting the ECN-Echo flag\
    \ (in the TCP header) in the next\n   outgoing ACK for the flow. The receiver\
    \ will continue to do this in\n   subsequent ACKs until it receives from the sender\
    \ an indication that\n   it (the sender) has responded to the congestion notification.\n\
    \   3. Upon receipt of this ACK, the sender triggers its congestion\n   avoidance\
    \ algorithm by halving its congestion window, cwnd, and\n   updating its congestion\
    \ window threshold value ssthresh. Once it has\n   taken these appropriate steps,\
    \ the sender sets the CWR bit on the\n   next data outgoing packet to tell the\
    \ receiver that it has reacted to\n   the (receiver's) notification of congestion.\
    \  The receiver reacts to\n   the CWR by halting the sending of the congestion\
    \ notifications (ECE)\n   to the sender if there is no new congestion in the network.\n\
    \   Note that the sender reaction to the indication of congestion in the\n   network\
    \ (when it receives an ACK packet that has the ECN-Echo flag\n   set) is equivalent\
    \ to the Fast Retransmit/Recovery algorithm (when\n   there is a congestion loss)\
    \ in NON-ECN-capable TCP i.e. the sender\n   halves the congestion window cwnd\
    \ and reduces the slow start\n   threshold ssthresh. Fast Retransmit/Recovery\
    \ is still available for\n   ECN capable stacks for responding to three duplicate\
    \ acknowledgments.\n"
- title: 4. Experimental setup
  contents:
  - "4. Experimental setup\n   For testing purposes we have added ECN to the Linux\
    \ TCP/IP stack,\n   kernels version 2.0.32. 2.2.5, 2.3.43 (there were also earlier\n\
    \   revisions of 2.3 which were tested).  The 2.0.32 implementation\n   conforms\
    \ to RFC 2481 [7] for the end systems only. We have also\n   modified the code\
    \ in the 2.1,2.2 and 2.3 cases for the router portion\n   as well as end system\
    \ to conform to the RFC. An outdated version of\n   the 2.0 code is available\
    \ at [18].  Note Linux version 2.0.32\n   implements TCP Reno congestion control\
    \ while kernels >= 2.2.0 default\n   to New Reno but will opt for a SACK/FACK\
    \ combo when the remote end\n   understands SACK.  Our initial tests were carried\
    \ out with the 2.0\n   kernel at the end system and 2.1 (pre 2.2) for the router\
    \ part.  The\n   majority of the test results here apply to the 2.0 tests. We\
    \  did\n   repeat these tests on a different testbed (move from Pentium to\n \
    \  Pentium-II class machines)with faster machines for the 2.2 and 2.3\n   kernels,\
    \ so the comparisons on the 2.0 and 2.2/3 are not relative.\n   We have updated\
    \ this memo release to reflect the tests against SACK\n   and New Reno.\n"
- title: 4.1. Testbed setup
  contents:
  - "4.1. Testbed setup\n                                             -----      ----\n\
    \                                            | ECN |    | ECN |\n            \
    \                                | ON  |    | OFF |\n          data direction\
    \ ---->>              -----      ----\n                                      \
    \        |          |\n      server                                  |       \
    \   |\n       ----        ------        ------       |          |\n      |   \
    \ |      |  R1  |      |  R2  |      |          |\n      |    | -----|      |\
    \ ---- |      | ----------------------\n       ----        ------ ^      ------\
    \             |\n                          ^                         |\n     \
    \                     |                        -----\n      congestion point ___|\
    \                       |  C  |\n                                            \
    \      |     |\n                                                   -----\n   The\
    \ figure above shows our test setup.\n   All the physical links are 10Mbps ethernet.\
    \  Using Class Based\n   Queuing (CBQ) [22], packets from the data server are\
    \ constricted to a\n   1.5Mbps pipe at the router R1. Data is always retrieved\
    \ from the\n   server towards the clients labelled , \"ECN ON\", \"ECN OFF\",\
    \ and \"C\".\n   Since the pipe from the server is 10Mbps, this creates congestion\
    \ at\n   the exit from the router towards the clients for competing flows. The\n\
    \   machines labeled \"ECN ON\" and \"ECN OFF\"  are running the same version\n\
    \   of Linux and have exactly the same hardware configuration. The server\n  \
    \ is always ECN capable (and can handle NON ECN flows as well using the\n   standard\
    \ congestion algorithms). The machine labeled \"C\" is used to\n   create congestion\
    \ in the network. Router R2 acts as a path-delay\n   controller.  With it we adjust\
    \ the RTT the clients see.  Router R1\n   has RED implemented in it and has capability\
    \ for supporting ECN\n   flows.  The path-delay router is a PC running the Nistnet\
    \ [16]\n   package on a Linux platform. The latency of the link for the\n   experiments\
    \ was set to be 20 millisecs.\n"
- title: 4.2. Validating the Implementation
  contents:
  - "4.2. Validating the Implementation\n   We spent time validating that the implementation\
    \ was conformant to\n   the specification in RFC 2481. To do this, the popular\
    \ tcpdump\n   sniffer [24] was modified to show the packets being marked. We\n\
    \   visually inspected tcpdump traces to validate the conformance to the\n   RFC\
    \ under a lot of different scenarios.  We also modified tcptrace\n   [25] in order\
    \ to plot the marked packets for visualization and\n   analysis.\n   Both tcpdump\
    \ and tcptrace revealed that the implementation was\n   conformant to the RFC.\n"
- title: 4.3. Terminology used
  contents:
  - "4.3. Terminology used\n   This section presents background terminology used in\
    \ the next few\n   sections.\n   * Congesting flows: These are TCP flows that\
    \ are started in the\n   background so as to create congestion from R1 towards\
    \ R2. We use the\n   laptop labeled \"C\" to introduce congesting flows. Note\
    \ that \"C\" as is\n   the case with the other clients retrieves data from the\
    \ server.\n   * Low, Moderate and High congestion: For the case of low congestion\n\
    \   we start two congesting flows in the background, for moderate\n   congestion\
    \ we start five congesting flows and for the case of high\n   congestion we start\
    \ ten congesting flows in the background.\n   * Competing flows: These are the\
    \ flows that we are interested in.\n   They are either ECN TCP flows from/to \"\
    ECN ON\" or NON ECN TCP flows\n   from/to \"ECN OFF\".\n   * Maximum drop rate:\
    \ This is the RED parameter that sets the maximum\n   probability of a packet\
    \ being marked at the router. This corresponds\n   to maxp as explained in Section\
    \ 2.1.\n   Our tests were repeated for varying levels of congestion with varying\n\
    \   maximum drop rates. The results are presented in the subsequent\n   sections.\n\
    \   * Low, Medium and High drop probability: We use the term low\n   probability\
    \ to mean a drop probability maxp of 0.02, medium\n   probability for 0.2 and\
    \ high probability for 0.5. We also\n   experimented with drop probabilities of\
    \ 0.05, 0.1 and 0.3.\n   * Goodput: We define goodput as the effective data rate\
    \ as observed\n   by the user, i.e., if we transmitted 4 data packets in which\
    \ two of\n   them were retransmitted packets, the efficiency is 50% and the\n\
    \   resulting goodput is 2*packet size/time taken to transmit.\n   * RED Region:\
    \ When the router's average queue size is between minth\n   and maxth we denote\
    \ that we are operating in the RED region.\n"
- title: 4.4. RED parameter selection
  contents:
  - "4.4. RED parameter selection\n   In our initial testing we noticed that as we\
    \ increase the number of\n   congesting flows the RED queue degenerates into a\
    \ simple Tail Drop\n   queue.  i.e. the average queue exceeds the maximum threshold\
    \ most of\n   the times.  Note that this phenomena has also been observed by [5]\n\
    \   who proposes a dynamic solution to alleviate it by adjusting the\n   packet\
    \ dropping probability \"maxp\" based on the past history of the\n   average queue\
    \ size.  Hence, it is necessary that in the course of our\n   experiments the\
    \ router operate in the RED region, i.e., we have to\n   make sure that the average\
    \ queue is maintained between minth and\n   maxth. If this is not maintained,\
    \ then the queue acts like a Tail\n   Drop queue and the advantages of ECN diminish.\
    \ Our goal is to\n   validate ECN's benefits when used with RED at the router.\
    \  To ensure\n   that we were operating in the RED region we monitored the average\n\
    \   queue size and the actual queue size in times of low, moderate and\n   high\
    \ congestion and fine-tuned the RED parameters such that the\n   average queue\
    \ zones around the RED region before running the\n   experiment proper.  Our results\
    \ are, therefore, not influenced by\n   operating in the wrong RED region.\n"
- title: 5. The Experiments
  contents:
  - "5. The Experiments\n   We start by making sure that the background flows do not\
    \ bias our\n   results by computing the fairness index [12] in Section 5.1. We\n\
    \   proceed to carry out the experiments for bulk transfer presenting the\n  \
    \ results and analysis in Section 5.2. In Section 5.3 the results for\n   transactional\
    \ transfers along with analysis is presented.  More\n   details on the experimental\
    \ results can be found in [27].\n"
- title: 5.1. Fairness
  contents:
  - "5.1. Fairness\n   In the course of the experiments we wanted to make sure that\
    \ our\n   choice of the type of background flows does not bias the results that\n\
    \   we collect.  Hence we carried out some tests initially with both ECN\n   and\
    \ NON ECN flows as the background flows. We repeated the\n   experiments for different\
    \ drop probabilities and calculated the\n   fairness index [12].  We also noticed\
    \ (when there were equal number\n   of ECN and NON ECN flows) that the number\
    \ of packets dropped for the\n   NON ECN flows was equal to the number of packets\
    \ marked for the ECN\n   flows, showing thereby that the RED algorithm was fair\
    \ to both kind\n   of flows.\n   Fairness index: The fairness index is a performance\
    \ metric described\n   in [12].  Jain [12] postulates that the network is a multi-user\n\
    \   system, and derives a metric to see how fairly each user is treated.\n   He\
    \ defines fairness as a function of the variability of throughput\n   across users.\
    \ For a given set of user throughputs (x1, x2...xn), the\n   fairness index to\
    \ the set is defined as follows:\n   f(x1,x2,.....,xn) = square((sum[i=1..n]xi))/(n*sum[i=1..n]square(xi))\n\
    \   The fairness index always lies between 0 and 1. A value of 1\n   indicates\
    \ that all flows got exactly the same throughput.  Each of\n   the tests was carried\
    \ out 10 times to gain confidence in our results.\n   To compute the fairness\
    \ index we used FTP to generate traffic.\n   Experiment details: At time t = 0\
    \ we start 2 NON ECN FTP sessions in\n   the background to create congestion.\
    \ At time t=20 seconds we start\n   two competing flows. We note the throughput\
    \ of all the flows in the\n   network and calculate the fairness index. The experiment\
    \ was carried\n   out for various maximum drop probabilities and for various congestion\n\
    \   levels.  The same procedure is repeated with the background flows as\n   ECN.\
    \ The fairness index was fairly constant in both the cases when\n   the background\
    \ flows were ECN and NON ECN indicating that there was\n   no bias when the background\
    \ flows were either ECN or NON ECN.\n   Max     Fairness                Fairness\n\
    \   Drop    With BG                 With BG\n   Prob    flows ECN            \
    \   flows NON ECN\n   0.02    0.996888                0.991946\n   0.05    0.995987\
    \                0.988286\n   0.1     0.985403                0.989726\n   0.2\
    \     0.979368                0.983342\n   With the observation that the nature\
    \ of background flows does not\n   alter the results, we proceed by using the\
    \ background flows as NON\n   ECN for the rest of the experiments.\n"
- title: 5.2. Bulk transfers
  contents:
  - "5.2. Bulk transfers\n   The metric we chose for bulk transfer is end user throughput.\n\
    \   Experiment Details: All TCP flows used are RENO TCP. For the case of\n   low\
    \ congestion we start 2 FTP flows in the background at time 0. Then\n   after\
    \ about 20 seconds we start the competing flows, one data\n   transfer to the\
    \ ECN machine and the second to the NON ECN machine.\n   The size of the file\
    \ used is 20MB. For the case of moderate\n   congestion we start 5 FTP flows in\
    \ the background and for the case of\n   high congestion we start 10 FTP flows\
    \ in the background. We repeat\n   the experiments for various maximum drop rates\
    \ each repeated for a\n   number of sets.\n   Observation and Analysis:\n   We\
    \ make three key observations:\n   1) As the congestion level increases, the relative\
    \ advantage for ECN\n   increases but the absolute advantage decreases (expected,\
    \ since there\n   are more flows competing for the same link resource). ECN still\
    \ does\n   better than NON ECN even under high congestion.  Infering a sample\n\
    \   from the collected results: at maximum drop probability of 0.1, for\n   example,\
    \ the relative advantage of ECN increases from 23% to 50% as\n   the congestion\
    \ level increases from low to high.\n   2) Maintaining congestion levels and varying\
    \ the maximum drop\n   probability (MDP) reveals that the relative advantage of\
    \ ECN\n   increases with increasing MDP. As an example, for the case of high\n\
    \   congestion as we vary the drop probability from 0.02 to 0.5 the\n   relative\
    \ advantage of ECN increases from 10% to 60%.\n   3) There were hardly any retransmissions\
    \ for ECN flows (except the\n   occasional packet drop in a minority of the tests\
    \ for the case of\n   high congestion and low maximum drop probability).\n   We\
    \ analyzed tcpdump traces for NON ECN with the help of tcptrace and\n   observed\
    \ that there were hardly any retransmits due to timeouts.\n   (Retransmit due\
    \ to timeouts are inferred by counting the number of 3\n   DUPACKS retransmit\
    \ and subtracting them from the total recorded\n   number of retransmits).  This\
    \ means that over a long period of time\n   (as is the case of long bulk transfers),\
    \ the data-driven loss\n   recovery mechanism of the Fast Retransmit/Recovery\
    \ algorithm is very\n   effective.  The algorithm for ECN on congestion notification\
    \ from ECE\n   is the same as that for a Fast Retransmit for NON ECN. Since both\
    \ are\n   operating in the RED region, ECN barely gets any advantage over NON\n\
    \   ECN from the signaling (packet drop vs. marking).\n   It is clear, however,\
    \ from the results that ECN flows benefit in bulk\n   transfers.  We believe that\
    \ the main advantage of ECN for bulk\n   transfers is that less time is spent\
    \ recovering (whereas NON ECN\n   spends time retransmitting), and timeouts are\
    \ avoided altogether.\n   [23] has shown that even with RED deployed, TCP RENO\
    \ could suffer\n   from multiple packet drops within the same window of data,\
    \ likely to\n   lead to multiple congestion reactions or timeouts (these problems\
    \ are\n   alleviated by ECN). However, while TCP Reno has performance problems\n\
    \   with multiple packets dropped in a window of data, New Reno and SACK\n   have\
    \ no such problems.\n   Thus, for scenarios with very high levels of congestion,\
    \ the\n   advantages of ECN for TCP Reno flows could be more dramatic than the\n\
    \   advantages of ECN for NewReno or SACK flows.  An important\n   observation\
    \ to make from our results is that we do not notice\n   multiple drops within\
    \ a single window of data. Thus, we would expect\n   that our results are not\
    \ heavily influenced by Reno's performance\n   problems with multiple packets\
    \ dropped from a window of data.  We\n   repeated these tests with ECN patched\
    \ newer Linux kernels. As\n   mentioned earlier these kernels would use a SACK/FACK\
    \ combo with a\n   fallback to New Reno.  SACK can be selectively turned off (defaulting\n\
    \   to New Reno).  Our results indicate that ECN still improves\n   performance\
    \ for the bulk transfers. More results are available in the\n   pdf version[27].\
    \ As in 1) above, maintaining a maximum drop\n   probability of 0.1 and increasing\
    \ the congestion level, it is\n   observed that ECN-SACK improves performance\
    \ from about 5% at low\n   congestion to about 15% at high congestion. In the\
    \ scenario where\n   high congestion is maintained and the maximum drop probability\
    \ is\n   moved from 0.02 to 0.5, the relative advantage of ECN-SACK improves\n\
    \   from 10% to 40%.  Although this numbers are lower than the ones\n   exhibited\
    \ by Reno, they do reflect the improvement that ECN offers\n   even in the presence\
    \ of robust recovery mechanisms such as SACK.\n"
- title: 5.3. Transactional transfers
  contents:
  - "5.3. Transactional transfers\n   We model transactional transfers by sending\
    \ a small request and\n   getting a response from a server before sending the\
    \ next request. To\n   generate transactional transfer traffic we use Netperf\
    \ [17] with the\n   CRR (Connect Request Response) option.  As an example let\
    \ us assume\n   that we are retrieving a small file of say 5 - 20 KB, then in\
    \ effect\n   we send a small request to the server and the server responds by\n\
    \   sending us the file. The transaction is complete when we receive the\n   complete\
    \ file. To gain confidence in our results we carry the\n   simulation for about\
    \ one hour. For each test there are a few thousand\n   of these requests and responses\
    \ taking place.  Although not exactly\n   modeling HTTP 1.0 traffic, where several\
    \ concurrent sessions are\n   opened, Netperf-CRR is nevertheless a close approximation.\
    \  Since\n   Netperf-CRR waits for one connection to complete before opening the\n\
    \   next one (0 think time), that single connection could be viewed as\n   the\
    \ slowest response in the set of the opened concurrent sessions (in\n   HTTP).\
    \  The transactional data sizes were selected based on [2] which\n   indicates\
    \ that the average web transaction was around 8 - 10 KB; The\n   smaller (5KB)\
    \ size was selected to guestimate the size of\n   transactional processing that\
    \ may become prevalent with policy\n   management schemes in the diffserv [4]\
    \ context.  Using Netperf we are\n   able to initiate these kind of transactional\
    \ transfers for a variable\n   length of time. The main metric of interest in\
    \ this case is the\n   transaction rate, which is recorded by Netperf.\n   * Define\
    \ Transaction rate as: The number of requests and complete\n   responses for a\
    \ particular requested size that we are able to do per\n   second. For example\
    \ if our request is of 1KB and the response is 5KB\n   then we define the transaction\
    \ rate as the number of such complete\n   transactions that we can accomplish\
    \ per second.\n   Experiment Details: Similar to the case of bulk transfers we\
    \ start\n   the background FTP flows to introduce the congestion in the network\n\
    \   at time 0. About 20 seconds later we start the transactional\n   transfers\
    \ and run each test for three minutes. We record the\n   transactions per second\
    \ that are complete. We repeat the test for\n   about an hour and plot the various\
    \ transactions per second, averaged\n   out over the runs. The experiment is repeated\
    \ for various maximum\n   drop probabilities, file sizes and various levels of\
    \ congestion.\n   Observation and Analysis\n   There are three key observations:\n\
    \   1) As congestion increases (with fixed drop probability) the relative\n  \
    \ advantage for ECN increases (again the absolute advantage does not\n   increase\
    \ since more flows are sharing the same bandwidth). For\n   example, from the\
    \ results, if we consider the 5KB transactional flow,\n   as we increase the congestion\
    \ from medium congestion (5 congesting\n   flows) to high congestion (10 congesting\
    \ flows) for a maximum drop\n   probability of 0.1 the relative gain for ECN increases\
    \ from 42% to\n   62%.\n   2) Maintaining the congestion level while adjusting\
    \ the maximum drop\n   probability indicates that the relative advantage for ECN\
    \ flows\n   increase.  From the case of high congestion for the 5KB flow we\n\
    \   observe that the number of transactions per second increases from 0.8\n  \
    \ to 2.2 which corresponds to an increase in relative gain for ECN of\n   20%\
    \ to 140%.\n   3) As the transactional data size increases, ECN's advantage\n\
    \   diminishes because the probability of recovering from a Fast\n   Retransmit\
    \ increases for NON ECN. ECN, therefore, has a huge\n   advantage as the transactional\
    \ data size gets smaller as is observed\n   in the results.  This can be explained\
    \ by looking at TCP recovery\n   mechanisms.  NON ECN in the short flows depends,\
    \ for recovery, on\n   congestion signaling via receiving 3 duplicate ACKs, or\
    \ worse by a\n   retransmit timer expiration, whereas ECN depends mostly on the\
    \ TCP-\n   ECE flag. This is by design in our experimental setup.  [3] shows\n\
    \   that most of the TCP loss recovery in fact happens in timeouts for\n   short\
    \ flows. The effectiveness of the Fast Retransmit/Recovery\n   algorithm is limited\
    \ by the fact that there might not be enough data\n   in the pipe to elicit 3\
    \ duplicate ACKs.  TCP RENO needs at least 4\n   outstanding packets to recover\
    \ from losses without going into a\n   timeout. For 5KB (4 packets for MTU of\
    \ 1500Bytes) a NON ECN flow will\n   always have to wait for a retransmit timeout\
    \ if any of its packets\n   are lost. ( This timeout could only have been avoided\
    \ if the flow had\n   used an initial window of four packets, and the first of\
    \ the four\n   packets was the packet dropped).  We repeated these experiments\
    \ with\n   the kernels implementing SACK/FACK and New Reno algorithms. Our\n \
    \  observation was that there was hardly any difference with what we saw\n   with\
    \ Reno. For example in the case of SACK-ECN enabling: maintaining\n   the maximum\
    \ drop probability to 0.1 and increasing the congestion\n   level for the 5KB\
    \ transaction we noticed that the relative gain for\n   the ECN enabled flows\
    \ increases from 47-80%.  If we maintain the\n   congestion level for the 5KB\
    \ transactions and increase the maximum\n   drop probabilities instead, we notice\
    \ that SACKs performance\n   increases from 15%-120%.  It is fair to comment that\
    \ the difference\n   in the testbeds (different machines, same topology) might\
    \ have\n   contributed to the results; however, it is worth noting that the\n\
    \   relative advantage of the SACK-ECN is obvious.\n"
- title: 6. Conclusion
  contents:
  - "6. Conclusion\n   ECN enhancements improve on both bulk and transactional TCP\
    \ traffic.\n   The improvement is more obvious in short transactional type of\
    \ flows\n   (popularly referred to as mice).\n   * Because less retransmits happen\
    \ with ECN, it means less traffic on\n   the network. Although the relative amount\
    \ of data retransmitted in\n   our case is small, the effect could be higher when\
    \ there are more\n   contributing end systems. The absence of retransmits also\
    \ implies an\n   improvement in the goodput. This becomes very important for scenarios\n\
    \   where bandwidth is expensive such as in low bandwidth links.  This\n   implies\
    \ also that ECN lends itself well to applications that require\n   reliability\
    \ but would prefer to avoid unnecessary retransmissions.\n   * The fact that ECN\
    \ avoids timeouts by getting faster notification\n   (as opposed to traditional\
    \ packet dropping inference from 3 duplicate\n   ACKs or, even worse, timeouts)\
    \ implies less time is spent during\n   error recovery - this also improves goodput.\n\
    \   * ECN could be used to help in service differentiation where the end\n   user\
    \ is able to \"probe\" for their target rate faster. Assured\n   forwarding [1]\
    \ in the diffserv working group at the IETF proposes\n   using RED with varying\
    \ drop probabilities as a service\n   differentiation mechanism.  It is possible\
    \ that multiple packets\n   within a single window in TCP RENO could be dropped\
    \ even in the\n   presence of RED, likely leading into timeouts [23]. ECN end\
    \ systems\n   ignore multiple notifications, which help in countering this scenario\n\
    \   resulting in improved goodput. The ECN end system also ends up\n   probing\
    \ the network faster (to reach an optimal bandwidth). [23] also\n   notes that\
    \ RENO is the most widely deployed TCP implementation today.\n   It is clear that\
    \ the advent of policy management schemes introduces\n   new requirements for\
    \ transactional type of applications, which\n   constitute a very short query\
    \ and a response in the order of a few\n   packets. ECN provides advantages to\
    \ transactional traffic as we have\n   shown in the experiments.\n"
- title: 7. Acknowledgements
  contents:
  - "7. Acknowledgements\n   We would like to thank Alan Chapman, Ioannis Lambadaris,\
    \ Thomas Kunz,\n   Biswajit Nandy, Nabil Seddigh, Sally Floyd, and Rupinder Makkar\
    \ for\n   their helpful feedback and valuable suggestions.\n"
- title: 8. Security Considerations
  contents:
  - "8. Security Considerations\n   Security considerations are as discussed in section\
    \ 9 of RFC 2481.\n"
- title: 9. References
  contents:
  - "9. References\n   [1]  Heinanen, J., Finland, T., Baker, F., Weiss, W. and J.\n\
    \        Wroclawski, \"Assured Forwarding PHB Group\", RFC 2597, June 1999.\n\
    \   [2]  B.A. Mat. \"An empirical model of HTTP network traffic.\"  In\n     \
    \   proceedings INFOCOMM'97.\n   [3]  Balakrishnan H., Padmanabhan V., Seshan\
    \ S., Stemn M. and Randy\n        H. Katz, \"TCP Behavior of a busy Internet Server:\
    \ Analysis and\n        Improvements\", Proceedings of IEEE Infocom, San Francisco,\
    \ CA,\n        USA, March '98\n        http://nms.lcs.mit.edu/~hari/papers/infocom98.ps.gz\n\
    \   [4]  Blake, S., Black, D., Carlson, M., Davies, E., Wang, Z. and W.\n    \
    \    Weiss, \"An Architecture for Differentiated Services\", RFC 2475,\n     \
    \   December 1998.\n   [5]  W. Feng, D. Kandlur, D. Saha, K. Shin, \"Techniques\
    \ for\n        Eliminating Packet Loss in Congested TCP/IP Networks\", U.\n  \
    \      Michigan CSE-TR-349-97, November 1997.\n   [6]  S. Floyd. \"TCP and Explicit\
    \ Congestion Notification.\" ACM\n        Computer Communications Review, 24,\
    \ October 1994.\n   [7]  Ramakrishnan, K. and S. Floyd, \"A Proposal to add Explicit\n\
    \        Congestion Notification (ECN) to IP\", RFC 2481, January 1999.\n   [8]\
    \  Kevin Fall, Sally Floyd, \"Comparisons of Tahoe, RENO and Sack\n        TCP\"\
    , Computer  Communications Review, V. 26 N. 3, July 1996,\n        pp. 5-21\n\
    \   [9]  S. Floyd and V. Jacobson. \"Random Early Detection Gateways for\n   \
    \     Congestion Avoidance\". IEEE/ACM Transactions on Networking,\n        3(1),\
    \ August 1993.\n   [10] E. Hashem. \"Analysis of random drop for gateway congestion\n\
    \        control.\" Rep. Lcs tr-465, Lav. Fot Comput. Sci., M.I.T., 1989.\n  \
    \ [11] V. Jacobson. \"Congestion Avoidance and Control.\" In Proceedings\n   \
    \     of SIGCOMM '88, Stanford, CA, August 1988.\n   [12] Raj Jain, \"The art\
    \ of computer systems performance analysis\",\n        John Wiley and sons QA76.9.E94J32,\
    \ 1991.\n   [13] T. V. Lakshman, Arnie Neidhardt, Teunis Ott, \"The Drop From\n\
    \        Front Strategy in TCP Over ATM and Its Interworking with Other\n    \
    \    Control Features\", Infocom 96, MA28.1.\n   [14] P. Mishra and H. Kanakia.\
    \ \"A hop by hop rate based congestion\n        control scheme.\" Proc. SIGCOMM\
    \ '92, pp. 112-123, August 1992.\n   [15] Floyd, S. and T. Henderson, \"The NewReno\
    \ Modification to TCP's\n        Fast Recovery Algorithm\", RFC 2582, April 1999.\n\
    \   [16] The NIST Network Emulation Tool\n        http://www.antd.nist.gov/itg/nistnet/\n\
    \   [17] The network performance tool\n        http://www.netperf.org/netperf/NetperfPage.html\n\
    \   [18] ftp://ftp.ee.lbl.gov/ECN/ECN-package.tgz\n   [19] Braden, B., Clark,\
    \ D., Crowcroft, J., Davie, B., Deering, S.,\n        Estrin, D., Floyd, S., Jacobson,\
    \ V., Minshall, G., Partridge,\n        C., Peterson, L., Ramakrishnan, K., Shenker,\
    \ S., Wroclawski, J.\n        and L. Zhang, \"Recommendations on Queue Management\
    \ and\n        Congestion Avoidance in the Internet\", RFC 2309, April 1998.\n\
    \   [20] K. K. Ramakrishnan and R. Jain. \"A Binary feedback scheme for\n    \
    \    congestion avoidance in computer networks.\" ACM Trans. Comput.\n       \
    \ Syst.,8(2):158-181, 1990.\n   [21] Mathis, M., Mahdavi, J., Floyd, S. and A.\
    \ Romanow, \"TCP\n        Selective Acknowledgement Options\", RFC 2018, October\
    \ 1996.\n   [22] S. Floyd and V. Jacobson, \"Link sharing and Resource Management\n\
    \        Models for packet  Networks\", IEEE/ACM Transactions on\n        Networking,\
    \ Vol. 3 No.4, August 1995.\n   [23] Prasad Bagal, Shivkumar Kalyanaraman, Bob\
    \ Packer, \"Comparative\n        study of RED, ECN and TCP Rate Control\".\n \
    \       http://www.packeteer.com/technology/Pdf/packeteer-final.pdf\n   [24] tcpdump,\
    \ the protocol packet capture & dumper program.\n        ftp://ftp.ee.lbl.gov/tcpdump.tar.Z\n\
    \   [25] TCP dump file analysis tool:\n        http://jarok.cs.ohiou.edu/software/tcptrace/tcptrace.html\n\
    \   [26] Thompson K., Miller, G.J., Wilder R., \"Wide-Area Internet\n        Traffic\
    \ Patterns and Characteristics\". IEEE Networks Magazine,\n        November/December\
    \ 1997.\n   [27] http://www7.nortel.com:8080/CTL/ecnperf.pdf\n"
- title: 10. Authors' Addresses
  contents:
  - "10. Authors' Addresses\n   Jamal Hadi Salim\n   Nortel Networks\n   3500 Carling\
    \ Ave\n   Ottawa, ON, K2H 8E9\n   Canada\n   EMail: hadi@nortelnetworks.com\n\
    \   Uvaiz Ahmed\n   Dept. of Systems and Computer Engineering\n   Carleton University\n\
    \   Ottawa\n   Canada\n   EMail: ahmed@sce.carleton.ca\n"
- title: 11. Full Copyright Statement
  contents:
  - "11. Full Copyright Statement\n   Copyright (C) The Internet Society (2000). \
    \ All Rights Reserved.\n   This document and translations of it may be copied\
    \ and furnished to\n   others, and derivative works that comment on or otherwise\
    \ explain it\n   or assist in its implementation may be prepared, copied, published\n\
    \   and distributed, in whole or in part, without restriction of any\n   kind,\
    \ provided that the above copyright notice and this paragraph are\n   included\
    \ on all such copies and derivative works.  However, this\n   document itself\
    \ may not be modified in any way, such as by removing\n   the copyright notice\
    \ or references to the Internet Society or other\n   Internet organizations, except\
    \ as needed for the purpose of\n   developing Internet standards in which case\
    \ the procedures for\n   copyrights defined in the Internet Standards process\
    \ must be\n   followed, or as required to translate it into languages other than\n\
    \   English.\n   The limited permissions granted above are perpetual and will\
    \ not be\n   revoked by the Internet Society or its successors or assigns.\n \
    \  This document and the information contained herein is provided on an\n   \"\
    AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE\
    \ DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED\
    \ TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE\
    \ ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A\
    \ PARTICULAR PURPOSE.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
