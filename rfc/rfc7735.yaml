- title: __initial_text__
  contents:
  - '                     Tracking Reviews of Documents

    '
- title: Abstract
  contents:
  - "Abstract\n   Several review teams ensure specific types of review are performed\
    \ on\n   Internet-Drafts as they progress towards becoming RFCs.  The tools\n\
    \   used by these teams to assign and track reviews would benefit from\n   tighter\
    \ integration to the Datatracker.  This document discusses\n   requirements for\
    \ improving those tools without disrupting current\n   work flows.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7735.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2016 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   2\n   2.  Overview of Current Workflows . . . . . . . . . . . .\
    \ . . . .   3\n   3.  Requirements  . . . . . . . . . . . . . . . . . . . . .\
    \ . . .   5\n     3.1.  Secretariat Focused . . . . . . . . . . . . . . . . .\
    \ . .   5\n     3.2.  Review-Team Secretary Focused . . . . . . . . . . . . .\
    \ .   5\n     3.3.  Reviewer Focused  . . . . . . . . . . . . . . . . . . . .\
    \   8\n     3.4.  Review Requester and Consumer Focused . . . . . . . . . .  10\n\
    \     3.5.  Statistics Focused  . . . . . . . . . . . . . . . . . . .  11\n  \
    \ 4.  Security Considerations . . . . . . . . . . . . . . . . . . .  12\n   5.\
    \  Informative References  . . . . . . . . . . . . . . . . . . .  12\n   Appendix\
    \ A.  A Starting Point for Django Models Supporting the\n                Review\
    \ Tool  . . . . . . . . . . . . . . . . . . . .  14\n   Appendix B.  Suggested\
    \ Features Deferred for Future Work  . . . .  15\n   Acknowledgements  . . . .\
    \ . . . . . . . . . . . . . . . . . . . .  16\n   Authors' Addresses  . . . .\
    \ . . . . . . . . . . . . . . . . . . .  16\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   As Internet-Drafts are processed, reviews are requested\
    \ from several\n   review teams.  For example, the General Area Review Team (Gen-ART)\n\
    \   and the Security Directorate (SecDir) perform reviews of documents\n   that\
    \ are in IETF Last Call.  Gen-ART always performs a follow-up\n   review when\
    \ the document is scheduled for an IESG Telechat.  SecDir\n   usually performs\
    \ a follow-up review, but the SecDir secretary may\n   choose not to request that\
    \ follow-up if any issues identified at Last\n   Call are addressed and there\
    \ are otherwise no major changes to the\n   document.  These teams also perform\
    \ earlier reviews of documents on\n   demand.  There are several other teams that\
    \ perform similar services,\n   often focusing on specific areas of expertise.\n\
    \   The secretaries of these teams manage a pool of volunteer reviewers.\n   Documents\
    \ are assigned to reviewers, taking various factors into\n   account.  For instance,\
    \ a reviewer will not be assigned a document\n   for which he is an author or\
    \ shepherd.  Reviewers are given a\n   deadline, usually driven by the end of\
    \ Last Call or an IESG Telechat\n   date.  The reviewer sends each completed review\
    \ to the team's mailing\n   list and to any other lists that are relevant for\
    \ the document being\n   reviewed.  Often, a thread ensues on one or more of those\
    \ lists to\n   resolve any issues found in the review.\n   The secretaries and\
    \ reviewers from several teams are using a tool\n   developed and maintained by\
    \ Tero Kivinen.  Much of its design\n   predates the modern Datatracker.  The\
    \ application currently keeps its\n   own data store and learns about documents\
    \ needing review by\n   inspecting Datatracker and tools.ietf.org pages.  Most\
    \ of those pages\n   are easy to parse, but the Last Call pages, in particular,\
    \ require\n   some effort.  Tighter integration with the Datatracker would simplify\n\
    \   the logic used to identify documents that are ready for review, make\n   it\
    \ simpler for the Datatracker to associate reviews with documents,\n   and allow\
    \ users to reuse their Datatracker credentials.  It would\n   also make it easier\
    \ to detect other potential review-triggering\n   events, such as a document entering\
    \ Working Group (WG) Last Call or\n   when an RFC's standard level is being changed\
    \ without revising the\n   RFC.  Tero currently believes this integration is best\
    \ achieved by a\n   new implementation of the tool.  This document captures requirements\n\
    \   for that reimplementation, with a focus on the workflows that the new\n  \
    \ implementation must take care not to disrupt.  It also discusses new\n   features,\
    \ including changes suggested for the existing tool at its\n   issue tracker [art-trac].\n\
    \   For more information about the various review teams, see the\n   following\
    \ references.\n          +-------------------------------+---------------------+\n\
    \          | General Area Review Team      | [Gen-ART] [RFC6385] |\n         \
    \ | Security Directorate          | [SecDir]            |\n          | Applications\
    \ Area Directorate | [AppsDir]           |\n          | Operations Area Directorate\
    \   | [OPS-dir]           |\n          | Routing Area Directorate      | [RTG-dir]\
    \           |\n          | MIB Doctors                   | [MIBdoctors]      \
    \  |\n          | YANG Doctors                  | [YANGdoctors]       |\n    \
    \      +-------------------------------+---------------------+\n"
- title: 2.  Overview of Current Workflows
  contents:
  - "2.  Overview of Current Workflows\n   This section gives a high-level overview\
    \ of how the review team\n   secretaries and reviewers use the existing tool.\
    \  It is not intended\n   to be comprehensive documentation of how review teams\
    \ operate.\n   Please see the references for those details.\n   For many teams,\
    \ the team's secretary periodically (typically once a\n   week) checks the tool\
    \ for documents it has identified as ready for\n   review.  The tool compiles\
    \ a list from Last Call announcements and\n   IESG Telechat agendas.  The secretary\
    \ creates a set of assignments\n   from this list and enters them into the reviewer\
    \ pool, choosing the\n   reviewers in roughly a round-robin order.  That order\
    \ can be\n   perturbed by several factors.  Reviewers have different levels of\n\
    \   availability.  Some are willing to review multiple documents a month.\n  \
    \ Others may only be willing to review a document every other month.\n   The assignment\
    \ process takes exceptional conditions such as reviewer\n   vacations into account.\
    \  Furthermore, secretaries are careful not to\n   assign a document to a reviewer\
    \ that is an author, shepherd,\n   responsible WG chair, or has some other already\
    \ existing association\n   with the document.  The preference is to get a reviewer\
    \ with a fresh\n   perspective.  The secretary may discover reasons to change\n\
    \   assignments while going through the list of documents.  In order to\n   not\
    \ cause a reviewer to make a false start on a review, the\n   secretaries complete\
    \ the full list of assignments before sending\n   notifications to anyone.  This\
    \ assignment process can take several\n   minutes and it is possible for new Last\
    \ Calls to be issued while the\n   secretary is making assignments.  The secretary\
    \ typically checks to\n   see if new documents are ready for review just before\
    \ issuing the\n   assignments and updates the assignments if necessary.\n   Some\
    \ teams operate in more of a review-on-demand model.  The Routing\n   Area Directorate\
    \ (RTG-dir), for example, primarily initiates reviews\n   at the request of a\
    \ Routing AD.  They may also start an early review\n   at the request of a WG\
    \ chair.  In either case, the reviewers are\n   chosen manually from the pool\
    \ of available reviewers driven by\n   context rather than using a round-robin\
    \ ordering.\n   The issued assignments are either sent to the review team's email\n\
    \   list or are emailed directly to the assigned reviewer.  The\n   assignments\
    \ are reflected in the tool.  For those teams handling\n   different types of\
    \ reviews (Last Call vs. Telechat, for example), the\n   secretary typically processes\
    \ the documents for each type of review\n   separately, and potentially with different\
    \ assignment criteria.  In\n   Gen-ART, for example, the Last Call reviewer for\
    \ a document will\n   almost always get the follow-up Telechat review assignment.\n\
    \   Similarly, SecDir assigns any re-reviews of a document to the same\n   reviewer.\
    \  Other teams may choose to assign a different reviewer.\n   Reviewers discover\
    \ their assignments through email or by looking at\n   their queue in the tool.\
    \  The secretaries for some teams (such as the\n   OPS-dir and RTG-dir) insulate\
    \ their team members from using the tool\n   directly.  These reviewers only work\
    \ through the review team's email\n   list or through direct email.  On teams\
    \ that have the reviewers use\n   the tool directly, most reviewers only check\
    \ the tool when they see\n   they have an assignment via the team's email list.\
    \  A reviewer has\n   the opportunity to reject the assignment for any reason.\
    \  While the\n   tool provides a way to reject assignments, reviewers typically\
    \ use\n   email to coordinate rejections with the team secretary.  The\n   secretary\
    \ will find another volunteer for any rejected assignments.\n   The reviewer can\
    \ indicate that the assignment is accepted in the tool\n   before starting the\
    \ review, but this feature is rarely used.\n   The reviewer sends a completed\
    \ review to the team's email list or\n   secretary, as well as any other lists\
    \ relevant to the review, and\n   usually the draft's primary email alias.  For\
    \ instance, many Last\n   Call reviews are also sent to the IETF general list.\
    \  The teams\n   typically have a template format for the review.  Those templates\n\
    \   usually start with a summary of the conclusion of the review.\n   Typical\
    \ summaries are \"ready for publication\" or \"on the right track\n   but has\
    \ open issues\".  The reviewer (or in the case of teams that\n   insulate their\
    \ reviewers, the secretary) uses the tool to indicate\n   that the review is complete,\
    \ provides the summary, and has an\n   opportunity to provide a link to the review\
    \ in the archives.  Note,\n   however, that having to wait for the document to\
    \ appear in the\n   archive to know the link to paste into the tool is a significant\n\
    \   enough impedance that this link is often not provided by the\n   reviewer.\
    \  The SecDir secretary manually collects these links from\n   the team's email\
    \ list and adds them to the tool.\n   Occasionally, a document is revised between\
    \ when a review assignment\n   is made and when the reviewer starts the review.\
    \  Different teams can\n   have different policies about whether the reviewer\
    \ should review the\n   assigned version or the current version.\n"
- title: 3.  Requirements
  contents:
  - '3.  Requirements

    '
- title: 3.1.  Secretariat Focused
  contents:
  - "3.1.  Secretariat Focused\n   o  The Secretariat must be able to configure secretaries\
    \ and\n      reviewers for review teams (by managing Role records).\n   o  The\
    \ Secretariat must be able to perform any secretary action on\n      behalf of\
    \ a review team secretary (and thus, must be able to\n      perform any reviewer\
    \ action on behalf of the reviewer).\n"
- title: 3.2.  Review-Team Secretary Focused
  contents:
  - "3.2.  Review-Team Secretary Focused\n   o  A secretary must be able to see what\
    \ documents are ready for a\n      review of a given type (such as a Last Call\
    \ review).\n   o  A secretary must be able to assign reviews for documents that\
    \ may\n      not have been automatically identified as ready for a review of a\n\
    \      given type.  (In addition to being the primary assignment method\n    \
    \  for teams that only initiate reviews on demand, this allows the\n      secretary\
    \ to work around errors and handle special cases,\n      including early review\
    \ requests.)\n   o  A secretary must be able to work on and issue a set of assignments\n\
    \      as an atomic unit.  No assignment should be issued until the\n      secretary\
    \ declares the set of assignments complete.\n   o  The tool must support teams\
    \ that have multiple secretaries.  The\n      tool should warn secretaries that\
    \ are simultaneously working on\n      assignments and protect against conflicting\
    \ assignments being\n      made.\n   o  It must be easy for the secretary to discover\
    \ that more documents\n      have become ready for review while working on an\
    \ assignment set.\n   o  The tool should make preparing the assignment email to\
    \ the team's\n      email list easy.  For instance, the tool could prepare the\n\
    \      message, give the secretary an opportunity to edit it, and handle\n   \
    \   sending it to the team's email list.\n   o  It must be possible for a secretary\
    \ to indicate that the review\n      team will not provide a review for a document\
    \ (or a given version\n      of a document).  This indication should be taken\
    \ into account when\n      presenting the documents that are ready for a review\
    \ of a given\n      type.  This will also make it possible to show on a document's\n\
    \      page that no review is expected from this team.\n   o  A secretary must\
    \ be able to easily see who the next available\n      reviewers are, in order.\n\
    \   o  A secretary must be able to edit a reviewer's availability, both\n    \
    \  in terms of frequency, not-available-until-date, and skip-next-\n      n-assignments.\
    \  (See the description of these settings in\n      Section 3.3.)\n   o  The tool\
    \ should make it easy for the secretary to see any team\n      members that have\
    \ requested to review a given document when it\n      becomes available for review.\n\
    \   o  The tool should make it easy for the secretary to identify that a\n   \
    \   reviewer is already involved with a document.  The current tool\n      allows\
    \ the secretary to provide a regular expression to match\n      against the document\
    \ name.  If the expression matches, the\n      document is not available for assignment\
    \ to this reviewer.  For\n      example, Tero will not be assigned documents matching\
    \ '^draft-\n      (kivinen|ietf-tcpinc)-.*$'.  The tool should also take any roles,\n\
    \      such as document shepherd, that the Datatracker knows about into\n    \
    \  consideration.\n   o  The tool should make it easy for the secretary to see\
    \ key features\n      of a document ready for assignment, such as its length,\
    \ its\n      authors, the group and area it is associated with, its title and\n\
    \      abstract, its states (such as IESG or WG states), and any other\n     \
    \ personnel (such as the shepherd and reviewers already assigned\n      from other\
    \ teams) involved in the draft.\n   o  The tool must make it easy for the secretary\
    \ to detect and process\n      re-review requests on the same version of a document\
    \ (such as when\n      a document has an additional Last Call only to deal with\
    \ new IPR\n      information).\n   o  Common operations to groups of documents\
    \ should be easy for the\n      secretary to process as a group with a minimum\
    \ amount of\n      interaction with the tool.  For instance, it should be possible\
    \ to\n      process all of the documents described by the immediately\n      preceding\
    \ bullet with one action.  Similarly, for teams that\n      assign re-reviews\
    \ to the same reviewer, issuing all re-review\n      requests should be a simple\
    \ action.\n   o  A secretary must be able to see which reviewers have outstanding\n\
    \      assignments.\n   o  The tool must make it easy for the secretary to see\
    \ the result of\n      previous reviews from this team for a given document. \
    \ In SecDir,\n      for example, if the request is for a revision that has only\
    \ minor\n      differences and the previous review result was \"Ready\", a new\n\
    \      assignment will not be made.  If the given document replaces one\n    \
    \  or more other prior documents, the tool must make it easy for the\n      secretary\
    \ to see the results of previous reviews of the replaced\n      documents.\n \
    \  o  The tool must make it easy for the secretary to see the result of\n    \
    \  previous reviews from this team for all documents across\n      configurable,\
    \ recent periods of time (such as the last 12 months).\n      A secretary of the\
    \ RTG-dir, for example, would use this result to\n      aid in the manual selection\
    \ of the next reviewer.\n   o  The tools must make it easy for the secretary to\
    \ see the recent\n      performance of a reviewer while making an assignment (see\n\
    \      Section 3.5).  This allows the secretary to detect overburdened or\n  \
    \    unresponsive volunteers earlier in the process.\n   o  A secretary must be\
    \ able to configure the tool to remind them to\n      follow up when actions are\
    \ due.  (For instance, a secretary could\n      receive email when a review is\
    \ about to become overdue.)\n   o  A secretary must be able to assign multiple\
    \ reviewers to a given\n      draft at any time.  In particular, a secretary must\
    \ be able to\n      assign an additional reviewer when an original reviewer indicates\n\
    \      their review is likely to be only partially complete.\n   o  A secretary\
    \ must be able to withdraw a review assignment.\n   o  A secretary must be able\
    \ to perform any reviewer action on behalf\n      of the reviewer.\n   o  A secretary\
    \ must be able to configure the review team's set of\n      reviewers (by managing\
    \ Role records for the team).\n   o  Information about a reviewer must not be\
    \ lost when a reviewer is\n      removed from a team.  (Frequently, reviewers\
    \ come back to teams\n      later.)\n   o  A secretary must be able to delegate\
    \ secretary capabilities in the\n      tool (similar to how a working group chair\
    \ can assign a Delegate).\n      This allows review teams to self-manage secretary\
    \ vacations.\n"
- title: 3.3.  Reviewer Focused
  contents:
  - "3.3.  Reviewer Focused\n   o  A reviewer must be able to indicate availability,\
    \ both in\n      frequency of reviews and as \"not available until this date\"\
    .  The\n      current tool speaks of frequency in these terms:\n      -  Assign\
    \ at maximum one new review per week\n      -  Assign at maximum one new review\
    \ per fortnight\n      -  Assign at maximum one new review per month\n      -\
    \  Assign at maximum one new review per two months\n      -  Assign at maximum\
    \ one new review per quarter\n   o  Reviewers must be able to indicate hiatus\
    \ periods.  Each period\n      may be either \"soft\" or \"hard\".\n      -  A\
    \ hiatus must have a start date.  It may have an end date or it\n         may\
    \ be indefinite.\n      -  During a hiatus, the reviewer will not be included\
    \ in the\n         normal review rotation.  When a provided end date is reached,\n\
    \         the reviewer will automatically be included in the rotation in\n   \
    \      their usual order.\n      -  During a \"soft\" hiatus, the reviewer must\
    \ not be assigned new\n         reviews but is expected to complete existing assignments\
    \ and do\n         follow-up reviews.\n      -  During a \"hard\" hiatus, the\
    \ reviewer must not be assigned any\n         new reviews and the secretary must\
    \ be prompted to reassign any\n         outstanding or follow-up reviews.\n  \
    \ o  Reviewers must be able to indicate that they should be skipped the\n    \
    \  next \"n\" times they would normally have received an assignment.\n   o  Reviewers\
    \ must be able to indicate that they are transitioning to\n      inactive and\
    \ provide a date for the end of the transition period.\n      During this transition\
    \ time, the reviewer must not be assigned new\n      reviews but is expected to\
    \ complete outstanding assignments and\n      follow-up reviews.  At the end of\
    \ the transition period, the\n      secretary must be prompted to reassign any\
    \ outstanding or follow-\n      up reviews.  (This allows review-team members\
    \ that are taking on\n      AD responsibility to transition gracefully to an inactive\
    \ state\n      for the team.)\n   o  Both the reviewer and the secretary will\
    \ be notified by email of\n      any modifications to a reviewer's availability.\n\
    \   o  A reviewer must be able to easily discover new review assignments.\n  \
    \    (The tool might send email directly to an assigned reviewer in\n      addition\
    \ to sending the set of assignments to the team's email\n      list.  The tool\
    \ might also use the Django Message framework to let\n      a reviewer that's\
    \ logged into the Datatracker know a new review\n      assignment has been made.)\n\
    \   o  Reviewers must be able to see their current set of outstanding\n      assignments,\
    \ completed assignments, and rejected assignments.  The\n      presentation of\
    \ those sets should either be separate or, if\n      combined, the sets should\
    \ be visually distinct.\n   o  A reviewer should be able to request to review\
    \ a particular\n      document.  The draft may be in any state: available and\n\
    \      unassigned; already assigned to another reviewer; or not yet\n      available.\n\
    \   o  A reviewer must be able to reject a review assignment, optionally\n   \
    \   providing the secretary with an explanation for the rejection.\n      The\
    \ tool will notify the secretary of the rejection by email.\n   o  A reviewer\
    \ must be able to indicate that they have accepted and\n      are working on an\
    \ assignment.\n   o  A reviewer must be able to indicate that a review is only\n\
    \      partially completed and ask the secretary to assign an additional\n   \
    \   reviewer.  The tool will notify the secretary of this condition by\n     \
    \ email.\n   o  It should be possible for a reviewer to reject or accept a review\n\
    \      either by using the tool's web interface or by replying to the\n      review\
    \ assignment email.\n   o  It must be easy for a reviewer to see when each assigned\
    \ review is\n      due.\n   o  A reviewer must be able to configure the tool to\
    \ remind them when\n      actions are due.  (For instance, a reviewer could receive\
    \ email\n      when a review is about to become overdue).\n   o  A reviewer must\
    \ be able to indicate that a review is complete,\n      capturing where the review\
    \ is in the archives and the high-level,\n      review-result summary.\n   o \
    \ It must be possible for a reviewer to clearly indicate which\n      version\
    \ of a document was reviewed.  Documents are sometimes\n      revised between\
    \ when a review was assigned and when it is due.\n      The tool should note the\
    \ current version of the document and\n      highlight when the review is not\
    \ for the current version.\n   o  It must be easy for a reviewer to submit a completed\
    \ review.\n      -  The current workflow, where the reviewer sends email to the\n\
    \         team's email list (possibly copying other lists) and then\n        \
    \ indicates where to find that review, must continue to be\n         supported.\
    \  The tool should make it easier to capture the link\n         to review in the\
    \ team's email list archives (perhaps by\n         suggesting links based on a\
    \ search into the archives).\n      -  The tool should allow the reviewer to enter\
    \ the review into the\n         tool via a web form (either as directly provided\
    \ text or\n         through a file-upload mechanism).  The tool will ensure the\n\
    \         review is posted to the appropriate lists and will construct\n     \
    \    the links to those posts in the archives.\n      -  The tool could also allow\
    \ the reviewer to submit the review to\n         the tool by email (perhaps by\
    \ replying to the assignment).  The\n         tool would then ensure the review\
    \ is posted to the appropriate\n         lists.\n"
- title: 3.4.  Review Requester and Consumer Focused
  contents:
  - "3.4.  Review Requester and Consumer Focused\n   o  It should be easy for an AD\
    \ or group chair to request any type of\n      review, but particularly an early\
    \ review, from a review team.\n   o  It should be possible for that person to\
    \ withdraw a review\n      request.\n   o  It must be easy to find all reviews\
    \ of a document when looking at\n      the document's main page in the Datatracker.\
    \  The reference to the\n      review must make it easy to see any responses to\
    \ the review on the\n      email lists it was sent to.  If a document \"replaces\"\
    \ one or more\n      other documents, reviews of the replaced documents should\
    \ be\n      included in the results.\n   o  It must be easy to find all reviews\
    \ of a document when looking at\n      search result pages and other lists of\
    \ documents, such as the\n      documents on an IESG Telechat agenda.\n"
- title: 3.5.  Statistics Focused
  contents:
  - "3.5.  Statistics Focused\n   o  It must be easy to see the following across all\
    \ teams, a given\n      team, or a given reviewer, and independently across all\
    \ time or\n      across configurable recent periods of time:\n      -  how many\
    \ reviews have been completed\n      -  how many reviews are in progress\n   \
    \   -  how many in progress reviews are late\n      -  how many completed reviews\
    \ were late\n      -  how many reviews were not completed at all\n      -  average\
    \ time to complete reviews (from assignment to\n         completion)\n   o  It\
    \ must be easy to see the following for all teams, for a given\n      team, or\
    \ for a given reviewer, across all time or across\n      configurable recent periods:\n\
    \      -  total counts of reviews in each review state (done, rejected,\n    \
    \     etc.)\n      -  total counts of completed reviews by result (ready, ready\
    \ with\n         nits, etc.)\n   o  The above statistics should also be calculated\
    \ reflecting the size\n      of the documents being reviewed (such as using the\
    \ number of pages\n      or words in the documents).\n   o  Where applicable,\
    \ statistics should take reviewer hiatus periods\n      into account.\n   o  Access\
    \ to the above statistics must be easy to configure.  Access\n      will be initially\
    \ limited as follows:\n      -  The Secretariat and ADs can see any statistic.\n\
    \      -  A team secretary can see any statistics for that team.\n      -  A reviewer\
    \ can see any team aggregate statistics or their own\n         reviewer-specific\
    \ statistics.\n   o  Where possible, the above statistics should be visible as\
    \ a time-\n      series graph.\n   o  The implementation should anticipate future\
    \ enhancements that\n      would allow ADs to indicate their position was informed\
    \ by a given\n      review.  Such enhancements would allow reporting correlations\n\
    \      between reviews and documents that receive one or more\n      \"discusses\"\
    .  However, implementing these enhancements is not part\n      of the current\
    \ project.\n"
- title: 4.  Security Considerations
  contents:
  - "4.  Security Considerations\n   This document discusses requirements for tools\
    \ that assist review\n   teams.  These requirements do not affect the security\
    \ of the Internet\n   in any significant fashion.  The tools themselves have authentication\n\
    \   and authorization considerations (team secretaries will be able to do\n  \
    \ different things than reviewers).\n"
- title: 5.  Informative References
  contents:
  - "5.  Informative References\n   [AppsDir]  IETF, \"Applications Area Directorate\
    \ Review Process\",\n              <http://trac.tools.ietf.org/area/app/trac/wiki/\n\
    \              AppsDirReview>.\n   [art-trac] IETF, \"Area Review Team Tool: {1}\
    \ Active Tickets\",\n              <https://wiki.tools.ietf.org/tools/art/trac/report/1>.\n\
    \   [Gen-ART]  IETF, \"General Area: General Area Review Team (GEN-ART)\n    \
    \          Guidelines\",\n              <http://trac.tools.ietf.org/area/gen/trac/wiki>.\n\
    \   [MIBdoctors]\n              IETF, \"MIB Doctors\",\n              <http://www.ietf.org/iesg/directorate/mib-doctors.html>.\n\
    \   [OPS-dir]  IETF, \"OPS Directorate (OPS-DIR)\",\n              <https://svn.tools.ietf.org/area/ops/trac/wiki/\n\
    \              Directorates>.\n   [RFC6385]  Barnes, M., Doria, A., Alvestrand,\
    \ H., and B. Carpenter,\n              \"General Area Review Team (Gen-ART) Experiences\"\
    ,\n              RFC 6385, DOI 10.17487/RFC6385, October 2011,\n             \
    \ <http://www.rfc-editor.org/info/rfc6385>.\n   [RTG-dir]  IETF, \"Routing Area\
    \ Directorate Wiki Page\",\n              <http://trac.tools.ietf.org/area/rtg/trac/wiki/RtgDir>.\n\
    \   [SecDir]   IETF, \"Security Directorate\",\n              <http://trac.tools.ietf.org/area/sec/trac/wiki/\n\
    \              SecurityDirectorate>.\n   [YANGdoctors]\n              IETF, \"\
    YANG Doctors\",\n              <http://www.ietf.org/iesg/directorate/yang-doctors.html>.\n"
- title: Appendix A.  A Starting Point for Django Models Supporting the Review
  contents:
  - "Appendix A.  A Starting Point for Django Models Supporting the Review\n     \
    \        Tool\n from django.db import models\n from ietf.doc.models import Document\n\
    \ from ietf.person.models import Email\n from ietf.group.models import Group,\
    \ Role\n from ietf.name.models import NameModel\n class ReviewRequestStateName(NameModel):\n\
    \     \"\"\" Requested, Accepted, Rejected, Withdrawn, Overtaken By Events,\n\
    \         No Response , Completed  \"\"\"\n class ReviewTypeName(NameModel):\n\
    \     \"\"\" Early Review, Last Call, Telechat \"\"\"\n class ReviewResultName(NameModel):\n\
    \     \"\"\"Almost ready, Has issues, Has nits, Not Ready,\n        On the right\
    \ track, Ready, Ready with issues,\n        Ready with nits, Serious Issues\"\"\
    \"\n class Reviewer(models.Model):\n     \"\"\"\n     These records associate\
    \ reviewers with review teams and keep track\n     of admin data associated with\
    \ the reviewer in the particular team.\n     There will be one record for each\
    \ combination of reviewer and team.\n     \"\"\"\n     role        = models.ForeignKey(Role)\n\
    \     frequency   = models.IntegerField(help_text=\n                         \
    \          \"Can review every N days\")\n     available   = models.DateTimeField(blank=True,null=True,\
    \ help_text=\n                         \"When will this reviewer be available\
    \ again\")\n     filter_re   = models.CharField(blank=True)\n     skip_next  \
    \ = models.IntegerField(help_text=\n                          \"Skip the next\
    \ N review assignments\")\n class ReviewResultSet(models.Model):\n     \"\"\"\n\
    \     This table provides a way to point out a set of ReviewResultName\n     entries\
    \ that are valid for a given team in order to be able to\n     limit the result\
    \ choices that can be set for a given review as a\n     function of which team\
    \ it is related to.\n     \"\"\"\n     team        = models.ForeignKey(Group)\n\
    \     valid       = models.ManyToManyField(ReviewResultName)\n class ReviewRequest(models.Model):\n\
    \     \"\"\"\n     There should be one ReviewRequest entered for each combination\
    \ of\n     document, rev, and reviewer.\n     \"\"\"\n     # Fields filled in\
    \ on the initial record creation:\n     time          = models.DateTimeField(auto_now_add=True)\n\
    \     type          = models.ReviewTypeName()\n     doc           = models.ForeignKey(Document,\n\
    \                            related_name='review_request_set')\n     team   \
    \       = models.ForeignKey(Group)\n     deadline      = models.DateTimeField()\n\
    \     requested_rev = models.CharField(verbose_name=\"requested_revision\",\n\
    \                             max_length=16, blank=True)\n     state         =\
    \ models.ForeignKey(ReviewRequestStateName)\n     # Fields filled in as reviewer\
    \ is assigned, and as the review\n     # is uploaded\n     reviewer      = models.ForeignKey(Reviewer,\
    \ null=True, blank=True)\n     review        = models.OneToOneField(Document,\
    \ null=True,\n                                                    blank=True)\n\
    \     reviewed_rev  = models.CharField(verbose_name=\"reviewed_revision\",\n \
    \                                     max_length=16, blank=True)\n     result\
    \        = models.ForeignKey(ReviewResultName)\n"
- title: Appendix B.  Suggested Features Deferred for Future Work
  contents:
  - "Appendix B.  Suggested Features Deferred for Future Work\n   Brian Carpenter\
    \ suggested a set of author/editor-focused requirements\n   that were deferred\
    \ for another iteration of improvement.  These\n   include providing a way for\
    \ the editors to acknowledge receipt of the\n   review, potentially tracking the\
    \ email conversation between the\n   reviewer and document editor, and indicating\
    \ which review topics the\n   editor believes a new revision addresses.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   Tero Kivinen and Henrik Levkowetz were instrumental in forming\
    \ this\n   set of requirements and in developing the initial Django models in\n\
    \   the appendix.\n   The following people provided reviews of this document:\
    \ David Black,\n   Deborah Brungard, Brian Carpenter, Elwyn Davies, Stephen Farrell,\n\
    \   Joel Halpern, Jonathan Hardwick, Russ Housley, Barry Leiba, Jean\n   Mahoney,\
    \ Randy Presuhn, Gunter Van De Velde, and Martin Vigoureux.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Robert Sparks\n   Oracle\n   7460 Warren Parkway\n   Suite\
    \ 300\n   Frisco, Texas  75034\n   United States\n   Email: rjsparks@nostrum.com\n\
    \   Tero Kivinen\n   INSIDE Secure\n   Eerikinkatu 28\n   Helsinki  FI-00180\n\
    \   Finland\n   Email: kivinen@iki.fi\n"
