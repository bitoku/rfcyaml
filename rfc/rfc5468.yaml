- title: __initial_text__
  contents:
  - '  Performance Analysis of Inter-Domain Path Computation Methodologies

    '
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2009 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\
    \ in effect on the date of\n   publication of this document (http://trustee.ietf.org/license-info).\n\
    \   Please review these documents carefully, as they describe your rights\n  \
    \ and restrictions with respect to this document.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document presents a performance comparison between the per-\n\
    \   domain path computation method and the Path Computation Element (PCE)\n  \
    \ Architecture-based Backward Recursive Path Computation (BRPC)\n   procedure.\
    \  Metrics to capture the significant performance aspects\n   are identified,\
    \ and detailed simulations are carried out on realistic\n   scenarios.  A performance\
    \ analysis for each of the path computation\n   methods is then undertaken.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................2\n\
    \   2. Terminology .....................................................3\n  \
    \ 3. Evaluation Metrics ..............................................4\n   4.\
    \ Simulation Setup ................................................5\n   5. Results\
    \ and Analysis ............................................6\n      5.1. Path\
    \ Cost ..................................................7\n      5.2. Crankback/Setup\
    \ Delay ......................................7\n      5.3. Signaling Failures\
    \ .........................................8\n      5.4. Failed TE-LSPs/Bandwidth\
    \ on Link Failures ..................8\n      5.5. TE LSP/Bandwidth Setup Capacity\
    \ ............................8\n   6. Security Considerations .........................................9\n\
    \   7. Acknowledgment ..................................................9\n  \
    \ 8. Informative References ..........................................9\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The IETF has specified two approaches for the computation\
    \ of inter-\n   domain (Generalized) Multi-Protocol Label Switching ((G)MPLS)\
    \ Traffic\n   Engineering (TE) Label Switched Paths (LSP): the per-domain path\n\
    \   computation approach defined in [RFC5152] and the PCE-based approach\n   specified\
    \ in [RFC4655].  More specifically, we study the PCE-based\n   path computation\
    \ model that makes use of the BRPC method outlined in\n   [RFC5441].  In the rest\
    \ of this document, we will call PD and PCE the\n   per-domain path computation\
    \ approach and the PCE path computation\n   approach, respectively.\n   In the\
    \ per-domain path computation approach, each path segment within\n   a domain\
    \ is computed during the signaling process by each entry node\n   of the domain\
    \ up to the next-hop exit node of that same domain.\n   In contrast, the PCE-based\
    \ approach and, in particular, the BRPC\n   method defined in [RFC5441] rely on\
    \ the collaboration between a set\n   of PCEs to find to shortest inter-domain\
    \ path after the computation\n   of which the corresponding TE LSP is signaled:\
    \ path computation is\n   undertaken using multiple PCEs in a backward recursive\
    \ fashion from\n   the destination domain to the source domain.  The notion of\
    \ a Virtual\n   Shortest Path Tree (VSPT) is introduced.  Each link of a VSPT\n\
    \   represents the shortest path satisfying the set of required\n   constraints\
    \ between the border nodes of a domain and the destination\n   LSR.  The VSPT\
    \ of each domain is returned by the corresponding PCE to\n   create a new VSPT\
    \ by PCEs present in other domains.  [RFC5441]\n   discusses the BRPC procedure\
    \ in complete detail.\n   This document presents some simulation results and analysis\
    \ to\n   compare the performance of the above two inter-domain path\n   computation\
    \ approaches.  Two realistic topologies with accompanying\n   traffic matrices\
    \ are used to undertake the simulations.\n   Note that although the simulation\
    \ results discussed in this document\n   have used inter-area networks, they also\
    \ apply to Inter-AS cases.\n   Disclaimer: although simulations have been made\
    \ on different and\n   realistic topologies showing consistent results, the metrics\
    \ shown\n   below may vary with the network topology.\n   Note that this document\
    \ refers to multiple figures that are only\n   available in the PDF version.\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   Terminology used in this document:\n   TE LSP: Traffic Engineered\
    \ Label Switched Path.\n   CSPF: Constrained Shortest Path First.\n   PCE: Path\
    \ Computation Element.\n   BRPC: Backward Recursive PCE-based Computation.\n \
    \  AS: Autonomous System.\n   ABR: Routers used to connect two IGP areas (areas\
    \ in OSPF or levels\n   in IS-IS).\n   ASBR: Routers used to connect together\
    \ ASes of a different or the\n   same Service Provider via one or more Inter-AS\
    \ links.\n   Border LSR: A border LSR is either an ABR in the context of inter-\n\
    \   area TE or an ASBR in the context of Inter-AS TE.\n   VSPT: Virtual Shortest\
    \ Path Tree.\n   LSA: Link State Advertisement.\n   LSR: Label Switching Router.\n\
    \   IGP: Interior Gateway Protocol.\n   TED: Traffic Engineering Database.\n \
    \  PD: Per-Domain\n"
- title: 3.  Evaluation Metrics
  contents:
  - "3.  Evaluation Metrics\n   This section discusses the metrics that are used to\
    \ quantify and\n   compare the performance of the two approaches.\n   o  Path\
    \ Cost.  The maximum and average path costs are observed for\n      each TE LSP.\
    \  The distributions for the maximum and average path\n      costs are then compared\
    \ for the two path computation approaches.\n   o  Signaling Failures.  Signaling\
    \ failures may occur in various\n      circumstances.  With PD, the head-end LSR\
    \ chooses the downstream\n      border router (ABR, ASBR) according to some selection\
    \ criteria\n      (IGP shortest path, ....) based on the information in its TED.\n\
    \      This ABR then selects the next ABR using its TED, continuing the\n    \
    \  process till the destination is reached.  At each step, the TED\n      information\
    \ could be out of date, potentially resulting in a\n      signaling failure during\
    \ setup.  In the BRPC procedure, the PCEs\n      are the ABRs that cooperate to\
    \ form the VSPT based on the\n      information in their respective TEDs.  As\
    \ in the case of the PD\n      approach, information in the TED could be out of\
    \ date, potentially\n      resulting in signaling failures during setup.  Also,\
    \ only with the\n      PD approach, another situation that leads to a signaling\
    \ failure\n      is when the selected exit ABR does not have any path obeying\
    \ the\n      set of constraints toward a downstream exit node or the TE LSP\n\
    \      destination.  This situation does not occur with the BRPC.  The\n     \
    \ signaling failure metric captures the total number of signaling\n      failures\
    \ that occur during initial setup and re-route (on link\n      failure) of a TE\
    \ LSP.  The distribution of the number of signaling\n      failures encountered\
    \ for all TE LSPs is then compared for the PD\n      and BRPC methods.\n   o \
    \ Crankback Signaling.  In this document, we made the assumption\n      that in\
    \ the case of PD, when an entry border node fails to find a\n      route in the\
    \ corresponding domain, boundary re-routing crankback\n      [RFC4920] signaling\
    \ was used.  A crankback signaling message\n      propagates to the entry border\
    \ node of the domain and a new exit\n      border node is chosen.  After this,\
    \ path computation takes place\n      to find a path segment to a new entry border\
    \ node of the next\n      domain.  This causes an additional delay in setup time.\
    \  This\n      metric captures the distribution of the number of crankback\n \
    \     signals and the corresponding delay in setup time for a TE LSP\n      when\
    \ using PD.  The total delay arising from the crankback\n      signaling is proportional\
    \ to the costs of the links over which the\n      signal travels, i.e., the path\
    \ that is setup from the entry border\n      node of a domain to its exit border\
    \ node (the assumption was made\n      that link metrics reflect propagation delays).\
    \  Similar to the\n      above metrics, the distribution of total crankback signaling\
    \ and\n      corresponding proportional delay across all TE LSPs is compared.\n\
    \   o  TE LSPs/Bandwidth Setup Capacity.  Due to the different path\n      computation\
    \ techniques, there is a significant difference in the\n      amount of TE LSPs/bandwidth\
    \ that can be set up.  This metric\n      captures the difference in the number\
    \ of TE LSPs and corresponding\n      bandwidth that can be set up using the two\
    \ path computation\n      techniques.  The traffic matrix is continuously scaled\
    \ and stopped\n      when the first TE LSP cannot be set up for both the methods.\
    \  The\n      difference in the scaling factor gives the extra bandwidth that\n\
    \      can be set up using the corresponding path computation technique.\n   o\
    \  Failed TE LSPs/Bandwidth on Link Failure.  Link failures are\n      induced\
    \ in the network during the course of the simulations\n      conducted.  This\
    \ metric captures the number of TE LSPs and the\n      corresponding bandwidth\
    \ that failed to find a route when one or\n      more links lying on its path\
    \ failed.\n"
- title: 4.  Simulation Setup
  contents:
  - "4.  Simulation Setup\n   A very detailed simulator has been developed to replicate\
    \ a real-life\n   network scenario accurately.  Following is the set of entities\
    \ used\n   in the simulation with a brief description of their behavior.\n   +------------+-------+-------+--------+--------+---------+----------+\n\
    \   |   Domain   |  # of |  # of |  OC48  |  OC192 |  [0,20) | [20,100] |\n  \
    \ |    Name    | nodes | links |  links |  links |   Mbps  |   Mbps   |\n   +------------+-------+-------+--------+--------+---------+----------+\n\
    \   |     D1     |   17  |   24  |   18   |    6   |   125   |    368   |\n  \
    \ |     D2     |   14  |   17  |   12   |    5   |    76   |    186   |\n   |\
    \     D3     |   19  |   26  |   20   |    6   |    14   |    20    |\n   |  \
    \   D4     |   9   |   12  |    9   |    3   |    7    |    18    |\n   |  MESH-CORE\
    \ |   83  |  167  |   132  |   35   |    0    |     0    |\n   | (backbone) |\
    \       |       |        |        |         |          |\n   |  SYM-CORE  |  \
    \ 29  |  377  |   26   |   11   |    0    |     0    |\n   | (backbone) |    \
    \   |       |        |        |         |          |\n   +------------+-------+-------+--------+--------+---------+----------+\n\
    \           Table 1.  Domain Details and TE LSP Size Distribution\n   o  Topology\
    \ Description.  To obtain meaningful results applicable to\n      present-day\
    \ Service Provider topologies, simulations have been run\n      on two representative\
    \ topologies.  They consists of a large\n      backbone area to which four smaller\
    \ areas are connected.  For the\n      first topology named MESH-CORE, a densely\
    \ connected backbone was\n      obtained from RocketFuel [ROCKETFUEL].  The second\
    \ topology has a\n      symmetrical backbone and is called SYM-CORE.  The four\
    \ connected\n      smaller areas are obtained from [DEF-DES].  Details of the\n\
    \      topologies are shown in Table 1 along with their layout in Figure\n   \
    \   1.  All TE LSPs set up on this network have their source and\n      destinations\
    \ in different areas and all of them need to traverse\n      the backbone network.\
    \  Table 1 also shows the number of TE LSPs\n      that have their sources in\
    \ the corresponding areas along with\n      their size distribution.\n   o  Node\
    \ Behavior.  Every node in the topology represents a router\n      that maintains\
    \ states for all the TE LSPs passing through it.\n      Each node in a domain\
    \ is a source for TE LSPs to all the other\n      nodes in the other domains.\
    \  As in a real-life scenario, where\n      routers boot up at random points in\
    \ time, the nodes in the\n      topologies also start sending traffic on the TE\
    \ LSPs originating\n      from them at a random start time (to take into account\
    \ the\n      different boot-up times).  All nodes are up within an hour of the\n\
    \      start of simulation.  All nodes maintain a TED that is updated\n      using\
    \ LSA updates as outlined in [RFC3630].  The flooding scope of\n      the Traffic\
    \ Engineering IGP updates are restricted only to the\n      domain in which they\
    \ originate in compliance with [RFC3630] and\n      [RFC5305].\n   o  TE LSP Setup.\
    \  When a node boots up, it sets up all TE LSPs that\n      originate from it\
    \ in descending order of size.  The network is\n      dimensioned such that all\
    \ TE LSPs can find a path.  Once set up,\n      all TE LSPs stay in the network\
    \ for the complete duration of the\n      simulation unless they fail due to a\
    \ link failure.  Even though\n      the TE LSPs are set up in descending order\
    \ of size from a head-end\n      router, from the network perspective, TE LSPs\
    \ are set up in random\n      fashion as the routers boot up at random times.\n\
    \   o  Inducing Failures.  For thorough performance analysis and\n      comparison,\
    \ link failures are induced in all the areas.  Each link\n      in a domain can\
    \ fail independently with a mean failure time of 24\n      hours and be restored\
    \ with a mean restore time of 15 minutes.\n      Both inter-failure and inter-restore\
    \ times are uniformly\n      distributed.  No attempt to re-optimize the path\
    \ of a TE LSP is\n      made when a link is restored.  The links that join two\
    \ domains\n      never fail.  This step has been taken to concentrate only on\
    \ how\n      link failures within domains affect the performance.\n"
- title: 5.  Results and Analysis
  contents:
  - "5.  Results and Analysis\n   Simulations were carried out on the two topologies\
    \ previously\n   described.  The results are presented and discussed in this section.\n\
    \   All figures are from the PDF version of this document.  In the\n   figures,\
    \ \"PD-Setup\" and \"PCE-Setup\" represent results corresponding\n   to the initial\
    \ setting up of TE LSPs on an empty network using the\n   per-domain and the PCE\
    \ approach, respectively.  Similarly, \"PD-\n   Failure\" and \"PCE-Failure\"\
    \ denote the results under the link failure\n   scenario.  A period of one week\
    \ was simulated and results were\n   collected after the transient period.  Figure\
    \ 2 and Figure 3\n   illustrate the behavior of the metrics for topologies MESH-CORE\
    \ and\n   SYM-CORE, respectively.\n"
- title: 5.1.  Path Cost
  contents:
  - "5.1.  Path Cost\n   Figures 2a and 3a show the distribution of the average path\
    \ cost of\n   the TE LSPs for MESH-CORE and SYM-CORE, respectively.  During the\n\
    \   initial setup, roughly 40% of TE LSPs for MESH-CORE and 70% of TE\n   LSPs\
    \ for SYM-CORE have path costs greater with PD (PD-Setup) than\n   with the PCE\
    \ approach (PCE-Setup).  This is due to the ability of the\n   BRPC procedure\
    \ to select the inter-domain shortest constrained paths\n   that satisfy the constraints.\
    \  Since the per-domain approach to path\n   computation is undertaken in stages\
    \ where every entry border router\n   to a domain computes the path in the corresponding\
    \ domain, the most\n   optimal (shortest constrained inter-domain) route is not\
    \ always\n   found.  When failures start to take place in the network, TE LSPs\
    \ are\n   re-routed over different paths resulting in path costs that are\n  \
    \ different from the initial costs.  PD-Failure and PCE-Failure in\n   Figures\
    \ 2a and 3a show the distribution of the average path costs\n   that the TE LSPs\
    \ have over the duration of the simulation with link\n   failures occurring. \
    \ Similarly, the average path costs with the PD\n   approach are much higher than\
    \ the PCE approach when link failures\n   occur.  Figures 2b and 3b show similar\
    \ trends and present the maximum\n   path costs for a TE LSP for the two topologies,\
    \ respectively.  It can\n   be seen that with per-domain path computation, the\
    \ maximum path costs\n   are larger for 30% and 100% of the TE LSPs for MESH-CORE\
    \ and SYM-\n   CORE, respectively.\n"
- title: 5.2.  Crankback/Setup Delay
  contents:
  - "5.2.  Crankback/Setup Delay\n   Due to crankbacks that take place in the per-domain\
    \ approach of path\n   computation, TE LSP setup time is significantly increased.\
    \  This\n   could lead to Quality-of-Service (QoS) requirements not being met,\n\
    \   especially during failures when re-routing needs to be quick in order\n  \
    \ to keep traffic disruption to a minimum (for example in the absence\n   of local\
    \ repair mechanisms such as defined in [RFC4090]).  Since\n   crankbacks do not\
    \ take place during path computation with a PCE,\n   setup delays are significantly\
    \ reduced.  Figures 2c and 3c show the\n   distributions of the number of crankbacks\
    \ that took place during the\n   setup of the corresponding TE LSPs for MESH-CORE\
    \ and SYM-CORE,\n   respectively.  It can be seen that all crankbacks occurred\
    \ when\n   failures were taking place in the networks.  Figures 2d and 3d\n  \
    \ illustrate the \"proportional\" setup delays experienced by the TE LSPs\n  \
    \ due to crankbacks for the two topologies.  It can be observed that\n   for a\
    \ large proportion of the TE LSPs, the setup delays arising out\n   of crankbacks\
    \ are very large, possibly proving to be very detrimental\n   to QoS requirements.\
    \  The large delays arise out of the crankback\n   signaling that needs to propagate\
    \ back and forth from the exit border\n   router of a domain to its entry border\
    \ router.  More crankbacks occur\n   for SYM-CORE as compared to MESH-CORE as\
    \ it is a very \"restricted\"\n   and \"constrained\" network in terms of connectivity.\
    \  This causes a\n   lack of routes and often several cycles of crankback signaling\
    \ are\n   required to find a constrained path.\n"
- title: 5.3.  Signaling Failures
  contents:
  - "5.3.  Signaling Failures\n   As discussed in the previous sections, signaling\
    \ failures occur\n   either due to an outdated TED or when a path cannot be found\
    \ from the\n   selected entry border router.  Figures 2e and 3e show the\n   distribution\
    \ of the total number of signaling failures experienced by\n   the TE LSPs during\
    \ setup.  About 38% and 55% of TE LSPs for MESH-CORE\n   and SYM-CORE, respectively,\
    \ experience a signaling failures with per-\n   domain path computation when link\
    \ failures take place in the network.\n   In contrast, only about 3% of the TE\
    \ LSPs experience signaling\n   failures with the PCE method.  It should be noted\
    \ that the signaling\n   failures experienced with the PCE correspond only to\
    \ the TEDs being\n   out of date.\n"
- title: 5.4.  Failed TE-LSPs/Bandwidth on Link Failures
  contents:
  - "5.4.  Failed TE-LSPs/Bandwidth on Link Failures\n   Figures 2f and 3f show the\
    \ number of TE LSPs and the associated\n   required bandwidth that fail to find\
    \ a route when link failures are\n   taking place in the topologies.  For MESH-CORE,\
    \ with the per-domain\n   approach, 395 TE LSPs failed to find a path corresponding\
    \ to 1612\n   Mbps of bandwidth.  For PCE, this number is lesser at 374\n   corresponding\
    \ to 1546 Mbps of bandwidth.  For SYM-CORE, with the per-\n   domain approach,\
    \ 434 TE LSPs fail to find a route corresponding to\n   1893 Mbps of bandwidth.\
    \  With the PCE approach, only 192 TE LSPs fail\n   to find a route, corresponding\
    \ to 895 Mbps of bandwidth.  It is\n   clearly visible that the PCE allows more\
    \ TE LSPs to find a route thus\n   leading to better performance during link failures.\n"
- title: 5.5.  TE LSP/Bandwidth Setup Capacity
  contents:
  - "5.5.  TE LSP/Bandwidth Setup Capacity\n   Since PCE and the per-domain path computation\
    \ approach differ in how\n   path computation takes place, more bandwidth can\
    \ be set up with PCE.\n   This is primarily due to the way in which BRPC functions.\
    \  To observe\n   the extra bandwidth that can fit into the network, the traffic\
    \ matrix\n   was scaled.  Scaling was stopped when the first TE LSP failed to\
    \ set\n   up with PCE.  This metric, like all the others discussed above, is\n\
    \   topology dependent (therefore, the choice of two topologies for this\n   study).\
    \  This metric highlights the ability of PCE to fit more\n   bandwidth in the\
    \ network.  For MESH-CORE, on scaling, 1556 Mbps more\n   could be set up with\
    \ PCE.  In comparison, for SYM-CORE, this value is\n   986 Mbps.  The amount of\
    \ extra bandwidth that can be set up on SYM-\n   CORE is lesser due to its restricted\
    \ nature and limited capacity.\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   This document does not raise any security issues.\n"
- title: 7.  Acknowledgment
  contents:
  - "7.  Acknowledgment\n   The authors would like to acknowledge Dimitri Papadimitriou\
    \ for his\n   helpful comments to clarify the text.\n"
- title: 8.  Informative References
  contents:
  - "8.  Informative References\n   [DEF-DES]    J. Guichard, F. Le Faucheur, and\
    \ J.-P. Vasseur,\n                \"Definitive MPLS Network Designs\", Cisco Press,\
    \ 2005.\n   [RFC5152]    Vasseur, JP., Ed., Ayyangar, A., Ed., and R. Zhang, \"\
    A\n                Per-Domain Path Computation Method for Establishing\n     \
    \           Inter-Domain Traffic Engineering (TE) Label Switched\n           \
    \     Paths (LSPs)\", RFC 5152, February 2008.\n   [RFC5441]    Vasseur, JP.,\
    \ Zhang, R., Bitar, N., and JL. Le Roux, \"A\n                Backward Recursive\
    \ PCE-Based Computation (BRPC)\n                Procedure to Compute  Shortest\
    \ Constrained Inter-Domain\n                Traffic Engineering Label Switched\
    \ Paths\", RFC 5441,\n                April 2009.\n   [RFC3630]    Katz, D., Kompella,\
    \ K., and D. Yeung, \"Traffic\n                Engineering (TE) Extensions to\
    \ OSPF Version 2\", RFC\n                3630, September 2003.\n   [RFC5305] \
    \   Li, T. and H. Smit, \"IS-IS Extensions for Traffic\n                Engineering\"\
    , RFC 5305, October 2008.\n   [RFC4090]    Pan, P., Ed., Swallow, G., Ed., and\
    \ A. Atlas, Ed., \"Fast\n                Reroute Extensions to RSVP-TE for LSP\
    \ Tunnels\", RFC\n                4090, May 2005.\n   [RFC4655]    Farrel, A.,\
    \ Vasseur, J.-P., and J. Ash, \"A Path\n                Computation Element (PCE)-Based\
    \ Architecture\", RFC 4655,\n                August 2006.\n   [RFC4920]    Farrel,\
    \ A., Ed., Satyanarayana, A., Iwata, A., Fujita,\n                N., and G. Ash,\
    \ \"Crankback Signaling Extensions for MPLS\n                and GMPLS RSVP-TE\"\
    , RFC 4920, July 2007.\n   [ROCKETFUEL] N. Spring, R. Mahajan, and D. Wehterall,\
    \ \"Measuring ISP\n                Topologies with Rocketfuel\", Proceedings of\
    \ ACM SIGCOMM,\n                2002.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Sukrit Dasgupta\n   Drexel University\n   Dept of ECE,\
    \ 3141 Chestnut Street\n   Philadelphia, PA  19104\n   USA\n   Phone: 215-895-1862\n\
    \   EMail: sukrit@ece.drexel.edu\n   URI:   www.pages.drexel.edu/~sd88\n   Jaudelice\
    \ C. de Oliveira\n   Drexel University\n   Dept. of ECE, 3141 Chestnut Street\n\
    \   Philadelphia, PA  19104\n   USA\n   Phone: 215-895-2248\n   EMail: jau@ece.drexel.edu\n\
    \   URI:   www.ece.drexel.edu/faculty/deoliveira\n   JP Vasseur\n   Cisco Systems\n\
    \   1414 Massachussetts Avenue\n   Boxborough, MA  01719\n   USA\n   EMail: jpv@cisco.com\n"
