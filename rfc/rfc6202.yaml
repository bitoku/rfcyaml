- title: __initial_text__
  contents:
  - "                    Known Issues and Best Practices\n    for the Use of Long\
    \ Polling and Streaming in Bidirectional HTTP\n"
- title: Abstract
  contents:
  - "Abstract\n   On today's Internet, the Hypertext Transfer Protocol (HTTP) is often\n\
    \   used (some would say abused) to enable asynchronous, \"server-\n   initiated\"\
    \ communication from a server to a client as well as\n   communication from a\
    \ client to a server.  This document describes\n   known issues and best practices\
    \ related to such \"bidirectional HTTP\"\n   applications, focusing on the two\
    \ most common mechanisms: HTTP long\n   polling and HTTP streaming.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc6202.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2011 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction . . . . . . . . . . . . . . . . . . .\
    \ . . . . . .  3\n   2.  HTTP Long Polling  . . . . . . . . . . . . . . . . .\
    \ . . . . .  4\n     2.1.  Definition . . . . . . . . . . . . . . . . . . . .\
    \ . . . .  4\n     2.2.  HTTP Long Polling Issues . . . . . . . . . . . . . .\
    \ . . .  5\n   3.  HTTP Streaming . . . . . . . . . . . . . . . . . . . . . .\
    \ . .  7\n     3.1.  Definition . . . . . . . . . . . . . . . . . . . . . . .\
    \ .  7\n     3.2.  HTTP Streaming Issues  . . . . . . . . . . . . . . . . . .\
    \  8\n   4.  Overview of Technologies . . . . . . . . . . . . . . . . . . . 10\n\
    \     4.1.  Bayeux . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n  \
    \   4.2.  BOSH . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n    \
    \ 4.3.  Server-Sent Events . . . . . . . . . . . . . . . . . . . . 13\n   5. \
    \ HTTP Best Practices  . . . . . . . . . . . . . . . . . . . . . 13\n     5.1.\
    \  Limits to the Maximum Number of Connections  . . . . . . . 13\n     5.2.  Pipelined\
    \ Connections  . . . . . . . . . . . . . . . . . . 14\n     5.3.  Proxies  . .\
    \ . . . . . . . . . . . . . . . . . . . . . . . 14\n     5.4.  HTTP Responses\
    \ . . . . . . . . . . . . . . . . . . . . . . 15\n     5.5.  Timeouts . . . .\
    \ . . . . . . . . . . . . . . . . . . . . . 15\n     5.6.  Impact on Intermediary\
    \ Entities  . . . . . . . . . . . . . 16\n   6.  Security Considerations  . .\
    \ . . . . . . . . . . . . . . . . . 16\n   7.  References . . . . . . . . . .\
    \ . . . . . . . . . . . . . . . . 17\n     7.1.  Normative References . . . .\
    \ . . . . . . . . . . . . . . . 17\n     7.2.  Informative References . . . .\
    \ . . . . . . . . . . . . . . 17\n   8.  Acknowledgments  . . . . . . . . . .\
    \ . . . . . . . . . . . . . 18\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The Hypertext Transfer Protocol [RFC2616] is a request/response\n\
    \   protocol.  HTTP defines the following entities: clients, proxies, and\n  \
    \ servers.  A client establishes connections to a server for the\n   purpose of\
    \ sending HTTP requests.  A server accepts connections from\n   clients in order\
    \ to service HTTP requests by sending back responses.\n   Proxies are intermediate\
    \ entities that can be involved in the\n   delivery of requests and responses\
    \ from the client to the server and\n   vice versa.\n   In the standard HTTP model,\
    \ a server cannot initiate a connection\n   with a client nor send an unrequested\
    \ HTTP response to a client;\n   thus, the server cannot push asynchronous events\
    \ to clients.\n   Therefore, in order to receive asynchronous events as soon as\n\
    \   possible, the client needs to poll the server periodically for new\n   content.\
    \  However, continual polling can consume significant\n   bandwidth by forcing\
    \ a request/response round trip when no data is\n   available.  It can also be\
    \ inefficient because it reduces the\n   responsiveness of the application since\
    \ data is queued until the\n   server receives the next poll request from the\
    \ client.\n   In order to improve this situation, several server-push programming\n\
    \   mechanisms have been implemented in recent years.  These mechanisms,\n   which\
    \ are often grouped under the common label \"Comet\" [COMET],\n   enable a web\
    \ server to send updates to clients without waiting for a\n   poll request from\
    \ the client.  Such mechanisms can deliver updates to\n   clients in a more timely\
    \ manner while avoiding the latency\n   experienced by client applications due\
    \ to the frequent opening and\n   closing of connections necessary to periodically\
    \ poll for data.\n   The two most common server-push mechanisms are HTTP long\
    \ polling and\n   HTTP streaming:\n   HTTP Long Polling:  The server attempts\
    \ to \"hold open\" (not\n      immediately reply to) each HTTP request, responding\
    \ only when\n      there are events to deliver.  In this way, there is always\
    \ a\n      pending request to which the server can reply for the purpose of\n\
    \      delivering events as they occur, thereby minimizing the latency in\n  \
    \    message delivery.\n   HTTP Streaming:  The server keeps a request open indefinitely;\
    \ that\n      is, it never terminates the request or closes the connection, even\n\
    \      after it pushes data to the client.\n   It is possible to define other\
    \ technologies for bidirectional HTTP;\n   however, such technologies typically\
    \ require changes to HTTP itself\n   (e.g., by defining new HTTP methods).  This\
    \ document focuses only on\n   bidirectional HTTP technologies that work within\
    \ the current scope of\n   HTTP as defined in [RFC2616] (HTTP 1.1) and [RFC1945]\
    \ (HTTP 1.0).\n   The authors acknowledge that both the HTTP long polling and\
    \ HTTP\n   streaming mechanisms stretch the original semantic of HTTP and that\n\
    \   the HTTP protocol was not designed for bidirectional communication.\n   This\
    \ document neither encourages nor discourages the use of these\n   mechanisms,\
    \ and takes no position on whether they provide appropriate\n   solutions to the\
    \ problem of providing bidirectional communication\n   between clients and servers.\
    \  Instead, this document merely\n   identifies technical issues with these mechanisms\
    \ and suggests best\n   practices for their deployment.\n   The remainder of this\
    \ document is organized as follows.  Section 2\n   analyzes the HTTP long polling\
    \ technique.  Section 3 analyzes the\n   HTTP streaming technique.  Section 4\
    \ provides an overview of the\n   specific technologies that use the server-push\
    \ technique.  Section 5\n   lists best practices for bidirectional HTTP using\
    \ existing\n   technologies.\n"
- title: 2.  HTTP Long Polling
  contents:
  - '2.  HTTP Long Polling

    '
- title: 2.1.  Definition
  contents:
  - "2.1.  Definition\n   With the traditional or \"short polling\" technique, a client\
    \ sends\n   regular requests to the server and each request attempts to \"pull\"\
    \n   any available events or data.  If there are no events or data\n   available,\
    \ the server returns an empty response and the client waits\n   for some time\
    \ before sending another poll request.  The polling\n   frequency depends on the\
    \ latency that the client can tolerate in\n   retrieving updated information from\
    \ the server.  This mechanism has\n   the drawback that the consumed resources\
    \ (server processing and\n   network) strongly depend on the acceptable latency\
    \ in the delivery of\n   updates from server to client.  If the acceptable latency\
    \ is low\n   (e.g., on the order of seconds), then the polling frequency can cause\n\
    \   an unacceptable burden on the server, the network, or both.\n   In contrast\
    \ with such \"short polling\", \"long polling\" attempts to\n   minimize both\
    \ the latency in server-client message delivery and the\n   use of processing/network\
    \ resources.  The server achieves these\n   efficiencies by responding to a request\
    \ only when a particular event,\n   status, or timeout has occurred.  Once the\
    \ server sends a long poll\n   response, typically the client immediately sends\
    \ a new long poll\n   request.  Effectively, this means that at any given time\
    \ the server\n   will be holding open a long poll request, to which it replies\
    \ when\n   new information is available for the client.  As a result, the server\n\
    \   is able to asynchronously \"initiate\" communication.\n   The basic life cycle\
    \ of an application using HTTP long polling is as\n   follows:\n   1.  The client\
    \ makes an initial request and then waits for a\n       response.\n   2.  The\
    \ server defers its response until an update is available or\n       until a particular\
    \ status or timeout has occurred.\n   3.  When an update is available, the server\
    \ sends a complete response\n       to the client.\n   4.  The client typically\
    \ sends a new long poll request, either\n       immediately upon receiving a response\
    \ or after a pause to allow\n       an acceptable latency period.\n   The HTTP\
    \ long polling mechanism can be applied to either persistent\n   or non-persistent\
    \ HTTP connections.  The use of persistent HTTP\n   connections will avoid the\
    \ additional overhead of establishing a new\n   TCP/IP connection [TCP] for every\
    \ long poll request.\n"
- title: 2.2.  HTTP Long Polling Issues
  contents:
  - "2.2.  HTTP Long Polling Issues\n   The HTTP long polling mechanism introduces\
    \ the following issues.\n   Header Overhead:  With the HTTP long polling technique,\
    \ every long\n      poll request and long poll response is a complete HTTP message\
    \ and\n      thus contains a full set of HTTP headers in the message framing.\n\
    \      For small, infrequent messages, the headers can represent a large\n   \
    \   percentage of the data transmitted.  If the network MTU (Maximum\n      Transmission\
    \ Unit) allows all the information (including the HTTP\n      header) to fit within\
    \ a single IP packet, this typically does not\n      represent a significant increase\
    \ in the burden for networking\n      entities.  On the other hand, the amount\
    \ of transferred data can\n      be significantly larger than the real payload\
    \ carried by HTTP, and\n      this can have a significant impact (e.g., when volume-based\n\
    \      charging is in place).\n   Maximal Latency:  After a long poll response\
    \ is sent to a client, the\n      server needs to wait for the next long poll\
    \ request before another\n      message can be sent to the client.  This means\
    \ that while the\n      average latency of long polling is close to one network\
    \ transit,\n      the maximal latency is over three network transits (long poll\n\
    \      response, next long poll request, long poll response).  However,\n    \
    \  because HTTP is carried over TCP/IP, packet loss and\n      retransmission\
    \ can occur; therefore, maximal latency for any\n      TCP/IP protocol will be\
    \ more than three network transits (lost\n      packet, next packet, negative\
    \ ack, retransmit).  When HTTP\n      pipelining (see Section 5.2) is available,\
    \ the latency due to the\n      server waiting for a new request can be avoided.\n\
    \   Connection Establishment:  A common criticism of both short polling\n    \
    \  and long polling is that these mechanisms frequently open TCP/IP\n      connections\
    \ and then close them.  However, both polling mechanisms\n      work well with\
    \ persistent HTTP connections that can be reused for\n      many poll requests.\
    \  Specifically, the short duration of the pause\n      between a long poll response\
    \ and the next long poll request avoids\n      the closing of idle connections.\n\
    \   Allocated Resources:  Operating systems will allocate resources to\n     \
    \ TCP/IP connections and to HTTP requests outstanding on those\n      connections.\
    \  The HTTP long polling mechanism requires that for\n      each client both a\
    \ TCP/IP connection and an HTTP request are held\n      open.  Thus, it is important\
    \ to consider the resources related to\n      both of these when sizing an HTTP\
    \ long polling application.\n      Typically, the resources used per TCP/IP connection\
    \ are minimal\n      and can scale reasonably.  Frequently, the resources allocated\
    \ to\n      HTTP requests can be significant, and scaling the total number of\n\
    \      requests outstanding can be limited on some gateways, proxies, and\n  \
    \    servers.\n   Graceful Degradation:  A long polling client or server that\
    \ is under\n      load has a natural tendency to gracefully degrade in performance\n\
    \      at a cost of message latency.  If load causes either a client or\n    \
    \  server to run slowly, then events to be pushed to the client will\n      queue\
    \ (waiting either for the client to send a long poll request\n      or for the\
    \ server to free up CPU cycles that can be used to\n      process a long poll\
    \ request that is being held at the server).  If\n      multiple messages are\
    \ queued for a client, they might be delivered\n      in a batch within a single\
    \ long poll response.  This can\n      significantly reduce the per-message overhead\
    \ and thus ease the\n      workload of the client or server for the given message\
    \ load.\n   Timeouts:  Long poll requests need to remain pending or \"hanging\"\
    \n      until the server has something to send to the client.  The timeout\n \
    \     issues related to these pending requests are discussed in\n      Section\
    \ 5.5.\n   Caching:  Caching mechanisms implemented by intermediate entities can\n\
    \      interfere with long poll requests.  This issue is discussed in\n      Section\
    \ 5.6.\n"
- title: 3.  HTTP Streaming
  contents:
  - '3.  HTTP Streaming

    '
- title: 3.1.  Definition
  contents:
  - "3.1.  Definition\n   The HTTP streaming mechanism keeps a request open indefinitely.\
    \  It\n   never terminates the request or closes the connection, even after the\n\
    \   server pushes data to the client.  This mechanism significantly\n   reduces\
    \ the network latency because the client and the server do not\n   need to open\
    \ and close the connection.\n   The basic life cycle of an application using HTTP\
    \ streaming is as\n   follows:\n   1.  The client makes an initial request and\
    \ then waits for a\n       response.\n   2.  The server defers the response to\
    \ a poll request until an update\n       is available, or until a particular status\
    \ or timeout has\n       occurred.\n   3.  Whenever an update is available, the\
    \ server sends it back to the\n       client as a part of the response.\n   4.\
    \  The data sent by the server does not terminate the request or the\n       connection.\
    \  The server returns to step 3.\n   The HTTP streaming mechanism is based on\
    \ the capability of the server\n   to send several pieces of information in the\
    \ same response, without\n   terminating the request or the connection.  This\
    \ result can be\n   achieved by both HTTP/1.1 and HTTP/1.0 servers.\n   An HTTP\
    \ response content length can be defined using three options:\n   Content-Length\
    \ header:  This indicates the size of the entity body in\n      the message, in\
    \ bytes.\n   Transfer-Encoding header:  The 'chunked' valued in this header\n\
    \      indicates the message will break into chunks of known size if\n      needed.\n\
    \   End of File (EOF):  This is actually the default approach for\n      HTTP/1.0\
    \ where the connections are not persistent.  Clients do not\n      need to know\
    \ the size of the body they are reading; instead they\n      expect to read the\
    \ body until the server closes the connection.\n      Although with HTTP/1.1 the\
    \ default is for persistent connections,\n      it is still possible to use EOF\
    \ by setting the 'Connection:close'\n      header in either the request or the\
    \ response, thereby indicating\n      that the connection is not to be considered\
    \ 'persistent' after the\n      current request/response is complete.  The client's\
    \ inclusion of\n      the 'Connection: close' header field in the request will\
    \ also\n      prevent pipelining.\n      The main issue with EOF is that it is\
    \ difficult to tell the\n      difference between a connection terminated by a\
    \ fault and one that\n      is correctly terminated.\n   An HTTP/1.0 server can\
    \ use only EOF as a streaming mechanism.  In\n   contrast, both EOF and \"chunked\
    \ transfer\" are available to an\n   HTTP/1.1 server.\n   The \"chunked transfer\"\
    \ mechanism is the one typically used by\n   HTTP/1.1 servers for streaming. \
    \ This is accomplished by including\n   the header \"Transfer-Encoding: chunked\"\
    \ at the beginning of the\n   response, which enables the server to send the following\
    \ parts of the\n   response in different \"chunks\" over the same connection.\
    \  Each chunk\n   starts with the hexadecimal expression of the length of its\
    \ data,\n   followed by CR/LF (the end of the response is indicated with a chunk\n\
    \   of size 0).\n           HTTP/1.1 200 OK\n           Content-Type: text/plain\n\
    \           Transfer-Encoding: chunked\n           25\n           This is the\
    \ data in the first chunk\n           1C\n           and this is the second one\n\
    \           0\n                   Figure 1: Transfer-Encoding response\n   To\
    \ achieve the same result, an HTTP/1.0 server will omit the Content-\n   Length\
    \ header in the response.  Thus, it will be able to send the\n   subsequent parts\
    \ of the response on the same connection (in this\n   case, the different parts\
    \ of the response are not explicitly\n   separated by HTTP protocol, and the end\
    \ of the response is achieved\n   by closing the connection).\n"
- title: 3.2.  HTTP Streaming Issues
  contents:
  - "3.2.  HTTP Streaming Issues\n   The HTTP streaming mechanism introduces the following\
    \ issues.\n   Network Intermediaries:  The HTTP protocol allows for intermediaries\n\
    \      (proxies, transparent proxies, gateways, etc.) to be involved in\n    \
    \  the transmission of a response from the server to the client.\n      There\
    \ is no requirement for an intermediary to immediately forward\n      a partial\
    \ response, and it is legal for the intermediary to buffer\n      the entire response\
    \ before sending any data to the client (e.g.,\n      caching transparent proxies).\
    \  HTTP streaming will not work with\n      such intermediaries.\n   Maximal Latency:\
    \  Theoretically, on a perfect network, an HTTP\n      streaming protocol's average\
    \ and maximal latency is one network\n      transit.  However, in practice, the\
    \ maximal latency is higher due\n      to network and browser limitations.  The\
    \ browser techniques used\n      to terminate HTTP streaming connections are often\
    \ associated with\n      JavaScript and/or DOM (Document Object Model) elements\
    \ that will\n      grow in size for every message received.  Thus, in order to\
    \ avoid\n      unlimited growth of memory usage in the client, an HTTP streaming\n\
    \      implementation occasionally needs to terminate the streaming\n      response\
    \ and send a request to initiate a new streaming response\n      (which is essentially\
    \ equivalent to a long poll).  Thus, the\n      maximal latency is at least three\
    \ network transits.  Also, because\n      HTTP is carried over TCP/IP, packet\
    \ loss and retransmission can\n      occur; therefore maximal latency for any\
    \ TCP/IP protocol will be\n      more than three network transits (lost packet,\
    \ next packet,\n      negative ack, retransmit).\n   Client Buffering:  There\
    \ is no requirement in existing HTTP\n      specifications for a client library\
    \ to make the data from a\n      partial HTTP response available to the client\
    \ application.  For\n      example, if each response chunk contains a statement\
    \ of\n      JavaScript, there is no requirement in the browser to execute that\n\
    \      JavaScript before the entire response is received.  However, in\n     \
    \ practice, most browsers do execute JavaScript received in partial\n      responses\
    \ -- although some require a buffer overflow to trigger\n      execution.  In\
    \ most implementations, blocks of white space can be\n      sent to achieve buffer\
    \ overflow.\n   Framing Techniques:  Using HTTP streaming, several application\n\
    \      messages can be sent within a single HTTP response.  The\n      separation\
    \ of the response stream into application messages needs\n      to be performed\
    \ at the application level and not at the HTTP\n      level.  In particular, it\
    \ is not possible to use the HTTP chunks\n      as application message delimiters,\
    \ since intermediate proxies\n      might \"re-chunk\" the message stream (for\
    \ example, by combining\n      different chunks into a longer one).  This issue\
    \ does not affect\n      the HTTP long polling technique, which provides a canonical\n\
    \      framing technique: each application message can be sent in a\n      different\
    \ HTTP response.\n"
- title: 4.  Overview of Technologies
  contents:
  - "4.  Overview of Technologies\n   This section provides an overview of existing\
    \ technologies that\n   implement HTTP-based server-push mechanisms to asynchronously\
    \ deliver\n   messages from the server to the client.\n"
- title: 4.1.  Bayeux
  contents:
  - "4.1.  Bayeux\n   The Bayeux protocol [BAYEUX] was developed in 2006-2007 by the\
    \ Dojo\n   Foundation.  Bayeux can use both the HTTP long polling and HTTP\n \
    \  streaming mechanisms.\n   In order to achieve bidirectional communications,\
    \ a Bayeux client\n   will use two HTTP connections to a Bayeux server so that\
    \ both server-\n   to-client and client-to-server messaging can occur asynchronously.\n\
    \   The Bayeux specification requires that implementations control\n   pipelining\
    \ of HTTP requests, so that requests are not pipelined\n   inappropriately (e.g.,\
    \ a client-to-server message pipelined behind a\n   long poll request).\n   In\
    \ practice, for JavaScript clients, such control over pipelining is\n   not possible\
    \ in current browsers.  Therefore, JavaScript\n   implementations of Bayeux attempt\
    \ to meet this requirement by\n   limiting themselves to a maximum of two outstanding\
    \ HTTP requests at\n   any one time, so that browser connection limits will not\
    \ be applied\n   and the requests will not be queued or pipelined.  While broadly\n\
    \   effective, this mechanism can be disrupted if non-Bayeux JavaScript\n   clients\
    \ simultaneously issue requests to the same host.\n   Bayeux connections are negotiated\
    \ between client and server with\n   handshake messages that allow the connection\
    \ type, authentication\n   method, and other parameters to be agreed upon between\
    \ the client and\n   the server.  Furthermore, during the handshake phase, the\
    \ client and\n   the server reveal to each other their acceptable bidirectional\n\
    \   techniques, and the client selects one from the intersection of those\n  \
    \ sets.\n   For non-browser or same-domain Bayeux, clients use HTTP POST requests\n\
    \   to the server for both the long poll request and the request to send\n   messages\
    \ to the server.  The Bayeux protocol packets are sent as the\n   body of the\
    \ HTTP messages using the \"application/json\" Internet media\n   type [RFC4627].\n\
    \   For browsers that are operating in cross-domain mode, Bayeux attempts\n  \
    \ to use Cross-Origin Resource Sharing [CORS] checking if the browser\n   and\
    \ server support it, so that normal HTTP POST requests can be used.\n   If this\
    \ mechanism fails, Bayeux clients use the \"JSONP\" mechanism as\n   described\
    \ in [JSONP].  In this last case, client-to-server messages\n   are sent as encoded\
    \ JSON on the URL query parameters, and server-to-\n   client messages are sent\
    \ as a JavaScript program that wraps the\n   message JSON with a JavaScript function\
    \ call to the already loaded\n   Bayeux implementation.\n"
- title: 4.2.  BOSH
  contents:
  - "4.2.  BOSH\n   BOSH, which stands for Bidirectional-streams Over Synchronous\
    \ HTTP\n   [BOSH], was developed by the XMPP Standards Foundation in 2003-2004.\n\
    \   The purpose of BOSH is to emulate normal TCP connections over HTTP\n   (TCP\
    \ is the standard connection mechanism used in the Extensible\n   Messaging and\
    \ Presence Protocol as described in [RFC6120]).  BOSH\n   employs the HTTP long\
    \ polling mechanism by allowing the server\n   (called a \"BOSH connection manager\"\
    ) to defer its response to a\n   request until it actually has data to send to\
    \ the client from the\n   application server itself (typically an XMPP server).\
    \  As soon as the\n   client receives a response from the connection manager,\
    \ it sends\n   another request to the connection manager, thereby ensuring that\
    \ the\n   connection manager is (almost) always holding a request that it can\n\
    \   use to \"push\" data to the client.\n   In some situations, the client needs\
    \ to send data to the server while\n   it is waiting for data to be pushed from\
    \ the connection manager.  To\n   prevent data from being pipelined behind the\
    \ long poll request that\n   is on hold, the client can send its outbound data\
    \ in a second HTTP\n   request over a second TCP connection.  BOSH forces the\
    \ server to\n   respond to the request it has been holding on the first connection\
    \ as\n   soon as it receives a new request from the client, even if it has no\n\
    \   data to send to the client.  It does so to make sure that the client\n   can\
    \ send more data immediately, if necessary -- even in the case\n   where the client\
    \ is not able to pipeline the requests -- while\n   simultaneously respecting\
    \ the two-connection limit discussed in\n   Section 5.1.\n   The number of long\
    \ poll request-response pairs is negotiated during\n   the first request sent\
    \ from the client to the connection manager.\n   Typically, BOSH clients and connection\
    \ managers will negotiate the\n   use of two pairs, although it is possible to\
    \ use only one pair or\n   more than two pairs.\n   The roles of the two request-response\
    \ pairs typically switch whenever\n   the client sends data to the connection\
    \ manager.  This means that\n   when the client issues a new request, the connection\
    \ manager\n   immediately answers the blocked request on the other TCP connection,\n\
    \   thus freeing it; in this way, in a scenario where only the client\n   sends\
    \ data, the even requests are sent over one connection, and the\n   odd ones are\
    \ sent over the other connection.\n   BOSH is able to work reliably both when\
    \ network conditions force\n   every HTTP request to be made over a different\
    \ TCP connection and\n   when it is possible to use HTTP/1.1 and then rely on\
    \ two persistent\n   TCP connections.\n   If the connection manager has no data\
    \ to send to the client for an\n   agreed amount of time (also negotiated during\
    \ the first request),\n   then the connection manager will respond to the request\
    \ it has been\n   holding with no data, and that response immediately triggers\
    \ a fresh\n   client request.  The connection manager does so to ensure that if\
    \ a\n   network connection is broken then both parties will realize that fact\n\
    \   within a reasonable amount of time.\n   Moreover, BOSH defines the negotiation\
    \ of an \"inactivity period\"\n   value that specifies the longest allowable inactivity\
    \ period (in\n   seconds).  This enables the client to ensure that the periods\
    \ with no\n   requests pending are never too long.\n   BOSH allows data to be\
    \ pushed immediately when HTTP pipelining is\n   available.  However, if HTTP\
    \ pipelining is not available and one of\n   the endpoints has just pushed some\
    \ data, BOSH will usually need to\n   wait for a network round-trip time until\
    \ the server is able to again\n   push data to the client.\n   BOSH uses standard\
    \ HTTP POST request and response bodies to encode\n   all information.\n   BOSH\
    \ normally uses HTTP pipelining over a persistent HTTP/1.1\n   connection.  However,\
    \ a client can deliver its POST requests in any\n   way permitted by HTTP 1.0\
    \ or HTTP 1.1.  (Although the use of HTTP\n   POST with pipelining is discouraged\
    \ in RFC 2616, BOSH employs various\n   methods, such as request identifiers,\
    \ to ensure that this usage does\n   not lead to indeterminate results if the\
    \ transport connection is\n   terminated prematurely.)\n   BOSH clients and connection\
    \ managers are not allowed to use Chunked\n   Transfer Coding, since intermediaries\
    \ might buffer each partial HTTP\n   request or response and only forward the\
    \ full request or response\n   once it is available.\n   BOSH allows the usage\
    \ of the Accept-Encoding and Content-Encoding\n   headers in the request and in\
    \ the response, respectively, and then\n   compresses the response body accordingly.\n\
    \   Each BOSH session can share the HTTP connection(s) it uses with other\n  \
    \ HTTP traffic, including other BOSH sessions and HTTP requests and\n   responses\
    \ completely unrelated to the BOSH protocol (e.g., Web page\n   downloads).\n"
- title: 4.3.  Server-Sent Events
  contents:
  - "4.3.  Server-Sent Events\n   W3C Server-Sent Events specification [WD-eventsource]\
    \ defines an API\n   that enables servers to push data to Web pages over HTTP\
    \ in the form\n   of DOM events.\n   The data is encoded as \"text/event-stream\"\
    \ content and pushed using\n   an HTTP streaming mechanism, but the specification\
    \ suggests disabling\n   HTTP chunking for serving event streams unless the rate\
    \ of messages\n   is high enough to avoid the possible negative effects of this\n\
    \   technique as described in Section 3.2.\n   However, it is not clear if there\
    \ are significant benefits to using\n   EOF rather than chunking with regards\
    \ to intermediaries, unless they\n   support only HTTP/1.0.\n"
- title: 5.  HTTP Best Practices
  contents:
  - '5.  HTTP Best Practices

    '
- title: 5.1.  Limits to the Maximum Number of Connections
  contents:
  - "5.1.  Limits to the Maximum Number of Connections\n   HTTP [RFC2616], Section\
    \ 8.1.4, recommends that a single user client\n   not maintain more than two connections\
    \ to any server or proxy, in\n   order to prevent the server from being overloaded\
    \ and to avoid\n   unexpected side effects in congested networks.  Until recently,\
    \ this\n   limit was implemented by most commonly deployed browsers, thus making\n\
    \   connections a scarce resource that needed to be shared within the\n   browser.\
    \  Note that the available JavaScript APIs in the browsers\n   hide the connections,\
    \ and the security model inhibits the sharing of\n   any resource between frames.\
    \  The new HTTP specification [HTTPBIS]\n   removes the two-connection limitation,\
    \ only encouraging clients to be\n   conservative when opening multiple connections.\
    \  In fact, recent\n   browsers have increased this limit to 6 or 8 connections;\
    \ however, it\n   is still not possible to discover the local limit, and usage\
    \ of\n   multiple frames and tabs still places 8 connections within easy\n   reach.\n\
    \   Web applications need to limit the number of long poll requests\n   initiated,\
    \ ideally to a single long poll that is shared between\n   frames, tabs, or windows\
    \ of the same browser.  However, the security\n   constraints of the browsers\
    \ make such sharing difficult.\n   A best practice for a server is to use cookies\
    \ [COOKIE] to detect\n   multiple long poll requests from the same browser and\
    \ to avoid\n   deferring both requests since this might cause connection starvation\n\
    \   and/or pipeline issues.\n"
- title: 5.2.  Pipelined Connections
  contents:
  - "5.2.  Pipelined Connections\n   HTTP [RFC2616] permits optional request pipelining\
    \ over persistent\n   connections.  Multiple requests can be enqueued before the\
    \ responses\n   arrive.\n   In the case of HTTP long polling, the use of HTTP\
    \ pipelining can\n   reduce latency when multiple messages need to be sent by\
    \ a server to\n   a client in a short period of time.  With HTTP pipelining, the\
    \ server\n   can receive and enqueue a set of HTTP requests.  Therefore, the\n\
    \   server does not need to receive a new HTTP request from the client\n   after\
    \ it has sent a message to the client within an HTTP response.\n   In principle,\
    \ the HTTP pipelining can be applied to HTTP GET and HTTP\n   POST requests, but\
    \ using HTTP POST requests is more critical.  In\n   fact, the use of HTTP POST\
    \ with pipelining is discouraged in RFC 2616\n   and needs to be handled with\
    \ special care.\n   There is an issue regarding the inability to control pipelining.\n\
    \   Normal requests can be pipelined behind a long poll, and are thus\n   delayed\
    \ until the long poll completes.\n   Mechanisms for bidirectional HTTP that want\
    \ to exploit HTTP\n   pipelining need to verify that HTTP pipelining is available\
    \ (e.g.,\n   supported by the client, the intermediaries, and the server); if\
    \ it's\n   not available, they need to fall back to solutions without HTTP\n \
    \  pipelining.\n"
- title: 5.3.  Proxies
  contents:
  - "5.3.  Proxies\n   Most proxies work well with HTTP long polling because a complete\
    \ HTTP\n   response will be sent either on an event or a timeout.  Proxies are\n\
    \   advised to return that response immediately to the user agent, which\n   immediately\
    \ acts on it.\n   The HTTP streaming mechanism uses partial responses and sends\
    \ some\n   JavaScript in an HTTP/1.1 chunk as described in Section 3.  This\n\
    \   mechanism can face problems caused by two factors: (1) it relies on\n   proxies\
    \ to forward each chunk (even though there is no requirement\n   for them to do\
    \ so, and some caching proxies do not), and (2) it\n   relies on user agents to\
    \ execute the chunk of JavaScript as it\n   arrives (even though there is also\
    \ no requirement for them to do so).\n   A \"reverse proxy\" basically is a proxy\
    \ that pretends to be the actual\n   server (as far as any client or client proxy\
    \ is concerned), but it\n   passes on the request to the actual server that is\
    \ usually sitting\n   behind another layer of firewalls.  Any HTTP short polling\
    \ or HTTP\n   long polling solution will work fine with this, as will most HTTP\n\
    \   streaming solutions.  The main downside is performance, since most\n   proxies\
    \ are not designed to hold many open connections.\n   Reverse proxies can come\
    \ to grief when they try to share connections\n   to the servers between multiple\
    \ clients.  As an example, Apache with\n   mod_jk shares a small set of connections\
    \ (often 8 or 16) between all\n   clients.  If long polls are sent on those shared\
    \ connections, then\n   the proxy can be starved of connections, which means that\
    \ other\n   requests (either long poll or normal) can be held up.  Thus, Comet\n\
    \   mechanisms currently need to avoid any connection sharing -- either\n   in\
    \ the browser or in any intermediary -- because the HTTP assumption\n   is that\
    \ each request will complete as fast as possible.\n   One of the main reasons\
    \ why both HTTP long polling and HTTP streaming\n   are perceived as having a\
    \ negative impact on servers and proxies is\n   that they use a synchronous programming\
    \ model for handling requests,\n   since the resources allocated to each request\
    \ are held for the\n   duration of the request.  Asynchronous proxies and servers\
    \ can handle\n   long polls using slightly more resources than normal HTTP traffic.\n\
    \   Unfortunately some synchronous proxies do exist (e.g., Apache mod_jk)\n  \
    \ and many HTTP application servers also have a blocking model for\n   their request\
    \ handling (e.g., the Java servlet 2.5 specification).\n"
- title: 5.4.  HTTP Responses
  contents:
  - "5.4.  HTTP Responses\n   In accordance with [RFC2616], the server responds to\
    \ a request it has\n   successfully received by sending a 200 OK answer, but only\
    \ when a\n   particular event, status, or timeout has occurred.  The 200 OK body\n\
    \   section contains the actual event, status, or timeout that occurred.\n   This\
    \ \"best practice\" is simply standard HTTP.\n"
- title: 5.5.  Timeouts
  contents:
  - "5.5.  Timeouts\n   The HTTP long polling mechanism allows the server to respond\
    \ to a\n   request only when a particular event, status, or timeout has\n   occurred.\
    \  In order to minimize (as much as possible) both latency in\n   server-client\
    \ message delivery and the processing/network resources\n   needed, the long poll\
    \ request timeout ought to be set to a high\n   value.\n   However, the timeout\
    \ value has to be chosen carefully; indeed,\n   problems can occur if this value\
    \ is set too high (e.g., the client\n   might receive a 408 Request Timeout answer\
    \ from the server or a 504\n   Gateway Timeout answer from a proxy).  The default\
    \ timeout value in a\n   browser is 300 seconds, but most network infrastructures\
    \ include\n   proxies and servers whose timeouts are not that long.\n   Several\
    \ experiments have shown success with timeouts as high as 120\n   seconds, but\
    \ generally 30 seconds is a safer value.  Therefore,\n   vendors of network equipment\
    \ wishing to be compatible with the HTTP\n   long polling mechanism are advised\
    \ to implement a timeout\n   substantially greater than 30 seconds (where \"substantially\"\
    \ means\n   several times more than the medium network transit time).\n"
- title: 5.6.  Impact on Intermediary Entities
  contents:
  - "5.6.  Impact on Intermediary Entities\n   There is no way for an end client or\
    \ host to signal to HTTP\n   intermediaries that long polling is in use; therefore,\
    \ long poll\n   requests are completely transparent for intermediary entities\
    \ and are\n   handled as normal requests.  This can have an impact on intermediary\n\
    \   entities that perform operations that are not useful in case of long\n   polling.\
    \  However, any capabilities that might interfere with\n   bidirectional flow\
    \ (e.g., caching) can be controlled with standard\n   headers or cookies.\n  \
    \ As a best practice, caching is always intentionally suppressed in a\n   long\
    \ poll request or response, i.e., the \"Cache-Control\" header is\n   set to \"\
    no-cache\".\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   This document is meant to describe current usage\
    \ of HTTP to enable\n   asynchronous or server-initiated communication.  It does\
    \ not propose\n   any change to the HTTP protocol or to the expected behavior\
    \ of HTTP\n   entities.  Therefore this document does not introduce new security\n\
    \   concerns into existing HTTP infrastructure.  The considerations\n   reported\
    \ hereafter refer to the solutions that are already\n   implemented and deployed.\n\
    \   One security concern with cross-domain HTTP long polling is related\n   to\
    \ the fact that often the mechanism is implemented by executing the\n   JavaScript\
    \ returned from the long poll request.  If the server is\n   prone to injection\
    \ attacks, then it could be far easier to trick a\n   browser into executing the\
    \ code [CORS].\n   Another security concern is that the number of open connections\
    \ that\n   needs to be maintained by a server in HTTP long polling and HTTP\n\
    \   streaming could more easily lead to denial-of-service (DoS) attacks\n   [RFC4732].\n"
- title: 7.  References
  contents:
  - '7.  References

    '
- title: 7.1.  Normative References
  contents:
  - "7.1.  Normative References\n   [RFC1945]         Berners-Lee, T., Fielding, R.,\
    \ and H. Nielsen,\n                     \"Hypertext Transfer Protocol -- HTTP/1.0\"\
    ,\n                     RFC 1945, May 1996.\n   [RFC2616]         Fielding, R.,\
    \ Gettys, J., Mogul, J., Frystyk, H.,\n                     Masinter, L., Leach,\
    \ P., and T. Berners-Lee,\n                     \"Hypertext Transfer Protocol\
    \ -- HTTP/1.1\",\n                     RFC 2616, June 1999.\n   [RFC4732]    \
    \     Handley, M., Rescorla, E., and IAB, \"Internet\n                     Denial-of-Service\
    \ Considerations\", RFC 4732,\n                     December 2006.\n"
- title: 7.2.  Informative References
  contents:
  - "7.2.  Informative References\n   [BAYEUX]          Russell, A., Wilkins, G.,\
    \ Davis, D., and M.\n                     Nesbitt, \"Bayeux Protocol -- Bayeux\
    \ 1.0.0\", 2007,\n                     <http://svn.cometd.com/trunk/bayeux/bayeux.html>.\n\
    \   [BOSH]            Paterson, I., Smith, D., and P. Saint-Andre,\n         \
    \            \"Bidirectional-streams Over Synchronous HTTP\n                 \
    \    (BOSH)\", XSF XEP 0124, February 2007.\n   [COMET]           Russell, A.,\
    \ \"Comet: Low Latency Data for the\n                     Browser\", March 2006,\
    \ <http://infrequently.org/\n                     2006/03/comet-low-latency-data-for-the-browser/\
    \ >.\n   [COOKIE]          Barth, A., \"HTTP State Management Mechanism\", Work\n\
    \                     in Progress, March 2011.\n   [CORS]            van Kesteren,\
    \ A., \"Cross-Origin Resource Sharing\",\n                     W3C Working Draft\
    \ WD-cors-20100727, latest version\n                     available at <http://www.w3.org/TR/cors/>,\n\
    \                     July 2010,\n                     <http://www.w3.org/TR/2010/WD-cors-20100727/>.\n\
    \   [HTTPBIS]         Fielding, R., Ed., Gettys, J., Mogul, J., Nielsen,\n   \
    \                  H., Masinter, L., Leach, P., Berners-Lee, T.,\n           \
    \          Lafon, Y., Ed., and J. Reschke, Ed., \"HTTP/1.1,\n                \
    \     part 1: URIs, Connections, and Message Parsing\",\n                    \
    \ Work in Progress, March 2011.\n   [JSONP]           Wikipedia, \"JSON with padding\"\
    ,\n                     <http://en.wikipedia.org/wiki/JSONP#JSONP>.\n   [RFC4627]\
    \         Crockford, D., \"The application/json Media Type for\n             \
    \        JavaScript Object Notation (JSON)\", RFC 4627,\n                    \
    \ July 2006.\n   [RFC6120]         Saint-Andre, P., \"Extensible Messaging and\
    \ Presence\n                     Protocol (XMPP): Core\", RFC 6120, March 2011.\n\
    \   [TCP]             Postel, J., \"Transmission Control Protocol\", STD 7,\n\
    \                     RFC 793, September 1981.\n   [WD-eventsource]  Hickson,\
    \ I., \"Server-Sent Events\", W3C Working\n                     Draft WD-eventsource-20091222,\
    \ latest version\n                     available at <http://www.w3.org/TR/eventsource/>,\n\
    \                     December 2009, <http://www.w3.org/TR/2009/\n           \
    \          WD-eventsource-20091222/>.\n"
- title: 8.  Acknowledgments
  contents:
  - "8.  Acknowledgments\n   Thanks to Joe Hildebrand, Julien Laganier, Jack Moffitt,\
    \ Subramanian\n   Moonesamy, Mark Nottingham, Julian Reschke, Martin Thomson,\
    \ and\n   Martin Tyler for their feedback.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Salvatore Loreto\n   Ericsson\n   Hirsalantie 11\n   Jorvas\
    \  02420\n   Finland\n   EMail: salvatore.loreto@ericsson.com\n   Peter Saint-Andre\n\
    \   Cisco\n   1899 Wyknoop Street, Suite 600\n   Denver, CO  80202\n   USA\n \
    \  Phone: +1-303-308-3282\n   EMail: psaintan@cisco.com\n   Stefano Salsano\n\
    \   University of Rome \"Tor Vergata\"\n   Via del Politecnico, 1\n   Rome  00133\n\
    \   Italy\n   EMail: stefano.salsano@uniroma2.it\n   Greg Wilkins\n   Webtide\n\
    \   EMail: gregw@webtide.com\n"
