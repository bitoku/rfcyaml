- title: __initial_text__
  contents:
  - "                            Workshop Report\n              Internet Research\
    \ Steering Group Workshop on\n                        Very-High-Speed Networks\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo is a report on a workshop sponsored by the\
    \ Internet\n   Research Steering Group.  This memo is for information only.  This\n\
    \   RFC does not specify an Internet standard.  Distribution of this memo\n  \
    \ is unlimited.\n"
- title: Introduction
  contents:
  - "Introduction\n   The goal of the workshop was to gather together a small number\
    \ of\n   leading researchers on high-speed networks in an environment\n   conducive\
    \ to lively thinking.  The hope is that by having such a\n   workshop the IRSG\
    \ has helped to stimulate new or improved research in\n   the area of high-speed\
    \ networks.\n   Attendance at the workshop was limited to fifty people, and attendees\n\
    \   had to apply to get in.  Applications were reviewed by a program\n   committee,\
    \ which accepted about half of them.  A few key individuals\n   were invited directly\
    \ by the program committee, without application.\n   The workshop was organized\
    \ by Dave Clark and Craig Partridge.\n   This workshop report is derived from\
    \ session writeups by each of the\n   session chairman, which were then reviewed\
    \ by the workshop\n   participants.\n"
- title: 'Session 1: Protocol Implementation (David D. Clark, Chair)'
  contents:
  - "Session 1: Protocol Implementation (David D. Clark, Chair)\n   This session was\
    \ concerned with what changes might be required in\n   protocols in order to achieve\
    \ very high-speed operation.\n   The session was introduced by David Clark (MIT\
    \ LCS), who claimed that\n   existing protocols would be sufficient to go at a\
    \ gigabit per second,\n   if that were the only goal.  In fact, proposals for\
    \ high-speed\n   networks usually include other requirements as well, such as\
    \ going\n   long distances, supporting many users, supporting new services such\n\
    \   as reserved bandwidth, and so on.  Only by examining the detailed\n   requirements\
    \ can one understand and compare various proposals for\n   protocols.  A variety\
    \ of techniques have been proposed to permit\n   protocols to operate at high\
    \ speeds, ranging from clever\n   implementation to complete relayering of function.\
    \  Clark asserted\n   that currently even the basic problem to be solved is not\
    \ clear, let\n   alone the proper approach to the solution.\n   Mats Bjorkman\
    \ (Uppsala University) described a project that involved\n   the use of an outboard\
    \ protocol processor to support high-speed\n   operation.  He asserted that his\
    \ approach would permit accelerated\n   processing of steady-state sequences of\
    \ packets.  Van Jacobson (LBL)\n   reported results that suggest that existing\
    \ protocols can operate at\n   high speeds without the need for outboard processors.\
    \  He also argued\n   that resource reservation can be integrated into a connectionless\n\
    \   protocol such as IP without losing the essence of the connectionless\n   architecture.\
    \  This is in contrast to a more commonly held belief\n   that full connection\
    \ setup will be necessary in order to support\n   resource reservation.  Jacobson\
    \ said that he has an experimental IP\n   gateway that supports resource reservation\
    \ for specific packet\n   sequences today.\n   Dave Borman (Cray Research) described\
    \ high-speed execution of TCP on\n   a Cray, where the overhead is most probably\
    \ the system and I/O\n   architecture rather than the protocol.  He believes that\
    \ protocols\n   such as TCP would be suitable for high-speed operation if the\
    \ windows\n   and sequence spaces were large enough. He reported that the current\n\
    \   speed of a TCP transfer between the processors of a Cray Y-MP was\n   over\
    \ 500 Mbps.  Jon Crowcroft (University College London) described\n   the current\
    \ network projects at UCL.  He offered a speculation that\n   congestion could\
    \ be managed in very high-speed networks by returning\n   to the sender any packets\
    \ for which transmission capacity was not\n   available.\n   Dave Feldmeier (Bellcore)\
    \ reported on the Bellcore participation in\n   the Aurora project, a joint experiment\
    \ of Bellcore, IBM, MIT, and\n   UPenn, which has the goal of installing and evaluating\
    \ two sorts of\n   switches at gigabit speeds between those four sites.  Bellcore\
    \ is\n   interested in switch and protocol design, and Feldmeier and his group\n\
    \   are designing and implementing a 1 Gbps transport protocol and\n   network\
    \ interface.  The protocol processor will have special support\n   for such things\
    \ as forward error correction to deal with ATM cell\n   loss in VLSI; a new FEC\
    \ code and chip design have been developed to\n   run at 1 Gbps.\n   Because of\
    \ the large number of speakers, there was no general\n   discussion after this\
    \ session.\n"
- title: 'Session 2: High-Speed Applications (Keith Lantz, Chair)'
  contents:
  - "Session 2: High-Speed Applications (Keith Lantz, Chair)\n   This session focused\
    \ on applications and the requirements they impose\n   on the underlying networks.\
    \  Keith Lantz (Olivetti Research\n   California) opened by introducing the concept\
    \ of the portable office\n   - a world where a user is able to take her work with\
    \ her wherever she\n   goes.  In such an office a worker can access the same services\
    \ and\n   the same people regardless of whether she is in the same building\n\
    \   with those services and people, at home, or at a distant site (such\n   as\
    \ a hotel) - or whether she is equipped with a highly portable,\n   multi-media\
    \ workstation, which she can literally carry with her\n   wherever she goes. \
    \ Thus, portable should be interpreted as referring\n   to portability of access\
    \ to services rather than to portability of\n   hardware.  Although not coordinated\
    \ in advance, each of the\n   presentations in this session can be viewed as a\
    \ perspective on the\n   portable office.\n   The bulk of Lantz's talk focused\
    \ on desktop teleconferencing - the\n   integration of traditional audio/video\
    \ teleconferencing technologies\n   with workstation-based network computing so\
    \ as to enable\n   geographically distributed individuals to collaborate, in real\
    \ time,\n   using multiple media (in particular, text, graphics, facsimile,\n\
    \   audio, and video) and all available computer-based tools, from their\n   respective\
    \ locales (i.e., office, home, or hotel).  Such a facility\n   places severe requirements\
    \ on the underlying network.  Specifically,\n   it requires support for several\
    \ data streams with widely varying\n   bandwidths (from a few Kbps to 1 Gbps)\
    \ but generally low delay, some\n   with minimal jitter (i.e., isochronous), and\
    \ all synchronized with\n   each other (i.e., multi-channel or media synchronization).\
    \  It\n   appears that high-speed network researchers are paying insufficient\n\
    \   attention to the last point, in particular.  For example, the bulk of\n  \
    \ the research on ATM has assumed that channels have independent\n   connection\
    \ request and burst statistics; this is clearly not the case\n   in the context\
    \ of desktop teleconferencing.\n   Lantz also stressed the need for adaptive protocols,\
    \ to accommodate\n   situations where the capacity of the network is exceeded,\
    \ or where it\n   is necessary to interoperate with low-speed networks, or where\
    \ human\n   factors suggest that the quality of service should change (e.g.,\n\
    \   increasing or decreasing the resolution of a video image).  Employing\n  \
    \ adaptive protocols suggests, first, that the interface to the network\n   protocols\
    \ must be hardware-independent and based only on quality of\n   service.  Second,\
    \ a variety of code conversion services should be\n   available, for example,\
    \ to convert from one audio encoding scheme to\n   another.  Promising examples\
    \ of adaptive protocols in the video\n   domain include variable-rate constant-quality\
    \ coding, layered or\n   embedded coding, progressive transmission, and (most\
    \ recently, at\n   UC-Berkeley) the extension of the concepts of structured graphics\
    \ to\n   video, such that the component elements of the video image are kept\n\
    \   logically separate throughout the production-to-presentation cycle.\n   Charlie\
    \ Catlett (National Center for Supercomputing Applications)\n   continued by analyzing\
    \ a specific scientific application, simulation\n   of a thunderstorm, with respect\
    \ to its network requirements.  The\n   application was analyzed from the standpoint\
    \ of identifying data flow\n   and the interrelationships between the computational\
    \ algorithms, the\n   supercomputer CPU throughput, the nature and size of the\
    \ data set,\n   and the available network services (throughput, delay, etc).\n\
    \   Simulation and the visualization of results typically involves\n   several\
    \ steps:\n      1.  Simulation\n      2.  Tessellation (transform simulation data\
    \ into three-dimensional\n          geometric volume descriptions, or polygons)\n\
    \      3.  Rendering (transform polygons into raster image)\n   For the thunderstorm\
    \ simulation, the simulation and tessellation are\n   currently done using a Cray\
    \ supercomputer and the resulting polygons\n   are sent to a Silicon Graphics\
    \ workstation to be rendered and\n   displayed.  The simulation creates data at\
    \ a rate of between 32 and\n   128 Mbps (depending on the number of Cray-2 processors\
    \ working on the\n   simulation) and the tessellation output data rate is in typically\
    \ in\n   the range of 10 to 100 Mbps, varying with the complexity of the\n   visualization\
    \ techniques.  The SGI workstation can display 100,000\n   polygons/sec which\
    \ for this example translates to up to 10\n   frames/sec.  Analysis tools such\
    \ as tracer particles and two-\n   dimensional slices are used interactively at\
    \ the workstation with\n   pre-calculated polygon sets.\n   In the next two to\
    \ three years, supercomputer speeds of 10-30 GFLOPS\n   and workstation speeds\
    \ of up to 1 GFLOPS and 1 million\n   polygons/second display are projected to\
    \ be available.  Increased\n   supercomputer power will yield a simulation data\
    \ creation rate of up\n   to several Gbps for this application.  The increased\
    \ workstation\n   power will allow both tessellation and rendering to be done\
    \ at the\n   workstation.  The use of shared window systems will allow multiple\n\
    \   researchers on the network to collaborate on a simulation, with the\n   possibility\
    \ of each scientist using his or her own visualization\n   techniques with the\
    \ tessellation process running on his or her\n   workstation.  Further developments,\
    \ such as network virtual memory,\n   will allow the tessellation processes on\
    \ the workstations to access\n   variables directly in supercomputer memory.\n\
    \   Terry Crowley (BBN Systems and Technologies) continued the theme of\n   collaboration,\
    \ in the context of real-time video and audio, shared\n   multimedia workspaces,\
    \ multimedia and video mail, distributed file\n   systems, scientific visualization,\
    \ network access to video and image\n   information, transaction processing systems,\
    \ and transferring data\n   and computational results between workstations and\
    \ supercomputers.\n   In general, such applications could help groups collaborate\
    \ by\n   directly providing communication channels (real-time video, shared\n\
    \   multimedia workspaces), by improving and expanding on the kinds of\n   information\
    \ that can be shared (multimedia and video mail,\n   supercomputer data and results),\
    \ and by reducing replication and the\n   complexity of sharing (distributed file\
    \ systems, network access to\n   video and image information).\n   Actual usage\
    \ patterns for these applications are hard to predict in\n   advance.  For example,\
    \ real-time video might be used for group\n   conferencing, for video phone calls,\
    \ for walking down the hall, or\n   for providing a long-term shared viewport\
    \ between remote locations in\n   order to help establish community ties.  Two\
    \ characteristics of\n   network traffic that we can expect are the need to provide\
    \ multiple\n   data streams to the end user and the need to synchronize these\n\
    \   streams.  These data streams will include real-time video, access to\n   stored\
    \ video, shared multimedia workspaces, and access to other\n   multimedia data.\
    \  A presentation involving multiple data streams must\n   be synchronized in\
    \ order to maintain cross-references between them\n   (e.g., pointing actions\
    \ within the shared multimedia workspace that\n   are combined with a voice request\
    \ to delete this and save that).\n   While much traffic will be point-to-point,\
    \ a significant amount of\n   traffic will involve conferences between multiple\
    \ sites.  A protocol\n   providing a multicast capability is critical.\n   Finally,\
    \ Greg Watson (HP) presented an overview of ongoing work at\n   the Hewlett-Packard\
    \ Bristol lab.  Their belief is that, while\n   applications for high-speed networks\
    \ employing supercomputers are the\n   the technology drivers, the economic drivers\
    \ will be applications\n   requiring moderate bandwidth (say 10 Mbps) that are\
    \ used by everyone\n   on the network.\n   They are investigating how multimedia\
    \ workstations can assist\n   distributed research teams - small teams of people\
    \ who are\n   geographically dispersed and who need to work closely on some area\
    \ of\n   research.  Each workstation provides multiple video channels,\n   together\
    \ with some distributed applications running on personal\n   computers.  The bandwidth\
    \ requirements per workstation are about 40\n   Mbps, assuming a certain degree\
    \ of compression of the video channels.\n   Currently the video is distributed\
    \ as an analog signal over CATV\n   equipment.  Ideally it would all be carried\
    \ over a single, unified\n   wide-area network operating in the one-to-several\
    \ Gbps range.\n   They have constructed a gigabit network prototype and are currently\n\
    \   experimenting with uncompressed video carried over the same network\n   as\
    \ normal data traffic.\n"
- title: 'Session 3: Lightwave Technology and its Implications (Ira Richer, Chair)'
  contents:
  - "Session 3: Lightwave Technology and its Implications (Ira Richer, Chair)\n  \
    \ Bob Kennedy (MIT) opened the session with a talk on network design in\n   an\
    \ era of excess bandwidth.  Kennedy's research is focused on multi-\n   purpose\
    \ networks in which bandwidth is not a scarce commodity,\n   networks with bandwidths\
    \ of tens of terahertz.  Kennedy points out\n   that a key challenge in such networks\
    \ is that electronics cannot keep\n   up with fiber speeds.  He proposes that\
    \ we consider all-optical\n   networks (in which all signals are optical) with\
    \ optoelectronic nodes\n   or gateways capable of recognizing and capturing only\
    \ traffic\n   destined for them, using time, frequency, or code divisions of the\n\
    \   huge bandwidth.  The routing algorithms in such networks would be\n   extremely\
    \ simple to avoid having to convert fiber-optics into slower\n   electronic pathways\
    \ to do switching.\n   Rich Gitlin (AT&T Bell Labs) gave a talk on issues and\
    \ opportunities\n   in broadband telecommunications networks, with emphasis on\
    \ the role\n   of fiber optic and photonic technology.  A three-level architecture\n\
    \   for a broadband telecommunications network was presented.  The\n   network\
    \ is B-ISDN/ATM 150 (Mbps) based and consists of: customer\n   premises equipment\
    \ (PBXs, LANs, multimedia terminals) that access the\n   network via a router/gateway,\
    \ a Network Node (which is a high\n   performance ATM packet switch) that serves\
    \ both as a LAN-to-LAN\n   interconnect and as a packet concentrator for traffic\
    \ destined for\n   CPE attached to other Network Nodes, and a backbone layer that\n\
    \   interconnects the NODES via a Digital Cross-Connect System that\n   provide\
    \ reconfigurable SONET circuits between the NODES (the use of\n   circuits minizes\
    \ delay and avoids the need for implementation of\n   peak-transmission-rate packet\
    \ switching).  Within this framework, the\n   most likely places for near-term\
    \ application of photonics, apart from\n   pure transport (ie, 150 Mbps channels\
    \ in a 2.4 Gbps SONET system),\n   are in the Cross-Connect (a Wavelength Division\
    \ Multiplexed based\n   structure was described) and in next-generation LANs that\
    \ provide\n   Gigabit per second throughputs by use of multiple fibers, concurrent\n\
    \   transmission, and new access mechanisms (such as store and forward).\n   A\
    \ planned interlocation Bell Labs multimedia gigabit/sec research\n   network,\
    \ LuckyNet, was described that attempts to extend many of the\n   above concepts\
    \ to achieve its principal goals: provision of a gigabit\n   per second capability\
    \ to a heterogeneous user community, the\n   stimulation of applications that\
    \ require Gpbs throughput (initial\n   applications are video conferencing and\
    \ LAN interconnect), and, to\n   the extent possible, be based on standards so\
    \ that interconnection\n   with other Gigabit testbeds is possible.\n"
- title: 'Session 4: High Speed Networks and the Phone System'
  contents:
  - "Session 4: High Speed Networks and the Phone System\n           (David Tennenhouse,\
    \ Chair)\n   David Tennenhouse (MIT) reported on the ATM workshop he hosted the\n\
    \   two days previous to this workshop.  His report will appear as part\n   of\
    \ the proceedings of his workshop.\n   Wally St. John (LANL) followed with a presentation\
    \ on the Los Alamos\n   gigabit testbed.  This testbed is based on the High Performance\n\
    \   Parallel Interface (HPPI) and on crossbar switch technology.  LANL\n   has\
    \ designed its own 16x16 crossbar switch and has also evaluated the\n   Network\
    \ Systems 8x8 crossbar switch. Future plans for the network\n   include expansion\
    \ to the CASA gigabit testbed.  The remote sites (San\n   Diego Supercomputer\
    \ Center, Caltech, and JPL) are configured\n   similarly to the LANL testbed.\
    \  The long-haul interface is from HPPI\n   to/from SONET (using ATM if in time).\n\
    \   Wally also discussed some of the problems related to building a\n   HPPI-SONET\
    \ gateway:\n      a)  Flow control.  The HPPI, by itself, is only readily extensible\n\
    \          to 64 km because of the READY-type flow control used in the\n     \
    \     physical layer.  The gateway will need to incorporate larger\n         \
    \ buffers and independent flow control.\n      b)  Error-rate expectations.  SONET\
    \ is only specified to have a\n          1E-10 BER on a per hop basis.  This is\
    \ inadequate for long\n          links.  Those in the know say that SONET will\
    \ be much better\n          but the designer is faced with the poor BER in the\
    \ SONET spec.\n      c)  Frame mapping.  There are several interesting issues\
    \ to be\n          considered in finding a good mapping from the HPPI packet\n\
    \          to the SONET frame.  Some are what SONET STS levels will be\n     \
    \     available in what time frame, the availability of concatenated\n       \
    \   service, and the error rate issue.\n   Dan Helman (UCSC) talked about work\
    \ he has been doing with Darrell\n   Long to examine the interconnection of Internet\
    \ networks via an ATM\n   B-ISDN network.  Since network interfaces and packet\
    \ processing are\n   the expensive parts of high-speed networks, they believe\
    \ it doesn't\n   make sense to use the ATM backbone only for transmission; it\
    \ should\n   be used for switching as well.  Therefore gateways (either shared\
    \ by\n   a subnet or integrated with fast hosts) are needed to encapsulate or\n\
    \   convert conventional protocols to ATM format.  Gateways will be\n   responsible\
    \ for caching connections to recently accessed\n   destinations.  Since many short-lived\
    \ low-bandwidth connections as\n   foreseen (e.g., for mail and ftp), routing\
    \ in the ATM network (to set\n   up connections) should not be complicated - a\
    \ form of static routing\n   should be adequate.  Connection performance can be\
    \ monitored by the\n   gateways.  Connections are reestablished if unacceptable.\
    \  All\n   decision making can be done by gateways and route servers at low\n\
    \   packet rates, rather than the high aggregate rate of the ATM network.\n  \
    \ One complicated issue to be addressed is how to transparently\n   introduce\
    \ an ATM backbone alongside the existing Internet.\n"
- title: 'Session 5: Distributed Systems (David Farber, Chair)'
  contents:
  - "Session 5: Distributed Systems (David Farber, Chair)\n   Craig Partridge (BBN\
    \ Systems and Technologies) started this session\n   by arguing that classic RPC\
    \ does not scale well to gigabit-speed\n   networks.  The gist of his argument\
    \ was that machines are getting\n   faster and faster, while the round-trip delay\
    \ of networks is staying\n   relatively constant because we cannot send faster\
    \ than the speed of\n   light.  As a result, the effective cost of doing a simple\
    \ RPC,\n   measured in instruction cycles spent waiting at the sending machine,\n\
    \   will become extremely high (millions of instruction cycles spent\n   waiting\
    \ for the reply to an RPC).  Furthermore, the methods currently\n   used to improve\
    \ RPC performance, such as futures and parallel RPC, do\n   not adequately solve\
    \ this problem.  Future requests will have to be\n   made much much earlier if\
    \ they are to complete by the time they are\n   needed.  Parallel RPC allows multiple\
    \ threads, but doesn't solve the\n   fact that each individual sequence of RPCs\
    \ still takes a very long\n   time.\n   Craig went on to suggest that there are\
    \ at least two possible ways\n   out of the problem.  One approach is to try to\
    \ do a lot of caching\n   (to waste bandwidth to keep the CPU fed).  A limitation\
    \ of this\n   approach is that at some point the cache becomes so big that you\
    \ have\n   to keep in consistent with other systems' caches, and you suddenly\n\
    \   find yourself doing synchronization RPCs to avoid doing normal RPCs\n   (oops!).\
    \  A more promising approach is to try to consolidate RPCs\n   being sent to the\
    \ same machine into larger operations which can be\n   sent as a single transaction,\
    \ run on the remote machine, and the\n   result returned.  (Craig noted that he\
    \ is pursuing this approach in\n   his doctoral dissertation at Harvard).\n  \
    \ Ken Schroder (BBN Systems and Technologies) gave a talk on the\n   challenges\
    \ of combining gigabit networks with wide-area heterogeneous\n   distributed operating\
    \ systems.  Ken feels the key goals of wide area\n   distributed systems will\
    \ be to support large volume data transfers\n   between users of conferencing\
    \ and similar applications, and to\n   deliver information to a large number of\
    \ end users sharing services\n   such as satellite image databases.  These distributed\
    \ systems will be\n   motivated by the natural distribution of users, of information\
    \ and of\n   expensive special purpose computer resources.\n   Ken pointed to\
    \ three of the key problems that must be addressed at\n   the system level in\
    \ these environments: how to provide high\n   utilization; how to manage consistency\
    \ and synchronization in the\n   presence of concurrency and non-determinism;\
    \ and how to construct\n   scalable system and application services.  Utilization\
    \ is key only to\n   high performance applications, where current systems would\
    \ be limited\n   by the cost of factors such as repeatedly copying messages,\n\
    \   converting data representations and switching between application and\n  \
    \ operating system.  Concurrency can be used improve performance, but\n   is also\
    \ likely to occur in many programs inadvertently because of\n   distribution.\
    \  Techniques are required both to exploit concurrency\n   when it is needed,\
    \ and to limit it when non-determinism can lead to\n   incorrect results.  Extensive\
    \ research on ensuring consistency and\n   resolving resource conflicts has been\
    \ done in the database area,\n   however distributed scheduling and the need for\
    \ high availability\n   despite partial system failures introduce special problems\
    \ that\n   require additional research.  Service scalability will be required\
    \ to\n   support customer needs as the size of the user community grow.  It\n\
    \   will require attention both ensuring that components do not break\n   when\
    \ they are subdivided across additional processors to support a\n   larger user\
    \ population, and to ensure that performance does to each\n   user can be affordably\
    \ maintained as new users are added.\n   In a bold presentation, Dave Cheriton\
    \ (Stanford) made a sweeping\n   argument that we are making a false dichotomy\
    \ between distributed\n   operating systems and networks.  In a gigabit world,\
    \ he argued, the\n   major resource in the system is the network, and in a normal\n\
    \   operating system we would expect such a critical resource to be\n   managed\
    \ by the operating system.  Or, put another way, the gigabit\n   network distributed\
    \ operating system should manage the network.\n   Cheriton went on to say that\
    \ if a gigabit distributed operating\n   system is managing the network, then\
    \ it is perfectly reasonable to\n   make the network very dumb (but fast) and\
    \ put the system intelligence\n   in the operating systems on the hosts that form\
    \ the distributed\n   system.\n   In another talk on interprocess communication,\
    \ Jonathan Smith (UPenn)\n   again raised the problem of network delay limiting\
    \ RPC performance.\n   In contrast to Partridge's earlier talk, Smith argued that\
    \ the\n   appropriate approach is anticipation or caching.  He justified his\n\
    \   argument with a simple cost example.  If a system is doing a page\n   fetch\
    \ between two systems which have a five millisecond round-trip\n   network delay\
    \ between them, the cost of fetching n pages is:\n                         5 msec\
    \ + (n-1) * 32 usec\n   Thus the cost of fetching an additional page is only 32\
    \ usec, but\n   underfetching and having to make another request to get a page\
    \ you\n   missed costs 5000 usec.  Based on these arguments, Smith suggested\n\
    \   that we re-examine work in virtual memory to see if there are\n   comfortable\
    \ ways to support distributed virtual memory with\n   anticipation.\n   In the\
    \ third talk on RPC in the session, Tommy Joseph (Olivetti), for\n   reasons similar\
    \ to those of Partridge and Smith, argued that we have\n   to get rid of RPC and\
    \ give programmers alternative programming\n   paradigms.  He sketched out ideas\
    \ for asynchronous paradigms using\n   causal consistency, in which systems ensure\
    \ that operations happen in\n   the proper order, without synchronizing through\
    \ a single system.\n"
- title: 'Session 6: Hosts and Host Interfaces (Gary Delp, Chair)'
  contents:
  - "Session 6: Hosts and Host Interfaces (Gary Delp, Chair)\n   Gary Delp (IBM Research)\
    \ discussed several issues involved in the\n   increase in speed of network attachment\
    \ to hosts of increasing\n   performance.  These issues included:\n      -  Media\
    \ Access - There are aspects of media access that are\n         best handled by\
    \ dedicated silicon, but there are also aspects\n         that are best left to\
    \ a general-purpose processor.\n      -  Compression - Some forms of compression/expansion\
    \ may belong\n         on the network interface; most will be application-specific.\n\
    \      -  Forward Error Correction - The predicted major packet loss\n       \
    \  mode is packet drops due to internal network congestion, rather\n         than\
    \ bit errors, so forward error correction internal to a\n         packet may not\
    \ be useful.  On the other hand, the latency cost\n         of not being able\
    \ to recover from bit errors is very high.\n         Some proposals were discussed\
    \ which suggest that FEC among\n         packet groups, with dedicated hardware\
    \ support, is the way\n         to go.\n      -  Encryption/Decryption - This\
    \ is a computationally intensive\n         task.  Most agree that if it is done\
    \ with all traffic, some\n         form of hardware support is helpful.  Where\
    \ does it fit in the\n         protocol stack?\n      -  Application Memory Mapping\
    \ - How much of the host memory\n         structure should be exposed to the network\
    \ interface?\n         Virtual memory and paging complicate this issue considerably.\n\
    \      -  Communication with Other Channel Controllers - Opinions were\n     \
    \    expressed that ranged from absolutely passive network\n         interfaces\
    \ to interfaces that run major portions of the\n         operating system and\
    \ bus arbitration codes.\n      -  Blocking/Segmentation - The consensus is that\
    \ B/S should\n         occur wherever the transport layer is processed.\n    \
    \  -  Routing - This is related to communications with other\n         controllers.\
    \  A routing-capable interface can reduce the bus\n         requirements by a\
    \ factor of two.\n      -  Intelligent participation in the host structure as\
    \ a gateway,\n         router, or bridge.\n      -  Presentation Layer issues\
    \ - All of the other overheads can be\n         completely overshadowed by this\
    \ issue if it is not solved well\n         and integrated into the overall host\
    \ architecture.  This points\n         out the need for some standardization of\
    \ representation (IEEE\n         floating point, etc.)\n   Eric Cooper (CMU) summarized\
    \ some initial experience with Nectar, a\n   high-speed fiber-optic LAN that has\
    \ been built at Carnegie Mellon.\n   Nectar consists of an arbitrary mesh of crossbar\
    \ switches connected\n   by means of 100 Mbps fiber-optic links.  Hosts are connected\
    \ to\n   crossbar switches via communication processor boards called CABs.\n \
    \  The CAB presents a memory-mapped interface to user processes and\n   off-loads\
    \ all protocol processing from the host.\n   Preliminary performance figures show\
    \ that latency is currently\n   limited by the number of VME operations required\
    \ by the host-to-CAB\n   shared memory interface in the course of sending and\
    \ receiving a\n   message.  The bottleneck in throughput is the speed of the VME\n\
    \   interface: although processes running on the CABs can communicate\n   over\
    \ Nectar at 70 Mbps, processes on the hosts are limited to\n   approximately 25\
    \ Mbps.\n   Jeff Mogul (DEC Western Research Lab) made these observations:\n \
    \  Although off-board protocol processors have been a popular means to\n   connect\
    \ a CPU to a network, they will be less useful in the future.\n   In the hypothetical\
    \ workstation of the late 1990s, with a 1000-MIPS\n   CPU and a Gbps LAN, an off-board\
    \ protocol processor will be of no\n   use.  The bottleneck will not be the computation\
    \ required to\n   implement the protocol, but the cost of moving the packet data\
    \ into\n   the CPU's cache and the cost of notifying the user process that the\n\
    \   data is available.  It will take far longer (hundreds of instruction\n   cycles)\
    \ to perform just the first cache miss (required to get the\n   packet into the\
    \ cache) than to perform all of the instructions\n   necessary to implement IP\
    \ and TCP (perhaps a hundred instructions).\n   A high-speed network interface\
    \ for a reasonably-priced system must be\n   designed with this cost structure\
    \ in mind; it should also eliminate\n   as many CPU interrupts as possible, since\
    \ interrupts are also very\n   expensive.  It makes more sense to let a user process\
    \ busy-wait on a\n   network-interface flag register than to suspend it and then\
    \ take an\n   interrupt; the normal CPU scheduling mechanism is more efficient\
    \ than\n   interrupts if the network interactions are rapid.\n   David Greaves\
    \ (Olivetti Research Ltd.) briefly described the need for\n   a total functionality\
    \ interface architecture that would allow the\n   complete elimination of communication\
    \ interrupts.  He described the\n   Cambridge high-speed ring as an ATM cell-like\
    \ interconnect that\n   currently runs at 500-1000 MBaud, and claims that ATM\
    \ at that speed\n   is a done deal.   Dave Tennenhouse also commented that ATM\
    \ at high\n   speeds with parallel processors is not the difficult thing that\n\
    \   several others have been claiming.\n   Bob Beach (Ultra Technologies) started\
    \ his talk with the observation\n   that networking could be really fast if only\
    \ we could just get rid of\n   the hosts.   He then supported his argument with\
    \ illustrations of\n   80-MByte/second transfers to frame buffers from Crays that\
    \ drop to\n   half that speed when the transfer is host-to-host.  Using null\n\
    \   network layers and proprietary MAC layers, the Ultra Net system can\n   communicate\
    \ application-to-application with ISO TP4 as the transport\n   layer at impressive\
    \ rates of speed.  The key to high-speed host\n   interconnects has been found\
    \ to be both large packets and large (on\n   the order of one megabyte) channel\
    \ transfer requests.  Direct DMA\n   interfaces exhibit much smaller transfer\
    \ latencies.\n   Derek McAuley (University Cambridge Computer Laboratory) described\n\
    \   work of the Fairisle project which is producing an ATM network based\n   on\
    \ fast packet switches.  A RISC processor (12 MIPS) is used in the\n   host interface\
    \ to do segmentation/reassembly/demultiplexing.  Line\n   rates of up to 150 Mbps\
    \ are possible even with this modest processor.\n   Derek has promised that performance\
    \ and requirement results from this\n   system will be published in the spring.\n\
    \   Bryan Lyles (XEROX PARC) volunteered to give an abbreviated talk in\n   exchange\
    \ for discussion rights.  He reported that Xerox PARC is\n   interested in ATM\
    \ technology and wants to install an ATM LAN at the\n   earliest possible opportunity.\
    \  Uses will include such applications\n   as video where guaranteed quality of\
    \ service (QOS) is required.  ATM\n   technology and the desire for guaranteed\
    \ QOS places a number of new\n   constraints on the host interface.  In particular,\
    \ they believe that\n   they will be forced towards rate-based congestion control.\
    \  Because\n   of implementation issues and burst control in the ATM switches,\
    \ the\n   senders will be forced to do rate based control on a cell-by-cell\n\
    \   basis.\n   Don Tolmie (Los Alamos National Laboratory) described the High-\n\
    \   Performance Parallel Interface (HPPI) of ANSI task group X3T9.3.  The\n  \
    \ HPPI is a standardized basic building block for implementing, or\n   connecting\
    \ to, networks at the Gbps speeds, be they ring, hub,\n   cross-bar, or long-haul\
    \ based.  The HPPI physical layer operates at\n   800 or 1600 Mbps over 25-meter\
    \ twisted-pair copper cables in a\n   point-to-point configuration.  The HPPI\
    \ physical layer has almost\n   completed the standards process, and a companion\
    \ HPPI data framing\n   standard is under way, and a Fiber Channel standard at\
    \ comparable\n   speeds is also being developed.  Major companies have completed,\
    \ or\n   are working on, HPPI interfaces for supercomputers, high-end\n   workstations,\
    \ fiber-optic extenders, and networking components.\n   The discussion at the\
    \ end of the session covered a range of topics.\n   The appropriateness of outboard\
    \ protocol processing was questioned.\n   Several people agreed that outboarding\
    \ on a Cray (or similar\n   cost/performance) machines makes economic sense. \
    \ Van Jacobson\n   contended that for workstations, a simple memory-mapped network\n\
    \   interface that provides packets visible to the host processor may\n   well\
    \ be the ideal solution.\n   Bryan Lyles reiterated several of his earlier points,\
    \ asserting that\n   when we talk about host interfaces and how to build them\
    \ we should\n   remember that we are really talking about process-to-process\n\
    \   communication, not CPU-to-CPU communication.  Not all processes run\n   on\
    \ the central CPU, e.g., graphics processors and multimedia.\n   Outboard protocol\
    \ processing may be a much better choice for these\n   architectures.\n   This\
    \ is especially true when we consider that memory/bus bandwidth is\n   often a\
    \ bottleneck.  When our systems run out of bandwidth, we are\n   forced towards\
    \ a NUMA model and multiple buses to localize memory\n   traffic.\n   Because\
    \ of QOS issues, the receiver must be able to tell the sender\n   how fast it\
    \ can send.  Throwing away cells (packets) will not work\n   because unwanted\
    \ packets will still clog the receiver's switch\n   interface, host interface,\
    \ and requires processing to throw away.\n"
- title: 'Session 7: Congestion Control (Scott Shenker, Chair)'
  contents:
  - "Session 7: Congestion Control (Scott Shenker, Chair)\n   The congestion control\
    \ session had six talks.  The first two talks\n   were rather general, discussing\
    \ new approaches and old myths.  The\n   other four talks discussed specific results\
    \ on various aspects of\n   packet (or cell) dropping: how to avoid drops, how\
    \ to mitigate their\n   impact on certain applications, a calculation of the end-to-end\n\
    \   throughput in the presence of drops, and how rate-based flow control\n   can\
    \ reduce buffer usage.  Thumbnail sketches of the talks follow.\n   In the first\
    \ of the general talks, Scott Shenker (XEROX PARC)\n   discussed how ideas from\
    \ economics can be applied to congestion\n   control.  Using economics, one can\
    \ articulate questions about the\n   goals of congestion control, the minimal\
    \ feedback necessary to\n   achieve those goals, and the incentive structure of\
    \ congestion\n   control.  Raj Jain (DEC) then discussed eight myths related to\n\
    \   congestion control in high-speed networks.  Among other points, Raj\n   argued\
    \ that (1) congestion problems will not become less important\n   when memory,\
    \ processors, and links become very fast and cheap, (2)\n   window flow control\
    \ is required along with rate flow control, and (3)\n   source-based controls\
    \ are required along with router-based control.\n   In the first of the more specific\
    \ talks, Isidro Castineyra (BBN\n   Communications Corporation) presented a back-of-the-envelope\n\
    \   calculation on the effect of cell drops on end-to-end throughput.\n   While\
    \ at extremely low drop rates the retransmission strategies of\n   go-back-n and\
    \ selective retransmission produced similar end-to-end\n   throughput, at higher\
    \ drop rates selective retransmission achieved\n   much higher throughput.  Next,\
    \ Tony DeSimone (AT&T) told us why\n   high-speed networks are not just fast low-speed\
    \ networks.   If the\n   buffer/window ratio is fixed, the drop rate decreases\
    \ as the network\n   speed increases.  Also, data was presented which showed that\
    \ adaptive\n   rate control can greatly decrease buffer utilization.  Jamal\n\
    \   Golestani (Bellcore) then presented his work on stop-and-go queueing.\n  \
    \ This is a simple stalling algorithm implemented at the switches which\n   guarantees\
    \ no dropped packets and greatly reduces delay jitter.  The\n   algorithm requires\
    \ prior bandwidth reservation and some flow control\n   on sources, and is compatible\
    \ with basic FIFO queues.  In the last\n   talk, Victor Frost (University of Kansas)\
    \ discussed the impact of\n   different dropping policies on the perceived quality\
    \ of a voice\n   connection.  When the source marks the drop priority of cells\
    \ and the\n   switch drops low priority cells first, the perceived quality of\
    \ the\n   connection is much higher than when cells are dropped randomly.\n"
- title: 'Session 8: Switch Architectures (Dave Sincoskie, Chair)'
  contents:
  - "Session 8: Switch Architectures (Dave Sincoskie, Chair)\n   Dave Mills (University\
    \ of Delaware) presented work on a project now\n   under way at the University\
    \ of Delaware to study architectures and\n   protocols for a high-speed network\
    \ and packet switch capable of\n   operation to the gigabit regime over distances\
    \ spanning the country.\n   It is intended for applications involving very large,\
    \ very fast, very\n   bursty traffic typical of supercomputing, remote sensing,\
    \ and\n   visualizing applications.  The network is assumed to be composed of\n\
    \   fiber trunks, while the switch architecture is based on a VLSI\n   baseband\
    \ crossbar design which can be configured for speeds from 25\n   Mbps to 1 Gbps.\n\
    \   Mills' approach involves an externally switched architecture in which\n  \
    \ the timing and routing of flows between crossbar switches are\n   determined\
    \ by sequencing tables and counters in high-speed memory\n   local to each crossbar.\
    \  The switch program is driven by a\n   reservation-TDMA protocol and distributed\
    \ scheduling algorithm\n   running in a co-located, general-purpose processor.\
    \  The end-to-end\n   customers are free to use any protocol or data format consistent\
    \ with\n   the timing of the network.  His primary interest in the initial\n \
    \  phases of the project is the study of appropriate reservation and\n   scheduling\
    \ algorithms.  He expect these algorithms to have much in\n   common with the\
    \ PODA algorithm used in the SATNET and WIDEBAND\n   satellite systems and to\
    \ the algorithms being considered for the\n   Multiple Satellite System (MSS).\n\
    \   John Robinson (JR, BBN Systems and Technologies) gave a talk called\n   Beyond\
    \ the Butterfly, which described work on a design for an ATM\n   cell switch,\
    \ known as MONET.  The talk described strategies for\n   buffering at the input\
    \ and output interfaces to a switch fabric\n   (crossbar or butterfly).  The main\
    \ idea was that cells should be\n   introduced to the switch fabric in random\
    \ sequence and to random\n   fabric entry ports to avoid persistent traffic patterns\
    \ having high\n   cell loss in the switch fabric, where losses arise due to contention\n\
    \   at output ports or within the switch fabric (in the case of a\n   butterfly).\
    \  Next, the relationship of this work to an earlier design\n   for a large-scale\
    \ parallel processor, the Monarch, was described.  In\n   closing, JR offered\
    \ the claim that this class of switch is realizable\n   in current technology\
    \ (barely) for operation over SONET OC-48 2.4\n   Gbps links.\n   Dave Sincoskie\
    \ (Bellcore) reported on two topics: recent switch\n   construction at Bellcore,\
    \ and high-speed processing of ATM cells\n   carrying VC or DG information.  Recent\
    \ switch design has resulted in\n   a switch architecture named SUNSHINE, a Batcher-banyan\
    \ switch which\n   uses recirculation and multiple output banyans to resolve contention\n\
    \   and increase throughput.  A paper on this switch will be published at\n  \
    \ ISS '90, and is available upon request from the author.  One of the\n   interesting\
    \ traffic results from simulations of SUNSHINE shows that\n   per-port output\
    \ queues of up to 1,000 cells (packets) may be\n   necessary for bursty traffic\
    \ patterns.  Also, Bill Marcus (at\n   Bellcore) has recently produced Batcher-banyan\
    \ (32x32) chips which\n   test up to 170Mb/sec per port.\n   The second point\
    \ in this talk was that there is little difference in\n   the switching processing\
    \ of Virtual Circuit (VC) and Datagram (DG)\n   traffic that which has been previously\
    \ broken into ATM cells at the\n   network edge.  The switch needs to do a header\
    \ translation operation\n   followed by some queueing (not necessarily FIFO).\
    \  The header\n   translation of the VC and DG cells differs mainly in the memory\n\
    \   organization of the address translation tables (dense vs. sparse).\n   The\
    \ discussion after the presentations seemed to wander off the topic\n   of switching,\
    \ back to some of the source-routing vs. network routing\n   issues discussed\
    \ earlier in the day.\n"
- title: 'Session 9: Open Mike Night (Craig Partridge, Chair)'
  contents:
  - "Session 9: Open Mike Night (Craig Partridge, Chair)\n   As an experiment, the\
    \ workshop held an open mike session during the\n   evening of the second day.\
    \  Participants were invited to speak for up\n   to five minutes on any subject\
    \ of their choice.  Minutes of this\n   session are sketchy because the chair\
    \ found himself pre-occupied by\n   keeping speakers roughly within their time\
    \ limits.\n   Charlie Catlett (NSCA) showed a film of the thunderstorm simulations\n\
    \   he discussed earlier.\n   Dave Cheriton (Stanford) made a controversial suggestion\
    \ that perhaps\n   one could manage congestion in the network simply by using\
    \ a steep\n   price curve, in which sending large amounts of data cost\n   exponentially\
    \ more than sending small amounts of data (thus leading\n   people only to ask\
    \ for large bandwidth when they needed it, and\n   having them pay so much, that\
    \ we can afford to give it to them).\n   Guru Parulkar (Washington University,\
    \ St. Louis) argued that the\n   recent discussion on appropriateness of existing\
    \ protocol and need\n   for new protocols (protocol architecture) for gigabit\
    \ networking\n   lacks the right focus.  The emphasis of the discussion should\
    \ be on\n   what is the right functionality for gigabit speeds, which is simpler\n\
    \   per packet processing, combination of rate and window based flow\n   control,\
    \ smart retransmission strategy, appropriate partitioning of\n   work among host\
    \ cpu+os, off board cpu, and custom hardware, and\n   others.  It is not surprising\
    \ that the existing protocols can be\n   modified to include this functionality.\
    \  By the same token, it is not\n   surprising that new protocols can be designed\
    \ which take advantage of\n   lessons of existing protocols and also include other\
    \ features\n   necessary for gigabit speeds.\n   Raj Jain (DEC) suggested we look\
    \ at new ways to measure protocol\n   performance, suggesting our current metrics\
    \ are insufficiently\n   informative.\n   Dan Helman (UCSC) asked the group to\
    \ consider, more carefully, who\n   exactly the users of the network will be.\
    \  Large consumers? or many\n   small consumers?\n"
- title: 'Session 10: Miscellaneous Topics (Bob Braden, Chair)'
  contents:
  - "Session 10: Miscellaneous Topics (Bob Braden, Chair)\n   As its title implies,\
    \ this session covered a variety of different\n   topics relating to high-speed\
    \ networking.\n   Jim Kurose (University of Massachussetts) described his studies\
    \ of\n   scheduling and discard policies for real-time (constrained delay)\n \
    \  traffic.  He showed that by enforcing local deadlines at switches\n   along\
    \ the path, it is possible to significantly reduce overall loss\n   for such traffic.\
    \  Since his results depend upon the traffic model\n   assumptions, he ended with\
    \ a plea for work on traffic models, stating\n   that Poisson models can sometimes\
    \ lead to results that are wrong by\n   many orders of magnitude.\n   Nachum Shacham\
    \ (SRI International) discussed the importance of error\n   correction schemes\
    \ that can recover lost cells, and as an example\n   presented a simple scheme\
    \ based upon longitudinal parity.  He also\n   showed a variant, diagonal parity,\
    \ which allows a single missing cell\n   to be recreated and its position in the\
    \ stream determined.\n   Two talks concerned high-speed LANs.  Biswanath Muhkerjee\
    \ (UC Davis)\n   surveyed the various proposals for fair scheduling on unidirectional\n\
    \   bus networks, especially those that are distance insensitive, i.e.,\n   that\
    \ can achieve 100% channel utilization independent of the bus\n   length and data\
    \ rate.  He described in particular his own scheme,\n   which he calls p-i persistant.\n\
    \   Howard Salwen (Proteon), speaking in place of Mehdi Massehi of IBM\n   Zurich\
    \ who was unable to attend, also discussed high-speed LAN\n   technologies.  At\
    \ 100 Mbps, a token ring has a clear advantage, but\n   at 1 Gbps, the speed of\
    \ light kills 802.6, for example.  He briefly\n   described Massehi's reservation-based\
    \ scheme, CRMA (Cyclic-\n   Reservation Multiple-Access).\n   Finally, Yechiam\
    \ Yemeni (YY, Columbia University) discussed his work\n   on a protocol silicon\
    \ compiler.  In order to exploit the potential\n   parallelism, he is planning\
    \ to use one processor per connection.\n   The session closed with a spirited\
    \ discussion of about the relative\n   merits of building an experimental network\
    \ versus simulating it.\n   Proponents of simulation pointed out the high cost\
    \ of building a\n   prototype and limitation on the solution space imposed by\
    \ a\n   particular hardware realization.  Proponents of building suggested\n \
    \  that artificial traffic can never explore the state space of a\n   network\
    \ as well as real traffic can, and that an experimental\n   prototype is important\
    \ for validating simulations.\n"
- title: 'Session 11: Protocol Architectures (Vint Cerf, Chair)'
  contents:
  - "Session 11: Protocol Architectures (Vint Cerf, Chair)\n   Nick Maxemchuk (AT&T\
    \ Bell Labs) summarized the distinctions between\n   circuit switching, virtual\
    \ circuits, and datagrams.  Circuits are\n   good for (nearly) constant rate sources.\
    \  Circuit switching dedicates\n   resources for the entire period of service.\
    \  You have to set up the\n   resource allocation before using it.  In a 1.7 Gbps\
    \ network, a 3000-\n   mile diameter consumes 10**7 bytes during the circuit set-up\
    \ round-\n   trip time, and potentially the same for circuit teardown.  Some\n\
    \   service requirements (file transfer, facsimile transmission) are far\n   smaller\
    \ than the wasted 2*10**7 bytes these circuit management delays\n   impose.  (Of\
    \ course, these costs are not as dramatic if the allocated\n   bandwidth is less\
    \ than the maximum possible.)\n   Virtual circuits allow shared use of bandwidth\
    \ (multiplexing) when\n   the primary source of traffic is idle (as in Voice Time\
    \ Assigned\n   Speech Interpolation).  The user notifies the network of planned\n\
    \   usage.\n   Datagrams (DG) are appropriate when there is no prior knowledge\
    \ of\n   use statistics or usage is far less than the capacity wasted during\n\
    \   circuit or virtual circuit set-up.  One can adaptively route traffic\n   among\
    \ equivalent resources.\n   In gigabit ATMs, the high service speed and decreased\
    \ cell size\n   increases the relative burstiness of service requests.  All of\
    \ these\n   characteristics combine to make DG service very attractive.\n   Maxemchuk\
    \ then described a deflection routing notion in which traffic\n   would be broken\
    \ into units of fixed length and allowed into the\n   network when capacity was\
    \ available and routed out by any available\n   channel, with preference being\
    \ given to the channel on the better\n   path.  This idea is similar to the hot\
    \ potato routing of Paul Baran's\n   1964 packet switching design.  With buffering\
    \ (one buffer), Maxemchuk\n   achieved a theoretical 90% utilization.  Large reassembly\
    \ buffers\n   provide for better throughput.\n   Maxemchuk did not have an answer\
    \ to the question: how do you make\n   sure empty \"slots\" are available where\
    \ needed? This is rather like\n   the problem encountered by D. Davies at the\
    \ UK National Physical\n   Laboratory in his isarythmic network design in which\
    \ a finite number\n   of crates are available for data transport throughout the\
    \ network.\n   Guru Parulkar (Washington University, St. Louis) presented a broad\n\
    \   view of an Internet architecture in which some portion of the system\n   would\
    \ operate at gigabit speeds.  In his model, internet, transport,\n   and application\
    \ protocols would operate end to end.  The internet\n   functions would be reflected\
    \ in gateways and in the host/net\n   interface, as they are in the current Internet.\
    \  However, the\n   internet would support a new type of service called a congram\
    \ which\n   aims at combining strengths of both soft connection and datagram.\n\
    \   In this architecture, a variable grade of service would be provided.\n   Users\
    \ could request congrams (UCON) or the system could set them up\n   internally\
    \ (Picons) to avoid end-to-end setup latency.  The various\n   grades of service\
    \ could be requested, conceptually, by asserting\n   various required (desired)\
    \ levels of error control, throughput,\n   delay, interarrival jitter, and so\
    \ on.  Gateways based on ATM\n   switches, for example, would use packet processors\
    \ at entry/exit to\n   do internet specific per packet processing, which may include\n\
    \   fragmentation and reassembly of packets (into and out of ATM cells).\n   At\
    \ the transport level, Parulkar argued for protocols which can\n   provide application-oriented\
    \ flow and error control with simple per\n   packet processing.  He also mentioned\
    \ the notion of a generalized RPC\n   (GRPC) in which code, data, and execution\
    \ might be variously local or\n   remote from the procedure initiator.  GRPC can\
    \ be implemented using\n   network level virtual storage mechanisms.\n   The basic\
    \ premise of Raj Yavatkar's presentation (University of\n   Kentucky) was that\
    \ processes requiring communication service would\n   specify their needs in terms\
    \ of peak and average data rate as well as\n   defining burst parameters (frequency\
    \ and size).  Bandwidth for a\n   given flow would be allocated at the effective\
    \ data rate that is\n   computed on the basis of flow parameters.  The effective\
    \ data rate\n   lies somewhere between the peak and average data rate based on\
    \ the\n   burst parameters.  Statistical multiplexing would take up the gap\n\
    \   between peak and effective rate when a sudden burst of traffic\n   arrives.\
    \  Bounds on packet loss rate can be computed for a given set\n   of flow parameters\
    \ and corresponding effective data rate.\n   This presentation led to a discussion\
    \ about deliberate disciplining\n   of inter-process communication demands to\
    \ match the requested flow\n   (service) profile.  This point was made in response\
    \ to the\n   observation that we often have little information about program\n\
    \   behavior and might have trouble estimating the network service\n   requirements\
    \ of any particular program.\n"
- title: Architectural Discussion
  contents:
  - "Architectural Discussion\n   An attempt was made to conduct a high-level discussion\
    \ on various\n   architectural questions.  The discussion yielded a variety of\n\
    \   opinions:\n      1.  The Internet would continue to exist in a form similar\n\
    \          to its current incarnation, and gateways would be required,\n     \
    \     at least to interface the existing facilities to the high\n          speed\
    \ packet switching environment.\n      2.  Strong interest was expressed by some\
    \ participants in access\n          to raw (naked ATM) services.  This would permit\
    \ users\n          to construct their own gigabit nets, at the IP level, at any\n\
    \          rate.  The extreme view of this was taken by David Cheriton\n     \
    \     who would prefer to have control over routing decisions and\n          other\
    \ behavior of the ATM network.\n      3.  The speed of light problem (latency,\
    \ round-trip delay)\n          is not going to go away and will have serious impact\
    \ on\n          control of the system.  The optimistic view was taken,\n     \
    \     for example, by Craig Partridge and Van Jacobson, who felt\n          that\
    \ many of the existing network and communications\n          management mechanisms\
    \ used in the present Internet protocols\n          would suffice, if suitably\
    \ implemented, at higher speeds.\n          A less rosy view was taken by David\
    \ Clark who observed\n          (as did others) that many transactions would be\
    \ serviced in\n          much less than one round-trip time, so that any end-to-end\n\
    \          controls would be largely useless.\n      4.  For applications requiring\
    \ fixed, periodic service,\n          reservation of resource seemed reasonably\
    \ attractive to many\n          participants, as long as the service period dominated\
    \ the\n          set-up time (round-trip delay) by an appreciable\n          margin.\n\
    \      5.  There was much discussion throughout the workshop of\n          congestion\
    \ control and flow control.  Although these\n          problems were not new,\
    \ they took on somewhat newer\n          character in the presence of much higher\
    \ round-trip delays\n          (measured in bits outstanding).  One view is that\
    \ end-to-end\n          flow control is needed, in any case, to moderate sources\n\
    \          sending to limited bandwidth receivers.  End-to-end flow\n        \
    \  control may not, however, be sufficient to protect the\n          interior\
    \ of the network from congestion problems, so\n          additional, intra-network\
    \ means are needed to cope with\n          congestion hot spots.   Eventually\
    \ such conditions\n          have to be reflected to the periphery of the network\
    \ to\n          moderate traffic sources.\n      6.  There was disagreement on\
    \ the build or simulate\n           question.  One view was in favor of building\
    \ network\n          components so as to collect and understand live application\n\
    \          data.  Another view held that without some careful\n          simulation,\
    \ one might have little idea what to build\n          (for example, Sincoskie's\
    \ large buffer pool requirement was\n          not apparent until the system was\
    \ simulated).\n"
- title: Comments from Workshop Evaluation Forms
  contents:
  - "Comments from Workshop Evaluation Forms\n   At the end of the IRSG workshop,\
    \ we asked attendees to fill out an\n   evaluation form.  Of the fifty-one attendees,\
    \ twenty-nine (56%)\n   turned in a form.\n   The evaluation form asked attendees\
    \ to answer two questions:\n      #1.  Do you feel that having attended this workshop\
    \ will help you\n           in your work on high speed networks during the next\
    \ year?\n      #2.  What new ideas, questions, or issues, did you feel were\n\
    \           brought up in the workshop?\n   In this section we discuss the answers\
    \ we got to both questions.\n"
- title: 'Question #1'
  contents:
  - "Question #1\n   There was a satisfying unanimity of opinion on question #1. \
    \ Twenty-\n   four attendees answered yes, often strongly (e.g., Absolutely and\n\
    \   very much so).  Of the remaining five respondents, three said they\n   expected\
    \ it to have some effect on their research and only two said\n   the workshop\
    \ would have little or no effect.\n   Some forms had some additional notes about\
    \ why the workshop helped\n   them.  Several people mentioned that there was considerable\
    \ benefit\n   to simply meeting and talking with people they hadn't met before.\
    \  A\n   few other people noted that the workshop had broadened their\n   perspective,\
    \ or improved their understanding of certain issues.  A\n   couple of people noted\
    \ that they'd heard ideas they thought they\n   could use immediately in their\
    \ research.\n"
- title: 'Question #2'
  contents:
  - "Question #2\n   Almost everyone listed ideas they'd seen presented at the conference\n\
    \   which were new to them.\n   It is clear that which new ideas were important\
    \ was a matter of\n   perspective - the workshop membership was chosen to represent\
    \ a broad\n   spectrum of specialties, and people in different specialities were\n\
    \   intrigued by different ideas.  However, there were some general\n   themes\
    \ raised in many questionnaires:\n      (1)  Limitations of our traffic models.\
    \  This particular subject\n           was mentioned, in some form, on many forms.\
    \  The attendees\n           generally felt we didn't understand how network traffic\
    \ would\n           behave on a gigabit network, and were concerned that people\n\
    \           might design (or worse, standardize) network protocols for\n     \
    \      high speed networks that would prove inadequate when used\n           with\
    \ real traffic.  Questions were raised about closed-loop\n           vs. open-loop\
    \ traffic models and the effects of varying types\n           of service.  This\
    \ concern led several people to encourage the\n           construction of a high-speed\
    \ testbed, so we can actually see\n           some real traffic.\n      (2)  Congestion\
    \ control.  Despite the limitations of our traffic\n           models, respondents\
    \ felt that congestion control at both\n           switching elements and network\
    \ wide was going to be even more\n           important than today, due to the\
    \ wider mix of traffic that\n           will appear on gigabit networks.  Most\
    \ forms mentioned at\n           least one of the congestion control talks as\
    \ a containing a\n           new idea.  The talks by Victor Frost, Jamal Golestani\
    \ and\n           Scott Shenker received the most praise.  Some attendees were\n\
    \           also interested in methods for keeping the lower-layer\n         \
    \  switching fabric from getting congested and mentioned the\n           talks\
    \ by Robinson and Maxemchuk as of interest.\n      (3)  Effects of fixed delay.\
    \  While the reviews were by no means\n           unanimous, many people had come\
    \ to the conclusion that the\n           most serious problem in gigabit networking\
    \ was not bandwidth,\n           but delay.  The workshop looked at this issue\
    \ in several\n           guises, and most people listed at least one aspect of\
    \ fixed\n           delay as a challenging new problem.  Questions that people\n\
    \           mentioned include:\n    -    How to avoid a one round-trip set up\
    \ delay, for less than one\n         round-trip time's worth of data?\n    - \
    \   How to recover from error without retransmission (and thus\n         additional\
    \ network delays)?  Several people were intrigued by\n         Nachum Shacham's\
    \ work on error detection and recovery.\n    -    Should we use window flow-control\
    \ or rate-based flow control\n         when delays were long?\n    -    Can we\
    \ modify the idea of remote procedure calls to deal with\n         the fact that\
    \ delays are relatively long?\n"
- title: A couple of attendees noted that while some of these problems looked
  contents:
  - 'A couple of attendees noted that while some of these problems looked

    '
- title: similar to those of today, the subtle differences caused by operating a
  contents:
  - 'similar to those of today, the subtle differences caused by operating a

    '
- title: network at gigabit speeds led them to believe the actual approaches to
  contents:
  - 'network at gigabit speeds led them to believe the actual approaches to

    '
- title: solving these problems would have to be very different from those of
  contents:
  - 'solving these problems would have to be very different from those of

    '
- title: today.
  contents:
  - 'today.

    '
- title: Security Considerations
  contents:
  - "Security Considerations\n   Security issues are not discussed in this memo.\n"
- title: Author's Address
  contents:
  - "Author's Address\n   Craig Partridge\n   Bolt Beranek and Newman Inc.\n   50\
    \ Moulton Street\n   Cambridge, MA 02138\n   Phone: (617) 873-2459\n   EMail:\
    \ craig@BBN.COM\n"
