- title: __initial_text__
  contents:
  - '           An Informal Management Model for Diffserv Routers

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2002).  All Rights Reserved.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document proposes an informal management model of Differentiated\n\
    \   Services (Diffserv) routers for use in their management and\n   configuration.\
    \  This model defines functional datapath elements\n   (e.g., classifiers, meters,\
    \ actions, marking, absolute dropping,\n   counting, multiplexing), algorithmic\
    \ droppers, queues and schedulers.\n   It describes possible configuration parameters\
    \ for these elements and\n   how they might be interconnected to realize the range\
    \ of traffic\n   conditioning and per-hop behavior (PHB) functionalities described\
    \ in\n   the Diffserv Architecture.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   4.1 Definition .................................................\
    \   13\n   4.1.1 Filters ..................................................  \
    \ 15\n   4.1.2 Overlapping Filters ......................................   15\n\
    \   4.2 Examples ...................................................   16\n  \
    \ 4.2.1 Behavior Aggregate (BA) Classifier .......................   16\n   4.2.2\
    \ Multi-Field (MF) Classifier ..............................   17\n   4.2.3 Free-form\
    \ Classifier .....................................   17\n   4.2.4 Other Possible\
    \ Classifiers ...............................   18\n   5 Meters .......................................................\
    \   19\n   5.1 Examples ...................................................  \
    \ 20\n   5.1.1 Average Rate Meter .......................................   20\n\
    \   5.1.2 Exponential Weighted Moving Average (EWMA) Meter .........   21\n  \
    \ 5.1.3 Two-Parameter Token Bucket Meter .........................   21\n   5.1.4\
    \ Multi-Stage Token Bucket Meter ...........................   22\n   5.1.5 Null\
    \ Meter ...............................................   23\n   6 Action Elements\
    \ ..............................................   23\n   6.1 DSCP Marker ................................................\
    \   24\n   6.2 Absolute Dropper ...........................................  \
    \ 24\n   6.3 Multiplexor ................................................   25\n\
    \   6.4 Counter ....................................................   25\n  \
    \ 6.5 Null Action ................................................   25\n   7\
    \ Queuing Elements .............................................   25\n   7.1\
    \ Queuing Model ..............................................   26\n   7.1.1\
    \ FIFO Queue ...............................................   27\n   7.1.2 Scheduler\
    \ ................................................   28\n   7.1.3 Algorithmic\
    \ Dropper ......................................   30\n   7.2 Sharing load among\
    \ traffic streams using queuing ...........   33\n   7.2.1 Load Sharing .............................................\
    \   34\n   7.2.2 Traffic Priority .........................................  \
    \ 35\n   8 Traffic Conditioning Blocks (TCBs) ...........................   35\n\
    \   8.1 TCB ........................................................   36\n  \
    \ 8.1.1 Building blocks for Queuing ..............................   37\n   8.2\
    \ An Example TCB .............................................   37\n   8.3 An\
    \ Example TCB to Support Multiple Customers ...............   42\n   8.4 TCBs\
    \ Supporting Microflow-based Services ...................   44\n   8.5 Cascaded\
    \ TCBs ..............................................   47\n   9 Security Considerations\
    \ ......................................   47\n   10 Acknowledgments .............................................\
    \   47\n   11 References ..................................................  \
    \ 47\n   Appendix A. Discussion of Token Buckets and Leaky Buckets ......   50\n\
    \   Authors' Addresses .............................................   55\n  \
    \ Full Copyright Statement........................................   56\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Differentiated Services (Diffserv) [DSARCH] is a set of\
    \ technologies\n   which allow network service providers to offer services with\n\
    \   different kinds of network quality-of-service (QoS) objectives to\n   different\
    \ customers and their traffic streams.  This document uses\n   terminology defined\
    \ in [DSARCH] and [NEWTERMS] (some of these\n   definitions are included here\
    \ in Section 2 for completeness).\n   The premise of Diffserv networks is that\
    \ routers within the core of\n   the network handle packets in different traffic\
    \ streams by forwarding\n   them using different per-hop behaviors (PHBs).  The\
    \ PHB to be applied\n   is indicated by a Diffserv codepoint (DSCP) in the IP\
    \ header of each\n   packet [DSFIELD].  The DSCP markings are applied either by\
    \ a trusted\n   upstream node, e.g., a customer, or by the edge routers on entry\
    \ to\n   the Diffserv network.\n   The advantage of such a scheme is that many\
    \ traffic streams can be\n   aggregated to one of a small number of behavior aggregates\
    \ (BA),\n   which are each forwarded using the same PHB at the router, thereby\n\
    \   simplifying the processing and associated storage.  In addition,\n   there\
    \ is no signaling other than what is carried in the DSCP of each\n   packet, and\
    \ no other related processing that is required in the core\n   of the Diffserv\
    \ network since QoS is invoked on a packet-by-packet\n   basis.\n   The Diffserv\
    \ architecture enables a variety of possible services\n   which could be deployed\
    \ in a network.  These services are reflected\n   to customers at the edges of\
    \ the Diffserv network in the form of a\n   Service Level Specification (SLS -\
    \ see [NEWTERMS]).  Whilst further\n   discussion of such services is outside\
    \ the scope of this document\n   (see [PDBDEF]), the ability to provide these\
    \ services depends on the\n   availability of cohesive management and configuration\
    \ tools that can\n   be used to provision and monitor a set of Diffserv routers\
    \ in a\n   coordinated manner.  To facilitate the development of such\n   configuration\
    \ and management tools it is helpful to define a\n   conceptual model of a Diffserv\
    \ router that abstracts away\n   implementation details of particular Diffserv\
    \ routers from the\n   parameters of interest for configuration and management.\
    \  The purpose\n   of this document is to define such a model.\n   The basic forwarding\
    \ functionality of a Diffserv router is defined in\n   other specifications; e.g.,\
    \ [DSARCH, DSFIELD, AF-PHB, EF-PHB].\n   This document is not intended in any\
    \ way to constrain or to dictate\n   the implementation alternatives of Diffserv\
    \ routers.  It is expected\n   that router implementers will demonstrate a great\
    \ deal of variability\n   in their implementations.  To the extent that implementers\
    \ are able\n   to model their implementations using the abstractions described\
    \ in\n   this document, configuration and management tools will more readily\n\
    \   be able to configure and manage networks incorporating Diffserv\n   routers\
    \ of assorted origins.\n   This model is intended to be abstract and capable of\
    \ representing the\n   configuration parameters important to Diffserv functionality\
    \ for a\n   variety of specific router implementations.  It is not intended as\
    \ a\n   guide to system implementation nor as a formal modeling description.\n\
    \   This model serves as the rationale for the design of an SNMP MIB\n   [DSMIB]\
    \ and for other configuration interfaces (e.g., other policy-\n   management protocols)\
    \ and, possibly, more detailed formal models\n   (e.g., [QOSDEVMOD]): these should\
    \ all be consistent with this model.\n   o  Section 3 starts by describing the\
    \ basic high-level blocks of a\n      Diffserv router.  It explains the concepts\
    \ used in the model,\n      including the hierarchical management model for these\
    \ blocks which\n      uses low-level functional datapath elements such as Classifiers,\n\
    \      Actions, Queues.\n   o  Section 4 describes Classifier elements.\n   o\
    \  Section 5 discusses Meter elements.\n   o  Section 6 discusses Action elements.\n\
    \   o  Section 7 discusses the basic queuing elements of Algorithmic\n      Droppers,\
    \ Queues, and Schedulers and their functional behaviors\n      (e.g., traffic\
    \ shaping).\n   o  Section 8 shows how the low-level elements can be combined\
    \ to\n      build modules called Traffic Conditioning Blocks (TCBs) which are\n\
    \      useful for management purposes.\n   o  Section 9 discusses security concerns.\n\
    \   o  Appendix A contains a brief discussion of the token bucket and\n      leaky\
    \ bucket algorithms used in this model and some of the\n      practical effects\
    \ of the use of token buckets within the Diffserv\n      architecture.\n"
- title: 2.  Glossary
  contents:
  - "2.  Glossary\n   This document uses terminology which is defined in [DSARCH].\
    \  There\n   is also current work-in-progress on this terminology in the IETF\
    \ and\n   some of the definitions provided here are taken from that work.  Some\n\
    \   of the terms from these other references are defined again here in\n   order\
    \ to provide additional detail, along with some new terms\n   specific to this\
    \ document.\n   Absolute      A functional datapath element which simply discards\
    \ all\n   Dropper       packets arriving at its input.\n   Algorithmic   A functional\
    \ datapath element which selectively\n   Dropper       discards packets that arrive\
    \ at its input, based on a\n                 discarding algorithm.  It has one\
    \ data input and one\n                 output.\n   Classifier    A functional\
    \ datapath element which consists of filters\n                 that select matching\
    \ and non-matching packets.  Based\n                 on this selection, packets\
    \ are forwarded along the\n                 appropriate datapath within the router.\
    \  A classifier,\n                 therefore, splits a single incoming traffic\
    \ stream into\n                 multiple outgoing streams.\n   Counter       A\
    \ functional datapath element which updates a packet\n                 counter\
    \ and also an octet counter for every\n                 packet that passes through\
    \ it.\n   Datapath      A conceptual path taken by packets with particular\n \
    \                characteristics through a Diffserv router.  Decisions\n     \
    \            as to the path taken by a packet are made by functional\n       \
    \          datapath elements such as Classifiers and Meters.\n   Filter      \
    \  A set of wildcard, prefix, masked, range and/or exact\n                 match\
    \ conditions on the content of a packet's\n                 headers or other data,\
    \ and/or on implicit or derived\n                 attributes associated with the\
    \ packet.  A filter is\n                 said to match only if each condition\
    \ is satisfied.\n   Functional    A basic building block of the conceptual router.\n\
    \   Datapath      Typical elements are Classifiers, Meters, Actions,\n   Element\
    \       Algorithmic Droppers, Queues and Schedulers.\n   Multiplexer   A multiplexor.\n\
    \   (Mux)\n   Multiplexor   A functional datapath element that merges multiple\n\
    \   (Mux)         traffic streams (datapaths) into a single traffic\n        \
    \         stream (datapath).\n   Non-work-     A property of a scheduling algorithm\
    \ such that it\n   conserving    services packets no sooner than a scheduled departure\n\
    \                 time, even if this means leaving packets queued\n          \
    \       while the output (e.g., a network link or connection\n               \
    \  to the next element) is idle.\n   Policing      The process of comparing the\
    \ arrival of data packets\n                 against a temporal profile and forwarding,\
    \ delaying\n                 or dropping them so as to make the output stream\n\
    \                 conformant to the profile.\n   Queuing       A combination of\
    \ functional datapath elements\n   Block         that modulates the transmission\
    \ of packets belonging\n                 to a traffic streams and determines their\n\
    \                 ordering, possibly storing them temporarily or\n           \
    \      discarding them.\n   Scheduling    An algorithm which determines which\
    \ queue of a set\n   algorithm     of queues to service next.  This may be based\
    \ on the\n                 relative priority of the queues, on a weighted fair\n\
    \                 bandwidth sharing policy or some other policy. Such\n      \
    \           an algorithm may be either work-conserving or non-\n             \
    \    work-conserving.\n   Service-Level A set of parameters and their values which\
    \ together\n   Specification define the treatment offered to a traffic stream\
    \ by a\n   (SLS)         Diffserv domain.\n   Shaping       The process of delaying\
    \ packets within a traffic stream\n                 to cause it to conform to\
    \ some defined temporal\n                 profile.  Shaping can be implemented\
    \ using a queue\n                 serviced by a non-work-conserving scheduling\
    \ algorithm.\n   Traffic       A logical datapath entity consisting of a number\
    \ of\n   Conditioning  functional datapath elements interconnected in\n   Block\
    \ (TCB)   such a way as to perform a specific set of traffic\n               \
    \  conditioning functions on an incoming traffic stream.\n                 A TCB\
    \ can be thought of as an entity with one\n                 input and one or more\
    \ outputs and a set of control\n                 parameters.\n   Traffic     \
    \  A set of parameters and their values which together\n   Conditioning  specify\
    \ a set of classifier rules and a traffic\n   Specification profile.  A TCS is\
    \ an integral element of a SLS.\n   (TCS)\n   Work-         A property of a scheduling\
    \ algorithm such that it\n   conserving    services a packet, if one is available,\
    \ at every\n                 transmission opportunity.\n"
- title: 3.  Conceptual Model
  contents:
  - "3.  Conceptual Model\n   This section introduces a block diagram of a Diffserv\
    \ router and\n   describes the various components illustrated in Figure 1.  Note\
    \ that\n   a Diffserv core router is likely to require only a subset of these\n\
    \   components: the model presented here is intended to cover the case of\n  \
    \ both Diffserv edge and core routers.\n"
- title: 3.1.  Components of a Diffserv Router
  contents:
  - "3.1.  Components of a Diffserv Router\n   The conceptual model includes abstract\
    \ definitions for the following:\n      o  Traffic Classification elements.\n\
    \      o  Metering functions.\n      o  Actions of Marking, Absolute Dropping,\
    \ Counting, and\n         Multiplexing.\n      o  Queuing elements, including\
    \ capabilities of algorithmic\n         dropping and scheduling.\n      o  Certain\
    \ combinations of the above functional datapath elements\n         into higher-level\
    \ blocks known as Traffic Conditioning Blocks\n         (TCBs).\n   The components\
    \ and combinations of components described in this\n   document form building\
    \ blocks that need to be manageable by Diffserv\n   configuration and management\
    \ tools.  One of the goals of this\n   document is to show how a model of a Diffserv\
    \ device can be built\n   using these component blocks.  This model is in the\
    \ form of a\n   connected directed acyclic graph (DAG) of functional datapath\n\
    \   elements that describes the traffic conditioning and queuing\n   behaviors\
    \ that any particular packet will experience when forwarded\n   to the Diffserv\
    \ router.  Figure 1 illustrates the major functional\n   blocks of a Diffserv\
    \ router.\n"
- title: 3.1.1.  Datapath
  contents:
  - "3.1.1.  Datapath\n   An ingress interface, routing core, and egress interface\
    \ are\n   illustrated at the center of the diagram.  In actual router\n   implementations,\
    \ there may be an arbitrary number of ingress and\n   egress interfaces interconnected\
    \ by the routing core.  The routing\n   core element serves as an abstraction\
    \ of a router's normal routing\n   and switching functionality.  The routing core\
    \ moves packets between\n   interfaces according to policies outside the scope\
    \ of Diffserv (note:\n   it is possible that such policies for output-interface\
    \ selection\n   might involve use of packet fields such as the DSCP but this is\n\
    \   outside the scope of this model).  The actual queuing delay and\n   packet\
    \ loss behavior of a specific router's switching\n   fabric/backplane is not modeled\
    \ by the routing core; these should be\n   modeled using the functional datapath\
    \ elements described later.  The\n   routing core of this model can be thought\
    \ of as an infinite\n   bandwidth, zero-delay interconnect between interfaces\
    \ - properties\n   like the behavior of the core when overloaded need to be reflected\n\
    \   back into the queuing elements that are modeled around it (e.g., when\n  \
    \ too much traffic is directed across the core at an egress interface),\n   the\
    \ excess must either be dropped or queued somewhere: the elements\n   performing\
    \ these functions must be modeled on one of the interfaces\n   involved.\n   The\
    \ components of interest at the ingress to and egress from\n   interfaces are\
    \ the functional datapath elements (e.g., Classifiers,\n   Queuing elements) that\
    \ support Diffserv traffic conditioning and\n   per-hop behaviors [DSARCH].  These\
    \ are the fundamental components\n   comprising a Diffserv router and are the\
    \ focal point of this model.\n               +---------------+\n             \
    \  | Diffserv      |\n        Mgmt   | configuration |\n      <----+-->| & management\
    \  |------------------+\n      SNMP,|   | interface     |                  |\n\
    \      COPS |   +---------------+                  |\n      etc. |        |  \
    \                           |\n           |        |                         \
    \    |\n           |        v                             v\n           |   +-------------+\
    \                 +-------------+\n           |   | ingress i/f |   +---------+\
    \   | egress i/f  |\n      -------->|  classify,  |-->| routing |-->|  classify,\
    \  |---->\n      data |   |  meter,     |   |  core   |   |  meter      |data\
    \ out\n      in   |   |  action,    |   +---------+   |  action,    |\n      \
    \     |   |  queuing    |                 |  queuing    |\n           |   +-------------+\
    \                 +-------------+\n           |        ^                     \
    \        ^\n           |        |                             |\n           |\
    \        |                             |\n           |   +------------+      \
    \               |\n           +-->| QOS agent  |                     |\n     \
    \ -------->| (optional) |---------------------+\n        QOS    |(e.g., RSVP)|\n\
    \        cntl   +------------+\n        msgs\n           Figure 1:  Diffserv Router\
    \ Major Functional Blocks\n"
- title: 3.1.2.  Configuration and Management Interface
  contents:
  - "3.1.2.  Configuration and Management Interface\n   Diffserv operating parameters\
    \ are monitored and provisioned through\n   this interface.  Monitored parameters\
    \ include statistics regarding\n   traffic carried at various Diffserv service\
    \ levels.  These statistics\n   may be important for accounting purposes and/or\
    \ for tracking\n   compliance to Traffic Conditioning Specifications (TCSs) negotiated\n\
    \   with customers.  Provisioned parameters are primarily the TCS\n   parameters\
    \ for Classifiers and Meters and the associated PHB\n   configuration parameters\
    \ for Actions and Queuing elements.  The\n   network administrator interacts with\
    \ the Diffserv configuration and\n   management interface via one or more management\
    \ protocols, such as\n   SNMP or COPS, or through other router configuration tools\
    \ such as\n   serial terminal or telnet consoles.\n   Specific policy rules and\
    \ goals governing the Diffserv behavior of a\n   router are presumed to be installed\
    \ by policy management mechanisms.\n   However, Diffserv routers are always subject\
    \ to implementation limits\n   which scope the kinds of policies which can be\
    \ successfully\n   implemented by the router.  External reporting of such implementation\n\
    \   capabilities is considered out of scope for this document.\n"
- title: 3.1.3.  Optional QoS Agent Module
  contents:
  - "3.1.3.  Optional QoS Agent Module\n   Diffserv routers may snoop or participate\
    \ in either per-microflow or\n   per-flow-aggregate signaling of QoS requirements\
    \ [E2E] (e.g., using\n   the RSVP protocol).  Snooping of RSVP messages may be\
    \ used, for\n   example, to learn how to classify traffic without actually\n \
    \  participating as a RSVP protocol peer.  Diffserv routers may reject\n   or\
    \ admit RSVP reservation requests to provide a means of admission\n   control\
    \ to Diffserv-based services or they may use these requests to\n   trigger provisioning\
    \ changes for a flow-aggregation in the Diffserv\n   network.  A flow-aggregation\
    \ in this context might be equivalent to a\n   Diffserv BA or it may be more fine-grained,\
    \ relying on a multi-field\n   (MF) classifier [DSARCH].  Note that the conceptual\
    \ model of such a\n   router implements the Integrated Services Model as described\
    \ in\n   [INTSERV], applying the control plane controls to the data classified\n\
    \   and conditioned in the data plane, as described in [E2E].\n   Note that a\
    \ QoS Agent component of a Diffserv router, if present,\n   might be active only\
    \ in the control plane and not in the data plane.\n   In this scenario, RSVP could\
    \ be used merely to signal reservation\n   state without installing any actual\
    \ reservations in the data plane of\n   the Diffserv router: the data plane could\
    \ still act purely on\n   Diffserv DSCPs and provide PHBs for handling data traffic\
    \ without the\n   normal per-microflow handling expected to support some Intserv\n\
    \   services.\n"
- title: 3.2.  Diffserv Functions at Ingress and Egress
  contents:
  - "3.2.  Diffserv Functions at Ingress and Egress\n   This document focuses on the\
    \ Diffserv-specific components of the\n   router.  Figure 2 shows a high-level\
    \ view of ingress and egress\n   interfaces of a router.  The diagram illustrates\
    \ two Diffserv router\n   interfaces, each having a set of ingress and a set of\
    \ egress\n   elements.  It shows classification, metering, action and queuing\n\
    \   functions which might be instantiated at each interface's ingress and\n  \
    \ egress.\n   The simple diagram of Figure 2 assumes that the set of Diffserv\n\
    \   functions to be carried out on traffic on a given interface are\n   independent\
    \ of those functions on all other interfaces.  There are\n   some architectures\
    \ where Diffserv functions may be shared amongst\n   multiple interfaces (e.g.,\
    \ processor and buffering resources that\n   handle multiple interfaces on the\
    \ same line card before forwarding\n   across a routing core).  The model presented\
    \ in this document may be\n   easily extended to handle such cases; however, this\
    \ topic is not\n   treated further here as it leads to excessive complexity in\
    \ the\n   explanation of the concepts.\n            Interface A              \
    \          Interface B\n          +-------------+     +---------+     +-------------+\n\
    \          | ingress:    |     |         |     | egress:     |\n          |  \
    \ classify, |     |         |     |   classify, |\n      --->|   meter,    |---->|\
    \         |---->|   meter,    |--->\n          |   action,   |     |         |\
    \     |   action,   |\n          |   queuing   |     | routing |     |   queuing\
    \   |\n          +-------------+     |  core   |     +-------------+\n       \
    \   | egress:     |     |         |     | ingress:    |\n          |   classify,\
    \ |     |         |     |   classify, |\n      <---|   meter,    |<----|     \
    \    |<----|   meter,    |<---\n          |   action,   |     |         |    \
    \ |   action,   |\n          |   queuing   |     +---------+     |   queuing \
    \  |\n          +-------------+                     +-------------+\n        \
    \  Figure 2. Traffic Conditioning and Queuing Elements\n   In principle, if one\
    \ were to construct a network entirely out of\n   two-port routers (connected\
    \ by LANs or similar media), then it might\n   be necessary for each router to\
    \ perform four QoS control functions in\n   the datapath on traffic in each direction:\n\
    \   -  Classify each message according to some set of rules, possibly\n      just\
    \ a \"match everything\" rule.\n   -  If necessary, determine whether the data\
    \ stream the message is\n      part of is within or outside its rate by metering\
    \ the stream.\n   -  Perform a set of resulting actions, including applying a\
    \ drop\n      policy appropriate to the classification and queue in question and\n\
    \      perhaps additionally marking the traffic with a Differentiated\n      Services\
    \ Code Point (DSCP) [DSFIELD].\n   -  Enqueue the traffic for output in the appropriate\
    \ queue.  The\n      scheduling of output from this queue may lead to shaping\
    \ of the\n      traffic or may simply cause it to be forwarded with some minimum\n\
    \      rate or maximum latency assurance.\n   If the network is now built out\
    \ of N-port routers, the expected\n   behavior of the network should be identical.\
    \  Therefore, this model\n   must provide for essentially the same set of functions\
    \ at the ingress\n   as on the egress of a router's interfaces.  The one point\
    \ of\n   difference in the model between ingress and the egress is that all\n\
    \   traffic at the egress of an interface is queued, while traffic at the\n  \
    \ ingress to an interface is likely to be queued only for shaping\n   purposes,\
    \ if at all.  Therefore, equivalent functional datapath\n   elements may be modeled\
    \ at both the ingress to and egress from an\n   interface.\n   Note that it is\
    \ not mandatory that each of these functional datapath\n   elements be implemented\
    \ at both ingress and egress; equally, the\n   model allows that multiple sets\
    \ of these elements may be placed in\n   series and/or in parallel at ingress\
    \ or at egress.  The arrangement\n   of elements is dependent on the service requirements\
    \ on a particular\n   interface on a particular router.  By modeling these elements\
    \ at both\n   ingress and egress, it is not implied that they must be implemented\n\
    \   in this way in a specific router.  For example, a router may\n   implement\
    \ all shaping and PHB queuing at the interface egress or may\n   instead implement\
    \ it only at the ingress.  Furthermore, the\n   classification needed to map a\
    \ packet to an egress queue (if present)\n   need not be implemented at the egress\
    \ but instead might be\n   implemented at the ingress, with the packet passed\
    \ through the\n   routing core with in-band control information to allow for egress\n\
    \   queue selection.\n   Specifically, some interfaces will be at the outer \"\
    edge\" and some\n   will be towards the \"core\" of the Diffserv domain.  It is\
    \ to be\n   expected (from the general principles guiding the motivation of\n\
    \   Diffserv) that \"edge\" interfaces, or at least the routers that\n   contain\
    \ them, will implement more complexity and require more\n   configuration than\
    \ those in the core although this is obviously not a\n   requirement.\n"
- title: 3.3.  Shaping and Policing
  contents:
  - "3.3.  Shaping and Policing\n   Diffserv nodes may apply shaping, policing and/or\
    \ marking to traffic\n   streams that exceed the bounds of their TCS in order\
    \ to prevent one\n   traffic stream from seizing more than its share of resources\
    \ from a\n   Diffserv network.  In this model, Shaping, sometimes considered as\
    \ a\n   TC action, is treated as a function of queuing elements - see section\n\
    \   7.  Algorithmic Dropping techniques (e.g., RED) are similarly treated\n  \
    \ since they are often closely associated with queues.  Policing is\n   modeled\
    \ as either a concatenation of a Meter with an Absolute Dropper\n   or as a concatenation\
    \ of an Algorithmic Dropper with a Scheduler.\n   These elements will discard\
    \ packets which exceed the TCS.\n"
- title: 3.4.  Hierarchical View of the Model
  contents:
  - "3.4.  Hierarchical View of the Model\n   From a device-level configuration management\
    \ perspective, the\n   following hierarchy exists:\n      At the lowest level\
    \ considered here, there are individual\n      functional datapath elements, each\
    \ with their own configuration\n      parameters and management counters and flags.\n\
    \      At the next level, the network administrator manages groupings of\n   \
    \   these functional datapath elements interconnected in a DAG.  These\n     \
    \ functional datapath elements are organized in self-contained TCBs\n      which\
    \ are used to implement some desired network policy (see\n      Section 8).  One\
    \ or more TCBs may be instantiated at each\n      interface's ingress or egress;\
    \ they may be connected in series\n      and/or in parallel configurations on\
    \ the multiple outputs of a\n      preceding TCB.  A TCB can be thought of as\
    \ a \"black box\" with one\n      input and one or more outputs (in the data path).\
    \  Each interface\n      may have a different TCB configuration and each direction\
    \ (ingress\n      or egress) may too.\n      At the topmost level considered here,\
    \ the network administrator\n      manages interfaces.  Each interface has ingress\
    \ and egress\n      functionality, with each of these expressed as one or more\
    \ TCBs.\n      This level of the hierarchy is what was illustrated in Figure 2.\n\
    \   Further levels may be built on top of this hierarchy, in particular\n   ones\
    \ for aiding in the repetitive configuration tasks likely for\n   routers with\
    \ many interfaces: some such \"template\" tools for Diffserv\n   routers are outside\
    \ the scope of this model but are under study by\n   other working groups within\
    \ IETF.\n"
- title: 4.  Classifiers
  contents:
  - '4.  Classifiers

    '
- title: 4.1.  Definition
  contents:
  - "4.1.  Definition\n   Classification is performed by a classifier element.  Classifiers\
    \ are\n   1:N (fan-out) devices: they take a single traffic stream as input and\n\
    \   generate N logically separate traffic streams as output.  Classifiers\n  \
    \ are parameterized by filters and output streams.  Packets from the\n   input\
    \ stream are sorted into various output streams by filters which\n   match the\
    \ contents of the packet or possibly match other attributes\n   associated with\
    \ the packet.  Various types of classifiers using\n   different filters are described\
    \ in the following sections.  Figure 3\n   illustrates a classifier, where the\
    \ outputs connect to succeeding\n   functional datapath elements.\n   The simplest\
    \ possible Classifier element is one that matches all\n   packets that are applied\
    \ at its input.  In this case, the Classifier\n   element is just a no-op and\
    \ may be omitted.\n   Note that we allow a Multiplexor (see Section 6.5) before\
    \ the\n   Classifier to allow input from multiple traffic streams.  For\n   example,\
    \ if traffic streams originating from multiple ingress\n   interfaces feed through\
    \ a single Classifier then the interface number\n   could be one of the packet\
    \ classification keys used by the\n   Classifier.  This optimization may be important\
    \ for scalability in\n   the management plane.  Classifiers may also be cascaded\
    \ in sequence\n   to perform more complex lookup operations whilst still maintaining\n\
    \   such scalability.\n   Another example of a packet attribute could be an integer\n\
    \   representing the BGP community string associated with the packet's\n   best-matching\
    \ route.  Other contextual information may also be used\n   by a Classifier (e.g.,\
    \ knowledge that a particular interface faces a\n   Diffserv domain or a legacy\
    \ IP TOS domain [DSARCH] could be used when\n   determining whether a DSCP is\
    \ present or not).\n      unclassified              classified\n      traffic\
    \                   traffic\n              +------------+\n              |   \
    \         |--> match Filter1 --> OutputA\n      ------->| classifier |--> match\
    \ Filter2 --> OutputB\n              |            |--> no match      --> OutputC\n\
    \              +------------+\n      Figure 3. An Example Classifier\n   The following\
    \ BA classifier separates traffic into one of three\n   output streams based on\
    \ matching filters:\n      Filter Matched        Output Stream\n      --------------\
    \       ---------------\n      Filter1                    A\n      Filter2   \
    \                 B\n      no match                   C\n   Where the filters\
    \ are defined to be the following BA filters\n   ([DSARCH], Section 4.2.1):\n\
    \      Filter        DSCP\n      ------       ------\n      Filter1       101010\n\
    \      Filter2       111111\n      Filter3       ****** (wildcard)\n"
- title: 4.1.1.  Filters
  contents:
  - "4.1.1.  Filters\n   A filter consists of a set of conditions on the component\
    \ values of a\n   packet's classification key (the header values, contents, and\n\
    \   attributes relevant for classification).  In the BA classifier\n   example\
    \ above, the classification key consists of one packet header\n   field, the DSCP,\
    \ and both Filter1 and Filter2 specify exact-match\n   conditions on the value\
    \ of the DSCP.  Filter3 is a wildcard default\n   filter which matches every packet,\
    \ but which is only selected in the\n   event that no other more specific filter\
    \ matches.\n   In general there are a set of possible component conditions including\n\
    \   exact, prefix, range, masked and wildcard matches.  Note that ranges\n   can\
    \ be represented (with less efficiency) as a set of prefixes and\n   that prefix\
    \ matches are just a special case of both masked and range\n   matches.\n   In\
    \ the case of a MF classifier, the classification key consists of a\n   number\
    \ of packet header fields.  The filter may specify a different\n   condition for\
    \ each key component, as illustrated in the example below\n   for a IPv4/TCP classifier:\n\
    \      Filter   IPv4 Src Addr  IPv4 Dest Addr  TCP SrcPort  TCP DestPort\n   \
    \   ------   -------------  --------------  -----------  ------------\n      Filter4\
    \  172.31.8.1/32  172.31.3.X/24       X          5003\n   In this example, the\
    \ fourth octet of the destination IPv4 address and\n   the source TCP port are\
    \ wildcard or \"don't care\".\n   MF classification of IP-fragmented packets is\
    \ impossible if the\n   filter uses transport-layer port numbers (e.g., TCP port\
    \ numbers).\n   MTU-discovery is therefore a prerequisite for proper operation\
    \ of a\n   Diffserv network that uses such classifiers.\n"
- title: 4.1.2.  Overlapping Filters
  contents:
  - "4.1.2.  Overlapping Filters\n   Note that it is easy to define sets of overlapping\
    \ filters in a\n   classifier.  For example:\n      Filter   IPv4 Src Addr  IPv4\
    \ Dest Addr\n      ------   -------------  --------------\n      Filter5  172.31.8.X/24\
    \      X/0\n      Filter6      X/0        172.30.10.1/32\n   A packet containing\
    \ {IP Dest Addr 172.31.8.1, IP Src Addr\n   172.30.10.1} cannot be uniquely classified\
    \ by this pair of filters\n   and so a precedence must be established between\
    \ Filter5 and Filter6\n   in order to break the tie.  This precedence must be\
    \ established\n   either (a) by a manager which knows that the router can accomplish\n\
    \   this particular ordering (e.g., by means of reported capabilities),\n   or\
    \ (b) by the router along with a mechanism to report to a manager\n   which precedence\
    \ is being used.  Such precedence mechanisms must be\n   supported in any translation\
    \ of this model into specific syntax for\n   configuration and management protocols.\n\
    \   As another example, one might want first to disallow certain\n   applications\
    \ from using the network at all, or to classify some\n   individual traffic streams\
    \ that are not Diffserv-marked.  Traffic\n   that is not classified by those tests\
    \ might then be inspected for a\n   DSCP.  The word \"then\" implies sequence\
    \ and this must be specified by\n   means of precedence.\n   An unambiguous classifier\
    \ requires that every possible classification\n   key match at least one filter\
    \ (possibly the wildcard default) and\n   that any ambiguity between overlapping\
    \ filters be resolved by\n   precedence.  Therefore, the classifiers on any given\
    \ interface must\n   be \"complete\" and will often include an \"everything else\"\
    \ filter as\n   the lowest precedence element in order for the result of\n   classification\
    \ to be deterministic.  Note that this completeness is\n   only required of the\
    \ first classifier that incoming traffic will meet\n   as it enters an interface\
    \ - subsequent classifiers on an interface\n   only need to handle the traffic\
    \ that it is known that they will\n   receive.\n   This model of classifier operation\
    \ makes the assumption that all\n   filters of the same precedence be applied\
    \ simultaneously.  Whilst\n   convenient from a modeling point-of-view, this may\
    \ or may not be how\n   the classifier is actually implemented - this assumption\
    \ is not\n   intended to dictate how the implementation actually handles this,\n\
    \   merely to clearly define the required end result.\n"
- title: 4.2.  Examples
  contents:
  - '4.2.  Examples

    '
- title: 4.2.1.  Behavior Aggregate (BA) Classifier
  contents:
  - "4.2.1.  Behavior Aggregate (BA) Classifier\n   The simplest Diffserv classifier\
    \ is a behavior aggregate (BA)\n   classifier [DSARCH].  A BA classifier uses\
    \ only the Diffserv\n   codepoint (DSCP) in a packet's IP header to determine\
    \ the logical\n   output stream to which the packet should be directed.  We allow\
    \ only\n   an exact-match condition on this field because the assigned DSCP\n\
    \   values have no structure, and therefore no subset of DSCP bits are\n   significant.\n\
    \   The following defines a possible BA filter:\n      Filter8:\n      Type: \
    \  BA\n      Value:  111000\n"
- title: 4.2.2.  Multi-Field (MF) Classifier
  contents:
  - "4.2.2.  Multi-Field (MF) Classifier\n   Another type of classifier is a multi-field\
    \ (MF) classifier [DSARCH].\n   This classifies packets based on one or more fields\
    \ in the packet\n   (possibly including the DSCP).  A common type of MF classifier\
    \ is a\n   6-tuple classifier that classifies based on six fields from the IP\n\
    \   and TCP or UDP headers (destination address, source address, IP\n   protocol,\
    \ source port, destination port, and DSCP).  MF classifiers\n   may classify on\
    \ other fields such as MAC addresses, VLAN tags, link-\n   layer traffic class\
    \ fields, or other higher-layer protocol fields.\n   The following defines a possible\
    \ MF filter:\n      Filter9:\n      Type:              IPv4-6-tuple\n      IPv4DestAddrValue:\
    \ 0.0.0.0\n      IPv4DestAddrMask:  0.0.0.0\n      IPv4SrcAddrValue:  172.31.8.0\n\
    \      IPv4SrcAddrMask:   255.255.255.0\n      IPv4DSCP:          28\n      IPv4Protocol:\
    \      6\n      IPv4DestL4PortMin: 0\n      IPv4DestL4PortMax: 65535\n      IPv4SrcL4PortMin:\
    \  20\n      IPv4SrcL4PortMax:  20\n   A similar type of classifier can be defined\
    \ for IPv6.\n"
- title: 4.2.3.  Free-form Classifier
  contents:
  - "4.2.3.  Free-form Classifier\n   A Free-form classifier is made up of a set of\
    \ user definable\n   arbitrary filters each made up of {bit-field size, offset\
    \ (from head\n   of packet), mask}:\n      Classifier2:\n      Filter12:    OutputA\n\
    \      Filter13:    OutputB\n      Default:     OutputC\n      Filter12:\n   \
    \   Type:        FreeForm\n      SizeBits:    3 (bits)\n      Offset:      16\
    \ (bytes)\n      Value:       100 (binary)\n      Mask:        101 (binary)\n\
    \      Filter13:\n      Type:        FreeForm\n      SizeBits:    12 (bits)\n\
    \      Offset:      16 (bytes)\n      Value:       100100000000 (binary)\n   \
    \   Mask:        111111111111 (binary)\n   Free-form filters can be combined into\
    \ filter groups to form very\n   powerful filters.\n"
- title: 4.2.4.  Other Possible Classifiers
  contents:
  - "4.2.4.  Other Possible Classifiers\n   Classification may also be performed based\
    \ on information at the\n   datalink layer below IP (e.g., VLAN or datalink-layer\
    \ priority) or\n   perhaps on the ingress or egress IP, logical or physical interface\n\
    \   identifier (e.g., the incoming channel number on a channelized\n   interface).\
    \  A classifier that filters based on IEEE 802.1p Priority\n   and on 802.1Q VLAN-ID\
    \ might be represented as:\n      Classifier3:\n      Filter14 AND Filter15: \
    \ OutputA\n      Default:                OutputB\n      Filter14:            \
    \            -- priority 4 or 5\n      Type:        Ieee8021pPriority\n      Value:\
    \       100 (binary)\n      Mask:        110 (binary)\n      Filter15:       \
    \                 -- VLAN 2304\n      Type:        Ieee8021QVlan\n      Value:\
    \       100100000000 (binary)\n      Mask:        111111111111 (binary)\n   Such\
    \ classifiers may be the subject of other standards or may be\n   proprietary\
    \ to a router vendor but they are not discussed further\n   here.\n"
- title: 5.  Meters
  contents:
  - "5.  Meters\n   Metering is defined in [DSARCH].  Diffserv network providers may\n\
    \   choose to offer services to customers based on a temporal (i.e.,\n   rate)\
    \ profile within which the customer submits traffic for the\n   service.  In this\
    \ event, a meter might be used to trigger real-time\n   traffic conditioning actions\
    \ (e.g., marking) by routing a non-\n   conforming packet through an appropriate\
    \ next-stage action element.\n   Alternatively, by counting conforming and/or\
    \ non-conforming traffic\n   using a Counter element downstream of the Meter,\
    \ it might also be\n   used to help in collecting data for out-of-band management\
    \ functions\n   such as billing applications.\n   Meters are logically 1:N (fan-out)\
    \ devices (although a multiplexor\n   can be used in front of a meter).  Meters\
    \ are parameterized by a\n   temporal profile and by conformance levels, each\
    \ of which is\n   associated with a meter's output.  Each output can be connected\
    \ to\n   another functional element.\n   Note that this model of a meter differs\
    \ slightly from that described\n   in [DSARCH].  In that description the meter\
    \ is not a datapath element\n   but is instead used to monitor the traffic stream\
    \ and send control\n   signals to action elements to dynamically modulate their\
    \ behavior\n   based on the conformance of the packet.  This difference in the\n\
    \   description does not change the function of a meter.  Figure 4\n   illustrates\
    \ a meter with 3 levels of conformance.\n   In some Diffserv examples (e.g., [AF-PHB]),\
    \ three levels of\n   conformance are discussed in terms of colors, with green\
    \ representing\n   conforming, yellow representing partially conforming and red\n\
    \   representing non-conforming.  These different conformance levels may\n   be\
    \ used to trigger different queuing, marking or dropping treatment\n   later on\
    \ in the processing.  Other example meters use a binary notion\n   of conformance;\
    \ in the general case N levels of conformance can be\n   supported.  In general\
    \ there is no constraint on the type of\n   functional datapath element following\
    \ a meter output, but care must\n   be taken not to inadvertently configure a\
    \ datapath that results in\n   packet reordering that is not consistent with the\
    \ requirements of the\n   relevant PHB specification.\n      unmetered       \
    \       metered\n      traffic                traffic\n                +---------+\n\
    \                |         |--------> conformance A\n      --------->|  meter\
    \  |--------> conformance B\n                |         |--------> conformance\
    \ C\n                +---------+\n      Figure 4. A Generic Meter\n   A meter,\
    \ according to this model, measures the rate at which packets\n   making up a\
    \ stream of traffic pass it, compares the rate to some set\n   of thresholds,\
    \ and produces some number of potential results (two or\n   more):  a given packet\
    \ is said to be \"conformant\" to a level of the\n   meter if, at the time that\
    \ the packet is being examined, the stream\n   appears to be within the rate limit\
    \ for the profile associated with\n   that level.  A fuller discussion of conformance\
    \ to meter profiles\n   (and the associated requirements that this places on the\
    \ schedulers\n   upstream) is provided in Appendix A.\n"
- title: 5.1.  Examples
  contents:
  - "5.1.  Examples\n   The following are some examples of possible meters.\n"
- title: 5.1.1.  Average Rate Meter
  contents:
  - "5.1.1.  Average Rate Meter\n   An example of a very simple meter is an average\
    \ rate meter.  This\n   type of meter measures the average rate at which packets\
    \ are\n   submitted to it over a specified averaging time.\n   An average rate\
    \ profile may take the following form:\n      Meter1:\n      Type:           \
    \     AverageRate\n      Profile:             Profile1\n      ConformingOutput:\
    \    Queue1\n      NonConformingOutput: Counter1\n      Profile1:\n      Type:\
    \                AverageRate\n      AverageRate:         120 kbps\n      Delta:\
    \               100 msec\n   A Meter measuring against this profile would continually\
    \ maintain a\n   count that indicates the total number and/or cumulative byte-count\
    \ of\n   packets arriving between time T (now) and time T - 100 msecs.  So\n \
    \  long as an arriving packet does not push the count over 12 kbits in\n   the\
    \ last 100 msec, the packet would be deemed conforming.  Any packet\n   that pushes\
    \ the count over 12 kbits would be deemed non-conforming.\n   Thus, this Meter\
    \ deems packets to correspond to one of two\n   conformance levels: conforming\
    \ or non-conforming, and sends them on\n   for the appropriate subsequent treatment.\n"
- title: 5.1.2.  Exponential Weighted Moving Average (EWMA) Meter
  contents:
  - "5.1.2.  Exponential Weighted Moving Average (EWMA) Meter\n   The EWMA form of\
    \ Meter is easy to implement in hardware and can be\n   parameterized as follows:\n\
    \      avg_rate(t) = (1 - Gain) * avg_rate(t') +  Gain * rate(t)\n      t = t'\
    \ + Delta\n   For a packet arriving at time t:\n      if (avg_rate(t) > AverageRate)\n\
    \         non-conforming\n      else\n         conforming\n   \"Gain\" controls\
    \ the time constant (e.g., frequency response) of what\n   is essentially a simple\
    \ IIR low-pass filter.  \"Rate(t)\" measures the\n   number of incoming bytes\
    \ in a small fixed sampling interval, Delta.\n   Any packet that arrives and pushes\
    \ the average rate over a predefined\n   rate AverageRate is deemed non-conforming.\
    \  An EWMA Meter profile\n   might look something like the following:\n      Meter2:\n\
    \      Type:                ExpWeightedMovingAvg\n      Profile:             Profile2\n\
    \      ConformingOutput:    Queue1\n      NonConformingOutput: AbsoluteDropper1\n\
    \      Profile2:\n      Type:                ExpWeightedMovingAvg\n      AverageRate:\
    \         25 kbps\n      Delta:               10 usec\n      Gain:           \
    \     1/16\n"
- title: 5.1.3.  Two-Parameter Token Bucket Meter
  contents:
  - "5.1.3.  Two-Parameter Token Bucket Meter\n   A more sophisticated Meter might\
    \ measure conformance to a token\n   bucket (TB) profile.  A TB profile generally\
    \ has two parameters, an\n   average token rate, R, and a burst size, B.  TB Meters\
    \ compare the\n   arrival rate of packets to the average rate specified by the\
    \ TB\n   profile.  Logically, tokens accumulate in a bucket at the average\n \
    \  rate, R, up to a maximum credit which is the burst size, B.  When a\n   packet\
    \ of length L arrives, a conformance test is applied.  There are\n   at least\
    \ two such tests in widespread use:\n   Strict conformance\n      Packets of length\
    \ L bytes are considered conforming only if there\n      are sufficient tokens\
    \ available in the bucket at the time of\n      packet arrival for the complete\
    \ packet (i.e., the current depth is\n      greater than or equal to L): no tokens\
    \ may be borrowed from future\n      token allocations.  For examples of this\
    \ approach, see [SRTCM] and\n      [TRTCM].\n   Loose conformance\n      Packets\
    \ of length L bytes are considered conforming if any tokens\n      are available\
    \ in the bucket at the time of packet arrival: up to L\n      bytes may then be\
    \ borrowed from future token allocations.\n   Packets are allowed to exceed the\
    \ average rate in bursts up to the\n   burst size.  For further discussion of\
    \ loose and strict conformance\n   to token bucket profiles, as well as system\
    \ and implementation\n   issues, see Appendix A.\n   A two-parameter TB meter\
    \ has exactly two possible conformance levels\n   (conforming, non-conforming).\
    \  Such a meter might appear as follows:\n      Meter3:\n      Type:         \
    \       SimpleTokenBucket\n      Profile:             Profile3\n      ConformanceType:\
    \     loose\n      ConformingOutput:    Queue1\n      NonConformingOutput: AbsoluteDropper1\n\
    \      Profile3:\n      Type:                SimpleTokenBucket\n      AverageRate:\
    \         200 kbps\n      BurstSize:           100 kbytes\n"
- title: 5.1.4.  Multi-Stage Token Bucket Meter
  contents:
  - "5.1.4.  Multi-Stage Token Bucket Meter\n   More complicated TB meters might define\
    \ multiple burst sizes and more\n   conformance levels.  Packets found to exceed\
    \ the larger burst size\n   are deemed non-conforming.  Packets found to exceed\
    \ the smaller burst\n   size are deemed partially-conforming.  Packets exceeding\
    \ neither are\n   deemed conforming.  Some token bucket meters designed for Diffserv\n\
    \   networks are described in more detail in [SRTCM, TRTCM]; in some of\n   these\
    \ references, three levels of conformance are discussed in terms\n   of colors\
    \ with green representing conforming, yellow representing\n   partially conforming,\
    \ and red representing non-conforming.  Note that\n   these multiple-conformance-level\
    \ meters can sometimes be implemented\n   using an appropriate sequence of multiple\
    \ two-parameter TB meters.\n   A profile for a multi-stage TB meter with three\
    \ levels of conformance\n   might look as follows:\n      Meter4:\n      Type:\
    \                TwoRateTokenBucket\n      ProfileA:            Profile4\n   \
    \   ConformanceTypeA:    strict\n      ConformingOutputA:   Queue1\n      ProfileB:\
    \            Profile5\n      ConformanceTypeB:    strict\n      ConformingOutputB:\
    \   Marker1\n      NonConformingOutput: AbsoluteDropper1\n      Profile4:\n  \
    \    Type:                SimpleTokenBucket\n      AverageRate:         100 kbps\n\
    \      BurstSize:           20 kbytes\n      Profile5:\n      Type:          \
    \      SimpleTokenBucket\n      AverageRate:         100 kbps\n      BurstSize:\
    \           100 kbytes\n"
- title: 5.1.5.  Null Meter
  contents:
  - "5.1.5.  Null Meter\n   A null meter has only one output: always conforming, and\
    \ no\n   associated temporal profile.  Such a meter is useful to define in the\n\
    \   event that the configuration or management interface does not have\n   the\
    \ flexibility to omit a meter in a datapath segment.\n      Meter5:\n      Type:\
    \                NullMeter\n      Output:              Queue1\n"
- title: 6.  Action Elements
  contents:
  - "6.  Action Elements\n   The classifiers and meters described up to this point\
    \ are fan-out\n   elements which are generally used to determine the appropriate\
    \ action\n   to apply to a packet.  The set of possible actions that can then\
    \ be\n   applied include:\n   -    Marking\n   -    Absolute Dropping\n   -  \
    \  Multiplexing\n   -    Counting\n   -    Null action - do nothing\n   The corresponding\
    \ action elements are described in the following\n   sections.\n"
- title: 6.1.  DSCP Marker
  contents:
  - "6.1.  DSCP Marker\n   DSCP Markers are 1:1 elements which set a codepoint (e.g.,\
    \ the DSCP\n   in an IP header).  DSCP Markers may also act on unmarked packets\n\
    \   (e.g., those submitted with DSCP of zero) or may re-mark previously\n   marked\
    \ packets.  In particular, the model supports the application of\n   marking based\
    \ on a preceding classifier match.  The mark set in a\n   packet will determine\
    \ its subsequent PHB treatment in downstream\n   nodes of a network and possibly\
    \ also in subsequent processing stages\n   within this router.\n   DSCP Markers\
    \ for Diffserv are normally parameterized by a single\n   parameter: the 6-bit\
    \ DSCP to be marked in the packet header.\n      Marker1:\n      Type:       \
    \         DSCPMarker\n      Mark:                010010\n"
- title: 6.2.  Absolute Dropper
  contents:
  - "6.2.  Absolute Dropper\n   Absolute Droppers simply discard packets.  There are\
    \ no parameters\n   for these droppers.  Because this Absolute Dropper is a terminating\n\
    \   point of the datapath and has no outputs, it is probably desirable to\n  \
    \ forward the packet through a Counter Action first for instrumentation\n   purposes.\n\
    \      AbsoluteDropper1:\n      Type:                AbsoluteDropper\n   Absolute\
    \ Droppers are not the only elements than can cause a packet\n   to be discarded:\
    \ another element is an Algorithmic Dropper element\n   (see Section 7.1.3). \
    \ However, since this element's behavior is\n   closely tied the state of one\
    \ or more queues, we choose to\n   distinguish it as a separate functional datapath\
    \ element.\n"
- title: 6.3.  Multiplexor
  contents:
  - "6.3.  Multiplexor\n   It is occasionally necessary to multiplex traffic streams\
    \ into a\n   functional datapath element with a single input.  A M:1 (fan-in)\n\
    \   multiplexor is a simple logical device for merging traffic streams.\n   It\
    \ is parameterized by its number of incoming ports.\n      Mux1:\n      Type:\
    \                Multiplexor\n      Output:              Queue2\n"
- title: 6.4.  Counter
  contents:
  - "6.4.  Counter\n   One passive action is to account for the fact that a data packet\
    \ was\n   processed.  The statistics that result might be used later for\n   customer\
    \ billing, service verification or network engineering\n   purposes.  Counters\
    \ are 1:1 functional datapath elements which update\n   a counter by L and a packet\
    \ counter by 1 every time a L-byte sized\n   packet passes through them.  Counters\
    \ can be used to count packets\n   about to be dropped by an Absolute Dropper\
    \ or to count packets\n   arriving at or departing from some other functional\
    \ element.\n      Counter1:\n      Type:                Counter\n      Output:\
    \              Queue1\n"
- title: 6.5.  Null Action
  contents:
  - "6.5.  Null Action\n   A null action has one input and one output.  The element\
    \ performs no\n   action on the packet.  Such an element is useful to define in\
    \ the\n   event that the configuration or management interface does not have\n\
    \   the flexibility to omit an action element in a datapath segment.\n      Null1:\n\
    \      Type:                Null\n      Output:              Queue1\n"
- title: 7.  Queuing Elements
  contents:
  - "7.  Queuing Elements\n   Queuing elements modulate the transmission of packets\
    \ belonging to\n   the different traffic streams and determine their ordering,\
    \ possibly\n   storing them temporarily or discarding them.  Packets are usually\n\
    \   stored either because there is a resource constraint (e.g., available\n  \
    \ bandwidth) which prevents immediate forwarding, or because the\n   queuing block\
    \ is being used to alter the temporal properties of a\n   traffic stream (i.e.,\
    \ shaping).  Packets are discarded for one of the\n   following reasons:\n   \
    \   -  because of buffering limitations.\n      -  because a buffer threshold\
    \ is exceeded (including when shaping\n         is performed).\n      -  as a\
    \ feedback control signal to reactive control protocols such\n         as TCP.\n\
    \      -  because a meter exceeds a configured profile (i.e., policing).\n   The\
    \ queuing elements in this model represent a logical abstraction of\n   a queuing\
    \ system which is used to configure PHB-related parameters.\n   The model can\
    \ be used to represent a broad variety of possible\n   implementations.  However,\
    \ it need not necessarily map one-to-one\n   with physical queuing systems in\
    \ a specific router implementation.\n   Implementors should map the configurable\
    \ parameters of the\n   implementation's queuing systems to these queuing element\
    \ parameters\n   as appropriate to achieve equivalent behaviors.\n"
- title: 7.1.  Queuing Model
  contents:
  - "7.1.  Queuing Model\n   Queuing is a function which lends itself to innovation.\
    \  It must be\n   modeled to allow a broad range of possible implementations to\
    \ be\n   represented using common structures and parameters.  This model uses\n\
    \   functional decomposition as a tool to permit the needed latitude.\n   Queuing\
    \ systems perform three distinct, but related, functions:  they\n   store packets,\
    \ they modulate the departure of packets belonging to\n   various traffic streams\
    \ and they selectively discard packets.  This\n   model decomposes queuing into\
    \ the component elements that perform\n   each of these functions: Queues, Schedulers,\
    \ and Algorithmic\n   Droppers, respectively.  These elements may be connected\
    \ together as\n   part of a TCB, as described in section 8.\n   The remainder\
    \ of this section discusses FIFO Queues: typically, the\n   Queue element of this\
    \ model will be implemented as a FIFO data\n   structure.  However, this does\
    \ not preclude implementations which are\n   not strictly FIFO, in that they also\
    \ support operations that remove\n   or examine packets (e.g., for use by discarders)\
    \ other than at the\n   head or tail.  However, such operations must not have\
    \ the effect of\n   reordering packets belonging to the same microflow.\n   Note\
    \ that the term FIFO has multiple different common usages: it is\n   sometimes\
    \ taken to mean, among other things, a data structure that\n   permits items to\
    \ be removed only in the order in which they were\n   inserted or a service discipline\
    \ which is non-reordering.\n"
- title: 7.1.1.  FIFO Queue
  contents:
  - "7.1.1.  FIFO Queue\n   In this model, a FIFO Queue element is a data structure\
    \ which at any\n   time may contain zero or more packets.  It may have one or\
    \ more\n   thresholds associated with it.  A FIFO has one or more inputs and\n\
    \   exactly one output.  It must support an enqueue operation to add a\n   packet\
    \ to the tail of the queue and a dequeue operation to remove a\n   packet from\
    \ the head of the queue.  Packets must be dequeued in the\n   order in which they\
    \ were enqueued.  A FIFO has a current depth, which\n   indicates the number of\
    \ packets and/or bytes that it contains at a\n   particular time.  FIFOs in this\
    \ model are modeled without inherent\n   limits on their depth - obviously this\
    \ does not reflect the reality\n   of implementations: FIFO size limits are modeled\
    \ here by an\n   algorithmic dropper associated with the FIFO, typically at its\
    \ input.\n   It is quite likely that every FIFO will be preceded by an algorithmic\n\
    \   dropper.  One exception might be the case where the packet stream has\n  \
    \ already been policed to a profile that can never exceed the scheduler\n   bandwidth\
    \ available at the FIFO's output - this would not need an\n   algorithmic dropper\
    \ at the input to the FIFO.\n   This representation of a FIFO allows for one common\
    \ type of depth\n   limit, one that results from a FIFO supplied from a limited\
    \ pool of\n   buffers, shared between multiple FIFOs.\n   In an implementation,\
    \ packets are presumably stored in one or more\n   buffers.  Buffers are allocated\
    \ from one or more free buffer pools.\n   If there are multiple instances of a\
    \ FIFO, their packet buffers may\n   or may not be allocated out of the same free\
    \ buffer pool.  Free\n   buffer pools may also have one or more thresholds associated\
    \ with\n   them, which may affect discarding and/or scheduling.  Other than\n\
    \   this, buffering mechanisms are implementation specific and not part\n   of\
    \ this model.\n   A FIFO might be represented using the following parameters:\n\
    \      Queue1:\n      Type:       FIFO\n      Output:     Scheduler1\n   Note\
    \ that a FIFO must provide triggers and/or current state\n   information to other\
    \ elements upstream and downstream from it: in\n   particular, it is likely that\
    \ the current depth will need to be used\n   by Algorithmic Dropper elements placed\
    \ before or after the FIFO.  It\n   will also likely need to provide an implicit\
    \ \"I have packets for you\"\n   signal to downstream Scheduler elements.\n"
- title: 7.1.2.  Scheduler
  contents:
  - "7.1.2.  Scheduler\n   A scheduler is an element which gates the departure of\
    \ each packet\n   that arrives at one of its inputs, based on a service discipline.\
    \  It\n   has one or more inputs and exactly one output.  Each input has an\n\
    \   upstream element to which it is connected, and a set of parameters\n   that\
    \ affects the scheduling of packets received at that input.\n   The service discipline\
    \ (also known as a scheduling algorithm) is an\n   algorithm which might take\
    \ any of the following as its input(s):\n   a) static parameters such as relative\
    \ priority associated with each\n      of the scheduler's inputs.\n   b) absolute\
    \ token bucket parameters for maximum or minimum rates\n      associated with\
    \ each of the scheduler's inputs.\n   c) parameters, such as packet length or\
    \ DSCP, associated with the\n      packet currently present at its input.\n  \
    \ d) absolute time and/or local state.\n   Possible service disciplines fall into\
    \ a number of categories,\n   including (but not limited to) first come, first\
    \ served (FCFS),\n   strict priority, weighted fair bandwidth sharing (e.g., WFQ),\
    \ rate-\n   limited strict priority, and rate-based.  Service disciplines can\
    \ be\n   further distinguished by whether they are work-conserving or non-\n \
    \  work-conserving (see Glossary).  Non-work-conserving schedulers can\n   be\
    \ used to shape traffic streams to match some profile by delaying\n   packets\
    \ that might be deemed non-conforming by some downstream node:\n   a packet is\
    \ delayed until such time as it would conform to a\n   downstream meter using\
    \ the same profile.\n   [DSARCH] defines PHBs without specifying required scheduling\n\
    \   algorithms.  However, PHBs such as the class selectors [DSFIELD], EF\n   [EF-PHB]\
    \ and AF [AF-PHB] have descriptions or configuration\n   parameters which strongly\
    \ suggest the sort of scheduling discipline\n   needed to implement them.  This\
    \ document discusses a minimal set of\n   queue parameters to enable realization\
    \ of these PHBs.  It does not\n   attempt to specify an all-embracing set of parameters\
    \ to cover all\n   possible implementation models.  A minimal set includes:\n\
    \   a) a minimum service rate profile which allows rate guarantees for\n     \
    \ each traffic stream as required by EF and AF without specifying\n      the details\
    \ of how excess bandwidth between these traffic streams\n      is shared.  Additional\
    \ parameters to control this behavior should\n      be made available, but are\
    \ dependent on the particular scheduling\n      algorithm implemented.\n   b)\
    \ a service priority, used only after the minimum rate profiles of\n      all\
    \ inputs have been satisfied, to decide how to allocate any\n      remaining bandwidth.\n\
    \   c) a maximum service rate profile, for use only with a non-work-\n      conserving\
    \ service discipline.\n   Any one of these profiles is composed, for the purposes\
    \ of this\n   model, of both a rate (in suitable units of bits, bytes or larger\n\
    \   chunks in some unit of time) and a burst size, as discussed further\n   in\
    \ Appendix A.\n   By way of example, for an implementation of the EF PHB using\
    \ a strict\n   priority scheduling algorithm that assumes that the aggregate EF\
    \ rate\n   has been appropriately bounded by upstream policing to avoid\n   starvation\
    \ of other BAs, the service rate profiles are not used: the\n   minimum service\
    \ rate profile would be defaulted to zero and the\n   maximum service rate profile\
    \ would effectively be the \"line rate\".\n   Such an implementation, with multiple\
    \ priority classes, could also be\n   used for the Diffserv class selectors [DSFIELD].\n\
    \   Alternatively, setting the service priority values for each input to\n   the\
    \ scheduler to the same value enables the scheduler to satisfy the\n   minimum\
    \ service rates for each input, so long as the sum of all\n   minimum service\
    \ rates is less than or equal to the line rate.\n   For example, a non-work-conserving\
    \ scheduler, allocating spare\n   bandwidth equally between all its inputs, might\
    \ be represented using\n   the following parameters:\n      Scheduler1:\n    \
    \  Type:           Scheduler2Input\n      Input1:\n      MaxRateProfile: Profile1\n\
    \      MinRateProfile: Profile2\n      Priority:       none\n      Input2:\n \
    \     MaxRateProfile: Profile3\n      MinRateProfile: Profile4\n      Priority:\
    \       none\n   A work-conserving scheduler might be represented using the following\n\
    \   parameters:\n      Scheduler2:\n      Type:           Scheduler3Input\n  \
    \    Input1:\n      MaxRateProfile: WorkConserving\n      MinRateProfile: Profile5\n\
    \      Priority:       1\n      Input2:\n      MaxRateProfile: WorkConserving\n\
    \      MinRateProfile: Profile6\n      Priority:       2\n      Input3:\n    \
    \  MaxRateProfile: WorkConserving\n      MinRateProfile: none\n      Priority:\
    \       3\n"
- title: 7.1.3.  Algorithmic Dropper
  contents:
  - "7.1.3.  Algorithmic Dropper\n   An Algorithmic Dropper is an element which selectively\
    \ discards\n   packets that arrive at its input, based on a discarding algorithm.\n\
    \   It has one data input and one output.  In this model (but not\n   necessarily\
    \ in a real implementation), a packet enters the dropper at\n   its input and\
    \ either its buffer is returned to a free buffer pool or\n   the packet exits\
    \ the dropper at the output.\n   Alternatively, an Algorithmic Dropper can be\
    \ thought of as invoking\n   operations on a FIFO Queue which selectively remove\
    \ a packet and\n   return its buffer to the free buffer pool based on a discarding\n\
    \   algorithm.  In this case, the operation could be modeled as being a\n   side-effect\
    \ on the FIFO upon which it operated, rather than as having\n   a discrete input\
    \ and output.  This treatment is equivalent and we\n   choose the one described\
    \ in the previous paragraph for this model.\n   One of the primary characteristics\
    \ of an Algorithmic Dropper is the\n   choice of which packet (if any) is to be\
    \ dropped: for the purposes of\n   this model, we restrict the packet selection\
    \ choices to one of the\n   following and we indicate the choice by the relative\
    \ positions of\n   Algorithmic Dropper and FIFO Queue elements in the model:\n\
    \   a) selection of a packet that is about to be added to the tail of a\n    \
    \  queue (a \"Tail Dropper\"): the output of the Algorithmic Dropper\n      element\
    \ is connected to the input of the relevant FIFO Queue\n      element.\n   b)\
    \ a packet that is currently at the head of a queue (a \"Head\n      Dropper\"\
    ): the output of the FIFO Queue element is connected to\n      the input of the\
    \ Algorithmic Dropper element.\n   Other packet selection methods could be added\
    \ to this model in the\n   form of a different type of datapath element.\n   The\
    \ Algorithmic Dropper is modeled as having a single input.  It is\n   possible\
    \ that packets which were classified differently by a\n   Classifier in this TCB\
    \ will end up passing through the same dropper.\n   The dropper's algorithm may\
    \ need to apply different calculations\n   based on characteristics of the incoming\
    \ packet (e.g., its DSCP).  So\n   there is a need, in implementations of this\
    \ model, to be able to\n   relate information about which classifier element was\
    \ matched by a\n   packet from a Classifier to an Algorithmic Dropper.  In the\
    \ rare\n   cases where this is required, the chosen model is to insert another\n\
    \   Classifier element at this point in the flow and for it to feed into\n   multiple\
    \ Algorithmic Dropper elements, each one implementing a drop\n   calculation that\
    \ is independent of any classification keys of the\n   packet: this will likely\
    \ require the creation of a new TCB to contain\n   the Classifier and the Algorithmic\
    \ Dropper elements.\n      NOTE: There are many other formulations of a model\
    \ that could\n      represent this linkage that are different from the one described\n\
    \      above: one formulation would have been to have a pointer from one\n   \
    \   of the drop probability calculation algorithms inside the dropper\n      to\
    \ the original Classifier element that selects this algorithm.\n      Another\
    \ way would have been to have multiple \"inputs\" to the\n      Algorithmic Dropper\
    \ element fed from the preceding elements,\n      leading eventually back to the\
    \ Classifier elements that matched\n      the packet.  Yet another formulation\
    \ might have been for the\n      Classifier to (logically) include some sort of\
    \ \"classification\n      identifier\" along with the packet along its path, for\
    \ use by any\n      subsequent element.  And yet another could have been to include\
    \ a\n      classifier inside the dropper, in order for it to pick out the\n  \
    \    drop algorithm to be applied.  These other approaches could be\n      used\
    \ by implementations but were deemed to be less clear than the\n      approach\
    \ taken here.\n   An Algorithmic Dropper, an example of which is illustrated in\
    \ Figure\n   5, has one or more triggers that cause it to make a decision whether\n\
    \   or not to drop one (or possibly more than one) packet.  A trigger may\n  \
    \ be internal (the arrival of a packet at the input to the dropper) or\n   it\
    \ may be external (resulting from one or more state changes at\n   another element,\
    \ such as a FIFO Queue depth crossing a threshold or a\n   scheduling event).\
    \  It is likely that an instantaneous FIFO depth\n   will need to be smoothed\
    \ over some averaging interval before being\n   used as a useful trigger.  Some\
    \ dropping algorithms may require\n   several trigger inputs feeding back from\
    \ events elsewhere in the\n   system (e.g., depth-smoothing functions that calculate\
    \ averages over\n   more than one time interval).\n              +------------------+\
    \      +-----------+\n              | +-------+        |  n   |smoothing  |\n\
    \              | |trigger|<----------/---|function(s)|\n              | |calc.\
    \  |        |      |(optional) |\n              | +-------+        |      +-----------+\n\
    \              |     |            |          ^\n              |     v        \
    \    |          |Depth\n     Input    | +-------+ no     |      ------------+\
    \   to Scheduler\n     ---------->|discard|-------------->    |x|x|x|x|------->\n\
    \              | |   ?   |        |      ------------+\n              | +-------+\
    \        |           FIFO\n              |    |yes          |\n              |\
    \  | | |           |\n              |  | v | count +   |\n              |  +---+\
    \ bit-bucket|\n              +------------------+\n              Algorithmic\n\
    \              Dropper\n      Figure 5. Example of Algorithmic Dropper from Tail\
    \ of a Queue\n   A trigger may be a boolean combination of events (e.g., a FIFO\
    \ depth\n   exceeding a threshold OR a buffer pool depth falling below a\n   threshold).\
    \  It takes as its input some set of dynamic parameters\n   (e.g., smoothed or\
    \ instantaneous FIFO depth), and some set of static\n   parameters (e.g., thresholds),\
    \ and possibly other parameters\n   associated with the packet.  It may also have\
    \ internal state (e.g.,\n   history of its past actions).  Note that, although\
    \ an Algorithmic\n   Dropper may require knowledge of data fields in a packet,\
    \ as\n   discovered by a Classifier in the same TCB, it may not modify the\n \
    \  packet (i.e., it is not a marker).\n   The result of the trigger calculation\
    \ is that the dropping algorithm\n   makes a decision on whether to forward or\
    \ to discard a packet.  The\n   discarding function is likely to keep counters\
    \ regarding the\n   discarded packets (there is no appropriate place here to include\
    \ a\n   Counter Action element).\n   The example in Figure 5 also shows a FIFO\
    \ Queue element from whose\n   tail the dropping is to take place and whose depth\
    \ characteristics\n   are used by this Algorithmic Dropper.  It also shows where\
    \ a depth-\n   smoothing function might be included: smoothing functions are outside\n\
    \   the scope of this document and are not modeled explicitly here, we\n   merely\
    \ indicate where they might be added.\n   RED, RED-on-In-and-Out (RIO) and Drop-on-threshold\
    \ are examples of\n   dropping algorithms.  Tail-dropping and head-dropping are\
    \ effected by\n   the location of the Algorithmic Dropper element relative to\
    \ the FIFO\n   Queue element.  As an example, a dropper using a RIO algorithm\
    \ might\n   be represented using 2 Algorithmic Droppers with the following\n \
    \  parameters:\n      AlgorithmicDropper1: (for in-profile traffic)\n      Type:\
    \                   AlgorithmicDropper\n      Discipline:             RED\n  \
    \    Trigger:                Internal\n      Output:                 Fifo1\n \
    \     MinThresh:              Fifo1.Depth > 20 kbyte\n      MaxThresh:       \
    \       Fifo1.Depth > 30 kbyte\n      SampleWeight            .002\n      MaxDropProb\
    \             1%\n      AlgorithmicDropper2: (for out-of-profile traffic)\n  \
    \    Type:                   AlgorithmicDropper\n      Discipline:           \
    \  RED\n      Trigger:                Internal\n      Output:                \
    \ Fifo1\n      MinThresh:              Fifo1.Depth > 10 kbyte\n      MaxThresh:\
    \              Fifo1.Depth > 20 kbyte\n      SampleWeight            .002\n  \
    \    MaxDropProb             2%\n   Another form of Algorithmic Dropper, a threshold-dropper,\
    \ might be\n   represented using the following parameters:\n      AlgorithmicDropper3:\n\
    \      Type:                   AlgorithmicDropper\n      Discipline:         \
    \    Drop-on-threshold\n      Trigger:                Fifo2.Depth > 20 kbyte\n\
    \      Output:                 Fifo1\n"
- title: 7.2.  Sharing load among traffic streams using queuing
  contents:
  - "7.2.  Sharing load among traffic streams using queuing\n   Queues are used, in\
    \ Differentiated Services, for a number of\n   purposes.  In essence, they are\
    \ simply places to store traffic until\n   it is transmitted.  However, when several\
    \ queues are used together in\n   a queuing system, they can also achieve effects\
    \ beyond that for given\n   traffic streams.  They can be used to limit variation\
    \ in delay or\n   impose a maximum rate (shaping), to permit several streams to\
    \ share a\n   link in a semi-predictable fashion (load sharing), or to move\n\
    \   variation in delay from some streams to other streams.\n   Traffic shaping\
    \ is often used to condition traffic, such that packets\n   arriving in a burst\
    \ will be \"smoothed\" and deemed conforming by\n   subsequent downstream meters\
    \ in this or other nodes.  In [DSARCH] a\n   shaper is described as a queuing\
    \ element controlled by a meter which\n   defines its temporal profile.  However,\
    \ this representation of a\n   shaper differs substantially from typical shaper\
    \ implementations.\n   In the model described here, a shaper is realized by using\
    \ a non-\n   work-conserving Scheduler.  Some implementations may elect to have\n\
    \   queues whose sole purpose is shaping, while others may integrate the\n   shaping\
    \ function with other buffering, discarding, and scheduling\n   associated with\
    \ access to a resource.  Shapers operate by delaying\n   the departure of packets\
    \ that would be deemed non-conforming by a\n   meter configured to the shaper's\
    \ maximum service rate profile.  The\n   packet is scheduled to depart no sooner\
    \ than such time that it would\n   become conforming.\n"
- title: 7.2.1.  Load Sharing
  contents:
  - "7.2.1.  Load Sharing\n   Load sharing is the traditional use of queues and was\
    \ theoretically\n   explored by Floyd & Jacobson [FJ95], although it has been\
    \ in use in\n   communications systems since the 1970's.\n   [DSARCH] discusses\
    \ load sharing as dividing an interface among\n   traffic classes predictably,\
    \ or applying a minimum rate to each of a\n   set of traffic classes, which might\
    \ be measured as an absolute lower\n   bound on the rate a traffic stream achieves\
    \ or a fraction of the rate\n   an interface offers.  It is generally implemented\
    \ as some form of\n   weighted queuing algorithm among a set of FIFO queues i.e.,\
    \ a WFQ\n   scheme.  This has interesting side-effects.\n   A key effect sought\
    \ is to ensure that the mean rate the traffic in a\n   stream experiences is never\
    \ lower than some threshold when there is\n   at least that much traffic to send.\
    \  When there is less traffic than\n   this, the queue tends to be starved of\
    \ traffic, meaning that the\n   queuing system will not delay its traffic by very\
    \ much.  When there\n   is significantly more traffic and the queue starts filling,\
    \ packets\n   in this class will be delayed significantly more than traffic in\n\
    \   other classes that are under-using their available capacity.  This\n   form\
    \ of queuing system therefore tends to move delay and variation in\n   delay from\
    \ under-used classes of traffic to heavier users, as well as\n   managing the\
    \ rates of the traffic streams.\n   A side-effect of a WRR or WFQ implementation\
    \ is that between any two\n   packets in a given traffic class, the scheduler\
    \ may emit one or more\n   packets from each of the other classes in the queuing\
    \ system.  In\n   cases where average behavior is in view, this is perfectly\n\
    \   acceptable.  In cases where traffic is very intolerant of jitter and\n   there\
    \ are a number of competing classes, this may have undesirable\n   consequences.\n"
- title: 7.2.2.  Traffic Priority
  contents:
  - "7.2.2.  Traffic Priority\n   Traffic Prioritization is a special case of load\
    \ sharing, wherein a\n   certain traffic class is deemed so jitter-intolerant\
    \ that if it has\n   traffic present, that traffic must be sent at the earliest\
    \ possible\n   time.  By extension, several priorities might be defined, such\
    \ that\n   traffic in each of several classes is given preferential service over\n\
    \   any traffic of a lower class.  It is the obvious implementation of IP\n  \
    \ Precedence as described in [RFC 791], of 802.1p traffic classes\n   [802.1D],\
    \ and other similar technologies.\n   Priority is often abused in real networks;\
    \ people tend to think that\n   traffic which has a high business priority deserves\
    \ this treatment\n   and talk more about the business imperatives than the actual\n\
    \   application requirements.  This can have severe consequences;\n   networks\
    \ have been configured which placed business-critical traffic\n   at a higher\
    \ priority than routing-protocol traffic, resulting in\n   collapse of the network's\
    \ management or control systems.  However, it\n   may have a legitimate use for\
    \ services based on an Expedited\n   Forwarding (EF) PHB, where it is absolutely\
    \ sure, thanks to policing\n   at all possible traffic entry points, that a traffic\
    \ stream does not\n   abuse its rate and that the application is indeed jitter-intolerant\n\
    \   enough to merit this type of handling.  Note that, even in cases with\n  \
    \ well-policed ingress points, there is still the possibility of\n   unexpected\
    \ traffic loops within an un-policed core part of the\n   network causing such\
    \ collapse.\n"
- title: 8.  Traffic Conditioning Blocks (TCBs)
  contents:
  - "8.  Traffic Conditioning Blocks (TCBs)\n   The Classifier, Meter, Action, Algorithmic\
    \ Dropper, Queue and\n   Scheduler functional datapath elements described above\
    \ can be\n   combined into Traffic Conditioning Blocks (TCBs).  A TCB is an\n\
    \   abstraction of a set of functional datapath elements that may be used\n  \
    \ to facilitate the definition of specific traffic conditioning\n   functionality\
    \ (e.g., it might be likened to a template which can be\n   replicated multiple\
    \ times for different traffic streams or different\n   customers).  It has no\
    \ likely physical representation in the\n   implementation of the data path: it\
    \ is invented purely as an\n   abstraction for use by management tools.\n   This\
    \ model describes the configuration and management of a Diffserv\n   interface\
    \ in terms of a TCB that contains, by definition, zero or\n   more Classifier,\
    \ Meter, Action, Algorithmic Dropper, Queue and\n   Scheduler elements.  These\
    \ elements are arranged arbitrarily\n   according to the policy being expressed,\
    \ but always in the order\n   here.  Traffic may be classified; classified traffic\
    \ may be metered;\n   each stream of traffic identified by a combination of classifiers\
    \ and\n   meters may have some set of actions performed on it, followed by drop\n\
    \   algorithms; packets of the traffic stream may ultimately be stored\n   into\
    \ a queue and then be scheduled out to the next TCB or physical\n   interface.\
    \  It is permissible to omit elements or include null\n   elements of any type,\
    \ or to concatenate multiple functional datapath\n   elements of the same type.\n\
    \   When the Diffserv treatment for a given packet needs to have such\n   building\
    \ blocks repeated, this is performed by cascading multiple\n   TCBs:  an output\
    \ of one TCB may drive the input of a succeeding one.\n   For example, consider\
    \ the case where traffic of a set of classes is\n   shaped to a set of rates,\
    \ but the total output rate of the group of\n   classes must also be limited to\
    \ a rate.  One might imagine a set of\n   network news feeds, each with a certain\
    \ maximum rate, and a policy\n   that their aggregate may not exceed some figure.\
    \  This may be simply\n   accomplished by cascading two TCBs.  The first classifies\
    \ the traffic\n   into its separate feeds and queues each feed separately.  The\
    \ feeds\n   (or a subset of them) are now fed into a second TCB, which places\
    \ all\n   input (these news feeds) into a single queue with a certain maximum\n\
    \   rate.  In implementation, one could imagine this as the several\n   literal\
    \ queues, a CBQ or WFQ system with an appropriate (and complex)\n   weighting\
    \ scheme, or a number of other approaches.  But they would\n   have the same externally\
    \ measurable effect on the traffic as if they\n   had been literally implemented\
    \ with separate TCBs.\n"
- title: 8.1.  TCB
  contents:
  - "8.1.  TCB\n   A generalized TCB might consist of the following stages:\n    \
    \  -  Classification stage\n      -  Metering stage\n      -  Action stage (involving\
    \ Markers, Absolute Droppers, Counters,\n         and Multiplexors)\n      - \
    \ Queuing stage (involving Algorithmic Droppers, Queues, and\n         Schedulers)\n\
    \   where each stage may consist of a set of parallel datapaths\n   consisting\
    \ of pipelined elements.\n   A Classifier or a Meter is typically a 1:N element,\
    \ an Action,\n   Algorithmic Dropper, or Queue is typically a 1:1 element and\
    \ a\n   Scheduler is a N:1 element.  A complete TCB should, however, result\n\
    \   in a 1:1 or 1:N abstract element.  Note that the fan-in or fan-out of\n  \
    \ an element is not an important defining characteristic of this\n   taxonomy.\n"
- title: 8.1.1.  Building blocks for Queuing
  contents:
  - "8.1.1.  Building blocks for Queuing\n   Some particular rules are applied to\
    \ the ordering of elements within\n   a Queuing stage within a TCB: elements of\
    \ the same type may appear\n   more than once, either in parallel or in series.\
    \  Typically, a\n   queuing stage will have relatively many elements in parallel\
    \ and few\n   in series.  Iteration and recursion are not supported constructs\
    \ (the\n   elements are arranged in an acyclic graph).  The following inter-\n\
    \   connections of elements are allowed:\n      -  The input of a Queue may be\
    \ the input of the queuing block, or\n         it may be connected to the output\
    \ of an Algorithmic Dropper, or\n         to an output of a Scheduler.\n     \
    \ -  Each input of a Scheduler may be connected to the output of a\n         Queue,\
    \ to the output of an Algorithmic Dropper, or to the\n         output of another\
    \ Scheduler.\n      -  The input of an Algorithmic Dropper may be the first element\
    \ of\n         the queuing stage, the output of another Algorithmic Dropper,\n\
    \         or it may be connected to the output of a Queue (to indicate\n     \
    \    head-dropping).\n      -  The output of the queuing block may be the output\
    \ of a Queue,\n         an Algorithmic Dropper, or a Scheduler.\n   Note, in particular,\
    \ that Schedulers may operate in series such so\n   that a packet at the head\
    \ of a Queue feeding the concatenated\n   Schedulers is serviced only after all\
    \ of the scheduling criteria are\n   met.  For example, a Queue which carries\
    \ EF traffic streams may be\n   served first by a non-work-conserving Scheduler\
    \ to shape the stream\n   to a maximum rate, then by a work-conserving Scheduler\
    \ to mix EF\n   traffic streams with other traffic streams.  Alternatively, there\n\
    \   might be a Queue and/or a dropper between the two Schedulers.\n   Note also\
    \ that some non-sensical scenarios (e.g., a Queue preceding\n   an Algorithmic\
    \ Dropper, directly feeding into another Queue), are\n   prohibited.\n"
- title: 8.2.  An Example TCB
  contents:
  - "8.2.  An Example TCB\n   A SLS is presumed to have been negotiated between the\
    \ customer and\n   the provider which specifies the handling of the customer's\
    \ traffic,\n   as defined by a TCS) by the provider's network.  The agreement\
    \ might\n   be of the following form:\n      DSCP     PHB   Profile     Treatment\n\
    \      ----     ---   -------     ----------------------\n      001001   EF  \
    \  Profile4    Discard non-conforming.\n      001100   AF11  Profile5    Shape\
    \ to profile, tail-drop when full.\n      001101   AF21  Profile3    Re-mark non-conforming\
    \ to DSCP 001000,\n                                 tail-drop when full.\n   \
    \   other    BE    none        Apply RED-like dropping.\n   This SLS specifies\
    \ that the customer may submit packets marked for\n   DSCP 001001 which will get\
    \ EF treatment so long as they remain\n   conforming to Profile4, which will be\
    \ discarded if they exceed this\n   profile.  The discarded packets are counted\
    \ in this example, perhaps\n   for use by the provider's sales department in convincing\
    \ the customer\n   to buy a larger SLS.  Packets marked for DSCP 001100 will be\
    \ shaped\n   to Profile5 before forwarding.  Packets marked for DSCP 001101 will\n\
    \   be metered to Profile3 with non-conforming packets \"downgraded\" by\n   being\
    \ re-marked with a DSCP of 001000.  It is implicit in this\n   agreement that\
    \ conforming packets are given the PHB originally\n   indicated by the packets'\
    \ DSCP field.\n   Figures 6 and 7 illustrates a TCB that might be used to handle\
    \ this\n   SLS at an ingress interface at the customer/provider boundary.\n  \
    \ The Classification stage of this example consists of a single BA\n   classifier.\
    \  The BA classifier is used to separate traffic based on\n   the Diffserv service\
    \ level requested by the customer (as indicated by\n   the DSCP in each submitted\
    \ packet's IP header).  We illustrate three\n   DSCP filter values: A, B, and\
    \ C. The 'X' in the BA classifier is a\n   wildcard filter that matches every\
    \ packet not otherwise matched.\n   The path for DSCP 001100 proceeds directly\
    \ to Dropper1 whilst the\n   paths for DSCP 001001 and 001101 include a metering\
    \ stage.  All other\n   traffic is passed directly on to Dropper3.  There is a\
    \ separate meter\n   for each set of packets corresponding to classifier outputs\
    \ A and C.\n   Each meter uses a specific profile, as specified in the TCS, for\
    \ the\n   corresponding Diffserv service level.  The meters in this example\n\
    \   each indicate one of two conformance levels: conforming or non-\n   conforming.\n\
    \   Following the Metering stage is an Action stage in some of the\n   branches.\
    \  Packets submitted for DSCP 001001 (Classifier output A)\n   that are deemed\
    \ non-conforming by Meter1 are counted and discarded\n   while packets that are\
    \ conforming are passed on to Queue1.  Packets\n   submitted for DSCP 001101 (Classifier\
    \ output C) that are deemed non-\n   conforming by Meter2 are re-marked and then\
    \ both conforming and non-\n   conforming packets are multiplexed together before\
    \ being passed on to\n   Dropper2/Queue3.\n   The Algorithmic Dropping, Queuing\
    \ and Scheduling stages are realized\n   as follows, illustrated in figure 7.\
    \  Note that the figure does not\n   show any of the implicit control linkages\
    \ between elements that allow\n   e.g., an Algorithmic Dropper to sense the current\
    \ state of a\n   succeeding Queue.\n                         +-----+\n       \
    \                  |    A|---------------------------> to Queue1\n           \
    \           +->|     |\n                      |  |    B|--+  +-----+    +-----+\n\
    \                      |  +-----+  |  |     |    |     |\n                   \
    \   |  Meter1   +->|     |--->|     |\n                      |              |\
    \     |    |     |\n                      |              +-----+    +-----+\n\
    \                      |              Counter1   Absolute\n"
- title: submitted +-----+     |                         Dropper1
  contents:
  - 'submitted +-----+     |                         Dropper1

    '
- title: traffic   |    A|-----+
  contents:
  - 'traffic   |    A|-----+

    '
- title: '--------->|    B|--------------------------------------> to AlgDropper1'
  contents:
  - "--------->|    B|--------------------------------------> to AlgDropper1\n   \
    \       |    C|-----+\n          |    X|--+  |\n          +-----+  |  |  +-----+\
    \                +-----+\n        Classifier1|  |  |    A|--------------->|A \
    \   |\n           (BA)    |  +->|     |                |     |--> to AlgDrop2\n\
    \                   |     |    B|--+  +-----+ +->|B    |\n                   |\
    \     +-----+  |  |     | |  +-----+\n                   |     Meter2   +->| \
    \    |-+    Mux1\n                   |                 |     |\n             \
    \      |                 +-----+\n                   |                 Marker1\n\
    \                   +-----------------------------------> to AlgDropper3\n   \
    \  Figure 6:  An Example Traffic Conditioning Block (Part 1)\n   Conforming DSCP\
    \ 001001 packets from Meter1 are passed directly to\n   Queue1: there is no way,\
    \ with configuration of the following\n   Scheduler to match the metering, for\
    \ these packets to overflow the\n   depth of Queue1, so there is no requirement\
    \ for dropping at this\n   point.  Packets marked for DSCP 001100 must be passed\
    \ through a\n   tail-dropper, AlgDropper1, which serves to limit the depth of\
    \ the\n   following queue, Queue2: packets that arrive to a full queue will be\n\
    \   discarded.  This is likely to be an error case: the customer is\n   obviously\
    \ not sticking to its agreed profile.  Similarly, all packets\n   from the original\
    \ DSCP 001101 stream (some may have been re-marked by\n   this stage) are passed\
    \ to AlgDropper2 and Queue3.  Packets marked for\n   all other DSCPs are passed\
    \ to AlgDropper3 which is a RED-like\n   Algorithmic Dropper: based on feedback\
    \ of the current depth of\n   Queue4, this dropper is supposed to discard enough\
    \ packets from its\n   input stream to keep the queue depth under control.\n \
    \  These four Queue elements are then serviced by a Scheduler element\n   Scheduler1:\
    \ this must be configured to give each of its inputs an\n   appropriate priority\
    \ and/or bandwidth share.  Inputs A and C are\n   given guarantees of bandwidth,\
    \ as appropriate for the contracted\n   profiles.  Input B is given a limit on\
    \ the bandwidth it can use\n   (i.e., a non-work-conserving discipline) in order\
    \ to achieve the\n   desired shaping of this stream.  Input D is given no limits\
    \ or\n   guarantees but a lower priority than the other queues, appropriate\n\
    \   for its best-effort status.  Traffic then exits the Scheduler in a\n   single\
    \ orderly stream.\n   The interconnections of the TCB elements illustrated in\
    \ Figures 6 and\n   7 can be represented textually as follows:\n        TCB1:\n\
    \        Classifier1:\n        FilterA:             Meter1\n        FilterB: \
    \            Dropper1\n        FilterC:             Meter2\n        Default: \
    \            Dropper3\n      from Meter1                     +-----+\n      ------------------------------->|\
    \     |----+\n                                      |     |    |\n           \
    \                           +-----+    |\n                                   \
    \   Queue1     |\n                                                 |  +-----+\n\
    \      from Classifier1 +-----+        +-----+    +->|A    |\n      ---------------->|\
    \     |------->|     |------>|B    |------->\n                       |     | \
    \       |     |  +--->|C    |  exiting\n                       +-----+       \
    \ +-----+  | +->|D    |  traffic\n                       AlgDropper1    Queue2\
    \   | |  +-----+\n                                               | |  Scheduler1\n\
    \      from Mux1        +-----+        +-----+  | |\n      ---------------->|\
    \     |------->|     |--+ |\n                       |     |        |     |   \
    \ |\n                       +-----+        +-----+    |\n                    \
    \   AlgDropper2    Queue3     |\n                                            \
    \     |\n      from Classifier1 +-----+        +-----+    |\n      ---------------->|\
    \     |------->|     |----+\n                       |     |        |     |\n \
    \                      +-----+        +-----+\n                       AlgDropper3\
    \    Queue4\n        Figure 7: An Example Traffic Conditioning Block (Part 2)\n\
    \        Meter1:\n        Type:                AverageRate\n        Profile: \
    \            Profile4\n        ConformingOutput:    Queue1\n        NonConformingOutput:\
    \ Counter1\n        Counter1:\n        Output:              AbsoluteDropper1\n\
    \        Meter2:\n        Type:                AverageRate\n        Profile: \
    \            Profile3\n        ConformingOutput:    Mux1.InputA\n        NonConformingOutput:\
    \ Marker1\n        Marker1:\n        Type:                DSCPMarker\n       \
    \ Mark:                001000\n        Output:              Mux1.InputB\n    \
    \    Mux1:\n        Output:              Dropper2\n        AlgDropper1:\n    \
    \    Type:                AlgorithmicDropper\n        Discipline:          Drop-on-threshold\n\
    \        Trigger:             Queue2.Depth > 10kbyte\n        Output:        \
    \      Queue2\n        AlgDropper2:\n        Type:                AlgorithmicDropper\n\
    \        Discipline:          Drop-on-threshold\n        Trigger:            \
    \ Queue3.Depth > 20kbyte\n        Output:              Queue3\n        AlgDropper3:\n\
    \        Type:                AlgorithmicDropper\n        Discipline:        \
    \  RED93\n        Trigger:             Internal\n        Output:             \
    \ Queue3\n        MinThresh:           Queue3.Depth > 20 kbyte\n        MaxThresh:\
    \           Queue3.Depth > 40 kbyte\n           <other RED parms too>\n      \
    \  Queue1:\n        Type:                FIFO\n        Output:              Scheduler1.InputA\n\
    \        Queue2:\n        Type:                FIFO\n        Output:         \
    \     Scheduler1.InputB\n        Queue3:\n        Type:                FIFO\n\
    \        Output:              Scheduler1.InputC\n        Queue4:\n        Type:\
    \                FIFO\n        Output:              Scheduler1.InputD\n      \
    \  Scheduler1:\n        Type:                Scheduler4Input\n        InputA:\n\
    \        MaxRateProfile:      none\n        MinRateProfile:      Profile4\n  \
    \      Priority:            20\n        InputB:\n        MaxRateProfile:     \
    \ Profile5\n        MinRateProfile:      none\n        Priority:            40\n\
    \        InputC:\n        MaxRateProfile:      none\n        MinRateProfile: \
    \     Profile3\n        Priority:            20\n        InputD:\n        MaxRateProfile:\
    \      none\n        MinRateProfile:      none\n        Priority:            10\n"
- title: 8.3.  An Example TCB to Support Multiple Customers
  contents:
  - "8.3.  An Example TCB to Support Multiple Customers\n   The TCB described above\
    \ can be installed on an ingress interface to\n   implement a provider/customer\
    \ TCS if the interface is dedicated to\n   the customer.  However, if a single\
    \ interface is shared between\n   multiple customers, then the TCB above will\
    \ not suffice, since it\n   does not differentiate among traffic from different\
    \ customers.  Its\n   classification stage uses only BA classifiers.\n   The configuration\
    \ is readily modified to support the case of multiple\n   customers per interface,\
    \ as follows.  First, a TCB is defined for\n   each customer to reflect the TCS\
    \ with that customer: TCB1, defined\n   above is the TCB for customer 1.  Similar\
    \ elements are created for\n   TCB2 and for TCB3 which reflect the agreements\
    \ with customers 2 and 3\n   respectively.  These 3 TCBs may or may not contain\
    \ similar elements\n   and parameters.\n   Finally, a classifier is added to the\
    \ front end to separate the\n   traffic from the three different customers.  This\
    \ forms a new TCB,\n   TCB4, which is illustrated in Figure 8.\n   A representation\
    \ of this multi-customer TCB might be:\n      TCB4:\n      Classifier4:\n    \
    \  Filter1:     to TCB1\n      Filter2:     to TCB2\n      Filter3:     to TCB3\n\
    \      No Match:    AbsoluteDropper4\n      AbsoluteDropper4:\n      Type:   \
    \             AbsoluteDropper\n      TCB1:\n      (as defined above)\n      TCB2:\n\
    \      (similar to TCB1, perhaps with different\n       elements or numeric parameters)\n\
    \      TCB3:\n      (similar to TCB1, perhaps with different\n       elements\
    \ or numeric parameters)\n   and the filters, based on each customer's source\
    \ MAC address, could\n   be defined as follows:\n      Filter1:\n      submitted\
    \ +-----+\n      traffic   |    A|--------> TCB1\n      --------->|    B|-------->\
    \ TCB2\n                |    C|--------> TCB3\n                |    X|------+\
    \   +-----+\n                +-----+      +-->|     |\n                Classifier4\
    \      +-----+\n                                 AbsoluteDrop4\n      Figure 8:\
    \ An Example of a Multi-Customer TCB\n      Type:        MacAddress\n      SrcValue:\
    \    01-02-03-04-05-06 (source MAC address of customer 1)\n      SrcMask:    \
    \ FF-FF-FF-FF-FF-FF\n      DestValue:   00-00-00-00-00-00\n      DestMask:   \
    \ 00-00-00-00-00-00\n      Filter2:\n      (similar to Filter1 but with customer\
    \ 2's source MAC address as\n       SrcValue)\n      Filter3:\n      (similar\
    \ to Filter1 but with customer 3's source MAC address as\n       SrcValue)\n \
    \  In this example, Classifier4 separates traffic submitted from\n   different\
    \ customers based on the source MAC address in submitted\n   packets.  Those packets\
    \ with recognized source MAC addresses are\n   passed to the TCB implementing\
    \ the TCS with the corresponding\n   customer.  Those packets with unrecognized\
    \ source MAC addresses are\n   passed to a dropper.\n   TCB4 has a Classifier\
    \ stage and an Action element stage performing\n   dropping of all unmatched traffic.\n"
- title: 8.4.  TCBs Supporting Microflow-based Services
  contents:
  - "8.4.  TCBs Supporting Microflow-based Services\n   The TCB illustrated above\
    \ describes a configuration that might be\n   suitable for enforcing a SLS at\
    \ a router's ingress.  It assumes that\n   the customer marks its own traffic\
    \ for the appropriate service level.\n   It then limits the rate of aggregate\
    \ traffic submitted at each\n   service level, thereby protecting the resources\
    \ of the Diffserv\n   network.  It does not provide any isolation between the\
    \ customer's\n   individual microflows.\n   A more complex example might be a\
    \ TCB configuration that offers\n   additional functionality to the customer.\
    \  It recognizes individual\n   customer microflows and marks each one independently.\
    \  It also\n   isolates the customer's individual microflows from each other in\n\
    \   order to prevent a single microflow from seizing an unfair share of\n   the\
    \ resources available to the customer at a certain service level.\n   This is\
    \ illustrated in Figure 9.\n   Suppose that the customer has an SLS which specifies\
    \ 2 service\n   levels, to be identified to the provider by DSCP A and DSCP B.\n\
    \   Traffic is first directed to a MF classifier which classifies traffic\n  \
    \ based on miscellaneous classification criteria, to a granularity\n   sufficient\
    \ to identify individual customer microflows.  Each\n   microflow can then be\
    \ marked for a specific DSCP The metering\n   elements limit the contribution\
    \ of each of the customer's microflows\n   to the service level for which it was\
    \ marked.  Packets exceeding the\n   allowable limit for the microflow are dropped.\n\
    \                     +-----+   +-----+\n    Classifier1      |     |   |    \
    \ |---------------+\n        (MF)      +->|     |-->|     |     +-----+   |\n\
    \      +-----+     |  |     |   |     |---->|     |   |\n      |    A|------ \
    \ +-----+   +-----+     +-----+   |\n   -->|    B|-----+  Marker1   Meter1   \
    \   Absolute  |\n      |    C|---+ |                        Dropper1  |   +-----+\n\
    \      |    X|-+ | |  +-----+   +-----+               +-->|A    |\n      +-----+\
    \ | | |  |     |   |     |------------------>|B    |--->\n              | | +->|\
    \     |-->|     |     +-----+   +-->|C    | to TCB2\n              | |    |  \
    \   |   |     |---->|     |   |   +-----+\n              | |    +-----+   +-----+\
    \     +-----+   |    Mux1\n              | |    Marker2   Meter2      Absolute\
    \  |\n              | |                          Dropper2  |\n              |\
    \ |    +-----+   +-----+               |\n              | |    |     |   |   \
    \  |---------------+\n              | |--->|     |-->|     |     +-----+\n   \
    \           |      |     |   |     |---->|     |\n              |      +-----+\
    \   +-----+     +-----+\n              |      Marker3   Meter3      Absolute\n\
    \              |                            Dropper3\n              V etc.\n \
    \     Figure 9: An Example of a Marking and Traffic Isolation TCB\n   This TCB\
    \ could be formally specified as follows:\n      TCB1:\n      Classifier1: (MF)\n\
    \      FilterA:             Marker1\n      FilterB:             Marker2\n    \
    \  FilterC:             Marker3\n      etc.\n      Marker1:\n      Output:   \
    \           Meter1\n      Marker2:\n      Output:              Meter2\n      Marker3:\n\
    \      Output:              Meter3\n      Meter1:\n      ConformingOutput:   \
    \ Mux1.InputA\n      NonConformingOutput: AbsoluteDropper1\n      Meter2:\n  \
    \    ConformingOutput:    Mux1.InputB\n      NonConformingOutput: AbsoluteDropper2\n\
    \      Meter3:\n      ConformingOutput:    Mux1.InputC\n      NonConformingOutput:\
    \ AbsoluteDropper3\n      etc.\n      Mux1:\n      Output:              to TCB2\n\
    \   Note that the detailed traffic element declarations are not shown\n   here.\
    \  Traffic is either dropped by TCB1 or emerges marked for one of\n   two DSCPs.\
    \  This traffic is then passed to TCB2 which is illustrated\n   in Figure 10.\n\
    \   TCB2 could then be specified as follows:\n      Classifier2: (BA)\n      FilterA:\
    \               Meter5\n      FilterB:               Meter6\n                \
    \     +-----+\n                     |     |---------------> to Queue1\n      \
    \            +->|     |     +-----+\n        +-----+   |  |     |---->|     |\n\
    \        |    A|---+  +-----+     +-----+\n      ->|     |       Meter5     AbsoluteDropper4\n\
    \        |    B|---+  +-----+\n        +-----+   |  |     |---------------> to\
    \ Queue2\n      Classifier2 +->|     |     +-----+\n         (BA)        |   \
    \  |---->|     |\n                     +-----+     +-----+\n                 \
    \     Meter6     AbsoluteDropper5\n      Figure 10: Additional Example: TCB2\n\
    \      Meter5:\n      ConformingOutput:      Queue1\n      NonConformingOutput:\
    \   AbsoluteDropper4\n      Meter6:\n      ConformingOutput:      Queue2\n   \
    \   NonConformingOutput:   AbsoluteDropper5\n"
- title: 8.5.  Cascaded TCBs
  contents:
  - "8.5.  Cascaded TCBs\n   Nothing in this model prevents more complex scenarios\
    \ in which one\n   microflow TCB precedes another (e.g., for TCBs implementing\
    \ separate\n   TCSs for the source and for a set of destinations).\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   Security vulnerabilities of Diffserv network\
    \ operation are discussed\n   in [DSARCH].  This document describes an abstract\
    \ functional model of\n   Diffserv router elements.  Certain denial-of-service\
    \ attacks such as\n   those resulting from resource starvation may be mitigated\
    \ by\n   appropriate configuration of these router elements; for example, by\n\
    \   rate limiting certain traffic streams or by authenticating traffic\n   marked\
    \ for higher quality-of-service.\n   There may be theft-of-service scenarios where\
    \ a malicious host can\n   exploit a loose token bucket policer to obtain slightly\
    \ better QoS\n   than that committed in the TCS.\n"
- title: 10.  Acknowledgments
  contents:
  - "10.  Acknowledgments\n   Concepts, terminology, and text have been borrowed liberally\
    \ from\n   [POLTERM], as well as from other IETF work on MIBs and policy-\n  \
    \ management.  We wish to thank the authors of some of those documents:\n   Fred\
    \ Baker, Michael Fine, Keith McCloghrie, John Seligson, Kwok Chan,\n   Scott Hahn,\
    \ and Andrea Westerinen for their contributions.\n   This document has benefited\
    \ from the comments and suggestions of\n   several participants of the Diffserv\
    \ working group, particularly\n   Shahram Davari, John Strassner, and Walter Weiss.\
    \  This document\n   could never have reached this level of rough consensus without\
    \ the\n   relentless pressure of the co-chairs Brian Carpenter and Kathie\n  \
    \ Nichols, for which the authors are grateful.\n"
- title: 11.  References
  contents:
  - "11.  References\n   [AF-PHB]    Heinanen, J., Baker, F., Weiss, W. and J. Wroclawski,\n\
    \               \"Assured Forwarding PHB Group\", RFC 2597, June 1999.\n   [DSARCH]\
    \    Carlson, M., Weiss, W., Blake, S., Wang, Z., Black, D.\n               and\
    \ E. Davies, \"An Architecture for Differentiated\n               Services\",\
    \ RFC 2475, December 1998.\n   [DSFIELD]   Nichols, K., Blake, S., Baker, F. and\
    \ D. Black,\n               \"Definition of the Differentiated Services Field\
    \ (DS\n               Field) in the IPv4 and IPv6 Headers\", RFC 2474, December\n\
    \               1998.\n   [DSMIB]     Baker, F., Smith, A., and K. Chan, \"Management\n\
    \               Information Base for the Differentiated Services\n           \
    \    Architecture\", RFC 3289, May 2002.\n   [E2E]       Bernet, Y., Yavatkar,\
    \ R., Ford, P., Baker, F., Zhang, L.,\n               Speer, M., Nichols, K.,\
    \ Braden, R., Davie, B.,\n               Wroclawski, J. and E. Felstaine, \"A\
    \ Framework for\n               Integrated Services Operation over Diffserv Networks\"\
    ,\n               RFC 2998, November 2000.\n   [EF-PHB]    Davie, B., Charny,\
    \ A., Bennett, J.C.R., Benson, K., Le\n               Boudec, J.Y., Courtney,\
    \ W., Davari, S., Firoiu, V. and D.\n               Stiliadis, \"An Expedited\
    \ Forwarding PHB (Per-Hop\n               Behavior)\", RFC 3246, March 2002.\n\
    \   [FJ95]      Floyd, S. and V. Jacobson, \"Link Sharing and Resource\n     \
    \          Management Models for Packet Networks\", IEEE/ACM\n               Transactions\
    \ on Networking, Vol. 3 No. 4, August 1995l.\n   [INTSERV]   Braden, R., Clark,\
    \ D. and S. Shenker, \"Integrated\n               Services in the Internet Architecture:\
    \ an Overview\", RFC\n               1633, June 1994.\n   [NEWTERMS]  Grossman,\
    \ D., \"New Terminology and Clarifications for\n               Diffserv\", RFC\
    \ 3260, April, 2002\n   [PDBDEF]    K. Nichols and B. Carpenter, \"Definition\
    \ of\n               Differentiated Services Per Domain Behaviors and Rules\n\
    \               for Their Specification\", RFC 3086, April 2001.\n   [POLTERM]\
    \   Westerinen, A., Schnizlein, J., Strassner, J., Scherling,\n              \
    \ M., Quinn, B., Herzog, S., Huynh, A., Carlson, M., Perry,\n               J.\
    \ and S. Waldbusser, \"Policy Terminology\", RFC 3198,\n               November\
    \ 2001.\n   [QOSDEVMOD] Strassner, J., Westerinen, A. and B. Moore, \"Information\n\
    \               Model for Describing Network Device QoS Mechanisms\", Work\n \
    \              in Progress.\n   [QUEUEMGMT] Braden, R., Clark, D., Crowcroft,\
    \ J., Davie, B., Deering,\n               S., Estrin, D., Floyd, S., Jacobson,\
    \ V., Minshall, C.,\n               Partridge, C., Peterson, L., Ramakrishnan,\
    \ K., Shenker,\n               S., Wroclawski, J. and L. Zhang, \"Recommendations\
    \ on\n               Queue Management and Congestion Avoidance in the\n      \
    \         Internet\", RFC 2309, April 1998.\n   [SRTCM]     Heinanen, J. and R.\
    \ Guerin, \"A Single Rate Three Color\n               Marker\", RFC 2697, September\
    \ 1999.\n   [TRTCM]     Heinanen, J. and R. Guerin, \"A Two Rate Three Color\n\
    \               Marker\", RFC 2698, September 1999.\n   [VIC]       McCanne, S.\
    \ and Jacobson, V., \"vic: A Flexible Framework\n               for Packet Video\"\
    , ACM Multimedia '95, November 1995, San\n               Francisco, CA, pp. 511-522.\n\
    \               <ftp://ftp.ee.lbl.gov/papers/vic-mm95.ps.Z>\n   [802.1D]   \"\
    Information technology - Telecommunications and\n               information exchange\
    \ between systems - Local and\n               metropolitan area networks - Common\
    \ specifications - Part\n               3: Media Access Control (MAC) Bridges:\
    \  Revision.  This\n               is a revision of ISO/IEC 10038: 1993, 802.1j-1992\
    \ and\n               802.6k-1992.  It incorporates P802.11c, P802.1p and\n  \
    \             P802.12e.\", ISO/IEC 15802-3: 1998.\n"
- title: Appendix A. Discussion of Token Buckets and Leaky Buckets
  contents:
  - "Appendix A. Discussion of Token Buckets and Leaky Buckets\n   \"Leaky bucket\"\
    \ and/or \"Token Bucket\" models are used to describe rate\n   control in several\
    \ architectures, including Frame Relay, ATM,\n   Integrated Services and Differentiated\
    \ Services.  Both of these\n   models are, by definition, theoretical relationships\
    \ between some\n   defined burst size, B, a rate, R, and a time interval, t:\n\
    \                  R = B/t\n   Thus, a token bucket or leaky bucket might specify\
    \ an information\n   rate of 1.2 Mbps with a burst size of 1500 bytes.  In this\
    \ case, the\n   token rate is 1,200,000 bits per second, the token burst is 12,000\n\
    \   bits and the token interval is 10 milliseconds.  The specification\n   says\
    \ that conforming traffic will, in the worst case, come in 100\n   bursts per\
    \ second of 1500 bytes each and at an average rate not\n   exceeding 1.2 Mbps.\n"
- title: A.1 Leaky Buckets
  contents:
  - "A.1 Leaky Buckets\n   A leaky bucket algorithm is primarily used for shaping\
    \ traffic as it\n   leaves an interface onto the network (handled under Queues\
    \ and\n   Schedulers in this model).  Traffic theoretically departs from an\n\
    \   interface at a rate of one bit every so many time units (in the\n   example,\
    \ one bit every 0.83 microseconds) but, in fact, departs in\n   multi-bit units\
    \ (packets) at a rate approximating the theoretical, as\n   measured over a longer\
    \ interval.  In the example, it might send one\n   1500 byte packet every 10 ms\
    \ or perhaps one 500 byte packet every 3.3\n   ms.  It is also possible to build\
    \ multi-rate leaky buckets in which\n   traffic departs from the interface at\
    \ varying rates depending on\n   recent activity or inactivity.\n   Implementations\
    \ generally seek as constant a transmission rate as\n   achievable.  In theory,\
    \ a 10 Mbps shaped transmission stream from an\n   algorithmic implementation\
    \ and a stream which is running at 10 Mbps\n   because its bottleneck link has\
    \ been a 10 Mbps Ethernet link should\n   be indistinguishable.  Depending on\
    \ configuration, the approximation\n   to theoretical smoothness may vary by moving\
    \ as much as an MTU from\n   one token interval to another.  Traffic may also\
    \ be jostled by other\n   traffic competing for the same transmission resources.\n"
- title: A.2 Token Buckets
  contents:
  - "A.2 Token Buckets\n   A token bucket, on the other hand, measures the arrival\
    \ rate of\n   traffic from another device.  This traffic may originally have been\n\
    \   shaped using a leaky bucket shaper or its equivalent.  The token\n   bucket\
    \ determines whether the traffic (still) conforms to the\n   specification.  Multi-rate\
    \ token buckets (e.g., token buckets with\n   both a peak rate and a mean rate,\
    \ and sometimes more) are commonly\n   used, such as those described in [SRTCM]\
    \ and [TRTCM].  In this case,\n   absolute smoothness is not expected, but conformance\
    \ to one or more\n   of the specified rates is.\n   Simplistically, a data stream\
    \ is said to conform to a simple token\n   bucket parameterized by a {R, B} if\
    \ the system receives in any time\n   interval, t, at most, an amount of data\
    \ not exceeding (R * t) + B.\n   For a multi-rate token bucket case, the data\
    \ stream is said to\n   conform if, for each of the rates, the stream conforms\
    \ to the token-\n   bucket profile appropriate for traffic of that class.  For\
    \ example,\n   received traffic that arrives pre-classified as one of the \"excess\"\
    \n   rates (e.g., AF12 or AF13 traffic for a device implementing the AF1x\n  \
    \ PHB) is only compared to the relevant \"excess\" token bucket profile.\n"
- title: A.3 Some Consequences
  contents:
  - "A.3 Some Consequences\n   The fact that Internet Protocol data is organized into\
    \ variable\n   length packets introduces some uncertainty in the conformance\n\
    \   decision made by any downstream Meter that is attempting to determine\n  \
    \ conformance to a traffic profile that is theoretically designed for\n   fixed-length\
    \ units of data.\n   When used as a leaky bucket shaper, the above definition\
    \ interacts\n   with clock granularity in ways one might not expect.  A leaky\
    \ bucket\n   releases a packet only when all of its bits would have been allowed:\n\
    \   it does not borrow from future capacity.  If the clock is very fine\n   grain,\
    \ on the order of the bit rate or faster, this is not an issue.\n   But if the\
    \ clock is relatively slow (and millisecond or multi-\n   millisecond clocks are\
    \ not unusual in networking equipment), this can\n   introduce jitter to the shaped\
    \ stream.\n   This leaves an implementor of a token bucket Meter with a dilemma.\n\
    \   When the number of bandwidth tokens, b, left in the token bucket is\n   positive\
    \ but less than the size of the packet being operated on, L,\n   one of three\
    \ actions can be performed:\n      (1)   The whole size of the packet can be subtracted\
    \ from the\n            bucket, leaving it negative, remembering that, when new\n\
    \            tokens are next added to the bucket, the new token\n            allocation,\
    \ B, must be added to b rather than simply setting\n            the bucket to\
    \ \"full\".  This option potentially puts more\n            than the desired burst\
    \ size of data into this token bucket\n            interval and correspondingly\
    \ less into the next.  It does,\n            however, keep the average amount\
    \ accepted per token bucket\n            interval equal to the token burst.  This\
    \ approach accepts\n            traffic if any one bit in the packet would have\
    \ been\n            accepted and borrows up to one MTU of capacity from one or\n\
    \            more subsequent intervals when necessary.  Such a token\n       \
    \     bucket meter implementation is said to offer \"loose\"\n            conformance\
    \ to the token bucket.\n      (2)   Alternatively, the packet can be rejected\
    \ and the amount of\n            tokens in the bucket left unchanged (and maybe\
    \ an attempt\n            could be made to accept the packet under another threshold\n\
    \            in another bucket), remembering that, when new tokens are\n     \
    \       next added to the bucket, the new token allocation, B, must\n        \
    \    be added to b rather than simply setting the bucket to\n            \"full\"\
    .  This potentially puts less than the permissible\n            burst size of\
    \ data into this token bucket interval and\n            correspondingly more into\
    \ the next.  Like the first option,\n            it keeps the average amount accepted\
    \ per token bucket\n            interval equal to the token burst.  This approach\
    \ accepts\n            traffic only if every bit in the packet would have been\n\
    \            accepted and borrows up to one MTU of capacity from one or\n    \
    \        more previous intervals when necessary.  Such a token bucket\n      \
    \      meter implementation is said to offer \"strict\" (or perhaps\n        \
    \    \"stricter\") conformance to the token bucket.  This option is\n        \
    \    consistent with [SRTCM] and [TRTCM] and is often used in ATM\n          \
    \  and frame-relay implementations.\n      (3)   The TB variable can be set to\
    \ zero to account for the first\n            part of the packet and the remainder\
    \ of the packet size can\n            be taken out of the next-colored bucket.\
    \  This, of course,\n            has another bug:  the same packet cannot have\
    \ both\n            conforming and non-conforming components in the Diffserv\n\
    \            architecture and so is not really appropriate here and we do\n  \
    \          not discuss this option further here.\n            Unfortunately, the\
    \ thing that cannot be done is exactly to\n            fit the token burst specification\
    \ with random sized packets:\n            therefore token buckets in a variable\
    \ length packet\n            environment always have a some variance from theoretical\n\
    \            reality.  This has also been observed in the ATM Guaranteed\n   \
    \         Frame Rate (GFR) service category specification and Frame\n        \
    \    Relay.  A number of observations may be made:\n   o  Operationally, a token\
    \ bucket meter is reasonable for traffic\n      which has been shaped by a leaky\
    \ bucket shaper or a serial line.\n      However, traffic in the Internet is rarely\
    \ shaped in that way: TCP\n      applies no shaping to its traffic, but rather\
    \ depends on longer-\n      range ACK-clocking behavior to help it approximate\
    \ a certain rate\n      and explicitly sends traffic bursts during slow start,\n\
    \      retransmission, and fast recovery.  Video-on-IP implementations\n     \
    \ such as [VIC] may have a leaky bucket shaper available to them,\n      but often\
    \ do not, and simply enqueue the output of their codec for\n      transmission\
    \ on the appropriate interface.  As a result, in each\n      of these cases, a\
    \ token bucket meter may reject traffic in the\n      short term (over a single\
    \ token interval) which it would have\n      accepted if it had a longer time\
    \ in view and which it needs to\n      accept for the application to work properly.\
    \  To work around this,\n      the token interval, B/R, must approximate or exceed\
    \ the RTT of the\n      session(s) in question and the burst size, B, must accommodate\
    \ the\n      largest burst that the originator might send.\n   o  The behavior\
    \ of a loose token bucket is significantly different\n      from the token bucket\
    \ description for ATM and for Frame Relay.\n   o  A loose token bucket does not\
    \ accept packets while the token count\n      is negative.  This means that, when\
    \ a large packet has just\n      borrowed tokens from the future, even a small\
    \ incoming packet\n      (e.g., a 40-byte TCP ACK/SYN) will not be accepted. \
    \ Therefore, if\n      such a loose token bucket is configured with a burst size\
    \ close to\n      the MTU, some discrimination against smaller packets can take\n\
    \      place: use of a larger burst size avoids this problem.\n   o  The converse\
    \ of the above is that a strict token bucket sometimes\n      does not accept\
    \ large packets when a loose one would do so.\n      Therefore, if such a strict\
    \ token bucket is configured with a\n      burst size close to the MTU, some discrimination\
    \ against larger\n      packets can take place: use of a larger burst size avoids\
    \ this\n      problem.\n   o  In real-world deployments, MTUs are often larger\
    \ than the burst\n      size offered by a link-layer network service provider.\
    \  If so then\n      it is possible that a strict token bucket meter would find\
    \ that\n      traffic never matches the specified profile: this may be avoided\n\
    \      by not allowing such a specification to be used.  This situation\n    \
    \  cannot arise with a loose token bucket since the smallest burst\n      size\
    \ that can be configured is 1 bit, by definition limiting a\n      loose token\
    \ bucket to having a burst size of greater than one MTU.\n   o  Both strict token\
    \ bucket specifications, as specified in [SRTCM]\n      and [TRTCM], and loose\
    \ ones, are subject to a persistent under-\n      run.  These accumulate burst\
    \ capacity over time, up to the maximum\n      burst size.  Suppose that the maximum\
    \ burst size is exactly the\n      size of the packets being sent - which one\
    \ might call the\n      \"strictest\" token bucket implementation.  In such a\
    \ case, when one\n      packet has been accepted, the token depth becomes zero\
    \ and starts\n      to accumulate again.  If the next packet is received any time\n\
    \      earlier than a token interval later, it will not be accepted.  If\n   \
    \   the next packet arrives exactly on time, it will be accepted and\n      the\
    \ token depth again set to zero.  If it arrives later, however,\n      accumulation\
    \ of tokens will have stopped because it is capped by\n      the maximum burst\
    \ size: during the interval between the bucket\n      becoming full and the actual\
    \ arrival of the packet, no new tokens\n      are added.  As a result, jitter\
    \ that accumulates across multiple\n      hops in the network conspires against\
    \ the algorithm to reduce the\n      actual acceptance rate.  Thus it usually\
    \ makes sense to set the\n      maximum token bucket size somewhat greater than\
    \ the MTU in order\n      to absorb some of the jitter and allow a practical acceptance\
    \ rate\n      more in line with the desired theoretical rate.\n"
- title: A.4 Mathematical Definition of Strict Token Bucket Conformance
  contents:
  - "A.4 Mathematical Definition of Strict Token Bucket Conformance\n   The strict\
    \ token bucket conformance behavior defined in [SRTCM] and\n   [TRTCM] is not\
    \ mandatory for compliance with any current Diffserv\n   standards, but we give\
    \ here a mathematical definition of two-\n   parameter token bucket operation\
    \ which is consistent with those\n   documents and which can also be used to define\
    \ a shaping profile.\n   Define a token bucket with bucket size B, token accumulation\
    \ rate R\n   and instantaneous token occupancy b(t).  Assume that b(0) = B.  Then\n\
    \   after an arbitrary interval with no packet arrivals, b(t) will not\n   change\
    \ since the bucket is already full of tokens.\n   Assume a packet of size L bytes\
    \ arrives at time t'.  The bucket\n   occupancy is still B.  Then, as long as\
    \ L <= B, the packet conforms\n   to the meter, and afterwards\n             \
    \     b(t') = B - L.\n   Assume now an interval delta_t = t - t' elapses before\
    \ the next\n   packet arrives, of size L' <= B.  Just before this, at time t-,\
    \ the\n   bucket has accumulated delta_t*R tokens over the interval, up to a\n\
    \   maximum of B tokens so that:\n                  b(t-) = min{ B, b(t') + delta_t*R\
    \ }\n   For a strict token bucket, the conformance test is as follows:\n     \
    \ if (b(t-) - L' >= 0) {\n          /* the packet conforms */\n          b(t)\
    \ = b(t-) - L';\n      }\n      else {\n          /* the packet does not conform\
    \ */\n          b(t) = b(t-);\n      }\n   This function can also be used to define\
    \ a shaping profile.  If a\n   packet of size L arrives at time t, it will be\
    \ eligible for\n   transmission at time te given as follows (we still assume L\
    \ <= B):\n                  te = max{ t, t\" }\n   where t\" = (L - b(t') + t'*R)\
    \ / R and b(t\") = L, the time when L\n   credits have accumulated in the bucket,\
    \ and when the packet would\n   conform if the token bucket were a meter. te !=\
    \ t\" only if t > t\".\n   A mathematical definition along these lines for loose\
    \ token bucket\n   conformance is left as an exercise for the reader.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Yoram Bernet\n   Microsoft\n   One Microsoft Way\n   Redmond,\
    \ WA  98052\n   Phone:  +1 425 936 9568\n   EMail: ybernet@msn.com\n   Steven\
    \ Blake\n   Ericsson\n   920 Main Campus Drive, Suite 500\n   Raleigh, NC  27606\n\
    \   Phone:  +1 919 472 9913\n   EMail: steven.blake@ericsson.com\n   Daniel Grossman\n\
    \   Motorola Inc.\n   20 Cabot Blvd.\n   Mansfield, MA  02048\n   Phone:  +1 508\
    \ 261 5312\n   EMail: dan@dma.isg.mot.com\n   Andrew Smith (editor)\n   Harbour\
    \ Networks\n   Jiuling Building\n   21 North Xisanhuan Ave.\n   Beijing, 100089\n\
    \   PRC\n   Fax: +1 415 345 1827\n   EMail: ah_smith@acm.org\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The Internet Society (2002).  All\
    \ Rights Reserved.\n   This document and translations of it may be copied and\
    \ furnished to\n   others, and derivative works that comment on or otherwise explain\
    \ it\n   or assist in its implementation may be prepared, copied, published\n\
    \   and distributed, in whole or in part, without restriction of any\n   kind,\
    \ provided that the above copyright notice and this paragraph are\n   included\
    \ on all such copies and derivative works.  However, this\n   document itself\
    \ may not be modified in any way, such as by removing\n   the copyright notice\
    \ or references to the Internet Society or other\n   Internet organizations, except\
    \ as needed for the purpose of\n   developing Internet standards in which case\
    \ the procedures for\n   copyrights defined in the Internet Standards process\
    \ must be\n   followed, or as required to translate it into languages other than\n\
    \   English.\n   The limited permissions granted above are perpetual and will\
    \ not be\n   revoked by the Internet Society or its successors or assigns.\n \
    \  This document and the information contained herein is provided on an\n   \"\
    AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE\
    \ DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED\
    \ TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE\
    \ ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A\
    \ PARTICULAR PURPOSE.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
