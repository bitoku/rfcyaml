- title: __initial_text__
  contents:
  - '                  Distributed Node Consensus Protocol

    '
- title: Abstract
  contents:
  - "Abstract\n   This document describes the Distributed Node Consensus Protocol\n\
    \   (DNCP), a generic state synchronization protocol that uses the\n   Trickle\
    \ algorithm and hash trees.  DNCP is an abstract protocol and\n   must be combined\
    \ with a specific profile to make a complete\n   implementable protocol.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This\
    \ document is a product of the Internet Engineering Task Force\n   (IETF).  It\
    \ represents the consensus of the IETF community.  It has\n   received public\
    \ review and has been approved for publication by the\n   Internet Engineering\
    \ Steering Group (IESG).  Further information on\n   Internet Standards is available\
    \ in Section 2 of RFC 5741.\n   Information about the current status of this document,\
    \ any errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc7787.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2016 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n     1.1.  Applicability . . . . . . . . . . . . . . . . . .\
    \ . . . .   4\n   2.  Terminology . . . . . . . . . . . . . . . . . . . . . .\
    \ . . .   6\n     2.1.  Requirements Language . . . . . . . . . . . . . . . .\
    \ . .   8\n   3.  Overview  . . . . . . . . . . . . . . . . . . . . . . . . .\
    \ .   8\n   4.  Operation . . . . . . . . . . . . . . . . . . . . . . . . . .\
    \   9\n     4.1.  Hash Tree . . . . . . . . . . . . . . . . . . . . . . . .  \
    \ 9\n       4.1.1.  Calculating Network State and Node Data Hashes  . . .  10\n\
    \       4.1.2.  Updating Network State and Node Data Hashes . . . . .  10\n  \
    \   4.2.  Data Transport  . . . . . . . . . . . . . . . . . . . . .  10\n    \
    \ 4.3.  Trickle-Driven Status Updates . . . . . . . . . . . . . .  12\n     4.4.\
    \  Processing of Received TLVs . . . . . . . . . . . . . . .  13\n     4.5.  Discovering,\
    \ Adding, and Removing Peers . . . . . . . . .  15\n     4.6.  Data Liveliness\
    \ Validation  . . . . . . . . . . . . . . .  16\n   5.  Data Model  . . . . .\
    \ . . . . . . . . . . . . . . . . . . . .  17\n   6.  Optional Extensions . .\
    \ . . . . . . . . . . . . . . . . . . .  19\n     6.1.  Keep-Alives . . . . .\
    \ . . . . . . . . . . . . . . . . . .  19\n       6.1.1.  Data Model Additions\
    \  . . . . . . . . . . . . . . . .  20\n       6.1.2.  Per-Endpoint Periodic Keep-Alives\
    \ . . . . . . . . . .  20\n       6.1.3.  Per-Peer Periodic Keep-Alives . . .\
    \ . . . . . . . . .  20\n       6.1.4.  Received TLV Processing Additions . .\
    \ . . . . . . . .  21\n       6.1.5.  Peer Removal  . . . . . . . . . . . . .\
    \ . . . . . . .  21\n     6.2.  Support for Dense Multicast-Enabled Links . .\
    \ . . . . . .  21\n   7.  Type-Length-Value Objects . . . . . . . . . . . . .\
    \ . . . . .  22\n     7.1.  Request TLVs  . . . . . . . . . . . . . . . . . .\
    \ . . . .  23\n       7.1.1.  Request Network State TLV . . . . . . . . . . .\
    \ . . .  23\n       7.1.2.  Request Node State TLV  . . . . . . . . . . . . .\
    \ . .  24\n     7.2.  Data TLVs . . . . . . . . . . . . . . . . . . . . . . .\
    \ .  24\n       7.2.1.  Node Endpoint TLV . . . . . . . . . . . . . . . . . .\
    \  24\n       7.2.2.  Network State TLV . . . . . . . . . . . . . . . . . .  25\n\
    \       7.2.3.  Node State TLV  . . . . . . . . . . . . . . . . . . .  25\n  \
    \   7.3.  Data TLVs within Node State TLV . . . . . . . . . . . . .  26\n    \
    \   7.3.1.  Peer TLV  . . . . . . . . . . . . . . . . . . . . . .  26\n      \
    \ 7.3.2.  Keep-Alive Interval TLV . . . . . . . . . . . . . . .  27\n   8.  Security\
    \ and Trust Management . . . . . . . . . . . . . . . .  27\n     8.1.  Trust Method\
    \ Based on Pre-Shared Key  . . . . . . . . . .  27\n     8.2.  PKI-Based Trust\
    \ Method  . . . . . . . . . . . . . . . . .  28\n     8.3.  Certificate-Based\
    \ Trust Consensus Method  . . . . . . . .  28\n       8.3.1.  Trust Verdicts \
    \ . . . . . . . . . . . . . . . . . . .  28\n       8.3.2.  Trust Cache . . .\
    \ . . . . . . . . . . . . . . . . . .  29\n       8.3.3.  Announcement of Verdicts\
    \  . . . . . . . . . . . . . .  30\n       8.3.4.  Bootstrap Ceremonies  . . .\
    \ . . . . . . . . . . . . .  31\n   9.  DNCP Profile-Specific Definitions . .\
    \ . . . . . . . . . . . .  32\n   10. Security Considerations . . . . . . . .\
    \ . . . . . . . . . . .  34\n   11. IANA Considerations . . . . . . . . . . .\
    \ . . . . . . . . . .  35\n   12. References  . . . . . . . . . . . . . . . .\
    \ . . . . . . . . .  36\n     12.1.  Normative References . . . . . . . . . .\
    \ . . . . . . . .  36\n     12.2.  Informative References . . . . . . . . . .\
    \ . . . . . . .  36\n   Appendix A.  Alternative Modes of Operation . . . . .\
    \ . . . . . .  38\n     A.1.  Read-Only Operation . . . . . . . . . . . . . .\
    \ . . . . .  38\n     A.2.  Forwarding Operation  . . . . . . . . . . . . . .\
    \ . . . .  38\n   Appendix B.  DNCP Profile Additional Guidance . . . . . . .\
    \ . . .  38\n     B.1.  Unicast Transport -- UDP or TCP?  . . . . . . . . . .\
    \ . .  38\n     B.2.  (Optional) Multicast Transport  . . . . . . . . . . . .\
    \ .  39\n     B.3.  (Optional) Transport Security . . . . . . . . . . . . . .\
    \  39\n   Appendix C.  Example Profile  . . . . . . . . . . . . . . . . . .  40\n\
    \   Acknowledgements  . . . . . . . . . . . . . . . . . . . . . . . .  41\n  \
    \ Authors' Addresses  . . . . . . . . . . . . . . . . . . . . . . .  41\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   DNCP is designed to provide a way for each participating\
    \ node to\n   publish a small set of TLV (Type-Length-Value) tuples (at most 64\
    \ KB)\n   and to provide a shared and common view about the data published by\n\
    \   every currently bidirectionally reachable DNCP node in a network.\n   For\
    \ state synchronization, a hash tree is used.  It is formed by\n   first calculating\
    \ a hash for the data set published by each node,\n   called node data, and then\
    \ calculating another hash over those node\n   data hashes.  The single resulting\
    \ hash, called network state hash,\n   is transmitted using the Trickle algorithm\
    \ [RFC6206] to ensure that\n   all nodes share the same view of the current state\
    \ of the published\n   data within the network.  The use of Trickle with only\
    \ short network\n   state hashes sent infrequently (in steady state, once the\
    \ maximum\n   Trickle interval per link or unicast connection has been reached)\n\
    \   makes DNCP very thrifty when updates happen rarely.\n   For maintaining liveliness\
    \ of the topology and the data within it, a\n   combination of Trickled network\
    \ state, keep-alives, and \"other\" means\n   of ensuring reachability are used.\
    \  The core idea is that if every\n   node ensures its peers are present, transitively,\
    \ the whole network\n   state also stays up to date.\n"
- title: 1.1.  Applicability
  contents:
  - "1.1.  Applicability\n   DNCP is useful for cases like autonomous bootstrapping,\
    \ discovery,\n   and negotiation of embedded network devices like routers.\n \
    \  Furthermore, it can be used as a basis to run distributed algorithms\n   like\
    \ [RFC7596] or use cases as described in Appendix C.  DNCP is\n   abstract, which\
    \ allows it to be tuned to a variety of applications by\n   defining profiles.\
    \  These profiles include choices of:\n   -  unicast transport:  a datagram or\
    \ stream-oriented protocol (e.g.,\n      TCP, UDP, or the Stream Control Transmission\
    \ Protocol (SCTP)) for\n      generic protocol operation.\n   -  optional transport\
    \ security:  whether and when to use security\n      based on Transport Layer\
    \ Security (TLS) or Datagram Transport\n      Layer Security (DTLS), if supported\
    \ over the chosen transport.\n   -  optional multicast transport:  a multicast-capable\
    \ protocol like\n      UDP allowing autonomous peer discovery or more efficient\
    \ use of\n      multiple access links.\n   -  communication scopes:  using either\
    \ hop by hop only relying on\n      link-local addressing (e.g., for LANs), addresses\
    \ with broader\n      scopes (e.g., over WANs or the Internet) relying on an existing\n\
    \      routing infrastructure, or a combination of both (e.g., to\n      exchange\
    \ state between multiple LANs over a WAN or the Internet).\n   -  payloads:  additional\
    \ specific payloads (e.g., IANA standardized,\n      enterprise-specific, or private\
    \ use).\n   -  extensions:  possible protocol extensions, either as predefined\
    \ in\n      this document or specific for a particular use case.\n   However,\
    \ there are certain cases where the protocol as defined in\n   this document is\
    \ a less suitable choice.  This list provides an\n   overview while the following\
    \ paragraphs provide more detailed\n   guidance on the individual matters.\n \
    \  -  large amounts of data:  nodes are limited to 64 KB of published\n      data.\n\
    \   -  very dense unicast-only networks:  nodes include information about\n  \
    \    all immediate neighbors as part of their published data.\n   -  predominantly\
    \ minimal data changes:  node data is always\n      transported as is, leading\
    \ to a relatively large transmission\n      overhead for changes affecting only\
    \ a small part of it.\n   -  frequently changing data:  DNCP with its use of Trickle\
    \ is\n      optimized for the steady state and less efficient otherwise.\n   -\
    \  large amounts of very constrained nodes:  DNCP requires each node\n      to\
    \ store the entirety of the data published by all nodes.\n   The topology of the\
    \ devices is not limited and automatically\n   discovered.  When relying on link-local\
    \ communication exclusively,\n   all links having DNCP nodes need to be at least\
    \ transitively\n   connected by routers running the protocol on multiple endpoints\
    \ in\n   order to form a connected network.  However, there is no requirement\n\
    \   for every device in a physical network to run the protocol.\n   Especially\
    \ if globally scoped addresses are used, DNCP peers do not\n   need to be on the\
    \ same or even neighboring physical links.\n   Autonomous discovery features are\
    \ usually used in local network\n   scenarios; however, with security enabled,\
    \ DNCP can also be used over\n   unsecured public networks.  Network size is restricted\
    \ merely by the\n   capabilities of the devices, i.e., each DNCP node needs to\
    \ be able to\n   store the entirety of the data published by all nodes.  The data\n\
    \   associated with each individual node identifier is limited to about\n   64\
    \ KB in this document; however, protocol extensions could be defined\n   to mitigate\
    \ this or other protocol limitations if the need arises.\n   DNCP is most suitable\
    \ for data that changes only infrequently to gain\n   the maximum benefit from\
    \ using Trickle.  As the network of nodes\n   grows, or the frequency of data\
    \ changes per node increases, Trickle\n   is eventually used less and less, and\
    \ the benefit of using DNCP\n   diminishes.  In these cases, Trickle just provides\
    \ extra complexity\n   within the specification and little added value.\n   The\
    \ suitability of DNCP for a particular application can be roughly\n   evaluated\
    \ by considering the expected average network-wide state\n   change interval A_NC_I;\
    \ it is computed by dividing the mean interval\n   at which a node originates\
    \ a new TLV set by the number of\n   participating nodes.  If keep-alives are\
    \ used, A_NC_I is the minimum\n   of the computed A_NC_I and the keep-alive interval.\
    \  If A_NC_I is\n   less than the (application-specific) Trickle minimum interval,\
    \ DNCP\n   is most likely unsuitable for the application as Trickle will not be\n\
    \   utilized most of the time.\n   If constant rapid state changes are needed,\
    \ the preferable choice is\n   to use an additional point-to-point channel whose\
    \ address or locator\n   is published using DNCP.  Nevertheless, if doing so does\
    \ not raise\n   A_NC_I above the (sensibly chosen) Trickle interval parameters\
    \ for a\n   particular application, using DNCP is probably not suitable for the\n\
    \   application.\n   Another consideration is the size of the published TLV set\
    \ by a node\n   compared to the size of deltas in the TLV set.  If the TLV set\n\
    \   published by a node is very large, and has frequent small changes,\n   DNCP\
    \ as currently specified in this specification may be unsuitable\n   as it lacks\
    \ a delta synchronization scheme to keep implementation\n   simple.\n   DNCP can\
    \ be used in networks where only unicast transport is\n   available.  While DNCP\
    \ uses the least amount of bandwidth when\n   multicast is utilized, even in pure\
    \ unicast mode, the use of Trickle\n   (ideally with k < 2) results in a protocol\
    \ with an exponential\n   backoff timer and fewer transmissions than a simpler\
    \ protocol not\n   using Trickle.\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   DNCP profile      the values for the set of parameters given\
    \ in\n                     Section 9.  They are prefixed with DNCP_ in this\n\
    \                     document.  The profile also specifies the set of\n     \
    \                optional DNCP extensions to be used.  For a simple\n        \
    \             example DNCP profile, see Appendix C.\n   DNCP-based        a protocol\
    \ that provides a DNCP profile, according\n   protocol          to Section 9,\
    \ and zero or more TLV assignments from\n                     the per-DNCP profile\
    \ TLV registry as well as their\n                     processing rules.\n   DNCP\
    \ node         a single node that runs a DNCP-based protocol.\n   Link       \
    \       a link-layer media over which directly connected\n                   \
    \  nodes can communicate.\n   DNCP network      a set of DNCP nodes running a\
    \ DNCP-based\n                     protocol(s) with a matching DNCP profile(s).\
    \  The\n                     set consists of nodes that have discovered each\n\
    \                     other using the transport method defined in the\n      \
    \               DNCP profile, via multicast on local links, and/or\n         \
    \            by using unicast communication.\n   Node identifier   an opaque fixed-length\
    \ identifier consisting of\n                     DNCP_NODE_IDENTIFIER_LENGTH bytes\
    \ that uniquely\n                     identifies a DNCP node within a DNCP network.\n\
    \   Interface         a node's attachment to a particular link.\n   Address  \
    \         an identifier used as the source or destination of\n               \
    \      a DNCP message flow, e.g., a tuple (IPv6 address,\n                   \
    \  UDP port) for an IPv6 UDP transport.\n   Endpoint          a locally configured\
    \ termination point for\n                     (potential or established) DNCP\
    \ message flows.  An\n                     endpoint is the source and destination\
    \ for separate\n                     unicast message flows to individual nodes\
    \ and\n                     optionally for multicast messages to all thereby\n\
    \                     reachable nodes (e.g., for node discovery).\n          \
    \           Endpoints are usually in one of the transport modes\n            \
    \         specified in Section 4.2.\n   Endpoint          a 32-bit opaque and\
    \ locally unique value, which\n   identifier        identifies a particular endpoint\
    \ of a particular\n                     DNCP node.  The value 0 is reserved for\
    \ DNCP and\n                     DNCP-based protocol purposes and not used to\n\
    \                     identify an actual endpoint.  This definition is in\n  \
    \                   sync with the interface index definition in\n            \
    \         [RFC3493], as the non-zero small positive integers\n               \
    \      should comfortably fit within 32 bits.\n   Peer              another DNCP\
    \ node with which a DNCP node\n                     communicates using at least\
    \ one particular local\n                     and remote endpoint pair.\n   Node\
    \ data         a set of TLVs published and owned by a node in the\n          \
    \           DNCP network.  Other nodes pass it along as is,\n                \
    \     even if they cannot fully interpret it.\n   Origination time  the (estimated)\
    \ time when the node data set with\n                     the current sequence\
    \ number was published.\n   Node state        a set of metadata attributes for\
    \ node data.  It\n                     includes a sequence number for versioning,\
    \ a hash\n                     value for comparing equality of stored node data,\n\
    \                     and a timestamp indicating the time passed since\n     \
    \                its last publication (i.e., since the origination\n         \
    \            time).  The hash function and the length of the\n               \
    \      hash value are defined in the DNCP profile.\n   Network state     a hash\
    \ value that represents the current state of\n   hash              the network.\
    \  The hash function and the length of\n                     the hash value are\
    \ defined in the DNCP profile.\n                     Whenever a node is added,\
    \ removed, or updates its\n                     published node data, this hash\
    \ value changes as\n                     well.  For calculation, please see Section\
    \ 4.1.\n   Trust verdict     a statement about the trustworthiness of a\n    \
    \                 certificate announced by a node participating in\n         \
    \            the certificate-based trust consensus mechanism.\n   Effective trust\
    \   the trust verdict with the highest priority within\n   verdict           the\
    \ set of trust verdicts announced for the\n                     certificate in\
    \ the DNCP network.\n   Topology graph    the undirected graph of DNCP nodes produced\
    \ by\n                     retaining only bidirectional peer relationships\n \
    \                    between nodes.\n   Bidirectionally   a peer is locally unidirectionally\
    \ reachable if a\n   reachable         consistent multicast or any unicast DNCP\
    \ message\n                     has been received by the local node (see Section\n\
    \                     4.5).  If said peer in return also considers the\n     \
    \                local node unidirectionally reachable, then\n               \
    \      bidirectionally reachability is established.  As\n                    \
    \ this process is based on publishing peer\n                     relationships\
    \ and evaluating the resulting topology\n                     graph as described\
    \ in Section 4.6, this information\n                     is available to the whole\
    \ DNCP network.\n   Trickle instance  a distinct Trickle [RFC6206] algorithm state\
    \ kept\n                     by a node (Section 5) and related to an endpoint\
    \ or\n                     a particular (peer, endpoint) tuple with Trickle\n\
    \                     variables I, t, and c.  See Section 4.3.\n"
- title: 2.1.  Requirements Language
  contents:
  - "2.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    NOT RECOMMENDED\", \"MAY\", and\n   \"OPTIONAL\" in this document are to be interpreted\
    \ as described in RFC\n   2119 [RFC2119].\n"
- title: 3.  Overview
  contents:
  - "3.  Overview\n   DNCP operates primarily using unicast exchanges between nodes,\
    \ and it\n   may use multicast for Trickle-based shared state dissemination and\n\
    \   topology discovery.  If used in pure unicast mode with unreliable\n   transport,\
    \ Trickle is also used between peers.\n   DNCP is based on exchanging TLVs (Section\
    \ 7) and defines a set of\n   mandatory and optional ones for its operation. \
    \ They are categorized\n   into TLVs for requesting information (Section 7.1),\
    \ transmitting data\n   (Section 7.2), and being published as data (Section 7.3).\
    \  DNCP-based\n   protocols usually specify additional ones to extend the capabilities.\n\
    \   DNCP discovers the topology of the nodes in the DNCP network and\n   maintains\
    \ the liveliness of published node data by ensuring that the\n   publishing node\
    \ is bidirectionally reachable.  New potential peers\n   can be discovered autonomously\
    \ on multicast-enabled links; their\n   addresses may be manually configured or\
    \ they may be found by some\n   other means defined in the particular DNCP profile.\
    \  The DNCP profile\n   may specify, for example, a well-known anycast address\
    \ or provision\n   the remote address to contact via some other protocol such\
    \ as DHCPv6\n   [RFC3315].\n   A hash tree of height 1, rooted in itself, is maintained\
    \ by each node\n   to represent the state of all currently reachable nodes (see\n\
    \   Section 4.1), and the Trickle algorithm is used to trigger\n   synchronization\
    \ (see Section 4.3).  The need to check peer nodes for\n   state changes is thereby\
    \ determined by comparing the current root of\n   their respective hash trees,\
    \ i.e., their individually calculated\n   network state hashes.\n   Before joining\
    \ a DNCP network, a node starts with a hash tree that\n   has only one leaf if\
    \ the node publishes some TLVs, and no leaves\n   otherwise.  It then announces\
    \ the network state hash calculated from\n   the hash tree by means of the Trickle\
    \ algorithm on all its configured\n   endpoints.\n   When an update is detected\
    \ by a node (e.g., by receiving a different\n   network state hash from a peer),\
    \ the originator of the event is\n   requested to provide a list of the state\
    \ of all nodes, i.e., all the\n   information it uses to calculate its own hash\
    \ tree.  The node uses\n   the list to determine whether its own information is\
    \ outdated and --\n   if necessary -- requests the actual node data that has changed.\n\
    \   Whenever a node's local copy of any node data and its hash tree are\n   updated\
    \ (e.g., due to its own or another node's node state changing\n   or due to a\
    \ peer being added or removed), its Trickle instances are\n   reset, which eventually\
    \ causes any update to be propagated to all of\n   its peers.\n"
- title: 4.  Operation
  contents:
  - '4.  Operation

    '
- title: 4.1.  Hash Tree
  contents:
  - "4.1.  Hash Tree\n   Each DNCP node maintains an arbitrary width hash tree of\
    \ height 1.\n   The root of the tree represents the overall network state hash\
    \ and is\n   used to determine whether the view of the network of two or more\n\
    \   nodes is consistent and shared.  Each leaf represents one\n   bidirectionally\
    \ reachable DNCP node.  Every time a node is added or\n   removed from the topology\
    \ graph (Section 4.6), it is likewise added\n   or removed as a leaf.  At any\
    \ time, the leaves of the tree are\n   ordered in ascending order of the node\
    \ identifiers of the nodes they\n   represent.\n"
- title: 4.1.1.  Calculating Network State and Node Data Hashes
  contents:
  - "4.1.1.  Calculating Network State and Node Data Hashes\n   The network state\
    \ hash and the node data hashes are calculated using\n   the hash function defined\
    \ in the DNCP profile (Section 9) and\n   truncated to the number of bits specified\
    \ therein.\n   Individual node data hashes are calculated by applying the function\n\
    \   and truncation on the respective node's node data as published in the\n  \
    \ Node State TLV.  Such node data sets are always ordered as defined in\n   Section\
    \ 7.2.3.\n   The network state hash is calculated by applying the function and\n\
    \   truncation on the concatenated network state.  This state is formed\n   by\
    \ first concatenating each node's sequence number (in network byte\n   order)\
    \ with its node data hash to form a per-node datum for each\n   node.  These per-node\
    \ data are then concatenated in ascending order\n   of the respective node's node\
    \ identifier, i.e., in the order that the\n   nodes appear in the hash tree.\n"
- title: 4.1.2.  Updating Network State and Node Data Hashes
  contents:
  - "4.1.2.  Updating Network State and Node Data Hashes\n   The network state hash\
    \ and the node data hashes are updated on-demand\n   and whenever any locally\
    \ stored per-node state changes.  This\n   includes local unidirectional reachability\
    \ encoded in the published\n   Peer TLVs (Section 7.3.1) and -- when combined\
    \ with remote data --\n   results in awareness of bidirectional reachability changes.\n"
- title: 4.2.  Data Transport
  contents:
  - "4.2.  Data Transport\n   DNCP has few requirements for the underlying transport;\
    \ it requires\n   some way of transmitting either a unicast datagram or stream\
    \ data to\n   a peer and, if used in multicast mode, a way of sending multicast\n\
    \   datagrams.  As multicast is used only to identify potential new DNCP\n   nodes\
    \ and to send status messages that merely notify that a unicast\n   exchange should\
    \ be triggered, the multicast transport does not have\n   to be secured.  If unicast\
    \ security is desired and one of the\n   built-in security methods is to be used,\
    \ support for some TLS-derived\n   transport scheme -- such as TLS [RFC5246] on\
    \ top of TCP or DTLS\n   [RFC6347] on top of UDP -- is also required.  They provide\
    \ for\n   integrity protection and confidentiality of the node data, as well as\n\
    \   authentication and authorization using the schemes defined in\n   \"Security\
    \ and Trust Management\" (Section 8).  A specific definition\n   of the transport(s)\
    \ in use and its parameters MUST be provided by the\n   DNCP profile.\n   TLVs\
    \ (Section 7) are sent across the transport as is, and they SHOULD\n   be sent\
    \ together where, e.g., MTU considerations do not recommend\n   sending them in\
    \ multiple batches.  DNCP does not fragment or\n   reassemble TLVs; thus, it MUST\
    \ be ensured that the underlying\n   transport performs these operations should\
    \ they be necessary.  If\n   this document indicates sending one or more TLVs,\
    \ then the sending\n   node does not need to keep track of the packets sent after\
    \ handing\n   them over to the respective transport, i.e., reliable DNCP operation\n\
    \   is ensured merely by the explicitly defined timers and state machines\n  \
    \ such as Trickle (Section 4.3).  TLVs in general are handled\n   individually\
    \ and statelessly (and thus do not need to be sent in any\n   particular order)\
    \ with one exception: To form bidirectional peer\n   relationships, DNCP requires\
    \ identification of the endpoints used for\n   communication.  As bidirectional\
    \ peer relationships are required for\n   validating liveliness of published node\
    \ data as described in\n   Section 4.6, a DNCP node MUST send a Node Endpoint\
    \ TLV\n   (Section 7.2.1).  When it is sent varies, depending on the underlying\n\
    \   transport, but conceptually it should be available whenever\n   processing\
    \ a Network State TLV:\n   o  If using a stream transport, the TLV MUST be sent\
    \ at least once\n      per connection but SHOULD NOT be sent more than once.\n\
    \   o  If using a datagram transport, it MUST be included in every\n      datagram\
    \ that also contains a Network State TLV (Section 7.2.2)\n      and MUST be located\
    \ before any such TLV.  It SHOULD also be\n      included in any other datagram\
    \ to speed up initial peer detection.\n   Given the assorted transport options\
    \ as well as potential endpoint\n   configuration, a DNCP endpoint may be used\
    \ in various transport\n   modes:\n   Unicast:\n      *  If only reliable unicast\
    \ transport is used, Trickle is not used\n         at all.  Whenever the locally\
    \ calculated network state hash\n         changes, a single Network State TLV\
    \ (Section 7.2.2) is sent to\n         every unicast peer.  Additionally, recently\
    \ changed Node State\n         TLVs (Section 7.2.3) MAY be included.\n      *\
    \  If only unreliable unicast transport is used, Trickle state is\n         kept\
    \ per peer, and it is used to send Network State TLVs\n         intermittently,\
    \ as specified in Section 4.3.\n   Multicast+Unicast:  If multicast datagram transport\
    \ is available on\n      an endpoint, Trickle state is only maintained for the\
    \ endpoint as\n      a whole.  It is used to send Network State TLVs periodically,\
    \ as\n      specified in Section 4.3.  Additionally, per-endpoint keep-alives\n\
    \      MAY be defined in the DNCP profile, as specified in Section 6.1.2.\n  \
    \ MulticastListen+Unicast:  Just like unicast, except multicast\n      transmissions\
    \ are listened to in order to detect changes of the\n      highest node identifier.\
    \  This mode is used only if the DNCP\n      profile supports dense multicast-enabled\
    \ link optimization\n      (Section 6.2).\n"
- title: 4.3.  Trickle-Driven Status Updates
  contents:
  - "4.3.  Trickle-Driven Status Updates\n   The Trickle algorithm [RFC6206] is used\
    \ to ensure protocol\n   reliability over unreliable multicast or unicast transports.\
    \  For\n   reliable unicast transports, its actual algorithm is unnecessary and\n\
    \   omitted (Section 4.2).  DNCP maintains multiple Trickle states as\n   defined\
    \ in Section 5.  Each such state can be based on different\n   parameters (see\
    \ below) and is responsible for ensuring that a\n   specific peer or all peers\
    \ on the respective endpoint are regularly\n   provided with the node's current\
    \ locally calculated network state\n   hash for state comparison, i.e., to detect\
    \ potential divergence in\n   the perceived network state.\n   Trickle defines\
    \ 3 parameters: Imin, Imax, and k.  Imin and Imax\n   represent the minimum value\
    \ for I and the maximum number of doublings\n   of Imin, where I is the time interval\
    \ during which at least k Trickle\n   updates must be seen on an endpoint to prevent\
    \ local state\n   transmission.  The actual suggested Trickle algorithm parameters\
    \ are\n   DNCP profile specific, as described in Section 9.\n   The Trickle state\
    \ for all Trickle instances defined in Section 5 is\n   considered inconsistent\
    \ and reset if and only if the locally\n   calculated network state hash changes.\
    \  This occurs either due to a\n   change in the local node's own node data or\
    \ due to the receipt of\n   more recent data from another node as explained in\
    \ Section 4.1.  A\n   node MUST NOT reset its Trickle state merely based on receiving\
    \ a\n   Network State TLV (Section 7.2.2) with a network state hash that is\n\
    \   different from its locally calculated one.\n   Every time a particular Trickle\
    \ instance indicates that an update\n   should be sent, the node MUST send a Network\
    \ State TLV\n   (Section 7.2.2) if and only if:\n   o  the endpoint is in Multicast+Unicast\
    \ transport mode, in which case\n      the TLV MUST be sent over multicast.\n\
    \   o  the endpoint is NOT in Multicast+Unicast transport mode, and the\n    \
    \  unicast transport is unreliable, in which case the TLV MUST be\n      sent\
    \ over unicast.\n   A (sub)set of all Node State TLVs (Section 7.2.3) MAY also\
    \ be\n   included, unless it is defined as undesirable for some reason by the\n\
    \   DNCP profile or to avoid exposure of the node state TLVs by\n   transmitting\
    \ them within insecure multicast when using secure\n   unicast.\n"
- title: 4.4.  Processing of Received TLVs
  contents:
  - "4.4.  Processing of Received TLVs\n   This section describes how received TLVs\
    \ are processed.  The DNCP\n   profile may specify when to ignore particular TLVs,\
    \ e.g., to modify\n   security properties -- see Section 9 for what may be safely\
    \ defined\n   to be ignored in a profile.  Any 'reply' mentioned in the steps\
    \ below\n   denotes the sending of the specified TLV(s) to the originator of the\n\
    \   TLV being processed.  All such replies MUST be sent using unicast.\n   If\
    \ the TLV being replied to was received via multicast and it was\n   sent to a\
    \ multiple access link, the reply MUST be delayed by a random\n   time span in\
    \ [0, Imin/2], to avoid potential simultaneous replies\n   that may cause problems\
    \ on some links, unless specified differently\n   in the DNCP profile.  The sending\
    \ of replies MAY also be rate limited\n   or omitted for a short period of time\
    \ by an implementation.  However,\n   if the TLV is not forbidden by the DNCP\
    \ profile, an implementation\n   MUST reply to retransmissions of the TLV with\
    \ a non-zero probability\n   to avoid starvation, which would break the state\
    \ synchronization.\n   A DNCP node MUST process TLVs received from any valid (e.g.,\n\
    \   correctly scoped) address, as specified by the DNCP profile and the\n   configuration\
    \ of a particular endpoint, whether this address is known\n   to be the address\
    \ of a peer or not.  This provision satisfies the\n   needs of monitoring or other\
    \ host software that needs to discover the\n   DNCP topology without adding to\
    \ the state in the network.\n   Upon receipt of:\n   o  Request Network State\
    \ TLV (Section 7.1.1): The receiver MUST reply\n      with a Network State TLV\
    \ (Section 7.2.2) and a Node State TLV\n      (Section 7.2.3) for each node data\
    \ used to calculate the network\n      state hash.  The Node State TLVs SHOULD\
    \ NOT contain the optional\n      node data part to avoid redundant transmission\
    \ of node data,\n      unless explicitly specified in the DNCP profile.\n   o\
    \  Request Node State TLV (Section 7.1.2): If the receiver has node\n      data\
    \ for the corresponding node, it MUST reply with a Node State\n      TLV (Section\
    \ 7.2.3) for the corresponding node.  The optional node\n      data part MUST\
    \ be included in the TLV.\n   o  Network State TLV (Section 7.2.2): If the network\
    \ state hash\n      differs from the locally calculated network state hash, and\
    \ the\n      receiver is unaware of any particular node state differences with\n\
    \      the sender, the receiver MUST reply with a Request Network State\n    \
    \  TLV (Section 7.1.1).  These replies MUST be rate limited to only\n      at\
    \ most one reply per link per unique network state hash within\n      Imin.  The\
    \ simplest way to ensure this rate limit is a timestamp\n      indicating requests\
    \ and sending at most one Request Network State\n      TLV (Section 7.1.1) per\
    \ Imin.  To facilitate faster state\n      synchronization, if a Request Network\
    \ State TLV is sent in a\n      reply, a local, current Network State TLV MAY\
    \ also be sent.\n   o  Node State TLV (Section 7.2.3):\n      *  If the node identifier\
    \ matches the local node identifier and\n         the TLV has a greater sequence\
    \ number than its current local\n         value, or the same sequence number and\
    \ a different hash, the\n         node SHOULD republish its own node data with\
    \ a sequence number\n         significantly greater than the received one (e.g.,\
    \ 1000) to\n         reclaim the node identifier.  This difference is needed in\n\
    \         order to ensure that it is higher than any potentially\n         lingering\
    \ copies of the node state in the network.  This may\n         occur normally\
    \ once due to the local node restarting and not\n         storing the most recently\
    \ used sequence number.  If this occurs\n         more than once or for nodes\
    \ not republishing their own node\n         data, the DNCP profile MUST provide\
    \ guidance on how to handle\n         these situations as it indicates the existence\
    \ of another\n         active node with the same node identifier.\n      *  If\
    \ the node identifier does not match the local node\n         identifier, and\
    \ one or more of the following conditions are\n         true:\n         +  The\
    \ local information is outdated for the corresponding node\n            (the local\
    \ sequence number is less than that within the\n            TLV).\n         +\
    \  The local information is potentially incorrect (the local\n            sequence\
    \ number matches but the node data hash differs).\n         +  There is no data\
    \ for that node altogether.\n         Then:\n         +  If the TLV contains the\
    \ Node Data field, it SHOULD also be\n            verified by ensuring that the\
    \ locally calculated hash of the\n            node data matches the content of\
    \ the H(Node Data) field\n            within the TLV.  If they differ, the TLV\
    \ SHOULD be ignored\n            and not processed further.\n         +  If the\
    \ TLV does not contain the Node Data field, and the\n            H(Node Data)\
    \ field within the TLV differs from the local\n            node data hash for\
    \ that node (or there is none), the\n            receiver MUST reply with a Request\
    \ Node State TLV\n            (Section 7.1.2) for the corresponding node.\n  \
    \       +  Otherwise, the receiver MUST update its locally stored state\n    \
    \        for that node (node data based on the Node Data field if\n          \
    \  present, sequence number, and relative time) to match the\n            received\
    \ TLV.\n      For comparison purposes of the sequence number, a looping\n    \
    \  comparison function MUST be used to avoid problems in case of\n      overflow.\
    \  The comparison function a < b <=> ((a - b) % (2^32)) &\n      (2^31) != 0 where\
    \ (a % b) represents the remainder of a modulo b\n      and (a & b) represents\
    \ bitwise conjunction of a and b is\n      RECOMMENDED unless the DNCP profile\
    \ defines another.\n   o  Any other TLV: TLVs not recognized by the receiver MUST\
    \ be\n      silently ignored unless they are sent within another TLV (for\n  \
    \    example, TLVs within the Node Data field of a Node State TLV).\n      TLVs\
    \ within the Node Data field of the Node State TLV not\n      recognized by the\
    \ receiver MUST be retained for distribution to\n      other nodes and for calculation\
    \ of the node data hash as described\n      in Section 7.2.3 but are ignored for\
    \ other purposes.\n   If secure unicast transport is configured for an endpoint,\
    \ any Node\n   State TLVs received over insecure multicast MUST be silently ignored.\n"
- title: 4.5.  Discovering, Adding, and Removing Peers
  contents:
  - "4.5.  Discovering, Adding, and Removing Peers\n   Peer relations are established\
    \ between neighbors using one or more\n   mutually connected endpoints.  Such\
    \ neighbors exchange information\n   about network state and published data directly,\
    \ and through\n   transitivity, this information then propagates throughout the\n\
    \   network.\n   New peers are discovered using the regular unicast or multicast\n\
    \   transport defined in the DNCP profile (Section 9).  This process is\n   not\
    \ distinguished from peer addition, i.e., an unknown peer is simply\n   discovered\
    \ by receiving regular DNCP protocol TLVs from it, and\n   dedicated discovery\
    \ messages or TLVs do not exist.  For unicast-only\n   transports, the individual\
    \ node's transport addresses are\n   preconfigured or obtained using an external\
    \ service discovery\n   protocol.  In the presence of a multicast transport, messages\
    \ from\n   unknown peers are handled in the same way as multicast messages from\n\
    \   peers that are already known; thus, new peers are simply discovered\n   when\
    \ sending their regular DNCP protocol TLVs using multicast.\n   When receiving\
    \ a Node Endpoint TLV (Section 7.2.1) on an endpoint\n   from an unknown peer:\n\
    \   o  If received over unicast, the remote node MUST be added as a peer\n   \
    \   on the endpoint, and a Peer TLV (Section 7.3.1) MUST be created\n      for\
    \ it.\n   o  If received over multicast, the node MAY be sent a (possibly rate-\n\
    \      limited) unicast Request Network State TLV (Section 7.1.1).\n   If keep-alives\
    \ specified in Section 6.1 are NOT sent by the peer\n   (either the DNCP profile\
    \ does not specify the use of keep-alives or\n   the particular peer chooses not\
    \ to send keep-alives), some other\n   existing local transport-specific means\
    \ (such as Ethernet carrier\n   detection or TCP keep-alive) MUST be used to ensure\
    \ its presence.  If\n   the peer does not send keep-alives, and no means to verify\
    \ presence\n   of the peer are available, the peer MUST be considered no longer\n\
    \   present, and it SHOULD NOT be added back as a peer until it starts\n   sending\
    \ keep-alives again.  When the peer is no longer present, the\n   Peer TLV and\
    \ the local DNCP peer state MUST be removed.  DNCP does\n   not define an explicit\
    \ message or TLV for indicating the termination\n   of DNCP operation by the terminating\
    \ node; however, a derived\n   protocol could specify an extension, if the need\
    \ arises.\n   If the local endpoint is in the Multicast-Listen+Unicast transport\n\
    \   mode, a Peer TLV (Section 7.3.1) MUST NOT be published for the peers\n   not\
    \ having the highest node identifier.\n"
- title: 4.6.  Data Liveliness Validation
  contents:
  - "4.6.  Data Liveliness Validation\n   Maintenance of the hash tree (Section 4.1)\
    \ and thereby network state\n   hash updates depend on up-to-date information\
    \ on bidirectional node\n   reachability derived from the contents of a topology\
    \ graph.  This\n   graph changes whenever nodes are added to or removed from the\
    \ network\n   or when bidirectional connectivity between existing nodes is\n \
    \  established or lost.  Therefore, the graph MUST be updated either\n   immediately\
    \ or with a small delay shorter than the DNCP profile-\n   defined Trickle Imin\
    \ whenever:\n   o  A Peer TLV or a whole node is added or removed, or\n   o  The\
    \ origination time (in milliseconds) of some node's node data is\n      less than\
    \ current time - 2^32 + 2^15.\n   The artificial upper limit for the origination\
    \ time is used to\n   gracefully avoid overflows of the origination time and allow\
    \ for the\n   node to republish its data as noted in Section 7.2.3.\n   The topology\
    \ graph update starts with the local node marked as\n   reachable and all other\
    \ nodes marked as unreachable.  Other nodes are\n   then iteratively marked as\
    \ reachable using the following algorithm: A\n   candidate not-yet-reachable node\
    \ N with an endpoint NE is marked as\n   reachable if there is a reachable node\
    \ R with an endpoint RE that\n   meets all of the following criteria:\n   o  The\
    \ origination time (in milliseconds) of R's node data is greater\n      than current\
    \ time - 2^32 + 2^15.\n   o  R publishes a Peer TLV with:\n      *  Peer Node\
    \ Identifier = N's node identifier\n      *  Peer Endpoint Identifier = NE's endpoint\
    \ identifier\n      *  Endpoint Identifier = RE's endpoint identifier\n   o  N\
    \ publishes a Peer TLV with:\n      *  Peer Node Identifier = R's node identifier\n\
    \      *  Peer Endpoint Identifier = RE's endpoint identifier\n      *  Endpoint\
    \ Identifier = NE's endpoint identifier\n   The algorithm terminates when no more\
    \ candidate nodes fulfilling\n   these criteria can be found.\n   DNCP nodes that\
    \ have not been reachable in the most recent topology\n   graph traversal MUST\
    \ NOT be used for calculation of the network state\n   hash, be provided to any\
    \ applications that need to use the whole TLV\n   graph, or be provided to remote\
    \ nodes.  They MAY be forgotten\n   immediately after the topology graph traversal;\
    \ however, it is\n   RECOMMENDED to keep them at least briefly to improve the\
    \ speed of\n   DNCP network state convergence.  This reduces the number of queries\n\
    \   needed to reconverge during both initial network convergence and when\n  \
    \ a part of the network loses and regains bidirectional connectivity\n   within\
    \ that time period.\n"
- title: 5.  Data Model
  contents:
  - "5.  Data Model\n   This section describes the local data structures a minimal\n\
    \   implementation might use.  This section is provided only as a\n   convenience\
    \ for the implementor.  Some of the optional extensions\n   (Section 6) describe\
    \ additional data requirements, and some optional\n   parts of the core protocol\
    \ may also require more.\n   A DNCP node has:\n   o  A data structure containing\
    \ data about the most recently sent\n      Request Network State TLVs (Section\
    \ 7.1.1).  The simplest option\n      is keeping a timestamp of the most recent\
    \ request (required to\n      fulfill reply rate limiting specified in Section\
    \ 4.4).\n   A DNCP node has the following for every DNCP node in the DNCP\n  \
    \ network:\n   o  Node identifier: the unique identifier of the node.  The length,\n\
    \      how it is produced, and how collisions are handled is up to the\n     \
    \ DNCP profile.\n   o  Node data: the set of TLV tuples published by that particular\n\
    \      node.  As they are transmitted in a particular order (see Node\n      State\
    \ TLV (Section 7.2.3) for details), maintaining the order\n      within the data\
    \ structure here may be reasonable.\n   o  Latest sequence number: the 32-bit\
    \ sequence number that is\n      incremented any time the TLV set is published.\
    \  The comparison\n      function used to compare them is described in Section\
    \ 4.4.\n   o  Origination time: the (estimated) time when the current TLV set\n\
    \      with the current sequence number was published.  It is used to\n      populate\
    \ the Milliseconds Since Origination field in a Node State\n      TLV (Section\
    \ 7.2.3).  Ideally, it also has millisecond accuracy.\n   Additionally, a DNCP\
    \ node has a set of endpoints for which DNCP is\n   configured to be used.  For\
    \ each such endpoint, a node has:\n   o  Endpoint identifier: the 32-bit opaque\
    \ locally unique value\n      identifying the endpoint within a node.  It SHOULD\
    \ NOT be reused\n      immediately after an endpoint is disabled.\n   o  Trickle\
    \ instance: the endpoint's Trickle instance with parameters\n      I, T, and c\
    \ (only on an endpoint in Multicast+Unicast transport\n      mode).\n   and one\
    \ (or more) of the following:\n   o  Interface: the assigned local network interface.\n\
    \   o  Unicast address: the DNCP node it should connect with.\n   o  Set of addresses:\
    \ the DNCP nodes from which connections are\n      accepted.\n   For each remote\
    \ (peer, endpoint) pair detected on a local endpoint, a\n   DNCP node has:\n \
    \  o  Node identifier: the unique identifier of the peer.\n   o  Endpoint identifier:\
    \ the unique endpoint identifier used by the\n      peer.\n   o  Peer address:\
    \ the most recently used address of the peer\n      (authenticated and authorized,\
    \ if security is enabled).\n   o  Trickle instance: the particular peer's Trickle\
    \ instance with\n      parameters I, T, and c (only on an endpoint in unicast\
    \ mode, when\n      using an unreliable unicast transport).\n"
- title: 6.  Optional Extensions
  contents:
  - "6.  Optional Extensions\n   This section specifies extensions to the core protocol\
    \ that a DNCP\n   profile may specify to be used.\n"
- title: 6.1.  Keep-Alives
  contents:
  - "6.1.  Keep-Alives\n   While DNCP provides mechanisms for discovery and adding\
    \ new peers on\n   an endpoint (Section 4.5), as well as state change notifications,\n\
    \   another mechanism may be needed to get rid of old, no longer valid\n   peers\
    \ if the transport or lower layers do not provide one as noted in\n   Section\
    \ 4.6.\n   If keep-alives are not specified in the DNCP profile, the rest of\n\
    \   this subsection MUST be ignored.\n   A DNCP profile MAY specify either per-endpoint\
    \ (sent using multicast\n   to all DNCP nodes connected to a multicast-enabled\
    \ link) or per-peer\n   (sent using unicast to each peer individually) keep-alive\
    \ support.\n   For every endpoint that a keep-alive is specified for in the DNCP\n\
    \   profile, the endpoint-specific keep-alive interval MUST be\n   maintained.\
    \  By default, it is DNCP_KEEPALIVE_INTERVAL.  If there is\n   a local value that\
    \ is preferred for that for any reason\n   (configuration, energy conservation,\
    \ media type, ...), it can be\n   substituted instead.  If a non-default keep-alive\
    \ interval is used on\n   any endpoint, a DNCP node MUST publish an appropriate\
    \ Keep-Alive\n   Interval TLV(s) (Section 7.3.2) within its node data.\n"
- title: 6.1.1.  Data Model Additions
  contents:
  - "6.1.1.  Data Model Additions\n   The following additions to the Data Model (Section\
    \ 5) are needed to\n   support keep-alives:\n   For each configured endpoint that\
    \ has per-endpoint keep-alives\n   enabled:\n   o  Last sent: If a timestamp that\
    \ indicates the last time a Network\n      State TLV (Section 7.2.2) was sent\
    \ over that interface.\n   For each remote (peer, endpoint) pair detected on a\
    \ local endpoint, a\n   DNCP node has:\n   o  Last contact timestamp: A timestamp\
    \ that indicates the last time a\n      consistent Network State TLV (Section\
    \ 7.2.2) was received from the\n      peer over multicast or when anything was\
    \ received over unicast.\n      Failing to update it for a certain amount of time\
    \ as specified in\n      Section 6.1.5 results in the removal of the peer.  When\
    \ adding a\n      new peer, it is initialized to the current time.\n   o  Last\
    \ sent: If per-peer keep-alives are enabled, a timestamp that\n      indicates\
    \ the last time a Network State TLV (Section 7.2.2) was\n      sent to that point-to-point\
    \ peer.  When adding a new peer, it is\n      initialized to the current time.\n"
- title: 6.1.2.  Per-Endpoint Periodic Keep-Alives
  contents:
  - "6.1.2.  Per-Endpoint Periodic Keep-Alives\n   If per-endpoint keep-alives are\
    \ enabled on an endpoint in\n   Multicast+Unicast transport mode, and if no traffic\
    \ containing a\n   Network State TLV (Section 7.2.2) has been sent to a particular\n\
    \   endpoint within the endpoint-specific keep-alive interval, a Network\n   State\
    \ TLV (Section 7.2.2) MUST be sent on that endpoint, and a new\n   Trickle interval\
    \ started, as specified in step 2 of Section 4.2 of\n   [RFC6206].  The actual\
    \ sending time SHOULD be further delayed by a\n   random time span in [0, Imin/2].\n"
- title: 6.1.3.  Per-Peer Periodic Keep-Alives
  contents:
  - "6.1.3.  Per-Peer Periodic Keep-Alives\n   If per-peer keep-alives are enabled\
    \ on a unicast-only endpoint, and\n   if no traffic containing a Network State\
    \ TLV (Section 7.2.2) has been\n   sent to a particular peer within the endpoint-specific\
    \ keep-alive\n   interval, a Network State TLV (Section 7.2.2) MUST be sent to\
    \ the\n   peer, and a new Trickle interval started, as specified in step 2 of\n\
    \   Section 4.2 of [RFC6206].\n"
- title: 6.1.4.  Received TLV Processing Additions
  contents:
  - "6.1.4.  Received TLV Processing Additions\n   If a TLV is received over unicast\
    \ from the peer, the Last contact\n   timestamp for the peer MUST be updated.\n\
    \   On receipt of a Network State TLV (Section 7.2.2) that is consistent\n   with\
    \ the locally calculated network state hash, the Last contact\n   timestamp for\
    \ the peer MUST be updated in order to maintain it as a\n   peer.\n"
- title: 6.1.5.  Peer Removal
  contents:
  - "6.1.5.  Peer Removal\n   For every peer on every endpoint, the endpoint-specific\
    \ keep-alive\n   interval must be calculated by looking for Keep-Alive Interval\
    \ TLVs\n   (Section 7.3.2) published by the node, and if none exist, use the\n\
    \   default value of DNCP_KEEPALIVE_INTERVAL.  If the peer's Last contact\n  \
    \ timestamp has not been updated for at least a locally chosen\n   potentially\
    \ endpoint-specific keep-alive multiplier (defaults to\n   DNCP_KEEPALIVE_MULTIPLIER)\
    \ times the peer's endpoint-specific keep-\n   alive interval, the Peer TLV for\
    \ that peer and the local DNCP peer\n   state MUST be removed.\n"
- title: 6.2.  Support for Dense Multicast-Enabled Links
  contents:
  - "6.2.  Support for Dense Multicast-Enabled Links\n   This optimization is needed\
    \ to avoid a state space explosion.  Given\n   a large set of DNCP nodes publishing\
    \ data on an endpoint that uses\n   multicast on a link, every node will add a\
    \ Peer TLV (Section 7.3.1)\n   for each peer.  While Trickle limits the amount\
    \ of traffic on the\n   link in stable state to some extent, the total amount\
    \ of data that is\n   added to and maintained in the DNCP network given N nodes\
    \ on a\n   multicast-enabled link is O(N^2).  Additionally, if per-peer keep-\n\
    \   alives are used, there will be O(N^2) keep-alives running on the link\n  \
    \ if the liveliness of peers is not ensured using some other way (e.g.,\n   TCP\
    \ connection lifetime, Layer 2 notification, or per-endpoint keep-\n   alive).\n\
    \   An upper bound for the number of peers that are allowed for a\n   particular\
    \ type of link that an endpoint in Multicast+Unicast\n   transport mode is used\
    \ on SHOULD be provided by a DNCP profile, but\n   it MAY also be chosen at runtime.\
    \  The main consideration when\n   selecting a bound (if any) for a particular\
    \ type of link should be\n   whether it supports multicast traffic and whether\
    \ a too large number\n   of peers case is likely to happen during the use of that\
    \ DNCP profile\n   on that particular type of link.  If neither is likely, there\
    \ is\n   little point specifying support for this for that particular link\n \
    \  type.\n   If a DNCP profile does not support this extension at all, the rest\
    \ of\n   this subsection MUST be ignored.  This is because when this extension\n\
    \   is used, the state within the DNCP network only contains a subset of\n   the\
    \ full topology of the network.  Therefore, every node must be\n   aware of the\
    \ potential of it being used in a particular DNCP profile.\n   If the specified\
    \ upper bound is exceeded for some endpoint in\n   Multicast+Unicast transport\
    \ mode and if the node does not have the\n   highest node identifier on the link,\
    \ it SHOULD treat the endpoint as\n   a unicast endpoint connected to the node\
    \ that has the highest node\n   identifier detected on the link, therefore transitioning\
    \ to\n   Multicast-listen+Unicast transport mode.  See Section 4.2 for\n   implications\
    \ on the specific endpoint behavior.  The nodes in\n   Multicast-listen+Unicast\
    \ transport mode MUST keep listening to\n   multicast traffic to both receive\
    \ messages from the node(s) still in\n   Multicast+Unicast mode and react to nodes\
    \ with a greater node\n   identifier appearing.  If the highest node identifier\
    \ present on the\n   link changes, the remote unicast address of the endpoints\
    \ in\n   Multicast-Listen+Unicast transport mode MUST be changed.  If the node\n\
    \   identifier of the local node is the highest one, the node MUST switch\n  \
    \ back to, or stay in, Multicast+Unicast mode and form peer\n   relationships\
    \ with all peers as specified in Section 4.5.\n"
- title: 7.  Type-Length-Value Objects
  contents:
  - "7.  Type-Length-Value Objects\n    0                   1                   2\
    \                   3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5\
    \ 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |            Type               |           Length              |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |               Value (if any) (+padding (if any))              |\n   ..\n\
    \   |                     (variable # of bytes)                     |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                     (optional nested TLVs)                    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   Each TLV is encoded as:\n   o  a 2-byte Type field\n   o  a 2-byte Length\
    \ field, which contains the length of the Value\n      field in bytes; 0 means\
    \ no value\n   o  the value itself (if any)\n   o  padding bytes with a value\
    \ of zero up to the next 4-byte boundary\n      if the Length is not divisible\
    \ by 4\n   While padding bytes MUST NOT be included in the number stored in the\n\
    \   Length field of the TLV, if the TLV is enclosed within another TLV,\n   then\
    \ the padding is included in the enclosing TLV's Length value.\n   Each TLV that\
    \ does not define optional fields or variable-length\n   content MAY be sent with\
    \ additional sub-TLVs appended after the TLV\n   to allow for extensibility. \
    \ When handling such TLV types, each node\n   MUST accept received TLVs that are\
    \ longer than the fixed fields\n   specified for the particular type and ignore\
    \ the sub-TLVs with either\n   unknown types or types not supported within that\
    \ particular TLV.  If\n   any sub-TLVs are present, the Length field of the TLV\
    \ describes the\n   number of bytes from the first byte of the TLV's own Value\
    \ (if any)\n   to the last (padding) byte of the last sub-TLV.\n   For example,\
    \ type=123 (0x7b) TLV with value 'x' (120 = 0x78) is\n   encoded as: 007B 0001\
    \ 7800 0000.  If it were to have a sub-TLV of\n   type=124 (0x7c) with value 'y',\
    \ it would be encoded as 007B 000C 7800\n   0000 007C 0001 7900 0000.\n   In this\
    \ section, the following special notation is used:\n      .. = octet string concatenation\
    \ operation.\n      H(x) = non-cryptographic hash function specified by the DNCP\n\
    \      profile.\n   In addition to the TLV types defined in this document, TLV\
    \ Types\n   11-31 and 512-767 are unassigned and may be sequentially registered,\n\
    \   starting at 11, by Standards Action [RFC5226] by extensions to DNCP\n   that\
    \ may be applicable in multiple DNCP profiles.\n"
- title: 7.1.  Request TLVs
  contents:
  - '7.1.  Request TLVs

    '
- title: 7.1.1.  Request Network State TLV
  contents:
  - "7.1.1.  Request Network State TLV\n    0                   1                \
    \   2                   3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3\
    \ 4 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |Type: Request network state (1)|          Length: >= 0         |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   This TLV is used to request response with a Network State TLV\n   (Section\
    \ 7.2.2) and all Node State TLVs (Section 7.2.3) (without node\n   data).\n"
- title: 7.1.2.  Request Node State TLV
  contents:
  - "7.1.2.  Request Node State TLV\n    0                   1                   2\
    \                   3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5\
    \ 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   | Type: Request node state (2)  |          Length: > 0          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                        Node Identifier                        |\n   |  \
    \                (length fixed in DNCP profile)               |\n   ...\n   |\
    \                                                               |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   This TLV is used to request a Node State TLV (Section 7.2.3)\n   (including\
    \ node data) for the node with the matching node identifier.\n"
- title: 7.2.  Data TLVs
  contents:
  - '7.2.  Data TLVs

    '
- title: 7.2.1.  Node Endpoint TLV
  contents:
  - "7.2.1.  Node Endpoint TLV\n    0                   1                   2    \
    \               3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\
    \ 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |   Type: Node endpoint (3)     |          Length: > 4          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                        Node Identifier                        |\n   |  \
    \                (length fixed in DNCP profile)               |\n   ...\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                      Endpoint Identifier                      |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   This TLV identifies both the local node's node identifier, as well as\n  \
    \ the particular endpoint's endpoint identifier.  Section 4.2 specifies\n   when\
    \ it is sent.\n"
- title: 7.2.2.  Network State TLV
  contents:
  - "7.2.2.  Network State TLV\n    0                   1                   2    \
    \               3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\
    \ 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |    Type: Network state (4)    |          Length: > 0          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |     H(sequence number of node 1 .. H(node data of node 1) ..  |\n   |  \
    \  .. sequence number of node N .. H(node data of node N))    |\n   |        \
    \          (length fixed in DNCP profile)               |\n   ...\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   This TLV contains the current network state hash calculated by its\n   sender\
    \ (Section 4.1 describes the algorithm).\n"
- title: 7.2.3.  Node State TLV
  contents:
  - "7.2.3.  Node State TLV\n    0                   1                   2       \
    \            3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8\
    \ 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |      Type: Node state (5)     |          Length: > 8          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                        Node Identifier                        |\n   |  \
    \                (length fixed in DNCP profile)               |\n   ...\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                       Sequence Number                         |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                Milliseconds Since Origination                 |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                         H(Node Data)                          |\n   |  \
    \                (length fixed in DNCP profile)               |\n   ...\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |       (optionally) Node Data (a set of nested TLVs)           |\n   ...\n\
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   This\
    \ TLV represents the local node's knowledge about the published\n   state of a\
    \ node in the DNCP network identified by the Node Identifier\n   field in the\
    \ TLV.\n   Every node, including the node publishing the node data, MUST update\n\
    \   the Milliseconds Since Origination whenever it sends a Node State TLV\n  \
    \ based on when the node estimates the data was originally published.\n   This\
    \ is, e.g., to ensure that any relative timestamps contained\n   within the published\
    \ node data can be correctly offset and\n   interpreted.  Ultimately, what is\
    \ provided is just an approximation,\n   as transmission delays are not accounted\
    \ for.\n   Absent any changes, if the originating node notices that the 32-bit\n\
    \   Milliseconds Since Origination value would be close to overflow\n   (greater\
    \ than 2^32 - 2^16), the node MUST republish its TLVs even if\n   there is no\
    \ change.  In other words, absent any other changes, the\n   TLV set MUST be republished\
    \ roughly every 48 days.\n   The actual node data of the node may be included\
    \ within the TLV as\n   well as in the optional Node Data field.  The set of TLVs\
    \ MUST be\n   strictly ordered based on ascending binary content (including TLV\n\
    \   type and length).  This enables, e.g., efficient state delta\n   processing\
    \ and no-copy indexing by TLV type by the recipient.  The\n   node data content\
    \ MUST be passed along exactly as it was received.\n   It SHOULD be also verified\
    \ on receipt that the locally calculated\n   H(Node Data) matches the content\
    \ of the field within the TLV, and if\n   the hash differs, the TLV SHOULD be\
    \ ignored.\n"
- title: 7.3.  Data TLVs within Node State TLV
  contents:
  - "7.3.  Data TLVs within Node State TLV\n   These TLVs are published by the DNCP\
    \ nodes and are therefore only\n   encoded in the Node Data field of Node State\
    \ TLVs.  If encountered\n   outside Node State TLV, they MUST be silently ignored.\n"
- title: 7.3.1.  Peer TLV
  contents:
  - "7.3.1.  Peer TLV\n    0                   1                   2             \
    \      3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n\
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |  \
    \     Type: Peer (8)          |          Length: > 8          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                      Peer Node Identifier                     |\n   |  \
    \                (length fixed in DNCP profile)               |\n   ...\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                    Peer Endpoint Identifier                   |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                   (Local) Endpoint Identifier                 |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   This TLV indicates that the node in question vouches that the\n   specified\
    \ peer is reachable by it on the specified local endpoint.\n   The presence of\
    \ this TLV at least guarantees that the node publishing\n   it has received traffic\
    \ from the peer recently.  For guaranteed up-\n   to-date bidirectional reachability,\
    \ the existence of both nodes'\n   matching Peer TLVs needs to be checked.\n"
- title: 7.3.2.  Keep-Alive Interval TLV
  contents:
  - "7.3.2.  Keep-Alive Interval TLV\n    0                   1                  \
    \ 2                   3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4\
    \ 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   | Type: Keep-alive interval (9) |          Length: >= 8         |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                      Endpoint Identifier                      |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                           Interval                            |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   This TLV indicates a non-default interval being used to send keep-\n   alives\
    \ as specified in Section 6.1.\n   Endpoint identifier is used to identify the\
    \ particular (local)\n   endpoint for which the interval applies on the sending\
    \ node.  If 0,\n   it applies for ALL endpoints for which no specific TLV exists.\n\
    \   Interval specifies the interval in milliseconds at which the node\n   sends\
    \ keep-alives.  A value of zero means no keep-alives are sent at\n   all; in that\
    \ case, some lower-layer mechanism that ensures the\n   presence of nodes MUST\
    \ be available and used.\n"
- title: 8.  Security and Trust Management
  contents:
  - "8.  Security and Trust Management\n   If specified in the DNCP profile, either\
    \ DTLS [RFC6347] or TLS\n   [RFC5246] may be used to authenticate and encrypt\
    \ either some (if\n   specified optional in the profile) or all unicast traffic.\
    \  The\n   following methods for establishing trust are defined, but it is up\
    \ to\n   the DNCP profile to specify which ones may, should, or must be\n   supported.\n"
- title: 8.1.  Trust Method Based on Pre-Shared Key
  contents:
  - "8.1.  Trust Method Based on Pre-Shared Key\n   A trust model based on Pre-Shared\
    \ Key (PSK) is a simple security\n   management mechanism that allows an administrator\
    \ to deploy devices\n   to an existing network by configuring them with a predefined\
    \ key,\n   similar to the configuration of an administrator password or Wi-Fi\n\
    \   Protected Access (WPA) key.  Although limited in nature, it is useful\n  \
    \ to provide a user-friendly security mechanism for smaller networks.\n"
- title: 8.2.  PKI-Based Trust Method
  contents:
  - "8.2.  PKI-Based Trust Method\n   A PKI-based trust model enables more advanced\
    \ management capabilities\n   at the cost of increased complexity and bootstrapping\
    \ effort.\n   However, it allows trust to be managed in a centralized manner and\
    \ is\n   therefore useful for larger networks with a need for an authoritative\n\
    \   trust management.\n"
- title: 8.3.  Certificate-Based Trust Consensus Method
  contents:
  - "8.3.  Certificate-Based Trust Consensus Method\n   For some scenarios -- such\
    \ as bootstrapping a mostly unmanaged\n   network -- the methods described above\
    \ may not provide a desirable\n   trade-off between security and user experience.\
    \  This section\n   includes guidance for implementing an opportunistic security\n\
    \   [RFC7435] method that DNCP profiles can build upon and adapt for\n   their\
    \ specific requirements.\n   The certificate-based consensus model is designed\
    \ to be a compromise\n   between trust management effort and flexibility.  It\
    \ is based on\n   X.509 certificates and allows each DNCP node to provide a trust\n\
    \   verdict on any other certificate, and a consensus is found to\n   determine\
    \ whether a node using this certificate or any certificate\n   signed by it is\
    \ to be trusted.\n   A DNCP node not using this security method MUST ignore all\
    \ announced\n   trust verdicts and MUST NOT announce any such verdicts by itself,\n\
    \   i.e., any other normative language in this subsection does not apply\n   to\
    \ it.\n   The current effective trust verdict for any certificate is defined as\n\
    \   the one with the highest priority from all trust verdicts announced\n   for\
    \ said certificate at the time.\n"
- title: 8.3.1.  Trust Verdicts
  contents:
  - "8.3.1.  Trust Verdicts\n   Trust verdicts are statements of DNCP nodes about\
    \ the trustworthiness\n   of X.509 certificates.  There are 5 possible trust verdicts\
    \ in order\n   of ascending priority:\n      0 (Neutral): no trust verdict exists,\
    \ but the DNCP network should\n      determine one.\n      1 (Cached Trust): the\
    \ last known effective trust verdict was\n      Configured or Cached Trust.\n\
    \      2 (Cached Distrust): the last known effective trust verdict was\n     \
    \ Configured or Cached Distrust.\n      3 (Configured Trust): trustworthy based\
    \ upon an external ceremony\n      or configuration.\n      4 (Configured Distrust):\
    \ not trustworthy based upon an external\n      ceremony or configuration.\n \
    \  Trust verdicts are differentiated in 3 groups:\n   o  Configured verdicts are\
    \ used to announce explicit trust verdicts a\n      node has based on any external\
    \ trust bootstrap or predefined\n      relations a node has formed with a given\
    \ certificate.\n   o  Cached verdicts are used to retain the last known trust\
    \ state in\n      case all nodes with configured verdicts about a given certificate\n\
    \      have been disconnected or turned off.\n   o  The Neutral verdict is used\
    \ to announce a new node intending to\n      join the network, so a final verdict\
    \ for it can be found.\n   The current effective trust verdict for any certificate\
    \ is defined as\n   the one with the highest priority within the set of trust\
    \ verdicts\n   announced for the certificate in the DNCP network.  A node MUST\
    \ be\n   trusted for participating in the DNCP network if and only if the\n  \
    \ current effective trust verdict for its own certificate or any one in\n   its\
    \ certificate hierarchy is (Cached or Configured) Trust, and none\n   of the certificates\
    \ in its hierarchy have an effective trust verdict\n   of (Cached or Configured)\
    \ Distrust.  In case a node has a configured\n   verdict, which is different from\
    \ the current effective trust verdict\n   for a certificate, the current effective\
    \ trust verdict takes\n   precedence in deciding trustworthiness.  Despite that,\
    \ the node still\n   retains and announces its configured verdict.\n"
- title: 8.3.2.  Trust Cache
  contents:
  - "8.3.2.  Trust Cache\n   Each node SHOULD maintain a trust cache containing the\
    \ current\n   effective trust verdicts for all certificates currently announced\
    \ in\n   the DNCP network.  This cache is used as a backup of the last known\n\
    \   state in case there is no node announcing a configured verdict for a\n   known\
    \ certificate.  It SHOULD be saved to a non-volatile memory at\n   reasonable\
    \ time intervals to survive a reboot or power outage.\n   Every time a node (re)joins\
    \ the network or detects the change of an\n   effective trust verdict for any\
    \ certificate, it will synchronize its\n   cache, i.e., store new effective trust\
    \ verdicts overwriting any\n   previously cached verdicts.  Configured verdicts\
    \ are stored in the\n   cache as their respective cached counterparts.  Neutral\
    \ verdicts are\n   never stored and do not override existing cached verdicts.\n"
- title: 8.3.3.  Announcement of Verdicts
  contents:
  - "8.3.3.  Announcement of Verdicts\n   A node SHOULD always announce any configured\
    \ verdicts it has\n   established by itself, and it MUST do so if announcing the\
    \ configured\n   verdict leads to a change in the current effective trust verdict\
    \ for\n   the respective certificate.  In absence of configured verdicts, it\n\
    \   MUST announce Cached Trust verdicts it has stored in its trust cache,\n  \
    \ if one of the following conditions applies:\n   o  The stored trust verdict\
    \ is Cached Trust, and the current\n      effective trust verdict for the certificate\
    \ is Neutral or does not\n      exist.\n   o  The stored trust verdict is Cached\
    \ Distrust, and the current\n      effective trust verdict for the certificate\
    \ is Cached Trust.\n   A node rechecks these conditions whenever it detects changes\
    \ of\n   announced trust verdicts anywhere in the network.\n   Upon encountering\
    \ a node with a hierarchy of certificates for which\n   there is no effective\
    \ trust verdict, a node adds a Neutral Trust-\n   Verdict TLV to its node data\
    \ for all certificates found in the\n   hierarchy and publishes it until an effective\
    \ trust verdict different\n   from Neutral can be found for any of the certificates,\
    \ or a\n   reasonable amount of time (10 minutes is suggested) with no reaction\n\
    \   and no further authentication attempts has passed.  Such trust\n   verdicts\
    \ SHOULD also be limited in rate and number to prevent\n   denial-of-service attacks.\n\
    \   Trust verdicts are announced using Trust-Verdict TLVs:\n    0            \
    \       1                   2                   3\n    0 1 2 3 4 5 6 7 8 9 0 1\
    \ 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |   Type: Trust-Verdict (10)    |        Length: > 36           |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |    Verdict    |                 (reserved)                    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                                                               |\n   |  \
    \                                                             |\n   |        \
    \                                                       |\n   |              \
    \        SHA-256 Fingerprint                      |\n   |                    \
    \                                           |\n   |                          \
    \                                     |\n   |                                \
    \                               |\n   |                                      \
    \                         |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                          Common Name                          |\n      Verdict\
    \ represents the numerical index of the trust verdict.\n      (reserved) is reserved\
    \ for future additions and MUST be set to 0\n      when creating TLVs and ignored\
    \ when parsing them.\n      SHA-256 Fingerprint contains the SHA-256 [RFC6234]\
    \ hash value of\n      the certificate in DER format.\n      Common name contains\
    \ the variable-length (1-64 bytes) common name\n      of the certificate.\n"
- title: 8.3.4.  Bootstrap Ceremonies
  contents:
  - "8.3.4.  Bootstrap Ceremonies\n   The following non-exhaustive list of methods\
    \ describes possible ways\n   to establish trust relationships between DNCP nodes\
    \ and node\n   certificates.  Trust establishment is a two-way process in which\
    \ the\n   existing network must trust the newly added node, and the newly added\n\
    \   node must trust at least one of its peer nodes.  It is therefore\n   necessary\
    \ that both the newly added node and an already trusted node\n   perform such\
    \ a ceremony to successfully introduce a node into the\n   DNCP network.  In all\
    \ cases, an administrator MUST be provided with\n   external means to identify\
    \ the node belonging to a certificate based\n   on its fingerprint and a meaningful\
    \ common name.\n"
- title: 8.3.4.1.  Trust by Identification
  contents:
  - "8.3.4.1.  Trust by Identification\n   A node implementing certificate-based trust\
    \ MUST provide an interface\n   to retrieve the current set of effective trust\
    \ verdicts,\n   fingerprints, and names of all certificates currently known and\
    \ set\n   configured verdicts to be announced.  Alternatively, it MAY provide\
    \ a\n   companion DNCP node or application with these capabilities with which\n\
    \   it has a pre-established trust relationship.\n"
- title: 8.3.4.2.  Preconfigured Trust
  contents:
  - "8.3.4.2.  Preconfigured Trust\n   A node MAY be preconfigured to trust a certain\
    \ set of node or CA\n   certificates.  However, such trust relationships MUST\
    \ NOT result in\n   unwanted or unrelated trust for nodes not intended to be run\
    \ inside\n   the same network (e.g., all other devices by the same manufacturer).\n"
- title: 8.3.4.3.  Trust on Button Press
  contents:
  - "8.3.4.3.  Trust on Button Press\n   A node MAY provide a physical or virtual\
    \ interface to put one or more\n   of its internal network interfaces temporarily\
    \ into a mode in which\n   it trusts the certificate of the first DNCP node it\
    \ can successfully\n   establish a connection with.\n"
- title: 8.3.4.4.  Trust on First Use
  contents:
  - "8.3.4.4.  Trust on First Use\n   A node that is not associated with any other\
    \ DNCP node MAY trust the\n   certificate of the first DNCP node it can successfully\
    \ establish a\n   connection with.  This method MUST NOT be used when the node\
    \ has\n   already associated with any other DNCP node.\n"
- title: 9.  DNCP Profile-Specific Definitions
  contents:
  - "9.  DNCP Profile-Specific Definitions\n   Each DNCP profile MUST specify the\
    \ following aspects:\n   o  Unicast and optionally a multicast transport protocol(s)\
    \ to be\n      used.  If a multicast-based node and status discovery is desired,\n\
    \      a datagram-based transport supporting multicast has to be\n      available.\n\
    \   o  How the chosen transport(s) is secured: Not at all, optionally, or\n  \
    \    always with the TLS scheme defined here using one or more of the\n      methods,\
    \ or with something else.  If the links with DNCP nodes can\n      be sufficiently\
    \ secured or isolated, it is possible to run DNCP in\n      a secure manner without\
    \ using any form of authentication or\n      encryption.\n   o  Transport protocols'\
    \ parameters such as port numbers to be used or\n      multicast addresses to\
    \ be used.  Unicast, multicast, and secure\n      unicast may each require different\
    \ parameters, if applicable.\n   o  When receiving TLVs, what sort of TLVs are\
    \ ignored in addition --\n      as specified in Section 4.4 -- e.g., for security\
    \ reasons.  While\n      the security of the node data published within the Node\
    \ State TLVs\n      is already ensured by the base specification (if secure unicast\n\
    \      transport is used, Node State TLVs are sent only via unicast as\n     \
    \ multicast ones are ignored on receipt), if a profile adds TLVs\n      that are\
    \ sent outside the node data, a profile should indicate\n      whether or not\
    \ those TLVs should be ignored if they are received\n      via multicast or non-secured\
    \ unicast.  A DNCP profile may define\n      the following DNCP TLVs to be safely\
    \ ignored:\n      *  Anything received over multicast, except Node Endpoint TLV\n\
    \         (Section 7.2.1) and Network State TLV (Section 7.2.2).\n      *  Any\
    \ TLVs received over unreliable unicast or multicast at a\n         rate that\
    \ is that is too high; Trickle will ensure eventual\n         convergence given\
    \ the rate slows down at some point.\n   o  How to deal with node identifier collision\
    \ as described in\n      Section 4.4.  Main options are either for one or both\
    \ nodes to\n      assign new node identifiers to themselves or to notify someone\n\
    \      about a fatal error condition in the DNCP network.\n   o  Imin, Imax, and\
    \ k ranges to be suggested for implementations to be\n      used in the Trickle\
    \ algorithm.  The Trickle algorithm does not\n      require these to be the same\
    \ across all implementations for it to\n      work, but similar orders of magnitude\
    \ help implementations of a\n      DNCP profile to behave more consistently and\
    \ to facilitate\n      estimation of lower and upper bounds for convergence behavior\
    \ of\n      the network.\n   o  Hash function H(x) to be used, and how many bits\
    \ of the output are\n      actually used.  The chosen hash function is used to\
    \ handle both\n      hashing of node data and producing network state hash, which\
    \ is a\n      hash of node data hashes.  SHA-256 defined in [RFC6234] is the\n\
    \      recommended default choice, but a non-cryptographic hash function\n   \
    \   could be used as well.  If there is a hash collision in the\n      network\
    \ state hash, the network will effectively be partitioned to\n      partitions\
    \ that believe they are up to date but are actually no\n      longer converged.\
    \  The network will converge either when some node\n      data anywhere in the\
    \ network changes or when conflicting Node\n      State TLVs get transmitted across\
    \ the partition (either caused by\n      \"Trickle-Driven Status Updates\" (Section\
    \ 4.3) or as part of the\n      \"Processing of Received TLVs\" (Section 4.4)).\
    \  If a node publishes\n      node data with a hash that collides with any previously\
    \ published\n      node data, the update may not be (fully) propagated, and the\
    \ old\n      version of node data may be used instead.\n   o  DNCP_NODE_IDENTIFIER_LENGTH:\
    \ The fixed length of a node identifier\n      (in bytes).\n   o  Whether to send\
    \ keep-alives, and if so, whether it is per-endpoint\n      (requires multicast\
    \ transport) or per-peer.  Keep-alive also has\n      associated parameters:\n\
    \      *  DNCP_KEEPALIVE_INTERVAL: How often keep-alives are to be sent\n    \
    \     by default (if enabled).\n      *  DNCP_KEEPALIVE_MULTIPLIER: How many times\
    \ the\n         DNCP_KEEPALIVE_INTERVAL (or peer-supplied keep-alive interval\n\
    \         value) node may not be heard from to be considered still valid.\n  \
    \       This is just a default used in absence of any other\n         configuration\
    \ information or particular per-endpoint\n         configuration.\n   o  Whether\
    \ to support dense multicast-enabled link optimization\n      (Section 6.2) or\
    \ not.\n   For some guidance on choosing transport and security options, please\n\
    \   see Appendix B.\n"
- title: 10.  Security Considerations
  contents:
  - "10.  Security Considerations\n   DNCP-based protocols may use multicast to indicate\
    \ DNCP state changes\n   and for keep-alive purposes.  However, no actual published\
    \ data TLVs\n   will be sent across that channel.  Therefore, an attacker may\
    \ only\n   learn hash values of the state within DNCP and may be able to trigger\n\
    \   unicast synchronization attempts between nodes on a local link this\n   way.\
    \  A DNCP node MUST therefore rate limit its reactions to\n   multicast packets.\n\
    \   When using DNCP to bootstrap a network, PKI-based solutions may have\n   issues\
    \ when validating certificates due to potentially unavailable\n   accurate time\
    \ or due to the inability to use the network to either\n   check Certificate Revocation\
    \ Lists or perform online validation.\n   The Certificate-based trust consensus\
    \ mechanism defined in this\n   document allows for a consenting revocation; however,\
    \ in case of a\n   compromised device, the trust cache may be poisoned before\
    \ the actual\n   revocation happens allowing the distrusted device to rejoin the\n\
    \   network using a different identity.  Stopping such an attack might\n   require\
    \ physical intervention and flushing of the trust caches.\n"
- title: 11.  IANA Considerations
  contents:
  - "11.  IANA Considerations\n   IANA has set up a registry for the (decimal 16-bit)\
    \ \"DNCP TLV Types\"\n   under \"Distributed Node Consensus Protocol (DNCP)\"\
    .  The registration\n   procedure is Standards Action [RFC5226].  The initial\
    \ contents are:\n      0: Reserved\n      1: Request network state\n      2: Request\
    \ node state\n      3: Node endpoint\n      4: Network state\n      5: Node state\n\
    \      6: Reserved for future use (was: Custom)\n      7: Reserved for future\
    \ use (was: Fragment count)\n      8: Peer\n      9: Keep-alive interval\n   \
    \   10: Trust-Verdict\n      11-31: Unassigned\n      32-511: Reserved for per-DNCP\
    \ profile use\n      512-767: Unassigned\n      768-1023: Reserved for Private\
    \ Use [RFC5226]\n      1024-65535: Reserved for future use\n"
- title: 12.  References
  contents:
  - '12.  References

    '
- title: 12.1.  Normative References
  contents:
  - "12.1.  Normative References\n   [RFC2119]  Bradner, S., \"Key words for use in\
    \ RFCs to Indicate\n              Requirement Levels\", BCP 14, RFC 2119,\n  \
    \            DOI 10.17487/RFC2119, March 1997,\n              <http://www.rfc-editor.org/info/rfc2119>.\n\
    \   [RFC5226]  Narten, T. and H. Alvestrand, \"Guidelines for Writing an\n   \
    \           IANA Considerations Section in RFCs\", BCP 26, RFC 5226,\n       \
    \       DOI 10.17487/RFC5226, May 2008,\n              <http://www.rfc-editor.org/info/rfc5226>.\n\
    \   [RFC6206]  Levis, P., Clausen, T., Hui, J., Gnawali, O., and J. Ko,\n    \
    \          \"The Trickle Algorithm\", RFC 6206, DOI 10.17487/RFC6206,\n      \
    \        March 2011, <http://www.rfc-editor.org/info/rfc6206>.\n   [RFC6234] \
    \ Eastlake 3rd, D. and T. Hansen, \"US Secure Hash Algorithms\n              (SHA\
    \ and SHA-based HMAC and HKDF)\", RFC 6234,\n              DOI 10.17487/RFC6234,\
    \ May 2011,\n              <http://www.rfc-editor.org/info/rfc6234>.\n"
- title: 12.2.  Informative References
  contents:
  - "12.2.  Informative References\n   [RFC3315]  Droms, R., Ed., Bound, J., Volz,\
    \ B., Lemon, T., Perkins,\n              C., and M. Carney, \"Dynamic Host Configuration\
    \ Protocol\n              for IPv6 (DHCPv6)\", RFC 3315, DOI 10.17487/RFC3315,\
    \ July\n              2003, <http://www.rfc-editor.org/info/rfc3315>.\n   [RFC3493]\
    \  Gilligan, R., Thomson, S., Bound, J., McCann, J., and W.\n              Stevens,\
    \ \"Basic Socket Interface Extensions for IPv6\",\n              RFC 3493, DOI\
    \ 10.17487/RFC3493, February 2003,\n              <http://www.rfc-editor.org/info/rfc3493>.\n\
    \   [RFC5246]  Dierks, T. and E. Rescorla, \"The Transport Layer Security\n  \
    \            (TLS) Protocol Version 1.2\", RFC 5246,\n              DOI 10.17487/RFC5246,\
    \ August 2008,\n              <http://www.rfc-editor.org/info/rfc5246>.\n   [RFC6347]\
    \  Rescorla, E. and N. Modadugu, \"Datagram Transport Layer\n              Security\
    \ Version 1.2\", RFC 6347, DOI 10.17487/RFC6347,\n              January 2012,\
    \ <http://www.rfc-editor.org/info/rfc6347>.\n   [RFC7435]  Dukhovni, V., \"Opportunistic\
    \ Security: Some Protection\n              Most of the Time\", RFC 7435, DOI 10.17487/RFC7435,\n\
    \              December 2014, <http://www.rfc-editor.org/info/rfc7435>.\n   [RFC7596]\
    \  Cui, Y., Sun, Q., Boucadair, M., Tsou, T., Lee, Y., and I.\n              Farrer,\
    \ \"Lightweight 4over6: An Extension to the Dual-\n              Stack Lite Architecture\"\
    , RFC 7596, DOI 10.17487/RFC7596,\n              July 2015, <http://www.rfc-editor.org/info/rfc7596>.\n"
- title: Appendix A.  Alternative Modes of Operation
  contents:
  - "Appendix A.  Alternative Modes of Operation\n   Beyond what is described in the\
    \ main text, the protocol allows for\n   other uses.  These are provided as examples.\n"
- title: A.1.  Read-Only Operation
  contents:
  - "A.1.  Read-Only Operation\n   If a node uses just a single endpoint and does\
    \ not need to publish\n   any TLVs, full DNCP node functionality is not required.\
    \  Such a\n   limited node can acquire and maintain a view of the TLV space by\n\
    \   implementing the processing logic as specified in Section 4.4.  Such\n   node\
    \ would not need Trickle, peer-maintenance, or even keep-alives at\n   all, as\
    \ the DNCP nodes' use of it would guarantee eventual receipt of\n   network state\
    \ hashes, and synchronization of node data, even in the\n   presence of unreliable\
    \ transport.\n"
- title: A.2.  Forwarding Operation
  contents:
  - "A.2.  Forwarding Operation\n   If a node with a pair of endpoints does not need\
    \ to publish any TLVs,\n   it can detect (for example) nodes with the highest\
    \ node identifier on\n   each of the endpoints (if any).  Any TLVs received from\
    \ one of them\n   would be forwarded verbatim as unicast to the other node with\
    \ the\n   highest node identifier.\n   Any tinkering with the TLVs would remove\
    \ guarantees of this scheme\n   working; however, passive monitoring would obviously\
    \ be fine.  This\n   type of simple forwarding cannot be chained, as it does not\
    \ send\n   anything proactively.\n"
- title: Appendix B.  DNCP Profile Additional Guidance
  contents:
  - "Appendix B.  DNCP Profile Additional Guidance\n   This appendix explains implications\
    \ of design choices made when\n   specifying the DNCP profile to use particular\
    \ transport or security\n   options.\n"
- title: B.1.  Unicast Transport -- UDP or TCP?
  contents:
  - "B.1.  Unicast Transport -- UDP or TCP?\n   The node data published by a DNCP\
    \ node is limited to 64 KB due to the\n   16-bit size of the length field of the\
    \ TLV it is published within.\n   Some transport choices may decrease this limit;\
    \ if using, e.g., UDP\n   datagrams for unicast transport, the upper bound of\
    \ the node data\n   size is whatever the nodes and the underlying network can\
    \ pass to\n   each other as DNCP does not define its own fragmentation scheme.\
    \  A\n   profile that chooses UDP has to be limited to small node data (e.g.,\n\
    \   somewhat smaller than IPv6 default MTU if using IPv6) or specify a\n   minimum\
    \ that all nodes have to support.  Even then, if using\n   non-link-local communications,\
    \ there is some concern about what\n   middleboxes do to fragmented packets. \
    \ Therefore, the use of stream\n   transport such as TCP is probably a good idea\
    \ if either\n   non-link-local communication is desired or fragmentation is expected\n\
    \   to cause problems.\n   TCP also provides some other facilities, such as a\
    \ relatively long\n   built-in keep-alive, which in conjunction with connection\
    \ closes\n   occurring from eventual failed retransmissions may be sufficient\
    \ to\n   avoid the use of in-protocol keep-alive defined in Section 6.1.\n   Additionally,\
    \ it is reliable, so there is no need for Trickle on such\n   unicast connections.\n\
    \   The major downside of using TCP instead of UDP with DNCP-based\n   profiles\
    \ lies in the loss of control over the time at which TLVs are\n   received; while\
    \ unreliable UDP datagrams also have some delay, TLVs\n   within reliable stream\
    \ transport may be delayed significantly due to\n   retransmissions.  This is\
    \ not a problem if no relative time-dependent\n   information is stored within\
    \ the TLVs in the DNCP-based protocol; for\n   such a protocol, TCP is a reasonable\
    \ choice for unicast transport if\n   it is available.\n"
- title: B.2.  (Optional) Multicast Transport
  contents:
  - "B.2.  (Optional) Multicast Transport\n   Multicast is needed for dynamic peer\
    \ discovery and to trigger unicast\n   exchanges; for that, unreliable datagram\
    \ transport (=typically UDP)\n   is the only transport option defined within this\
    \ specification,\n   although DNCP-based protocols may themselves define some\
    \ other\n   transport or peer discovery mechanism (e.g., based on Multicast DNS\n\
    \   (mDNS) or DNS).\n   If multicast is used, a well-known address should be specified\
    \ and\n   for, e.g., IPv6, respectively, the desired address scopes.  In most\n\
    \   cases, link-local and possibly site-local are useful scopes.\n"
- title: B.3.  (Optional) Transport Security
  contents:
  - "B.3.  (Optional) Transport Security\n   In terms of provided security, DTLS and\
    \ TLS are equivalent; they also\n   consume a similar amount of state on the devices.\
    \  While TLS is on\n   top of a stream protocol, using DTLS also requires relatively\
    \ long\n   session caching within the DTLS layer to avoid expensive\n   reauthentication/authorization\
    \ steps if and when any state within the\n   DNCP network changes or per-peer\
    \ keep-alive (if enabled) is sent.\n   TLS implementations (at the time of writing\
    \ the specification) seem\n   more mature and available (as open source) than\
    \ DTLS ones.  This may\n   be due to a long history of use with HTTPS.\n   Some\
    \ libraries seem not to support multiplexing between insecure and\n   secure communication\
    \ on the same port, so specifying distinct ports\n   for secured and unsecured\
    \ communication may be beneficial.\n"
- title: Appendix C.  Example Profile
  contents:
  - "Appendix C.  Example Profile\n   This is the DNCP profile of SHSP, an experimental\
    \ (and for the\n   purposes of this document fictional) home automation protocol.\
    \  The\n   protocol itself is used to make a key-value store published by each\n\
    \   of the nodes available to all other nodes for distributed monitoring\n   and\
    \ control of a home infrastructure.  It defines only one additional\n   TLV type:\
    \ a key=value TLV that contains a single key=value assignment\n   for publication.\n\
    \   o  Unicast transport: IPv6 TCP on port EXAMPLE-P1 since only absolute\n  \
    \    timestamps are used within the key=value data and since it focuses\n    \
    \  primarily on Linux-based nodes that support both protocols as\n      well.\
    \  Connections from and to non-link-local addresses are\n      ignored to avoid\
    \ exposing this protocol outside the secure links.\n   o  Multicast transport:\
    \ IPv6 UDP on port EXAMPLE-P2 to link-local\n      scoped multicast address ff02:EXAMPLE.\
    \  At least one node per link\n      in the home is assumed to facilitate node\
    \ discovery without\n      depending on any other infrastructure.\n   o  Security:\
    \ None.  It is to be used only on trusted links (WPA2-x\n      wireless, physically\
    \ secure wired links).\n   o  Additional TLVs to be ignored: None.  No DNCP security\
    \ is\n      specified, and no new TLVs are defined outside of node data.\n   o\
    \  Node identifier length (DNCP_NODE_IDENTIFIER_LENGTH): 32 bits that\n      are\
    \ randomly generated.\n   o  Node identifier collision handling: Pick new random\
    \ node\n      identifier.\n   o  Trickle parameters: Imin = 200 ms, Imax = 7,\
    \ k = 1.  It means at\n      least one multicast per link in 25 seconds in stable\
    \ state (0.2 *\n      2^7).\n   o  Hash function H(x) + length: SHA-256, only\
    \ 128 bits used.  It's\n      relatively fast, and 128 bits should be plenty to\
    \ prevent random\n      conflicts (64 bits would most likely be sufficient, too).\n\
    \   o  No in-protocol keep-alives (Section 6.1); TCP keep-alive is to be\n   \
    \   used.  In practice, TCP keep-alive is seldom encountered anyway,\n      as\
    \ changes in network state cause packets to be sent on the\n      unicast connections,\
    \ and those that fail sufficiently many\n      retransmissions are dropped much\
    \ before the keep-alive actually\n      would fire.\n   o  No support for dense\
    \ multicast-enabled link optimization\n      (Section 6.2); SHSP is a simple protocol\
    \ for a few nodes (network\n      wide, not even to mention on a single link)\
    \ and therefore would\n      not provide any benefit.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   Thanks to Ole Troan, Pierre Pfister, Mark Baugher, Mark\
    \ Townsley,\n   Juliusz Chroboczek, Jiazi Yi, Mikael Abrahamsson, Brian Carpenter,\n\
    \   Thomas Clausen, DENG Hui, and Margaret Cullen for their contributions\n  \
    \ to the document.\n   Thanks to Kaiwen Jin and Xavier Bonnetain for their related\
    \ research\n   work.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Markus Stenberg\n   Independent\n   Helsinki  00930\n\
    \   Finland\n   Email: markus.stenberg@iki.fi\n   Steven Barth\n   Independent\n\
    \   Halle  06114\n   Germany\n   Email: cyrus@openwrt.org\n"
