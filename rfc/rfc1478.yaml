- title: __initial_text__
  contents:
  - '            An Architecture for Inter-Domain Policy Routing

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This RFC specifies an IAB standards track protocol for\
    \ the Internet\n   community, and requests discussion and suggestions for improvements.\n\
    \   Please refer to the current edition of the \"IAB Official Protocol\n   Standards\"\
    \ for the standardization state and status of this protocol.\n   Distribution\
    \ of this memo is unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   We present an architecture for inter-domain policy routing (IDPR).\n\
    \   The objective of IDPR is to construct and maintain routes, between\n   source\
    \ and destination administrative domains, that provide user\n   traffic with the\
    \ requested services within the constraints stipulated\n   for the domains transited.\
    \  The IDPR architecture is designed to\n   accommodate an internetwork containing\
    \ tens of thousands of\n   administrative domains with heterogeneous service requirements\
    \ and\n   restrictions.\n"
- title: Contributors
  contents:
  - "Contributors\n   The following people have contributed to the IDPR architecture:\
    \ Bob\n   Braden, Lee Breslau, Ross Callon, Noel Chiappa, Dave Clark, Pat\n  \
    \ Clark, Deborah Estrin, Marianne Lepp, Mike Little, Martha Steenstrup,\n   Zaw-Sing\
    \ Su, Paul Tsuchiya, and Gene Tsudik.  Yakov Rekhter supplied\n   many useful\
    \ comments on a previous draft of this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction. . . . . . . . . . . . . . . . . . . .\
    \ . . . . . . 3\n   1.1. The Internet Environment. . . . . . . . . . . . . . .\
    \ . . . . 4\n   2. Approaches to Policy Routing. . . . . . . . . . . . . . . .\
    \ . . 5\n   2.1. Policy Route Generation . . . . . . . . . . . . . . . . . . .\
    \ 5\n   2.1.1. Distance Vector Approach. . . . . . . . . . . . . . . . . . 5\n\
    \   2.1.2. Link State Approach . . . . . . . . . . . . . . . . . . . . 7\n   2.2.\
    \ Routing Information Distribution. . . . . . . . . . . . . . . 8\n   2.2.1. Distance\
    \ Vector Approach. . . . . . . . . . . . . . . . . . 8\n   2.2.2. Link State Approach\
    \ . . . . . . . . . . . . . . . . . . . .10\n   2.3. Message Forwarding along\
    \ Policy Routes. . . . . . . . . . . .10\n   2.3.1. Hop-by-Hop Approach . . .\
    \ . . . . . . . . . . . . . . . . .11\n   2.3.1.1. A Clarification . . . . . .\
    \ . . . . . . . . . . . . . . .11\n   2.3.2. Source Specified Approach . . . .\
    \ . . . . . . . . . . . . .12\n   3. The IDPR Architecture . . . . . . . . . .\
    \ . . . . . . . . . . .13\n   3.1. IDPR Functions. . . . . . . . . . . . . . .\
    \ . . . . . . . . .13\n   3.2. IDPR Entities . . . . . . . . . . . . . . . . .\
    \ . . . . . . .13\n   3.2.1. Path Agents . . . . . . . . . . . . . . . . . . .\
    \ . . . . .16\n   3.2.2. IDPR Servers. . . . . . . . . . . . . . . . . . . . .\
    \ . . .17\n   3.2.3. Entity Identifiers. . . . . . . . . . . . . . . . . . . .\
    \ .19\n   3.3. Security and Reliability. . . . . . . . . . . . . . . . . . .20\n\
    \   3.3.1. Retransmissions and Acknowledgements. . . . . . . . . . . .20\n   3.3.2.\
    \ Integrity Checks. . . . . . . . . . . . . . . . . . . . . .21\n   3.3.3. Source\
    \ Authentication . . . . . . . . . . . . . . . . . . .21\n   3.3.4. Timestamps.\
    \ . . . . . . . . . . . . . . . . . . . . . . . .21\n   3.4. An Example of IDPR\
    \ Operation. . . . . . . . . . . . . . . . .22\n   4. Accommodating a Large, Heterogeneous\
    \ Internet . . . . . . . . .25\n   4.1. Domain Level Routing. . . . . . . . .\
    \ . . . . . . . . . . . .25\n   4.2. Route Generation. . . . . . . . . . . . .\
    \ . . . . . . . . . .27\n   4.3. Super Domains . . . . . . . . . . . . . . . .\
    \ . . . . . . . .29\n   4.4. Domain Communities. . . . . . . . . . . . . . . .\
    \ . . . . . .30\n   4.5. Robustness in the Presence of Failures. . . . . . . .\
    \ . . . .31\n   4.5.1. Path Repair . . . . . . . . . . . . . . . . . . . . . .\
    \ . .31\n   4.5.2. Partitions. . . . . . . . . . . . . . . . . . . . . . . . .33\n\
    \   5. References. . . . . . . . . . . . . . . . . . . . . . . . . . .XX\n   5.\
    \ Security Considerations . . . . . . . . . . . . . . . . . . . .34\n   6. Author's\
    \ Address  . . . . . . . . . . . . . . . . . . . . . . .34\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   As data communications technologies evolve and user populations\
    \ grow,\n   the demand for internetworking increases.  Internetworks usually\n\
    \   proliferate through interconnection of autonomous, heterogeneous\n   networks\
    \ administered by separate authorities.  We use the term\n   \"administrative\
    \ domain\" (AD) to refer to any collection of contiguous\n   networks, gateways,\
    \ links, and hosts governed by a single\n   administrative authority who selects\
    \ the intra-domain routing\n   procedures and addressing schemes, specifies service\
    \ restrictions for\n   transit traffic, and defines service requirements for locally-\n\
    \   generated traffic.\n   Interconnection of administrative domains can broaden\
    \ the range of\n   services available in an internetwork.  Hence, traffic with\
    \ special\n   service requirements is more likely to receive the service requested.\n\
    \   However, administrators of domains offering special transit services\n   are\
    \ more likely to establish stringent access restrictions, in order\n   to maintain\
    \ control over the use of their domains' resources.\n   An internetwork composed\
    \ of many domains with diverse service\n   requirements and restrictions requires\
    \ \"policy routing\" to transport\n   traffic between source and destination.\
    \  Policy routing constitutes\n   route generation and message forwarding procedures\
    \ for producing and\n   using routes that simultaneously satisfy user service\
    \ requirements\n   and respect transit domain service restrictions.\n   With policy\
    \ routing, each domain administrator sets \"transit\n   policies\" that dictate\
    \ how and by whom the resources within its\n   domain should be used.  Transit\
    \ policies are usually public, and they\n   specify offered services comprising:\n\
    \   - Access restrictions: e.g., applied to traffic to or from certain\n     domains\
    \ or classes of users.\n   - Quality: e.g., delay, throughput, or error characteristics.\n\
    \   - Monetary cost: e.g., charge per byte, message, or unit time.\n   Each domain\
    \ administrator also sets \"source policies\" for traffic\n   originating within\
    \ its domain.  Source policies are usually private,\n   and they specify requested\
    \ services comprising:\n   - Access restrictions: e.g., domains to favor or avoid\
    \ in routes.\n   - Quality: e.g., acceptable delay, throughput, or reliability.\n\
    \   - Monetary cost: e.g., acceptable session cost.\n   In this document, we describe\
    \ an architecture for inter-domain policy\n   routing (IDPR), and we provide a\
    \ set of functions which can form the\n   basis for a suite of IDPR protocols\
    \ and procedures.\n"
- title: 1.1.  The Internet Environment
  contents:
  - "1.1.  The Internet Environment\n   The Internet currently comprises over 7000\
    \ operational networks and\n   over 10,000 registered networks.  In fact, for\
    \ the last several\n   years, the number of constituent networks has approximately\
    \ doubled\n   annually.  Although we do not expect the Internet to sustain this\n\
    \   growth rate, we must provide an architecture for IDPR that can\n   accommodate\
    \ the Internet five to ten years in the future.  According\n   to the functional\
    \ requirements for inter-autonomous system (i.e.,\n   inter-domain) routing set\
    \ forth in [6], the IDPR architecture and\n   protocols must be able to handle\
    \ O(100,000) networks distributed over\n   O(10,000) domains.\n   Internet connectivity\
    \ has increased along with the number of\n   component networks.  In the early\
    \ 1980s, the Internet was purely\n   hierarchical, with the ARPANET as the single\
    \ backbone.  The current\n   Internet possesses a semblance of a hierarchy in\
    \ the collection of\n   backbone, regional, metropolitan, and campus domains that\
    \ compose it.\n   However, technological, economical, and political incentives\
    \ have\n   prompted the introduction of inter-domain links outside of those in\n\
    \   the strict hierarchy.  Hence, the Internet has the properties of both\n  \
    \ hierarchical and mesh connectivity.\n   We expect that the Internet will evolve\
    \ in the following way.  Over\n   the next five years, the Internet will grow\
    \ to contain O(10) backbone\n   domains, most providing connectivity between many\
    \ source and\n   destination domains and offering a wide range of qualities of\n\
    \   service, for a fee.  Most domains will connect directly or indirectly\n  \
    \ to at least one Internet backbone domain, in order to communicate\n   with other\
    \ domains.  In addition, some domains may install direct\n   links to their most\
    \ favored destinations.  Domains at the lower\n   levels of the hierarchy will\
    \ provide some transit service, limited to\n   traffic between selected sources\
    \ and destinations.  However, the\n   majority of Internet domains will be \"\
    stubs\", that is, domains that\n   do not provide any transit service for other\
    \ domains.\n   The bulk of Internet traffic will be generated by hosts in these\
    \ stub\n   domains, and thus, the applications running in these hosts will\n \
    \  determine the traffic service requirements.  We expect application\n   diversity\
    \ encompassing electronic mail, desktop videoconferencing,\n   scientific visualization,\
    \ and distributed simulation, to list a few.\n   Many of these applications have\
    \ strict requirements on loss, delay,\n   and throughput.\n   Ensuring that Internet\
    \ traffic traverses routes that provide the\n   required services without violating\
    \ domain usage restrictions will be\n   the task of policy routing in the Internet\
    \ in the next several years.\n   Refer to [1]-[10] for more information on the\
    \ role of policy routing\n   in the Internet.\n"
- title: 2.  Approaches to Policy Routing
  contents:
  - "2.  Approaches to Policy Routing\n   In this section, we provide an assessment\
    \ of candidate approaches to\n   policy routing, concentrating on the \"distance\
    \ vector\" and \"link\n   state\" alternatives for routing information distribution\
    \ and route\n   generation and on the \"hop-by-hop\" and \"source specified\"\n\
    \   alternatives for data message forwarding.  The IDPR architecture\n   supports\
    \ link state routing information distribution and route\n   generation in conjunction\
    \ with source specified message forwarding.\n   We justify these choices for IDPR\
    \ below.\n"
- title: 2.1.  Policy Route Generation
  contents:
  - "2.1.  Policy Route Generation\n   We present policy route generation from the\
    \ distance vector\n   perspective and from the link state perspective.\n"
- title: 2.1.1.  Distance Vector Approach
  contents:
  - "2.1.1.  Distance Vector Approach\n   Distance vector route generation distributes\
    \ the computation of a\n   single route among multiple routing entities along\
    \ the route.  Hence,\n   distance vector route generation is potentially susceptible\
    \ to the\n   problems of routing loop formation and slow adaptation to changes\
    \ in\n   an internetwork.  However, there exist several techniques that can be\n\
    \   applied during distance vector route generation to reduce the\n   severity\
    \ of, or even eliminate, these problems.  For information on a\n   loop-free,\
    \ quickly adapting distance vector routing procedure,\n   consult [13] and [14].\n\
    \   During policy route generation, each recipient of a distance vector\n   message\
    \ assesses the acceptability of the associated route and\n   determines the set\
    \ of neighboring domains to which the message should\n   be propagated.  In the\
    \ context of policy routing, both of the\n   following conditions are necessary\
    \ for route acceptability:\n   - The route is consistent with at least one transit\
    \ policy for each\n     domain, not including the current routing entity's domain,\
    \ contained\n     in the route.  To enable each recipient of a distance vector\
    \ message\n     to verify consistency of the associated route with the transit\n\
    \     policies of all constituent domains, each routing entity should\n     include\
    \ its domain's identity and transit policies in each\n     acceptable distance\
    \ vector message it propagates.\n   - The route is consistent with at least one\
    \ source policy for at least\n     one domain in the Internet.  To enable each\
    \ recipient of a distance\n     vector message to verify consistency of the associated\
    \ route with\n     the source policies of particular domains, each domain must\
    \ provide\n     other domains with access to its source policies.\n   In addition,\
    \ at least one of the following conditions is necessary\n   for route acceptability:\n\
    \   - The route is consistent with at least one of the transit policies\n    \
    \ for the current routing entity's domain.  In this case, the routing\n     entity\
    \ accepts the distance vector message and then proceeds to\n     compare the associated\
    \ route with its other routes to the\n     destinations listed in the message.\
    \  If the routing entity decides\n     that the new route is preferable, it updates\
    \ the distance vector\n     message with its domain's identity and transit policies\
    \ and then\n     propagates the message to the appropriate neighboring domains.\
    \  We\n     discuss distance vector message distribution in more detail in\n \
    \    section 2.2.1.\n   The route is consistent with at least one of the source\
    \ policies for\n   the current routing entity's domain.  In this case, the routing\n\
    \   entity need not propagate the distance vector message but does retain\n  \
    \ the associated route for use by traffic from local hosts, bound for\n   the\
    \ destinations listed in the message.\n   The routing entity discards any distance\
    \ vector message that does not\n   meet these necessary conditions.\n   With distance\
    \ vector policy route generation, a routing entity may\n   select and store multiple\
    \ routes of different characteristics, such\n   as qualities of service, to a\
    \ single destination.  A routing entity\n   uses the quality of service information,\
    \ provided in the transit\n   policies contained in accepted distance vector messages,\
    \ to\n   discriminate between routes based on quality of service.  Moreover, a\n\
    \   routing entity may select routes that are specific to certain source\n   domains,\
    \ provided that the routing entity has access to the source\n   policies of those\
    \ domains.\n   In the distance vector context, the flexibility of policy route\n\
    \   generation afforded by accounting for other domains' transit and\n   source\
    \ policies in route selection has the following disadvantages:\n   - Each recipient\
    \ of a distance vector message must bear the cost of\n     verifying the consistency\
    \ of the associated route with the\n     constituent domains' transit policies.\n\
    \   - Source policies must be made public.  Thus, a domain must divulge\n    \
    \ potentially private information.\n   - Each recipient of a distance vector message\
    \ must bear the\n     potentially high costs of selecting routes for arbitrary\
    \ source\n     domains.  In particular, a routing entity must store the source\n\
    \     policies of other domains, account for these source policies during\n  \
    \   route selection, and maintain source-specific forwarding\n     information.\
    \  Moreover, there must be a mechanism for distributing\n     source policy information\
    \ among domains.  Depending on the mechanism\n     selected, distribution of source\
    \ policies may add to the costs paid\n     by each routing entity in supporting\
    \ source-specific routing.\n   We note, however, that failure to distribute source\
    \ policies to all\n   domains may have unfortunate consequences.  In the worst\
    \ case, a\n   domain may not learn of any acceptable routes to a given destination,\n\
    \   even though acceptable routes do exist.  For example, suppose that AD\n  \
    \ V is connected to AD W and that AD W can reach AD Z through either AD\n   X\
    \ or AD Y.  Suppose also that AD~W, as a recipient of distance vector\n   messages\
    \ originating in AD Z, prefers the route through AD Y to the\n   route through\
    \ AD X.  Furthermore, suppose that AD W has no knowledge\n   of AD V's source\
    \ policy precluding traffic from traversing AD Y.\n   Hence, AD W distributes\
    \ to AD V the distance vector message\n   containing the route WYZ but not the\
    \ distance vector message\n   containing the route WXZ.  AD V is thus left with\
    \ no known route to\n   AD Z, although a viable route traversing AD W and AD X\
    \ does exist.\n"
- title: 2.1.2.  Link State Approach
  contents:
  - "2.1.2.  Link State Approach\n   Link state route generation permits concentration\
    \ of the computation\n   of a single route within a single routing entity at the\
    \ source of the\n   route.  In the policy routing context, entities within a domain\n\
    \   generate link state messages containing information about the\n   originating\
    \ domain, including the set of transit policies that apply\n   and the connectivity\
    \ to adjacent domains, and they distribute these\n   messages to neighboring domains.\
    \  Each recipient of a link state\n   message stores the routing information for\
    \ anticipated policy route\n   generation and also distributes it to neighboring\
    \ domains.  Based on\n   the set of link state messages collected from other domains\
    \ and on\n   its domain's source and transit policies, a routing entity constructs\n\
    \   and selects policy routes from its domain to other domains in the\n   Internet.\n\
    \   We have selected link state policy route generation for IDPR for the\n   following\
    \ reasons:\n   - Each domain has complete control over policy route generation\
    \ from\n     the perspective of itself as source.\n   - The cost of computing\
    \ a route is completely contained within the\n     source domain.  Hence, routing\
    \ entities in other domains need not\n     bear the cost of generating policy\
    \ routes that their domains' local\n     hosts may never use.\n   - Source policies\
    \ may be kept private and hence need not be\n     distributed.  Thus, there are\
    \ no memory, processing, or transmission\n     bandwidth costs incurred for distributing\
    \ and storing source\n     policies.\n"
- title: 2.2.  Routing Information Distribution
  contents:
  - "2.2.  Routing Information Distribution\n   A domain's routing information and\
    \ the set of domains to which that\n   routing information is distributed each\
    \ influence the set of generable\n   policy routes that include the given domain.\
    \  In particular, a domain\n   administrator may promote the generation of routes\
    \ that obey its\n   domain's transit policies by ensuring that its domain's routing\n\
    \   information:\n   - Includes resource access restrictions.\n   - Is distributed\
    \ only to those domains that are permitted to use these\n     resources.\n   Both\
    \ of these mechanisms, distributing restrictions with and\n   restricting distribution\
    \ of a domain's routing information, can be\n   applied in both the distance vector\
    \ and link state contexts.\n"
- title: 2.2.1.  Distance Vector Approach
  contents:
  - "2.2.1.  Distance Vector Approach\n   A routing entity may distribute its domain's\
    \ resource access\n   restrictions by including the appropriate transit policy\
    \ information\n   in each distance vector it accepts and propagates.  Also, the\
    \ routing\n   entity may restrict distribution of an accepted distance vector\n\
    \   message by limiting the set of neighboring domains to which it\n   propagates\
    \ the message.  In fact, restricting distribution of routing\n   information is\
    \ inherent in the distance vector approach, as a routing\n   entity propagates\
    \ only the preferred routes among all the distance\n   vector messages that it\
    \ accepts.\n   Although restricting distribution of distance vector messages is\n\
    \   easy, coordinating restricted distribution among domains requires\n   each\
    \ domain to know other domains' distribution restrictions.  Each\n   domain may\
    \ have a set of distribution restrictions that apply to all\n   distance vector\
    \ messages generated by that domain as well as sets of\n   distribution restrictions\
    \ that apply to distance vector messages\n   generated by other domains.\n   As\
    \ a distance vector message propagates among domains, each routing\n   entity\
    \ should exercise the distribution restrictions associated with\n   each domain\
    \ constituting the route thus far constructed.  In\n   particular, a routing entity\
    \ should send an accepted distance vector\n   message to a given neighbor, only\
    \ if distribution of that message to\n   that neighbor is not precluded by any\
    \ domain contained in the route.\n   To enable a routing entity to exercise these\
    \ distribution\n   restrictions, each domain must permit other domains access\
    \ to its\n   routing information distribution restrictions.  However, we expect\n\
    \   that domains may prefer to keep distribution restrictions, like\n   source\
    \ policies, private.  There are at least two ways to make a\n   domain's routing\
    \ information distribution restrictions generally\n   available to other domains:\n\
    \   - Prior to propagation of an accepted distance vector message, a\n     routing\
    \ entity includes in the message its domain's distribution\n     restrictions\
    \ (all or only those to that apply to the given message).\n     This method requires\
    \ no additional protocol for disseminating the\n     distribution restrictions,\
    \ but it may significantly increase the\n     size of each distance vector message.\n\
    \   - Each domain independently disseminates its distribution restrictions\n \
    \    to all other domains, so that each domain will be able to exercise\n    \
    \ all other domains' distribution restrictions.  This method requires\n     an\
    \ additional protocol for disseminating the distribution\n     restrictions, and\
    \ it may require a significant amount of memory at\n     each routing entity for\
    \ storing all domains' distribution\n     restrictions.\n   We note that a domain\
    \ administrator may describe the optimal\n   distribution pattern of distance\
    \ vector messages originating in its\n   domain, as a directed graph rooted at\
    \ its domain.  Furthermore, if\n   all domains in the directed graph honor the\
    \ directionality and if the\n   graph is also acyclic, no routing loops may form,\
    \ because no two\n   domains are able to exchange distance vector messages pertaining\
    \ to\n   the same destination.  However, an acyclic graph also means that some\n\
    \   domains may be unable to discover alternate paths when connectivity\n   between\
    \ adjacent domains fails, as we show below.\n   We reconsider the example from\
    \ section 2.1.1.  Suppose that the\n   distance vector distribution graph for\
    \ AD Z is such that all distance\n   vectors originating in AD Z flow toward AD\
    \ V.  In particular,\n   distance vectors from AD Z enter AD W from AD X and AD\
    \ Y and leave AD\n   W for AD V.  Now, suppose that the link between the AD Z\
    \ and AD X\n   breaks.  AD X no longer has knowledge of any viable route to AD\
    \ Z,\n   although such a route exists through AD W.  To ensure discovery of\n\
    \   alternate routes to AD Z during connectivity failures, the distance\n   vector\
    \ distribution graph for AD Z must contain bidirectional links\n   between AD\
    \ W and AD X and between AD W and AD Y.\n"
- title: 2.2.2.  Link State Approach
  contents:
  - "2.2.2.  Link State Approach\n   With link state routing information distribution,\
    \ all recipients of a\n   domain's link state message gain knowledge of that domain's\
    \ transit\n   policies and hence service restrictions.  For reasons of efficiency\n\
    \   or privacy, a domain may also restrict the set of domains to which\n   its\
    \ link state messages should be distributed.  Thus, a domain has\n   complete\
    \ control over distributing restrictions with and restricting\n   distribution\
    \ of its routing information.\n   A domain's link state messages automatically\
    \ travel to all other\n   domains if no distribution restrictions are imposed.\
    \  Moreover, to\n   ensure that distribution restrictions, when imposed, are applied,\
    \ the\n   domain may use source specified forwarding of its link state\n   messages,\
    \ such that the messages are distributed and interpreted only\n   by the destination\
    \ domains for which they were intended.  Thus, only\n   those domains receive\
    \ the given domain's link state messages and\n   hence gain knowledge of that\
    \ domain's service offerings.\n   We have selected link state routing information\
    \ distribution for IDPR\n   for the following reasons:\n   - A domain has complete\
    \ control over the distribution of its own\n     routing information.\n   - Routing\
    \ information distribution restrictions may be kept private\n     and hence need\
    \ not be distributed.  Thus, there are no memory,\n     processing, or transmission\
    \ bandwidth costs incurred for\n     distributing and storing distribution restrictions.\n"
- title: 2.3.  Message Forwarding along Policy Routes
  contents:
  - "2.3.  Message Forwarding along Policy Routes\n   To transport data messages along\
    \ a selected policy route, a routing\n   entity may use either hop-by-hop or source\
    \ specified message\n   forwarding.\n"
- title: 2.3.1.  Hop-by-Hop Approach
  contents:
  - "2.3.1.  Hop-by-Hop Approach\n   With hop-by-hop message forwarding, each routing\
    \ entity makes an\n   independent forwarding decision based on a message's source,\n\
    \   destination, and requested services and on information contained in\n   the\
    \ entity's forwarding information database.  Hop-by-hop message\n   forwarding\
    \ follows a source-selected policy route only if all routing\n   entities along\
    \ the route have consistent routing information and make\n   consistent use of\
    \ this information when generating and selecting\n   policy routes and when establishing\
    \ forwarding information.  In\n   particular, all domains along the route must\
    \ have consistent\n   information about the source domain's source policies and\
    \ consistent,\n   but not necessarily complete, information about transit policies\
    \ and\n   domain adjacencies within the Internet.  In general, this implies\n\
    \   that each domain should have knowledge of all other domains' source\n   policies,\
    \ transit policies, and domain adjacencies.\n   When hop-by-hop message forwarding\
    \ is applied in the presence of\n   inconsistent routing information, the actual\
    \ route traversed by data\n   messages not only may differ from the route selected\
    \ by the source\n   but also may contain loops.  In the policy routing context,\
    \ private\n   source policies and restricted distribution of routing information\n\
    \   are two potential causes of routing information inconsistencies among\n  \
    \ domains.  Moreover, we expect routing information inconsistencies\n   among\
    \ domains in a large Internet, independent of whether the\n   Internet supports\
    \ policy routing, as some domains may not want or may\n   not be able to store\
    \ routing information from the entire Internet.\n"
- title: 2.3.1.1.  A Clarification
  contents:
  - "2.3.1.1.  A Clarification\n   In a previous draft, we presented the following\
    \ example which results\n   in persistent routing loops, when hop-by-hop message\
    \ forwarding is\n   used in conjunction with distance vector routing information\n\
    \   distribution and route selection.  Consider the sequence of events:\n   -\
    \ AD X receives a distance vector message containing a route to AD Z,\n     which\
    \ does not include AD Y.  AD X selects and distributes this route\n     as its\
    \ primary route to AD Z.\n   - AD Y receives a distance vector message containing\
    \ a route to AD Z,\n     which does not include AD X.  AD Y selects and distributes\
    \ this route\n     as its primary route to AD Z.\n   - AD X eventually receives\
    \ the distance vector message containing the\n     route to AD Z, which includes\
    \ AD Y but not AD X.  AD X prefers this\n     route over its previous route to\
    \ AD Z and selects this new route as\n     its primary route to AD Z.\n   - AD\
    \ Y eventually receives the distance vector message containing the\n     route\
    \ to AD Z, which includes AD X but not AD Y.  AD Y prefers this\n     route over\
    \ its previous route to AD Z and selects this new route as\n     its primary route\
    \ to AD Z.\n   Thus, AD X selects a route to AD Z that includes AD Y, and AD Y\n\
    \   selects a route to AD Z that includes AD X.\n   Suppose that all domains along\
    \ the route selected by AD X, except for\n   AD Y, make forwarding decisions consistent\
    \ with AD X's route, and\n   that all domains along the route selected by AD Y,\
    \ except for AD X,\n   make forwarding decisions consistent with AD Y's route.\
    \  Neither AD\n   X's selected route nor AD Y's selected route contains a loop.\n\
    \   Nevertheless, data messages destined for AD Z and forwarded to either\n  \
    \ AD X or AD Y will continue to circulate between AD X and AD Y, until\n   there\
    \ is a route change.  The reason is that AD X and AD Y have\n   conflicting notions\
    \ of the route to AD Z, with each domain existing\n   as a hop on the other's\
    \ route.\n   We note that while BGP-3 [8] is susceptible to such routing loops,\n\
    \   BGP-4 [9] is not.  We thank Tony Li and Yakov Rekhter for their help\n   in\
    \ clarifying this difference between BGP-3 and BGP-4.\n"
- title: 2.3.2.  Source Specified Approach
  contents:
  - "2.3.2.  Source Specified Approach\n   With source specified message forwarding,\
    \ the source domain dictates\n   the data message forwarding decisions to the\
    \ routing entities in each\n   intermediate domain, which then forward data messages\
    \ according to\n   the source specification.  Thus, the source domain ensures\
    \ that any\n   data message originating within it follows its selected routes.\n\
    \   For source specified message forwarding, each data message must carry\n  \
    \ either an entire source specified route or a path identifier.\n   Including\
    \ the complete route in each data message incurs a per\n   message transmission\
    \ and processing cost for transporting and\n   interpreting the source route.\
    \  Using path identifiers does not incur\n   these costs.  However, to use path\
    \ identifiers, the source domain\n   must initiate, prior to data message forwarding,\
    \ a path setup\n   procedure that forms an association between the path identifier\
    \ and\n   the next hop in the routing entities in each domain along the path.\n\
    \   Thus, path setup may impose an initial delay before data message\n   forwarding\
    \ can begin.\n   We have selected source specified message forwarding for IDPR\
    \ data\n   messages for the following reasons:\n   - Source specified message\
    \ forwarding respects the source policies of\n     the source domain, regardless\
    \ of whether intermediate domains along\n     the route have knowledge of these\
    \ source policies.\n   - Source specified message forwarding is loop-free, regardless\
    \ of\n     whether the all domains along the route maintain consistent routing\n\
    \     information.\n   Also, we have chosen path identifiers over complete routes,\
    \ to affect\n   source specified message forwarding, because of the reduced\n\
    \   transmission and processing cost per data message.\n"
- title: 3.  The IDPR Architecture
  contents:
  - "3.  The IDPR Architecture\n   We now present the architecture for IDPR, including\
    \ a description of\n   the IDPR functions, the entities that perform these functions,\
    \ and\n   the features of IDPR that aid in accommodating Internet growth.\n"
- title: 3.1.  IDPR Functions
  contents:
  - "3.1.  IDPR Functions\n   Inter-domain policy routing comprises the following\
    \ functions:\n   - Collecting and distributing routing information including domain\n\
    \     transit policies and inter-domain connectivity.\n   - Generating and selecting\
    \ policy routes based on the routing\n     information distributed and on the\
    \ source policies configured or\n     requested.\n   - Setting up paths across\
    \ the Internet using the policy routes\n     generated.\n   - Forwarding messages\
    \ across and between domains along the established\n     paths.\n   - Maintaining\
    \ databases of routing information, inter-domain policy\n     routes, forwarding\
    \ information, and configuration information.\n"
- title: 3.2.  IDPR Entities
  contents:
  - "3.2.  IDPR Entities\n   From the perspective of IDPR, the Internet comprises\
    \ administrative\n   domains connected by \"virtual gateways\" (see below), which\
    \ are in\n   turn connected by intra-domain routes supporting the transit policies\n\
    \   configured by the domain administrators.  Each domain administrator\n   defines\
    \ the set of transit policies that apply across its domain and\n   the virtual\
    \ gateways between which each transit policy applies.\n   Several different transit\
    \ policies may be configured for the intra-\n   domain routes connecting a pair\
    \ of virtual gateways.  Moreover, a\n   transit policy between two virtual gateways\
    \ may be directional.  That\n   is, the transit policy may apply to traffic flowing\
    \ in one direction,\n   between the virtual gateways, but not in the other direction.\n\
    \   Virtual gateways (VGs) are the only connecting points recognized by\n   IDPR\
    \ between adjacent administrative domains.  Each virtual gateway\n   is actually\
    \ a collection of directly-connected \"policy gateways\" (see\n   below) in two\
    \ adjacent domains, whose existence has been sanctioned\n   by the administrators\
    \ of both domains.  Domain administrators may\n   agree to establish more than\
    \ one virtual gateway between their\n   domains.  For example, if two domains\
    \ are to be connected at two\n   geographically distant locations, the domain\
    \ administrators may wish\n   to preserve these connecting points as distinct\
    \ at the inter-domain\n   level, by establishing two distinct virtual gateways.\n\
    \   Policy gateways (PGs) are the physical gateways within a virtual\n   gateway.\
    \  Each policy gateway forwards transit traffic according to\n   the service restrictions\
    \ stipulated by its domain's transit policies\n   applicable to its virtual gateway.\
    \  A single policy gateway may\n   belong to multiple virtual gateways.  Within\
    \ a domain, two policy\n   gateways are \"neighbors\" if they are in different\
    \ virtual gateways.\n   Within a virtual gateway, two policy gateways are \"peers\"\
    \ if they are\n   in the same domain and are \"adjacent\" if they are in different\n\
    \   domains.  Peer policy gateways must be able to communicate over\n   intra-domain\
    \ routes that support the transit policies that apply to\n   their virtual gateways.\
    \  Adjacent policy gateways are \"directly\n   connected\" if they are the only\
    \ Internet addressable entities\n   attached to the connecting medium.  Note that\
    \ this definition implies\n   that not only point-to-point links but also multiaccess\
    \ networks may\n   serve as direct connections between adjacent policy gateways.\n\
    \   Combining multiple policy gateways into a single virtual gateway\n   affords\
    \ three advantages:\n   - A reduction in the amount of IDPR routing information\
    \ that must be\n     distributed and maintained throughout the Internet.\n   -\
    \ An increase in the reliability of IDPR routes through redundancy of\n     physical\
    \ connections between domains.\n   - An opportunity for load sharing of IDPR traffic\
    \ among policy\n     gateways.\n   Several different entities are responsible\
    \ for performing the IDPR\n   functions:\n   - Policy gateways collect and distribute\
    \ routing information,\n     participate in path setup, forward data messages\
    \ along established\n     paths, and maintain forwarding information databases.\n\
    \   - \"Path agents\" act on behalf of hosts to select policy routes, to set\n\
    \     up and manage paths, and to maintain forwarding information\n     databases.\n\
    \   - Special-purpose servers maintain all other IDPR databases as\n     follows:\n\
    \      o Each \"route server\" is responsible for both its database of\n     \
    \   routing information, including domain connectivity and transit\n        policy\
    \ information, and its database of policy routes.  Also,\n        each route server\
    \ generates policy routes on behalf of its\n        domain, using entries from\
    \ its routing information database\n        and source policy information supplied\
    \ through configuration\n        or obtained directly from the path agents.\n\
    \      o  Each \"mapping server\" is responsible for its database of\n       \
    \  mappings that resolve Internet names and addresses to\n         administrative\
    \ domains.\n      o  Each \"configuration server\" is responsible for its database\
    \ of\n         configured information that applies to policy gateways, path\n\
    \         agents, and route servers in the given administrative domain.\n    \
    \     The configuration information for a given domain includes\n         source\
    \ and transit policies and mappings between local IDPR\n         entities and\
    \ their Internet addresses.\n   To maximize IDPR's manageability, one should embed\
    \ all of IDPR's\n   required functionality within the IDPR protocols and procedures.\n\
    \   However, to minimize duplication of implementation effort, one should\n  \
    \ take advantage of required functionality already provided by\n   mechanisms\
    \ external to IDPR.  Two such cases are the mapping server\n   functionality and\
    \ the configuration server functionality.  The\n   functions of the mapping server\
    \ can be integrated into an existing\n   name service such as the DNS, and the\
    \ functions of the configuration\n   server can be integrated into the domain's\
    \ existing network\n   management system.\n   Within the Internet, only policy\
    \ gateways, path agents, and route\n   servers must be able to generate, recognize,\
    \ and process IDPR\n   messages.  The existence of IDPR is invisible to all other\
    \ gateways\n   and hosts.  Mapping servers and configuration servers perform\n\
    \   necessary but ancillary functions for IDPR, and they are not required\n  \
    \ to execute the IDPR protocols.\n"
- title: 3.2.1.  Path Agents
  contents:
  - "3.2.1.  Path Agents\n   Any Internet host can reap the benefits of IDPR, as long\
    \ as there\n   exists a path agent configured to act on its behalf and a means\
    \ by\n   which the host's messages can reach that path agent.  Path agents\n \
    \  select and set up policy routes for hosts, accounting for service\n   requirements.\
    \  To obtain a host's service requirements, a path agent\n   may either consult\
    \ its configured IDPR source policy information or\n   extract service requirements\
    \ directly from the host's data messages,\n   provided such information is available\
    \ in these data messages.\n   Separating the path agent functions from the hosts\
    \ means that host\n   software need not be modified to support IDPR.  Moreover,\
    \ it means\n   that a path agent can aggregate onto a single policy route traffic\n\
    \   from several different hosts, as long as the source domains,\n   destination\
    \ domains, and service requirements are the same for all of\n   these host traffic\
    \ flows.  Policy gateways are the natural choice for\n   the entities that perform\
    \ the path agent functions on behalf of\n   hosts, as policy gateways are the\
    \ only inter-domain connecting points\n   recognized by IDPR.\n   Each domain\
    \ administrator determines the set of hosts that its\n   domain's path agents\
    \ will handle.  We expect that a domain\n   administrator will normally configure\
    \ path agents in its domain to\n   act on behalf of its domain's hosts only. \
    \ However, a path agent can\n   be configured to act on behalf of any Internet\
    \ host.  This\n   flexibility permits one domain to act as an IDPR \"proxy\" for\
    \ another\n   domain.  For example, a small stub domain may wish to have policy\n\
    \   routing available to a few of its hosts but may not want to set up\n   its\
    \ domain to support all of the IDPR functionality.  The\n   administrator of the\
    \ stub domain can negotiate the proxy function\n   with the administrator of another\
    \ domain, who agrees that its domain\n   will provide policy routes on behalf\
    \ of the stub domain's hosts.\n   If a source domain supports IDPR and limits\
    \ all domain egress points\n   to policy gateways, then each message generated\
    \ by a host in that\n   source domain and destined for a host in another domain\
    \ must pass\n   through at least one policy gateway, and hence path agent, in\
    \ the\n   source domain.  A host need not know how to reach any policy gateways\n\
    \   in its domain; it need only know how to reach a gateway on its own\n   local\
    \ network.  Gateways within the source domain direct inter-domain\n   host traffic\
    \ toward policy gateways, using default routes or routes\n   derived from other\
    \ inter-domain routing procedures.\n   If a source domain does not support IDPR\
    \ and requires an IDPR proxy\n   domain to provide its hosts with policy routing,\
    \ the administrator of\n   that source domain must carefully choose the proxy\
    \ domain.  All\n   intervening gateways between hosts in the source domain and\
    \ path\n   agents in the proxy domain forward traffic according to default\n \
    \  routes or routes derived from other inter-domain routing procedures.\n   In\
    \ order for traffic from hosts in the source domain to reach the\n   proxy domain\
    \ with no special intervention, the proxy domain must lie\n   on an existing non-IDPR\
    \ inter-domain route from the source to the\n   destination domain.  Hence, to\
    \ minimize the knowledge a domain\n   administrator must have about inter-domain\
    \ routes when selecting a\n   proxy domain, we recommend that a domain administrator\
    \ select its\n   proxy domain from the set of adjacent domains.\n   In either\
    \ case, the first policy gateway to receive messages from an\n   inter-domain\
    \ traffic flow originating at the source domain acts as\n   the path agent for\
    \ the host generating that flow.\n"
- title: 3.2.2.  IDPR Servers
  contents:
  - "3.2.2.  IDPR Servers\n   IDPR servers are the entities that manage the IDPR databases\
    \ and that\n   respond to queries for information from policy gateways or other\n\
    \   servers.  Each IDPR server may be a dedicated device, physically\n   separate\
    \ from the policy gateway, or it may be part of the\n   functionality of the policy\
    \ gateway itself.  Separating the server\n   functions from the policy gateways\
    \ reduces the processing and memory\n   requirements for and increases the data\
    \ traffic carrying capacity of\n   the policy gateways.\n   The following IDPR\
    \ databases: routing information, route, mapping,\n   and configuration, may be\
    \ distributed hierarchically, with partial\n   redundancy throughout the Internet.\
    \  This arrangement implies a\n   hierarchy of the associated servers, where a\
    \ server's position in the\n   hierarchy determines the extent of its database.\
    \  At the bottom of\n   the hierarchy are the \"local servers\" that maintain\
    \ information\n   pertinent to a single domain; at the top of the hierarchy are\
    \ the\n   \"global servers\" that maintain information pertinent to all domains\n\
    \   in the Internet.  There may be zero or more levels in between the\n   local\
    \ and global levels.\n   Hierarchical database organization relieves most IDPR\
    \ servers of the\n   burden of maintaining information about large portions of\
    \ the\n   Internet, most of which their clients will never request.\n   Distributed\
    \ database organization, with redundancy, allows clients to\n   spread queries\
    \ among IDPR servers, thus reducing the load on any one\n   server.  Furthermore,\
    \ failure to communicate with a given IDPR server\n   does not mean the loss of\
    \ the entire service, as a client may obtain\n   the information from another\
    \ server.  We note that some IDPR\n   databases, such as the mapping database,\
    \ may grow so large that it is\n   not feasible to store the entire database at\
    \ any single server.\n   IDPR routing information databases need not be completely\
    \ consistent\n   for proper policy route generation and use, because message\n\
    \   forwarding along policy routes is completely specified by the source\n   path\
    \ agent.  The absence of a requirement for consistency among IDPR\n   routing\
    \ information databases implies that there is no requirement\n   for strict synchronization\
    \ of these databases.  Such synchronization\n   is costly in terms of the message\
    \ processing and transmission\n   bandwidth required.  Nevertheless, each IDPR\
    \ route server should have\n   a query/response mechanism for making its routing\
    \ information\n   database consistent with that of another route server, if necessary.\n\
    \   A route server uses this mechanism to update its routing information\n   database\
    \ following detection of a gap or potential error in database\n   contents, for\
    \ example, when the route server returns to service after\n   disconnection from\
    \ the Internet.\n   A route server in one domain wishing to communicate with a\
    \ route\n   server in another domain must establish a policy route to the other\n\
    \   route server's domain.  To generate and establish a policy route, the\n  \
    \ route server must know the other route server's domain, and it must\n   have\
    \ sufficient routing information to construct a route to that\n   domain.  As\
    \ route servers may often intercommunicate in order to\n   obtain routing information,\
    \ one might assume an ensuing deadlock in\n   which a route server requires routing\
    \ information from another route\n   server but does not have sufficient mapping\
    \ and routing information\n   to establish a policy route to that route server.\
    \  However, such a\n   deadlock should seldom persist, if the following IDPR functionality\n\
    \   is in place:\n   - A mechanism that allows a route server to gain access,\
    \ during route\n     server initialization, to the identities of the other route\
    \ servers\n     within its domain.  Using this information, the route server need\
    \ not\n     establish policy routes in order to query these route servers for\n\
    \     routing information.\n   - A mechanism that allows a route server to gain\
    \ access, during route\n     server initialization, to its domain's adjacencies.\
    \  Using this\n     information, the route server may establish policy routes\
    \ to the\n     adjacent domains in order to query their route servers for routing\n\
    \     information when none is available within its own domain.\n   - Once operational,\
    \ a route server should collect (memory capacity\n     permitting) all the routing\
    \ information to which it has access.  A\n     domain usually does not restrict\
    \ distribution of its routing\n     information but instead distributes its routing\
    \ information to all\n     other Internet domains.  Hence, a route server in a\
    \ given domain is\n     likely to receive routing information from most Internet\
    \ domains.\n   - A mechanism that allows an operational route server to obtain\
    \ the\n     identities of external route servers from which it can obtain routing\n\
    \     information and of the domains containing these route servers.\n     Furthermore,\
    \ this mechanism should not require mapping server queries.\n     Rather, each\
    \ domain should distribute in its routing information\n     messages the identities\
    \ of all route servers, within its domain, that\n     may be queried by clients\
    \ outside of its domain.\n   When a host in one domain wishes to communicate with\
    \ a host in\n   another domain, the path agent in the source domain must establish\
    \ a\n   policy route to a path agent in the destination domain.  However, the\n\
    \   source path agent must first query a mapping server, to determine the\n  \
    \ identity of the destination domain.  The queried mapping server may\n   in turn\
    \ contact other mapping servers to obtain a reply.  As with\n   route server communication,\
    \ one might assume an ensuing deadlock in\n   which a mapping server requires\
    \ mapping information from an external\n   mapping server but the path agent handling\
    \ the traffic does not have\n   sufficient mapping information to determine the\
    \ domain of, and hence\n   establish a policy route to, that mapping server.\n\
    \   We have previously described how to minimize the potential for\n   deadlock\
    \ in obtaining routing information.  To minimize the potential\n   for deadlock\
    \ in obtaining mapping information, there should be a\n   mechanism that allows\
    \ a mapping server to gain access, during mapping\n   server initialization, to\
    \ the identities of other mapping servers and\n   the domains in which they reside.\
    \  Thus, when a mapping server needs\n   to query an external mapping server,\
    \ it knows the identity of that\n   mapping server and sends a message.  The path\
    \ agent handling this\n   traffic queries a local mapping server to resolve the\
    \ identity of the\n   external mapping server to the proper domain and then proceeds\
    \ to\n   establish a policy route to that domain.\n"
- title: 3.2.3.  Entity Identifiers
  contents:
  - "3.2.3.  Entity Identifiers\n   Each domain has a unique identifier within the\
    \ Internet, specifically\n   an ordinal number in the enumeration of Internet\
    \ domains, determined\n   by the Internet Assigned Numbers Authority (IANA) who\
    \ is responsible\n   for maintaining such information.\n   Each virtual gateway\
    \ has a unique local identifier within a domain,\n   derived from the adjacent\
    \ domain's identifier together with the\n   virtual gateway's ordinal number within\
    \ an enumeration of the virtual\n   gateways connecting the two domains.  The\
    \ administrators of both\n   domains mutually agree upon the enumeration of the\
    \ virtual gateways\n   within their shared set of virtual gateways; selecting\
    \ a single\n   virtual gateway enumeration that applies in both domains eliminates\n\
    \   the need to maintain a mapping between separate virtual gateway\n   ordinal\
    \ numbers in each domain.\n   Each policy gateway and route server has a unique\
    \ local identifier\n   within its domain, specifically an ordinal number in the\
    \ domain\n   administrator's enumeration of IDPR entities within its domain. \
    \ This\n   local identifier, when combined with the domain identifier, produces\n\
    \   a unique identifier within the Internet for the policy gateway or\n   route\
    \ server.\n"
- title: 3.3.  Security and Reliability
  contents:
  - "3.3.  Security and Reliability\n   The correctness of control information, and\
    \ in particular routing-\n   related information, distributed throughout the Internet\
    \ is a\n   critical factor affecting the Internet's ability to transport data.\n\
    \   As the number and heterogeneity of Internet domains increases, so too\n  \
    \ does the potential for both information corruption and denial of\n   service\
    \ attacks.  Thus, we have imbued the IDPR architecture with a\n   variety of mechanisms\
    \ to:\n   - Promote timely delivery of control information.\n   - Minimize acceptance\
    \ and distribution of corrupted control\n     information.\n   - Verify authenticity\
    \ of a source of control information.\n   - Reduce the chances for certain types\
    \ of denial of service attacks.\n   Consult [11] for a general security architecture\
    \ for routing and [12]\n   for a security architecture for inter-domain routing.\n"
- title: 3.3.1.  Retransmissions and Acknowledgements
  contents:
  - "3.3.1.  Retransmissions and Acknowledgements\n   All IDPR entities must make\
    \ an effort to accept and distribute only\n   correct IDPR control messages. \
    \ Each IDPR entity that transmits an\n   IDPR control message expects an acknowledgement\
    \ from the recipient\n   and must retransmit the message up to a maximum number\
    \ of times when\n   an acknowledgement is not forthcoming.  An IDPR entity that\
    \ receives\n   an IDPR control message must verify message content integrity and\n\
    \   source authenticity before accepting, acknowledging, and possibly\n   redistributing\
    \ the message.\n"
- title: 3.3.2.  Integrity Checks
  contents:
  - "3.3.2.  Integrity Checks\n   Integrity checks on message contents promote the\
    \ detection of\n   corrupted information.  Each IDPR entity that receives an IDPR\n\
    \   control message must perform several integrity checks on the\n   contents.\
    \  Individual IDPR protocols may apply more stringent\n   integrity checks than\
    \ those listed below.  The required checks\n   include confirmation of:\n   -\
    \ Recognized message version.\n   - Consistent message length.\n   - Valid message\
    \ checksum.\n   Each IDPR entity may also apply these integrity checks to IDPR\
    \ data\n   messages.  Although the IDPR architecture only requires data message\n\
    \   integrity checks at the last IDPR entity on a path, it does not\n   preclude\
    \ intermediate policy gateways from performing these checks as\n   well.\n"
- title: 3.3.3.  Source Authentication
  contents:
  - "3.3.3.  Source Authentication\n   Authentication of a message's source promotes\
    \ the detection of a\n   rogue entity masquerading as another legitimate entity.\
    \  Each IDPR\n   entity that receives an IDPR control message must verify the\n\
    \   authenticity of the message source.  We recommend that the source of\n   the\
    \ message supply a digital signature for authentication by message\n   recipients.\
    \  The digital signature should cover the entire message\n   contents (or a hash\
    \ function thereof), so that it can serve as the\n   message checksum as well\
    \ as the source authentication information.\n   Each IDPR entity may also authenticate\
    \ the source of IDPR data\n   messages; however, the IDPR architecture does not\
    \ require source\n   authentication of data messages.  Instead, we recommend that\
    \ higher\n   level (end-to-end) protocols, not IDPR, assume the responsibility\
    \ for\n   data message source authentication, because of the amount of\n   computation\
    \ involved in verifying a digital signature.\n"
- title: 3.3.4.  Timestamps
  contents:
  - "3.3.4.  Timestamps\n   Message timestamps promote the detection of out-of-date\
    \ messages as\n   well as message replays.  Each IDPR control message must carry\
    \ a\n   timestamp supplied by the source, which serves to indicate the age of\n\
    \   the message.  IDPR entities use the absolute value of a timestamp to\n   confirm\
    \ that the message is current and use the relative difference\n   between timestamps\
    \ to determine which message contains the most\n   recent information.  Hence,\
    \ all IDPR entities must possess internal\n   clocks that are synchronized to\
    \ some degree, in order for the\n   absolute value of a message timestamp to be\
    \ meaningful.  The\n   synchronization granularity required by the IDPR architecture\
    \ is on\n   the order of minutes and can be achieved manually.\n   Each IDPR entity\
    \ that receives an IDPR control message must check\n   that the message is timely.\
    \  Any IDPR control message whose timestamp\n   lies outside of the acceptable\
    \ range may contain stale or corrupted\n   information or may have been issued\
    \ by a source whose internal clock\n   has lost synchronization with the message\
    \ recipient's internal clock.\n   IDPR data messages also carry timestamps; however,\
    \ the IDPR\n   architecture does not require timestamp acceptability checks on\
    \ IDPR\n   data messages.  Instead, we recommend that IDPR entities only check\n\
    \   IDPR data message timestamps during problem diagnosis, for example,\n   when\
    \ checking for suspected message replays.\n"
- title: 3.4.  An Example of IDPR Operation
  contents:
  - "3.4.  An Example of IDPR Operation\n   We illustrate how IDPR works by stepping\
    \ through an example.  In this\n   example, we assume that all domains support\
    \ IDPR and that all domain\n   egress points are policy gateways.\n   Suppose\
    \ host Hx in domain AD X wants to communicate with host Hy in\n   domain AD Y.\
    \  Hx need not know the identity of its own domain or of\n   Hy's domain in order\
    \ to send messages to Hy.  Instead, Hx simply\n   forwards a message bound for\
    \ Hy to one of the gateways on its local\n   network, according to its local forwarding\
    \ information.  If the\n   recipient gateway is a policy gateway, the resident\
    \ path agent\n   determines how to forward the message outside of the domain.\n\
    \   Otherwise, the recipient gateway forwards the message to another\n   gateway\
    \ in AD X, according to its local forwarding information.\n   Eventually, the\
    \ message will arrive at a policy gateway in AD X, as\n   described previously\
    \ in section 3.2.1.\n   The path agent resident in the recipient policy gateway\
    \ uses the\n   message header, including source and destination addresses and\
    \ any\n   requested service information (for example, type of service), in\n \
    \  order to determine whether it is an intra-domain or inter-domain\n   message,\
    \ and if inter-domain, whether it requires an IDPR policy\n   route.  Specifically,\
    \ the path agent attempts to locate a forwarding\n   information database entry\
    \ for the given traffic flow.  The\n   forwarding information database will already\
    \ contain entries for all\n   of the following:\n   - All intra-domain traffic\
    \ flows.  Intra-domain forwarding information\n     is integrated into the forwarding\
    \ database as soon as it is received.\n   - Inter-domain traffic flows that do\
    \ not require IDPR policy routes.\n     Non-IDPR inter-domain forwarding information\
    \ is integrated into the\n     forwarding database as soon as it is received.\n\
    \   - IDPR inter-domain traffic flows for which a path has already been set\n\
    \     up.  IDPR forwarding information is integrated into the forwarding\n   \
    \  database only during path setup.\n   The path agent uses the message header\
    \ contents to guide the search\n   for a forwarding information database entry\
    \ for a traffic flow; we\n   suggest a radix search to locate a database entry.\
    \  When the search\n   terminates, it either produces a forwarding information\
    \ database\n   entry or a directive to generate such an entry for an IDPR traffic\n\
    \   flow.  If the search terminates in an existing database entry, the\n   path\
    \ agent forwards the message according to that entry.\n   Suppose that the search\
    \ terminates indicating that the traffic flow\n   between Hx and Hy requires an\
    \ IDPR route and that no forwarding\n   information database entry yet exists\
    \ for this flow.  In this case,\n   the path agent first determines the source\
    \ and destination domains\n   associated with the message's source and destination\
    \ addresses,\n   before attempting to obtain a policy route.  The path agent relies\
    \ on\n   the mapping servers to supply the domain information, but it caches\n\
    \   all mapping server responses locally to limit the number of future\n   queries.\
    \  When attempting to resolve an address to a domain, the path\n   agent always\
    \ checks its local cache before contacting a mapping\n   server.\n   After obtaining\
    \ the source and destination domain information, the\n   path agent attempts to\
    \ obtain a policy route to carry the traffic\n   from Hx to Hy.  The path agent\
    \ relies on the route servers to supply\n   policy routes, but it caches all route\
    \ server responses locally to\n   limit the number of future queries.  When attempting\
    \ to locate a\n   suitable policy route, the path agent consults its local cache\
    \ before\n   contacting a route server.  A policy route contained in the cache\
    \ is\n   suitable provided that its associated source domain is AD X, its\n  \
    \ associated destination domain is AD Y, and it satisfies the service\n   requirements\
    \ specified in the data message or through source policy\n   configuration.\n\
    \   If no suitable cache entry exists, the path agent queries the route\n   server,\
    \ providing it with the source and destination domains together\n   with the requested\
    \ services.  Upon receiving a policy route query, a\n   route server consults\
    \ its route database.  If it cannot locate a\n   suitable route in its route database,\
    \ the route server attempts to\n   generate at least one route to domain AD Y,\
    \ consistent with the\n   requested services for Hx.\n   The response to a successful\
    \ route query consists of a set of\n   candidate routes, from which the path agent\
    \ makes its selection.  We\n   expect that a path agent will normally choose a\
    \ single route from a\n   candidate set.  Nevertheless, the IDPR architecture\
    \ does not preclude\n   a path agent from selecting multiple routes from the candidate\
    \ set.\n   A path agent may desire multiple routes to support features such as\n\
    \   fault tolerance or load balancing; however, the IDPR architecture\n   does\
    \ not specify how the path agent should use multiple routes.  In\n   any case,\
    \ a route server always returns a response to a path agent's\n   query, even if\
    \ it is not successful in locating a suitable policy\n   route.\n   If the policy\
    \ route is a new route provided by the route server,\n   there will be no existing\
    \ path for the route and thus the path agent\n   must set up such a path.  However,\
    \ if the policy route is an existing\n   route extracted from the path agent's\
    \ cache, there may well be an\n   existing path for the route, set up to accommodate\
    \ a different host\n   traffic flow.  The IDPR architecture permits multiple host\
    \ traffic\n   flows to use the same path, provided that all flows sharing the\
    \ path\n   travel between the same endpoint domains and have the same service\n\
    \   requirements.  Nevertheless, the IDPR architecture does not preclude\n   a\
    \ path agent from setting up distinct paths along the same policy\n   route to\
    \ preserve the distinction between host traffic flows.\n   The path agent associates\
    \ an identifier with the path, which will be\n   included in each message that\
    \ travels down the path and will be used\n   by the policy gateways along the\
    \ path in order to determine how to\n   forward the message.  If the path already\
    \ exists, the path agent uses\n   the preexisting identifier.  However, for new\
    \ paths, the path agent\n   chooses a path identifier that is different from those\
    \ of all other\n   paths that it manages.  The path agent also updates its forwarding\n\
    \   information database to reference the path identifier and modifies\n   its\
    \ search procedure to yield the correct forwarding information\n   database entry\
    \ given the data message header.\n   For new paths, the path agent initiates path\
    \ setup, communicating the\n   policy route, in terms of requested services, constituent\
    \ domains,\n   relevant transit policies, and the connecting virtual gateways,\
    \ to\n   policy gateways in intermediate domains.  Using this information, an\n\
    \   intermediate policy gateway determines whether to accept or refuse\n   the\
    \ path and to which policy gateway to forward the path setup\n   information.\
    \  The path setup procedure allows policy gateways to set\n   up a path in both\
    \ directions simultaneously.  Each intermediate\n   policy gateway, after path\
    \ acceptance, updates its forwarding\n   information database to include an entry\
    \ that associates the path\n   identifier with the appropriate previous and next\
    \ hop policy\n   gateways.  Paths remain in place until they are torn down because\
    \ of\n   failure, expiration, or when resources are scarce, preemption in\n  \
    \ favor of other paths.\n   When a policy gateway in AD Y accepts a path, it notifies\
    \ the source\n   path agent in AD X.  We expect that the source path agent will\n\
    \   normally wait until a path has been successfully established before\n   using\
    \ it to transport data traffic.  However, the IDPR architecture\n   does not preclude\
    \ a path agent from forwarding data messages along a\n   path prior to confirmation\
    \ of successful path establishment.  In this\n   case, the source path agent transmits\
    \ data messages along the path\n   with full knowledge that the path may not yet\
    \ have been successfully\n   established at all intermediate policy gateways and\
    \ thus that these\n   data messages will be immediately discarded by any policy\
    \ gateway not\n   yet able to recognize the path identifier.\n   We note that\
    \ data communication between Hx and Hy may occur over two\n   separate IDPR paths:\
    \ one from AD X to AD Y and one from AD Y to AD X.\n   The reasons are that within\
    \ a domain, hosts know nothing about path\n   agents nor IDPR paths, and path\
    \ agents know nothing about other path\n   agents' existing IDPR paths.  Thus,\
    \ in AD Y, the path agent that\n   terminates the path from AD X may not be the\
    \ same as the path agent\n   that receives traffic from Hy destined for Hx.  In\
    \ this case, receipt\n   of traffic from Hy forces the second path agent to set\
    \ up a new path\n   from AD Y to AD X.\n"
- title: 4.  Accommodating a Large, Heterogeneous Internet
  contents:
  - "4.  Accommodating a Large, Heterogeneous Internet\n   The IDPR architecture must\
    \ be able to accommodate an Internet\n   containing O(10,000) domains, supporting\
    \ diverse source and transit\n   policies.  Thus, we have endowed the IDPR architecture\
    \ with many\n   features that allow it to function effectively in such an\n  \
    \ environment.\n"
- title: 4.1.  Domain Level Routing
  contents:
  - "4.1.  Domain Level Routing\n   The IDPR architecture provides policy routing\
    \ among administrative\n   domains.  In order to construct policy routes, route\
    \ servers require\n   routing information at the domain level only; no intra-domain\
    \ details\n   need be included in IDPR routing information.  The size of the\n\
    \   routing information database maintained by a route server depends not\n  \
    \ on the number of Internet gateways, networks, and links, but on how\n   these\
    \ gateways, networks, and links are grouped into domains and on\n   what services\
    \ they offer.  Therefore, the number of entries in an\n   IDPR routing information\
    \ database depends on the number of domains\n   and the number and size of the\
    \ transit policies supported by these\n   domains.\n   Policy gateways distribute\
    \ IDPR routing information only when\n   detectable inter-domain changes occur\
    \ and may also elect to\n   distribute routing information periodically (for example,\
    \ on the\n   order of once per day) as a backup.  We expect that a pair of policy\n\
    \   gateways within a domain will normally be connected such that when\n   the\
    \ primary intra-domain route between them fails, the intra-domain\n   routing\
    \ procedure will be able to construct an alternate route.\n   Thus, an intra-domain\
    \ failure is unlikely to be visible at the\n   inter-domain level and hence unlikely\
    \ to force an inter-domain\n   routing change.  Therefore, we expect that policy\
    \ gateways will not\n   often generate and distribute IDPR routing information\
    \ messages.\n   IDPR entities rely on intra-domain routing procedures operating\n\
    \   within domains to transport inter-domain messages across domains.\n   Hence,\
    \ IDPR messages must appear well-formed according to the intra-\n   domain routing\
    \ and addressing procedures in each domain traversed.\n   Recall that source authentication\
    \ information (refer to section 3.3.3\n   above) may cover the entire IDPR message.\
    \  Thus, the IDPR portion of\n   such a message cannot be modified at intermediate\
    \ domains along the\n   path without causing source authenticity checks to fail.\
    \  Therefore,\n   at domain boundaries, IDPR messages require encapsulation and\n\
    \   decapsulation according to the routing procedures and addressing\n   schemes\
    \ operating with the given domain.  Only policy gateways and\n   route servers\
    \ must be capable of handling IDPR-specific messages;\n   other gateways and hosts\
    \ simply treat the encapsulated IDPR messages\n   like any other message.  Thus,\
    \ for the Internet to support IDPR, only\n   a small proportion of Internet entities\
    \ require special IDPR\n   software.\n   With domain level routes, many different\
    \ traffic flows may use not\n   only the same policy route but also the same path,\
    \ as long as their\n   source domains, destination domains, and service requirements\
    \ are\n   compatible.  The size of the forwarding information database\n   maintained\
    \ by a policy gateway depends not on the number of Internet\n   hosts but on how\
    \ these hosts are grouped into domains, which hosts\n   intercommunicate, and\
    \ on how much distinction a source domain wishes\n   to preserve among its traffic\
    \ flows.  Therefore, the number of\n   entries in an IDPR forwarding information\
    \ database depends on the\n   number of domains and the number of source policies\
    \ supported by\n   those domains.  Moreover, memory associated with failed, expired,\
    \ or\n   disused paths can be reclaimed for new paths, and thus forwarding\n \
    \  information for many paths can be accommodated in a policy gateway's\n   forwarding\
    \ information database.\n"
- title: 4.2.  Route Generation
  contents:
  - "4.2.  Route Generation\n   Route generation is the most computationally complex\
    \ part of IDPR,\n   because of the number of domains and the number and heterogeneity\
    \ of\n   policies that it must accommodate.  Route servers must generate\n   policy\
    \ routes that satisfy the requested services of the source\n   domains and respect\
    \ the offered services of the transit domains.\n   We distinguish requested qualities\
    \ of service and route generation\n   with respect to them as follows:\n   - Requested\
    \ service limits include upper bounds on route delay, route\n     delay variation,\
    \ and monetary cost for the session and lower bounds\n     on available route\
    \ bandwidth.  Generating a route that must satisfy\n     more than one quality\
    \ of service constraint, for example route delay\n     of no more than X seconds\
    \ and available route bandwidth of no less\n     than Y bits per second, is an\
    \ NP-complete problem.\n   - Optimal requested services include minimum route\
    \ delay, minimum\n     route delay variation, minimum monetary cost for the session,\
    \ and\n     maximum available route bandwidth.  In the worst case, the\n     computational\
    \ complexity of generating a route that is optimal with\n     respect to a given\
    \ requested service is O((N + L) log N) for\n     Dijkstra's shortest path first\
    \ (SPF) search and O(N + (L * L)) for\n     breadth-first (BF) search, where N\
    \ is the number of nodes and L is\n     the number of links in the search graph.\
    \  Multi-criteria\n     optimization, for example finding a route with minimal\
    \ delay\n     variation and minimal monetary cost for the session, may be defined\n\
    \     in several ways.  One approach to multi-criteria optimization is to\n  \
    \   assign each link a single value equal to a weighted sum of the\n     values\
    \ of the individual offered qualities of service and generate a\n     route that\
    \ is optimal with respect to this new criterion.  However,\n     it may not always\
    \ be possible to achieve the desired route\n     generation behavior using such\
    \ a linear combination of qualities of\n     service.\n   To help contain the\
    \ combinatorial explosion of processing and memory\n   costs associated with route\
    \ generation, we supply the following\n   guidelines for generation of suitable\
    \ policy routes:\n   - Each route server should only generate policy routes from\
    \ the\n     perspective of its own domain as source; it need not generate policy\n\
    \     routes for arbitrary source/destination domain pairs.  Thus, we can\n  \
    \   distribute the computational burden over all route servers.\n   - Route servers\
    \ should precompute routes for which they anticipate\n     requests and should\
    \ generate routes on demand only in order to\n     satisfy unanticipated route\
    \ requests.  Hence, a single route server\n     can distribute its computational\
    \ burden over time.\n   - Route servers should cache the results of route generation,\
    \ in order\n     to minimize the computation associated with responding to future\n\
    \     route requests.\n   - To handle requested service limits, a route server\
    \ should always\n     select the first route generated that satisfies all of the\
    \ requested\n     service limits.\n   - To handle multi-criteria optimization\
    \ in route selection, a route\n     server should generate routes that are optimal\
    \ with respect to the\n     first specified optimal requested service listed in\
    \ the source\n     policy.  The route server should resolve ties between otherwise\n\
    \     equivalent routes by evaluating these routes according to the other\n  \
    \   optimal requested services, in the order in which they are\n     specified.\
    \  With respect to the route server's routing information\n     database, the\
    \ selected route is optimal according to the first\n     optimal requested service\
    \ but is not necessarily optimal according\n     to any other optimal requested\
    \ service.\n   - To handle a mixture of requested service limits and optimal\n\
    \     requested services, a route server should generate routes that\n     satisfy\
    \ all of the requested service limits.  The route server\n     should resolve\
    \ ties between otherwise equivalent routes by\n     evaluating those routes as\
    \ described in the multi-criteria\n     optimization case above.\n   - All else\
    \ being equal, a route server should always prefer\n     minimum-hop routes, because\
    \ they minimize the amount of network\n     resources consumed by the routes.\n\
    \   All domains need not execute the identical route generation\n   procedure.\
    \  Each domain administrator is free to specify the IDPR\n   route generation\
    \ procedure for route servers in its own domain,\n   making the procedure as simple\
    \ or as complex as desired.\n"
- title: 4.3.  SuperDomains
  contents:
  - "4.3.  SuperDomains\n   A \"super domain\" is itself an administrative domain,\
    \ comprising a set\n   of contiguous domains with similar transit policies and\
    \ formed\n   through consensus of the administrators of the constituent domains.\n\
    \   Super domains provide a mechanism for reducing the amount of IDPR\n   routing\
    \ information distributed throughout the Internet.  Given a set\n   of n contiguous\
    \ domains with consistent transit policies, the amount\n   of routing information\
    \ associated with the set is approximately n\n   times smaller when the set is\
    \ considered as a single super domain\n   than when it is considered as n individual\
    \ domains.\n   When forming a super domain from constituent domains whose transit\n\
    \   policies do not form a consistent set, one must determine which\n   transit\
    \ policies to distribute in the routing information for the\n   super domain.\
    \  The range of possibilities is bounded by the following\n   two alternatives,\
    \ each of which reduces the amount of routing\n   information associated with\
    \ the set of constituent domains:\n   - The transit policies supported by the\
    \ super domain are derived from\n     the union of the access restrictions and\
    \ the intersection of the\n     qualities of service, over all constituent domains.\
    \  In this case,\n     the formation of the super domain reduces the number of\
    \ services\n     offered by the constituent domains, but guarantees that none\
    \ of\n     these domains' access restrictions are violated.\n   - The transit\
    \ policies supported by the super domain are derived from\n     the intersection\
    \ of the access restrictions and the union of the\n     qualities of service.\
    \  In this case, the formation of the super\n     domain increases the number\
    \ of services offered by the constituent\n     domains, but forces relaxation\
    \ of these domains' access\n     restrictions.\n   Thus, we recommend that domain\
    \ administrators refrain from\n   arbitrarily grouping domains into super domains,\
    \ unless they fully\n   understand the consequences.\n   The existence of super\
    \ domains imposes a hierarchy on domains within\n   the Internet.  For model consistency,\
    \ we assume that there is a\n   single super domain at the top of the hierarchy,\
    \ which contains the\n   set of all high-level domains.  A domain's identity is\
    \ defined\n   relative to the domain hierarchy.  Specifically, a domain's identity\n\
    \   may be defined in terms of the domains containing it, the domains it\n   contains,\
    \ or both.\n   For any domain AD X, the universe of distribution for its routing\n\
    \   information usually extends only to those domains contained in AD X's\n  \
    \ immediate super domain and at the same level of the hierarchy as AD\n   X. \
    \ However, the IDPR architecture does not preclude AD X from\n   distributing\
    \ its routing information to domains at arbitrarily high\n   levels in the hierarchy,\
    \ as long as the immediate super domain of\n   these domains is also a super domain\
    \ of AD X.  For example, the\n   administrator of an individual domain within\
    \ a super domain may wish\n   to have one of its transit policies advertised outside\
    \ of the\n   immediate super domain, so that other domains can take advantage\
    \ of a\n   quality of service not offered by the super domain itself.  In this\n\
    \   case, the super domain and the consituent domain may distribute\n   routing\
    \ information at the same level in the domain hierarchy, even\n   though one domain\
    \ actually contains the other.\n   We note that the existence of super domains\
    \ may restrict the number\n   of routes available to source domains with access\
    \ restrictions.  For\n   example, suppose that a source domain AD X has source\
    \ policies that\n   preclude its traffic from traversing a domain AD Y and that\
    \ AD Y is\n   contained in a super domain AD Z.  If domains within AD Z do not\n\
    \   advertise routing information separately, then route servers within\n   AD\
    \ X do not have enough routing information to construct routes that\n   traverse\
    \ AD Z but that avoid AD Y.  Hence, route servers in AD X must\n   generate routes\
    \ that avoid AD Z altogether.\n"
- title: 4.4.  Domain Communities
  contents:
  - "4.4.  Domain Communities\n   A \"domain community\" is a group of domains to\
    \ which a given domain\n   distributes routing information, and hence domain communities\
    \ may be\n   used to limit routing information distribution.  Domain communities\n\
    \   not only reduce the costs associated with distributing and storing\n   routing\
    \ information but also allow concealment of routing information\n   from domains\
    \ outside of the community.  Unlike a super domain, a\n   domain community is\
    \ not necessarily an administrative domain.\n   However, formation of a domain\
    \ community may or may not involve the\n   consent of the administrators of the\
    \ member domains, and the\n   definition of the community may be implicit or explicit.\n\
    \   Each domain administrator determines the extent of distribution of\n   its\
    \ domain's routing information and hence unilaterally defines a\n   domain community.\
    \  By default, this community encompasses all\n   Internet domains.  However,\
    \ the domain administrator may restrict\n   community membership by describing\
    \ the community as a neighborhood\n   (defined, for example, in terms of domain\
    \ hops) or as a list of\n   member domains.\n   A group of domain administrators\
    \ may mutually agree on distribution\n   of their domains' routing information\
    \ among their domains and hence\n   multilaterally define a domain community.\
    \  By default, this community\n   encompasses all Internet domains.  However,\
    \ the domain administrators\n   may restrict community membership by describing\
    \ the community as a\n   list of member domains.  In fact, this domain community\
    \ may serve as\n   a multicast group for routing information distribution.\n"
- title: 4.5.  Robustness in the Presence of Failures
  contents:
  - "4.5.  Robustness in the Presence of Failures\n   The IDPR architecture possesses\
    \ the following features that make it\n   resistent to failures in the Internet:\n\
    \   - Multiple connections between adjacent policy gateways in a virtual\n   \
    \  gateway and between peer and neighbor policy gateways across an\n     administrative\
    \ domain minimize the number of single component\n     failures that are visible\
    \ at the inter-domain level.\n   - Policy gateways distribute IDPR routing information\
    \ immediately\n     after detecting a connectivity failure at the inter-domain\
    \ level,\n     and route servers immediately incorporate this information into\n\
    \     their routing information databases.  This ensures that new policy\n   \
    \  routes will not include those domains involved in the connectivity\n     failure.\n\
    \   - The routing information database query/response mechanism ensures\n    \
    \ rapid updating of the routing information database for a previously\n     failed\
    \ route server following the route server's reconnection to the\n     Internet.\n\
    \   - To minimize user service disruption following a\n     failure in the primary\
    \ path, policy gateways attempt local path\n     repair immediately after detecting\
    \ a connectivity failure.\n     Moreover, path agents may maintain standby alternate\
    \ paths that can\n     become the primary path if necessary.\n   - Policy gateways\
    \ within a domain continuously monitor domain\n     connectivity and hence can\
    \ detect and identify domain partitions.\n     Moreover, IDPR can continue to\
    \ operate properly in the presence of\n     partitioned domains.\n"
- title: 4.5.1.  Path Repair
  contents:
  - "4.5.1.  Path Repair\n   Failure of one or more entities on a given policy route\
    \ may render\n   the route unusable.  If the failure is within a domain, IDPR\
    \ relies\n   on the intra-domain routing procedure to find an alternate route\n\
    \   across the domain, which leaves the path unaffected.  If the failure\n   is\
    \ in a virtual gateway, policy gateways must bear the responsibility\n   of repairing\
    \ the path.  Policy gateways nearest to the failure are\n   the first to recognize\
    \ its existence and hence can react most quickly\n   to repair the path.\n   Relinquishing\
    \ control over path repair to policy gateways in other\n   domains may be unacceptable\
    \ to some domain administrators.  The\n   reason is that these policy gateways\
    \ cannot guarantee construction of\n   a path that satisfies the source policies\
    \ of the source domain, as\n   they have no knowledge of other domains' source\
    \ policies.\n   Nevertheless, limited local path repair is feasible, without\n\
    \   distributing either source policy information throughout the Internet\n  \
    \ or detailed path information among policy gateways in a domain or in\n   a virtual\
    \ gateway.  We say that a path is \"locally repairable\" if\n   there exists an\
    \ alternate route between two policy gateways,\n   separated by at most one policy\
    \ gateway, on the path.  This\n   definition covers path repair in the presence\
    \ of failed routes\n   between consecutive policy gateways as well as failed policy\
    \ gateways\n   themselves.\n   A policy gateway attempts local path repair, proceeding\
    \ in the\n   forward direction of the path, upon detecting that the next policy\n\
    \   gateway on a path is no longer reachable.  The policy gateway must\n   retain\
    \ enough of the original path setup information to repair the\n   path locally.\
    \  Using the path setup information, the policy gateway\n   attempts to locate\
    \ a route around the unreachable policy gateway.\n   Specifically, the policy\
    \ gateway attempts to establish contact with\n   either:\n   - A peer of the unreachable\
    \ policy gateway.  In this case, the\n     contacted policy gateway attempts to\
    \ locate the next policy gateway\n     following the unreachable policy gateway,\
    \ on the original path.\n   - A peer of itself, if the unreachable policy gateway\
    \ is an adjacent\n     policy gateway and if the given policy gateway no longer\
    \ has direct\n     connections to any adjacent policy gateways.  In this case,\
    \ the\n     contacted policy gateway attempts to locate a peer of the\n     unreachable\
    \ policy gateway, which in turn attempts to locate the\n     next policy gateway\
    \ following the unreachable policy gateway, on the\n     original path.\n   If\
    \ it successfully reaches the next policy gateway, the contacted\n   policy gateway\
    \ informs the requesting policy gateway.  In this case,\n   the requesting, contacted,\
    \ and next policy gateways update their\n   forwarding information databases to\
    \ conform to the new part of the\n   path.  If it does not successfully reach\
    \ the next policy gateway, the\n   contacted policy gateway initiates teardown\
    \ of the original path; in\n   this case, the source path agent is responsible\
    \ for finding a new\n   route to the destination.\n"
- title: 4.5.2.  Partitions
  contents:
  - "4.5.2.  Partitions\n   A \"domain partition\" exists whenever there are at least\
    \ two entities\n   within the domain that can no longer communicate over any intra-\n\
    \   domain route.  Domain partitions not only disrupt intra-domain\n   communication\
    \ but also may interfere with inter-domain communication,\n   particularly when\
    \ the partitioned domain is a transit domain.\n   Therefore, we have designed\
    \ the IDPR architecture to permit effective\n   use of partitioned domains and\
    \ hence maximize Internet connectivity\n   in the presence of domain partitions.\n\
    \   When a domain is partitioned, it becomes a set of multiple distinct\n   \"\
    components\".  A domain component is a subset of the domain's\n   entities such\
    \ that all entities within the subset are mutually\n   reachable via intra-domain\
    \ routes, but no entities in the complement\n   of the subset are reachable via\
    \ intra-domain routes from entities\n   within the subset.  Each domain component\
    \ has a unique identifier,\n   namely the identifier of the domain together with\
    \ the ordinal number\n   of the lowest-numbered operational policy gateway within\
    \ the domain\n   component.  No negotiation among policy gateways is necessary\
    \ to\n   determine the domain component's lowest-numbered operational policy\n\
    \   gateway.  Instead, within each domain component, all policy gateway\n   members\
    \ discover mutual reachability through intra-domain\n   reachability information.\
    \  Therefore, all members have a consistent\n   view of which is the lowest-numbered\
    \ operational policy gateway in\n   the component.\n   IDPR entities can detect\
    \ and compensate for all domain partitions\n   that isolate at least two groups\
    \ of policy gateways from each other.\n   They cannot, however, detect any domain\
    \ partition that isolates\n   groups of hosts only.  Note that a domain partition\
    \ may segregate\n   portions of a virtual gateway, such that peer policy gateways\
    \ lie in\n   separate domain components.  Although itself partitioned, the virtual\n\
    \   gateway does not assume any additional identities.  However, from the\n  \
    \ perspective of the adjacent domain, the virtual gateway now connects\n   to\
    \ two separate domain components.\n   Policy gateways use partition information\
    \ to select routes across\n   virtual gateways to the correct domain components.\
    \  They also\n   distribute partition information to route servers as part of\
    \ the IDPR\n   routing information.  Thus, route servers know which domains are\n\
    \   partitioned.  However, route servers do not know which hosts reside\n   in\
    \ which components of a partitioned domain; tracking this\n   information would\
    \ require extensive computation and communication.\n   Instead, when a route server\
    \ discovers that the destination of a\n   requested route is a partitioned domain,\
    \ it attempts to generate a\n   suitable policy route to each component of the\
    \ destination domain.\n   Generation of multiple routes, on detection of a partitioned\n\
    \   destination domain, maximizes the chances of obtaining at least one\n   policy\
    \ route that can be used for communication between the source\n   and destination\
    \ hosts.\n   5.  References\n   [1]  Rekhter, Y., \"EGP and Policy Based Routing\
    \ in the New NSFNET\n        Backbone\", RFC 1092, February 1989.\n   [2]  Clark,\
    \ D., \"Policy Routing in Internet Protocols\", RFC 1102, May\n        1989.\n\
    \   [3]  Braun, H-W., \"Models of Policy Based Routing\", RFC 1104, June\n   \
    \     1989.\n   [4]  Leiner, B., \"Policy Issues in Interconnecting Networks\"\
    , RFC\n        1124, September 1989.\n   [5]  Estrin, D., \"Requirements for Policy\
    \ Based Routing in the\n        Research Internet\", RFC 1125, November 1989.\n\
    \   [6]  Little, M., \"Goals and Functional Requirements for Inter-\n        Autonomous\
    \ System Routing\", RFC 1126, July 1989.\n   [7]  Honig, J., Katz, D., Mathis,\
    \ M., Rekhter, Y., and Yu, J.,\n        \"Application of the Border Gateway Protocol\
    \ in the Internet\",\n        RFC 1164, June 1990.\n   [8]  Lougheed, K. and Rekhter,\
    \ Y., \"A Border Gateway Protocol 3\n        (BGP-3)\", RFC 1267, October 1991.\n\
    \   [9]  Rekhter, Y. and Li, T. Editors, \"A Border Gateway Protocol 4\n     \
    \   (BGP-4)\", Work in Progress, September 1992.\n   [10] ISO, \"Information Processing\
    \ Systems - Telecommunications and\n        Information Exchange between Systems\
    \ - Protocol for Exchange of\n        Inter-domain Routeing Information among\
    \ Intermediate Systems to\n        Support Forwarding of ISO 8473 PDUs\", ISO/IEC\
    \ DIS 10747, August\n        1992.\n   [11] Perlman, R., \"Network Layer Protocols\
    \ with Byzantine Robust-\n        ness\", Ph.D. Thesis, Department of Electrical\
    \ Engineering and\n        Computer Science, MIT, August 1988.\n   [12] Estrin,\
    \ D. and Tsudik, G., \"Secure Control of Transit Internet-\n        work Traffic\"\
    , TR-89-15, Computer Science Department, University\n        of Southern California.\n\
    \   [13] Garcia-Luna-Aceves, J.J., \"A Unified Approach for Loop-Free\n      \
    \  Routing using Link States or Distance Vectors\", ACM Computer\n        Communication\
    \ Review, Vol. 19, No. 4, SIGCOMM 1989, pp. 212-223.\n   [14] Zaumen, W.T. and\
    \ Garcia-Luna-Aceves, J.J., \"Dynamics of Distri-\n        buted Shortest-Path\
    \ Routing Algorithms\", ACM Computer Communica-\n        tion Review, Vol. 21,\
    \ No. 4, SIGCOMM 1991, pp. 31-42.\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n        Refer to section 3.3 for details on security\
    \ in IDPR.\n"
- title: 7.  Author's Address
  contents:
  - "7.  Author's Address\n        Martha Steenstrup\n        BBN Systems and Technologies\n\
    \        10 Moulton Street\n        Cambridge, MA 02138\n        Phone: (617)\
    \ 873-3192\n        Email: msteenst@bbn.com\n"
