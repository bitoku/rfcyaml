- title: __initial_text__
  contents:
  - '                   Gateway Congestion Control Survey

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It is a\n   survey of some of the major directions and issues.  It does not\n\
    \   specify an Internet standard.  Distribution of this memo is\n   unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   The growth of network intensive Internet applications has made\n\
    \   gateway congestion control a high priority.  The IETF Performance and\n  \
    \ Congestion Control Working Group surveyed and reviewed gateway\n   congestion\
    \ control and avoidance approaches.  The purpose of this\n   paper is to present\
    \ our review of the congestion control approaches,\n   as a way of encouraging\
    \ new discussion and experimentation.  Included\n   in the survey are Source Quench,\
    \ Random Drop, Congestion Indication\n   (DEC Bit), and Fair Queueing.  The task\
    \ remains for Internet\n   implementors to determine and agree on the most effective\
    \ mechanisms\n   for controlling gateway congestion.\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Internet users regularly encounter congestion, often in\
    \ mild forms.\n   However, severe congestion episodes have been reported also;\
    \ and\n   gateway congestion remains an obstacle for Internet applications such\n\
    \   as scientific supercomputing data transfer.  The need for Internet\n   congestion\
    \ control originally became apparent during several periods\n   of 1986 and 1987,\
    \ when the Internet experienced the \"congestion\n   collapse\" condition predicted\
    \ by Nagle [Nag84].  A large number of\n   widely dispersed Internet sites experienced\
    \ simultaneous slowdown or\n   cessation of networking services for prolonged\
    \ periods.  BBN, the\n   firm responsible for maintaining the then backbone of\
    \ the Internet,\n   the ARPANET, responded to the collapse by adding link capacity\n\
    \   [Gar87].\n   Much of the Internet now uses as a transmission backbone the\
    \ National\n   Science Foundation Network (NSFNET). Extensive monitoring and\n\
    \   capacity planning are being done for the NSFNET backbone; still, as\n   the\
    \ demand for this capacity grows, and as resource-intensive\n   applications such\
    \ as wide-area file system management [Sp89]\n   increasingly use the backbone,\
    \ effective congestion control policies\n   will be a critical requirement.\n\
    \   Only a few mechanisms currently exist in Internet hosts and gateways\n   to\
    \ avoid or control congestion.  The mechanisms for handling\n   congestion set\
    \ forth in the specifications for the DoD Internet\n   protocols are limited to:\n\
    \      Window flow control in TCP [Pos81b], intended primarily for\n      controlling\
    \ the demand on the receiver's capacity, both in terms\n      of processing and\
    \ buffers.\n      Source quench in ICMP, the message sent by IP to request that\
    \ a\n      sender throttle back [Pos81a].\n   One approach to enhancing Internet\
    \ congestion control has been to\n   overlay the simple existing mechanisms in\
    \ TCP and ICMP with more\n   powerful ones.  Since 1987, the TCP congestion control\
    \ policy, Slow-\n   start, a collection of several algorithms developed by Van\
    \ Jacobson\n   and Mike Karels [Jac88], has been widely adopted. Successful Internet\n\
    \   experiences with Slow-start led to the Host Requirements RFC [HREQ89]\n  \
    \ classifying the algorithms as mandatory for TCP.  Slow-start modifies\n   the\
    \ user's demand when congestion reaches such a point that packets\n   are dropped\
    \ at the gateway.  By the time such overflows occur, the\n   gateway is congested.\
    \  Jacobson writes that the Slow-start policy is\n   intended to function best\
    \ with a complementary gateway policy\n   [Jac88].\n"
- title: 1.1  Definitions
  contents:
  - "1.1  Definitions\n   The characteristics of the Internet that we are interested\
    \ in include\n   that it is, in general, an arbitrary mesh-connected network.\
    \  The\n   internetwork protocol is connectionless.  The number of users that\n\
    \   place demands on the network is not limited by any explicit\n   mechanism;\
    \ no reservation of resources occurs and transport layer\n   set-ups are not disallowed\
    \ due to lack of resources.  A path from a\n   source to destination host may\
    \ have multiple hops, through several\n   gateways and links.  Paths through the\
    \ Internet may be heterogeneous\n   (though homogeneous paths also exist and experience\
    \ congestion).\n   That is, links may be of different speeds.  Also, the gateways\
    \ and\n   hosts may be of different speeds or may be providing only a part of\n\
    \   their processing power to communication-related activity.  The\n   buffers\
    \ for storing information flowing through Internet gateways are\n   finite.  The\
    \ nature of the internet protocol is to drop packets when\n   these buffers overflow.\n\
    \   Gateway congestion arises when the demand for one or more of the\n   resources\
    \ of the gateway exceeds the capacity of that resource.  The\n   resources include\
    \ transmission links, processing, and space used for\n   buffering.  Operationally,\
    \ uncongested gateways operate with little\n   queueing on average, where the\
    \ queue is the waiting line for a\n   particular resource of the gateway.  One\
    \ commonly used quantitative\n   definition [Kle79] for when a resource is congested\
    \ is when the\n   operating point is greater than the point at which resource\
    \ power is\n   maximum, where resource power is defined as the ratio of throughput\n\
    \   to delay (See Section 2.2).  At this operating point, the average\n   queue\
    \ size is close to one, including the packet in service.  Note\n   that this is\
    \ a long-term average queue size.  Several definitions\n   exist for the timescale\
    \ of averaging for congestion detection and\n   control, such as dominant round-trip\
    \ time and queue regeneration\n   cycle (see Section 2.1).\n   Mechanisms for\
    \ handling congestion may be divided into two\n   categories, congestion recovery\
    \ and congestion avoidance.  Congestion\n   recovery tries to restore an operating\
    \ state, when demand has already\n   exceeded capacity.  Congestion avoidance\
    \ is preventive in nature.  It\n   tries to keep the demand on the network at\
    \ or near the point of\n   maximum power, so that congestion never occurs.  Without\
    \ congestion\n   recovery, the network may cease to operate entirely (zero\n \
    \  throughput), whereas the Internet has been operating without\n   congestion\
    \ avoidance for a long time.  Overall performance may\n   improve with an effective\
    \ congestion avoidance mechanism.  Even if\n   effective congestion avoidance\
    \ was in place, congestion recovery\n   schemes would still be required, to retain\
    \ throughput in the face of\n   sudden changes (increase of demand, loss of resources)\
    \ that can lead\n   to congestion.\n   In this paper, the term \"user\" refers\
    \ to each individual transport\n   (TCP or another transport protocol) entity.\
    \  For example, a TCP\n   connection is a \"user\" in this terminology.  The terms\
    \ \"flow\" and\n   \"stream\" are used by some authors in the same sense.  Some\
    \ of the\n   congestion control policies discussed in this paper, such as\n  \
    \ Selective Feedback Congestion Indication and Fair Queueing aggregate\n   multiple\
    \ TCP connections from a single host (or between a source\n   host-destination\
    \ host pair) as a virtual user.\n   The term \"cooperating transport entities\"\
    \ will be defined as a set of\n   TCP connections (for example) which follow an\
    \ effective method of\n   adjusting their demand on the Internet in response to\
    \ congestion.\n   The most restrictive interpretation of this term is that the\n\
    \   transport entities follow identical algorithms for congestion control\n  \
    \ and avoidance.  However, there may be some variation in these\n   algorithms.\
    \  The extent to which heterogeneous end-system congestion\n   control and avoidance\
    \ may be accommodated by gateway policies should\n   be a subject of future research.\
    \ The role played in Internet\n   performance of non-cooperating transport entities\
    \ is discussed in\n   Section 5.\n"
- title: 1.2  Goals and Scope of This Paper
  contents:
  - "1.2  Goals and Scope of This Paper\n   The task remains for Internet implementors\
    \ to determine effective\n   mechanisms for controlling gateway congestion.  There\
    \ has been\n   minimal common practice on which to base recommendations for Internet\n\
    \   gateway congestion control.  In this survey, we describe the\n   characteristics\
    \ of one experimental gateway congestion management\n   policy, Random Drop, and\
    \ several that are better-known:  Source\n   Quench, Congestion Indication, Selective\
    \ Feedback Congestion\n   Indication, and Fair Queueing, both Bit-Round and Stochastic.\
    \  A\n   motivation for documenting Random Drop is that it has as primary\n  \
    \ goals low overhead and suitability for scaling up for Internets with\n   higher\
    \ speed links.  Both of these are important goals for future\n   gateway implementations\
    \ that will have fast links, fast processors,\n   and will have to serve large\
    \ numbers of interconnected hosts.\n   The structure of this paper is as follows.\
    \  First, we discuss\n   performance goals, including timescale and fairness considerations.\n\
    \   Second, we discuss the gateway congestion control policies.  Random\n   Drop\
    \ is sketched out, with a recommendation for using it for\n   congestion recovery\
    \ and a separate section on its use as congestion\n   avoidance.  Third, since\
    \ gateway congestion control in itself does\n   not change the end-systems' demand,\
    \ we briefly present the effective\n   responses to these policies by two end-system\
    \ congestion control\n   schemes, Slow-start and End-System Congestion Indication.\
    \  Among our\n   conclusions, we address the issues of transport entities that\
    \ do not\n   cooperate with gateway congestion control.  As an appendix, because\n\
    \   of the potential interactions with gateway congestion policies, we\n   report\
    \ on a scheme to help in controlling the performance of Internet\n   gateways\
    \ to connection-oriented subnets (in particular, X.25).\n   Resources in the current\
    \ Internet are not charged to users of them.\n   Congestion avoidance techniques\
    \ cannot be expected to help when users\n   increase beyond the capacity of the\
    \ underlying facilities.  There are\n   two possible solutions for this, increase\
    \ the facilities and\n   available bandwidth, or forcibly reduce the demand. \
    \ When congestion\n   is persistent despite implemented congestion control mechanisms,\n\
    \   administrative responses are needed.  These are naturally not within\n   the\
    \ scope of this paper.  Also outside the scope of this paper are\n   routing techniques\
    \ that may be used to relocate demand away from\n   congested individual resources\
    \ (e.g., path-splitting and load-\n   balancing).\n"
- title: 2.  Performance Goals
  contents:
  - "2.  Performance Goals\n   To be able to discuss design and use of various mechanisms\
    \ for\n   improving Internetwork performance, we need to have clear performance\n\
    \   goals for the operation of gateways and sets of end-systems.\n   Internet\
    \ experience shows that congestion control should be based on\n   adaptive principles;\
    \ this requires efficient computation of metrics\n   by algorithms for congestion\
    \ control.  The first issue is that of the\n   interval over which these metrics\
    \ are estimated and/or measured.\n"
- title: 2.1  Interval for Measurement/Estimation of Performance Metrics
  contents:
  - "2.1  Interval for Measurement/Estimation of Performance Metrics\n   Network performance\
    \ metrics may be distorted if they are computed\n   over intervals that are too\
    \ short or too long relative to the dynamic\n   characteristics of the network.\
    \  For instance, within a small\n   interval, two FTP users with equal paths may\
    \ appear to have sharply\n   different demands, as an effect of brief, transient\
    \ fluctuations in\n   their respective processing.  An overly long averaging interval\n\
    \   results in distortions because of the changing number of users\n   sharing\
    \ the resource measured during the time.  It is similarly\n   important for congestion\
    \ control mechanisms exerted at end systems to\n   find an appropriate interval\
    \ for control.\n   The first approach to the monitoring, or averaging, interval\
    \ for\n   congestion control is one based on round-trip times.  The rationale\n\
    \   for it is as follows:  control mechanisms must adapt to changes in\n   Internet\
    \ congestion as quickly as possible.  Even on an uncongested\n   path, changed\
    \ conditions will not be detected by the sender faster\n   than a round-trip time.\
    \  The effect of a sending end-system's control\n   will also not be seen in less\
    \ than a round-trip time in the entire\n   path as well as at the end systems.\
    \  For the control mechanism to be\n   adaptive, new information on the path is\
    \ needed before making a\n   modification to the control exerted.  The statistics\
    \ and metrics used\n   in congestion control must be able to provide information\
    \ to the\n   control mechanism so that it can make an informed decision.\n   Transient\
    \ information which may be obsolete before a change is made\n   by the end-system\
    \ should be avoided.  This implies the\n   monitoring/estimating interval is one\
    \ lasting one or more round\n   trips.  The requirements described here give bounds\
    \ on:\n      How short an interval:  not small enough that obsolete information\n\
    \      is used for control;\n      How long:  not more than the period at which\
    \ the end-system makes\n      changes.\n   But, from the point of view of the\
    \ gateway congestion control policy,\n   what is a round-trip time?  If all the\
    \ users of a given gateway have\n   the same path through the Internet, they also\
    \ have the same round-\n   trip time through the gateway.  But this is rarely\
    \ the case.\n   A meaningful interval must be found for users with both short\
    \ and\n   long paths. Two approaches have been suggested for estimating this\n\
    \   dynamically, queue regeneration cycle and frequency analysis.\n   Use of the\
    \ queue regeneration cycle has been described as part of the\n   Congestion Indication\
    \ policy.  The time period used for averaging\n   here begins when a resource\
    \ goes from the idle to busy state.  The\n   basic interval for averaging is a\
    \ \"regeneration cycle\" which is in\n   the form of busy and idle intervals.\
    \  Because an average based on a\n   single previous regeneration may become old\
    \ information, the\n   recommendation in [JRC87] is to average over the sum of\
    \ two\n   intervals, that is, the previous (busy and idle) period, and the time\n\
    \   since the beginning of the current busy period.\n   If the gateway users are\
    \ window-based transport entities, it is\n   possible to see how the regeneration\
    \ interval responds to their\n   round-trip times.  If a user with a long round-trip\
    \ time has the\n   dominant traffic, the queue length may be zero only when that\
    \ user is\n   awaiting acknowledgements.  Then the users with short paths will\n\
    \   receive gateway congestion information that is averaged over several\n   of\
    \ their round-trip times.  If the short path traffic dominates the\n   activity\
    \ in the gateway, i.e., the connections with shorter round-\n   trip times are\
    \ the dominant users of the gateway resources, then the\n   regeneration interval\
    \ is shorter and the information communicated to\n   them can be more timely.\
    \ In this case, users with longer paths\n   receive, in one of their round-trip\
    \ times, multiple samples of the\n   dominant traffic; the end system averaging\
    \ is based on individual\n   user's intervals, so that these multiple samples\
    \ are integrated\n   appropriately for these connections with longer paths.\n\
    \   The use of frequency analysis has been described by [Jac89]. In this\n   approach,\
    \ the gateway congestion control is done at intervals based\n   on spectral analysis\
    \ of the traffic arrivals.  It is possible for\n   users to have round-trip times\
    \ close to each other, but be out of\n   phase from each other. A spectral analysis\
    \ algorithm detects this.\n   Otherwise, if multiple round-trip times are significant,\
    \ multiple\n   intervals will be identified.  Either one of these will be\n  \
    \ predominant, or several will be comparable. An as yet difficult\n   problem\
    \ for the design of algorithms accomplishing this technique is\n   the likelihood\
    \ of \"locking\" to the frequency of periodic traffic of\n   low intensity, such\
    \ as routing updates.\n"
- title: 2.2  Power and its Relationship to the Operating Point
  contents:
  - "2.2  Power and its Relationship to the Operating Point\n   Performance goals\
    \ for a congestion control/avoidance strategy embody\n   a conflict in that they\
    \ call for as high a throughput as possible,\n   with as little delay as possible.\
    \  A measure that is often used to\n   reflect the tradeoff between these goals\
    \ is power, the ratio of\n   throughput to delay.  We would like to maximize the\
    \ value of power\n   for a given resource.  In the standard expression for power,\n\
    \     Power = (Throughput^alpha)/Delay\n   the exponent alpha is chosen for throughput,\
    \ based on the relative\n   emphasis placed on throughput versus delay: if throughput\
    \ is more\n   important, then a value of A alpha greater than one is chosen. \
    \ If\n   throughput and delay are equally important (e.g., both bulk transfer\n\
    \   traffic and interactive traffic are equally important), then alpha\n   equal\
    \ to one is chosen. The operating point where power is maximized\n   is the \"\
    knee\" in the throughput and delay curves.  It is desirable\n   that the operating\
    \ point of the resource be driven towards the knee,\n   where power is maximized.\
    \  A useful property of power is that it is\n   decreasing whether the resource\
    \ is under- or over-utilized relative\n   to the knee.\n   In an internetwork\
    \ comprising nodes and links of diverse speeds and\n   utilization, bottlenecks\
    \ or concentrations of demand may form.  Any\n   particular user can see a single\
    \ bottleneck, which is the slowest or\n   busiest link or gateway in the path\
    \ (or possibly identical \"balanced\"\n   bottlenecks).  The throughput that the\
    \ path can sustain is limited by\n   the bottleneck.  The delay for packets through\
    \ a particular path is\n   determined by the service times and queueing at each\
    \ individual hop.\n   The queueing delay is dominated by the queueing at the bottleneck\n\
    \   resource(s).  The contribution to the delay over other hops is\n   primarily\
    \ the service time, although the propagation delay over\n   certain hops, such\
    \ as a satellite link, can be significant.  We would\n   like to operate all shared\
    \ resources at their knee and maximize the\n   power of every user's bottleneck.\n\
    \   The above goal underscores the significance of gateway congestion\n   control.\
    \  If techniques can be found to operate gateways at their\n   resource knee,\
    \ it can improve Internet performance broadly.\n"
- title: 2.3  Fairness
  contents:
  - "2.3  Fairness\n   We would like gateways to allocate resources fairly to users.\
    \  A\n   concept of fairness is only relevant when multiple users share a\n  \
    \ gateway and their total demand is greater than its capacity.  If\n   demands\
    \ were equal, a fair allocation of the resource would be to\n   provide an equal\
    \ share to each user.  But even over short intervals,\n   demands are not equal.\
    \  Identifying the fair share of the resource\n   for the user becomes hard. \
    \ Having identified it, it is desirable to\n   allocate at least this fair share\
    \ to each user.  However, not all\n   users may take advantage of this allocation.\
    \  The unused capacity can\n   be given to other users.  The resulting final allocation\
    \ is termed a\n   maximally fair allocation.  [RJC87] gives a quantitative method\
    \ for\n   comparing the allocation by a given policy to the maximally fair\n \
    \  allocation.\n   It is known that the Internet environment has heterogeneous\
    \ transport\n   entities, which do not follow the same congestion control policies\n\
    \   (our definition of cooperating transports). Then, the controls given\n   by\
    \ a gateway may not affect all users and the congestion control\n   policy may\
    \ have unequal effects.  Is \"fairness\" obtainable in such a\n   heterogeneous\
    \ community?  In Fair Queueing, transport entities with\n   differing congestion\
    \ control policies can be insulated from each\n   other and each given a set share\
    \ of gateway bandwidth.\n   It is important to realize that since Internet gateways\
    \ cannot refuse\n   new users, fairness in gateway congestion control can lead\
    \ to all\n   users receiving small (sub-divided) amounts of the gateway resources\n\
    \   inadequate to meet their performance requirements.  None of the\n   policies\
    \ described in this paper currently addresses this.  Then,\n   there may be policy\
    \ reasons for unequal allocation of the gateway\n   resources.  This has been\
    \ addressed by Bit-Round Fair Queueing.\n"
- title: 2.4  Network Management
  contents:
  - "2.4  Network Management\n   Network performance goals may be assessed by measurements\
    \ in either\n   the end-system or gateway frame of reference.  Performance goals\
    \ are\n   often resource-centered and the measurement of the global performance\n\
    \   of \"the network,\" is not only difficult to measure but is also\n   difficult\
    \ to define.  Resource-centered metrics are more easily\n   obtained, and do not\
    \ require synchronization.  That resource-centered\n   metrics are appropriate\
    \ ones for use in optimization of power is\n   shown by [Jaf81].\n   It would\
    \ be valuable for the goal of developing effective gateway\n   congestion handling\
    \ if Management Information Base (MIB) objects\n   useful for evaluating gateway\
    \ congestion were developed.  The\n   reflections on the control interval described\
    \ above should be applied\n   when network management applications are designed\
    \ for this purpose.\n   In particular, obtaining an instantaneous queue length\
    \ from the\n   managed gateway is not meaningful for the purposes of congestion\n\
    \   management.\n"
- title: 3.  Gateway Congestion Control Policies
  contents:
  - "3.  Gateway Congestion Control Policies\n   There have been proposed a handful\
    \ of approaches to dealing with\n   congestion in the gateway. Some of these are\
    \ Source Quench, Random\n   Drop, Congestion Indication, Selective Feedback Congestion\n\
    \   Indication, Fair Queueing, and Bit-Round Fair Queueing.  They differ\n   in\
    \ whether they use a control message, and indeed, whether they view\n   control\
    \ of the end-systems as necessary, but none of them in itself\n   lowers the demand\
    \ of users and consequent load on the network.  End-\n   system policies that\
    \ reduce demand in conjunction with gateway\n   congestion control are described\
    \ in Section 4.\n"
- title: 3.1  Source Quench
  contents:
  - "3.1  Source Quench\n   The method of gateway congestion control currently used\
    \ in the\n   Internet is the Source Quench message of the RFC-792 [Pos81a]\n \
    \  Internet Control Message Protocol (ICMP). When a gateway responds to\n   congestion\
    \ by dropping datagrams, it may send an ICMP Source Quench\n   message to the\
    \ source of the dropped datagram.  This is a congestion\n   recovery policy.\n\
    \   The Gateway Requirements RFC, RFC-1009 [GREQ87], specifies that\n   gateways\
    \ should only send Source Quench messages with a limited\n   frequency, to conserve\
    \ CPU resources during the time of heavy load.\n   We note that operating the\
    \ gateway for long periods under such loaded\n   conditions should be averted\
    \ by a gateway congestion control policy.\n   A revised Gateway Requirements RFC\
    \ is being prepared by the IETF.\n   Another significant drawback of the Source\
    \ Quench policy is that its\n   details are discretionary, or, alternatively,\
    \ that the policy is\n   really a family of varied policies.  Major Internet gateway\n\
    \   manufacturers have implemented a variety of source quench\n   frequencies.\
    \  It is impossible for the end-system user on receiving a\n   Source Quench to\
    \ be certain of the circumstances in which it was\n   issued.  This makes the\
    \ needed end-system response problematic:  is\n   the Source Quench an indication\
    \ of heavy congestion, approaching\n   congestion, a burst causing massive overload,\
    \ or a burst slightly\n   exceeding reasonable load?\n   To the extent that gateways\
    \ drop the last arrived datagram on\n   overload, Source Quench messages may be\
    \ distributed unfairly.  This\n   is because the position at the end of the queue\
    \ may be unfairly often\n   occupied by the packets of low demand, intermittent\
    \ users, since\n   these do not send regular bursts of packets that can preempt\
    \ most of\n   the queue space.\n   [Fin89] developed algorithms for when to issue\
    \ Source Quench and for\n   responding to it with a rate-reduction in the IP layer\
    \ on the sending\n   host.  The system controls end-to-end performance of connections\
    \ in a\n   manner similar to the congestion avoidance portion of Slow-start TCP\n\
    \   [Jac88].\n"
- title: 3.2  Random Drop
  contents:
  - "3.2  Random Drop\n   Random Drop is a gateway congestion control policy intended\
    \ to give\n   feedback to users whose traffic congests the gateway by dropping\n\
    \   packets on a statistical basis.  The key to this policy is the\n   hypothesis\
    \ that a packet randomly selected from all incoming traffic\n   will belong to\
    \ a particular user with a probability proportional to\n   the average rate of\
    \ transmission of that user.  Dropping a randomly\n   selected  packet results\
    \ in users which generate much traffic having\n   a greater number of packets\
    \ dropped compared with those generating\n   little traffic.  The selection of\
    \ packets to be dropped is completely\n   uniform.  Therefore, a user who generates\
    \ traffic of an amount below\n   the \"fair share\" (as defined in Section 2.3)\
    \ may also experience a\n   small amount of packet loss at a congested gateway.\
    \ This character of\n   uniformity is in fact a primary goal that Random Drop\
    \ attempts to\n   achieve.\n   The other primary goal that Random Drop attempts\
    \ to achieve is a\n   theoretical overhead which is scaled to the number of shared\n\
    \   resources in the gateway rather than the number of its users.  If a\n   gateway\
    \ congestion algorithm has more computation the more users\n   there are, this\
    \ can lead to processing demands that are higher as\n   congestion increases.\
    \  Also the low-overhead goal of Random Drop\n   addresses concerns about the\
    \ scale of gateway processing that will be\n   required in the mid-term Internet\
    \ as gateways with fast processors\n   and links are shared by very large active\
    \ sets of users.\n"
- title: 3.2.1  For Congestion Recovery
  contents:
  - "3.2.1  For Congestion Recovery\n   Random Drop has been proposed as an improvement\
    \ to packet dropping at\n   the operating point where the gateway's packet buffers\
    \ overflow.\n   This is using Random Drop strictly as a congestion recovery\n\
    \   mechanism.\n   In Random Drop congestion recovery, instead of dropping the\
    \ last\n   packet to arrive at the queue, a packet is selected randomly from the\n\
    \   queue.  Measurements of an implementation of Random Drop Congestion\n   Recovery\
    \ [Man90] showed that a user with a low demand, due to a\n   longer round-trip\
    \ time path than other users of the gateway, had a\n   higher drop rate with RDCR\
    \ than without.  The throughput accorded to\n   users with the same round-trip\
    \ time paths was nearly equal with RDCR\n   as compared to without it.  These\
    \ results suggest that RDCR should be\n   avoided unless it is used within a scheme\
    \ that groups traffic more or\n   less by round-trip time.\n"
- title: 3.2.2  For Congestion Avoidance
  contents:
  - "3.2.2  For Congestion Avoidance\n   Random Drop is also proposed as a congestion\
    \ avoidance policy\n   [Jac89].  The intent is to initiate dropping packets when\
    \ the gateway\n   is anticipated to become congested and remain so unless there\
    \ is some\n   control exercised.  This  implies  selection  of  incoming packets\
    \ to\n   be randomly dropped at a rate derived from identifying the level of\n\
    \   congestion at the gateway.  The rate is the number of arrivals\n   allowed\
    \ between drops. It depends on the current operating point and\n   the prediction\
    \ of congestion.\n   A part of the policy is to determine that congestion will\
    \ soon occur\n   and that the gateway is beginning to operate beyond the knee\
    \ of the\n   power curve.  With a suitably chosen interval (Section 2.1), the\n\
    \   number of packets from each individual user in a sample over that\n   interval\
    \ is proportional to each user's demand on the gateway.  Then,\n   dropping one\
    \ or more random packets indicates to some user(s) the\n   need to reduce the\
    \ level of demand that is driving the gateway beyond\n   the desired operating\
    \ point.  This is the goal that a policy of\n   Random Drop for congestion avoidance\
    \ attempts to achieve.\n   There are several parameters to be determined for a\
    \ Random Drop\n   congestion avoidance policy. The first is an interval, in terms\
    \ of\n   number of packet arrivals, over which packets are dropped with\n   uniform\
    \ probability.  For instance, in a sample implementation, if\n   this interval\
    \ spanned 2000 packet arrivals, and a suitable\n   probability of drop was 0.001,\
    \ then two random variables would be\n   drawn in a uniform distribution in the\
    \ range of 1 to 2,000.  The\n   values drawn would be used by counting to select\
    \ the packets dropped\n   at arrival.  The second parameter is the value for the\
    \ probability of\n   drop.  This parameter would be a function of an estimate\
    \ of the\n   number of users, their appropriate control intervals, and possibly\n\
    \   the length of time that congestion has persisted.  [Jac89] has\n   suggested\
    \ successively increasing the probability of drop when\n   congestion persists\
    \ over multiple control intervals.  The motivation\n   for increasing the packet\
    \ drop probability is that the implicit\n   estimate of the number of users and\
    \ random selection of their packets\n   to drop does not guarantee that all users\
    \ have received enough\n   signals to decrease demand.  Increasing the probability\
    \ of drop\n   increases the probability that enough feedback is provided.\n  \
    \ Congestion detection is also needed in Random Drop congestion\n   avoidance,\
    \ and could be implemented in a variety of ways.  The\n   simplest is a static\
    \ threshold, but dynamically averaged measures of\n   demand or utilization are\
    \ suggested.\n   The packets dropped in Random Drop congestion avoidance would\
    \ not be\n   from the initial inputs to the gateway.  We suggest that they would\n\
    \   be selected only from packets destined for the resource which is\n   predicted\
    \ to be approaching congestion.  For example, in the case of\n   a gateway with\
    \ multiple outbound links, access to each individual\n   link is treated as a\
    \ separate resource, the Random Drop policy is\n   applied at each link independently.\
    \  Random Drop congestion avoidance\n   would provide uniform treatment of all\
    \ cooperating transport users,\n   even over individual patterns of traffic multiplexed\
    \ within one\n   user's stream.  There is no aggregation of users.\n   Simulation\
    \ studies [Zha89], [Has90] have presented evidence that\n   Random Drop is not\
    \ fair across cooperating and non-cooperating\n   transport users.  A transport\
    \ user whose sending policies included\n   Go-Back-N retransmissions and did not\
    \ include Slow-start received an\n   excessive share of bandwidth from a simple\
    \ implementation of Random\n   Drop.  The simultaneously active Slow-start users\
    \ received unfairly\n   low shares.  Considering this, it can be seen that when\
    \ users do not\n   respond to control, over a prolonged period, the Random Drop\n\
    \   congestion avoidance mechanism would have an increased probability of\n  \
    \ penalizing users with lower demand.  Their packets dropped, these\n   users\
    \ exert the controls leading to their giving up bandwidth.\n   Another problem\
    \ can be seen to arise in Random Drop [She89] across\n   users whose communication\
    \ paths are of different lengths.  If the\n   path spans congested resources at\
    \ multiple gateways, then the user's\n   probability of receiving an unfair drop\
    \ and subsequent control (if\n   cooperating) is exponentially increased.  This\
    \ is a significant\n   scaling problem.\n   Unequal paths cause problems for other\
    \ congestion avoidance policies\n   as well.  Selective Feedback Congestion Indication\
    \ was devised to\n   enhance Congestion Indication specifically because of the\
    \ problem of\n   unequal paths.  In Fair Queueing by source-destination pairs,\
    \ each\n   path gets its own queue in all the gateways.\n"
- title: 3.3  Congestion Indication
  contents:
  - "3.3  Congestion Indication\n   The Congestion Indication policy is often referred\
    \ to as the DEC Bit\n   policy. It was developed at DEC [JRC87], originally for\
    \ the Digital\n   Network Architecture (DNA).  It has also been specified for\
    \ the\n   congestion avoidance of the ISO protocols TP4 and CLNP [NIST88].\n \
    \  Like Source Quench, it uses explicit communications from the\n   congested\
    \ gateway to the user.  However, to use the lowest possible\n   network resources\
    \ for indicating congestion, the information is\n   communicated in a single bit,\
    \ the Congestion Experienced Bit, set in\n   the network header of the packets\
    \ already being forwarded by a\n   gateway.  Based on the condition of this bit,\
    \ the end-system user\n   makes an adjustment to the sending window. In the NSP\
    \ transport\n   protocol of DECNET, the source makes an adjustment to its window;\
    \ in\n   the ISO transport protocol, TP4, the destination makes this\n   adjustment\
    \ in the window offered to the sender.\n   This policy attempts to avoid congestion\
    \ by setting the bit whenever\n   the average queue length over the previous queue\
    \ regeneration cycle\n   plus part of the current cycle is one or more.  The feedback\
    \ is\n   determined independently at each resource.\n"
- title: 3.4  Selective Feedback Congestion Indication
  contents:
  - "3.4  Selective Feedback Congestion Indication\n   The simple Congestion Indication\
    \ policy works based upon the total\n   demand on the gateway.  The total number\
    \ of users or the fact that\n   only a few of the users might be causing congestion\
    \ is not\n   considered.  For fairness, only those users who are sending more\
    \ than\n   their fair share should be asked to reduce their load, while others\n\
    \   could attempt to increase where possible.  In Selective Feedback\n   Congestion\
    \ Indication, the Congestion Experienced Bit is used to\n   carry out this goal.\n\
    \   Selective Feedback works by keeping a count of the number of packets\n   sent\
    \ by different users since the beginning of the queue averaging\n   interval.\
    \  This is equivalent to monitoring their throughputs. Based\n   on the total\
    \ throughput, a fair share for each user is determined and\n   the congestion\
    \ bit is set, when congestion approaches, for the users\n   whose demand is higher\
    \ than their fair share.  If the gateway is\n   operating below the throughput-delay\
    \ knee, congestion indications are\n   not set.\n   A min-max algorithm used to\
    \ determine the fair share of capacity and\n   other details of this policy are\
    \ described in [RJC87].  One parameter\n   to be computed is the capacity of each\
    \ resource to be divided among\n   the users.  This metric depends on the distribution\
    \ of inter-arrival\n   times and packet sizes of the users.  Attempting to determine\
    \ these\n   in real time in the gateway is unacceptable.  The capacity is instead\n\
    \   estimated from on the throughput seen when the gateway is operating\n   in\
    \ congestion, as indicated by the average queue length.  In\n   congestion (above\
    \ the knee), the service rate of the gateway limits\n   its throughput.  Multiplying\
    \ the throughput obtained at this\n   operating point by a capacity factor (between\
    \ 0.5 and 0.9) to adjust\n   for the distributions yields an acceptable capacity\
    \ estimate in\n   simulations.\n   Selective Feedback Congestion Indication takes\
    \ as input a vector of\n   the number of packets sent by each source-destination\
    \ pair of end-\n   systems.  Other alternatives include 1) destination address,\
    \ 2)\n   input/output link, and 3) transport connection (source/destination\n\
    \   addresses and ports).\n   These alternatives give different granularities\
    \ for fairness.  In the\n   case where paths are the same or round-trip times\
    \ of users are close\n   together, throughputs are equalized similarly by basing\
    \ the selective\n   feedback on source or destination address.  In fact, if the\
    \ RTTs are\n   close enough, the simple congestion indication policy would result\
    \ in\n   a fair allocation.  Counts based on source/destination pairs ensure\n\
    \   that paths with different lengths and network conditions get a fair\n   throughput\
    \ at the individual gateways.  Counting packets based on\n   link pairs has a\
    \ low overhead, but may result in unfairness to users\n   whose demand is below\
    \ the fair share and are using a long path.\n   Counts based on transport layer\
    \ connection identifiers, if this\n   information was available to Internet gateways,\
    \ would make good\n   distinctions, since the differences of demand of different\n\
    \   applications and instances of applications would be separately\n   monitored.\n\
    \   Problems with Selective Feedback Congestion Indication include that\n   the\
    \ gateway has significant processing to do.  With the feasible\n   choice of aggregation\
    \ at the gateway, unfairness across users within\n   the group is likely.  For\
    \ example, an interactive connection\n   aggregated with one or more bulk transfer\
    \ connections will receive\n   congestion indications though its own use of the\
    \ gateway resources is\n   very low.\n"
- title: 3.5  Fair Queueing
  contents:
  - "3.5  Fair Queueing\n   Fair Queueing is the policy of maintaining separate gateway\
    \ output\n   queues for individual end-systems by source-destination pair.  In\
    \ the\n   policy as proposed by [Nag85], the gateway's processing and link\n \
    \  resources are distributed to the end-systems on a round-robin basis.\n   On\
    \ congestion, packets are dropped from the longest queue.  This\n   policy leads\
    \ to equal allocations of resources to each source-\n   destination pair.  A source-destination\
    \ pair that demands more than a\n   fair share simply increases its own queueing\
    \ delay and congestion\n   drops.\n"
- title: 3.5.1  Bit-Round Fair Queueing
  contents:
  - "3.5.1  Bit-Round Fair Queueing\n   An enhancement of Nagle Fair Queueing, the\
    \ Bit-Round Fair Queuing\n   algorithm described and simulated by [DKS89] addresses\
    \ several\n   shortcomings of Nagle's scheme. It computes the order of service\
    \ to\n   packets using their lengths, with a technique that emulates a bit-\n\
    \   by-bit round-robin discipline, so that long packets do not get an\n   advantage\
    \ over short ones.  Otherwise the round-robin would be\n   unfair, for example,\
    \ giving more bandwidth to hosts whose traffic is\n   mainly long packets than\
    \ to hosts sourcing short packets.\n   The aggregation of users of a source-destination\
    \ pair by Fair\n   Queueing has the property of grouping the users whose round-trips\
    \ are\n   similar. This may be one reason that the combination of Bit-Round\n\
    \   Fair Queueing with Congestion Indication had particularly good\n   simulated\
    \ performance in [DKS89].\n   The aggregation of users has the expected drawbacks,\
    \ as well.  A\n   TELNET user in a queue with an FTP user does not get delay benefits;\n\
    \   and host pairs with high volume of connections get treated the same\n   as\
    \ a host pair with small number of connections and as a result gets\n   unfair\
    \ services.\n   A problem can be mentioned about Fair Queueing, though it is related\n\
    \   to implementation, and perhaps not properly part of a policy\n   discussion.\
    \  This is a concern that the resources (processing) used\n   for determining\
    \ where to queue incoming packets would themselves be\n   subject to congestion,\
    \ but not to the benefits of the Fair Queuing\n   discipline.  In a situation\
    \ where the gateway processor was not\n   adequate to the demands on it, the gateway\
    \ would need an alternative\n   policy for congestion control of the queue awaiting\
    \ processing.\n   Clever implementation can probably find an efficient way to\
    \ route\n   packets to the queues they belong in before other input processing\
    \ is\n   done, so that processing resources can be controlled, too.  There is\n\
    \   in addition, the concern that bit-by-bit round FQ requires non-FCFS\n   queueing\
    \ even within the same source destination pairs to allow for\n   fairness to different\
    \ connections between these end systems.\n   Another potential concern about Fair\
    \ Queueing is whether it can scale\n   up to gateways with very large source-destination\
    \ populations.  For\n   example, the state in a Fair Queueing implementation scales\
    \ with the\n   number of active end-to-end paths, which will be high in backbone\n\
    \   gateways.\n"
- title: 3.5.2  Stochastic Fairness Queuing
  contents:
  - "3.5.2  Stochastic Fairness Queuing\n   Stochastic Fairness Queueing (SFQ) has\
    \ been suggested as a technique\n   [McK90] to address the implementation issues\
    \ relating to Fair\n   Queueing.  The first overhead that is reduced is that of\
    \ looking up\n   the source-destination address pair in an incoming packet and\n\
    \   determining which queue that packet will have to be placed in.  SFQ\n   does\
    \ not require as many memory accesses as Fair Queueing to place\n   the packet\
    \ in the appropriate queue.  SFQ is thus claimed to be more\n   amenable to implementation\
    \ for high-speed networks [McK90].\n   SFQ uses a simple hash function to map\
    \ from the source-destination\n   address pair to a fixed set of queues.  Since\
    \ the assignment of an\n   address pair to a queue is probabilistic, there is\
    \ the likelihood of\n   multiple address pairs colliding and mapping to the same\
    \ queue.  This\n   would potentially degrade the additional fairness that is gained\
    \ with\n   Fairness Queueing.  If two or more address pairs collide, they would\n\
    \   continue to do so.  To deal with the situation when such a collision\n   occurs,\
    \ SFQ periodically perturbs the hash function so that these\n   address pairs\
    \ will be unlikely to collide subsequently.\n   The price paid for achieving this\
    \ implementation efficiency is that\n   SFQ requires a potentially large number\
    \ of queues (we must note\n   however, that these queues may be organized orthogonally\
    \ from the\n   buffers in which packets are stored. The buffers themselves may\
    \ be\n   drawn from a common pool, and buffers in each queue organized as a\n\
    \   linked list pointed to from each queue header).  For example, [McK90]\n  \
    \ indicates that to get good, consistent performance, we may need to\n   have\
    \ up to 5 to 10 times the number of active source-destination\n   pairs. In a\
    \ typical gateway, this may require around 1000 to 2000\n   queues.\n   [McK90]\
    \ reports simulation results with SFQ. The particular hash\n   function that is\
    \ studied is using the HDLC's cyclic redundancy check\n   (CRC).  The hash function\
    \ is perturbed by multiplying each byte by a\n   sequence number in the range\
    \ 1 to 255 before applying the CRC.  The\n   metric considered is the standard\
    \ deviation of the number of packets\n   output per source-destination pair. \
    \ A perfectly fair policy would\n   have a standard deviation of zero and an unfair\
    \ policy would have a\n   large standard deviation.  In the example studied (which\
    \ has up to 20\n   source-destination (s-d) pairs going through a single overloaded\n\
    \   gateway), SFQ with 1280 queues (i.e., 64 times the number of source-\n   destination\
    \ pairs), approaches about 3 times the standard deviation\n   of Fairness Queueing.\
    \  This must be compared to a FCFS queueing\n   discipline having a standard deviation\
    \ which is almost 175 times the\n   standard deviation of Fairness Queueing.\n\
    \   It is conjectured in [McK90] that a good value for the interval in\n   between\
    \ perturbations of the hash function appears to be in the area\n   between twice\
    \ the queue flush time of the stochastic fairness queue\n   and one-tenth the\
    \ average conversation time between a source-\n   destination pair.\n   SFQ also\
    \ may alleviate the anticipated scaling problems that may be\n   an issue with\
    \ Fair Queueing by not strictly requiring the number of\n   queues equal to the\
    \ possible source-destination pairs that may be\n   presenting a load on a particular\
    \ gateway. However, SFQ achieves this\n   property by trading off some of the\
    \ fairness for implementation\n   efficiency.\n   [McK90] examines alternative\
    \ strategies for implementation of Fair\n   Queueing and SFQ and estimates their\
    \ complexity on common hardware\n   platforms (e.g., a Motorola 68020).  It is\
    \ suggested that mapping an\n   IP address pair may require around 24 instructions\
    \ per packet for\n   Fair Queueing in the best case; in contrast SFQ requires\
    \ 10\n   instructions in the worst case.  The primary source of this gain is\n\
    \   that SFQ avoids a comparison of the s-d address pair on the packet to\n  \
    \ the identity of the queue header.  The relative benefit of SFQ over\n   Fair\
    \ Queueing is anticipated to be greater when the addresses are\n   longer.\n \
    \  SFQ offers promising implemenatation benefits.  However, the price to\n   be\
    \ paid in terms of having a significantly larger number of queues\n   (and the\
    \ consequent data structures and their management) than the\n   number of s-d\
    \ pairs placing a load on the gateway is a concern.  SFQ\n   is also attractive\
    \ in that it may be used in concert with the DEC-bit\n   scheme for Selective\
    \ Feedback Congestion Indication to provide\n   fairness as well as congestion\
    \ avoidance.\n"
- title: 4.  END-SYSTEM CONGESTION CONTROL POLICIES
  contents:
  - "4.  END-SYSTEM CONGESTION CONTROL POLICIES\n   Ideally in gateway congestion\
    \ control, the end-system transport\n   entities adjust (decrease) their demand\
    \ in response to control\n   exerted by the gateway.  Schemes have been put in\
    \ practice for\n   transport entities to adjust their demand dynamically in response\
    \ to\n   congestion feedback.  To accomplish this, in general, they call for\n\
    \   the user to gradually increase demand until information is received\n   that\
    \ the load on the gateway is too high.  In response to this\n   information, the\
    \ user decreases load, then begins an exploratory\n   increases again.  This cycle\
    \ is repeated continuously, with the goal\n   that the total demand will oscillate\
    \ around the optimal level.\n   We have already noted that a Slow-start connection\
    \ may give up\n   considerable bandwidth when sharing a gateway with aggressive\n\
    \   transport entities.  There is currently no way to enforce that end-\n   systems\
    \ use a congestion avoidance policy, though the Host\n   Requirements RFC [HR89]\
    \ has defined Slow-start as mandatory for TCP.\n   This adverse effect on Slow-start\
    \ connections is mitigated by the\n   Fair Queueing policy.  Our conclusions discuss\
    \ further the\n   coexistence of different end-system strategies.\n   This section\
    \ briefly presents two fielded transport congestion\n   control and avoidance\
    \ schemes, Slow-start and End-System Congestion\n   Indication, and the responses\
    \ by means of which they cooperate with\n   gateway policies.  They both use the\
    \ control paradigm described\n   above.  Slow-start, as mentioned in Section 1,\
    \ was developed by\n   [Jac88], and widely fielded in the BSD TCP implementation.\
    \  End-\n   system Congestion Indication was developed by [JRC87].  It is fielded\n\
    \   in DEC's Digital Network Architecture, and has been specified as well\n  \
    \ for ISO TP4 [NIST88].\n   Both Slow-start and End-system Congestion Indication\
    \ view the\n   relationship between users and gateways as a control system. They\n\
    \   have feedback and control components, the latter further broken down\n   into\
    \ a procedure bringing a new connection to equilibrium, and a\n   procedure to\
    \ maintain demand at the proper level.  They make use of\n   policies for increasing\
    \ and decreasing the transport sender's window\n   size.  These require the sender\
    \ to follow a set of self-restraining\n   rules which dynamically adjust the send\
    \ window whenever congestion is\n   sensed.\n   A predecessor of these, CUTE,\
    \ developed by [Jai86], introduced the\n   use of retransmission timeouts as congestion\
    \ feedback.  The Slow-\n   start scheme was also designed to use timeouts in the\
    \ same way.  The\n   End-System policies for Congestion Indication use the Congestion\n\
    \   Experienced Bit delivered in the network header as the primary\n   feedback,\
    \ but retransmission timeouts also provoke an end-system\n   congestion response.\n\
    \   In reliable transport protocols like TCP and TP4, the retransmission\n   timer\
    \ must do its best to satisfy two conflicting goals. On one hand,\n   the timer\
    \ must rapidly detect lost packets. And, on the other hand,\n   the timer must\
    \ minimize false alarms.  Since the retransmit timer is\n   used as a congestion\
    \ signal in these end-system policies, it is all\n   the more important that timeouts\
    \ reliably correspond to packet drops.\n   One important rule for retransmission\
    \ is to avoid bad sampling in the\n   measurements that will be used in estimating\
    \ the round-trip delay.\n   [KP87] describes techniques to ensure accurate sampling.\
    \  The Host\n   Requirements RFC [HR89] makes these techniques mandatory for TCP.\n\
    \   The utilization of a resource can be defined as the ratio of its\n   average\
    \ arrival rate to its mean service rate. This metric varies\n   between 0 and\
    \ 1.0. In a state of congestion, one or more resources\n   (link, gateway buffer,\
    \ gateway CPU) has a utilization approaching\n   1.0.  The average delay (round\
    \ trip time) and its variance approach\n   infinity. Therefore, as the utilization\
    \ of a network increases, it\n   becomes increasingly important to take into account\
    \ the variance of\n   the round trip time in estimating it for the retransmission\
    \ timeout.\n   The TCP retransmission timer is based on an estimate of the round\n\
    \   trip time.  [Jac88] calls the round trip time estimator the single\n   most\
    \ important feature of any protocol implementation that expects to\n   survive\
    \ a heavy load. The retransmit timeout procedure in RFC-793\n   [Pos81b] includes\
    \ a fixed parameter, beta, to account for the\n   variance in the delay. An estimate\
    \ of round trip time using the\n   suggested values for beta is valid for a network\
    \ which is at most 30%\n   utilized.  Thus, the RFC-793 retransmission timeout\
    \ estimator will\n   fail under heavy congestion, causing unnecessary retransmissions\
    \ that\n   add to the load, and making congestive loss detection impossible.\n\
    \   Slow-start TCP uses the mean deviation as an estimate of the variance\n  \
    \ to improve the estimate. As a rough view of what happens with the\n   Slow-start\
    \ retransmission calculation, delays can change by\n   approximately two standard\
    \ deviations without the timer going off in\n   a false alarm.  The same method\
    \ of estimation may be applicable to\n   TP4.  The procedure is:\n           Error\
    \     = Measured - Estimated\n           Estimated = Estimated + Gain_1 * Error\n\
    \           Deviation = Deviation + Gain_2 * (|Error| - Deviation)\n         \
    \  Timeout   = Estimated + 2 * Deviation\n           Where:  Gain_1, Gain_2 are\
    \ gain factors.\n"
- title: 4.1  Response to No Policy in Gateway
  contents:
  - "4.1  Response to No Policy in Gateway\n   Since packets must be dropped during\
    \ congestion because of the finite\n   buffer space, feedback of congestion to\
    \ the users exists even when\n   there is no gateway congestion policy.  Dropping\
    \ the packets is an\n   attempt to recover from congestion, though it needs to\
    \ be noted that\n   congestion collapse is not prevented by packet drops if end-systems\n\
    \   accelerate their sending rate in these conditions.  The accurate\n   detection\
    \ of congestive loss by all retransmission timers in the\n   end-systems is extremely\
    \ important for gateway congestion recovery.\n"
- title: 4.2  Response to Source Quench
  contents:
  - "4.2  Response to Source Quench\n   Given that a Source Quench message has ambiguous\
    \ meaning, but usually\n   is issued for congestion recovery, the response of\
    \ Slow-start to a\n   Source Quench is to return to the beginning of the cycle\
    \ of increase.\n   This is an early response, since the Source Quench on overflow\
    \ will\n   also lead to a retransmission timeout later.\n"
- title: 4.3 Response to Random Drop
  contents:
  - "4.3 Response to Random Drop\n   The end-system's view of Random Drop is the same\
    \ as its view of loss\n   caused by overflow at the gateway. This is a drawback\
    \ of the use of\n   packet drops as congestion feedback for congestion avoidance:\
    \ the\n   decrease policy on congestion feedback cannot be made more drastic\n\
    \   for overflows than for the drops the gateway does for congestion\n   avoidance.\
    \  Slow-start responds rapidly to gateway feedback.  In a\n   case where the users\
    \ are cooperating (all Slow-start), a desired\n   outcome would be that this responsiveness\
    \ would lead quickly to a\n   decreased probability of drop.  There would be,\
    \ as an ideal, a self-\n   adjusting overall amount of control.\n"
- title: 4.4  Response to Congestion Indication
  contents:
  - "4.4  Response to Congestion Indication\n   Since the Congestion Indication mechanism\
    \ attempts to keep gateways\n   uncongested, cooperating end-system congestion\
    \ control policies need\n   not reduce demand as much as with other gateway policies.\
    \  The\n   difference between the Slow-start response to packet drops and the\n\
    \   End-System Congestion Indication response to Congestion Experienced\n   Bits\
    \ is primarily in the decrease policy.  Slow-start decreases the\n   window to\
    \ one packet on a retransmission timeout.  End-System\n   Congestion Indication\
    \ decreases the window by a fraction (for\n   instance, to 7/8 of the original\
    \ value), when the Congestion\n   Experienced Bit is set in half of the packets\
    \ in a sample spanning\n   two round-trip times (two windows full).\n"
- title: 4.5  Response to Fair Queuing
  contents:
  - "4.5  Response to Fair Queuing\n   A Fair Queueing policy may issue control indications,\
    \ as in the\n   simulated Bit-Round Fair Queueing with DEC Bit, or it may depend\
    \ only\n   on the passive effects of the queueing.  When the passive control is\n\
    \   the main effect (perhaps because most users are not responsive to\n   control\
    \ indications), the behavior of retransmission timers will be\n   very important.\
    \  If retransmitting users send more packets in\n   response to increases in their\
    \ delay and drops, Fair Queueing will be\n   prone to degraded performance, though\
    \ collapse (zero throughput for\n   all users) may be prevented for a longer period\
    \ of time.\n"
- title: 5.  Conclusions
  contents:
  - "5.  Conclusions\n   The impact of users with excessive demand is a driving force\
    \ as\n   proposed gateway policies come closer to implementation.  Random Drop\n\
    \   and Congestion Indication can be fair only if the transport entities\n   sharing\
    \ the gateway are all cooperative and reduce demand as needed.\n   Within some\
    \ portions of the Internet, good behavior of end-systems\n   eventually may be\
    \ enforced through administration.  But for various\n   reasons, we can expect\
    \ non-cooperating transports to be a persistent\n   population in the Internet.\
    \  Congestion avoidance mechanisms will not\n   be deployed all at once, even\
    \ if they are adopted as standards.\n   Without enforcement, or even with penalties\
    \ for excessive demand,\n   some end-systems will never implement congestion avoidance\n\
    \   mechanisms.\n   Since it is outside the context of any of the gateway policies,\
    \ we\n   will mention here a suggestion for a relatively small-scale response\n\
    \   to users which implement especially aggressive policies. This has\n   been\
    \ made experimentally by [Jac89].  It would implement a low\n   priority queue,\
    \ to which the majority of traffic is not routed.  The\n   candidate traffic to\
    \ be queued there would be identified by a cache\n   of recent recipients of whatever\
    \ control indications the gateway\n   policy makes.  Remaining in the cache over\
    \ multiple control intervals\n   is the criterion for being routed to the low\
    \ priority queue.  In\n   approaching or established congestion, the bandwidth\
    \ given to the\n   low-service queue is decreased.\n   The goal of end-system\
    \ cooperation itself has been questioned.  As\n   [She89] points out, it is difficult\
    \ to define.  A TCP implementation\n   that retransmits before it determines that\
    \ has been loss indicated\n   and in a Go-Back-N manner is clearly non-cooperating.\
    \  On the other\n   hand, a transport entity with selective acknowledgement may\
    \ demand\n   more from the gateways than TCP, even while responding to congestion\n\
    \   in a cooperative way.\n   Fair Queueing maintains its control of allocations\
    \ regardless of the\n   end-system congestion avoidance policies.  [Nag85] and\
    \ [DKS89] argue\n   that the extra delays and congestion drops that result from\n\
    \   misbehavior could work to enforce good end-system policies.  Are the\n   rewards\
    \ and penalties less sharply defined when one or more\n   misbehaving systems\
    \ cause the whole gateway's performance to be less?\n   While the tax on all users\
    \ imposed by the \"over-users\" is much less\n   than in gateways with other policies,\
    \ how can it be made sufficiently\n   low?\n   In the sections on Selective Feedback\
    \ Congestion Indication and Bit-\n   Round Fair Queueing we have pointed out that\
    \ more needs to be done on\n   two particular fronts:\n      How can increased\
    \ computational overhead be avoided?\n      The allocation of resources to source-destination\
    \ pairs is, in\n      many scenarios, unfair to individual users. Bit-Round Fair\n\
    \      Queueing offers a potential administrative remedy, but even if it\n   \
    \   is applied, how should the unequal allocations be propagated\n      through\
    \ multiple gateways?\n   The first question has been taken up by [McK90].\n  \
    \ Since Selective Feedback Congestion Indication (or Congestion\n   Indication\
    \ used with Fair Queueing) uses a network bit, its use in\n   the Internet requires\
    \ protocol support; the transition of some\n   portions of the Internet to OSI\
    \ protocols may make such a change\n   attractive in the future.  The interactions\
    \ between heterogeneous\n   congestion control policies in the Internet will need\
    \ to be explored.\n   The goals of Random Drop Congestion Avoidance are presented\
    \ in this\n   survey, but without any claim that the problems of this policy can\
    \ be\n   solved.  These goals themselves, of uniform, dynamic treatment of\n \
    \  users (streams/flows), of low overhead, and of good scaling\n   characteristics\
    \ in large and loaded networks, are significant.\n"
- title: 'Appendix:  Congestion and Connection-oriented Subnets'
  contents:
  - "Appendix:  Congestion and Connection-oriented Subnets\n   This section presents\
    \ a recommendation for gateway implementation in\n   an areas that unavoidably\
    \ interacts with gateway congestion control,\n   the impact of connection-oriented\
    \ subnets, such as those based on\n   X.25.\n   The need to manage a connection\
    \ oriented service (X.25) in order to\n   transport datagram traffic (IP) can\
    \ cause problems for gateway\n   congestion control.  Being a pure datagram protocol,\
    \ IP provides no\n   information delimiting when a pair of IP entities need to\
    \ establish a\n   session between themselves.  The solution involves compromise\
    \ among\n   delay, cost, and resources.  Delay is introduced by call\n   establishment\
    \ when a new X.25 SVC (Switched Virtual Circuit) needs to\n   be established,\
    \ and also by queueing delays for the physical line.\n   Cost includes any charges\
    \ by the X.25 network service provider.\n   Besides the resources most gateways\
    \ have (CPU, memory, links), a\n   maximum supported or permitted number of virtual\
    \ circuits may be in\n   contest.\n   SVCs are established on demand when an IP\
    \ packet needs to be sent and\n   there is no SVC established or pending establishment\
    \ to the\n   destination IP entity.  Optionally, when cost considerations, and\n\
    \   sufficient numbers of unused virtual circuits allow, redundant SVCs\n   may\
    \ be established between the same pair of IP entities.  Redundant\n   SVCs can\
    \ have the effect of improving performance when coping with\n   large end-to-end\
    \ delay, small maximum packet sizes and small maximum\n   packet windows.  It\
    \ is generally preferred though, to negotiate large\n   packet sizes and packet\
    \ windows on a single SVC.  Redundant SVCs must\n   especially be discouraged\
    \ when virtual circuit resources are small\n   compared with the number of destination\
    \ IP entities among the active\n   users of the gateway.\n   SVCs are closed after\
    \ some period of inactivity indicates that\n   communication may have been suspended\
    \ between the IP entities.  This\n   timeout is significant in the operation of\
    \ the interface.  Setting\n   the value too low can result in closing of the SVC\
    \ even though the\n   traffic has not stopped.  This results in potentially large\
    \ delays\n   for the packets which reopen the SVC and may also incur charges\n\
    \   associated with SVC calls.  Also, clearing of SVCs is, by definition,\n  \
    \ nongraceful.  If an SVC is closed before the last packets are\n   acknowledged,\
    \ there is no guarantee of delivery.  Packet losses are\n   introduced by this\
    \ destructive close independent of gateway traffic\n   and congestion.\n   When\
    \ a new circuit is needed and all available circuits are currently\n   in use,\
    \ there is a temptation to pick one to close (possibly using\n   some Least Recently\
    \ Used criterion).  But if connectivity demands are\n   larger than available\
    \ circuit resources, this strategy can lead to a\n   type of thrashing where circuits\
    \ are constantly being closed and\n   reopened.  In the worst case, a circuit\
    \ is opened, a single packet\n   sent and the circuit closed (without a guarantee\
    \ of packet delivery).\n   To counteract this, each circuit should be allowed\
    \ to remain open a\n   minimum amount of time.\n   One possible SVC strategy is\
    \ to dynamically change the time a circuit\n   will be allowed to remain open\
    \ based on the number of circuits in\n   use.  Three administratively controlled\
    \ variables are used, a minimum\n   time, a maximum time and an adaptation factor\
    \ in seconds per\n   available circuit.  In this scheme, a circuit is closed after\
    \ it has\n   been idle for a time period equal to the minimum plus the adaptation\n\
    \   factor times the number of available circuits, limited by the maximum\n  \
    \ time.  By administratively adjusting these variables, one has\n   considerable\
    \ flexibility in adjusting the SVC utilization to meet the\n   needs of a specific\
    \ gateway.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   This paper is the outcome of discussions in the Performance\
    \ and\n   Congestion Control Working Group between April 1988 and July 1989.\n\
    \   Both PCC WG and plenary IETF members gave us helpful reviews of\n   earlier\
    \ drafts.  Several of the ideas described here were contributed\n   by the members\
    \ of the PCC WG.  The Appendix was written by Art\n   Berggreen.  We are particularly\
    \ thankful also to Van Jacobson, Scott\n   Shenker, Bruce Schofield, Paul McKenney,\
    \ Matt Mathis, Geof Stone, and\n   Lixia Zhang for participation and reviews.\n"
- title: References
  contents:
  - "References\n   [DKS89] Demers, A., Keshav, S., and S. Shenker, \"Analysis and\n\
    \   Simulation of a Fair Queueing Algorithm\", Proceedings of SIGCOMM '89.\n \
    \  [Fin89] Finn, G., \"A Connectionless Congestion Control Algorithm\",\n   Computer\
    \ Communications Review, Vol. 19, No. 5, October 1989.\n   [Gar87] Gardner, M.,\
    \ \"BBN Report on the ARPANET\", Proceedings of the\n   McLean IETF, SRI-NIC IETF-87/3P,\
    \ July 1987.\n   [GREQ87] Braden R., and J. Postel, \"Requirements for Internet\n\
    \   Gateways\", RFC 1009, USC/Information Sciences Institute, June 1987.\n   [HREQ89]\
    \ Braden R., Editor, \"Requirements for Internet Hosts --\n   Communications Layers\"\
    , RFC 1122, Internet Engineering Task Force,\n   October 1989.\n   [Has90] Hashem,\
    \ E., \"Random Drop Congestion Control\", M.S. Thesis,\n   Massachusetts Institute\
    \ of Technology, Department of Computer\n   Science, 1990.\n   [Jac88] Jacobson,\
    \ V., \"Congestion Avoidance and Control\", Proceedings\n   of SIGCOMM '88.\n\
    \   [Jac89] Jacobson, V., \"Presentations to the IETF Performance and\n   Congestion\
    \ Control Working Group\".\n   [Jaf81] Jaffe, J., \"Bottleneck Flow Control\"\
    , IEEE Transactions on\n   Communications, COM-29(7), July, 1981.\n   [Jai86]\
    \ Jain, R., \"A Timeout-based Congestion Control Scheme for\n   Window Flow-controlled\
    \ Networks\", IEEE Journal on Selected Areas in\n   Communications, SAC-4(7),\
    \ October 1986.\n   [JRC87] Jain, R., Ramakrishnan, K., and D. Chiu, \"Congestion\n\
    \   Avoidance in Computer Networks With a Connectionless Network Layer\",\n  \
    \ Technical Report DEC-TR-506, Digital Equipment Corporation.\n   [Kle79] Kleinrock,\
    \ L., \"Power and Deterministic Rules of Thumb for\n   Probabilistic Problems\
    \ in Computer Communications\",  Proceedings of\n   the ICC '79.\n   [KP87] Karn,\
    \ P., and C. Partridge, \"Improving Round Trip Estimates in\n   Reliable Transport\
    \ Protocols\", Proceedings of SIGCOMM '87.\n   [Man90] Mankin, A., \"Random Drop\
    \ Congestion Control\", Proceedings of\n   SIGCOMM '90.\n   [McK90] McKenney,\
    \ P., \"Stochastic Fairness Queueing\", Proceedings of\n   INFOCOM '90.\n   [Nag84]\
    \ Nagle, J., \"Congestion Control in IP/TCP Internetworks\", RFC\n   896, FACC\
    \ Palo Alto, 6 January 1984.\n   [Nag85] Nagle, J., \"On Packet Switches With\
    \ Infinite Storage\", RFC\n   970, FACC Palo Alto, December 1985.\n   [NIST88]\
    \ NIST, \"Stable Implementation Agreements for OSI Protocols,\n   Version 2, Edition\
    \ 1\", National Institute of Standards and Technology\n   Special Publication\
    \ 500-162, December 1988.\n   [Pos81a] Postel, J., \"Internet Control Message\
    \ Protocol - DARPA\n   Internet Program Protocol Specification\", RFC-792, USC/Information\n\
    \   Sciences Institute, September 1981.\n   [Pos81b] Postel, J., \"Transmission\
    \ Control Protocol - DARPA Internet\n   Program Protocol Specification\", RFC-793,\
    \ DARPA, September 1981.\n   [RJC87] Ramakrishnan, K., Jain, R., and D. Chiu,\
    \ \"A Selective Binary\n   Feedback Scheme for General Topologies\", Technical\
    \ Report DEC-TR-510,\n   Digital Equipment Corporation.\n   [She89] Shenker, S.,\
    \ \"Correspondence with the IETF Performance and\n   Congestion Control Working\
    \ Group\".\n   [Sp89] Spector, A., and M. Kazar, \"Uniting File Systems\", Unix\n\
    \   Review, Vol.  7, No. 3, March 1989.\n   [Zha89] Zhang, L., \"A New Architecture\
    \ for Packet Switching Network\n   Protocols\", Ph.D Thesis, Massachusetts Institute\
    \ of Technology,\n   Department of Computer Science, 1989.\n"
- title: Security Considerations
  contents:
  - "Security Considerations\n   Security issues are not discussed in this memo.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Allison Mankin\n   The MITRE Corporation\n   M/S W425\n\
    \   7525 Colshire Drive\n   McLean, VA  22102\n   Email: mankin@gateway.mitre.org\n\
    \   K.K. Ramakrishnan\n   Digital Equipment Corporation\n   M/S LKG1-2/A19\n \
    \  550 King Street\n   Littleton, MA  01754\n   Email: rama@kalvi.enet.dec.com\n"
