- title: __initial_text__
  contents:
  - '                  Framework for IP Performance Metrics

    '
- title: 1. Status of this Memo
  contents:
  - "1. Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: 2. Copyright Notice
  contents:
  - "2. Copyright Notice\n   Copyright (C) The Internet Society (1998).  All Rights\
    \ Reserved.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n     9.2  Temporal Composition of Formal Models and Empirical\
    \ Metrics.13\n     11.2  Self-Consistency...........................................24\n\
    \     11.3  Defining Statistical Distributions.........................25\n  \
    \   11.4  Testing For Goodness-of-Fit................................27\n   12.\
    \ AVOIDING STOCHASTIC METRICS....................................28\n   13. PACKETS\
    \ OF TYPE P..............................................29\n   14. INTERNET ADDRESSES\
    \ VS. HOSTS...................................30\n   15. STANDARD-FORMED PACKETS........................................30\n\
    \   16. ACKNOWLEDGEMENTS...............................................31\n  \
    \ 17. SECURITY CONSIDERATIONS........................................31\n   18.\
    \ APPENDIX.......................................................32\n   19. REFERENCES.....................................................38\n\
    \   20. AUTHORS' ADDRESSES.............................................39\n  \
    \ 21. FULL COPYRIGHT STATEMENT.......................................40\n"
- title: 3. Introduction
  contents:
  - "3. Introduction\n   The purpose of this memo is to define a general framework\
    \ for\n   particular metrics to be developed by the IETF's IP Performance\n  \
    \ Metrics effort, begun by the Benchmarking Methodology Working Group\n   (BMWG)\
    \ of the Operational Requirements Area, and being continued by\n   the IP Performance\
    \ Metrics Working Group (IPPM) of the Transport\n   Area.\n   We begin by laying\
    \ out several criteria for the metrics that we\n   adopt.  These criteria are\
    \ designed to promote an IPPM effort that\n   will maximize an accurate common\
    \ understanding by Internet users and\n   Internet providers of the performance\
    \ and reliability both of end-\n   to-end paths through the Internet and of specific\
    \ 'IP clouds' that\n   comprise portions of those paths.\n   We next define some\
    \ Internet vocabulary that will allow us to speak\n   clearly about Internet components\
    \ such as routers, paths, and clouds.\n   We then define the fundamental concepts\
    \ of 'metric' and 'measurement\n   methodology', which allow us to speak clearly\
    \ about measurement\n   issues.  Given these concepts, we proceed to discuss the\
    \ important\n   issue of measurement uncertainties and errors, and develop a key,\n\
    \   somewhat subtle notion of how they relate to the analytical framework\n  \
    \ shared by many aspects of the Internet engineering discipline.  We\n   then\
    \ introduce the notion of empirically defined metrics, and finish\n   this part\
    \ of the document with a general discussion of how metrics\n   can be 'composed'.\n\
    \   The remainder of the document deals with a variety of issues related\n   to\
    \ defining sound metrics and methodologies:  how to deal with\n   imperfect clocks;\
    \ the notion of 'wire time' as distinct from 'host\n   time'; how to aggregate\
    \ sets of singleton metrics into samples and\n   derive sound statistics from\
    \ those samples; why it is recommended to\n   avoid thinking about Internet properties\
    \ in probabilistic terms (such\n   as the probability that a packet is dropped),\
    \ since these terms often\n   include implicit assumptions about how the network\
    \ behaves; the\n   utility of defining metrics in terms of packets of a generic\
    \ type;\n   the benefits of preferring IP addresses to DNS host names; and the\n\
    \   notion of 'standard-formed' packets.  An appendix discusses the\n   Anderson-Darling\
    \ test for gauging whether a set of values matches a\n   given statistical distribution,\
    \ and gives C code for an\n   implementation of the test.\n   In some sections\
    \ of the memo, we will surround some commentary text\n   with the brackets {Comment:\
    \ ... }.  We stress that this commentary is\n   only commentary, and is not itself\
    \ part of the framework document or\n   a proposal of particular metrics.  In\
    \ some cases this commentary will\n   discuss some of the properties of metrics\
    \ that might be envisioned,\n   but the reader should assume that any such discussion\
    \ is intended\n   only to shed light on points made in the framework document,\
    \ and not\n   to suggest any specific metrics.\n"
- title: 4. Criteria for IP Performance Metrics
  contents:
  - "4. Criteria for IP Performance Metrics\n   The overarching goal of the IP Performance\
    \ Metrics effort is to\n   achieve a situation in which users and providers of\
    \ Internet\n   transport service have an accurate common understanding of the\n\
    \   performance and reliability of the Internet component 'clouds' that\n   they\
    \ use/provide.\n   To achieve this, performance and reliability metrics for paths\n\
    \   through the Internet must be developed.  In several IETF meetings\n   criteria\
    \ for these metrics have been specified:\n +    The metrics must be concrete and\
    \ well-defined,\n +    A methodology for a metric should have the property that\
    \ it is\n      repeatable: if the methodology is used multiple times under\n \
    \     identical conditions, the same measurements should result in the\n     \
    \ same measurements.\n +    The metrics must exhibit no bias for IP clouds implemented\
    \ with\n      identical technology,\n +    The metrics must exhibit understood\
    \ and fair bias for IP clouds\n      implemented with non-identical technology,\n\
    \ +    The metrics must be useful to users and providers in understanding\n  \
    \    the performance they experience or provide,\n +    The metrics must avoid\
    \ inducing artificial performance goals.\n"
- title: 5. Terminology for Paths and Clouds
  contents:
  - "5. Terminology for Paths and Clouds\n   The following list defines terms that\
    \ need to be precise in the\n   development of path metrics.  We begin with low-level\
    \ notions of\n   'host', 'router', and 'link', then proceed to define the notions\
    \ of\n   'path', 'IP cloud', and 'exchange' that allow us to segment a path\n\
    \   into relevant pieces.\n   host A computer capable of communicating using the\
    \ Internet\n        protocols; includes \"routers\".\n   link A single link-level\
    \ connection between two (or more) hosts;\n        includes leased lines, ethernets,\
    \ frame relay clouds, etc.\n   routerA host which facilitates network-level communication\
    \ between\n        hosts by forwarding IP packets.\n   path A sequence of the\
    \ form < h0, l1, h1, ..., ln, hn >, where n >=\n        0, each hi is a host,\
    \ each li is a link between hi-1 and hi,\n        each h1...hn-1 is a router.\
    \  A pair <li, hi> is termed a 'hop'.\n        In an appropriate operational configuration,\
    \ the links and\n        routers in the path facilitate network-layer communication\
    \ of\n        packets from h0 to hn.  Note that path is a unidirectional\n   \
    \     concept.\n   subpath\n        Given a path, a subpath is any subsequence\
    \ of the given path\n        which is itself a path.  (Thus, the first and last\
    \ element of a\n        subpath is a host.)\n   cloudAn undirected (possibly cyclic)\
    \ graph whose vertices are routers\n        and whose edges are links that connect\
    \ pairs of routers.\n        Formally, ethernets, frame relay clouds, and other\
    \ links that\n        connect more than two routers are modelled as fully-connected\n\
    \        meshes of graph edges.  Note that to connect to a cloud means to\n  \
    \      connect to a router of the cloud over a link; this link is not\n      \
    \  itself part of the cloud.\n   exchange\n        A special case of a link, an\
    \ exchange directly connects either a\n        host to a cloud and/or one cloud\
    \ to another cloud.\n   cloud subpath\n        A subpath of a given path, all\
    \ of whose hosts are routers of a\n        given cloud.\n   path digest\n    \
    \    A sequence of the form < h0, e1, C1, ..., en, hn >, where n >=\n        0,\
    \ h0 and hn are hosts, each e1 ... en is an exchange, and each\n        C1 ...\
    \ Cn-1 is a cloud subpath.\n"
- title: 6. Fundamental Concepts
  contents:
  - '6. Fundamental Concepts

    '
- title: 6.1. Metrics
  contents:
  - "6.1. Metrics\n   In the operational Internet, there are several quantities related\
    \ to\n   the performance and reliability of the Internet that we'd like to\n \
    \  know the value of.  When such a quantity is carefully specified, we\n   term\
    \ the quantity a metric.  We anticipate that there will be\n   separate RFCs for\
    \ each metric (or for each closely related group of\n   metrics).\n   In some\
    \ cases, there might be no obvious means to effectively measure\n   the metric;\
    \ this is allowed, and even understood to be very useful in\n   some cases.  It\
    \ is required, however, that the specification of the\n   metric be as clear as\
    \ possible about what quantity is being\n   specified.  Thus, difficulty in practical\
    \ measurement is sometimes\n   allowed, but ambiguity in meaning is not.\n   Each\
    \ metric will be defined in terms of standard units of\n   measurement.  The international\
    \ metric system will be used, with the\n   following points specifically noted:\n\
    \ +    When a unit is expressed in simple meters (for distance/length) or\n  \
    \    seconds (for duration), appropriate related units based on\n      thousands\
    \ or thousandths of acceptable units are acceptable.\n      Thus, distances expressed\
    \ in kilometers (km), durations expressed\n      in milliseconds (ms), or microseconds\
    \ (us) are allowed, but not\n      centimeters (because the prefix is not in terms\
    \ of thousands or\n      thousandths).\n +    When a unit is expressed in a combination\
    \ of units, appropriate\n      related units based on thousands or thousandths\
    \ of acceptable\n      units are acceptable, but all such thousands/thousandths\
    \ must be\n      grouped at the beginning.  Thus, kilo-meters per second (km/s)\
    \ is\n      allowed, but meters per millisecond is not.\n +    The unit of information\
    \ is the bit.\n +    When metric prefixes are used with bits or with combinations\n\
    \      including bits, those prefixes will have their metric meaning\n      (related\
    \ to decimal 1000), and not the meaning conventional with\n      computer storage\
    \ (related to decimal 1024).  In any RFC that\n      defines a metric whose units\
    \ include bits, this convention will be\n      followed and will be repeated to\
    \ ensure clarity for the reader.\n +    When a time is given, it will be expressed\
    \ in UTC.\n   Note that these points apply to the specifications for metrics and\n\
    \   not, for example, to packet formats where octets will likely be used\n   in\
    \ preference/addition to bits.\n   Finally, we note that some metrics may be defined\
    \ purely in terms of\n   other metrics; such metrics are call 'derived metrics'.\n"
- title: 6.2. Measurement Methodology
  contents:
  - "6.2. Measurement Methodology\n   For a given set of well-defined metrics, a number\
    \ of distinct\n   measurement methodologies may exist.  A partial list includes:\n\
    \ +    Direct measurement of a performance metric using injected test\n      traffic.\
    \  Example: measurement of the round-trip delay of an IP\n      packet of a given\
    \ size over a given route at a given time.\n +    Projection of a metric from\
    \ lower-level measurements.  Example:\n      given accurate measurements of propagation\
    \ delay and bandwidth for\n      each step along a path, projection of the complete\
    \ delay for the\n      path for an IP packet of a given size.\n +    Estimation\
    \ of a constituent metric from a set of more aggregated\n      measurements. \
    \ Example: given accurate measurements of delay for a\n      given one-hop path\
    \ for IP packets of different sizes, estimation\n      of propagation delay for\
    \ the link of that one-hop path.\n +    Estimation of a given metric at one time\
    \ from a set of related\n      metrics at other times.  Example: given an accurate\
    \ measurement of\n      flow capacity at a past time, together with a set of accurate\n\
    \      delay measurements for that past time and the current time, and\n     \
    \ given a model of flow dynamics, estimate the flow capacity that\n      would\
    \ be observed at the current time.\n   This list is by no means exhaustive.  The\
    \ purpose is to point out the\n   variety of measurement techniques.\n   When\
    \ a given metric is specified, a given measurement approach might\n   be noted\
    \ and discussed.  That approach, however, is not formally part\n   of the specification.\n\
    \   A methodology for a metric should have the property that it is\n   repeatable:\
    \ if the methodology is used multiple times under identical\n   conditions, it\
    \ should result in consistent measurements.\n   Backing off a little from the\
    \ word 'identical' in the previous\n   paragraph, we could more accurately use\
    \ the word 'continuity' to\n   describe a property of a given methodology: a methodology\
    \ for a given\n   metric exhibits continuity if, for small variations in conditions,\
    \ it\n   results in small variations in the resulting measurements.  Slightly\n\
    \   more precisely, for every positive epsilon, there exists a positive\n   delta,\
    \ such that if two sets of conditions are within delta of each\n   other, then\
    \ the resulting measurements will be within epsilon of each\n   other.  At this\
    \ point, this should be taken as a heuristic driving\n   our intuition about one\
    \ kind of robustness property rather than as a\n   precise notion.\n   A metric\
    \ that has at least one methodology that exhibits continuity\n   is said itself\
    \ to exhibit continuity.\n   Note that some metrics, such as hop-count along a\
    \ path, are integer-\n   valued and therefore cannot exhibit continuity in quite\
    \ the sense\n   given above.\n   Note further that, in practice, it may not be\
    \ practical to know (or\n   be able to quantify) the conditions relevant to a\
    \ measurement at a\n   given time.  For example, since the instantaneous load\
    \ (in packets to\n   be served) at a given router in a high-speed wide-area network\
    \ can\n   vary widely over relatively brief periods and will be very hard for\n\
    \   an external observer to quantify, various statistics of a given\n   metric\
    \ may be more repeatable, or may better exhibit continuity.  In\n   that case\
    \ those particular statistics should be specified when the\n   metric is specified.\n\
    \   Finally, some measurement methodologies may be 'conservative' in the\n   sense\
    \ that the act of measurement does not modify, or only slightly\n   modifies,\
    \ the value of the performance metric the methodology\n   attempts to measure.\
    \  {Comment: for example, in a wide-are high-speed\n   network under modest load,\
    \ a test using several small 'ping' packets\n   to measure delay would likely\
    \ not interfere (much) with the delay\n   properties of that network as observed\
    \ by others.  The corresponding\n   statement about tests using a large flow to\
    \ measure flow capacity\n   would likely fail.}\n"
- title: 6.3. Measurements, Uncertainties, and Errors
  contents:
  - "6.3. Measurements, Uncertainties, and Errors\n   Even the very best measurement\
    \ methodologies for the very most well\n   behaved metrics will exhibit errors.\
    \  Those who develop such\n   measurement methodologies, however, should strive\
    \ to:\n +    minimize their uncertainties/errors,\n +    understand and document\
    \ the sources of uncertainty/error, and\n +    quantify the amounts of uncertainty/error.\n\
    \   For example, when developing a method for measuring delay, understand\n  \
    \ how any errors in your clocks introduce errors into your delay\n   measurement,\
    \ and quantify this effect as well as you can.  In some\n   cases, this will result\
    \ in a requirement that a clock be at least up\n   to a certain quality if it\
    \ is to be used to make a certain\n   measurement.\n   As a second example, consider\
    \ the timing error due to measurement\n   overheads within the computer making\
    \ the measurement, as opposed to\n   delays due to the Internet component being\
    \ measured.  The former is a\n   measurement error, while the latter reflects\
    \ the metric of interest.\n   Note that one technique that can help avoid this\
    \ overhead is the use\n   of a packet filter/sniffer, running on a separate computer\
    \ that\n   records network packets and timestamps them accurately (see the\n \
    \  discussion of 'wire time' below).  The resulting trace can then be\n   analyzed\
    \ to assess the test traffic, minimizing the effect of\n   measurement host delays,\
    \ or at least allowing those delays to be\n   accounted for.  We note that this\
    \ technique may prove beneficial even\n   if the packet filter/sniffer runs on\
    \ the same machine, because such\n   measurements generally provide 'kernel-level'\
    \ timestamping as opposed\n   to less-accurate 'application-level' timestamping.\n\
    \   Finally, we note that derived metrics (defined above) or metrics that\n  \
    \ exhibit spatial or temporal composition (defined below) offer\n   particular\
    \ occasion for the analysis of measurement uncertainties,\n   namely how the uncertainties\
    \ propagate (conceptually) due to the\n   derivation or composition.\n"
- title: 7. Metrics and the Analytical Framework
  contents:
  - "7. Metrics and the Analytical Framework\n   As the Internet has evolved from\
    \ the early packet-switching studies\n   of the 1960s, the Internet engineering\
    \ community has evolved a common\n   analytical framework of concepts.  This analytical\
    \ framework, or A-\n   frame, used by designers and implementers of protocols,\
    \ by those\n   involved in measurement, and by those who study computer network\n\
    \   performance using the tools of simulation and analysis, has great\n   advantage\
    \ to our work.  A major objective here is to generate network\n   characterizations\
    \ that are consistent in both analytical and\n   practical settings, since this\
    \ will maximize the chances that non-\n   empirical network study can be better\
    \ correlated with, and used to\n   further our understanding of, real network\
    \ behavior.\n   Whenever possible, therefore, we would like to develop and leverage\n\
    \   off of the A-frame.  Thus, whenever a metric to be specified is\n   understood\
    \ to be closely related to concepts within the A-frame, we\n   will attempt to\
    \ specify the metric in the A-frame's terms.  In such a\n   specification we will\
    \ develop the A-frame by precisely defining the\n   concepts needed for the metric,\
    \ then leverage off of the A-frame by\n   defining the metric in terms of those\
    \ concepts.\n   Such a metric will be called an 'analytically specified metric'\
    \ or,\n   more simply, an analytical metric.\n   {Comment: Examples of such analytical\
    \ metrics might include:\n"
- title: propagation time of a link
  contents:
  - "propagation time of a link\n     The time, in seconds, required by a single bit\
    \ to travel from the\n     output port on one Internet host across a single link\
    \ to another\n     Internet host.\n"
- title: bandwidth of a link for packets of size k
  contents:
  - "bandwidth of a link for packets of size k\n     The capacity, in bits/second,\
    \ where only those bits of the IP\n     packet are counted, for packets of size\
    \ k bytes.\n"
- title: routeThe path, as defined in Section 5, from A to B at a given time.
  contents:
  - 'routeThe path, as defined in Section 5, from A to B at a given time.

    '
- title: hop count of a route
  contents:
  - "hop count of a route\n     The value 'n' of the route path.\n     }\n     Note\
    \ that we make no a priori list of just what A-frame concepts\n     will emerge\
    \ in these specifications, but we do encourage their use\n     and urge that they\
    \ be carefully specified so that, as our set of\n     metrics develops, so will\
    \ a specified set of A-frame concepts\n     technically consistent with each other\
    \ and consonant with the\n     common understanding of those concepts within the\
    \ general Internet\n     community.\n     These A-frame concepts will be intended\
    \ to abstract from actual\n     Internet components in such a way that:\n +  \
    \  the essential function of the component is retained,\n +    properties of the\
    \ component relevant to the metrics we aim to\n      create are retained,\n +\
    \    a subset of these component properties are potentially defined as\n     \
    \ analytical metrics, and\n +    those properties of actual Internet components\
    \ not relevant to\n      defining the metrics we aim to create are dropped.\n\
    \   For example, when considering a router in the context of packet\n   forwarding,\
    \ we might model the router as a component that receives\n   packets on an input\
    \ link, queues them on a FIFO packet queue of\n   finite size, employs tail-drop\
    \ when the packet queue is full, and\n   forwards them on an output link.  The\
    \ transmission speed (in\n   bits/second) of the input and output links, the latency\
    \ in the router\n   (in seconds), and the maximum size of the packet queue (in\
    \ bits) are\n   relevant analytical metrics.\n   In some cases, such analytical\
    \ metrics used in relation to a router\n   will be very closely related to specific\
    \ metrics of the performance\n   of Internet paths.  For example, an obvious formula\
    \ (L + P/B)\n   involving the latency in the router (L), the packet size (in bits)\n\
    \   (P), and the transmission speed of the output link (B) might closely\n   approximate\
    \ the increase in packet delay due to the insertion of a\n   given router along\
    \ a path.\n   We stress, however, that well-chosen and well-specified A-frame\n\
    \   concepts and their analytical metrics will support more general\n   metric\
    \ creation efforts in less obvious ways.\n   {Comment: for example, when considering\
    \ the flow capacity of a path,\n   it may be of real value to be able to model\
    \ each of the routers along\n   the path as packet forwarders as above.  Techniques\
    \ for estimating\n   the flow capacity of a path might use the maximum packet\
    \ queue size\n   as a parameter in decidedly non-obvious ways.  For example, as\
    \ the\n   maximum queue size increases, so will the ability of the router to\n\
    \   continuously move traffic along an output link despite fluctuations\n   in\
    \ traffic from an input link.  Estimating this increase, however,\n   remains\
    \ a research topic.}\n   Note that, when we specify A-frame concepts and analytical\
    \ metrics,\n   we will inevitably make simplifying assumptions.  The key role\
    \ of\n   these concepts is to abstract the properties of the Internet\n   components\
    \ relevant to given metrics.  Judgement is required to avoid\n   making assumptions\
    \ that bias the modeling and metric effort toward\n   one kind of design.\n  \
    \ {Comment: for example, routers might not use tail-drop, even though\n   tail-drop\
    \ might be easier to model analytically.}\n   Finally, note that different elements\
    \ of the A-frame might well make\n   different simplifying assumptions.  For example,\
    \ the abstraction of a\n   router used to further the definition of path delay\
    \ might treat the\n   router's packet queue as a single FIFO queue, but the abstraction\
    \ of\n   a router used to further the definition of the handling of an RSVP-\n\
    \   enabled packet might treat the router's packet queue as supporting\n   bounded\
    \ delay -- a contradictory assumption.  This is not to say that\n   we make contradictory\
    \ assumptions at the same time, but that two\n   different parts of our work might\
    \ refine the simpler base concept in\n   two divergent ways for different purposes.\n\
    \   {Comment: in more mathematical terms, we would say that the A-frame\n   taken\
    \ as a whole need not be consistent; but the set of particular\n   A-frame elements\
    \ used to define a particular metric must be.}\n"
- title: 8. Empirically Specified Metrics
  contents:
  - "8. Empirically Specified Metrics\n   There are useful performance and reliability\
    \ metrics that do not fit\n   so neatly into the A-frame, usually because the\
    \ A-frame lacks the\n   detail or power for dealing with them.  For example, \"\
    the best flow\n   capacity achievable along a path using an RFC-2001-compliant\
    \ TCP\"\n   would be good to be able to measure, but we have no analytical\n \
    \  framework of sufficient richness to allow us to cast that flow\n   capacity\
    \ as an analytical metric.\n   These notions can still be well specified by instead\
    \ describing a\n   reference methodology for measuring them.\n   Such a metric\
    \ will be called an 'empirically specified metric', or\n   more simply, an empirical\
    \ metric.\n   Such empirical metrics should have three properties:\n +    we should\
    \ have a clear definition for each in terms of Internet\n      components,\n +\
    \    we should have at least one effective means to measure them, and\n +    to\
    \ the extent possible, we should have an (necessarily incomplete)\n      understanding\
    \ of the metric in terms of the A-frame so that we can\n      use our measurements\
    \ to reason about the performance and\n      reliability of A-frame components\
    \ and of aggregations of A-frame\n      components.\n"
- title: 9. Two Forms of Composition
  contents:
  - '9. Two Forms of Composition

    '
- title: 9.1. Spatial Composition of Metrics
  contents:
  - "9.1. Spatial Composition of Metrics\n   In some cases, it may be realistic and\
    \ useful to define metrics in\n   such a fashion that they exhibit spatial composition.\n\
    \   By spatial composition, we mean a characteristic of some path\n   metrics,\
    \ in which the metric as applied to a (complete) path can also\n   be defined\
    \ for various subpaths, and in which the appropriate A-frame\n   concepts for\
    \ the metric suggest useful relationships between the\n   metric applied to these\
    \ various subpaths (including the complete\n   path, the various cloud subpaths\
    \ of a given path digest, and even\n   single routers along the path).  The effectiveness\
    \ of spatial\n   composition depends:\n +    on the usefulness in analysis of\
    \ these relationships as applied to\n      the relevant A-frame components, and\n\
    \ +    on the practical use of the corresponding relationships as applied\n  \
    \    to metrics and to measurement methodologies.\n   {Comment: for example, consider\
    \ some metric for delay of a 100-byte\n   packet across a path P, and consider\
    \ further a path digest <h0, e1,\n   C1, ..., en, hn> of P.  The definition of\
    \ such a metric might include\n   a conjecture that the delay across P is very\
    \ nearly the sum of the\n   corresponding metric across the exchanges (ei) and\
    \ clouds (Ci) of the\n   given path digest.  The definition would further include\
    \ a note on\n   how a corresponding relation applies to relevant A-frame components,\n\
    \   both for the path P and for the exchanges and clouds of the path\n   digest.}\n\
    \   When the definition of a metric includes a conjecture that the metric\n  \
    \ across the path is related to the metric across the subpaths of the\n   path,\
    \ that conjecture constitutes a claim that the metric exhibits\n   spatial composition.\
    \  The definition should then include:\n +    the specific conjecture applied\
    \ to the metric,\n +    a justification of the practical utility of the composition\
    \ in\n      terms of making accurate measurements of the metric on the path,\n\
    \ +    a justification of the usefulness of the composition in terms of\n    \
    \  making analysis of the path using A-frame concepts more effective,\n      and\n\
    \ +    an analysis of how the conjecture could be incorrect.\n"
- title: 9.2. Temporal Composition of Formal Models and Empirical Metrics
  contents:
  - "9.2. Temporal Composition of Formal Models and Empirical Metrics\n   In some\
    \ cases, it may be realistic and useful to define metrics in\n   such a fashion\
    \ that they exhibit temporal composition.\n   By temporal composition, we mean\
    \ a characteristic of some path\n   metric, in which the metric as applied to\
    \ a path at a given time T is\n   also defined for various times t0 < t1 < ...\
    \ < tn < T, and in which\n   the appropriate A-frame concepts for the metric suggests\
    \ useful\n   relationships between the metric applied at times t0, ..., tn and\
    \ the\n   metric applied at time T.  The effectiveness of temporal composition\n\
    \   depends:\n +    on the usefulness in analysis of these relationships as applied\
    \ to\n      the relevant A-frame components, and\n +    on the practical use of\
    \ the corresponding relationships as applied\n      to metrics and to measurement\
    \ methodologies.\n   {Comment: for example, consider a  metric for the expected\
    \ flow\n   capacity across a path P during the five-minute period surrounding\n\
    \   the time T, and suppose further that we have the corresponding values\n  \
    \ for each of the four previous five-minute periods t0, t1, t2, and t3.\n   The\
    \ definition of such a metric might include a conjecture that the\n   flow capacity\
    \ at time T can be estimated from a certain kind of\n   extrapolation from the\
    \ values of t0, ..., t3.  The definition would\n   further include a note on how\
    \ a corresponding relation applies to\n   relevant A-frame components.\n   Note:\
    \ any (spatial or temporal) compositions involving flow capacity\n   are likely\
    \ to be subtle, and temporal compositions are generally more\n   subtle than spatial\
    \ compositions, so the reader should understand\n   that the foregoing example\
    \ is intentionally naive.}\n   When the definition of a metric includes a conjecture\
    \ that the metric\n   across the path at a given time T is related to the metric\
    \ across the\n   path for a set of other times, that conjecture constitutes a\
    \ claim\n   that the metric exhibits temporal composition.  The definition should\n\
    \   then include:\n +    the specific conjecture applied to the metric,\n +  \
    \  a justification of the practical utility of the composition in\n      terms\
    \ of making accurate measurements of the metric on the path,\n      and\n +  \
    \  a justification of the usefulness of the composition in terms of\n      making\
    \ analysis of the path using A-frame concepts more effective.\n"
- title: 10. Issues related to Time
  contents:
  - '10. Issues related to Time

    '
- title: 10.1. Clock Issues
  contents:
  - "10.1. Clock Issues\n   Measurements of time lie at the heart of many Internet\
    \ metrics.\n   Because of this, it will often be crucial when designing a\n  \
    \ methodology for measuring a metric to understand the different types\n   of\
    \ errors and uncertainties introduced by imperfect clocks.  In this\n   section\
    \ we define terminology for discussing the characteristics of\n   clocks and touch\
    \ upon related measurement issues which need to be\n   addressed by any sound\
    \ methodology.\n   The Network Time Protocol (NTP; RFC 1305) defines a nomenclature\
    \ for\n   discussing clock characteristics, which we will also use when\n   appropriate\
    \ [Mi92].  The main goal of NTP is to provide accurate\n   timekeeping over fairly\
    \ long time scales, such as minutes to days,\n   while for measurement purposes\
    \ often what is more important is\n   short-term accuracy, between the beginning\
    \ of the measurement and the\n   end, or over the course of gathering a body of\
    \ measurements (a\n   sample).  This difference in goals sometimes leads to different\n\
    \   definitions of terminology as well, as discussed below.\n   To begin, we define\
    \ a clock's \"offset\" at a particular moment as the\n   difference between the\
    \ time reported by the clock and the \"true\" time\n   as defined by UTC.  If\
    \ the clock reports a time Tc and the true time\n   is Tt, then the clock's offset\
    \ is Tc - Tt.\n   We will refer to a clock as \"accurate\" at a particular moment\
    \ if the\n   clock's offset is zero, and more generally a clock's \"accuracy\"\
    \ is\n   how close the absolute value of the offset is to zero.  For NTP,\n  \
    \ accuracy also includes a notion of the frequency of the clock; for\n   our purposes,\
    \ we instead incorporate this notion into that of \"skew\",\n   because we define\
    \ accuracy in terms of a single moment in time rather\n   than over an interval\
    \ of time.\n   A clock's \"skew\" at a particular moment is the frequency difference\n\
    \   (first derivative of its offset with respect to true time) between\n   the\
    \ clock and true time.\n   As noted in RFC 1305, real clocks exhibit some variation\
    \ in skew.\n   That is, the second derivative of the clock's offset with respect\
    \ to\n   true time is generally non-zero.  In keeping with RFC 1305, we define\n\
    \   this quantity as the clock's \"drift\".\n   A clock's \"resolution\" is the\
    \ smallest unit by which the clock's time\n   is updated.  It gives a lower bound\
    \ on the clock's uncertainty.\n   (Note that clocks can have very fine resolutions\
    \ and yet be wildly\n   inaccurate.)  Resolution is defined in terms of seconds.\
    \  However,\n   resolution is relative to the clock's reported time and not to\
    \ true\n   time, so for example a resolution of 10 ms only means that the clock\n\
    \   updates its notion of time in 0.01 second increments, not that this\n   is\
    \ the true amount of time between updates.\n   {Comment: Systems differ on how\
    \ an application interface to the clock\n   reports the time on subsequent calls\
    \ during which the clock has not\n   advanced.  Some systems simply return the\
    \ same unchanged time as\n   given for previous calls.  Others may add a small\
    \ increment to the\n   reported time to maintain monotone-increasing timestamps.\
    \  For\n   systems that do the latter, we do *not* consider these small\n   increments\
    \ when defining the clock's resolution.  They are instead an\n   impediment to\
    \ assessing the clock's resolution, since a natural\n   method for doing so is\
    \ to repeatedly query the clock to determine the\n   smallest non-zero difference\
    \ in reported times.}\n   It is expected that a clock's resolution changes only\
    \ rarely (for\n   example, due to a hardware upgrade).\n   There are a number\
    \ of interesting metrics for which some natural\n   measurement methodologies\
    \ involve comparing times reported by two\n   different clocks.  An example is\
    \ one-way packet delay [AK97].  Here,\n   the time required for a packet to travel\
    \ through the network is\n   measured by comparing the time reported by a clock\
    \ at one end of the\n   packet's path, corresponding to when the packet first\
    \ entered the\n   network, with the time reported by a clock at the other end\
    \ of the\n   path, corresponding to when the packet finished traversing the\n\
    \   network.\n   We are thus also interested in terminology for describing how\
    \ two\n   clocks C1 and C2 compare.  To do so, we introduce terms related to\n\
    \   those above in which the notion of \"true time\" is replaced by the\n   time\
    \ as reported by clock C1.  For example, clock C2's offset\n   relative to C1\
    \ at a particular moment is Tc2 - Tc1, the instantaneous\n   difference in time\
    \ reported by C2 and C1.  To disambiguate between\n   the use of the terms to\
    \ compare two clocks versus the use of the\n   terms to compare to true time,\
    \ we will in the former case use the\n   phrase \"relative\".  So the offset defined\
    \ earlier in this paragraph\n   is the \"relative offset\" between C2 and C1.\n\
    \   When comparing clocks, the analog of \"resolution\" is not \"relative\n  \
    \ resolution\", but instead \"joint resolution\", which is the sum of the\n  \
    \ resolutions of C1 and C2.  The joint resolution then indicates a\n   conservative\
    \ lower bound on the accuracy of any time intervals\n   computed by subtracting\
    \ timestamps generated by one clock from those\n   generated by the other.\n \
    \  If two clocks are \"accurate\" with respect to one another (their\n   relative\
    \ offset is zero), we will refer to the pair of clocks as\n   \"synchronized\"\
    .  Note that clocks can be highly synchronized yet\n   arbitrarily inaccurate\
    \ in terms of how well they tell true time.\n   This point is important because\
    \ for many Internet measurements,\n   synchronization between two clocks is more\
    \ important than the\n   accuracy of the clocks.  The is somewhat true of skew,\
    \ too: as long\n   as the absolute skew is not too great, then minimal relative\
    \ skew is\n   more important, as it can induce systematic trends in packet transit\n\
    \   times measured by comparing timestamps produced by the two clocks.\n   These\
    \ distinctions arise because for Internet measurement what is\n   often most important\
    \ are differences in time as computed by comparing\n   the output of two clocks.\
    \  The process of computing the difference\n   removes any error due to clock\
    \ inaccuracies with respect to true\n   time; but it is crucial that the differences\
    \ themselves accurately\n   reflect differences in true time.\n   Measurement\
    \ methodologies will often begin with the step of assuring\n   that two clocks\
    \ are synchronized and have minimal skew and drift.\n   {Comment: An effective\
    \ way to assure these conditions (and also clock\n   accuracy) is by using clocks\
    \ that derive their notion of time from an\n   external source, rather than only\
    \ the host computer's clock.  (These\n   latter are often subject to large errors.)\
    \ It is further preferable\n   that the clocks directly derive their time, for\
    \ example by having\n   immediate access to a GPS (Global Positioning System)\
    \ unit.}\n   Two important concerns arise if the clocks indirectly derive their\n\
    \   time using a network time synchronization protocol such as NTP:\n +    First,\
    \ NTP's accuracy depends in part on the properties\n      (particularly delay)\
    \ of the Internet paths used by the NTP peers,\n      and these might be exactly\
    \ the properties that we wish to measure,\n      so it would be unsound to use\
    \ NTP to calibrate such measurements.\n +    Second, NTP focuses on clock accuracy,\
    \ which can come at the\n      expense of short-term clock skew and drift.  For\
    \ example, when a\n      host's clock is indirectly synchronized to a time source,\
    \ if the\n      synchronization intervals occur infrequently, then the host will\n\
    \      sometimes be faced with the problem of how to adjust its current,\n   \
    \   incorrect time, Ti, with a considerably different, more accurate\n      time\
    \ it has just learned, Ta.  Two general ways in which this is\n      done are\
    \ to either immediately set the current time to Ta, or to\n      adjust the local\
    \ clock's update frequency (hence, its skew) so\n      that at some point in the\
    \ future the local time Ti' will agree\n      with the more accurate time Ta'.\
    \  The first mechanism introduces\n      discontinuities and can also violate\
    \ common assumptions that\n      timestamps are monotone increasing.  If the host's\
    \ clock is set\n      backward in time, sometimes this can be easily detected.\
    \  If the\n      clock is set forward in time, this can be harder to detect. \
    \ The\n      skew induced by the second mechanism can lead to considerable\n \
    \     inaccuracies when computing differences in time, as discussed\n      above.\n\
    \   To illustrate why skew is a crucial concern, consider samples of\n   one-way\
    \ delays between two Internet hosts made at one minute\n   intervals.  The true\
    \ transmission delay between the hosts might\n   plausibly be on the order of\
    \ 50 ms for a transcontinental path.  If\n   the skew between the two clocks is\
    \ 0.01%, that is, 1 part in 10,000,\n   then after 10 minutes of observation the\
    \ error introduced into the\n   measurement is 60 ms.  Unless corrected, this\
    \ error is enough to\n   completely wipe out any accuracy in the transmission\
    \ delay\n   measurement.  Finally, we note that assessing skew errors between\n\
    \   unsynchronized network clocks is an open research area.  (See [Pa97]\n   for\
    \ a discussion of detecting and compensating for these sorts of\n   errors.) This\
    \ shortcoming makes use of a solid, independent clock\n   source such as GPS especially\
    \ desirable.\n"
- title: 10.2. The Notion of "Wire Time"
  contents:
  - "10.2. The Notion of \"Wire Time\"\n   Internet measurement is often complicated\
    \ by the use of Internet\n   hosts themselves to perform the measurement.  These\
    \ hosts can\n   introduce delays, bottlenecks, and the like that are due to hardware\n\
    \   or operating system effects and have nothing to do with the network\n   behavior\
    \ we would like to measure.  This problem is particularly\n   acute when timestamping\
    \ of network events occurs at the application\n   level.\n   In order to provide\
    \ a general way of talking about these effects, we\n   introduce two notions of\
    \ \"wire time\".  These notions are only defined\n   in terms of an Internet host\
    \ H observing an Internet link L at a\n   particular location:\n +    For a given\
    \ packet P, the 'wire arrival time' of P at H on L is\n      the first time T\
    \ at which any bit of P has appeared at H's\n      observational position on L.\n\
    \ +    For a given packet P, the 'wire exit time' of P at H on L is the\n    \
    \  first time T at which all the bits of P have appeared at H's\n      observational\
    \ position on L.\n   Note that intrinsic to the definition is the notion of where\
    \ on the\n   link we are observing.  This distinction is important because for\n\
    \   large-latency links, we may obtain very different times depending on\n   exactly\
    \ where we are observing the link.  We could allow the\n   observational position\
    \ to be an arbitrary location along the link;\n   however, we define it to be\
    \ in terms of an Internet host because we\n   anticipate in practice that, for\
    \ IPPM metrics, all such timing will\n   be constrained to be performed by Internet\
    \ hosts, rather than\n   specialized hardware devices that might be able to monitor\
    \ a link at\n   locations where a host cannot.  This definition also takes care\
    \ of\n   the problem of links that are comprised of multiple physical\n   channels.\
    \  Because these multiple channels are not visible at the IP\n   layer, they cannot\
    \ be individually observed in terms of the above\n   definitions.\n   It is possible,\
    \ though one hopes uncommon, that a packet P might make\n   multiple trips over\
    \ a particular link L, due to a forwarding loop.\n   These trips might even overlap,\
    \ depending on the link technology.\n   Whenever this occurs, we define a separate\
    \ wire time associated with\n   each instance of P seen at H's position on the\
    \ link.  This definition\n   is worth making because it serves as a reminder that\
    \ notions like\n   *the* unique time a packet passes a point in the Internet are\n\
    \   inherently slippery.\n   The term wire time has historically been used to\
    \ loosely denote the\n   time at which a packet appeared on a link, without exactly\
    \ specifying\n   whether this refers to the first bit, the last bit, or some other\n\
    \   consideration.  This informal definition is generally already very\n   useful,\
    \ as it is usually used to make a distinction between when the\n   packet's propagation\
    \ delays begin and cease to be due to the network\n   rather than the endpoint\
    \ hosts.\n   When appropriate, metrics should be defined in terms of wire times\n\
    \   rather than host endpoint times, so that the metric's definition\n   highlights\
    \ the issue of separating delays due to the host from those\n   due to the network.\n\
    \   We note that one potential difficulty when dealing with wire times\n   concerns\
    \ IP fragments.  It may be the case that, due to\n   fragmentation, only a portion\
    \ of a particular packet passes by H's\n   location.  Such fragments are themselves\
    \ legitimate packets and have\n   well-defined wire times associated with them;\
    \ but the larger IP\n   packet corresponding to their aggregate may not.\n   We\
    \ also note that these notions have not, to our knowledge, been\n   previously\
    \ defined in exact terms for Internet traffic.\n   Consequently, we may find with\
    \ experience that these definitions\n   require some adjustment in the future.\n\
    \   {Comment: It can sometimes be difficult to measure wire times.  One\n   technique\
    \ is to use a packet filter to monitor traffic on a link.\n   The architecture\
    \ of these filters often attempts to associate with\n   each packet a timestamp\
    \ as close to the wire time as possible.  We\n   note however that one common\
    \ source of error is to run the packet\n   filter on one of the endpoint hosts.\
    \  In this case, it has been\n   observed that some packet filters receive for\
    \ some packets timestamps\n   corresponding to when the packet was *scheduled*\
    \ to be injected into\n   the network, rather than when it actually was *sent*\
    \ out onto the\n   network (wire time).  There can be a substantial difference\
    \ between\n   these two times.  A technique for dealing with this problem is to\
    \ run\n   the packet filter on a separate host that passively monitors the\n \
    \  given link.  This can be problematic however for some link\n   technologies.\
    \  See [Pa97] for a discussion of the sorts of errors\n   packet filters can exhibit.\
    \  Finally, we note that packet filters\n   will often only capture the first\
    \ fragment of a fragmented IP packet,\n   due to the use of filtering on fields\
    \ in the IP and transport\n   protocol headers.  As we generally desire our measurement\n\
    \   methodologies to avoid the complexity of creating fragmented traffic,\n  \
    \ one strategy for dealing with their presence as detected by a packet\n   filter\
    \ is to flag that the measured traffic has an unusual form and\n   abandon further\
    \ analysis of the packet timing.}\n"
- title: 11. Singletons, Samples, and Statistics
  contents:
  - "11. Singletons, Samples, and Statistics\n   With experience we have found it\
    \ useful to introduce a separation\n   between three distinct -- yet related --\
    \ notions:\n +    By a 'singleton' metric, we refer to metrics that are, in a\
    \ sense,\n      atomic.  For example, a single instance of \"bulk throughput\n\
    \      capacity\" from one host to another might be defined as a singleton\n \
    \     metric, even though the instance involves measuring the timing of\n    \
    \  a number of Internet packets.\n +    By a 'sample' metric, we refer to metrics\
    \ derived from a given\n      singleton metric by taking a number of distinct\
    \ instances\n      together.  For example, we might define a sample metric of\
    \ one-way\n      delays from one host to another as an hour's worth of\n     \
    \ measurements, each made at Poisson intervals with a mean spacing\n      of one\
    \ second.\n +    By a 'statistical' metric, we refer to metrics derived from a\n\
    \      given sample metric by computing some statistic of the values\n      defined\
    \ by the singleton metric on the sample.  For example, the\n      mean of all\
    \ the one-way delay values on the sample given above\n      might be defined as\
    \ a statistical metric.\n   By applying these notions of singleton, sample, and\
    \ statistic in a\n   consistent way, we will be able to reuse lessons learned\
    \ about how to\n   define samples and statistics on various metrics.  The orthogonality\n\
    \   among these three notions will thus make all our work more effective\n   and\
    \ more intelligible by the community.\n   In the remainder of this section, we\
    \ will cover some topics in\n   sampling and statistics that we believe will be\
    \ important to a\n   variety of metric definitions and measurement efforts.\n"
- title: 11.1. Methods of Collecting Samples
  contents:
  - "11.1. Methods of Collecting Samples\n   The main reason for collecting samples\
    \ is to see what sort of\n   variations and consistencies are present in the metric\
    \ being\n   measured.  These variations might be with respect to different points\n\
    \   in the Internet, or different measurement times.  When assessing\n   variations\
    \ based on a sample, one generally makes an assumption that\n   the sample is\
    \ \"unbiased\", meaning that the process of collecting the\n   measurements in\
    \ the sample did not skew the sample so that it no\n   longer accurately reflects\
    \ the metric's variations and consistencies.\n   One common way of collecting\
    \ samples is to make measurements\n   separated by fixed amounts of time: periodic\
    \ sampling.  Periodic\n   sampling is particularly attractive because of its simplicity,\
    \ but it\n   suffers from two potential problems:\n +    If the metric being measured\
    \ itself exhibits periodic behavior,\n      then there is a possibility that the\
    \ sampling will observe only\n      part of the periodic behavior if the periods\
    \ happen to agree\n      (either directly, or if one is a multiple of the other).\
    \  Related\n      to this problem is the notion that periodic sampling can be\
    \ easily\n      anticipated.  Predictable sampling is susceptible to manipulation\n\
    \      if there are mechanisms by which a network component's behavior\n     \
    \ can be temporarily changed such that the sampling only sees the\n      modified\
    \ behavior.\n +    The act of measurement can perturb what is being measured (for\n\
    \      example, injecting measurement traffic into a network alters the\n    \
    \  congestion level of the network), and repeated periodic\n      perturbations\
    \ can drive a network into a state of synchronization\n      (cf. [FJ94]), greatly\
    \ magnifying what might individually be minor\n      effects.\n   A more sound\
    \ approach is based on \"random additive sampling\": samples\n   are separated\
    \ by independent, randomly generated intervals that have\n   a common statistical\
    \ distribution G(t) [BM92].  The quality of this\n   sampling depends on the distribution\
    \ G(t).  For example, if G(t)\n   generates a constant value g with probability\
    \ one, then the sampling\n   reduces to periodic sampling with a period of g.\n\
    \   Random additive sampling gains significant advantages.  In general,\n   it\
    \ avoids synchronization effects and yields an unbiased estimate of\n   the property\
    \ being sampled.  The only significant drawbacks with it\n   are:\n +    it complicates\
    \ frequency-domain analysis, because the samples do\n      not occur at fixed\
    \ intervals such as assumed by Fourier-transform\n      techniques; and\n +  \
    \  unless G(t) is the exponential distribution (see below), sampling\n      still\
    \ remains somewhat predictable, as discussed for periodic\n      sampling above.\n"
- title: 11.1.1. Poisson Sampling
  contents:
  - "11.1.1. Poisson Sampling\n   It can be proved that if G(t) is an exponential\
    \ distribution with\n   rate lambda, that is\n       G(t) = 1 - exp(-lambda *\
    \ t)\n   then the arrival of new samples *cannot* be predicted (and, again,\n\
    \   the sampling is unbiased).  Furthermore, the sampling is\n   asymptotically\
    \ unbiased even if the act of sampling affects the\n   network's state.  Such\
    \ sampling is referred to as \"Poisson sampling\".\n   It is not prone to inducing\
    \ synchronization, it can be used to\n   accurately collect measurements of periodic\
    \ behavior, and it is not\n   prone to manipulation by anticipating when new samples\
    \ will occur.\n   Because of these valuable properties, we in general prefer that\n\
    \   samples of Internet measurements are gathered using Poisson sampling.\n  \
    \ {Comment: We note, however, that there may be circumstances that\n   favor use\
    \ of a different G(t).  For example, the exponential\n   distribution is unbounded,\
    \ so its use will on occasion generate\n   lengthy spaces between sampling times.\
    \  We might instead desire to\n   bound the longest such interval to a maximum\
    \ value dT, to speed the\n   convergence of the estimation derived from the sampling.\
    \  This could\n   be done by using\n       G(t) = Unif(0, dT)\n   that is, the\
    \ uniform distribution between 0 and dT.  This sampling,\n   of course, becomes\
    \ highly predictable if an interval of nearly length\n   dT has elapsed without\
    \ a sample occurring.}\n   In its purest form, Poisson sampling is done by generating\n\
    \   independent, exponentially distributed intervals and gathering a\n   single\
    \ measurement after each interval has elapsed.  It can be shown\n   that if starting\
    \ at time T one performs Poisson sampling over an\n   interval dT, during which\
    \ a total of N measurements happen to be\n   made, then those measurements will\
    \ be uniformly distributed over the\n   interval [T, T+dT].  So another way of\
    \ conducting Poisson sampling is\n   to pick dT and N and generate N random sampling\
    \ times uniformly over\n   the interval [T, T+dT].  The two approaches are equivalent,\
    \ except if\n   N and dT are externally known.  In that case, the property of\
    \ not\n   being able to predict measurement times is weakened (the other\n   properties\
    \ still hold).  The N/dT approach has an advantage that\n   dealing with fixed\
    \ values of N and dT can be simpler than dealing\n   with a fixed lambda but variable\
    \ numbers of measurements over\n   variably-sized intervals.\n"
- title: 11.1.2. Geometric Sampling
  contents:
  - "11.1.2. Geometric Sampling\n   Closely related to Poisson sampling is \"geometric\
    \ sampling\", in which\n   external events are measured with a fixed probability\
    \ p.  For\n   example, one might capture all the packets over a link but only\n\
    \   record the packet to a trace file if a randomly generated number\n   uniformly\
    \ distributed between 0 and 1 is less than a given p.\n   Geometric sampling has\
    \ the same properties of being unbiased and not\n   predictable in advance as\
    \ Poisson sampling, so if it fits a\n   particular Internet measurement task,\
    \ it too is sound.  See [CPB93]\n   for more discussion.\n"
- title: 11.1.3. Generating Poisson Sampling Intervals
  contents:
  - "11.1.3. Generating Poisson Sampling Intervals\n   To generate Poisson sampling\
    \ intervals, one first determines the rate\n   lambda at which the singleton measurements\
    \ will on average be made\n   (e.g., for an average sampling interval of 30 seconds,\
    \ we have lambda\n   = 1/30, if the units of time are seconds).  One then generates\
    \ a\n   series of exponentially-distributed (pseudo) random numbers E1, E2,\n\
    \   ..., En.  The first measurement is made at time E1, the next at time\n   E1+E2,\
    \ and so on.\n   One technique for generating exponentially-distributed (pseudo)\n\
    \   random numbers is based on the ability to generate U1, U2, ..., Un,\n   (pseudo)\
    \ random numbers that are uniformly distributed between 0 and\n   1.  Many computers\
    \ provide libraries that can do this.  Given such\n   Ui, to generate Ei one uses:\n\
    \       Ei = -log(Ui) / lambda\n   where log(Ui) is the natural logarithm of Ui.\
    \  {Comment: This\n   technique is an instance of the more general \"inverse transform\"\
    \n   method for generating random numbers with a given distribution.}\n   Implementation\
    \ details:\n   There are at least three different methods for approximating Poisson\n\
    \   sampling, which we describe here as Methods 1 through 3.  Method 1 is\n  \
    \ the easiest to implement and has the most error, and method 3 is the\n   most\
    \ difficult to implement and has the least error (potentially\n   none).\n   Method\
    \ 1 is to proceed as follows:\n   1.  Generate E1 and wait that long.\n   2. \
    \ Perform a measurement.\n   3.  Generate E2 and wait that long.\n   4.  Perform\
    \ a measurement.\n   5.  Generate E3 and wait that long.\n   6.  Perform a measurement\
    \ ...\n   The problem with this approach is that the \"Perform a measurement\"\
    \n   steps themselves take time, so the sampling is not done at times E1,\n  \
    \ E1+E2, etc., but rather at E1, E1+M1+E2, etc., where Mi is the amount\n   of\
    \ time required for the i'th measurement.  If Mi is very small\n   compared to\
    \ 1/lambda then the potential error introduced by this\n   technique is likewise\
    \ small.  As Mi becomes a non-negligible fraction\n   of 1/lambda, the potential\
    \ error increases.\n   Method 2 attempts to correct this error by taking into\
    \ account the\n   amount of time required by the measurements (i.e., the Mi's)\
    \ and\n   adjusting the waiting intervals accordingly:\n   1.  Generate E1 and\
    \ wait that long.\n   2.  Perform a measurement and measure M1, the time it took\
    \ to do so.\n   3.  Generate E2 and wait for a time E2-M1.\n   4.  Perform a measurement\
    \ and measure M2 ..\n   This approach works fine as long as E{i+1} >= Mi.  But\
    \ if E{i+1} < Mi\n   then it is impossible to wait the proper amount of time.\
    \  (Note that\n   this case corresponds to needing to perform two measurements\n\
    \   simultaneously.)\n   Method 3 is generating a schedule of measurement times\
    \ E1, E1+E2,\n   etc., and then sticking to it:\n   1.  Generate E1, E2, ...,\
    \ En.\n   2.  Compute measurement times T1, T2, ..., Tn, as Ti = E1 + ... + Ei.\n\
    \   3.  Arrange that at times T1, T2, ..., Tn, a measurement is made.\n   By allowing\
    \ simultaneous measurements, Method 3 avoids the\n   shortcomings of Methods 1\
    \ and 2.  If, however, simultaneous\n   measurements interfere with one another,\
    \ then Method 3 does not gain\n   any benefit and may actually prove worse than\
    \ Methods 1 or 2.\n   For Internet phenomena, it is not known to what degree the\n\
    \   inaccuracies of these methods are significant.  If the Mi's are much\n   less\
    \ than 1/lambda, then any of the three should suffice.  If the\n   Mi's are less\
    \ than 1/lambda but perhaps not greatly less, then Method\n   2 is preferred to\
    \ Method 1.  If simultaneous measurements do not\n   interfere with one another,\
    \ then Method 3 is preferred, though it can\n   be considerably harder to implement.\n"
- title: 11.2. Self-Consistency
  contents:
  - "11.2. Self-Consistency\n   A fundamental requirement for a sound measurement\
    \ methodology is that\n   measurement be made using as few unconfirmed assumptions\
    \ as possible.\n   Experience has painfully shown how easy it is to make an (often\n\
    \   implicit) assumption that turns out to be incorrect.  An example is\n   incorporating\
    \ into a measurement the reading of a clock synchronized\n   to a highly accurate\
    \ source.  It is easy to assume that the clock is\n   therefore accurate; but\
    \ due to software bugs, a loss of power in the\n   source, or a loss of communication\
    \ between the source and the clock,\n   the clock could actually be quite inaccurate.\n\
    \   This is not to argue that one must not make *any* assumptions when\n   measuring,\
    \ but rather that, to the extent which is practical,\n   assumptions should be\
    \ tested.  One powerful way for doing so involves\n   checking for self-consistency.\
    \  Such checking applies both to the\n   observed value(s) of the measurement\
    \ *and the values used by the\n   measurement process itself*.  A simple example\
    \ of the former is that\n   when computing a round trip time, one should check\
    \ to see if it is\n   negative.  Since negative time intervals are non-physical,\
    \ if it ever\n   is negative that finding immediately flags an error.  *These\
    \ sorts of\n   errors should then be investigated!* It is crucial to determine\
    \ where\n   the error lies, because only by doing so diligently can we build up\n\
    \   faith in a methodology's fundamental soundness.  For example, it\n   could\
    \ be that the round trip time is negative because during the\n   measurement the\
    \ clock was set backward in the process of\n   synchronizing it with another source.\
    \  But it could also be that the\n   measurement program accesses uninitialized\
    \ memory in one of its\n   computations and, only very rarely, that leads to a\
    \ bogus\n   computation.  This second error is more serious, if the same program\n\
    \   is used by others to perform the same measurement, since then they\n   too\
    \ will suffer from incorrect results.  Furthermore, once uncovered\n   it can\
    \ be completely fixed.\n   A more subtle example of testing for self-consistency\
    \ comes from\n   gathering samples of one-way Internet delays.  If one has a large\n\
    \   sample of such delays, it may well be highly telling to, for example,\n  \
    \ fit a line to the pairs of (time of measurement, measured delay), to\n   see\
    \ if the resulting line has a clearly non-zero slope.  If so, a\n   possible interpretation\
    \ is that one of the clocks used in the\n   measurements is skewed relative to\
    \ the other.  Another interpretation\n   is that the slope is actually due to\
    \ genuine network effects.\n   Determining which is indeed the case will often\
    \ be highly\n   illuminating.  (See [Pa97] for a discussion of distinguishing\
    \ between\n   relative clock skew and genuine network effects.) Furthermore, if\n\
    \   making this check is part of the methodology, then a finding that the\n  \
    \ long-term slope is very near zero is positive evidence that the\n   measurements\
    \ are probably not biased by a difference in skew.\n   A final example illustrates\
    \ checking the measurement process itself\n   for self-consistency.  Above we\
    \ outline Poisson sampling techniques,\n   based on generating exponentially-distributed\
    \ intervals.  A sound\n   measurement methodology would include testing the generated\
    \ intervals\n   to see whether they are indeed exponentially distributed (and\
    \ also to\n   see if they suffer from correlation).  In the appendix we discuss\
    \ and\n   give C code for one such technique, a general-purpose, well-regarded\n\
    \   goodness-of-fit test called the Anderson-Darling test.\n   Finally, we note\
    \ that what is truly relevant for Poisson sampling of\n   Internet metrics is\
    \ often not when the measurements began but the\n   wire times corresponding to\
    \ the measurement process.  These could\n   well be different, due to complications\
    \ on the hosts used to perform\n   the measurement.  Thus, even those with complete\
    \ faith in their\n   pseudo-random number generators and subsequent algorithms\
    \ are\n   encouraged to consider how they might test the assumptions of each\n\
    \   measurement procedure as much as possible.\n"
- title: 11.3. Defining Statistical Distributions
  contents:
  - "11.3. Defining Statistical Distributions\n   One way of describing a collection\
    \ of measurements (a sample) is as a\n   statistical distribution -- informally,\
    \ as percentiles.  There are\n   several slightly different ways of doing so.\
    \  In this section we\n   define a standard definition to give uniformity to these\n\
    \   descriptions.\n   The \"empirical distribution function\" (EDF) of a set of\
    \ scalar\n   measurements is a function F(x) which for any x gives the fractional\n\
    \   proportion of the total measurements that were <= x.  If x is less\n   than\
    \ the minimum value observed, then F(x) is 0.  If it is greater or\n   equal to\
    \ the maximum value observed, then F(x) is 1.\n   For example, given the 6 measurements:\n\
    \   -2, 7, 7, 4, 18, -5\n   Then F(-8) = 0, F(-5) = 1/6, F(-5.0001) = 0, F(-4.999)\
    \ = 1/6, F(7) =\n   5/6, F(18) = 1, F(239) = 1.\n   Note that we can recover the\
    \ different measured values and how many\n   times each occurred from F(x) --\
    \ no information regarding the range\n   in values is lost.  Summarizing measurements\
    \ using histograms, on the\n   other hand, in general loses information about\
    \ the different values\n   observed, so the EDF is preferred.\n   Using either\
    \ the EDF or a histogram, however, we do lose information\n   regarding the order\
    \ in which the values were observed.  Whether this\n   loss is potentially significant\
    \ will depend on the metric being\n   measured.\n   We will use the term \"percentile\"\
    \ to refer to the smallest value of x\n   for which F(x) >= a given percentage.\
    \  So the 50th percentile of the\n   example above is 4, since F(4) = 3/6 = 50%;\
    \ the 25th percentile is\n   -2, since F(-5) = 1/6 < 25%, and F(-2) = 2/6 >= 25%;\
    \ the 100th\n   percentile is 18; and the 0th percentile is -infinity, as is the\
    \ 15th\n   percentile.\n   Care must be taken when using percentiles to summarize\
    \ a sample,\n   because they can lend an unwarranted appearance of more precision\n\
    \   than is really available.  Any such summary must include the sample\n   size\
    \ N, because any percentile difference finer than 1/N is below the\n   resolution\
    \ of the sample.\n   See [DS86] for more details regarding EDF's.\n   We close\
    \ with a note on the common (and important!) notion of median.\n   In statistics,\
    \ the median of a distribution is defined to be the\n   point X for which the\
    \ probability of observing a value <= X is equal\n   to the probability of observing\
    \ a value > X.  When estimating the\n   median of a set of observations, the estimate\
    \ depends on whether the\n   number of observations, N, is odd or even:\n +  \
    \  If N is odd, then the 50th percentile as defined above is used as\n      the\
    \ estimated median.\n +    If N is even, then the estimated median is the average\
    \ of the\n      central two observations; that is, if the observations are sorted\n\
    \      in ascending order and numbered from 1 to N, where N = 2*K, then\n    \
    \  the estimated median is the average of the (K)'th and (K+1)'th\n      observations.\n\
    \   Usually the term \"estimated\" is dropped from the phrase \"estimated\n  \
    \ median\" and this value is simply referred to as the \"median\".\n"
- title: 11.4. Testing For Goodness-of-Fit
  contents:
  - "11.4. Testing For Goodness-of-Fit\n   For some forms of measurement calibration\
    \ we need to test whether a\n   set of numbers is consistent with those numbers\
    \ having been drawn\n   from a particular distribution.  An example is that to\
    \ apply a self-\n   consistency check to measurements made using a Poisson process,\
    \ one\n   test is to see whether the spacing between the sampling times does\n\
    \   indeed reflect an exponential distribution; or if the dT/N approach\n   discussed\
    \ above was used, whether the times are uniformly distributed\n   across [T, dT].\n\
    \   {Comment: There are at least three possible sets of values we could\n   test:\
    \ the scheduled packet transmission times, as determined by use\n   of a pseudo-random\
    \ number generator; user-level timestamps made just\n   before or after the system\
    \ call for transmitting the packet; and wire\n   times for the packets as recorded\
    \ using a packet filter.  All three\n   of these are potentially informative:\
    \ failures for the scheduled\n   times to match an exponential distribution indicate\
    \ inaccuracies in\n   the random number generation; failures for the user-level\
    \ times\n   indicate inaccuracies in the timers used to schedule transmission;\n\
    \   and failures for the wire times indicate inaccuracies in actually\n   transmitting\
    \ the packets, perhaps due to contention for a shared\n   resource.}\n   There\
    \ are a large number of statistical goodness-of-fit techniques\n   for performing\
    \ such tests.  See [DS86] for a thorough discussion.\n   That reference recommends\
    \ the Anderson-Darling EDF test as being a\n   good all-purpose test, as well\
    \ as one that is especially good at\n   detecting deviations from a given distribution\
    \ in the lower and upper\n   tails of the EDF.\n   It is important to understand\
    \ that the nature of goodness-of-fit\n   tests is that one first selects a \"\
    significance level\", which is the\n   probability that the test will erroneously\
    \ declare that the EDF of a\n   given set of measurements fails to match a particular\
    \ distribution\n   when in fact the measurements do indeed reflect that distribution.\n\
    \   Unless otherwise stated, IPPM goodness-of-fit tests are done using 5%\n  \
    \ significance.  This means that if the test is applied to 100 samples\n   and\
    \ 5 of those samples are deemed to have failed the test, then the\n   samples\
    \ are all consistent with the distribution being tested.  If\n   significantly\
    \ more of the samples fail the test, then the assumption\n   that the samples\
    \ are consistent with the distribution being tested\n   must be rejected.  If\
    \ significantly fewer of the samples fail the\n   test, then the samples have\
    \ potentially been doctored too well to fit\n   the distribution.  Similarly,\
    \ some goodness-of-fit tests (including\n   Anderson-Darling) can detect whether\
    \ it is likely that a given sample\n   was doctored.  We also use a significance\
    \ of 5% for this case; that\n   is, the test will report that a given honest sample\
    \ is \"too good to\n   be true\" 5% of the time, so if the test reports this finding\n\
    \   significantly more often than one time out of twenty, it is an\n   indication\
    \ that something unusual is occurring.\n   The appendix gives sample C code for\
    \ implementing the Anderson-\n   Darling test, as well as further discussing its\
    \ use.\n   See [Pa94] for a discussion of goodness-of-fit and closeness-of-fit\n\
    \   tests in the context of network measurement.\n"
- title: 12. Avoiding Stochastic Metrics
  contents:
  - "12. Avoiding Stochastic Metrics\n   When defining metrics applying to a path,\
    \ subpath, cloud, or other\n   network element, we in general do not define them\
    \ in stochastic terms\n   (probabilities).  We instead prefer a deterministic\
    \ definition.  So,\n   for example, rather than defining a metric about a \"packet\
    \ loss\n   probability between A and B\", we would define a metric about a\n \
    \  \"packet loss rate between A and B\".  (A measurement given by the\n   first\
    \ definition might be \"0.73\", and by the second \"73 packets out\n   of 100\"\
    .)\n   We emphasize that the above distinction concerns the *definitions* of\n\
    \   *metrics*.  It is not intended to apply to what sort of techniques we\n  \
    \ might use to analyze the results of measurements.\n   The reason for this distinction\
    \ is as follows.  When definitions are\n   made in terms of probabilities, there\
    \ are often hidden assumptions in\n   the definition about a stochastic model\
    \ of the behavior being\n   measured.  The fundamental goal with avoiding probabilities\
    \ in our\n   metric definitions is to avoid biasing our definitions by these\n\
    \   hidden assumptions.\n   For example, an easy hidden assumption to make is\
    \ that packet loss in\n   a network component due to queueing overflows can be\
    \ described as\n   something that happens to any given packet with a particular\n\
    \   probability.  In today's Internet, however, queueing drops are\n   actually\
    \ usually *deterministic*, and assuming that they should be\n   described probabilistically\
    \ can obscure crucial correlations between\n   queueing drops among a set of packets.\
    \  So it's better to explicitly\n   note stochastic assumptions, rather than have\
    \ them sneak into our\n   definitions implicitly.\n   This does *not* mean that\
    \ we abandon stochastic models for\n   *understanding* network performance! It\
    \ only means that when defining\n   IP metrics we avoid terms such as \"probability\"\
    \ for terms like\n   \"proportion\" or \"rate\".  We will still use, for example,\
    \ random\n   sampling in order to estimate probabilities used by stochastic models\n\
    \   related to the IP metrics.  We also do not rule out the possibility\n   of\
    \ stochastic metrics when they are truly appropriate (for example,\n   perhaps\
    \ to model transmission errors caused by certain types of line\n   noise).\n"
- title: 13. Packets of Type P
  contents:
  - "13. Packets of Type P\n   A fundamental property of many Internet metrics is\
    \ that the value of\n   the metric depends on the type of IP packet(s) used to\
    \ make the\n   measurement.  Consider an IP-connectivity metric: one obtains\n\
    \   different results depending on whether one is interested in\n   connectivity\
    \ for packets destined for well-known TCP ports or\n   unreserved UDP ports, or\
    \ those with invalid IP checksums, or those\n   with TTL's of 16, for example.\
    \  In some circumstances these\n   distinctions will be highly interesting (for\
    \ example, in the presence\n   of firewalls, or RSVP reservations).\n   Because\
    \ of this distinction, we introduce the generic notion of a\n   \"packet of type\
    \ P\", where in some contexts P will be explicitly\n   defined (i.e., exactly\
    \ what type of packet we mean), partially\n   defined (e.g., \"with a payload\
    \ of B octets\"), or left generic.  Thus\n   we may talk about generic IP-type-P-connectivity\
    \ or more specific\n   IP-port-HTTP-connectivity.  Some metrics and methodologies\
    \ may be\n   fruitfully defined using generic type P definitions which are then\n\
    \   made specific when performing actual measurements.\n   Whenever a metric's\
    \ value depends on the type of the packets involved\n   in the metric, the metric's\
    \ name will include either a specific type\n   or a phrase such as \"type-P\"\
    .  Thus we will not define an \"IP-\n   connectivity\" metric but instead an \"\
    IP-type-P-connectivity\" metric\n   and/or perhaps an \"IP-port-HTTP-connectivity\"\
    \ metric.  This naming\n   convention serves as an important reminder that one\
    \ must be conscious\n   of the exact type of traffic being measured.\n   A closely\
    \ related note: it would be very useful to know if a given\n   Internet component\
    \ treats equally a class C of different types of\n   packets.  If so, then any\
    \ one of those types of packets can be used\n   for subsequent measurement of\
    \ the component.  This suggests we devise\n   a metric or suite of metrics that\
    \ attempt to determine C.\n"
- title: 14. Internet Addresses vs. Hosts
  contents:
  - "14. Internet Addresses vs. Hosts\n   When considering a metric for some path\
    \ through the Internet, it is\n   often natural to think about it as being for\
    \ the path from Internet\n   host H1 to host H2.  A definition in these terms,\
    \ though, can be\n   ambiguous, because Internet hosts can be attached to more\
    \ than one\n   network.  In this case, the result of the metric will depend on\
    \ which\n   of these networks is actually used.\n   Because of this ambiguity,\
    \ usually such definitions should instead be\n   defined in terms of Internet\
    \ IP addresses.  For the common case of a\n   unidirectional path through the\
    \ Internet, we will use the term \"Src\"\n   to denote the IP address of the beginning\
    \ of the path, and \"Dst\" to\n   denote the IP address of the end.\n"
- title: 15. Standard-Formed Packets
  contents:
  - "15. Standard-Formed Packets\n   Unless otherwise stated, all metric definitions\
    \ that concern IP\n   packets include an implicit assumption that the packet is\
    \ *standard\n   formed*.  A packet is standard formed if it meets all of the\n\
    \   following criteria:\n +    Its length as given in the IP header corresponds\
    \ to the size of\n      the IP header plus the size of the payload.\n +    It\
    \ includes a valid IP header: the version field is 4 (later, we\n      will expand\
    \ this to include 6); the header length is >= 5; the\n      checksum is correct.\n\
    \ +    It is not an IP fragment.\n +    The source and destination addresses correspond\
    \ to the hosts in\n      question.\n +    Either the packet possesses sufficient\
    \ TTL to travel from the\n      source to the destination if the TTL is decremented\
    \ by one at each\n      hop, or it possesses the maximum TTL of 255.\n +    It\
    \ does not contain IP options unless explicitly noted.\n +    If a transport header\
    \ is present, it too contains a valid checksum\n      and other valid fields.\n\
    \   We further require that if a packet is described as having a \"length\n  \
    \ of B octets\", then 0 <= B <= 65535; and if B is the payload length in\n   octets,\
    \ then B <= (65535-IP header size in octets).\n   So, for example, one might imagine\
    \ defining an IP connectivity metric\n   as \"IP-type-P-connectivity for standard-formed\
    \ packets with the IP\n   TOS field set to 0\", or, more succinctly, \"IP-type-P-connectivity\n\
    \   with the IP TOS field set to 0\", since standard-formed is already\n   implied\
    \ by convention.\n   A particular type of standard-formed packet often useful\
    \ to consider\n   is the \"minimal IP packet from A to B\" - this is an IP packet\
    \ with\n   the following properties:\n +    It is standard-formed.\n +    Its\
    \ data payload is 0 octets.\n +    It contains no options.\n   (Note that we do\
    \ not define its protocol field, as different values\n   may lead to different\
    \ treatment by the network.)\n   When defining IP metrics we keep in mind that\
    \ no packet smaller or\n   simpler than this can be transmitted over a correctly\
    \ operating IP\n   network.\n"
- title: 16. Acknowledgements
  contents:
  - "16. Acknowledgements\n   The comments of Brian Carpenter, Bill Cerveny, Padma\
    \ Krishnaswamy\n   Jeff Sedayao and Howard Stanislevic are appreciated.\n"
- title: 17. Security Considerations
  contents:
  - "17. Security Considerations\n   This document concerns definitions and concepts\
    \ related to Internet\n   measurement.  We discuss measurement procedures only\
    \ in high-level\n   terms, regarding principles that lend themselves to sound\n\
    \   measurement.  As such, the topics discussed do not affect the\n   security\
    \ of the Internet or of applications which run on it.\n   That said, it should\
    \ be recognized that conducting Internet\n   measurements can raise both security\
    \ and privacy concerns.  Active\n   techniques, in which traffic is injected into\
    \ the network, can be\n   abused for denial-of-service attacks disguised as legitimate\n\
    \   measurement activity.  Passive techniques, in which existing traffic\n   is\
    \ recorded and analyzed, can expose the contents of Internet traffic\n   to unintended\
    \ recipients.  Consequently, the definition of each\n   metric and methodology\
    \ must include a corresponding discussion of\n   security considerations.\n"
- title: 18. Appendix
  contents:
  - "18. Appendix\n   Below we give routines written in C for computing the Anderson-\n\
    \   Darling test statistic (A2) for determining whether a set of values\n   is\
    \ consistent with a given statistical distribution.  Externally, the\n   two main\
    \ routines of interest are:\n       double exp_A2_known_mean(double x[], int n,\
    \ double mean)\n       double unif_A2_known_range(double x[], int n,\n       \
    \                           double min_val, double max_val)\n   Both take as their\
    \ first argument, x, the array of n values to be\n   tested.  (Upon return, the\
    \ elements of x are sorted.)  The remaining\n   parameters characterize the distribution\
    \ to be used: either the mean\n   (1/lambda), for an exponential distribution,\
    \ or the lower and upper\n   bounds, for a uniform distribution.  The names of\
    \ the routines stress\n   that these values must be known in advance, and *not*\
    \ estimated from\n   the data (for example, by computing its sample mean).  Estimating\
    \ the\n   parameters from the data *changes* the significance level of the test\n\
    \   statistic.  While [DS86] gives alternate significance tables for some\n  \
    \ instances in which the parameters are estimated from the data, for\n   our purposes\
    \ we expect that we should indeed know the parameters in\n   advance, since what\
    \ we will be testing are generally values such as\n   packet sending times that\
    \ we wish to verify follow a known\n   distribution.\n   Both routines return\
    \ a significance level, as described earlier. This\n   is a value between 0 and\
    \ 1.  The correct use of the routines is to\n   pick in advance the threshold\
    \ for the significance level to test;\n   generally, this will be 0.05, corresponding\
    \ to 5%, as also described\n   above.  Subsequently, if the routines return a\
    \ value strictly less\n   than this threshold, then the data are deemed to be\
    \ inconsistent with\n   the presumed distribution, *subject to an error corresponding\
    \ to the\n   significance level*.  That is, for a significance level of 5%, 5%\
    \ of\n   the time data that is indeed drawn from the presumed distribution\n \
    \  will be erroneously deemed inconsistent.\n   Thus, it is important to bear\
    \ in mind that if these routines are used\n   frequently, then one will indeed\
    \ encounter occasional failures, even\n   if the data is unblemished.\n   Another\
    \ important point concerning significance levels is that it is\n   unsound to\
    \ compare them in order to determine which of two sets of\n   values is a \"better\"\
    \ fit to a presumed distribution.  Such testing\n   should instead be done using\
    \ \"closeness-of-fit metrics\" such as the\n   lambda^2 metric described in [Pa94].\n\
    \   While the routines provided are for exponential and uniform\n   distributions\
    \ with known parameters, it is generally straight-forward\n   to write comparable\
    \ routines for any distribution with known\n   parameters.  The heart of the A2\
    \ tests lies in a statistic computed\n   for testing whether a set of values is\
    \ consistent with a uniform\n   distribution between 0 and 1, which we term Unif(0,\
    \ 1).  If we wish\n   to test whether a set of values, X, is consistent with a\
    \ given\n   distribution G(x), we first compute\n       Y = G_inverse(X)\n   If\
    \ X is indeed distributed according to G(x), then Y will be\n   distributed according\
    \ to Unif(0, 1); so by testing Y for consistency\n   with Unif(0, 1), we also\
    \ test X for consistency with G(x).\n   We note, however, that the process of\
    \ computing Y above might yield\n   values of Y outside the range (0..1).  Such\
    \ values should not occur\n   if X is indeed distributed according to G(x), but\
    \ easily can occur if\n   it is not.  In the latter case, we need to avoid computing\
    \ the\n   central A2 statistic, since floating-point exceptions may occur if\n\
    \   any of the values lie outside (0..1).  Accordingly, the routines\n   check\
    \ for this possibility, and if encountered, return a raw A2\n   statistic of -1.\
    \  The routine that converts the raw A2 statistic to a\n   significance level\
    \ likewise propagates this value, returning a\n   significance level of -1.  So,\
    \ any use of these routines must be\n   prepared for a possible negative significance\
    \ level.\n   The last important point regarding use of A2 statistic concerns n,\n\
    \   the number of values being tested.  If n < 5 then the test is not\n   meaningful,\
    \ and in this case a significance level of -1 is returned.\n   On the other hand,\
    \ for \"real\" data the test *gains* power as n\n   becomes larger.  It is well\
    \ known in the statistics community that\n   real data almost never exactly matches\
    \ a theoretical distribution,\n   even in cases such as rolling dice a great many\
    \ times (see [Pa94] for\n   a brief discussion and references).  The A2 test is\
    \ sensitive enough\n   that, for sufficiently large sets of real data, the test\
    \ will almost\n   always fail, because it will manage to detect slight imperfections\
    \ in\n   the fit of the data to the distribution.\n   For example, we have found\
    \ that when testing 8,192 measured wire\n   times for packets sent at Poisson\
    \ intervals, the measurements almost\n   always fail the A2 test.  On the other\
    \ hand, testing 128 measurements\n   failed at 5% significance only about 5% of\
    \ the time, as expected.\n   Thus, in general, when the test fails, care must\
    \ be taken to\n   understand why it failed.\n   The remainder of this appendix\
    \ gives C code for the routines\n   mentioned above.\n   /* Routines for computing\
    \ the Anderson-Darling A2 test statistic.\n    *\n    * Implemented based on the\
    \ description in \"Goodness-of-Fit\n    * Techniques,\" R. D'Agostino and M. Stephens,\
    \ editors,\n    * Marcel Dekker, Inc., 1986.\n    */\n   #include <stdio.h>\n\
    \   #include <stdlib.h>\n   #include <math.h>\n   /* Returns the raw A^2 test\
    \ statistic for n sorted samples\n    * z[0] .. z[n-1], for z ~ Unif(0,1).\n \
    \   */\n   extern double compute_A2(double z[], int n);\n   /* Returns the significance\
    \ level associated with a A^2 test\n    * statistic value of A2, assuming no parameters\
    \ of the tested\n    * distribution were estimated from the data.\n    */\n  \
    \ extern double A2_significance(double A2);\n   /* Returns the A^2 significance\
    \ level for testing n observations\n    * x[0] .. x[n-1] against an exponential\
    \ distribution with the\n    * given mean.\n    *\n    * SIDE EFFECT: the x[0..n-1]\
    \ are sorted upon return.\n    */\n   extern double exp_A2_known_mean(double x[],\
    \ int n, double mean);\n   /* Returns the A^2 significance level for testing n\
    \ observations\n    * x[0] .. x[n-1] against the uniform distribution [min_val,\
    \ max_val].\n    *\n    * SIDE EFFECT: the x[0..n-1] are sorted upon return.\n\
    \    */\n   extern double unif_A2_known_range(double x[], int n,\n           \
    \            double min_val, double max_val);\n   /* Returns a pseudo-random number\
    \ distributed according to an\n    * exponential distribution with the given mean.\n\
    \    */\n   extern double random_exponential(double mean);\n   /* Helper function\
    \ used by qsort() to sort double-precision\n    * floating-point values.\n   \
    \ */\n   static int\n   compare_double(const void *v1, const void *v2)\n   {\n\
    \       double d1 = *(double *) v1;\n       double d2 = *(double *) v2;\n    \
    \   if (d1 < d2)\n           return -1;\n       else if (d1 > d2)\n          \
    \ return 1;\n       else\n           return 0;\n   }\n   double\n   compute_A2(double\
    \ z[], int n)\n   {\n       int i;\n       double sum = 0.0;\n       if ( n <\
    \ 5 )\n           /* Too few values. */\n           return -1.0;\n       /* If\
    \ any of the values are outside the range (0, 1) then\n        * fail immediately\
    \ (and avoid a possible floating point\n        * exception in the code below).\n\
    \        */\n       for (i = 0; i < n; ++i)\n           if ( z[i] <= 0.0 || z[i]\
    \ >= 1.0 )\n               return -1.0;\n       /* Page 101 of D'Agostino and\
    \ Stephens. */\n       for (i = 1; i <= n; ++i) {\n           sum += (2 * i -\
    \ 1) * log(z[i-1]);\n           sum += (2 * n + 1 - 2 * i) * log(1.0 - z[i-1]);\n\
    \       }\n       return -n - (1.0 / n) * sum;\n   }\n   double\n   A2_significance(double\
    \ A2)\n   {\n       /* Page 105 of D'Agostino and Stephens. */\n       if (A2\
    \ < 0.0)\n           return A2;    /* Bogus A2 value - propagate it. */\n    \
    \   /* Check for possibly doctored values. */\n       if (A2 <= 0.201)\n     \
    \      return 0.99;\n       else if (A2 <= 0.240)\n           return 0.975;\n\
    \       else if (A2 <= 0.283)\n           return 0.95;\n       else if (A2 <=\
    \ 0.346)\n           return 0.90;\n       else if (A2 <= 0.399)\n           return\
    \ 0.85;\n       /* Now check for possible inconsistency. */\n       if (A2 <=\
    \ 1.248)\n           return 0.25;\n       else if (A2 <= 1.610)\n           return\
    \ 0.15;\n       else if (A2 <= 1.933)\n           return 0.10;\n       else if\
    \ (A2 <= 2.492)\n           return 0.05;\n       else if (A2 <= 3.070)\n     \
    \      return 0.025;\n       else if (A2 <= 3.880)\n           return 0.01;\n\
    \       else if (A2 <= 4.500)\n           return 0.005;\n       else if (A2 <=\
    \ 6.000)\n           return 0.001;\n       else\n           return 0.0;\n   }\n\
    \   double\n   exp_A2_known_mean(double x[], int n, double mean)\n   {\n     \
    \  int i;\n       double A2;\n       /* Sort the first n values. */\n       qsort(x,\
    \ n, sizeof(x[0]), compare_double);\n       /* Assuming they match an exponential\
    \ distribution, transform\n        * them to Unif(0,1).\n        */\n       for\
    \ (i = 0; i < n; ++i) {\n           x[i] = 1.0 - exp(-x[i] / mean);\n       }\n\
    \       /* Now make the A^2 test to see if they're truly uniform. */\n       A2\
    \ = compute_A2(x, n);\n       return A2_significance(A2);\n   }\n   double\n \
    \  unif_A2_known_range(double x[], int n, double min_val, double max_val)\n  \
    \ {\n       int i;\n       double A2;\n       double range = max_val - min_val;\n\
    \       /* Sort the first n values. */\n       qsort(x, n, sizeof(x[0]), compare_double);\n\
    \       /* Transform Unif(min_val, max_val) to Unif(0,1). */\n       for (i =\
    \ 0; i < n; ++i)\n           x[i] = (x[i] - min_val) / range;\n       /* Now make\
    \ the A^2 test to see if they're truly uniform. */\n       A2 = compute_A2(x,\
    \ n);\n       return A2_significance(A2);\n   }\n   double\n   random_exponential(double\
    \ mean)\n   {\n       return -mean * log1p(-drand48());\n   }\n"
- title: 19. References
  contents:
  - "19. References\n   [AK97] G. Almes and S. Kalidindi, \"A One-way Delay Metric\
    \ for IPPM\",\n   Work in Progress, November 1997.\n   [BM92] I. Bilinskis and\
    \ A. Mikelsons, Randomized Signal Processing,\n   Prentice Hall International,\
    \ 1992.\n   [DS86] R. D'Agostino and M. Stephens, editors, Goodness-of-Fit\n \
    \  Techniques, Marcel Dekker, Inc., 1986.\n   [CPB93] K. Claffy, G. Polyzos, and\
    \ H-W. Braun, \"Application of\n   Sampling Methodologies to Network Traffic Characterization,\"\
    \ Proc.\n   SIGCOMM '93, pp. 194-203, San Francisco, September 1993.\n   [FJ94]\
    \ S. Floyd and V. Jacobson, \"The Synchronization of Periodic\n   Routing Messages,\"\
    \ IEEE/ACM Transactions on Networking, 2(2), pp.\n   122-136, April 1994.\n  \
    \ [Mi92] Mills, D., \"Network Time Protocol (Version 3) Specification,\n   Implementation\
    \ and Analysis\", RFC 1305, March 1992.\n   [Pa94] V. Paxson, \"Empirically-Derived\
    \ Analytic Models of Wide-Area\n   TCP Connections,\" IEEE/ACM Transactions on\
    \ Networking, 2(4), pp.\n   316-336, August 1994.\n   [Pa96] V. Paxson, \"Towards\
    \ a Framework for Defining Internet\n   Performance Metrics,\" Proceedings of\
    \ INET '96,\n   ftp://ftp.ee.lbl.gov/papers/metrics-framework-INET96.ps.Z\n  \
    \ [Pa97] V. Paxson, \"Measurements and Analysis of End-to-End Internet\n   Dynamics,\"\
    \ Ph.D. dissertation, U.C. Berkeley, 1997,\n   ftp://ftp.ee.lbl.gov/papers/vp-thesis/dis.ps.gz.\n"
- title: 20. Authors' Addresses
  contents:
  - "20. Authors' Addresses\n   Vern Paxson\n   MS 50B/2239\n   Lawrence Berkeley\
    \ National Laboratory\n   University of California\n   Berkeley, CA  94720\n \
    \  USA\n   Phone: +1 510/486-7504\n   EMail: vern@ee.lbl.gov\n   Guy Almes\n \
    \  Advanced Network & Services, Inc.\n   200 Business Park Drive\n   Armonk, NY\
    \  10504\n   USA\n   Phone: +1 914/765-1120\n   EMail: almes@advanced.org\n  \
    \ Jamshid Mahdavi\n   Pittsburgh Supercomputing Center\n   4400 5th Avenue\n \
    \  Pittsburgh, PA  15213\n   USA\n   Phone: +1 412/268-6282\n   EMail: mahdavi@psc.edu\n\
    \   Matt Mathis\n   Pittsburgh Supercomputing Center\n   4400 5th Avenue\n   Pittsburgh,\
    \ PA  15213\n   USA\n   Phone: +1 412/268-3319\n   EMail: mathis@psc.edu\n"
- title: 21. Full Copyright Statement
  contents:
  - "21. Full Copyright Statement\n   Copyright (C) The Internet Society (1998). \
    \ All Rights Reserved.\n   This document and translations of it may be copied\
    \ and furnished to\n   others, and derivative works that comment on or otherwise\
    \ explain it\n   or assist in its implementation may be prepared, copied, published\n\
    \   and distributed, in whole or in part, without restriction of any\n   kind,\
    \ provided that the above copyright notice and this paragraph are\n   included\
    \ on all such copies and derivative works.  However, this\n   document itself\
    \ may not be modified in any way, such as by removing\n   the copyright notice\
    \ or references to the Internet Society or other\n   Internet organizations, except\
    \ as needed for the purpose of\n   developing Internet standards in which case\
    \ the procedures for\n   copyrights defined in the Internet Standards process\
    \ must be\n   followed, or as required to translate it into languages other than\n\
    \   English.\n   The limited permissions granted above are perpetual and will\
    \ not be\n   revoked by the Internet Society or its successors or assigns.\n \
    \  This document and the information contained herein is provided on an\n   \"\
    AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE\
    \ DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED\
    \ TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE\
    \ ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A\
    \ PARTICULAR PURPOSE.\n"
