- title: __initial_text__
  contents:
  - '    Industrial Routing Requirements in Low-Power and Lossy Networks

    '
- title: Abstract
  contents:
  - "Abstract\n   The wide deployment of lower-cost wireless devices will significantly\n\
    \   improve the productivity and safety of industrial plants while\n   increasing\
    \ the efficiency of plant workers by extending the\n   information set available\
    \ about the plant operations.  The aim of\n   this document is to analyze the\
    \ functional requirements for a routing\n   protocol used in industrial Low-power\
    \ and Lossy Networks (LLNs) of\n   field devices.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2009 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction . . . . . . . . . . . . . . . . . . .\
    \ . . . . . .  3\n     1.1.  Requirements Language  . . . . . . . . . . . . .\
    \ . . . . .  3\n   2.  Terminology  . . . . . . . . . . . . . . . . . . . . .\
    \ . . . .  4\n   3.  Overview . . . . . . . . . . . . . . . . . . . . . . . .\
    \ . . .  4\n     3.1.  Applications and Traffic Patterns  . . . . . . . . . .\
    \ . .  5\n     3.2.  Network Topology of Industrial Applications  . . . . . .\
    \ .  9\n       3.2.1.  The Physical Topology  . . . . . . . . . . . . . . . .\
    \ 10\n       3.2.2.  Logical Topologies . . . . . . . . . . . . . . . . . . 12\n\
    \   4.  Requirements Related to Traffic Characteristics  . . . . . . . 13\n  \
    \   4.1.  Service Requirements . . . . . . . . . . . . . . . . . . . 14\n    \
    \ 4.2.  Configurable Application Requirement . . . . . . . . . . . 15\n     4.3.\
    \  Different Routes for Different Flows . . . . . . . . . . . 15\n   5.  Reliability\
    \ Requirements . . . . . . . . . . . . . . . . . . . 16\n   6.  Device-Aware Routing\
    \ Requirements  . . . . . . . . . . . . . . 18\n   7.  Broadcast/Multicast Requirements\
    \ . . . . . . . . . . . . . . . 19\n   8.  Protocol Performance Requirements \
    \ . . . . . . . . . . . . . . 20\n   9.  Mobility Requirements  . . . . . . .\
    \ . . . . . . . . . . . . . 21\n   10. Manageability Requirements . . . . . .\
    \ . . . . . . . . . . . . 21\n   11. Antagonistic Requirements  . . . . . . .\
    \ . . . . . . . . . . . 22\n   12. Security Considerations  . . . . . . . . .\
    \ . . . . . . . . . . 23\n   13. Acknowledgements . . . . . . . . . . . . . .\
    \ . . . . . . . . . 25\n   14. References . . . . . . . . . . . . . . . . . .\
    \ . . . . . . . . 25\n     14.1. Normative References . . . . . . . . . . . .\
    \ . . . . . . . 25\n     14.2. Informative References . . . . . . . . . . . .\
    \ . . . . . . 25\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Information Technology (IT) is already, and increasingly\
    \ will be\n   applied to industrial Control Technology (CT) in application areas\n\
    \   where those IT technologies can be constrained sufficiently by\n   Service\
    \ Level Agreements (SLA) or other modest changes that they are\n   able to meet\
    \ the operational needs of industrial CT.  When that\n   happens, the CT benefits\
    \ from the large intellectual, experiential,\n   and training investment that\
    \ has already occurred in those IT\n   precursors.  One can conclude that future\
    \ reuse of additional IT\n   protocols for industrial CT will continue to occur\
    \ due to the\n   significant intellectual, experiential, and training economies\
    \ that\n   result from that reuse.\n   Following that logic, many vendors are\
    \ already extending or replacing\n   their local fieldbus [IEC61158] technology\
    \ with Ethernet and IP-based\n   solutions.  Examples of this evolution include\
    \ Common Industrial\n   Protocol (CIP) EtherNet/IP, Modbus/TCP, Fieldbus Foundation\
    \ High\n   Speed Ethernet (HSE), PROFInet, and Invensys/Foxboro FOXnet.  At the\n\
    \   same time, wireless, low-power field devices are being introduced\n   that\
    \ facilitate a significant increase in the amount of information\n   that industrial\
    \ users can collect and the number of control points\n   that can be remotely\
    \ managed.\n   IPv6 appears as a core technology at the conjunction of both trends,\n\
    \   as illustrated by the current [ISA100.11a] industrial Wireless Sensor\n  \
    \ Networking specification, where technologies for layers 1-4 that were\n   developed\
    \ for purposes other than industrial CT -- [IEEE802.15.4] PHY\n   and MAC, IPv6\
    \ over Low-Power Wireless Personal Area Networks\n   (6LoWPANs) [RFC4919], and\
    \ UDP -- are adapted to industrial CT use.\n   But due to the lack of open standards\
    \ for routing in Low-power and\n   Lossy Networks (LLNs), even ISA100.11a leaves\
    \ the routing operation\n   to proprietary methods.\n   The aim of this document\
    \ is to analyze the requirements from the\n   industrial environment for a routing\
    \ protocol in Low power and Lossy\n   Networks (LLNs) based on IPv6 to power the\
    \ next generation of Control\n   Technology.\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in RFC 2119 [RFC2119].\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   This document employs terminology defined in the ROLL (Routing\
    \ Over\n   Low-power and Lossy networks) terminology document [ROLL-TERM].  This\n\
    \   document also refers to industrial standards:\n   HART: Highway Addressable\
    \ Remote Transducer, a group of\n   specifications for industrial process and\
    \ control devices\n   administered by the HART Communication Foundation (see [HART]).\
    \  The\n   latest version for the specifications is HART7, which includes the\n\
    \   additions for WirelessHART [IEC62591].\n   ISA: International Society of Automation,\
    \ an ANSI-accredited\n   standards-making society.  ISA100 is an ISA committee\
    \ whose charter\n   includes defining a family of standards for industrial automation.\n\
    \   [ISA100.11a] is a working group within ISA100 that is working on a\n   standard\
    \ for monitoring and non-critical process control\n   applications.\n"
- title: 3.  Overview
  contents:
  - "3.  Overview\n   Wireless, low-power field devices enable industrial users to\n\
    \   significantly increase the amount of information collected and the\n   number\
    \ of control points that can be remotely managed.  The\n   deployment of these\
    \ wireless devices will significantly improve the\n   productivity and safety\
    \ of the plants while increasing the efficiency\n   of the plant workers.  IPv6\
    \ is perceived as a key technology to\n   provide the scalability and interoperability\
    \ that are required in\n   that space, and it is more and more present in standards\
    \ and products\n   under development and early deployments.\n   Cable is perceived\
    \ as a more proven, safer technology, and existing,\n   operational deployments\
    \ are very stable in time.  For these reasons,\n   it is not expected that wireless\
    \ will replace wire in any foreseeable\n   future; the consensus in the industrial\
    \ space is rather that wireless\n   will tremendously augment the scope and benefits\
    \ of automation by\n   enabling the control of devices that were not connected\
    \ in the past\n   for reasons of cost and/or deployment complexities.  But for\
    \ LLNs to\n   be adopted in the industrial environment, the wireless network needs\n\
    \   to have three qualities: low power, high reliability, and easy\n   installation\
    \ and maintenance.  The routing protocol used for LLNs is\n   important to fulfilling\
    \ these goals.\n   Industrial automation is segmented into two distinct application\n\
    \   spaces, known as \"process\" or \"process control\" and \"discrete\n   manufacturing\"\
    \ or \"factory automation\".  In industrial process\n   control, the product is\
    \ typically a fluid (oil, gas, chemicals,\n   etc.).  In factory automation or\
    \ discrete manufacturing, the products\n   are individual elements (screws, cars,\
    \ dolls).  While there is some\n   overlap of products and systems between these\
    \ two segments, they are\n   surprisingly separate communities.  The specifications\
    \ targeting\n   industrial process control tend to have more tolerance for network\n\
    \   latency than what is needed for factory automation.\n   Irrespective of this\
    \ different 'process' and 'discrete' plant nature,\n   both plant types will have\
    \ similar needs for automating the\n   collection of data that used to be collected\
    \ manually, or was not\n   collected before.  Examples are wireless sensors that\
    \ report the\n   state of a fuse, report the state of a luminary, HVAC status,\
    \ report\n   vibration levels on pumps, report man-down, and so on.\n   Other\
    \ novel application arenas that equally apply to both 'process'\n   and 'discrete'\
    \ involve mobile sensors that roam in and out of plants,\n   such as active sensor\
    \ tags on containers or vehicles.\n   Some if not all of these applications will\
    \ need to be served by the\n   same low-power and lossy wireless network technology.\
    \  This may mean\n   several disconnected, autonomous LLNs connecting to multiple\
    \ hosts,\n   but sharing the same ether.  Interconnecting such networks, if only\n\
    \   to supervise channel and priority allocations, or to fully\n   synchronize,\
    \ or to share path capacity within a set of physical\n   network components may\
    \ be desired, or may not be desired for\n   practical reasons, such as e.g., cyber\
    \ security concerns in relation\n   to plant safety and integrity.\n   All application\
    \ spaces desire battery-operated networks of hundreds\n   of sensors and actuators\
    \ communicating with LLN access points.  In an\n   oil refinery, the total number\
    \ of devices might exceed one million,\n   but the devices will be clustered into\
    \ smaller networks that in most\n   cases interconnect and report to an existing\
    \ plant network\n   infrastructure.\n   Existing wired sensor networks in this\
    \ space typically use\n   communication protocols with low data rates, from 1200\
    \ baud (e.g.,\n   wired HART) to the 100-200 kbps range for most of the others.\
    \  The\n   existing protocols are often master/slave with command/response.\n"
- title: 3.1.  Applications and Traffic Patterns
  contents:
  - "3.1.  Applications and Traffic Patterns\n   The industrial market classifies\
    \ process applications into three\n   broad categories and six classes.\n   o\
    \  Safety\n      *  Class 0: Emergency action - Always a critical function\n \
    \  o  Control\n      *  Class 1: Closed-loop regulatory control - Often a critical\n\
    \         function\n      *  Class 2: Closed-loop supervisory control - Usually\
    \ a non-\n         critical function\n      *  Class 3: Open-loop control - Operator\
    \ takes action and controls\n         the actuator (human in the loop)\n   o \
    \ Monitoring\n      *  Class 4: Alerting - Short-term operational effect (for\
    \ example,\n         event-based maintenance)\n      *  Class 5: Logging and downloading\
    \ / uploading - No immediate\n         operational consequence (e.g., history\
    \ collection, sequence-of-\n         events, preventive maintenance)\n   Safety-critical\
    \ functions effect the basic safety integrity of the\n   plant.  These normally\
    \ dormant functions kick in only when process\n   control systems, or their operators,\
    \ have failed.  By design and by\n   regular interval inspection, they have a\
    \ well-understood probability\n   of failure on demand in the range of typically\
    \ once per 10-1000\n   years.\n   In-time deliveries of messages become more relevant\
    \ as the class\n   number decreases.\n   Note that for a control application,\
    \ the jitter is just as important\n   as latency and has a potential of destabilizing\
    \ control algorithms.\n   Industrial users are interested in deploying wireless\
    \ networks for\n   the monitoring classes 4 and 5, and in the non-critical portions\
    \ of\n   classes 2 through 3.\n   Classes 4 and 5 also include asset monitoring\
    \ and tracking, which\n   include equipment monitoring and are essentially separate\
    \ from\n   process monitoring.  An example of equipment monitoring is the\n  \
    \ recording of motor vibrations to detect bearing wear.  However,\n   similar\
    \ sensors detecting excessive vibration levels could be used as\n   safeguarding\
    \ loops that immediately initiate a trip, and thus end up\n   being class 0.\n\
    \   In the near future, most LLN systems in industrial automation\n   environments\
    \ will be for low-frequency data collection.  Packets\n   containing samples will\
    \ be generated continuously, and 90% of the\n   market is covered by packet rates\
    \ of between 1/second and 1/hour,\n   with the average under 1/minute.  In industrial\
    \ process, these\n   sensors include temperature, pressure, fluid flow, tank level,\
    \ and\n   corrosion.  Some sensors are bursty, such as vibration monitors that\n\
    \   may generate and transmit tens of kilobytes (hundreds to thousands of\n  \
    \ packets) of time-series data at reporting rates of minutes to days.\n   Almost\
    \ all of these sensors will have built-in microprocessors that\n   may detect\
    \ alarm conditions.  Time-critical alarm packets are\n   expected to be granted\
    \ a lower latency than periodic sensor data\n   streams.\n   Some devices will\
    \ transmit a log file every day, again with typically\n   tens of kilobytes of\
    \ data.  For these applications, there is very\n   little \"downstream\" traffic\
    \ coming from the LLN access point and\n   traveling to particular sensors.  During\
    \ diagnostics, however, a\n   technician may be investigating a fault from a control\
    \ room and\n   expect to have \"low\" latency (human tolerable) in a command/response\n\
    \   mode.\n   Low-rate control, often with a \"human in the loop\" (also referred\
    \ to\n   as \"open loop\"), is implemented via communication to a control room\n\
    \   because that's where the human in the loop will be.  The sensor data\n   makes\
    \ its way through the LLN access point to the centralized\n   controller where\
    \ it is processed, the operator sees the information\n   and takes action, and\
    \ the control information is then sent out to the\n   actuator node in the network.\n\
    \   In the future, it is envisioned that some open-loop processes will be\n  \
    \ automated (closed loop) and packets will flow over local loops and\n   not involve\
    \ the LLN access point.  These closed-loop controls for\n   non-critical applications\
    \ will be implemented on LLNs.  Non-critical\n   closed-loop applications have\
    \ a latency requirement that can be as\n   low as 100 milliseconds but many control\
    \ loops are tolerant of\n   latencies above 1 second.\n   More likely though is\
    \ that loops will be closed in the field\n   entirely, and in such a case, having\
    \ wireless links within the\n   control loop does not usually present actual value.\
    \  Most control\n   loops have sensors and actuators within such proximity that\
    \ a wire\n   between them remains the most sensible option from an economic point\n\
    \   of view.  This 'control in the field' architecture is already common\n   practice\
    \ with wired fieldbusses.  An 'upstream' wireless link would\n   only be used\
    \ to influence the in-field controller settings and to\n   occasionally capture\
    \ diagnostics.  Even though the link back to a\n   control room might be wireless,\
    \ this architecture reduces the tight\n   latency and availability requirements\
    \ for the wireless links.\n   Closing loops in the field:\n   o  does not prevent\
    \ the same loop from being closed through a remote\n      multivariable controller\
    \ during some modes of operation, while\n      being closed directly in the field\
    \ during other modes of operation\n      (e.g., fallback, or when timing is more\
    \ critical)\n   o  does not imply that the loop will be closed with a wired\n\
    \      connection, or that the wired connection is more energy efficient\n   \
    \   even when it exists as an alternate to the wireless connection.\n   A realistic\
    \ future scenario is for a field device with a battery or\n   ultra-capacitor\
    \ power storage to have both wireless and unpowered\n   wired communications capability\
    \ (e.g., galvanically isolated RS-485),\n   where the wireless communication is\
    \ more flexible and, for local loop\n   operation, more energy efficient.  The\
    \ wired communication capability\n   serves as a backup interconnect among the\
    \ loop elements, but without\n   a wired connection back to the operations center\
    \ blockhouse.  In\n   other words, the loop elements are interconnected through\
    \ wiring to a\n   nearby junction box, but the 2 km home-run link from the junction\
    \ box\n   to the control center does not exist.\n   When wireless communication\
    \ conditions are good, devices use wireless\n   for loop interconnect, and either\
    \ one wireless device reports alarms\n   and other status to the control center\
    \ for all elements of the loop,\n   or each element reports independently.  When\
    \ wireless communications\n   are sporadic, the loop interconnect uses the self-powered\n\
    \   galvanically isolated RS-485 link and one of the devices with good\n   wireless\
    \ communications to the control center serves as a router for\n   those devices\
    \ that are unable to contact the control center directly.\n   The above approach\
    \ is particularly attractive for large storage tanks\n   in tank farms, where\
    \ devices may not all have good wireless\n   visibility of the control center,\
    \ and where a home-run cable from the\n   tank to the control center is undesirable\
    \ due to the electro-\n   potential differences between the tank location and\
    \ the distant\n   control center that arise during lightning storms.\n   In fast\
    \ control, tens of milliseconds of latency is typical.  In many\n   of these systems,\
    \ if a packet does not arrive within the specified\n   interval, the system enters\
    \ an emergency shutdown state, often with\n   substantial financial repercussions.\
    \  For a one-second control loop\n   in a system with a target of 30 years for\
    \ the mean time between\n   shutdowns, the latency requirement implies nine 9s\
    \ of reliability\n   (aka 99.9999999% reliability).  Given such exposure, given\
    \ the\n   intrinsic vulnerability of wireless link availability, and given the\n\
    \   emergence of control in the field architectures, most users tend not\n   to\
    \ aim for fast closed-loop control with wireless links within that\n   fast loop.\n"
- title: 3.2.  Network Topology of Industrial Applications
  contents:
  - "3.2.  Network Topology of Industrial Applications\n   Although network topology\
    \ is difficult to generalize, the majority of\n   existing applications can be\
    \ met by networks of 10 to 200 field\n   devices and a maximum number of hops\
    \ of 20.  It is assumed that the\n   field devices themselves will provide routing\
    \ capability for the\n   network, and additional repeaters/routers will not be\
    \ required in\n   most cases.\n   For the vast majority of industrial applications,\
    \ the traffic is\n   mostly composed of real-time publish/subscribe sensor data\
    \ also\n   referred to as buffered, from the field devices over an LLN towards\n\
    \   one or more sinks.  Increasingly over time, these sinks will be a\n   part\
    \ of a backbone, but today they are often fragmented and isolated.\n   The wireless\
    \ sensor network (WSN) is an LLN of field devices for\n   which two logical roles\
    \ are defined, the field routers and the non-\n   routing devices.  It is acceptable\
    \ and even probable that the\n   repartition of the roles across the field devices\
    \ changes over time\n   to balance the cost of the forwarding operation amongst\
    \ the nodes.\n   In order to scale a control network in terms of density, one\
    \ possible\n   architecture is to deploy a backbone as a canopy that aggregates\n\
    \   multiple smaller LLNs.  The backbone is a high-speed infrastructure\n   network\
    \ that may interconnect multiple WSNs through backbone routers.\n   Infrastructure\
    \ devices can be connected to the backbone.  A gateway/\n   manager that interconnects\
    \ the backbone to the plant network of the\n   corporate network can be viewed\
    \ as collapsing the backbone and the\n   infrastructure devices into a single\
    \ device that operates all the\n   required logical roles.  The backbone is likely\
    \ to become an option\n   in the industrial network.\n   Typically, such backbones\
    \ interconnect to the 'legacy' wired plant\n   infrastructure, which is known\
    \ as the plant network or Process\n   Control Domain (PCD).  These plant automation\
    \ networks are segregated\n   domain-wise from the office network or office domain\
    \ (OD), which in\n   itself is typically segregated from the Internet.\n   Sinks\
    \ for LLN sensor data reside on the plant network (the PCD), the\n   business\
    \ network (the OD), and on the Internet.  Applications close\n   to existing plant\
    \ automation, such as wired process control and\n   monitoring systems running\
    \ on fieldbusses, that require high\n   availability and low latencies, and that\
    \ are managed by 'Control and\n   Automation' departments typically reside on\
    \ the PCD.  Other\n   applications such as automated corrosion monitoring, cathodic\n\
    \   protection voltage verification, or machine condition (vibration)\n   monitoring\
    \ where one sample per week is considered over-sampling,\n   would more likely\
    \ deliver their sensor readings in the OD.  Such\n   applications are 'owned'\
    \ by, e.g., maintenance departments.\n   Yet other applications like third-party-maintained\
    \ luminaries, or\n   vendor-managed inventory systems, where a supplier of chemicals\
    \ needs\n   access to tank level readings at his customer's site, will be best\n\
    \   served with direct Internet connectivity all the way to its sensor at\n  \
    \ his customer's site.  Temporary 'babysitting sensors' deployed for\n   just\
    \ a few days, say during startup or troubleshooting or for ad hoc\n   measurement\
    \ campaigns for research and development purposes, are\n   other examples where\
    \ Internet would be the domain where wireless\n   sensor data would land, and\
    \ other domains such as the OD and PCD\n   should preferably be circumvented if\
    \ quick deployment without\n   potentially impacting plant safety integrity is\
    \ required.\n   This multiple-domain multiple-application connectivity creates\
    \ a\n   significant challenge.  Many different applications will all share\n \
    \  the same medium, the ether, within the fence, preferably sharing the\n   same\
    \ frequency bands, and preferably sharing the same protocols,\n   preferably synchronized\
    \ to optimize coexistence challenges, yet\n   logically segregated to avoid creation\
    \ of intolerable shortcuts\n   between existing wired domains.\n   Given this\
    \ challenge, LLNs are best to be treated as all sitting on\n   yet another segregated\
    \ domain, segregated from all other wired\n   domains where conventional security\
    \ is organized by perimeter.\n   Moving away from the traditional perimeter-security\
    \ mindset means\n   moving towards stronger end-device identity authentication,\
    \ so that\n   LLN access points can split the various wireless data streams and\n\
    \   interconnect back to the appropriate domain (pending the gateways'\n   establishment\
    \ of the message originators' identity and trust).\n   Similar considerations\
    \ are to be given to how multiple applications\n   may or may not be allowed to\
    \ share routing devices and their\n   potentially redundant bandwidth within the\
    \ network.  Challenges here\n   are to balance available capacity, required latencies,\
    \ expected\n   priorities, and (last but not least) available (battery) energy\n\
    \   within the routing devices.\n"
- title: 3.2.1.  The Physical Topology
  contents:
  - "3.2.1.  The Physical Topology\n   There is no specific physical topology for\
    \ an industrial process\n   control network.\n   One extreme example is a multi-square-kilometer\
    \ refinery where\n   isolated tanks, some of them with power but most with no\
    \ backbone\n   connectivity, compose a farm that spans over of the surface of\
    \ the\n   plant.  A few hundred field devices are deployed to ensure the global\n\
    \   coverage using a wireless self-forming self-healing mesh network that\n  \
    \ might be 5 to 10 hops across.  Local feedback loops and mobile\n   workers tend\
    \ to be only 1 or 2 hops.  The backbone is in the refinery\n   proper, many hops\
    \ away.  Even there, powered infrastructure is also\n   typically several hops\
    \ away.  In that case, hopping to/from the\n   powered infrastructure may often\
    \ be more costly than the direct\n   route.\n   In the opposite extreme case,\
    \ the backbone network spans all the\n   nodes and most nodes are in direct sight\
    \ of one or more backbone\n   routers.  Most communication between field devices\
    \ and infrastructure\n   devices, as well as field device to field device, occurs\
    \ across the\n   backbone.  From afar, this model resembles the WiFi ESS (Extended\n\
    \   Service Set).  But from a layer-3 (L3) perspective, the issues are\n   the\
    \ default (backbone) router selection and the routing inside the\n   backbone,\
    \ whereas the radio hop towards the field device is in fact a\n   simple local\
    \ delivery.\n            ---------+----------------------------\n            \
    \         |          Plant Network\n                     |\n                 \
    \ +-----+\n                  |     | Gateway             M : Mobile device\n \
    \                 |     |                     o : Field device\n             \
    \     +-----+\n                     |\n                     |      Backbone\n\
    \               +--------------------+------------------+\n               |  \
    \                  |                  |\n            +-----+             +-----+\
    \             +-----+\n            |     | Backbone    |     | Backbone    | \
    \    | Backbone\n            |     | router      |     | router      |     | router\n\
    \            +-----+             +-----+             +-----+\n               o\
    \    o   o    o     o   o  o   o   o   o  o   o o\n           o o   o  o   o \
    \ o  o o   o  o  o   o   o   o  o  o  o o\n          o  o o  o o    o   o   o\
    \  o  o  o    M    o  o  o o o\n          o   o  M o  o  o     o  o    o  o  o\
    \    o  o   o  o   o\n            o   o o       o        o  o         o      \
    \  o o\n                    o           o          o             o     o\n   \
    \                        LLN\n                Figure 1: Backbone-Based Physical\
    \ Topology\n   An intermediate case is illustrated in Figure 1 with a backbone\
    \ that\n   spans the Wireless Sensor Network in such a fashion that any WSN node\n\
    \   is only a few wireless hops away from the nearest backbone router.\n   WSN\
    \ nodes are expected to organize into self-forming, self-healing,\n   self-optimizing\
    \ logical topologies that enable leveraging the\n   backbone when it is most efficient\
    \ to do so.\n   It must be noted that the routing function is expected to be so\n\
    \   simple that any field device could assume the role of a router,\n   depending\
    \ on the self-discovery of the topology and the power status\n   of the neighbors.\
    \  On the other hand, only devices equipped with the\n   appropriate hardware\
    \ and software combination could assume the role\n   of an endpoint for a given\
    \ purpose, such as sensor or actuator.\n"
- title: 3.2.2.  Logical Topologies
  contents:
  - "3.2.2.  Logical Topologies\n   Most of the traffic over the LLN is publish/subscribe\
    \ of sensor data\n   from the field device towards a sink that can be a backbone\
    \ router, a\n   gateway, or a controller/manager.  The destination of the sensor\
    \ data\n   is an infrastructure device that sits on the backbone and is\n   reachable\
    \ via one or more backbone routers.\n   For security, reliability, availability,\
    \ or serviceability reasons,\n   it is often required that the logical topologies\
    \ are not physically\n   congruent over the radio network; that is, they form\
    \ logical\n   partitions of the LLN.  For instance, a routing topology that is\
    \ set\n   up for control should be isolated from a topology that reports the\n\
    \   temperature and the status of the vents, if that second topology has\n   lesser\
    \ constraints for the security policy.  This isolation might be\n   implemented\
    \ as Virtual LANs and Virtual Routing Tables in shared\n   nodes in the backbone,\
    \ but correspond effectively to physical nodes\n   in the wireless network.\n\
    \   Since publishing the data is the raison d'etre for most of the\n   sensors,\
    \ in some cases it makes sense to build proactively a set of\n   routes between\
    \ the sensors and one or more backbone routers and\n   maintain those routes at\
    \ all time.  Also, because of the lossy nature\n   of the network, the routing\
    \ in place should attempt to propose\n   multiple paths in the form of Directed\
    \ Acyclic Graphs oriented\n   towards the destination.\n   In contrast with the\
    \ general requirement of maintaining default\n   routes towards the sinks, the\
    \ need for field device to field device\n   (FD-to-FD) connectivity is very specific\
    \ and rare, though the traffic\n   associated might be of foremost importance.\
    \  FD-to-FD routes are\n   often the most critical, optimized, and well-maintained\
    \ routes.  A\n   class 0 safeguarding loop requires guaranteed delivery and extremely\n\
    \   tight response times.  Both the respect of criteria in the route\n   computation\
    \ and the quality of the maintenance of the route are\n   critical for the field\
    \ devices' operation.  Typically, a control loop\n   will be using a dedicated\
    \ direct wire that has very different\n   capabilities, cost, and constraints\
    \ than the wireless medium, with\n   the need to use a wireless path as a backup\
    \ route only in case of\n   loss of the wired path.\n   Considering that each\
    \ FD-to-FD route computation has specific\n   constraints in terms of latency\
    \ and availability, it can be expected\n   that the shortest path possible will\
    \ often be selected and that this\n   path will be routed inside the LLN as opposed\
    \ to via the backbone.\n   It can also be noted that the lifetimes of the routes\
    \ might range\n   from minutes for a mobile worker to tens of years for a command\
    \ and\n   control closed loop.  Finally, time-varying user requirements for\n\
    \   latency and bandwidth will change the constraints on the routes,\n   which\
    \ might either trigger a constrained route recomputation, a\n   reprovisioning\
    \ of the underlying L2 protocols, or both in that order.\n   For instance, a wireless\
    \ worker may initiate a bulk transfer to\n   configure or diagnose a field device.\
    \  A level sensor device may need\n   to perform a calibration and send a bulk\
    \ file to a plant.\n"
- title: 4.  Requirements Related to Traffic Characteristics
  contents:
  - "4.  Requirements Related to Traffic Characteristics\n   [ISA100.11a] selected\
    \ IPv6 as its network layer for a number of\n   reasons, including the huge address\
    \ space and the large potential\n   size of a subnet, which can range up to 10K\
    \ nodes in a plant\n   deployment.  In the ISA100 model, industrial applications\
    \ fall into\n   four large service categories:\n   1.  Periodic data (aka buffered).\
    \  Data that is generated\n       periodically and has a well understood data\
    \ bandwidth\n       requirement, both deterministic and predictable.  Timely delivery\n\
    \       of such data is often the core function of a wireless sensor\n       network\
    \ and permanent resources are assigned to ensure that the\n       required bandwidth\
    \ stays available.  Buffered data usually\n       exhibits a short time to live,\
    \ and the newer reading obsoletes\n       the previous.  In some cases, alarms\
    \ are low-priority information\n       that gets repeated over and over.  The\
    \ end-to-end latency of this\n       data is not as important as the regularity\
    \ with which the data is\n       presented to the plant application.\n   2.  Event\
    \ data.  This category includes alarms and aperiodic data\n       reports with\
    \ bursty data bandwidth requirements.  In certain\n       cases, alarms are critical\
    \ and require a priority service from\n       the network.\n   3.  Client/Server.\
    \  Many industrial applications are based on a\n       client/server model and\
    \ implement a command response protocol.\n       The data bandwidth required is\
    \ often bursty.  The acceptable\n       round-trip latency for some legacy systems\
    \ was based on the time\n       to send tens of bytes over a 1200 baud link. \
    \ Hundreds of\n       milliseconds is typical.  This type of request is statistically\n\
    \       multiplexed over the LLN and cost-based, fair-share, best-effort\n   \
    \    service is usually expected.\n   4.  Bulk transfer.  Bulk transfers involve\
    \ the transmission of blocks\n       of data in multiple packets where temporary\
    \ resources are\n       assigned to meet a transaction time constraint.  Transient\n\
    \       resources are assigned for a limited time (related to file size\n    \
    \   and data rate) to meet the bulk transfers service requirements.\n"
- title: 4.1.  Service Requirements
  contents:
  - "4.1.  Service Requirements\n   The following service parameters can affect routing\
    \ decisions in a\n   resource-constrained network:\n   o  Data bandwidth - the\
    \ bandwidth might be allocated permanently or\n      for a period of time to a\
    \ specific flow that usually exhibits\n      well-defined properties of burstiness\
    \ and throughput.  Some\n      bandwidth will also be statistically shared between\
    \ flows in a\n      best-effort fashion.\n   o  Latency - the time taken for the\
    \ data to transit the network from\n      the source to the destination.  This\
    \ may be expressed in terms of\n      a deadline for delivery.  Most monitoring\
    \ latencies will be in\n      seconds to minutes.\n   o  Transmission phase -\
    \ process applications can be synchronized to\n      wall clock time and require\
    \ coordinated transmissions.  A common\n      coordination frequency is 4 Hz (250\
    \ ms).\n   o  Service contract type - revocation priority.  LLNs have limited\n\
    \      network resources that can vary with time.  This means the system\n   \
    \   can become fully subscribed or even over-subscribed.  System\n      policies\
    \ determine how resources are allocated when resources are\n      over-subscribed.\
    \  The choices are blocking and graceful\n      degradation.\n   o  Transmission\
    \ priority - the means by which limited resources\n      within field devices\
    \ are allocated across multiple services.  For\n      transmissions, a device\
    \ has to select which packet in its queue\n      will be sent at the next transmission\
    \ opportunity.  Packet\n      priority is used as one criterion for selecting\
    \ the next packet.\n      For reception, a device has to decide how to store a\
    \ received\n      packet.  The field devices are memory-constrained and receive\n\
    \      buffers may become full.  Packet priority is used to select which\n   \
    \   packets are stored or discarded.\n   The routing protocol MUST also support\
    \ different metric types for\n   each link used to compute the path according\
    \ to some objective\n   function (e.g., minimize latency) depending on the nature\
    \ of the\n   traffic.\n   For these reasons, the ROLL routing infrastructure is\
    \ REQUIRED to\n   compute and update constrained routes on demand, and it can\
    \ be\n   expected that this model will become more prevalent for FD-to-FD\n  \
    \ connectivity as well as for some FD-to-infrastructure-device\n   connectivity\
    \ over time.\n   Industrial application data flows between field devices are not\n\
    \   necessarily symmetric.  In particular, asymmetrical cost and\n   unidirectional\
    \ routes are common for published data and alerts, which\n   represent the most\
    \ part of the sensor traffic.  The routing protocol\n   MUST be able to compute\
    \ a set of unidirectional routes with\n   potentially different costs that are\
    \ composed of one or more non-\n   congruent paths.\n   As multiple paths are\
    \ set up and a variety of flows traverse the\n   network towards a same destination\
    \ (for instance, a node acting as a\n   sink for the LLN), the use of an additional\
    \ marking/tagging mechanism\n   based on upper-layer information will be REQUIRED\
    \ for intermediate\n   routers to discriminate the flows and perform the appropriate\
    \ routing\n   decision using only the content of the IPv6 packet (e.g., use of\n\
    \   DSCP, Flow Label).\n"
- title: 4.2.  Configurable Application Requirement
  contents:
  - "4.2.  Configurable Application Requirement\n   Time-varying user requirements\
    \ for latency and bandwidth may require\n   changes in the provisioning of the\
    \ underlying L2 protocols.  A\n   technician may initiate a query/response session\
    \ or bulk transfer to\n   diagnose or configure a field device.  A level sensor\
    \ device may need\n   to perform a calibration and send a bulk file to a plant.\
    \  The\n   routing protocol MUST support the ability to recompute paths based\
    \ on\n   network-layer abstractions of the underlying link attributes/metrics\n\
    \   that may change dynamically.\n"
- title: 4.3.  Different Routes for Different Flows
  contents:
  - "4.3.  Different Routes for Different Flows\n   Because different services categories\
    \ have different service\n   requirements, it is often desirable to have different\
    \ routes for\n   different data flows between the same two endpoints.  For example,\n\
    \   alarm or periodic data from A to Z may require path diversity with\n   specific\
    \ latency and reliability.  A file transfer between A and Z\n   may not need path\
    \ diversity.  The routing algorithm MUST be able to\n   generate different routes\
    \ with different characteristics (e.g.,\n   optimized according to different costs,\
    \ etc.).\n   Dynamic or configured states of links and nodes influence the\n \
    \  capability of a given path to fulfill operational requirements such\n   as\
    \ stability, battery cost, or latency.  Constraints such as battery\n   lifetime\
    \ derive from the application itself, and because industrial\n   applications\
    \ data flows are typically well-defined and well-\n   controlled, it is usually\
    \ possible to estimate the battery\n   consumption of a router for a given topology.\n\
    \   The routing protocol MUST support the ability to (re)compute paths\n   based\
    \ on network-layer abstractions of upper-layer constraints to\n   maintain the\
    \ level of operation within required parameters.  Such\n   information MAY be\
    \ advertised by the routing protocol as metrics that\n   enable routing algorithms\
    \ to establish appropriate paths that fit the\n   upper-layer constraints.\n \
    \  The handling of an IPv6 packet by the network layer operates on the\n   standard\
    \ properties and the settings of the IPv6 packet header\n   fields.  These fields\
    \ include the 3-tuple of the Flow Label and the\n   Source and Destination Address\
    \ that can be used to identify a flow\n   and the Traffic Class octet that can\
    \ be used to influence the Per Hop\n   Behavior in intermediate routers.\n   An\
    \ application MAY choose how to set those fields for each packet or\n   for streams\
    \ of packets, and the routing protocol specification SHOULD\n   state how different\
    \ field settings will be handled to perform\n   different routing decisions.\n"
- title: 5.  Reliability Requirements
  contents:
  - "5.  Reliability Requirements\n   LLN reliability constitutes several unrelated\
    \ aspects:\n   1)  Availability of source-to-destination connectivity when the\n\
    \       application needs it, expressed in number of successes divided by\n  \
    \     number of attempts.\n   2)  Availability of source-to-destination connectivity\
    \ when the\n       application might need it, expressed in number of potential\n\
    \       failures / available bandwidth,\n   3)  Ability, expressed in number of\
    \ successes divided by number of\n       attempts to get data delivered from source\
    \ to destination within\n       a capped time,\n   4)  How well a network (serving\
    \ many applications) achieves end-to-\n       end delivery of packets within a\
    \ bounded latency,\n   5)  Trustworthiness of data that is delivered to the sinks,\n\
    \   6)  and others depending on the specific case.\n   This makes quantifying\
    \ reliability the equivalent of plotting it on a\n   three (or more) dimensional\
    \ graph.  Different applications have\n   different requirements, and expressing\
    \ reliability as a one\n   dimensional parameter, like 'reliability on my wireless\
    \ network is\n   99.9%' often creates more confusion than clarity.\n   The impact\
    \ of not receiving sensor data due to sporadic network\n   outages can be devastating\
    \ if this happens unnoticed.  However, if\n   destinations that expect periodic\
    \ sensor data or alarm status updates\n   fail to get them, then automatically\
    \ these systems can take\n   appropriate actions that prevent dangerous situations.\
    \  Pending the\n   wireless application, appropriate action ranges from initiating\
    \ a\n   shutdown within 100 ms, to using a last known good value for as much\n\
    \   as N successive samples, to sending out an operator into the plant to\n  \
    \ collect monthly data in the conventional way, i.e., some portable\n   sensor,\
    \ or paper and a clipboard.\n   The impact of receiving corrupted data, and not\
    \ being able to detect\n   that received data is corrupt, is often more dangerous.\
    \  Data\n   corruption can either come from random bit errors due to white noise,\n\
    \   or from occasional bursty interference sources like thunderstorms or\n   leaky\
    \ microwave ovens, but also from conscious attacks by\n   adversaries.\n   Another\
    \ critical aspect for the routing is the capability to ensure\n   maximum disruption\
    \ time and route maintenance.  The maximum\n   disruption time is the time it\
    \ takes at most for a specific path to\n   be restored when broken.  Route maintenance\
    \ ensures that a path is\n   monitored cannot stay disrupted for more than the\
    \ maximum disruption\n   time.  Maintenance should also ensure that a path continues\
    \ to\n   provide the service for which it was established, for instance, in\n\
    \   terms of bandwidth, jitter, and latency.\n   In industrial applications, availability\
    \ is usually defined with\n   respect to end-to-end delivery of packets within\
    \ a bounded latency.\n   Availability requirements vary over many orders of magnitude.\
    \  Some\n   non-critical monitoring applications may tolerate an availability\
    \ of\n   less than 90% with hours of latency.  Most industrial standards, such\n\
    \   as HART7 [IEC62591], have set user availability expectations at\n   99.9%.\
    \  Regulatory requirements are a driver for some industrial\n   applications.\
    \  Regulatory monitoring requires high data integrity\n   because lost data is\
    \ assumed to be out of compliance and subject to\n   fines.  This can drive up\
    \ either availability or trustworthiness\n   requirements.\n   Because LLN link\
    \ stability is often low, path diversity is critical.\n   Hop-by-hop link diversity\
    \ is used to improve latency-bounded\n   reliability by sending data over diverse\
    \ paths.\n   Because data from field devices are aggregated and funneled at the\n\
    \   LLN access point before they are routed to plant applications, LLN\n   access\
    \ point redundancy is an important factor in overall\n   availability.  A route\
    \ that connects a field device to a plant\n   application may have multiple paths\
    \ that go through more than one LLN\n   access point.  The routing protocol MUST\
    \ be able to compute paths of\n   not-necessarily-equal cost toward a given destination\
    \ so as to enable\n   load-balancing across a variety of paths.  The availability\
    \ of each\n   path in a multipath route can change over time.  Hence, it is\n\
    \   important to measure the availability on a per-path basis and select\n   a\
    \ path (or paths) according to the availability requirements.\n"
- title: 6.  Device-Aware Routing Requirements
  contents:
  - "6.  Device-Aware Routing Requirements\n   Wireless LLN nodes in industrial environments\
    \ are powered by a\n   variety of sources.  Battery-operated devices with lifetime\n\
    \   requirements of at least five years are the most common.  Battery\n   operated\
    \ devices have a cap on their total energy, and typically can\n   report an estimate\
    \ of remaining energy, and typically do not have\n   constraints on the short-term\
    \ average power consumption.  Energy-\n   scavenging devices are more complex.\
    \  These systems contain both a\n   power-scavenging device (such as solar, vibration,\
    \ or temperature\n   difference) and an energy storage device, such as a rechargeable\n\
    \   battery or a capacitor.  These systems, therefore, have limits on\n   both\
    \ long-term average power consumption (which cannot exceed the\n   average scavenged\
    \ power over the same interval) as well as the short-\n   term limits imposed\
    \ by the energy storage requirements.  For solar-\n   powered systems, the energy\
    \ storage system is generally designed to\n   provide days of power in the absence\
    \ of sunlight.  Many industrial\n   sensors run off of a 4-20 mA current loop,\
    \ and can scavenge on the\n   order of milliwatts from that source.  Vibration\
    \ monitoring systems\n   are a natural choice for vibration scavenging, which\
    \ typically only\n   provides tens or hundreds of microwatts.  Due to industrial\n\
    \   temperature ranges and desired lifetimes, the choices of energy\n   storage\
    \ devices can be limited, and the resulting stored energy is\n   often comparable\
    \ to the energy cost of sending or receiving a packet\n   rather than the energy\
    \ of operating the node for several days.  And\n   of course, some nodes will\
    \ be line-powered.\n   Example 1: solar panel, lead-acid battery sized for two\
    \ weeks of\n   rain.\n   Example 2: vibration scavenger, 1 mF tantalum capacitor.\n\
    \   Field devices have limited resources.  Low-power, low-cost devices\n   have\
    \ limited memory for storing route information.  Typical field\n   devices will\
    \ have a finite number of routes they can support for\n   their embedded sensor/actuator\
    \ application and for forwarding other\n   devices packets in a mesh network slotted-link.\n\
    \   Users may strongly prefer that the same device have different\n   lifetime\
    \ requirements in different locations.  A sensor monitoring a\n   non-critical\
    \ parameter in an easily accessed location may have a\n   lifetime requirement\
    \ that is shorter and may tolerate more\n   statistical variation than a mission-critical\
    \ sensor in a hard-to-\n   reach place that requires a plant shutdown in order\
    \ to replace.\n   The routing algorithm MUST support node-constrained routing\
    \ (e.g.,\n   taking into account the existing energy state as a node constraint).\n\
    \   Node constraints include power and memory, as well as constraints\n   placed\
    \ on the device by the user, such as battery life.\n"
- title: 7.  Broadcast/Multicast Requirements
  contents:
  - "7.  Broadcast/Multicast Requirements\n   Some existing industrial plant applications\
    \ do not use broadcast or\n   multicast addressing to communicate to field devices.\
    \  Unicast\n   address support is sufficient for them.\n   In some other industrial\
    \ process automation environments, multicast\n   over IP is used to deliver to\
    \ multiple nodes that may be functionally\n   similar or not.  Example usages\
    \ are:\n   1)  Delivery of alerts to multiple similar servers in an automation\n\
    \       control room.  Alerts are multicast to a group address based on\n    \
    \   the part of the automation process where the alerts arose (e.g.,\n       the\
    \ multicast address \"all-nodes-interested-in-alerts-for-\n       process-unit-X\"\
    ).  This is always a restricted-scope multicast,\n       not a broadcast.\n  \
    \ 2)  Delivery of common packets to multiple routers over a backbone,\n      \
    \ where the packets result in each receiving router initiating\n       multicast\
    \ (sometimes as a full broadcast) within the LLN.  For\n       instance, this\
    \ can be a byproduct of having potentially\n       physically separated backbone\
    \ routers that can inject messages\n       into different portions of the same\
    \ larger LLN.\n   3)  Publication of measurement data to more than one subscriber.\n\
    \       This feature is useful in some peer-to-peer control applications.\n  \
    \     For example, level position may be useful to a controller that\n       operates\
    \ the flow valve and also to the overfill alarm indicator.\n       Both controller\
    \ and alarm indicator would receive the same\n       publication sent as a multicast\
    \ by the level gauge.\n   All of these uses require an 1:N security mechanism\
    \ as well; they\n   aren't of any use if the end-to-end security is only point-to-point.\n\
    \   It is quite possible that first-generation wireless automation field\n   networks\
    \ can be adequately useful without either of these\n   capabilities, but in the\
    \ near future, wireless field devices with\n   communication controllers and protocol\
    \ stacks will require control\n   and configuration, such as firmware downloading,\
    \ that may benefit\n   from broadcast or multicast addressing.\n   The routing\
    \ protocol SHOULD support multicast addressing.\n"
- title: 8.  Protocol Performance Requirements
  contents:
  - "8.  Protocol Performance Requirements\n   The routing protocol MUST converge\
    \ after the addition of a new device\n   within several minutes, and SHOULD converge\
    \ within tens of seconds\n   such that a device is able to establish connectivity\
    \ to any other\n   point in the network or determine that there is a connectivity\
    \ issue.\n   Any routing algorithm used to determine how to route packets in the\n\
    \   network, MUST be capable of routing packets to and from a newly added\n  \
    \ device within several minutes of its addition, and SHOULD be able to\n   perform\
    \ this function within tens of seconds.\n   The routing protocol MUST distribute\
    \ sufficient information about\n   link failures to enable traffic to be routed\
    \ such that all service\n   requirements (especially latency) continue to be met.\
    \  This places a\n   requirement on the speed of distribution and convergence\
    \ of this\n   information as well as the responsiveness of any routing algorithms\n\
    \   used to determine how to route packets.  This requirement only\n   applies\
    \ at normal link failure rates (see Section 5) and MAY degrade\n   during failure\
    \ storms.\n   Any algorithm that computes routes for packets in the network MUST\
    \ be\n   able to perform route computations in advance of needing to use the\n\
    \   route.  Since such algorithms are required to react to link failures,\n  \
    \ link usage information, and other dynamic link properties as the\n   information\
    \ is distributed by the routing protocol, the algorithms\n   SHOULD recompute\
    \ route based on the receipt of new information.\n"
- title: 9.  Mobility Requirements
  contents:
  - "9.  Mobility Requirements\n   Various economic factors have contributed to a\
    \ reduction of trained\n   workers in the industrial plant.  A very common problem\
    \ is that of\n   the \"wireless worker\".  Carrying a PDA or something similar,\
    \ this\n   worker will be able to accomplish more work in less time than the\n\
    \   older, better-trained workers that he or she replaces.  Whether the\n   premise\
    \ is valid, the use case is commonly presented: the worker will\n   be wirelessly\
    \ connected to the plant IT system to download\n   documentation, instructions,\
    \ etc., and will need to be able to\n   connect \"directly\" to the sensors and\
    \ control points in or near the\n   equipment on which he or she is working. \
    \ It is possible that this\n   \"direct\" connection could come via the normal\
    \ LLNs data collection\n   network.  This connection is likely to require higher\
    \ bandwidth and\n   lower latency than the normal data collection operation.\n\
    \   PDAs are typically used as the user interfaces for plant historians,\n   asset\
    \ management systems, and the like.  It is undecided if these\n   PDAs will use\
    \ the LLN directly to talk to field sensors, or if they\n   will rather use other\
    \ wireless connectivity that proxies back into\n   the field or to anywhere else.\n\
    \   The routing protocol SHOULD support the wireless worker with fast\n   network\
    \ connection times of a few of seconds, and low command and\n   response latencies\
    \ to the plant behind the LLN access points, to\n   applications, and to field\
    \ devices.  The routing protocol SHOULD also\n   support the bandwidth allocation\
    \ for bulk transfers between the field\n   device and the handheld device of the\
    \ wireless worker.  The routing\n   protocol SHOULD support walking speeds for\
    \ maintaining network\n   connectivity as the handheld device changes position\
    \ in the wireless\n   network.\n   Some field devices will be mobile.  These devices\
    \ may be located on\n   moving parts such as rotating components, or they may\
    \ be located on\n   vehicles such as cranes or fork lifts.  The routing protocol\
    \ SHOULD\n   support vehicular speeds of up to 35 kmph.\n"
- title: 10.  Manageability Requirements
  contents:
  - "10.  Manageability Requirements\n   The process and control industry is manpower\
    \ constrained.  The aging\n   demographics of plant personnel are causing a looming\
    \ manpower\n   problem for industry across many markets.  The goal for the\n \
    \  industrial networks is to have the installation process not require\n   any\
    \ new skills for the plant personnel.  The person would install the\n   wireless\
    \ sensor or wireless actuator the same way the wired sensor or\n   wired actuator\
    \ is installed, except the step to connect wire is\n   eliminated.\n   Most users\
    \ in fact demand even much further simplified provisioning\n   methods, a plug\
    \ and play operation that would be fully transparent to\n   the user.  This requires\
    \ availability of open and untrusted side\n   channels for new joiners, and it\
    \ requires strong and automated\n   authentication so that networks can automatically\
    \ accept or reject\n   new joiners.  Ideally, for a user, adding new routing devices\
    \ should\n   be as easy as dragging and dropping an icon from a pool of\n   authenticated\
    \ new joiners into a pool for the wired domain that this\n   new sensor should\
    \ connect to.  Under the hood, invisible to the user,\n   auditable security mechanisms\
    \ should take care of new device\n   authentication, and secret join key distribution.\
    \  These more\n   sophisticated 'over the air' secure provisioning methods should\n\
    \   eliminate the use of traditional configuration tools for setting up\n   devices\
    \ prior to being ready to securely join an LLN access point.\n   The routing protocol\
    \ SHOULD be fully configurable over the air as\n   part of the joining process\
    \ of a new routing device.\n   There will be many new applications where even\
    \ without any human\n   intervention at the plant, devices that have never been\
    \ on site\n   before, should be allowed, based on their credentials and\n   cryptographic\
    \ capabilities, to connect anyway.  Examples are third-\n   party road tankers,\
    \ rail cargo containers with overfill protection\n   sensors, or consumer cars\
    \ that need to be refueled with hydrogen by\n   robots at future fueling stations.\n\
    \   The routing protocol for LLNs is expected to be easy to deploy and\n   manage.\
    \  Because the number of field devices in a network is large,\n   provisioning\
    \ the devices manually may not make sense.  The proper\n   operation of the routing\
    \ protocol MAY require that the node be\n   commissioned with information about\
    \ itself, like identity, security\n   tokens, radio standards and frequencies,\
    \ etc.\n   The routing protocol SHOULD NOT require to preprovision information\n\
    \   about the environment where the node will be deployed.  The routing\n   protocol\
    \ MUST enable the full discovery and setup of the environment\n   (available links,\
    \ selected peers, reachable network).  The protocol\n   MUST enable the distribution\
    \ of its own configuration to be performed\n   by some external mechanism from\
    \ a centralized management controller.\n"
- title: 11.  Antagonistic Requirements
  contents:
  - "11.  Antagonistic Requirements\n   This document contains a number of strongly\
    \ required constraints on\n   the ROLL routing protocol.  Some of those strong\
    \ requirements might\n   appear antagonistic and, as such, impossible to fulfill\
    \ at the same\n   time.\n   For instance, the strong requirement of power economy\
    \ applies on\n   general routing but is variant since it is reasonable to spend\
    \ more\n   energy on ensuring the availability of a short emergency closed-loop\n\
    \   path than it is to maintain an alert path that is used for regular\n   updates\
    \ on the operating status of the device.  In the same fashion,\n   the strong\
    \ requirement on easy provisioning does not match easily the\n   strong security\
    \ requirements that can be needed to implement a\n   factory policy.  Then again,\
    \ a non-default non-trivial setup can be\n   acceptable as long as the default\
    \ configuration enables a device to\n   join with some degree of security.\n \
    \  Convergence time and network size are also antagonistic.  The values\n   expressed\
    \ in Section 8 (\"Protocol Performance Requirements\") apply to\n   an average\
    \ network with tens of devices.  The use of a backbone can\n   maintain that level\
    \ of performance and still enable to grow the\n   network to thousands of node.\
    \  In any case, it is acceptable to grow\n   reasonably the convergence time with\
    \ the network size.\n"
- title: 12.  Security Considerations
  contents:
  - "12.  Security Considerations\n   Given that wireless sensor networks in industrial\
    \ automation operate\n   in systems that have substantial financial and human\
    \ safety\n   implications, security is of considerable concern.  Levels of\n \
    \  security violation that are tolerated as a \"cost of doing business\"\n   in\
    \ the banking industry are not acceptable when in some cases\n   literally thousands\
    \ of lives may be at risk.\n   Security is easily confused with guarantee for\
    \ availability.  When\n   discussing wireless security, it's important to distinguish\
    \ clearly\n   between the risks of temporarily losing connectivity, say due to\
    \ a\n   thunderstorm, and the risks associated with knowledgeable adversaries\n\
    \   attacking a wireless system.  The conscious attacks need to be split\n   between\
    \ 1) attacks on the actual application served by the wireless\n   devices and\
    \ 2) attacks that exploit the presence of a wireless access\n   point that may\
    \ provide connectivity onto legacy wired plant networks,\n   so these are attacks\
    \ that have little to do with the wireless devices\n   in the LLNs.  In the second\
    \ type of attack, access points that might\n   be wireless backdoors that allow\
    \ an attacker outside the fence to\n   access typically non-secured process control\
    \ and/or office networks\n   are typically the ones that do create exposures where\
    \ lives are at\n   risk.  This implies that the LLN access point on its own must\
    \ possess\n   functionality that guarantees domain segregation, and thus prohibits\n\
    \   many types of traffic further upstream.\n   The current generation of industrial\
    \ wireless device manufacturers is\n   specifying security at the MAC (Media Access\
    \ Control) layer and the\n   transport layer.  A shared key is used to authenticate\
    \ messages at\n   the MAC layer.  At the transport layer, commands are encrypted\
    \ with\n   statistically unique randomly generated end-to-end session keys.\n\
    \   HART7 [IEC62591] and ISA100.11a are examples of security systems for\n   industrial\
    \ wireless networks.\n   Although such symmetric key encryption and authentication\
    \ mechanisms\n   at MAC and transport layers may protect reasonably well during\
    \ the\n   lifecycle, the initial network boot (provisioning) step in many cases\n\
    \   requires more sophisticated steps to securely land the initial secret\n  \
    \ keys in field devices.  Also, it is vital that during these steps,\n   the ease\
    \ of deployment and the freedom of mixing and matching\n   products from different\
    \ suppliers does not complicate life for those\n   that deploy and commission.\
    \  Given the average skill levels in the\n   field and the serious resource constraints\
    \ in the market, investing a\n   little bit more in sensor-node hardware and software\
    \ so that new\n   devices automatically can be deemed trustworthy, and thus\n\
    \   automatically join the domains that they should join, with just one\n   drag-and-drop\
    \ action for those in charge of deploying, will yield\n   faster adoption and\
    \ proliferation of the LLN technology.\n   Industrial plants may not maintain\
    \ the same level of physical\n   security for field devices that is associated\
    \ with traditional\n   network sites such as locked IT centers.  In industrial\
    \ plants, it\n   must be assumed that the field devices have marginal physical\n\
    \   security and might be compromised.  The routing protocol SHOULD limit\n  \
    \ the risk incurred by one node being compromised, for instance by\n   proposing\
    \ a non-congruent path for a given route and balancing the\n   traffic across\
    \ the network.\n   The routing protocol SHOULD compartmentalize the trust placed\
    \ in\n   field devices so that a compromised field device does not destroy the\n\
    \   security of the whole network.  The routing MUST be configured and\n   managed\
    \ using secure messages and protocols that prevent outsider\n   attacks and limit\
    \ insider attacks from field devices installed in\n   insecure locations in the\
    \ plant.\n   The wireless environment typically forces the abandonment of\n  \
    \ classical 'by perimeter' thinking when trying to secure network\n   domains.\
    \  Wireless nodes in LLN networks should thus be regarded as\n   little islands\
    \ with trusted kernels, situated in an ocean of\n   untrusted connectivity, an\
    \ ocean that might be full of pirate ships.\n   Consequently, confidence in node\
    \ identity and ability to challenge\n   authenticity of source node credentials\
    \ gets more relevant.\n   Cryptographic boundaries inside devices that clearly\
    \ demark the\n   border between trusted and untrusted areas need to be drawn.\n\
    \   Protection against compromise of the cryptographic boundaries inside\n   the\
    \ hardware of devices is outside of the scope of this document.\n   Note that\
    \ because nodes are usually expected to be capable of\n   routing, the end-node\
    \ security requirements are usually a superset of\n   the router requirements,\
    \ in order to prevent a end node from being\n   used to inject forged information\
    \ into the network that could alter\n   the plant operations.\n   Additional details\
    \ of security across all application scenarios are\n   provided in the ROLL security\
    \ framework [ROLL-SEC-FMWK].\n   Implications of these security requirements for\
    \ the routing protocol\n   itself are a topic for future work.\n"
- title: 13.  Acknowledgements
  contents:
  - "13.  Acknowledgements\n   Many thanks to Rick Enns, Alexander Chernoguzov, and\
    \ Chol Su Kang for\n   their contributions.\n"
- title: 14.  References
  contents:
  - '14.  References

    '
- title: 14.1.  Normative References
  contents:
  - "14.1.  Normative References\n   [RFC2119]        Bradner, S., \"Key words for\
    \ use in RFCs to Indicate\n                    Requirement Levels\", BCP 14, RFC\
    \ 2119, March 1997.\n"
- title: 14.2.  Informative References
  contents:
  - "14.2.  Informative References\n   [HART]           HART (Highway Addressable\
    \ Remote Transducer)\n                    Communication Foundation, \"HART Communication\n\
    \                    Protocol and Foundation - Home Page\",\n                \
    \    <http://www.hartcomm.org>.\n   [IEC61158]       IEC, \"Industrial communication\
    \ networks - Fieldbus\n                    specifications\", IEC 61158 series.\n\
    \   [IEC62591]       IEC, \"Industrial communication networks - Wireless\n   \
    \                 communication network and communication profiles -\n       \
    \             WirelessHART\", IEC 62591.\n   [IEEE802.15.4]   IEEE, \"Telecommunications\
    \ and information exchange\n                    between systems -- Local and metropolitan\
    \ area\n                    networks -- Specific requirements Part 15.4:\n   \
    \                 Wireless Medium Access Control (MAC) and Physical\n        \
    \            Layer (PHY) Specifications for Low-Rate Wireless\n              \
    \      Personal Area Networks (WPANs)\", IEEE 802.15.4,\n                    2006.\n\
    \   [ISA100.11a]     ISA, \"Wireless systems for industrial automation:\n    \
    \                Process control and related applications\",\n               \
    \     ISA 100.11a, May 2008, <http://www.isa.org/\n                    Community/SP100WirelessSystemsforAutomation>.\n\
    \   [RFC4919]        Kushalnagar, N., Montenegro, G., and C. Schumacher,\n   \
    \                 \"IPv6 over Low-Power Wireless Personal Area Networks\n    \
    \                (6LoWPANs): Overview, Assumptions, Problem\n                \
    \    Statement, and Goals\", RFC 4919, August 2007.\n   [ROLL-SEC-FMWK]  Tsao,\
    \ T., Alexander, R., Dohler, M., Daza, V., and\n                    A. Lozano,\
    \ \"A Security Framework for Routing over\n                    Low Power and Lossy\
    \ Networks\", Work in Progress,\n                    September 2009.\n   [ROLL-TERM]\
    \      Vasseur, JP., \"Terminology in Low power And Lossy\n                  \
    \  Networks\", Work in Progress, October 2009.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Kris Pister (editor)\n   Dust Networks\n   30695 Huntwood\
    \ Ave.\n   Hayward, CA  94544\n   USA\n   EMail: kpister@dustnetworks.com\n  \
    \ Pascal Thubert (editor)\n   Cisco Systems\n   Village d'Entreprises Green Side\n\
    \   400, Avenue de Roumanille\n   Batiment T3\n   Biot - Sophia Antipolis  06410\n\
    \   FRANCE\n   Phone: +33 497 23 26 34\n   EMail: pthubert@cisco.com\n   Sicco\
    \ Dwars\n   Shell Global Solutions International B.V.\n   Sir Winston Churchilllaan\
    \ 299\n   Rijswijk  2288 DC\n   Netherlands\n   Phone: +31 70 447 2660\n   EMail:\
    \ sicco.dwars@shell.com\n   Tom Phinney\n   Consultant\n   5012 W. Torrey Pines\
    \ Circle\n   Glendale, AZ  85308-3221\n   USA\n   Phone: +1 602 938 3163\n   EMail:\
    \ tom.phinney@cox.net\n"
