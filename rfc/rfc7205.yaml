- title: __initial_text__
  contents:
  - '                Use Cases for Telepresence Multistreams

    '
- title: Abstract
  contents:
  - "Abstract\n   Telepresence conferencing systems seek to create an environment\
    \ that\n   gives users (or user groups) that are not co-located a feeling of co-\n\
    \   located presence through multimedia communication that includes at\n   least\
    \ audio and video signals of high fidelity.  A number of\n   techniques for handling\
    \ audio and video streams are used to create\n   this experience.  When these\
    \ techniques are not similar,\n   interoperability between different systems is\
    \ difficult at best, and\n   often not possible.  Conveying information about\
    \ the relationships\n   between multiple streams of media would enable senders\
    \ and receivers\n   to make choices to allow telepresence systems to interwork.\
    \  This\n   memo describes the most typical and important use cases for sending\n\
    \   multiple streams in a telepresence conference.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7205.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2014 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n   2.  Overview of Telepresence Scenarios  . . . . . . . . .\
    \ . . . .   4\n   3.  Use Cases . . . . . . . . . . . . . . . . . . . . . . .\
    \ . . .   6\n     3.1.  Point-to-Point Meeting: Symmetric . . . . . . . . . .\
    \ . .   7\n     3.2.  Point-to-Point Meeting: Asymmetric  . . . . . . . . . .\
    \ .   7\n     3.3.  Multipoint Meeting  . . . . . . . . . . . . . . . . . . .\
    \   9\n     3.4.  Presentation  . . . . . . . . . . . . . . . . . . . . . .  10\n\
    \     3.5.  Heterogeneous Systems . . . . . . . . . . . . . . . . . .  11\n  \
    \   3.6.  Multipoint Education Usage  . . . . . . . . . . . . . . .  12\n    \
    \ 3.7.  Multipoint Multiview (Virtual Space)  . . . . . . . . . .  14\n     3.8.\
    \  Multiple Presentation Streams - Telemedicine  . . . . . .  15\n   4.  Acknowledgements\
    \  . . . . . . . . . . . . . . . . . . . . . .  16\n   5.  Security Considerations\
    \ . . . . . . . . . . . . . . . . . . .  16\n   6.  Informative References  .\
    \ . . . . . . . . . . . . . . . . . .  16\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Telepresence applications try to provide a \"being there\"\
    \ experience\n   for conversational video conferencing.  Often, this telepresence\n\
    \   application is described as \"immersive telepresence\" in order to\n   distinguish\
    \ it from traditional video conferencing and from other\n   forms of remote presence\
    \ not related to conversational video\n   conferencing, such as avatars and robots.\
    \  The salient\n   characteristics of telepresence are often described as: being\
    \ actual\n   sized, providing immersive video, preserving interpersonal\n   interaction,\
    \ and allowing non-verbal communication.\n   Although telepresence systems are\
    \ based on open standards such as RTP\n   [RFC3550], SIP [RFC3261], H.264 [ITU.H264],\
    \ and the H.323 [ITU.H323]\n   suite of protocols, they cannot easily interoperate\
    \ with each other\n   without operator assistance and expensive additional equipment\
    \ that\n   translates from one vendor's protocol to another.\n   The basic features\
    \ that give telepresence its distinctive\n   characteristics are implemented in\
    \ disparate ways in different\n   systems.  Currently, telepresence systems from\
    \ diverse vendors\n   interoperate to some extent, but this is not supported in\
    \ a\n   standards-based fashion.  Interworking requires that translation and\n\
    \   transcoding devices be included in the architecture.  Such devices\n   increase\
    \ latency, reducing the quality of interpersonal interaction.\n   Use of these\
    \ devices is often not automatic; it frequently requires\n   substantial manual\
    \ configuration and a detailed understanding of the\n   nature of underlying audio\
    \ and video streams.  This state of affairs\n   is not acceptable for the continued\
    \ growth of telepresence -- these\n   systems should have the same ease of interoperability\
    \ as do\n   telephones.  Thus, a standard way of describing the multiple streams\n\
    \   constituting the media flows and the fundamental aspects of their\n   behavior\
    \ would allow telepresence systems to interwork.\n   This document presents a\
    \ set of use cases describing typical\n   scenarios.  Requirements will be derived\
    \ from these use cases in a\n   separate document.  The use cases are described\
    \ from the viewpoint of\n   the users.  They are illustrative of the user experience\
    \ that needs\n   to be supported.  It is possible to implement these use cases\
    \ in a\n   variety of different ways.\n   Many different scenarios need to be\
    \ supported.  This document\n   describes in detail the most common and basic\
    \ use cases.  These will\n   cover most of the requirements.  There may be additional\
    \ scenarios\n   that bring new features and requirements that can be used to extend\n\
    \   the initial work.\n   Point-to-point and multipoint telepresence conferences\
    \ are\n   considered.  In some use cases, the number of screens is the same at\n\
    \   all sites; in others, the number of screens differs at different\n   sites.\
    \  Both use cases are considered.  Also included is a use case\n   describing\
    \ display of presentation material or content.\n   The multipoint use cases may\
    \ include a variety of systems from\n   conference room systems to handheld devices,\
    \ and such a use case is\n   described in the document.\n   This document's structure\
    \ is as follows: Section 2 gives an overview\n   of scenarios, and Section 3 describes\
    \ use cases.\n"
- title: 2.  Overview of Telepresence Scenarios
  contents:
  - "2.  Overview of Telepresence Scenarios\n   This section describes the general\
    \ characteristics of the use cases\n   and what the scenarios are intended to\
    \ show.  The typical setting is\n   a business conference, which was the initial\
    \ focus of telepresence.\n   Recently, consumer products are also being developed.\
    \  We\n   specifically do not include in our scenarios the physical\n   infrastructure\
    \ aspects of telepresence, such as room construction,\n   layout, and decoration.\
    \  Furthermore, these use cases do not describe\n   all the aspects needed to\
    \ create the best user experience (for\n   example, the human factors).\n   We\
    \ also specifically do not attempt to precisely define the\n   boundaries between\
    \ telepresence systems and other systems, nor do we\n   attempt to identify the\
    \ \"best\" solution for each presented scenario.\n   Telepresence systems are\
    \ typically composed of one or more video\n   cameras and encoders and one or\
    \ more display screens of large size\n   (diagonal around 60 inches).  Microphones\
    \ pick up sound, and audio\n   codec(s) produce one or more audio streams.  The\
    \ cameras used to\n   capture the telepresence users are referred to as \"participant\n\
    \   cameras\" (and likewise for screens).  There may also be other\n   cameras,\
    \ such as for document display.  These will be referred to as\n   \"presentation\
    \ cameras\" or \"content cameras\", which generally have\n   different formats,\
    \ aspect ratios, and frame rates from the\n   participant cameras.  The presentation\
    \ streams may be shown on\n   participant screens or on auxiliary display screens.\
    \  A user's\n   computer may also serve as a virtual content camera, generating\
    \ an\n   animation or playing a video for display to the remote participants.\n\
    \   We describe such a telepresence system as sending one or more video\n   streams,\
    \ audio streams, and presentation streams to the remote\n   system(s).\n   The\
    \ fundamental parameters describing today's typical telepresence\n   scenarios\
    \ include:\n   1.   The number of participating sites\n   2.   The number of visible\
    \ seats at a site\n   3.   The number of cameras\n   4.   The number and type\
    \ of microphones\n   5.   The number of audio channels\n   6.   The screen size\n\
    \   7.   The screen capabilities -- such as resolution, frame rate,\n        aspect\
    \ ratio\n   8.   The arrangement of the screens in relation to each other\n  \
    \ 9.   The number of primary screens at each site\n   10.  Type and number of\
    \ presentation screens\n   11.  Multipoint conference display strategies -- for\
    \ example, the\n        camera-to-screen mappings may be static or dynamic\n \
    \  12.  The camera point of capture\n   13.  The cameras fields of view and how\
    \ they spatially relate to each\n        other\n   As discussed in the introduction,\
    \ the basic features that give\n   telepresence its distinctive characteristics\
    \ are implemented in\n   disparate ways in different systems.\n   There is no\
    \ agreed upon way to adequately describe the semantics of\n   how streams of various\
    \ media types relate to each other.  Without a\n   standard for stream semantics\
    \ to describe the particular roles and\n   activities of each stream in the conference,\
    \ interoperability is\n   cumbersome at best.\n   In a multiple-screen conference,\
    \ the video and audio streams sent\n   from remote participants must be understood\
    \ by receivers so that they\n   can be presented in a coherent and life-like manner.\
    \  This includes\n   the ability to present remote participants at their actual\
    \ size for\n   their apparent distance, while maintaining correct eye contact,\n\
    \   gesticular cues, and simultaneously providing a spatial audio sound\n   stage\
    \ that is consistent with the displayed video.\n   The receiving device that decides\
    \ how to render incoming information\n   needs to understand a number of variables\
    \ such as the spatial\n   position of the speaker, the field of view of the cameras,\
    \ the camera\n   zoom, which media stream is related to each of the screens, etc.\
    \  It\n   is not simply that individual streams must be adequately described,\n\
    \   to a large extent this already exists, but rather that the semantics\n   of\
    \ the relationships between the streams must be communicated.  Note\n   that all\
    \ of this is still required even if the basic aspects of the\n   streams, such\
    \ as the bit rate, frame rate, and aspect ratio, are\n   known.  Thus, this problem\
    \ has aspects considerably beyond those\n   encountered in interoperation of video\
    \ conferencing systems that have\n   a single camera/screen.\n"
- title: 3.  Use Cases
  contents:
  - "3.  Use Cases\n   The use cases focus on typical implementations.  There are\
    \ a number\n   of possible variants for these use cases; for example, the audio\n\
    \   supported may differ at the end points (such as mono or stereo versus\n  \
    \ surround sound), etc.\n   Many of these systems offer a \"full conference room\"\
    \ solution, where\n   local participants sit at one side of a table and remote\
    \ participants\n   are displayed as if they are sitting on the other side of the\
    \ table.\n   The cameras and screens are typically arranged to provide a panoramic\n\
    \   view of the remote room (left to right from the local user's\n   viewpoint).\n\
    \   The sense of immersion and non-verbal communication is fostered by a\n   number\
    \ of technical features, such as:\n   1.  Good eye contact, which is achieved\
    \ by careful placement of\n       participants, cameras, and screens.\n   2. \
    \ Camera field of view and screen sizes are matched so that the\n       images\
    \ of the remote room appear to be full size.\n   3.  The left side of each room\
    \ is presented on the right screen at\n       the far end; similarly, the right\
    \ side of the room is presented\n       on the left screen.  The effect of this\
    \ is that participants of\n       each site appear to be sitting across the table\
    \ from each other.\n       If 2 participants on the same site glance at each other,\
    \ all\n       participants can observe it.  Likewise, if a participant at one\n\
    \       site gestures to a participant on the other site, all\n       participants\
    \ observe the gesture itself and the participants it\n       includes.\n"
- title: '3.1.  Point-to-Point Meeting: Symmetric'
  contents:
  - "3.1.  Point-to-Point Meeting: Symmetric\n   In this case, each of the 2 sites\
    \ has an identical number of screens,\n   with cameras having fixed fields of\
    \ view, and 1 camera for each\n   screen.  The sound type is the same at each\
    \ end.  As an example,\n   there could be 3 cameras and 3 screens in each room,\
    \ with stereo\n   sound being sent and received at each end.\n   Each screen is\
    \ paired with a corresponding camera.  Each camera/\n   screen pair is typically\
    \ connected to a separate codec, producing an\n   encoded stream of video for\
    \ transmission to the remote site, and\n   receiving a similarly encoded stream\
    \ from the remote site.\n   Each system has one or multiple microphones for capturing\
    \ audio.  In\n   some cases, stereophonic microphones are employed.  In other\
    \ systems,\n   a microphone may be placed in front of each participant (or pair\
    \ of\n   participants).  In typical systems, all the microphones are connected\n\
    \   to a single codec that sends and receives the audio streams as either\n  \
    \ stereo or surround sound.  The number of microphones and the number\n   of audio\
    \ channels are often not the same as the number of cameras.\n   Also, the number\
    \ of microphones is often not the same as the number\n   of loudspeakers.\n  \
    \ The audio may be transmitted as multi-channel (stereo/surround sound)\n   or\
    \ as distinct and separate monophonic streams.  Audio levels should\n   be matched,\
    \ so the sound levels at both sites are identical.\n   Loudspeaker and microphone\
    \ placements are chosen so that the sound\n   \"stage\" (orientation of apparent\
    \ audio sources) is coordinated with\n   the video.  That is, if a participant\
    \ at one site speaks, the\n   participants at the remote site perceive her voice\
    \ as originating\n   from her visual image.  In order to accomplish this, the\
    \ audio needs\n   to be mapped at the received site in the same fashion as the\
    \ video.\n   That is, audio received from the right side of the room needs to\
    \ be\n   output from loudspeaker(s) on the left side at the remote site, and\n\
    \   vice versa.\n"
- title: '3.2.  Point-to-Point Meeting: Asymmetric'
  contents:
  - "3.2.  Point-to-Point Meeting: Asymmetric\n   In this case, each site has a different\
    \ number of screens and cameras\n   than the other site.  The important characteristic\
    \ of this scenario\n   is that the number of screens is different between the\
    \ 2 sites.  This\n   creates challenges that are handled differently by different\n\
    \   telepresence systems.\n   This use case builds on the basic scenario of 3\
    \ screens to 3 screens.\n   Here, we use the common case of 3 screens and 3 cameras\
    \ at one site,\n   and 1 screen and 1 camera at the other site, connected by a\
    \ point-to-\n   point call.  The screen sizes and camera fields of view at both\
    \ sites\n   are basically similar, such that each camera view is designed to show\n\
    \   2 people sitting side by side.  Thus, the 1-screen room has up to 2\n   people\
    \ seated at the table, while the 3-screen room may have up to 6\n   people at\
    \ the table.\n   The basic considerations of defining left and right and indicating\n\
    \   relative placement of the multiple audio and video streams are the\n   same\
    \ as in the 3-3 use case.  However, handling the mismatch between\n   the 2 sites\
    \ of the number of screens and cameras requires more\n   complicated maneuvers.\n\
    \   For the video sent from the 1-camera room to the 3-screen room,\n   usually\
    \ what is done is to simply use 1 of the 3 screens and keep the\n   second and\
    \ third screens inactive or, for example, put up the current\n   date.  This would\
    \ maintain the \"full-size\" image of the remote side.\n   For the other direction,\
    \ the 3-camera room sending video to the\n   1-screen room, there are more complicated\
    \ variations to consider.\n   Here are several possible ways in which the video\
    \ streams can be\n   handled.\n   1.  The 1-screen system might simply show only\
    \ 1 of the 3 camera\n       images, since the receiving side has only 1 screen.\
    \  2 people are\n       seen at full size, but 4 people are not seen at all. \
    \ The choice\n       of which one of the 3 streams to display could be fixed,\
    \ or could\n       be selected by the users.  It could also be made automatically\n\
    \       based on who is speaking in the 3-screen room, such that the\n       people\
    \ in the 1-screen room always see the person who is\n       speaking.  If the\
    \ automatic selection is done at the sender, the\n       transmission of streams\
    \ that are not displayed could be\n       suppressed, which would avoid wasting\
    \ bandwidth.\n   2.  The 1-screen system might be capable of receiving and decoding\n\
    \       all 3 streams from all 3 cameras.  The 1-screen system could then\n  \
    \     compose the 3 streams into 1 local image for display on the\n       single\
    \ screen.  All 6 people would be seen, but smaller than full\n       size.  This\
    \ could be done in conjunction with reducing the image\n       resolution of the\
    \ streams, such that encode/decode resources and\n       bandwidth are not wasted\
    \ on streams that will be downsized for\n       display anyway.\n   3.  The 3-screen\
    \ system might be capable of including all 6 people in\n       a single stream\
    \ to send to the 1-screen system.  For example, it\n       could use PTZ (Pan\
    \ Tilt Zoom) cameras to physically adjust the\n       cameras such that 1 camera\
    \ captures the whole room of 6 people.\n       Or, it could recompose the 3 camera\
    \ images into 1 encoded stream\n       to send to the remote site.  These variations\
    \ also show all 6\n       people but at a reduced size.\n   4.  Or, there could\
    \ be a combination of these approaches, such as\n       simultaneously showing\
    \ the speaker in full size with a composite\n       of all 6 participants in a\
    \ smaller size.\n   The receiving telepresence system needs to have information\
    \ about the\n   content of the streams it receives to make any of these decisions.\n\
    \   If the systems are capable of supporting more than one strategy,\n   there\
    \ needs to be some negotiation between the 2 sites to figure out\n   which of\
    \ the possible variations they will use in a specific point-\n   to-point call.\n"
- title: 3.3.  Multipoint Meeting
  contents:
  - "3.3.  Multipoint Meeting\n   In a multipoint telepresence conference, there are\
    \ more than 2 sites\n   participating.  Additional complexity is required to enable\
    \ media\n   streams from each participant to show up on the screens of the other\n\
    \   participants.\n   Clearly, there are a great number of topologies that can\
    \ be used to\n   display the streams from multiple sites participating in a\n\
    \   conference.\n   One major objective for telepresence is to be able to preserve\
    \ the\n   \"being there\" user experience.  However, in multi-site conferences,\n\
    \   it is often (in fact, usually) not possible to simultaneously provide\n  \
    \ full-size video, eye contact, and common perception of gestures and\n   gaze\
    \ by all participants.  Several policies can be used for stream\n   distribution\
    \ and display: all provide good results, but they all make\n   different compromises.\n\
    \   One common policy is called site switching.  Let's say the speaker is\n  \
    \ at site A and the other participants are at various \"remote\" sites.\n   When\
    \ the room at site A shown, all the camera images from site A are\n   forwarded\
    \ to the remote sites.  Therefore, at each receiving remote\n   site, all the\
    \ screens display camera images from site A.  This can be\n   used to preserve\
    \ full-size image display, and also provide full\n   visual context of the displayed\
    \ far end, site A.  In site switching,\n   there is a fixed relation between the\
    \ cameras in each room and the\n   screens in remote rooms.  The room or participants\
    \ being shown are\n   switched from time to time based on who is speaking or by\
    \ manual\n   control, e.g., from site A to site B.\n   Segment switching is another\
    \ policy choice.  In segment switching\n   (assuming still that site A is where\
    \ the speaker is, and \"remote\"\n   refers to all the other sites), rather than\
    \ sending all the images\n   from site A, only the speaker at site A is shown.\
    \  The camera images\n   of the current speaker and previous speakers (if any)\
    \ are forwarded\n   to the other sites in the conference.  Therefore, the screens\
    \ in each\n   site are usually displaying images from different remote sites --\
    \ the\n   current speaker at site A and the previous ones.  This strategy can\n\
    \   be used to preserve full-size image display and also capture the non-\n  \
    \ verbal communication between the speakers.  In segment switching, the\n   display\
    \ depends on the activity in the remote rooms (generally, but\n   not necessarily\
    \ based on audio/speech detection).\n   A third possibility is to reduce the image\
    \ size so that multiple\n   camera views can be composited onto one or more screens.\
    \  This does\n   not preserve full-size image display, but it provides the most\
    \ visual\n   context (since more sites or segments can be seen).  Typically in\n\
    \   this case, the display mapping is static, i.e., each part of each\n   room\
    \ is shown in the same location on the display screens throughout\n   the conference.\n\
    \   Other policies and combinations are also possible.  For example,\n   there\
    \ can be a static display of all screens from all remote rooms,\n   with part\
    \ or all of one screen being used to show the current speaker\n   at full size.\n"
- title: 3.4.  Presentation
  contents:
  - "3.4.  Presentation\n   In addition to the video and audio streams showing the\
    \ participants,\n   additional streams are used for presentations.\n   In systems\
    \ available today, generally only one additional video\n   stream is available\
    \ for presentations.  Often, this presentation\n   stream is half-duplex in nature,\
    \ with presenters taking turns.  The\n   presentation stream may be captured from\
    \ a PC screen, or it may come\n   from a multimedia source such as a document\
    \ camera, camcorder, or a\n   DVD.  In a multipoint meeting, the presentation\
    \ streams for the\n   currently active presentation are always distributed to\
    \ all sites in\n   the meeting, so that the presentations are viewed by all.\n\
    \   Some systems display the presentation streams on a screen that is\n   mounted\
    \ either above or below the 3 participant screens.  Other\n   systems provide\
    \ screens on the conference table for observing\n   presentations.  If multiple\
    \ presentation screens are used, they\n   generally display identical content.\
    \  There is considerable variation\n   in the placement, number, and size of presentation\
    \ screens.\n   In some systems, presentation audio is pre-mixed with the room\
    \ audio.\n   In others, a separate presentation audio stream is provided (if the\n\
    \   presentation includes audio).\n   In H.323 [ITU.H323] systems, H.239 [ITU.H239]\
    \ is typically used to\n   control the video presentation stream.  In SIP systems,\
    \ similar\n   control mechanisms can be provided using the Binary Floor Control\n\
    \   Protocol (BFCP) [RFC4582] for the presentation token.  These\n   mechanisms\
    \ are suitable for managing a single presentation stream.\n   Although today's\
    \ systems remain limited to a single video\n   presentation stream, there are\
    \ obvious uses for multiple presentation\n   streams:\n   1.  Frequently, the\
    \ meeting convener is following a meeting agenda,\n       and it is useful for\
    \ her to be able to show that agenda to all\n       participants during the meeting.\
    \  Other participants at various\n       remote sites are able to make presentations\
    \ during the meeting,\n       with the presenters taking turns.  The presentations\
    \ and the\n       agenda are both shown, either on separate screens, or perhaps\n\
    \       rescaled and shown on a single screen.\n   2.  A single multimedia presentation\
    \ can itself include multiple\n       video streams that should be shown together.\
    \  For instance, a\n       presenter may be discussing the fairness of media coverage.\
    \  In\n       addition to slides that support the presenter's conclusions, she\n\
    \       also has video excerpts from various news programs that she shows\n  \
    \     to illustrate her findings.  She uses a DVD player for the video\n     \
    \  excerpts so that she can pause and reposition the video as\n       needed.\n\
    \   3.  An educator who is presenting a multiscreen slide show.  This\n      \
    \ show requires that the placement of the images on the multiple\n       screens\
    \ at each site be consistent.\n   There are many other examples where multiple\
    \ presentation streams are\n   useful.\n"
- title: 3.5.  Heterogeneous Systems
  contents:
  - "3.5.  Heterogeneous Systems\n   It is common in meeting scenarios for people\
    \ to join the conference\n   from a variety of environments, using different types\
    \ of endpoint\n   devices.  A multiscreen immersive telepresence conference may\
    \ include\n   someone on a PC-based video conferencing system, a participant\n\
    \   calling in by phone, and (soon) someone on a handheld device.\n   What experience/view\
    \ will each of these devices have?\n   Some may be able to handle multiple streams,\
    \ and others can handle\n   only a single stream.  (Here, we are not talking about\
    \ legacy\n   systems, but rather systems built to participate in such a\n   conference,\
    \ although they are single stream only.)  In a single video\n   stream, the stream\
    \ may contain one or more compositions depending on\n   the available screen space\
    \ on the device.  In most cases, an\n   intermediate transcoding device will be\
    \ relied upon to produce a\n   single stream, perhaps with some kind of continuous\
    \ presence.\n   Bit rates will vary -- the handheld device and phone having lower\
    \ bit\n   rates than PC and multiscreen systems.\n   Layout is accomplished according\
    \ to different policies.  For example,\n   a handheld device and PC may receive\
    \ the active speaker stream.  The\n   decision can either be made explicitly by\
    \ the receiver or by the\n   sender if it can receive some kind of rendering hint.\
    \  The same is\n   true for audio -- i.e., that it receives a mixed stream or\
    \ a number\n   of the loudest speakers if mixing is not available in the network.\n\
    \   For the PC-based conferencing participant, the user's experience\n   depends\
    \ on the application.  It could be single stream, similar to a\n   handheld device\
    \ but with a bigger screen.  Or, it could be multiple\n   streams, similar to\
    \ an immersive telepresence system but with a\n   smaller screen.  Control for\
    \ manipulation of streams can be local in\n   the software application, or in\
    \ another location and sent to the\n   application over the network.\n   The handheld\
    \ device is the most extreme.  How will that participant\n   be viewed and heard?\
    \  It should be an equal participant, though the\n   bandwidth will be significantly\
    \ less than an immersive system.  A\n   receiver may choose to display output\
    \ coming from a handheld device\n   differently based on the resolution, but that\
    \ would be the case with\n   any low-resolution video stream, e.g., from a powerful\
    \ PC on a bad\n   network.\n   The handheld device will send and receive a single\
    \ video stream,\n   which could be a composite or a subset of the conference.\
    \  The\n   handheld device could say what it wants or could accept whatever the\n\
    \   sender (conference server or sending endpoint) thinks is best.  The\n   handheld\
    \ device will have to signal any actions it wants to take the\n   same way that\
    \ an immersive system signals actions.\n"
- title: 3.6.  Multipoint Education Usage
  contents:
  - "3.6.  Multipoint Education Usage\n   The importance of this example is that the\
    \ multiple video streams are\n   not used to create an immersive conferencing\
    \ experience with\n   panoramic views at all the sites.  Instead, the multiple\
    \ streams are\n   dynamically used to enable full participation of remote students\
    \ in a\n   university class.  In some instances, the same video stream is\n  \
    \ displayed on multiple screens in the room; in other instances, an\n   available\
    \ stream is not displayed at all.\n   The main site is a university auditorium\
    \ that is equipped with 3\n   cameras.  One camera is focused on the professor\
    \ at the podium.  A\n   second camera is mounted on the wall behind the professor\
    \ and\n   captures the class in its entirety.  The third camera is co-located\n\
    \   with the second and is designed to capture a close-up view of a\n   questioner\
    \ in the audience.  It automatically zooms in on that\n   student using sound\
    \ localization.\n   Although the auditorium is equipped with 3 cameras, it is\
    \ only\n   equipped with 2 screens.  One is a large screen located at the front\n\
    \   so that the class can see it.  The other is located at the rear so\n   the\
    \ professor can see it.  When someone asks a question, the front\n   screen shows\
    \ the questioner.  Otherwise, it shows the professor\n   (ensuring everyone can\
    \ easily see her).\n   The remote sites are typical immersive telepresence rooms,\
    \ each with\n   3 camera/screen pairs.\n   All remote sites display the professor\
    \ on the center screen at full\n   size.  A second screen shows the entire classroom\
    \ view when the\n   professor is speaking.  However, when a student asks a question,\
    \ the\n   second screen shows the close-up view of the student at full size.\n\
    \   Sometimes the student is in the auditorium; sometimes the speaking\n   student\
    \ is at another remote site.  The remote systems never display\n   the students\
    \ that are actually in that room.\n   If someone at a remote site asks a question,\
    \ then the screen in the\n   auditorium will show the remote student at full size\
    \ (as if they were\n   present in the auditorium itself).  The screen in the rear\
    \ also shows\n   this questioner, allowing the professor to see and respond to\
    \ the\n   student without needing to turn her back on the main class.\n   When\
    \ no one is asking a question, the screen in the rear briefly\n   shows a full-room\
    \ view of each remote site in turn, allowing the\n   professor to monitor the\
    \ entire class (remote and local students).\n   The professor can also use a control\
    \ on the podium to see a\n   particular site -- she can choose either a full-room\
    \ view or a\n   single-camera view.\n   Realization of this use case does not\
    \ require any negotiation between\n   the participating sites.  Endpoint devices\
    \ (and a Multipoint Control\n   Unit (MCU), if present) need to know who is speaking\
    \ and what video\n   stream includes the view of that speaker.  The remote systems\
    \ need\n   some knowledge of which stream should be placed in the center.  The\n\
    \   ability of the professor to see specific sites (or for the system to\n   show\
    \ all the sites in turn) would also require the auditorium system\n   to know\
    \ what sites are available and to be able to request a\n   particular view of\
    \ any site.  Bandwidth is optimized if video that is\n   not being shown at a\
    \ particular site is not distributed to that site.\n"
- title: 3.7.  Multipoint Multiview (Virtual Space)
  contents:
  - "3.7.  Multipoint Multiview (Virtual Space)\n   This use case describes a virtual\
    \ space multipoint meeting with good\n   eye contact and spatial layout of participants.\
    \  The use case was\n   proposed very early in the development of video conferencing\
    \ systems\n   as described in 1983 by Allardyce and Randal [virtualspace].  The\
    \ use\n   case is illustrated in Figure 2-5 of their report.  The virtual space\n\
    \   expands the point-to-point case by having all multipoint conference\n   participants\
    \ \"seated\" in a virtual room.  In this case, each\n   participant has a fixed\
    \ \"seat\" in the virtual room, so each\n   participant expects to see a different\
    \ view having a different\n   participant on his left and right side.  Today,\
    \ the use case is\n   implemented in multiple telepresence-type video conferencing\
    \ systems\n   on the market.  The term \"virtual space\" was used in their report.\n\
    \   The main difference between the result obtained with modern systems\n   and\
    \ those from 1983 are larger screen sizes.\n   Virtual space multipoint as defined\
    \ here assumes endpoints with\n   multiple cameras and screens.  Usually, there\
    \ is the same number of\n   cameras and screens at a given endpoint.  A camera\
    \ is positioned\n   above each screen.  A key aspect of virtual space multipoint\
    \ is the\n   details of how the cameras are aimed.  The cameras are each aimed\
    \ on\n   the same area of view of the participants at the site.  Thus, each\n\
    \   camera takes a picture of the same set of people but from a different\n  \
    \ angle.  Each endpoint sender in the virtual space multipoint meeting\n   therefore\
    \ offers a choice of video streams to remote receivers, each\n   stream representing\
    \ a different viewpoint.  For example, a camera\n   positioned above a screen\
    \ to a participant's left may take video\n   pictures of the participant's left\
    \ ear; while at the same time, a\n   camera positioned above a screen to the participant's\
    \ right may take\n   video pictures of the participant's right ear.\n   Since\
    \ a sending endpoint has a camera associated with each screen, an\n   association\
    \ is made between the receiving stream output on a\n   particular screen and the\
    \ corresponding sending stream from the\n   camera associated with that screen.\
    \  These associations are repeated\n   for each screen/camera pair in a meeting.\
    \  The result of this system\n   is a horizontal arrangement of video images from\
    \ remote sites, one\n   per screen.  The image from each screen is paired with\
    \ the camera\n   output from the camera above that screen, resulting in excellent\
    \ eye\n   contact.\n"
- title: 3.8.  Multiple Presentation Streams - Telemedicine
  contents:
  - "3.8.  Multiple Presentation Streams - Telemedicine\n   This use case describes\
    \ a scenario where multiple presentation\n   streams are used.  In this use case,\
    \ the local site is a surgery room\n   connected to one or more remote sites that\
    \ may have different\n   capabilities.  At the local site, 3 main cameras capture\
    \ the whole\n   room (the typical 3-camera telepresence case).  Also, multiple\n\
    \   presentation inputs are available: a surgery camera that is used to\n   provide\
    \ a zoomed view of the operation, an endoscopic monitor, a\n   flouroscope (X-ray\
    \ imaging), an ultrasound diagnostic device, an\n   electrocardiogram (ECG) monitor,\
    \ etc.  These devices are used to\n   provide multiple local video presentation\
    \ streams to help the surgeon\n   monitor the status of the patient and assist\
    \ in the surgical process.\n   The local site may have 3 main screens and one\
    \ (or more) presentation\n   screen(s).  The main screens can be used to display\
    \ the remote\n   experts.  The presentation screen(s) can be used to display multiple\n\
    \   presentation streams from local and remote sites simultaneously.  The\n  \
    \ 3 main cameras capture different parts of the surgery room.  The\n   surgeon\
    \ can decide the number, the size, and the placement of the\n   presentations\
    \ displayed on the local presentation screen(s).  He can\n   also indicate which\
    \ local presentation captures are provided for the\n   remote sites.  The local\
    \ site can send multiple presentation captures\n   to remote sites, and it can\
    \ receive from them multiple presentations\n   related to the patient or the procedure.\n\
    \   One type of remote site is a single- or dual-screen and one-camera\n   system\
    \ used by a consulting expert.  In the general case, the remote\n   sites can\
    \ be part of a multipoint telepresence conference.  The\n   presentation screens\
    \ at the remote sites allow the experts to see the\n   details of the operation\
    \ and related data.  Like the main site, the\n   experts can decide the number,\
    \ the size, and the placement of the\n   presentations displayed on the presentation\
    \ screens.  The\n   presentation screens can display presentation streams from\
    \ the\n   surgery room, from other remote sites, or from local presentation\n\
    \   streams.  Thus, the experts can also start sending presentation\n   streams\
    \ that can carry medical records, pathology data, or their\n   references and\
    \ analysis, etc.\n   Another type of remote site is a typical immersive telepresence\
    \ room\n   with 3 camera/screen pairs, allowing more experts to join the\n   consultation.\
    \  These sites can also be used for education.  The\n   teacher, who is not necessarily\
    \ the surgeon, and the students are in\n   different remote sites.  Students can\
    \ observe and learn the details\n   of the whole procedure, while the teacher\
    \ can explain and answer\n   questions during the operation.\n   All remote education\
    \ sites can display the surgery room.  Another\n   option is to display the surgery\
    \ room on the center screen, and the\n   rest of the screens can show the teacher\
    \ and the student who is\n   asking a question.  For all the above sites, multiple\
    \ presentation\n   screens can be used to enhance visibility: one screen for the\
    \ zoomed\n   surgery stream and the others for medical image streams, such as\
    \ MRI\n   images, cardiograms, ultrasonic images, and pathology data.\n"
- title: 4.  Acknowledgements
  contents:
  - "4.  Acknowledgements\n   The document has benefitted from input from a number\
    \ of people\n   including Alex Eleftheriadis, Marshall Eubanks, Tommy Andre Nyquist,\n\
    \   Mark Gorzynski, Charles Eckel, Nermeen Ismail, Mary Barnes, Pascal\n   Buhler,\
    \ and Jim Cole.\n   Special acknowledgement to Lennard Xiao, who contributed the\
    \ text for\n   the telemedicine use case, and to Claudio Allocchio for his detailed\n\
    \   review of the document.\n"
- title: 5.  Security Considerations
  contents:
  - "5.  Security Considerations\n   While there are likely to be security considerations\
    \ for any solution\n   for telepresence interoperability, this document has no\
    \ security\n   considerations.\n"
- title: 6.  Informative References
  contents:
  - "6.  Informative References\n   [ITU.H239]  ITU-T, \"Role management and additional\
    \ media channels for\n               H.300-series terminals\", ITU-T Recommendation\
    \ H.239,\n               September 2005.\n   [ITU.H264]  ITU-T, \"Advanced video\
    \ coding for generic audiovisual\n               services\", ITU-T Recommendation\
    \ H.264, April 2013.\n   [ITU.H323]  ITU-T, \"Packet-based Multimedia Communications\
    \ Systems\",\n               ITU-T Recommendation H.323, December 2009.\n   [RFC3261]\
    \   Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston,\n               A.,\
    \ Peterson, J., Sparks, R., Handley, M., and E.\n               Schooler, \"SIP:\
    \ Session Initiation Protocol\", RFC 3261,\n               June 2002.\n   [RFC3550]\
    \   Schulzrinne, H., Casner, S., Frederick, R., and V.\n               Jacobson,\
    \ \"RTP: A Transport Protocol for Real-Time\n               Applications\", STD\
    \ 64, RFC 3550, July 2003.\n   [RFC4582]   Camarillo, G., Ott, J., and K. Drage,\
    \ \"The Binary Floor\n               Control Protocol (BFCP)\", RFC 4582, November\
    \ 2006.\n   [virtualspace]\n               Allardyce, L. and L. Randall, \"Development\
    \ of\n               Teleconferencing Methodologies with Emphasis on Virtual\n\
    \               Space Video and Interactive Graphics\", April 1983,\n        \
    \       <http://www.dtic.mil/docs/citations/ADA127738>.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Allyn Romanow\n   Cisco\n   San Jose, CA  95134\n   US\n\
    \   EMail: allyn@cisco.com\n   Stephen Botzko\n   Polycom\n   Andover, MA  01810\n\
    \   US\n   EMail: stephen.botzko@polycom.com\n   Mark Duckworth\n   Polycom\n\
    \   Andover, MA  01810\n   US\n   EMail: mark.duckworth@polycom.com\n   Roni Even\
    \ (editor)\n   Huawei Technologies\n   Tel Aviv\n   Israel\n   EMail: roni.even@mail01.huawei.com\n"
