- title: __initial_text__
  contents:
  - '                       WHITE PAGES MEETING REPORT

    '
- title: STATUS OF THIS MEMO
  contents:
  - "STATUS OF THIS MEMO\n   This memo provides information for the Internet community.\
    \  This memo\n   does not specify an Internet standard of any kind.  Distribution\
    \ of\n   this memo is unlimited.\n"
- title: INTRODUCTION
  contents:
  - "INTRODUCTION\n   This report describes the results of a meeting held at the November\n\
    \   IETF (Internet Engineering Task Force) in Houston, TX, on November 2,\n  \
    \ 1993, to discuss the future of and approaches to a white pages\n   directory\
    \ services for the Internet.\n   As proposed to the National Science Foundation\
    \ (NSF), USC/Information\n   Sciences Institute (ISI) conducted the meeting to\
    \ discuss the\n   viability of the X.500 directory as a practical approach to\
    \ providing\n   white pages service for the Internet in the near future and to\n\
    \   identify and discuss any alternatives.\n   An electronic mail mailing list\
    \ was organized and discussions were\n   held via email for two weeks prior to\
    \ the meeting.\n"
- title: 1. EXECUTIVE SUMMARY
  contents:
  - "1. EXECUTIVE SUMMARY\n   This report is organized around four questions:\n  \
    \ 1) What functions should a white pages directory perform?\n      There are two\
    \ functions the white pages service must provide:\n      searching and retrieving.\n\
    \      Searching is the ability to find people given some fuzzy\n      information\
    \ about them.  Such as \"Find the Postel in southern\n      California\".  Searches\
    \ may often return a list of matches.\n      While the idea of indexing has been\
    \ around for some time, such as\n      the IN-ADDR tree in the Domain Name System\
    \ (DNS), a new\n      acknowledgment of its importance has emerged from these\n\
    \      discussions.  Users want fast searching across the distributed\n      database\
    \ on attributes different from the database structure.\n      Pre-computed indices\
    \ satisfy this desire, though only for\n      specified searches.\n      Retrieval\
    \ is obtaining additional information associated with a\n      person, such as\
    \ an address, telephone number, email mailbox, or\n      security certificate.\n\
    \      Security certificates (a type of information associated with an\n     \
    \ individual) are essential for the use of end-to-end\n      authentication, integrity,\
    \ and privacy in Internet applications.\n      The development of secure applications\
    \ in the Internet is\n      dependent on a directory system for retrieving the\
    \ security\n      certificate associated with an individual.  For example, the\n\
    \      privacy enhanced electronic mail (PEM) system has been developed\n    \
    \  and is ready to go into service, and is now hindered by the lack\n      of\
    \ an easily used directory of security certificates.  An open\n      question\
    \ is whether or not such a directory needs to be internally\n      secure.\n \
    \  2) What approaches will provide us with a white pages directory?\n      It\
    \ is evident that there are and will be several technologies in\n      use.  In\
    \ order to provide a white pages directory service that\n      accommodates multiple\
    \ technologies, we should promote\n      interoperation and work toward a specification\
    \ of the simplest\n      common communication form that is powerful enough to\
    \ provide the\n      necessary functionality.  This \"common ground\" approach\
    \ aims to\n      provide the ubiquitous WPS (White Pages Service) with a high\n\
    \      functionality and a low entry cost.\n   3) What are the problems to be\
    \ overcome?\n      It must be much easier to be part of the Internet white pages\
    \ than\n      to bring up a X.500 DSA (Directory Service Agent), yet we must\n\
    \      make good use of the already deployed X.500 DSAs.  Simpler white\n    \
    \  pages services (such as Whois++) must be defined to promote\n      multiple\
    \ implementations.  To promote reliable operation, there\n      must be some central\
    \ management of the X.500 system.  A common\n      naming scheme must be identified\
    \ and documented.  A set of index-\n      servers, and indexing techniques, must\
    \ be developed.  The storage\n      and retrieval of security certificates must\
    \ be provided.\n   4) What should the deployment strategy be?\n      Some central\
    \ management must be provided, and easy to use user\n      interfaces (such as\
    \ the Gopher \"gateway\"), must be widely\n      deployed.  The selection of a\
    \ naming scheme must be documented.\n      We should capitalize on the existing\
    \ infrastructure of already\n      deployed X.500 DSAs.  The \"common ground\"\
    \ model should be adopted.\n      A specification of the simplest common communication\
    \ form must be\n      developed.  Information about how to set up a new server\
    \ (of\n      whatever kind) in \"cookbook\" form should be made available.\n \
    \  RECOMMENDATIONS\n    1.  Adopt the common ground approach.  Encourage multiple\
    \ client and\n        server types, and the standardization of an interoperation\n\
    \        protocol between them.  The clients may be simple clients,\n        front-ends,\
    \ \"gateways\", or embedded in other information access\n        clients, such\
    \ as Gopher or WWW (World Wide Web) client programs.\n        The interoperation\
    \ protocol will define message types, message\n        sequences, and data fields.\
    \  An element of this protocol should\n        be the use of Universal Record\
    \ Locators (URLs).\n    2.  Promote the development of index-servers.  The index-servers\n\
    \        should use several different methods both for gathering data for\n  \
    \      their indices, and for searching their indices.\n    3.  Support a central\
    \ management for the X.500 system.  To get the\n        best advantage of the\
    \ effort already invested in the X.500\n        directory system it is essential\
    \ to provide the relatively small\n        amount of central management necessary\
    \ to keep the system\n        functioning.\n    4.  Support the development of\
    \ security certificate storage and\n        retrieval from the white pages service.\
    \  One practical approach\n        is initially to focus on getting support from\
    \ the existing X.500\n        directory infrastructure.  This effort should also\
    \ include\n        design and development of the storage and retrieval of security\n\
    \        certificates for other white pages services, such as Whois++.\n"
- title: 2. HISTORY
  contents:
  - "2. HISTORY\n   In February 1989, a meeting on Internet white pages service was\n\
    \   initiated by the FRICC (Federal Research Internet Coordinating\n   Committee)\
    \ and the ensuing discussions resulted in RFC 1107 [1] that\n   offered some technical\
    \ conclusions.  Widespread deployment was to\n   have taken place by mid-1992.\n\
    \         RFC 1107: K. Sollins, \"Plan for Internet Directory Services\",\n  \
    \       [1].\n   Several other RFCs have been written suggesting deployment strategies\n\
    \   and plans for an X.500 Directory Service.\n   They are:\n         RFC 1275:\
    \ S. Hardcastle-Kille, \"Replication Requirements to\n         provide an Internet\
    \ Directory using X.500\", [2].\n         RFC 1308: C. Weider, J. Reynolds, \"\
    Executive Introduction to\n         Directory Services Using the X.500 Protocol\"\
    , [3].\n         RFC 1309: C. Weider, J. Reynolds, S. Heker, \"Technical Overview\n\
    \         of Directory Services Using the X.500 Protocol\", [4].\n         RFC\
    \ 1430: S. Hardcastle-Kille, E. Huizer, V. Cerf, R. Hobby &\n         S. Kent,\
    \ \"A Strategic Plan for Deploying an Internet X.500\n         Directory Service\"\
    , [5].\n   Also, a current working draft submitted by A. Jurg of SURFnet\n   entitled,\
    \ \"Introduction to White pages services based on X.500\",\n   describes why we\
    \ need a global white pages service and why X.500 is\n   the answer [6].\n   The\
    \ North America Directory Forum (NADF) also has done some useful\n   work setting\
    \ conventions for commercial providers of X.500 directory\n   service.  Their\
    \ series of memos is relevant to this discussion.  (See\n   RFC 1417 for an overview\
    \ of this note series [7].)  In particular,\n   NADF standing document 5 (SD-5)\
    \ \"An X.500 Naming Scheme for National\n   DIT Subtrees and its Application for\
    \ c=CA and c=US\" is of interest\n   for its model of naming based on civil naming\
    \ authorities [8].\n   Deployment of a X.500 directory service including that\
    \ under the PSI\n   (Performance Systems International) White Pages Pilot Project\
    \ and the\n   PARADISE Project is significant, and continues to grow, albeit at\
    \ a\n   slower rate than the Internet.\n"
- title: 3. QUESTIONS
  contents:
  - "3. QUESTIONS\n   Four questions were posed to the discussion list:\n      1)\
    \ What functions should a white pages directory perform?\n      2) What approaches\
    \ will provide us with a white pages directory?\n      3) What are the problems\
    \ to be overcome?\n      4) What should the deployment strategy be?\n"
- title: 3.A. WHAT FUNCTIONS SHOULD A WHITE PAGES DIRECTORY PERFORM?
  contents:
  - "3.A. WHAT FUNCTIONS SHOULD A WHITE PAGES DIRECTORY PERFORM?\n   The basic function\
    \ of a white pages service is to find people and\n   information about people.\n\
    \   In finding people, the service should work fast when searching for\n   people\
    \ by name, even if the information regarding location or\n   organization is vague.\
    \  In finding information about people, the\n   service should retrieve information\
    \ associated with people, such as a\n   phone number, a postal or email address,\
    \ or even a certificate for\n   security applications (authentication, integrity,\
    \ and privacy).\n   Sometimes additional information associated with people is\
    \ provided\n   by a directory service, such as a list of publications, a description\n\
    \   of current projects, or a current travel itinerary.\n   Back in 1989, RFC\
    \ 1107 detailed 8 requirements of a white pages\n   service: (1) functionality,\
    \ (2) correctness of information, (3) size,\n   (4) usage and query rate, (5)\
    \ response time, (6) partitioned\n   authority, (7) access control, (8) multiple\
    \ transport protocol\n   support; and 4 additional features that would make it\
    \ more useful:\n   (1) descriptive naming that could support a yellow pages service,\
    \ (2)\n   accountability, (3) multiple interfaces, and (4) multiple clients.\n\
    \   Since the writing of RFC 1107, many additional functions have been\n   identified.\
    \  A White Pages Functionality List is attached as Appendix\n   1.  The problem\
    \ is harder now, the Internet is much bigger, and there\n   are many more options\
    \ available (Whois++, Netfind, LDAP (Lightweight\n   Direct Access Protocol),\
    \ different versions of X.500 implementations,\n   etc.)\n   A white pages directory\
    \ should be flexible, should have low resource\n   requirements, and should fit\
    \ into other systems that may be currently\n   in use; it should not cost a lot,\
    \ so that future transitions are not\n   too costly; there should be the ability\
    \ to migrate to something else,\n   if a better solution becomes available; there\
    \ should be a way to\n   share local directory information with the Internet in\
    \ a seamless\n   fashion and with little extra effort; the query responses should\
    \ be\n   reliable enough and consistent enough that automated tools could be\n\
    \   used.\n"
- title: 3.B. WHAT APPROACHES WILL PROVIDE US WITH A WHITE PAGES DIRECTORY?
  contents:
  - "3.B. WHAT APPROACHES WILL PROVIDE US WITH A WHITE PAGES DIRECTORY?\n   People\
    \ have different needs, tastes, etc.  Consequently, a large part\n   of the ultimate\
    \ solution will include bridging among these various\n   solutions.  Already we\
    \ see a Gopher to X.500 gateway, a Whois++ to\n   X.500 gateway, and the beginnings\
    \ of a WWW to X.500 gateway.  Gopher\n   can talk to CSO (a phonebook service\
    \ developed by University of\n   Illinois), WAIS (Wide Area Information Server),\
    \ etc.  WWW can talk to\n   everything.  Netfind knows about several other protocols.\n\
    \   Gopher and WAIS \"achieved orbit\" simply by providing means for people\n\
    \   to export and to access useful information; neither system had to\n   provide\
    \ ubiquitous service.  For white pages, if the service doesn't\n   provide answers\
    \ to specific user queries some reasonable proportion\n   of the time, users view\
    \ it as as failure.  One way to achieve a high\n   hit rate in an exponentially\
    \ growing Internet is to use a proactive\n   data gathering architecture (e.g.,\
    \ as realized by archie and\n   Netfind).  Important as they are, replication,\
    \ authentication, etc.,\n   are irrelevant if no one uses the service.\n   There\
    \ are pluses and minuses to a proactive data gathering method.\n   On the plus\
    \ side, one can build a large database quickly.  On the\n   minus side, one can\
    \ get garbage in the database.  One possibility is\n   to use a proactive approach\
    \ to (a) acquire data for administrative\n   review before being added to the\
    \ database, and/or (b) to check the\n   data for consistency with the real world.\
    \  Additionally, there is\n   some question about the legality of proactive methods\
    \ in some\n   countries.\n   One solution is to combine existing technology and\
    \ infrastructure to\n   provide a good white pages service, based on a X.500 core\
    \ plus a set\n   of additional index/references servers.  DNS can be used to \"\
    refer\"\n   to the appropriate zone in the X.500 name space, using WAIS or\n \
    \  Whois++, to build up indexes to the X.500 server which will be able\n   to\
    \ process a given request.  These can be index-servers or centroids\n   or something\
    \ new.\n   Some X.500 purists might feel this approach muddles the connecting\n\
    \   fabric among X.500 servers, since the site index, DNS records, and\n   customization\
    \ gateways are all outside of X.500.  On the other hand,\n   making X.500 reachable\
    \ from a common front-end would provide added\n   incentive for sites to install\
    \ X.500 servers.  Plus, it provides an\n   immediate (if interim) solution to\
    \ the need for a global site index\n   in X.500.  Since the goal is to have a\
    \ good white pages service,\n   X.500 purity is not essential.\n   It may be that\
    \ there are parts of the white pages problem that cannot\n   be addressed without\
    \ \"complex technology\".  A solution that allows\n   the user to progress up\
    \ the ladder of complexity, according to taste,\n   perceived need, and available\
    \ resources may be a much healthier\n   approach.  However, experience to date\
    \ with simpler solutions\n   (Whois++, Netfind, archie) indicates that a good\
    \ percentage of the\n   problem of finding information can be addressed with simpler\n\
    \   approaches.  Users know this and will resist attempts to make them\n   pay\
    \ the full price for the full solution when it is not needed.\n   Whereas managers\
    \ and funders may be concerned with the complexity of\n   the technology, users\
    \ are generally more concerned with the quality\n   and ease of use of the service.\
    \  A danger in supporting a mix of\n   technologies is that the service may become\
    \ so variable that the\n   loose constraints of weak service in some places lead\
    \ users to see\n   the whole system as too loose and weak.\n   Some organizations\
    \ will not operate services that they cannot get for\n   free or they cannot try\
    \ cheaply before investing time and money.\n   Some people prefer a bare-bones,\
    \ no support solution that only gives\n   them 85 percent of what they want. \
    \ Paying for the service would not\n   be a problem for many sites, once the value\
    \ of the service has been\n   proven.  Although there is no requirement to provide\
    \ free software\n   for everybody, we do need viable funding and support mechanisms.\
    \  A\n   solution can not be simply dictated with any expectation that it will\n\
    \   stick.\n   Finally, are there viable alternative technologies to X.500 now\
    \ or do\n   we need to design something new?  What kind of time frame are we\n\
    \   talking about for development and deployment?  And will the new\n   technology\
    \ be extensible enough to provide for the as yet unimagined\n   uses that will\
    \ be required of directory services 5 years from now?\n   And will this directory\
    \ service ultimately provide more capabilities\n   than just white pages?\n"
- title: 3.C. WHAT ARE THE PROBLEMS TO BE OVERCOME?
  contents:
  - "3.C. WHAT ARE THE PROBLEMS TO BE OVERCOME?\n   There are two classes of problems\
    \ to be examined; technology issues\n   and infrastructure.\n   TECHNOLOGY:\n\
    \   How do we populate the database and make software easily available?\n   Many\
    \ people suggest that a public domain version of X.500 is\n   necessary before\
    \ a wide spread X.500 service is operational.  The\n   current public domain version\
    \ is said to be difficult to install and\n   to bring into operation, but many\
    \ organizations have successfully\n   installed it and have had their systems\
    \ up and running for some time.\n   Note that the current public domain program,\
    \ quipu, is not quite\n   standard X.500, and is more suited to research than\
    \ production\n   service.  Many people who tried earlier versions of quipu abandoned\n\
    \   X.500 due to its costly start up time, and inherent complexity.\n   The ISODE\
    \ (ISO Development Environment) Consortium is currently\n   developing newer features\
    \ and is addressing most of the major\n   problems.  However, there is the perception\
    \ that the companies in the\n   consortium have yet to turn these improvements\
    \ into actual products,\n   though the consortium says the companies have commercial\
    \ off-the-\n   shelf (COTS) products available now.  The improved products are\n\
    \   certainly needed now, since if they are too late in being deployed,\n   other\
    \ solutions will be implemented in lieu of X.500.\n   The remaining problem with\
    \ an X.500 White Pages is having a high\n   quality public domain DSA.  The ISODE\
    \ Consortium will make its\n   version available for no charge to Universities\
    \ (or any non-profit or\n   government organization whose primary purpose is research)\
    \ but if\n   that leaves a sizeable group using the old quipu implementation,\
    \ then\n   there is a significant problem.  In such a case, an answer may be for\n\
    \   some funding to upgrade the public version of quipu.\n   In addition, the\
    \ quipu DSA should be simplified so that it is easy to\n   use.  Tim Howes' new\
    \ disk-based quipu DSA solves many of the memory\n   problems in DSA resource\
    \ utilization.  If one fixes the DSA resource\n   utilization problem, makes it\
    \ fairly easy to install, makes it freely\n   available, and publishes a popular\
    \ press book about it, X.500 may\n   have a better chance of success.\n   The\
    \ client side of X.500 needs more work.  Many people would rather\n   not expend\
    \ the extra effort to get X.500 up.  X.500 takes a sharp\n   learning curve. \
    \ There is a perception that the client side also\n   needs a complex Directory\
    \ User Interface (DUI) built on ISODE.  Yet\n   there are alternative DUIs, such\
    \ as those based on LDAP.  Another\n   aspect of the client side is that access\
    \ to the directory should be\n   built into other applications like gopher and\
    \ email (especially,\n   accessing PEM X.509 certificates).\n   We also need data\
    \ conversion tools to make the transition between\n   different systems possible.\
    \  For example, NASA had more than one\n   system to convert.\n   Searching abilities\
    \ for X.500 need to be improved.  LDAP is great\n   help, but the following capabilities\
    \ are still needed:\n   -- commercial grade easily maintainable servers with back-end\n\
    \      database support.\n   -- clients that can do exhaustive search and/or cache\
    \ useful\n      information and use heuristics to narrow the search space in case\n\
    \      of ill-formed queries.\n   -- index servers that store index information\
    \ on a \"few\" key\n      attributes that DUIs can consult in narrowing the search\
    \ space.\n      How about index attributes at various levels in the tree that\n\
    \      capture the information in the corresponding subtree?\n   Work still needs\
    \ to be done with Whois++ to see if it will scale to\n   the level of X.500.\n\
    \   An extended Netfind is attractive because it would work without any\n   additional\
    \ infrastructure changes (naming, common schema, etc.), or\n   even the addition\
    \ of any new protocols.\n   INFRASTRUCTURE:\n   The key issues are central management\
    \ and naming rules.\n   X.500 is not run as a service in the U.S., and therefore\
    \ those using\n   X.500 in the U.S. are not assured of the reliability of root\
    \ servers.\n   X.500 cannot be taken seriously until there is some central\n \
    \  management and coordinated administration support in place.  Someone\n   has\
    \ to be responsible for maintaining the root; this effort is\n   comparable to\
    \ maintaining the root of the DNS.  PSI provided this\n   service until the end\
    \ of the FOX project [9]; should they receive\n   funding to continue this?  Should\
    \ this be a commercial enterprise?\n   Or should this function be added to the\
    \ duties of the InterNIC?\n   New sites need assistance in getting their servers\
    \ up and linked to a\n   central server.\n   There are two dimensions along which\
    \ to consider the infrastructure:\n   1) general purpose vs. specific, and 2)\
    \ tight vs. loose information\n   framework.\n   General purpose leads to more\
    \ complex protocols - the generality is\n   an overhead, but gives the potential\
    \ to provide a framework for a\n   wide variety of services.  Special purpose\
    \ protocols are simpler, but\n   may lead to duplication or restricted scope.\n\
    \   Tight information framework costs effort to coerce existing data and\n   to\
    \ build structures.  Once in place, it gives better managability and\n   more\
    \ uniform access.  The tight information framework can be\n   subdivided further\
    \ into: 1) the naming approach, and 2) the object\n   and attribute extensibility.\n\
    \   Examples of systems placed in this space are: a) X.500 is a general\n   purpose\
    \ and tight information framework, b) DNS is a specific and\n   tight information\
    \ framework, c) there are various research efforts in\n   the general purpose\
    \ and loose information framework, and d) Whois++\n   employs a specific and loose\
    \ information framework.\n   We need to look at which parts of this spectrum we\
    \ need to provide\n   services.  This may lead to concluding that several services\
    \ are\n   desirable.\n"
- title: 3.D. WHAT SHOULD THE DEPLOYMENT STRATEGY BE?
  contents:
  - "3.D. WHAT SHOULD THE DEPLOYMENT STRATEGY BE?\n   No solution will arise simply\
    \ by providing technical specifications.\n   The solution must fit the way the\
    \ Internet adopts information\n   technology.  The information systems that have\
    \ gained real momentum\n   in the Internet (WAIS, Gopher, etc.) followed the model:\n\
    \   -- A small group goes off and builds a piece of software that\n      supplies\
    \ badly needed functionality at feasible effort to\n      providers and users.\n\
    \   -- The community rapidly adopts the system as a de facto standard.\n   --\
    \ Many people join the developers in improving the system and\n      standardizing\
    \ the protocols.\n   What can this report do to help make this happen for Internet\
    \ white\n   pages?\n   Deployment Issues.\n   -- A strict hierarchical layout\
    \ is not suitable for all directory\n      applications and hence we should not\
    \ force fit it.\n   -- A typical organization's hierarchical information itself\
    \ is often\n      proprietary; they may not want to divulge it to the outside\
    \ world.\n      It will always be true that Institutions (not just commercial)\n\
    \      will always have some information that they do not wish to display\n  \
    \    to the public in any directory.  This is especially true for\n      Institutions\
    \ that want to protect themselves from headhunters, and\n      sales personnel.\n\
    \   -- There is the problem of multiple directory service providers, but\n   \
    \   see NADF work on \"Naming Links\" and their \"CAN/KAN\" technology\n     \
    \ [7].\n      A more general approach such as using a knowledge server (or a set\n\
    \      of servers) might be better.  The knowledge servers would have to\n   \
    \   know about which server to contact for a given query and thus may\n      refer\
    \ to either service provider servers or directly to\n      institution-operated\
    \ servers.  The key problem is how to collect\n      the knowledge and keep it\
    \ up to date.  There are some questions\n      about the viability of \"naming\
    \ links\" without a protocol\n      modification.\n   -- Guidelines are needed\
    \ for methods of searching and using directory\n      information.\n   -- A registration\
    \ authority is needed to register names at various\n      levels of the hierarchy\
    \ to ensure uniqueness or adoption of the\n      civil naming structure as delineated\
    \ by the NADF.\n   It is true that deployment of X.500 has not seen exponential\
    \ growth\n   as have other popular services on the Internet.  But rather than\n\
    \   abandoning X.500 now, these efforts, which are attempting to address\n   some\
    \ of the causes, should continue to move forward.  Certainly\n   installation\
    \ complexity and performance problems with the quipu\n   implementation need solutions.\
    \  These problems are being worked on.\n   One concern with the X.500 service\
    \ has been the lack of ubiquitous\n   user agents.  Very few hosts run the ISODE\
    \ package.  The use of LDAP\n   improves this situation.  The X.500-gopher gateway\
    \ has had the\n   greatest impact on providing wide-spread access to the X.500\
    \ service.\n   Since adding X.500 as a service on the ESnet Gopher, the use of\
    \ the\n   ESnet DSA has risen dramatically.\n   Another serious problem affecting\
    \ the deployment of X.500, at least\n   in the U.S., is the minimal support given\
    \ to building and maintaining\n   the necessary infrastructure since the demise\
    \ of the Fox Project [9].\n   Without funding for this effort, X.500 may not stand\
    \ a chance in the\n   United States.\n"
- title: 4. REVIEW OF TECHNOLOGIES
  contents:
  - "4. REVIEW OF TECHNOLOGIES\n   There are now many systems for finding information,\
    \ some of these are\n   oriented to white pages, some include white pages, and\
    \ others\n   currently ignore white pages.  In any case, it makes sense to review\n\
    \   these systems to see how they might fit into the provision of an\n   Internet\
    \ white pages service.\n"
- title: 4.A. X.500
  contents:
  - "4.A. X.500\n   Several arguments in X.500's favor are its flexibility, distributed\n\
    \   architecture, security, superiority to paper directories, and that it\n  \
    \ can be used by applications as well as by humans.  X.500 is designed\n   to\
    \ provide a uniform database facility with replication,\n   modification, and\
    \ authorization.  Because it is distributed, it is\n   particularly suited for\
    \ a large global White Pages directory.  In\n   principle, it has good searching\
    \ capabilities, allowing searches at\n   any level or in any subtree of the DIT\
    \ (Directory Information Tree).\n   There are DUIs available for all types of\
    \ workstations and X.500 is\n   an international standard.  In theory, X.500 can\
    \ provide vastly\n   better directory service than other systems, however, in\
    \ practice,\n   X.500 is difficult, too complicated, and inconvenient to use.\
    \  It\n   should provide a better service.  X.500 is a technology that may be\n\
    \   used to provide a white pages service, although some features of\n   X.500\
    \ may not be needed to provide just a white pages service.\n   The are three reasons\
    \ X.500 deployment has been slow, and these are\n   largely the same reasons people\
    \ don't like it:\n   1) The available X.500 implementations (mostly quipu based\
    \ on the\n      ISODE) are very large and complicated software packages that are\n\
    \      hard to work with.  This is partly because they solve the general\n   \
    \   X.500 problem, rather than the subset needed to provide an\n      Internet\
    \ white pages directory.  In practice, this means that a\n      portion of the\
    \ code/complexity is effectively unused.\n      The LDAP work has virtually eliminated\
    \ this concern on the client\n      side of things, as LDAP is both simple and\
    \ lightweight.  Yet, the\n      complexity problem still exists on the server\
    \ side of things, so\n      people continue to have trouble bringing up data for\
    \ simple\n      clients to access.\n      It has been suggested that the complexity\
    \ in X.500 is due to the\n      protocol stack and the ISODE base.  If this is\
    \ true, then LDAP may\n      be simple because it uses TCP directly without the\
    \ ISODE base.  A\n      version of X.500 server that took the same approach might\
    \ also be\n      \"simple\" or at least simpler.  Furthermore, the difficulty\
    \ in\n      getting an X.500 server up may be related to finding the data to\n\
    \      put in the server, and so may be a general data management problem\n  \
    \    rather than an X.500 specific problem.\n      There is some evidence that\
    \ eventually a large percentage of the\n      use of directory services may be\
    \ from applications rather than\n      direct user queries.  For example, mail-user-agents\
    \ exist that are\n      X.500 capable with an integrated DUA (Directory User Agent).\n\
    \   2) You have to \"know a lot\" to get a directory service up and running\n\
    \      with X.500.  You have to know about object classes and attributes\n   \
    \   to get your data into X.500.  You have to get a distinguished name\n     \
    \ for your organization and come up with an internal tree structure.\n      You\
    \ have to contact someone before you can \"come online\" in the\n      pilot.\
    \  It's not like gopher where you type \"make\", tell a few\n      friends, and\
    \ you're up and running.\n      Note that a gopher server is not a white pages\
    \ service, and as\n      noted elsewhere in this report, there are a number of\
    \ issues that\n      apply to white pages service that are not addressed by gopher.\n\
    \      Some of these problems could be alleviated by putting in place\n      better\
    \ procedures.  It should not any be harder to get connected\n      to X.500 than\
    \ it is to get connected to the DNS, for example.\n      However, there is a certain\
    \ amount of complexity that may be\n      inherent in directory services.  Just\
    \ compare Whois++ and X.500.\n      X.500 has object classes.  Whois++ has templates.\
    \  X.500 has\n      attributes.  Whois++ has fields.  X.500 has distinguished\
    \ names.\n      Whois++ has handles.\n   3) Getting data to populate the directory,\
    \ converting it into the\n      proper form, and keeping it up-to-date turns out\
    \ to be a hard\n      problem.  Often this means talking to the administrative\
    \ computing\n      department at your organization.\n      This problem exists\
    \ regardless of the protocol used.  It should be\n      easy to access this data\
    \ through the protocol you're using, but\n      that says more about implementations\
    \ than it does about the\n      protocol.  Of course, if the only X.500 implementation\
    \ you have\n      makes it really hard to do, and the Whois++ implementation you\n\
    \      have makes it easy, it's hard for that not to reflect on the\n      protocols.\n\
    \   The fact that there are sites like University of Michigan, University\n  \
    \ of Minnesota, Rutgers University, NASA, LBL, etc. running X.500 in\n   serious\
    \ production mode shows that the problem has more to do with\n   the current state\
    \ of X.500 software procedures.  It takes a lot of\n   effort to get it going.\
    \  The level of effort required to keep it\n   going is relatively very small.\n\
    \   The yellow pages problem is not really a problem.  If you look at it\n   in\
    \ the traditional phonebook-style yellow pages way, then X.500 can\n   do the\
    \ job just like the phone book does.  Just organize the\n   directory based on\
    \ different (i.e., non-geographical) criteria.  If\n   you want to \"search everything\"\
    , then you need to prune the search\n   space.  To do this you can use the Whois++\
    \ centroids idea, or\n   something similar.  But this idea is as applicable to\
    \ X.500 as it is\n   to Whois++.  Maybe X.500 can use the centroids idea most\
    \ effectively.\n   Additionally, it should be noted that there is not one single\
    \ Yellow\n   Pages service, but that according to the type of query there could\
    \ be\n   several such as querying by role, by location, by email address.\n  \
    \ No one is failing to run X.500 because they perceive it fails to\n   solve the\
    \ yellow pages problem.  The reasons are more likely one or\n   more of the three\
    \ above.\n   X.500's extra complexity is paying off for University of Michigan.\n\
    \   University of Michigan started with just people information in their\n   tree.\
    \  Once that infrastructure was in place, it was easy for them to\n   add more\
    \ things to handle mailing lists/email groups, yellow pages\n   applications like\
    \ a documentation index, directory of images, etc.\n   The ESnet community is\
    \ using X.500 right now to provide a White Pages\n   service; users succeed everyday\
    \ in searching for information about\n   colleagues given only a name and an organizational\
    \ affiliation; and\n   yes, they do load data into X.500 from an Oracle database.\n\
    \   LBL finds X.500 very useful.  They can lookup DNS information, find\n   what\
    \ Zone a Macintosh is in, lookup departmental information, view\n   the current\
    \ weather satellite image, and lookup people information.\n   LDAP should remove\
    \ many of the complaints about X.500.  Implementing\n   a number of LDAP clients\
    \ is very easy and has all the functionality\n   needed.  Perhaps DAP should be\
    \ scrapped.\n   Another approach is the interfacing of X.500 servers to WWW (the\n\
    \   interface is sometimes called XWI).  Using the mosaic program from\n   the\
    \ NCSA, one can access X.500 data.\n   INTERNET X.500\n   The ISO/ITU may not\
    \ make progress on improving X.500 in the time\n   frame required for an Internet\
    \ white pages service.  One approach is\n   to have the Internet community (e.g.,\
    \ the IETF) take responsibility\n   for developing a subset or profile of that\
    \ part of X.500 it will use,\n   and developing solutions for the ambiguous and\
    \ undefined parts of\n   X.500 that are necessary to provide a complete service.\n\
    \   Tasks this approach might include are:\n   1. Internet (IETF) control of the\
    \ base of the core service white\n      pages infrastructure and standard.\n \
    \  2. Base the standard on the 1993 specification, especially\n      replication\
    \ and access control.\n   3. For early deployment choose which parts of the replication\n\
    \      protocol are really urgently needed.  It may be possible to define\n  \
    \    a subset and to make it mandatory for the Internet.\n   4. Define an easy\
    \ and stable API (Application Program Interface) for\n      key access protocols\
    \ (DAP, LDAP).\n   5. Use a standard knowledge model.\n   6. Make sure that high\
    \ performance implementations will exist for the\n      most important servers,\
    \ roles principally for the upper layers of\n      the DSA tree.\n   7. Make sure\
    \ that servers will exist that will be able to efficiently\n      get the objects\
    \ (or better the attributes) from existing\n      traditional databases for use\
    \ at the leaves of the DSA tree.\n"
- title: 4.B. WHOIS++
  contents:
  - "4.B. WHOIS++\n   The very first discussions of this protocol started in July\
    \ 1992.  In\n   less than 15 months there were 3 working public domain\n   implementations,\
    \ at least 3 more are on the way, and a Whois++\n   front-end to X.500.  In addition,\
    \ the developers who are working on\n   the resource location system infrastructure\
    \ (URL/URI) have committed\n   to implementing it on top of Whois++ because of\
    \ its superior search\n   capabilities.\n   Some of the main problems with getting\
    \ a White Pages directory going\n   have been: (1) search, (2) lack of public\
    \ domain versions, (3)\n   implementations are too large, (4) high start up cost,\
    \ and (5) the\n   implementations don't make a lot of sense for a local directory,\n\
    \   particularly for small organizations.  Whois++ can and does address\n   all\
    \ these problems very nicely.\n   Search is built into Whois++, and there is a\
    \ strong commitment from\n   the developers to keep this a high priority.\n  \
    \ The protocols are simple enough that someone can write a server in 3\n   days.\
    \  And people have done it.  If the protocols stay simple, it\n   will always\
    \ be easy for someone to whip out a new public domain\n   server.  In this respect,\
    \ Whois++ is much like WAIS or Gopher.\n   The typical Whois++ implementation\
    \ is about 10 megabytes, including\n   the WAIS source code that provides the\
    \ data engine.  Even assuming a\n   rough doubling of the code as additional necessary\
    \ functionality is\n   built in, that's still quite reasonable, and compares favorably\
    \ with\n   the available implementations of X.500.  In addition, WAIS is disk-\n\
    \   based from the start, and is optimized for local searching.  Thus,\n   this\
    \ requires only disk storage for the data and the indexes.  In a\n   recent test,\
    \ Chris Weider used a 5 megabyte source data file with the\n   Whois++ code. \
    \ The indices came to about another 7 megabytes, and the\n   code was under 10\
    \ megabytes.  The total is 22 megabytes for a Whois++\n   server.\n   The available\
    \ Whois++ implementations take about 25 minutes to\n   compile on a Sun SPARCstation\
    \ IPC.  Indexing a 5 megabyte data file\n   takes about another 20 minutes on\
    \ an IPC.  Installation is very easy.\n   In addition, since the Whois++ server\
    \ protocol is designed to be only\n   a front-end, organizations can keep their\
    \ data in any form they want.\n   Whois++ makes sense as a local directory service.\
    \  The\n   implementations are small, install quickly, and the raw query\n   language\
    \ is very simple.  The simplicity of the interaction between\n   the client and\
    \ the server make it easy to experiment with and to\n   write clients for, something\
    \ that wasn't true of X.500 until LDAP.\n   In addition, Whois++ can be run strictly\
    \ as a local service, with\n   integration into the global infrastructure done\
    \ at any time.\n   It is true that Whois++ is not yet a fully functional White\
    \ Pages\n   service.  It requires a lot of work before it will be so.  However,\n\
    \   X.500 is not that much closer to the goal than Whois++ is.\n   Work needs\
    \ to be done on replication and authentication of data.  The\n   current Whois++\
    \ system does not lend itself to delegation.  Research\n   is still needed to\
    \ improve the system and see if it scales well.\n"
- title: 4.C. NETFIND
  contents:
  - "4.C. NETFIND\n   Right now, the white pages service with the most coverage in\
    \ the\n   Internet is Mike Schwartz' Netfind.  Netfind works in two stages: 1)\n\
    \   find out where to ask, and 2) start asking.\n   The first stage is based on\
    \ a database of netnews articles, UUCP\n   maps, NIC WHOIS databases, and DNS\
    \ traversals, which then maps\n   organizations and localities to domain names.\
    \  The second stage\n   consists of finger queries, Whois queries, smtp expns\
    \ and vrfys, and\n   DNS lookups.\n   The key feature of Netfind is that it is\
    \ proactive.  It doesn't\n   require that the system administrator bring up a\
    \ new server, populate\n   it with all kinds of information, keep the information\
    \ in sync, worry\n   about update, etc.  It just works.\n   A suggestion was made\
    \ that Netfind could be used as a way to populate\n   the X.500 directory.  A\
    \ tool might do a series of Netfind queries,\n   making the corresponding X.500\
    \ entries as it progresses.\n   Essentially, X.500 entries would be \"discovered\"\
    \ as people look for\n   them using Netfind.  Others do not believe this is feasible.\n\
    \   Another perhaps less interesting merger of Netfind and X.500 is to\n   have\
    \ Netfind add X.500 as one of the places it looks to find\n   organizations (and\
    \ people).\n   A search can lead you to where a person has an account (e.g.,\n\
    \   law.xxx.edu) only to find a problem with the DNS services for that\n   domain,\
    \ or the finger service is unavailable, or the machines are not\n   be running\
    \ Unix (there are lots of VMS machines and IBM mainframes\n   still out there).\
    \  In addition, there are security gateways.  The\n   trends in computing are\
    \ towards the use of powerful portables and\n   mobile computing and hence Netfind's\
    \ approach may not work.  However,\n   Netfind proves to be an excellent yellow-pages\
    \ service for domain\n   information in DNS servers - given a set of keywords\
    \ it lists a set\n   of possible domain names.\n   Suppose we store a pointer\
    \ in DNS to a white-pages server for a\n   domain.  We can use Netfind to come\
    \ up with a list of servers to\n   search, query these servers, then combine the\
    \ responses.  However, we\n   need a formal method of gathering white-pages data\
    \ and informal\n   methods will not work and may even get into legal problems.\n\
    \   The user search phase of Netfind is a short-term solution to\n   providing\
    \ an Internet white pages.  For the longer term, the\n   applicability of the\
    \ site discovery part of Netfind is more relevant,\n   and more work has been\
    \ put into that part of the system over the past\n   2 years than into the user\
    \ search phase.\n   Given Netfind's \"installed customer base\" (25k queries per\
    \ day, users\n   in 4875 domains in 54 countries), one approach that might make\
    \ sense\n   is to use Netfind as a migration path to a better directory, and\n\
    \   gradually phase Netfind's user search scheme out of existence.  The\n   idea\
    \ of putting a record in the DNS to point to the directory service\n   to search\
    \ at a site is a good start.\n   One idea for further development is to have the\
    \ DNS record point to a\n   \"customization\" server that a site can install to\
    \ tailor the way\n   Netfind (or whatever replaces Netfind) searches their site.\
    \  This\n   would provide sites a choice of degrees of effort and levels of\n\
    \   service.  The least common denominator is what Netfind presently\n   does:\
    \ DNS/SMTP/finger.  A site could upgrade by installing a\n   customization server\
    \ that points to the best hosts to finger, or that\n   says \"we don't want Netfind\
    \ to search here\" (if people are\n   sufficiently concerned about the legal/privacy\
    \ issues, the default\n   could be changed so that searches must be explicitly\
    \ enabled).  The\n   next step up is to use the customization server as a gateway\
    \ to a\n   local Whois, CSO, X.500, or home grown white pages server.  In the\n\
    \   long run, if X.500 (or Whois++, etc.) really catches on, it could\n   subsume\
    \ the site indexing part of Netfind and use the above approach\n   as an evolution\
    \ path to full X.500 deployment.  However, other\n   approaches may be more productive.\
    \  One key to Netfind's success has\n   been not relying on organizations to do\
    \ anything to support Netfind,\n   however the customization server breaks this\
    \ model.\n   Netfind is very useful.  Users don't have to do anything to wherever\n\
    \   they store their people data to have it \"included\" in Netfind.  But\n  \
    \ just like archie, it would be more useful if there were a more common\n   structure\
    \ to the information it gives you, and therefore to the\n   information contained\
    \ in the databases it accesses.  It's this common\n   structure that we should\
    \ be encouraging people to move toward.\n   As a result of suggestions made at\
    \ the November meeting, Netfind has\n   been extended to make use of URL information\
    \ stored in DNS records.\n   Based on this mechanism, Netfind can now interoperate\
    \ with X.500,\n   WHOIS, and PH, and can also allow sites to tune which hosts\
    \ Netfind\n   uses for SMTP or Finger, or restrict Netfind from searching their\n\
    \   site entirely.\n"
- title: 4.D. ARCHIE
  contents:
  - "4.D. ARCHIE\n   Archie is a success because it is a directory of files that are\n\
    \   accessible over the network.  Every FTP site makes a \"conscious\"\n   decision\
    \ to make the files available for anonymous FTP over the\n   network.  The mechanism\
    \ that archie uses to gather the data is the\n   same as that used to transfer\
    \ the files.  Thus, the success rate is\n   near 100%.  In a similar vein, if\
    \ Internet sites make a \"conscious\"\n   decision to make white-pages data available\
    \ over the network, it is\n   possible to link these servers to create a world-wide\
    \ directory, such\n   as X.500, or build an index that helps to isolate the servers\
    \ to be\n   searched, Whois++.  Users don't have to do anything to their FTP\n\
    \   archives to have them included in archie.  But everybody recognizes\n   that\
    \ it could be more useful if only there were some more common\n   structure to\
    \ the information, and to the information contained in the\n   archives.  Archie\
    \ came after the anonymous FTP sites were in wide-\n   spread use.  Unfortunately\
    \ for white-pages, we are building tools,\n   but there is no data.\n"
- title: 4.E. FINGER
  contents:
  - "4.E. FINGER\n   The Finger program that allows one to get either information\
    \ about an\n   individual with an account, or a list of currently logged in users,\n\
    \   from a host running the server, can be used to check a suggestion\n   that\
    \ a particular individual has an account on a particular host.\n   This does not\
    \ provide an efficient method to search for an\n   individual.\n"
- title: 4.F. GOPHER
  contents:
  - "4.F. GOPHER\n   A \"gateway\" between Gopher and X.500 has been created so that\
    \ one can\n   examine X.500 data from a Gopher client.  Similar \"gateways\" are\n\
    \   needed for other white pages systems.\n"
- title: 4.G. WWW
  contents:
  - "4.G. WWW\n   One extension to WWW would be an attribute type for the WWW URI/URL\n\
    \   with the possibility for any client to request from the X.500 server\n   (1)\
    \ either the locator (thus the client would decide to access or not\n   the actual\
    \ data), or (2) for client not capable of accessing this\n   data, the data itself\
    \ (packed) in the ASN.1 encoded result.\n   This would give access to potentially\
    \ any piece of information\n   available on the network through X.500, and in\
    \ the white pages case\n   to photos or voice messages for persons.\n   This solution\
    \ is preferable to one consisting of storing this\n   multimedia information directly\
    \ in the directory, because it allows\n   WWW capable DUIs to access directly\
    \ any piece of data no matter how\n   large.  This work on URIs is not WWW-specific.\n"
- title: 5. ISSUES
  contents:
  - '5. ISSUES

    '
- title: 5.A. DATA PROTECTION
  contents:
  - "5.A. DATA PROTECTION\n   Outside of the U.S., nearly all developed countries\
    \ have rather\n   strict data protection acts (to ensure privacy mostly) that\
    \ governs\n   any database on personal data.\n   It is mandatory for the people\
    \ in charge of such white pages\n   databases to have full control over the information\
    \ that can be\n   stored and retrieved in such a database, and to provide access\n\
    \   controls over the information that is made available.\n   If modification\
    \ is allowed, then authentication is required.  The\n   database manager must\
    \ be able to prevent users from making available\n   unallowed information.\n\
    \   When we are dealing with personal records the issues are a little\n   more\
    \ involved than exporting files.  We can not allow trawling of\n   data and we\
    \ need access-controls so that several applications can use\n   the directory\
    \ and hence we need authentication.\n   X.500 might have developed faster if security\
    \ issues were not part of\n   the implementation.  There is tension between quick\
    \ lightweight\n   implementations and the attempt to operate in a larger environment\n\
    \   with business issues incorporated.  The initial belief was that data\n   is\
    \ owned by the people who put the data into the system, however,\n   most data\
    \ protection laws appoint the organizations holding the data\n   responsible for\
    \ the quality of the data of their individuals.\n   Experience also shows that\
    \ the people most affected by inaccurate\n   data are the people who are trying\
    \ to access the data.  These\n   problems apply to all technologies.\n"
- title: 5.B. STANDARDS
  contents:
  - "5.B. STANDARDS\n   Several types of standards are needed: (1) standards for\n\
    \   interoperation between different white pages systems (e.g., X.500 and\n  \
    \ Whois++), (2) standards for naming conventions, and (3) and standards\n   within\
    \ the structured data of each system (what fields or attributes\n   are required\
    \ and optional, and what are their data types).\n   The standards for interoperation\
    \ may be developed from the work now\n   in progress on URLs, with some additional\
    \ protocol developed to\n   govern the types of messages and message sequences.\n\
    \   Both the naming of the systems and the naming of individuals would\n   benefit\
    \ from consistent naming conventions.  The use of the NADF\n   naming scheme should\
    \ be considered.\n   When structured data is exchanged, standards are needed for\
    \ the data\n   types and the structural organization.  In X.500, much effort has\n\
    \   gone into the definition of various structures or schemas, and yet\n   few\
    \ standard schemas have emerged.\n   There is a general consensus that a \"cookbook\"\
    \ for Administrators\n   would make X.500 implementation easier and more attractive.\
    \  These\n   are essential for getting X.500 in wider use.  It is also essential\n\
    \   that other technologies such as Whois++, Netfind, and archie also\n   have\
    \ complete user guides available.\n"
- title: 5.C. SEARCHING AND RETRIEVING
  contents:
  - "5.C. SEARCHING AND RETRIEVING\n   The main complaint, especially from those who\
    \ enjoyed using a\n   centralized database (such as the InterNIC Whois service),\
    \ is the\n   need to search for all the John Doe's in the world.  Given that the\n\
    \   directory needs to be distributed, there is no way of answering this\n   question\
    \ without incurring additional cost.\n   This is a problem with any distributed\
    \ directory - you just can't\n   search every leaf in the tree in any reasonable\
    \ amount of time.  You\n   need to provide some mechanism to limit the number\
    \ of servers that\n   need to be contacted.  The traditional way to handle this\
    \ is with\n   hierarchy.  This requires the searcher to have some idea of the\n\
    \   structure of the directory.  It also comes up against one of the\n   standard\
    \ problems with hierarchical databases - if you need to search\n   based on a\
    \ characteristic that is NOT part of the hierarchy, you are\n   back to searching\
    \ every node in the tree, or you can search an index\n   (see below).\n   In general:\n\
    \   -- the larger the directory the more need for a distributed solution\n   \
    \   (for upkeep and managability).\n   -- once you are distributed, the search\
    \ space for any given search\n      MUST be limited.\n   -- this makes it necessary\
    \ to provide more information as part of the\n      query (and thus makes the\
    \ directory harder to use).\n   Any directory system can be used in a manner that\
    \ makes searching\n   less than easy.  With a User Friendly Name (UFN) query,\
    \ a user can\n   usually find an entry (presuming it exists) without a lot of\
    \ trouble.\n   Using additional listings (as per NADF SD-5) helps to hide geographic\n\
    \   or civil naming infrastructure knowledge requirements.\n   Search power is\
    \ a function of DSA design in X.500, not a function of\n   Distinguished Naming.\
    \  Search can be aided by addition in X.500 of\n   non-distinguishing attributes,\
    \ and by using the NADF Naming Scheme it\n   is possible to lodge an entry anywhere\
    \ in the DIT that you believe is\n   where it will be looked for.\n   One approach\
    \ to the distributed search problem is to create another\n   less distributed\
    \ database to search, such as an index.  This is done\n   by doing a (non-interactive)\
    \ pre-search, and collecting the results\n   in an index.  When a user wants to\
    \ do a real time search, one first\n   searches the index to find pointers to\
    \ the appropriate data records\n   in the distributed database.  One example of\
    \ this is the building of\n   centroids that contain index information.  There\
    \ may be a class of\n   servers that hold indices, called \"index-servers\".\n"
- title: 5.D. INDEXING
  contents:
  - "5.D. INDEXING\n   The suggestion for how to do fast searching is to do indexing.\
    \  That\n   is to pre-compute an index of people from across the distributed\n\
    \   database and hold that index in an index server.  When a user wants\n   to\
    \ search for someone, he first contacts the index-server.  The\n   index-server\
    \ searches its index data and returns a pointer (or a few\n   pointers) to specific\
    \ databases that hold data on people that match\n   the search criteria.  Other\
    \ systems which do something comparable to\n   this are archie (for FTP file archives),\
    \ WAIS, and Netfind.\n"
- title: 5.E. COLLECTION AND MAINTENANCE
  contents:
  - "5.E. COLLECTION AND MAINTENANCE\n   The information must be \"live\" - that is,\
    \ it must be used.  Often one\n   way to ensure this is to use the data (perhaps\
    \ locally) for something\n   other than white pages.  If it isn't, most people\
    \ won't bother to\n   keep the information up to date.  The white pages in the\
    \ phone book\n   have the advantage that the local phone company is in contact\
    \ with\n   the listee monthly (through the billing system), and if the address\n\
    \   is not up to date, bills don't get delivered, and there is feedback\n   that\
    \ the address is wrong.  There is even better contact for the\n   phone number,\
    \ since the local phone company must know that for their\n   basic service to\
    \ work properly.  It is this aspect of directory\n   functionality that leads\
    \ towards a distributed directory system for\n   the Internet.\n   One approach\
    \ is to use existing databases to supply the white pages\n   data.  It then would\
    \ be helpful to define a particular use of SQL\n   (Structured Query Language)\
    \ as a standard interface language between\n   the databases and the X.500 DSA\
    \ or other white pages server.  Then\n   one needs either to have the directory\
    \ service access the existing\n   database using an interface language it already\
    \ knows (e.g., SQL), or\n   to have tools that periodically update the directory\
    \ database from\n   the existing database.  Some sort of \"standard\" query format\
    \ (and\n   protocol) for directory queries, with \"standard\" field names will\
    \ be\n   needed to make this work in general.  In a way, both X.500 and\n   Whois++\
    \ provide this.  This approach implies customization at every\n   existing database\
    \ to interface to the \"standard\" query format.\n   Some strongly believe that\
    \ the white pages service needs to be\n   created from the bottom up with each\
    \ organization supplying and\n   maintaining its own information, and that such\
    \ information has to be\n   the same -- or a portion of the same -- information\
    \ the organization\n   uses locally.  Otherwise the global information will be\
    \ stale and\n   incomplete.\n   One way to make this work is to distribute software\
    \ that:\n      - is useful locally,\n      - fits into the global scheme,\n  \
    \    - is available free, and\n      - works on most Unix systems.\n   With respect\
    \ to privacy, it would be good for the local software to\n   have controls that\
    \ make it possible to put company sensitive\n   information into the locally maintained\
    \ directory and have only a\n   portion of it exported for outsiders.\n"
- title: 5.F. NAMING STRUCTURE
  contents:
  - "5.F. NAMING STRUCTURE\n   We need a clear naming scheme capable of associating\
    \ a name with\n   attributes, without any possible ambiguities, that is stable\
    \ over\n   time, but also capable of coping with changes.  This scheme should\n\
    \   have a clear idea of naming authorities and be able to store\n   information\
    \ required by authentication mechanisms (e.g., PEM or X.509\n   certificates).\n\
    \   The NADF is working to establish a National Public Directory Service,\n  \
    \ based on the use of existing Civil Naming Authorities to register\n   entry\
    \ owners' names, and to deal with the shared-entry problem with a\n   shared public\
    \ DIT supported by competing commercial service\n   providers.  At this point,\
    \ we do not have any sense at the moment as\n   to how [un]successful the NADF\
    \ may be in accomplishing this.\n   The NADF eventually concluded that the directory\
    \ should be organized\n   so entries can be found where people (or other entities)\
    \ will look\n   for them, not where civil naming authorities would place their\n\
    \   archival name registration records.\n   There are some incompatibilities between\
    \ use of the NADF Naming\n   Scheme, the White Pages Pilot Naming Scheme, and\
    \ the PARADISE Naming\n   Scheme.  This should be resolved.\n"
- title: 5.G. CLAYMAN PROPOSAL
  contents:
  - "5.G. CLAYMAN PROPOSAL\n   RFC 1107 offered a \"strawman\" proposal for an Internet\
    \ Directory\n   Service.  The next step after strawman is sometimes called \"\
    clayman\",\n   and here a clayman proposal is presented.\n   We assume only white\
    \ pages service is to be provided, and we let\n   sites run whatever access technologies\
    \ they want to (with whatever\n   access controls they feel comfortable).\n  \
    \ Then the architecture can be that the discovery process leads to a\n   set of\
    \ URLs.  A URL is like an address, but it is a typed address\n   with identifiers,\
    \ access method, not a protocol.  The client sorts\n   the URLs and may discard\
    \ some that it cannot deal with.  The client\n   talks to \"meaningful URLs\"\
    \ (such as Whois, Finger, X.500).\n   This approach results in low entry cost\
    \ for the servers that want to\n   make information available, a Darwinian selection\
    \ of access\n   technologies, coalescence in the Internet marketplace, and a white\n\
    \   pages service will tend toward homogeneity and ubiquity.\n   Some issues for\
    \ further study are what discovery technology to use\n   (Netfind together with\
    \ Whois++ including centroids?), how to handle\n   non-standard URLs (one possible\
    \ solution is to put server on top of\n   these (non-standard URLs) which reevaluates\
    \ the pointer and acts as a\n   front-end to a database), which data model to\
    \ use (Finger or X.500),\n   and how to utilize a common discovery technology\
    \ (e.g., centroids) in\n   a multiprotocol communication architecture.\n   The\
    \ rationale for this meta-WPS approach is that it builds on current\n   practices,\
    \ while striving to provide a ubiquitous directory service.\n   Since there are\
    \ various efforts going on to develop WPS based on\n   various different protocols,\
    \ one can envisage a future with a meta-\n   WPS that uses a combination of an\
    \ intelligent user agent and a\n   distributed indexing service to access the\
    \ requested data from any\n   available WPS.  The user perceived functionality\
    \ of such a meta-WPS\n   will necessarily be restricted to the lowest common denominator.\
    \  One\n   will hope that through \"market\" forces, the number of protocols used\n\
    \   will decrease (or converge), and that the functionality will\n   increase.\n\
    \   The degree to which proactive data gathering is permitted may be\n   limited\
    \ by national laws.  It may be appropriate to gather data about\n   which hosts\
    \ have databases, but not about the data in those\n   databases.\n"
- title: 6. CONCLUSIONS
  contents:
  - "6. CONCLUSIONS\n   We now revisit the questions we set out to answer and briefly\n\
    \   describe the key conclusions.\n"
- title: 6.A.  WHAT FUNCTIONS SHOULD A WHITE PAGES DIRECTORY PERFORM?
  contents:
  - "6.A.  WHAT FUNCTIONS SHOULD A WHITE PAGES DIRECTORY PERFORM?\n   After all the\
    \ discussion we come to the conclusion that there are two\n   functions the white\
    \ pages service must provide: searching and\n   retrieving.\n   Searching is the\
    \ ability to find people given some fuzzy information\n   about them.  Such as\
    \ \"Find the Postel in southern California\".\n   Searches may often return a\
    \ list of matches.\n   The recognition of the importance of indexing in searching\
    \ is a major\n   conclusion of these discussions.  It is clear that users want\
    \ fast\n   searching across the distributed database on attributes different\n\
    \   from the database structure.  It is possible that pre-computed\n   indices\
    \ can satisfy this desire.\n   Retrieval is obtaining additional information associated\
    \ with a\n   person, such as address, telephone number, email mailbox, and\n \
    \  security certificate.\n   This last, security certificates, is a type of information\
    \ associated\n   with an individual that is essential for the use of end-to-end\n\
    \   authentication, integrity, and privacy, in Internet applications.\n   The\
    \ development of secure application in the Internet is dependent on\n   a directory\
    \ system for retrieving the security certificate associated\n   with an individual.\
    \  The PEM system has been developed and is ready\n   to go into service, but\
    \ is now held back by the lack of an easily\n   used directory of security certificates.\n\
    \   PEM security certificates are part of the X.509 standard.  If X.500\n   is\
    \ going to be set aside, then other alternatives need to be\n   explored.  If\
    \ X.500 distinguished naming is scrapped, some other\n   structure will need to\
    \ come into existence to replace it.\n"
- title: 6.B.  WHAT APPROACHES WILL PROVIDE US WITH A WHITE PAGES DIRECTORY?
  contents:
  - "6.B.  WHAT APPROACHES WILL PROVIDE US WITH A WHITE PAGES DIRECTORY?\n   It is\
    \ clear that there will be several technologies in use.  The\n   approach must\
    \ be to promote the interoperation of the multiple\n   technologies.  This is\
    \ traditionally done by having conventions or\n   standards for the interfaces\
    \ and communication forms between the\n   different systems.  The need is for\
    \ a specification of the simplest\n   common communication form that is powerful\
    \ enough to provide the\n   necessary functionality.  This allows a variety of\
    \ user interfaces on\n   any number of client systems communicating with different\
    \ types of\n   servers.  The IETF working group (WG) method of developing standards\n\
    \   seems well suited to this problem.\n   This \"common ground\" approach aims\
    \ to provide the ubiquitous WPS with\n   a high functionality and a low entry\
    \ cost.  This may done by singling\n   out issues that are common for various\
    \ competing WPS and coordinate\n   work on these in specific and dedicated IETF\
    \ WGs (e.g., data model\n   coordination).  The IETF will continue development\
    \ of X.500 and\n   Whois++ as two separate entities.  The work on these two protocols\n\
    \   will be broken down in various small and focussed WGs that address\n   specific\
    \ technical issues, using ideas from both X.500 and Whois++.\n   The goal being\
    \ to produce common standards for information formats,\n   data model and access\
    \ protocols.  Where possible the results of such\n   a WG will be used in both\
    \ Whois++ and X.500, although it is envisaged\n   that several WGs may work on\
    \ issues that remain specific to one of\n   the protocols.  The IDS (Integrated\
    \ Directory Services) WG continues\n   to work on non-protocol specific issues.\
    \  To achieve coordination\n   that leads to convergence rather than divergence,\
    \ the applications\n   area directorate will provide guidance to the Application\
    \ Area\n   Directors as well as to the various WGs, and the User Services Area\n\
    \   Council (USAC) will provide the necessary user perspective.\n"
- title: 6.C.  WHAT ARE THE PROBLEMS TO BE OVERCOME?
  contents:
  - "6.C.  WHAT ARE THE PROBLEMS TO BE OVERCOME?\n   There are several problems that\
    \ can be solved to make progress\n   towards a white pages service more rapid.\
    \  We need:\n   To make it much easier to be part of the Internet white pages\
    \ than\n   bringing up a X.500 DSA, yet making good use of the already deployed\n\
    \   X.500 DSAs.\n   To define new simpler white pages services (such as Whois++)\
    \ such\n   that numerous people can create implementations.\n   To provide some\
    \ central management of the X.500 system to promote\n   good operation.\n   To\
    \ select a naming scheme.\n   To develop a set of index-servers, and indexing\
    \ techniques, to\n   provide for fast searching.\n   To provide for the storage\
    \ and retrieval of security certificates.\n"
- title: 6.D.  WHAT SHOULD THE DEPLOYMENT STRATEGY BE?
  contents:
  - "6.D.  WHAT SHOULD THE DEPLOYMENT STRATEGY BE?\n   We should capitalize on the\
    \ existing infrastructure of already\n   deployed X.500 DSAs.  This means that\
    \ some central management must be\n   provided, and easy to use user interfaces\
    \ (such as the Gopher\n   \"gateway\"), must be widely deployed.\n   -- Document\
    \ the selection of a naming scheme (e.g., the NADF scheme).\n   -- Adopt the \"\
    common ground\" model.  Encourage the development of\n      several different\
    \ services, with a goal of interworking between\n      them.\n   -- Develop a\
    \ specification of the simplest common communication form\n      that is powerful\
    \ enough to provide the necessary functionality.\n      The IETF working group\
    \ method of developing standards seems well\n      suited to this problem.\n \
    \  -- Make available information about how to set up new servers (of\n      what\
    \ ever kind) in \"cookbook\" form.\n"
- title: 7. SUMMARY
  contents:
  - "7. SUMMARY\n   While many issues have been raised, there are just a few where\
    \ we\n   recommend the action be taken to support specific elements of the\n \
    \  overall white pages system.\n   RECOMMENDATIONS\n    1.  Adopt the common ground\
    \ approach - give all protocols equal\n        access to all data.  That is, encourage\
    \ multiple client and\n        server types, and the standardization of an interoperation\n\
    \        protocol between them.  The clients may be simple clients,\n        front-ends,\
    \ \"gateways\", or embedded in other information access\n        clients, such\
    \ as Gopher or WWW client programs.  The\n        interoperation protocol will\
    \ define some message types, message\n        sequences, and data fields.   An\
    \ element of this protocol should\n        be the use of URLs.\n    2.  Promote\
    \ the development of index-servers.  The index-servers\n        should use several\
    \ different methods of gathering data for their\n        indices, and several\
    \ different methods for searching their\n        indices.\n    3.  Support a central\
    \ management for the X.500 system.  To get the\n        best advantage of the\
    \ effort already invested in the X.500\n        directory system it is essential\
    \ to provide the relatively small\n        amount of central management necessary\
    \ to keep the system\n        functioning.\n    4.  Support the development of\
    \ security certificate storage and\n        retrieval from the white pages service.\
    \  The most practical\n        approach is to initially focus on getting this\
    \ supported by the\n        existing X.500 directory infrastructure.  It should\
    \ also include\n        design and development of the storage and retrieval of\
    \ security\n        certificates in other white pages services, such as Whois++.\n"
- title: 8.  REFERENCES
  contents:
  - "8.  REFERENCES\n   [1]  Sollins, K., \"Plan for Internet Directory Services\"\
    , RFC 1107,\n        M.I.T. Laboratory for Computer Science, July 1989.\n   [2]\
    \  Hardcastle-Kille, S., \"Replication Requirements to provide an\n        Internet\
    \ Directory using X.500, RFC 1275, University College\n        London, November\
    \ 1991.\n   [3]  Weider, C., and J. Reynolds, \"Executive Introduction to\n  \
    \      Directory Services Using the X.500 Protocol\", FYI 13, RFC 1308,\n    \
    \    ANS, USC/Information Sciences Institute, March 1992.\n   [4]  Weider, C.,\
    \ Reynolds, J., and S. Heker, \"Technical Overview of\n        Directory Services\
    \ Using the X.500 Protocol\", FYI 14, RFC 1309,\n        ANS, USC/Information\
    \ Sciences Institute,, JvNC, March 1992.\n   [5]  Hardcastle-Kille, S., Huizer,\
    \ E., Cerf, V., Hobby, R., and S.\n        Kent, \"A Strategic Plan for Deploying\
    \ an Internet X.500\n        Directory Service\", RFC 1430, ISODE Consortium,\
    \ SURFnet bv,\n        Corporation for National Research Initiatives, University\
    \ of\n        California, Davis, Bolt, Beranek, and Newman, February 1993.\n \
    \  [6]  Jurg, A., \"Introduction to White pages services based on X.500\",\n \
    \       Work in Progress, October 1993.\n   [7]  The North American Directory\
    \ Forum, \"NADF Standing Documents: A\n        Brief Overview\", RFC 1417, The\
    \ North American Directory Forum\",\n        NADF, February 1993.\n   [8]  NADF,\
    \ An X.500 Naming Scheme for National DIT Subtrees and its\n        Application\
    \ for c=CA and c=US\", Standing Document 5 (SD-5).\n   [9]  Garcia-Luna, J., Knopper,\
    \ M., Lang, R., Schoffstall, M.,\n        Schraeder, W., Weider, C., Yeong, W,\
    \ Anderson, C., (ed.), and J.\n        Postel (ed.), \"Research in Directory Services:\
    \ Fielding\n        Operational X.500 (FOX)\", Fox Project Final Report, January\n\
    \        1992.\n"
- title: 9. GLOSSARY
  contents:
  - "9. GLOSSARY\n      API - Application Program Interface\n      COTS - commercial\
    \ off the shelf\n      CSO - a phonebook service developed by University of Illinois\n\
    \      DAP - Direct Access Protocol\n      DIT - Directory Information Tree\n\
    \      DNS - Domain Name System\n      DUI - Directory User Interface\n      DUA\
    \ - Directory User Agent\n      DSA - Directory Service Agent\n      FOX - Fielding\
    \ Operational X.500 project\n      FRICC - Federal Research Internet Coordinating\
    \ Committee\n      IETF - Internet Engineering Task Force\n      ISODE - ISO Development\
    \ Environment\n      LDAP - Lightweight Direct Access Protocol\n      NADF - North\
    \ American Directory Forum\n      PEM - Privacy Enhanced Mail\n      PSI - Performance\
    \ Systems International\n      SQL - Structured Query Language\n      QUIPU -\
    \ an X.500 DSA which is a component of the ISODE package\n      UFN - User Friendly\
    \ Name\n      URI - Uniform Resource Identifier\n      URL - Uniform Resource\
    \ Locator\n      WAIS - Wide Area Information Server\n      WPS - White Pages\
    \ Service\n      WWW - World Wide Web\n"
- title: 9.  ACKNOWLEDGMENTS
  contents:
  - "9.  ACKNOWLEDGMENTS\n   This report is assembled from the words of the following\
    \ participants\n   in the email discussion and the meeting.  The authors are responsible\n\
    \   for selecting and combining the material.  Credit for all the good\n   ideas\
    \ goes to the participants.  Any bad ideas are the responsibility\n   of the authors.\n\
    \      Allan Cargille                  University of Wisconsin\n      Steve Crocker\
    \                   TIS\n      Peter Deutsch                   BUNYIP\n      Peter\
    \ Ford                      LANL\n      Jim Galvin                      TIS\n\
    \      Joan Gargano                    UC Davis\n      Arlene Getchell       \
    \          ES.NET\n      Rick Huber                      INTERNIC - AT&T\n   \
    \   Christian Huitema               INRIA\n      Erik Huizer                 \
    \    SURFNET\n      Tim Howes                       University of Michigan\n \
    \     Steve Kent                      BBN\n      Steve Kille                 \
    \    ISODE Consortium\n      Mark Kosters                    INTERNIC - Network\
    \ Solutions\n      Paul Mockapetris                ARPA\n      Paul-Andre Pays\
    \                 INRIA\n      Dave Piscitello                 BELLCORE\n    \
    \  Marshall Rose                   Dover Beach Consulting\n      Sri Sataluri\
    \                    INTERNIC - AT&T\n      Mike Schwartz                   University\
    \ of Colorado\n      David Staudt                    NSF\n      Einar Stefferud\
    \                 NMA\n      Chris Weider                    MERIT\n      Scott\
    \ Williamson                INTERNIC - Network Solutions\n      Russ Wright  \
    \                   LBL\n      Peter Yee                       NASA\n"
- title: 10.  SECURITY CONSIDERATIONS
  contents:
  - "10.  SECURITY CONSIDERATIONS\n   While there are comments in this memo about\
    \ privacy and security,\n   there is no serious analysis of security considerations\
    \ for a white\n   pages or directory service in this memo.\n"
- title: 11.  AUTHORS' ADDRESSES
  contents:
  - "11.  AUTHORS' ADDRESSES\n   Jon Postel\n   USC/Information Sciences Institute\n\
    \   4676 Admiralty Way\n   Marina del Rey, CA 90292\n   Phone: 310-822-1511\n\
    \   Fax:   310-823-6714\n   EMail: Postel@ISI.EDU\n   Celeste Anderson\n   USC/Information\
    \ Sciences Institute\n   4676 Admiralty Way\n   Marina del Rey, CA 90292\n   Phone:\
    \ 310-822-1511\n   Fax:   310-823-6714\n   EMail: Celeste@ISI.EDU\n"
- title: APPENDIX 1
  contents:
  - "APPENDIX 1\n   The following White Pages Functionality List was developed by\
    \ Chris\n   Weider and amended by participants in the current discussion of an\n\
    \   Internet white pages service.\n   Functionality list for a White Pages / Directory\
    \ services\n   Serving information on People only\n   1.1 Protocol Requirements\n\
    \      a) Distributability\n      b) Security\n      c) Searchability and easy\
    \ navigation\n      d) Reliability (in particular, replication)\n      e) Ability\
    \ to serve the information desired (in particular,\n         multi-media information)\n\
    \      f) Obvious benefits to encourage installation\n      g) Protocol support\
    \ for maintenance of data and 'knowledge'\n      h) Ability to support machine\
    \ use of the data\n      i) Must be based on Open Standards and respond rapidly\
    \ to correct\n         deficiencies\n      j) Serve new types of information (not\
    \ initially planned) only\n         only upon request\n      k) Allow different\
    \ operation modes\n   1.2 Implementation Requirements\n      a) Searchability\
    \ and easy navigation\n      b) An obvious and fairly painless upgrade path for\
    \ organizations\n      c) Obvious benefits to encourage installation\n      d)\
    \ Ubiquitous clients\n      e) Clients that can do exhaustive search and/or cache\
    \ useful\n         information and use heuristics to narrow the search space in\n\
    \         case of ill-formed queries\n      f) Ability to support machine use\
    \ of the data\n      g) Stable APIs\n   1.3 Sociological Requirements\n      a)\
    \ Shallow learning curve for novice users (both client and\n         server)\n\
    \      b) Public domain servers and clients to encourage experimentation\n   \
    \   c) Easy techniques for maintaining data, to encourage users to\n         keep\
    \ their data up-to-date\n      d) (particularly for organizations) The ability\
    \ to hide an\n         organization's internal structure while making the data\
    \ public.\n      e) Widely recognized authorities to guarantee unique naming during\n\
    \         registrations (This is specifically X.500 centric)\n      f) The ability\
    \ to support the privacy / legal requirements of all\n         participants while\
    \ still being able to achieve good coverage.\n      g) Supportable infrastructure\
    \ (Perhaps an identification of what\n         infrastructure support requires\
    \ and how that will be\n         maintained)\n   Although the original focus of\
    \ this discussion was on White Pages,\n   many participants believe that a Yellow\
    \ Pages service should be built\n   into a White Pages scheme.\n   Functionality\
    \ List for Yellow Pages service\n   Yellow pages services, with data primarily\
    \ on people\n   2.1 Protocol Requirements\n      a) all listed in 1.1\n      b)\
    \ Very good searching, perhaps with semantic support OR\n      b2) Protocol support\
    \ for easy selection of proper keywords to\n         allow searching\n      c)\
    \ Ways to easily update and maintain the information required by\n         the\
    \ Yellow Pages services\n      d) Ability to set up specific servers for specific\
    \ applications or\n         a family of applications while still working with\
    \ the WP\n         information bases\n   2.2 Implementation Requirements\n   \
    \   a) All listed in 1.2\n      b) Server or client support for relevance feedback\n\
    \   2.3 Sociological Requirements\n      a) all listed in 1.3\n   Advanced directory\
    \ services for resource location (not just people\n   data)\n   3.1 Protocol Requirements\n\
    \      a) All listed in 2.1\n      b) Ability to track very rapidly changing data\n\
    \      c) Extremely good and rapid search techniques\n   3.2 Implementation Requirements\n\
    \      a) All listed in 2.2\n      b) Ability to integrate well with retrieval\
    \ systems\n      c) Speed, Speed, Speed\n   3.3 Sociological Requirements\n  \
    \    a) All listed in 1.3\n      b) Protocol support for 'explain' functions:\
    \ 'Why didn't this\n         query work?'\n"
