- title: __initial_text__
  contents:
  - '        Overview and Principles of Internet Traffic Engineering

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2002).  All Rights Reserved.\n"
- title: Abstract
  contents:
  - "Abstract\n   This memo describes the principles of Traffic Engineering (TE) in\
    \ the\n   Internet.  The document is intended to promote better understanding\n\
    \   of the issues surrounding traffic engineering in IP networks, and to\n   provide\
    \ a common basis for the development of traffic engineering\n   capabilities for\
    \ the Internet.  The principles, architectures, and\n   methodologies for performance\
    \ evaluation and performance optimization\n   of operational IP networks are discussed\
    \ throughout this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   3.0 Traffic Engineering Process Model.............................21\n\
    \      3.1 Components of the Traffic Engineering Process Model........23\n   \
    \   3.2 Measurement................................................23\n      3.3\
    \ Modeling, Analysis, and Simulation.........................24\n      3.4 Optimization...............................................25\n\
    \   4.0 Historical Review and Recent Developments.....................26\n   \
    \   4.1 Traffic Engineering in Classical Telephone Networks........26\n      4.2\
    \ Evolution of Traffic Engineering in the Internet...........28\n         4.2.1\
    \ Adaptive Routing in ARPANET...........................28\n         4.2.2 Dynamic\
    \ Routing in the Internet.......................29\n         4.2.3 ToS Routing...........................................30\n\
    \         4.2.4 Equal Cost Multi-Path.................................30\n   \
    \      4.2.5 Nimrod................................................31\n      4.3\
    \ Overlay Model..............................................31\n      4.4 Constraint-Based\
    \ Routing...................................32\n      4.5 Overview of Other IETF\
    \ Projects Related to Traffic\n          Engineering................................................32\n\
    \         4.5.1 Integrated Services...................................32\n   \
    \      4.5.2 RSVP..................................................33\n      \
    \   4.5.3 Differentiated Services...............................34\n         4.5.4\
    \ MPLS..................................................35\n         4.5.5 IP\
    \ Performance Metrics................................36\n         4.5.6 Flow Measurement......................................37\n\
    \         4.5.7 Endpoint Congestion Management........................37\n   \
    \   4.6 Overview of ITU Activities Related to Traffic\n          Engineering................................................38\n\
    \      4.7 Content Distribution.......................................39\n   5.0\
    \ Taxonomy of Traffic Engineering Systems.......................40\n      5.1\
    \ Time-Dependent Versus State-Dependent......................40\n      5.2 Offline\
    \ Versus Online......................................41\n      5.3 Centralized\
    \ Versus Distributed.............................42\n      5.4 Local Versus Global........................................42\n\
    \      5.5 Prescriptive Versus Descriptive............................42\n   \
    \   5.6 Open-Loop Versus Closed-Loop...............................43\n      5.7\
    \ Tactical vs Strategic......................................43\n   6.0 Recommendations\
    \ for Internet Traffic Engineering..............43\n      6.1 Generic Non-functional\
    \ Recommendations.....................44\n      6.2 Routing Recommendations....................................46\n\
    \      6.3 Traffic Mapping Recommendations............................48\n   \
    \   6.4 Measurement Recommendations................................49\n      6.5\
    \ Network Survivability......................................50\n         6.5.1\
    \ Survivability in MPLS Based Networks..................52\n         6.5.2 Protection\
    \ Option.....................................53\n      6.6 Traffic Engineering\
    \ in Diffserv Environments...............54\n      6.7 Network Controllability....................................56\n\
    \   7.0 Inter-Domain Considerations...................................57\n   8.0\
    \ Overview of Contemporary TE Practices in Operational\n       IP Networks...................................................59\n\
    \   9.0 Conclusion....................................................63\n   10.0\
    \ Security Considerations......................................63\n   11.0 Acknowledgments..............................................63\n\
    \   12.0 References...................................................64\n   13.0\
    \ Authors' Addresses...........................................70\n   14.0 Full\
    \ Copyright Statement.....................................71\n"
- title: 1.0 Introduction
  contents:
  - "1.0 Introduction\n   This memo describes the principles of Internet traffic engineering.\n\
    \   The objective of the document is to articulate the general issues and\n  \
    \ principles for Internet traffic engineering; and where appropriate to\n   provide\
    \ recommendations, guidelines, and options for the development\n   of online and\
    \ offline Internet traffic engineering capabilities and\n   support systems.\n\
    \   This document can aid service providers in devising and implementing\n   traffic\
    \ engineering solutions for their networks.  Networking\n   hardware and software\
    \ vendors will also find this document helpful in\n   the development of mechanisms\
    \ and support systems for the Internet\n   environment that support the traffic\
    \ engineering function.\n   This document provides a terminology for describing\
    \ and understanding\n   common Internet traffic engineering concepts.  This document\
    \ also\n   provides a taxonomy of known traffic engineering styles.  In this\n\
    \   context, a traffic engineering style abstracts important aspects from\n  \
    \ a traffic engineering methodology.  Traffic engineering styles can be\n   viewed\
    \ in different ways depending upon the specific context in which\n   they are\
    \ used and the specific purpose which they serve.  The\n   combination of styles\
    \ and views results in a natural taxonomy of\n   traffic engineering systems.\n\
    \   Even though Internet traffic engineering is most effective when\n   applied\
    \ end-to-end, the initial focus of this document document is\n   intra-domain\
    \ traffic engineering (that is, traffic engineering within\n   a given autonomous\
    \ system).  However, because a preponderance of\n   Internet traffic tends to\
    \ be inter-domain (originating in one\n   autonomous system and terminating in\
    \ another), this document provides\n   an overview of aspects pertaining to inter-domain\
    \ traffic\n   engineering.\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in RFC 2119.\n"
- title: 1.1. What is Internet Traffic Engineering?
  contents:
  - "1.1. What is Internet Traffic Engineering?\n   Internet traffic engineering is\
    \ defined as that aspect of Internet\n   network engineering dealing with the\
    \ issue of performance evaluation\n   and performance optimization of operational\
    \ IP networks.  Traffic\n   Engineering encompasses the application of technology\
    \ and scientific\n   principles to the measurement, characterization, modeling,\
    \ and\n   control of Internet traffic [RFC-2702, AWD2].\n   Enhancing the performance\
    \ of an operational network, at both the\n   traffic and resource levels, are\
    \ major objectives of Internet traffic\n   engineering.  This is accomplished\
    \ by addressing traffic oriented\n   performance requirements, while utilizing\
    \ network resources\n   economically and reliably.  Traffic oriented performance\
    \ measures\n   include delay, delay variation, packet loss, and throughput.\n\
    \   An important objective of Internet traffic engineering is to\n   facilitate\
    \ reliable network operations [RFC-2702].  Reliable network\n   operations can\
    \ be facilitated by providing mechanisms that enhance\n   network integrity and\
    \ by embracing policies emphasizing network\n   survivability.  This results in\
    \ a minimization of the vulnerability\n   of the network to service outages arising\
    \ from errors, faults, and\n   failures occurring within the infrastructure.\n\
    \   The Internet exists in order to transfer information from source\n   nodes\
    \ to destination nodes.  Accordingly, one of the most significant\n   functions\
    \ performed by the Internet is the routing of traffic from\n   ingress nodes to\
    \ egress nodes.  Therefore, one of the most\n   distinctive functions performed\
    \ by Internet traffic engineering is\n   the control and optimization of the routing\
    \ function, to steer\n   traffic through the network in the most effective way.\n\
    \   Ultimately, it is the performance of the network as seen by end users\n  \
    \ of network services that is truly paramount.  This crucial point\n   should\
    \ be considered throughout the development of traffic\n   engineering mechanisms\
    \ and policies.  The characteristics visible to\n   end users are the emergent\
    \ properties of the network, which are the\n   characteristics of the network\
    \ when viewed as a whole.  A central\n   goal of the service provider, therefore,\
    \ is to enhance the emergent\n   properties of the network while taking economic\
    \ considerations into\n   account.\n   The importance of the above observation\
    \ regarding the emergent\n   properties of networks is that special care must\
    \ be taken when\n   choosing network performance measures to optimize.  Optimizing\
    \ the\n   wrong measures may achieve certain local objectives, but may have\n\
    \   disastrous consequences on the emergent properties of the network and\n  \
    \ thereby on the quality of service perceived by end-users of network\n   services.\n\
    \   A subtle, but practical advantage of the systematic application of\n   traffic\
    \ engineering concepts to operational networks is that it helps\n   to identify\
    \ and structure goals and priorities in terms of enhancing\n   the quality of\
    \ service delivered to end-users of network services.\n   The application of traffic\
    \ engineering concepts also aids in the\n   measurement and analysis of the achievement\
    \ of these goals.\n   The optimization aspects of traffic engineering can be achieved\n\
    \   through capacity management and traffic management.  As used in this\n   document,\
    \ capacity management includes capacity planning, routing\n   control, and resource\
    \ management.  Network resources of particular\n   interest include link bandwidth,\
    \ buffer space, and computational\n   resources.  Likewise, as used in this document,\
    \ traffic management\n   includes (1) nodal traffic control functions such as\
    \ traffic\n   conditioning, queue management, scheduling, and (2) other functions\n\
    \   that regulate traffic flow through the network or that arbitrate\n   access\
    \ to network resources between different packets or between\n   different traffic\
    \ streams.\n   The optimization objectives of Internet traffic engineering should\
    \ be\n   viewed as a continual and iterative process of network performance\n\
    \   improvement and not simply as a one time goal.  Traffic engineering\n   also\
    \ demands continual development of new technologies and new\n   methodologies\
    \ for network performance enhancement.\n   The optimization objectives of Internet\
    \ traffic engineering may\n   change over time as new requirements are imposed,\
    \ as new technologies\n   emerge, or as new insights are brought to bear on the\
    \ underlying\n   problems.  Moreover, different networks may have different\n\
    \   optimization objectives, depending upon their business models,\n   capabilities,\
    \ and operating constraints.  The optimization aspects of\n   traffic engineering\
    \ are ultimately concerned with network control\n   regardless of the specific\
    \ optimization goals in any particular\n   environment.\n   Thus, the optimization\
    \ aspects of traffic engineering can be viewed\n   from a control perspective.\
    \  The aspect of control within the\n   Internet traffic engineering arena can\
    \ be pro-active and/or reactive.\n   In the pro-active case, the traffic engineering\
    \ control system takes\n   preventive action to obviate predicted unfavorable\
    \ future network\n   states.  It may also take perfective action to induce a more\n\
    \   desirable state in the future.  In the reactive case, the control\n   system\
    \ responds correctively and perhaps adaptively to events that\n   have already\
    \ transpired in the network.\n   The control dimension of Internet traffic engineering\
    \ responds at\n   multiple levels of temporal resolution to network events.  Certain\n\
    \   aspects of capacity management, such as capacity planning, respond at\n  \
    \ very coarse temporal levels, ranging from days to possibly years.\n   The introduction\
    \ of automatically switched optical transport networks\n   (e.g., based on the\
    \ Multi-protocol Lambda Switching concepts) could\n   significantly reduce the\
    \ lifecycle for capacity planning by\n   expediting provisioning of optical bandwidth.\
    \  Routing control\n   functions operate at intermediate levels of temporal resolution,\n\
    \   ranging from milliseconds to days.  Finally, the packet level\n   processing\
    \ functions (e.g., rate shaping, queue management, and\n   scheduling) operate\
    \ at very fine levels of temporal resolution,\n   ranging from picoseconds to\
    \ milliseconds while responding to the\n   real-time statistical behavior of traffic.\
    \  The subsystems of\n   Internet traffic engineering control include: capacity\
    \ augmentation,\n   routing control, traffic control, and resource control (including\n\
    \   control of service policies at network elements).  When capacity is\n   to\
    \ be augmented for tactical purposes, it may be desirable to devise\n   a deployment\
    \ plan that expedites bandwidth provisioning while\n   minimizing installation\
    \ costs.\n   Inputs into the traffic engineering control system include network\n\
    \   state variables, policy variables, and decision variables.\n   One major challenge\
    \ of Internet traffic engineering is the\n   realization of automated control\
    \ capabilities that adapt quickly and\n   cost effectively to significant changes\
    \ in a network's state, while\n   still maintaining stability.\n   Another critical\
    \ dimension of Internet traffic engineering is network\n   performance evaluation,\
    \ which is important for assessing the\n   effectiveness of traffic engineering\
    \ methods, and for monitoring and\n   verifying compliance with network performance\
    \ goals.  Results from\n   performance evaluation can be used to identify existing\
    \ problems,\n   guide network re-optimization, and aid in the prediction of potential\n\
    \   future problems.\n   Performance evaluation can be achieved in many different\
    \ ways.  The\n   most notable techniques include analytical methods, simulation,\
    \ and\n   empirical methods based on measurements.  When analytical methods or\n\
    \   simulation are used, network nodes and links can be modeled to\n   capture\
    \ relevant operational features such as topology, bandwidth,\n   buffer space,\
    \ and nodal service policies (link scheduling, packet\n   prioritization, buffer\
    \ management, etc.).  Analytical traffic models\n   can be used to depict dynamic\
    \ and behavioral traffic characteristics,\n   such as burstiness, statistical\
    \ distributions, and dependence.\n   Performance evaluation can be quite complicated\
    \ in practical network\n   contexts.  A number of techniques can be used to simplify\
    \ the\n   analysis, such as abstraction, decomposition, and approximation.  For\n\
    \   example, simplifying concepts such as effective bandwidth and\n   effective\
    \ buffer [Elwalid] may be used to approximate nodal behaviors\n   at the packet\
    \ level and simplify the analysis at the connection\n   level.  Network analysis\
    \ techniques using, for example, queuing\n   models and approximation schemes\
    \ based on asymptotic and\n   decomposition techniques can render the analysis\
    \ even more tractable.\n   In particular, an emerging set of concepts known as\
    \ network calculus\n   [CRUZ] based on deterministic bounds may simplify network\
    \ analysis\n   relative to classical stochastic techniques.  When using analytical\n\
    \   techniques, care should be taken to ensure that the models faithfully\n  \
    \ reflect the relevant operational characteristics of the modeled\n   network\
    \ entities.\n   Simulation can be used to evaluate network performance or to verify\n\
    \   and validate analytical approximations.  Simulation can, however, be\n   computationally\
    \ costly and may not always provide sufficient\n   insights.  An appropriate approach\
    \ to a given network performance\n   evaluation problem may involve a hybrid combination\
    \ of analytical\n   techniques, simulation, and empirical methods.\n   As a general\
    \ rule, traffic engineering concepts and mechanisms must\n   be sufficiently specific\
    \ and well defined to address known\n   requirements, but simultaneously flexible\
    \ and extensible to\n   accommodate unforeseen future demands.\n"
- title: 1.2. Scope
  contents:
  - "1.2. Scope\n   The scope of this document is intra-domain traffic engineering;\
    \ that\n   is, traffic engineering within a given autonomous system in the\n \
    \  Internet.  This document will discuss concepts pertaining to intra-\n   domain\
    \ traffic control, including such issues as routing control,\n   micro and macro\
    \ resource allocation, and the control coordination\n   problems that arise consequently.\n\
    \   This document will describe and characterize techniques already in\n   use\
    \ or in advanced development for Internet traffic engineering.  The\n   way these\
    \ techniques fit together will be discussed and scenarios in\n   which they are\
    \ useful will be identified.\n   While this document considers various intra-domain\
    \ traffic\n   engineering approaches, it focuses more on traffic engineering with\n\
    \   MPLS.  Traffic engineering based upon manipulation of IGP metrics is\n   not\
    \ addressed in detail.  This topic may be addressed by other\n   working group\
    \ document(s).\n   Although the emphasis is on intra-domain traffic engineering,\
    \ in\n   Section 7.0, an overview of the high level considerations pertaining\n\
    \   to inter-domain traffic engineering will be provided.  Inter-domain\n   Internet\
    \ traffic engineering is crucial to the performance\n   enhancement of the global\
    \ Internet infrastructure.\n   Whenever possible, relevant requirements from existing\
    \ IETF documents\n   and other sources will be incorporated by reference.\n"
- title: 1.3 Terminology
  contents:
  - "1.3 Terminology\n   This subsection provides terminology which is useful for\
    \ Internet\n   traffic engineering.  The definitions presented apply to this\n\
    \   document.  These terms may have other meanings elsewhere.\n      - Baseline\
    \ analysis:\n            A study conducted to serve as a baseline for comparison\
    \ to\n            the actual behavior of the network.\n      - Busy hour:\n  \
    \          A one hour period within a specified interval of time\n           \
    \ (typically 24 hours) in which the traffic load in a network\n            or\
    \ sub-network is greatest.\n      - Bottleneck:\n            A network element\
    \ whose input traffic rate tends to be\n            greater than its output rate.\n\
    \      - Congestion:\n            A state of a network resource in which the traffic\
    \ incident\n            on the resource exceeds its output capacity over an interval\n\
    \            of time.\n      - Congestion avoidance:\n            An approach\
    \ to congestion management that attempts to\n            obviate the occurrence\
    \ of congestion.\n      - Congestion control:\n            An approach to congestion\
    \ management that attempts to remedy\n            congestion problems that have\
    \ already occurred.\n      - Constraint-based routing:\n            A class of\
    \ routing protocols that take specified traffic\n            attributes, network\
    \ constraints, and policy constraints into\n            account when making routing\
    \ decisions.  Constraint-based\n            routing is applicable to traffic aggregates\
    \ as well as\n            flows.  It is a generalization of QoS routing.\n   \
    \   - Demand side congestion management:\n            A congestion management\
    \ scheme that addresses congestion\n            problems by regulating or conditioning\
    \ offered load.\n      - Effective bandwidth:\n            The minimum amount\
    \ of bandwidth that can be assigned to a\n            flow or traffic aggregate\
    \ in order to deliver 'acceptable\n            service quality' to the flow or\
    \ traffic aggregate.\n      - Egress traffic:\n            Traffic exiting a network\
    \ or network element.\n      - Hot-spot:\n            A network element or subsystem\
    \ which is in a state of\n            congestion.\n      - Ingress traffic:\n\
    \            Traffic entering a network or network element.\n      - Inter-domain\
    \ traffic:\n            Traffic that originates in one Autonomous system and\n\
    \            terminates in another.\n      - Loss network:\n            A network\
    \ that does not provide adequate buffering for\n            traffic, so that traffic\
    \ entering a busy resource within the\n            network will be dropped rather\
    \ than queued.\n      - Metric:\n            A parameter defined in terms of standard\
    \ units of\n            measurement.\n      - Measurement Methodology:\n     \
    \       A repeatable measurement technique used to derive one or\n           \
    \ more metrics of interest.\n      - Network Survivability:\n            The capability\
    \ to provide a prescribed level of QoS for\n            existing services after\
    \ a given number of failures occur\n            within the network.\n      - Offline\
    \ traffic engineering:\n            A traffic engineering system that exists outside\
    \ of the\n            network.\n      - Online traffic engineering:\n        \
    \    A traffic engineering system that exists within the network,\n          \
    \  typically implemented on or as adjuncts to operational\n            network\
    \ elements.\n      - Performance measures:\n            Metrics that provide quantitative\
    \ or qualitative measures of\n            the performance of systems or subsystems\
    \ of interest.\n      - Performance management:\n            A systematic approach\
    \ to improving effectiveness in the\n            accomplishment of specific networking\
    \ goals related to\n            performance improvement.\n      - Performance\
    \ Metric:\n            A performance parameter defined in terms of standard units\n\
    \            of measurement.\n      - Provisioning:\n            The process of\
    \ assigning or configuring network resources to\n            meet certain requests.\n\
    \      - QoS routing:\n            Class of routing systems that selects paths\
    \ to be used by a\n            flow based on the QoS requirements of the flow.\n\
    \      - Service Level Agreement:\n            A contract between a provider and\
    \ a customer that guarantees\n            specific levels of performance and reliability\
    \ at a certain\n            cost.\n      - Stability:\n            An operational\
    \ state in which a network does not oscillate\n            in a disruptive manner\
    \ from one mode to another mode.\n      - Supply side congestion management:\n\
    \            A congestion management scheme that provisions additional\n     \
    \       network resources to address existing and/or anticipated\n           \
    \ congestion problems.\n      - Transit traffic:\n            Traffic whose origin\
    \ and destination are both outside of the\n            network under consideration.\n\
    \      - Traffic characteristic:\n            A description of the temporal behavior\
    \ or a description of\n            the attributes of a given traffic flow or traffic\
    \ aggregate.\n      - Traffic engineering system:\n            A collection of\
    \ objects, mechanisms, and protocols that are\n            used conjunctively\
    \ to accomplish traffic engineering\n            objectives.\n      - Traffic\
    \ flow:\n            A stream of packets between two end-points that can be\n\
    \            characterized in a certain way.  A micro-flow has a more\n      \
    \      specific definition: A micro-flow is a stream of packets\n            with\
    \ the same source and destination addresses, source and\n            destination\
    \ ports, and protocol ID.\n      - Traffic intensity:\n            A measure of\
    \ traffic loading with respect to a resource\n            capacity over a specified\
    \ period of time.  In classical\n            telephony systems, traffic intensity\
    \ is measured in units of\n            Erlang.\n      - Traffic matrix:\n    \
    \        A representation of the traffic demand between a set of\n           \
    \ origin and destination abstract nodes.  An abstract node can\n            consist\
    \ of one or more network elements.\n      - Traffic monitoring:\n            The\
    \ process of observing traffic characteristics at a given\n            point in\
    \ a network and collecting the traffic information\n            for analysis and\
    \ further action.\n      - Traffic trunk:\n            An aggregation of traffic\
    \ flows belonging to the same class\n            which are forwarded through a\
    \ common path.  A traffic trunk\n            may be characterized by an ingress\
    \ and egress node, and a\n            set of attributes which determine its behavioral\n\
    \            characteristics and requirements from the network.\n"
- title: 2.0 Background
  contents:
  - "2.0 Background\n   The Internet has quickly evolved into a very critical communications\n\
    \   infrastructure, supporting significant economic, educational, and\n   social\
    \ activities.  Simultaneously, the delivery of Internet\n   communications services\
    \ has become very competitive and end-users are\n   demanding very high quality\
    \ service from their service providers.\n   Consequently, performance optimization\
    \ of large scale IP networks,\n   especially public Internet backbones, have become\
    \ an important\n   problem.  Network performance requirements are multi-dimensional,\n\
    \   complex, and sometimes contradictory; making the traffic engineering\n   problem\
    \ very challenging.\n   The network must convey IP packets from ingress nodes\
    \ to egress nodes\n   efficiently, expeditiously, and economically.  Furthermore,\
    \ in a\n   multiclass service environment (e.g., Diffserv capable networks), the\n\
    \   resource sharing parameters of the network must be appropriately\n   determined\
    \ and configured according to prevailing policies and\n   service models to resolve\
    \ resource contention issues arising from\n   mutual interference between packets\
    \ traversing through the network.\n   Thus, consideration must be given to resolving\
    \ competition for\n   network resources between traffic streams belonging to the\
    \ same\n   service class (intra-class contention resolution) and traffic streams\n\
    \   belonging to different classes (inter-class contention resolution).\n"
- title: 2.1 Context of Internet Traffic Engineering
  contents:
  - "2.1 Context of Internet Traffic Engineering\n   The context of Internet traffic\
    \ engineering pertains to the scenarios\n   where traffic engineering is used.\
    \  A traffic engineering methodology\n   establishes appropriate rules to resolve\
    \ traffic performance issues\n   occurring in a specific context.  The context\
    \ of Internet traffic\n   engineering includes:\n      (1)   A network context\
    \ defining the universe of discourse, and in\n            particular the situations\
    \ in which the traffic engineering\n            problems occur.  The network context\
    \ includes network\n            structure, network policies, network characteristics,\n\
    \            network constraints, network quality attributes, and network\n  \
    \          optimization criteria.\n      (2)   A problem context defining the\
    \ general and concrete issues\n            that traffic engineering addresses.\
    \  The problem context\n            includes identification, abstraction of relevant\
    \ features,\n            representation, formulation, specification of the\n \
    \           requirements on the solution space, and specification of the\n   \
    \         desirable features of acceptable solutions.\n      (3)   A solution\
    \ context suggesting how to address the issues\n            identified by the\
    \ problem context.  The solution context\n            includes analysis, evaluation\
    \ of alternatives, prescription,\n            and resolution.\n      (4)   An\
    \ implementation and operational context in which the\n            solutions are\
    \ methodologically instantiated.  The\n            implementation and operational\
    \ context includes planning,\n            organization, and execution.\n   The\
    \ context of Internet traffic engineering and the different problem\n   scenarios\
    \ are discussed in the following subsections.\n"
- title: 2.2 Network Context
  contents:
  - "2.2 Network Context\n   IP networks range in size from small clusters of routers\
    \ situated\n   within a given location, to thousands of interconnected routers,\n\
    \   switches, and other components distributed all over the world.\n   Conceptually,\
    \ at the most basic level of abstraction, an IP network\n   can be represented\
    \ as a distributed dynamical system consisting of:\n   (1) a set of interconnected\
    \ resources which provide transport\n   services for IP traffic subject to certain\
    \ constraints, (2) a demand\n   system representing the offered load to be transported\
    \ through the\n   network, and (3) a response system consisting of network processes,\n\
    \   protocols, and related mechanisms which facilitate the movement of\n   traffic\
    \ through the network [see also AWD2].\n   The network elements and resources\
    \ may have specific characteristics\n   restricting the manner in which the demand\
    \ is handled.  Additionally,\n   network resources may be equipped with traffic\
    \ control mechanisms\n   superintending the way in which the demand is serviced.\
    \  Traffic\n   control mechanisms may, for example, be used to control various\n\
    \   packet processing activities within a given resource, arbitrate\n   contention\
    \ for access to the resource by different packets, and\n   regulate traffic behavior\
    \ through the resource.  A configuration\n   management and provisioning system\
    \ may allow the settings of the\n   traffic control mechanisms to be manipulated\
    \ by external or internal\n   entities in order to exercise control over the way\
    \ in which the\n   network elements respond to internal and external stimuli.\n\
    \   The details of how the network provides transport services for\n   packets\
    \ are specified in the policies of the network administrators\n   and are installed\
    \ through network configuration management and policy\n   based provisioning systems.\
    \  Generally, the types of services\n   provided by the network also depends upon\
    \ the technology and\n   characteristics of the network elements and protocols,\
    \ the prevailing\n   service and utility models, and the ability of the network\n\
    \   administrators to translate policies into network configurations.\n   Contemporary\
    \ Internet networks have three significant\n   characteristics:  (1) they provide\
    \ real-time services, (2) they have\n   become mission critical, and (3) their\
    \ operating environments are\n   very dynamic.  The dynamic characteristics of\
    \ IP networks can be\n   attributed in part to fluctuations in demand, to the\
    \ interaction\n   between various network protocols and processes, to the rapid\n\
    \   evolution of the infrastructure which demands the constant inclusion\n   of\
    \ new technologies and new network elements, and to transient and\n   persistent\
    \ impairments which occur within the system.\n   Packets contend for the use of\
    \ network resources as they are conveyed\n   through the network.  A network resource\
    \ is considered to be\n   congested if the arrival rate of packets exceed the\
    \ output capacity\n   of the resource over an interval of time.  Congestion may\
    \ result in\n   some of the arrival packets being delayed or even dropped.\n \
    \  Congestion increases transit delays, delay variation, packet loss,\n   and\
    \ reduces the predictability of network services.  Clearly,\n   congestion is\
    \ a highly undesirable phenomenon.\n   Combating congestion at a reasonable cost\
    \ is a major objective of\n   Internet traffic engineering.\n   Efficient sharing\
    \ of network resources by multiple traffic streams is\n   a basic economic premise\
    \ for packet switched networks in general and\n   for the Internet in particular.\
    \  A fundamental challenge in network\n   operation, especially in a large scale\
    \ public IP network, is to\n   increase the efficiency of resource utilization\
    \ while minimizing the\n   possibility of congestion.\n   Increasingly, the Internet\
    \ will have to function in the presence of\n   different classes of traffic with\
    \ different service requirements.\n   The advent of Differentiated Services [RFC-2475]\
    \ makes this\n   requirement particularly acute.  Thus, packets may be grouped\
    \ into\n   behavior aggregates such that each behavior aggregate may have a\n\
    \   common set of behavioral characteristics or a common set of delivery\n   requirements.\
    \  In practice, the delivery requirements of a specific\n   set of packets may\
    \ be specified explicitly or implicitly.  Two of the\n   most important traffic\
    \ delivery requirements are capacity constraints\n   and QoS constraints.\n  \
    \ Capacity constraints can be expressed statistically as peak rates,\n   mean\
    \ rates, burst sizes, or as some deterministic notion of effective\n   bandwidth.\
    \  QoS requirements can be expressed in terms of (1)\n   integrity constraints\
    \ such as packet loss and (2) in terms of\n   temporal constraints such as timing\
    \ restrictions for the delivery of\n   each packet (delay) and timing restrictions\
    \ for the delivery of\n   consecutive packets belonging to the same traffic stream\
    \ (delay\n   variation).\n"
- title: 2.3 Problem Context
  contents:
  - "2.3 Problem Context\n   Fundamental problems exist in association with the operation\
    \ of a\n   network described by the simple model of the previous subsection.\n\
    \   This subsection reviews the problem context in relation to the\n   traffic\
    \ engineering function.\n   The identification, abstraction, representation, and\
    \ measurement of\n   network features relevant to traffic engineering is a significant\n\
    \   issue.\n   One particularly important class of problems concerns how to\n\
    \   explicitly formulate the problems that traffic engineering attempts\n   to\
    \ solve, how to identify the requirements on the solution space, how\n   to specify\
    \ the desirable features of good solutions, how to actually\n   solve the problems,\
    \ and how to measure and characterize the\n   effectiveness of the solutions.\n\
    \   Another class of problems concerns how to measure and estimate\n   relevant\
    \ network state parameters.  Effective traffic engineering\n   relies on a good\
    \ estimate of the offered traffic load as well as a\n   view of the underlying\
    \ topology and associated resource constraints.\n   A network-wide view of the\
    \ topology is also a must for offline\n   planning.\n   Still another class of\
    \ problems concerns how to characterize the\n   state of the network and how to\
    \ evaluate its performance under a\n   variety of scenarios.  The performance\
    \ evaluation problem is two-\n   fold.  One aspect of this problem relates to\
    \ the evaluation of the\n   system level performance of the network.  The other\
    \ aspect relates to\n   the evaluation of the resource level performance, which\
    \ restricts\n   attention to the performance analysis of individual network\n\
    \   resources.  In this memo, we refer to the system level\n   characteristics\
    \ of the network as the \"macro-states\" and the resource\n   level characteristics\
    \ as the \"micro-states.\" The system level\n   characteristics are also known\
    \ as the emergent properties of the\n   network as noted earlier.  Correspondingly,\
    \ we shall refer to the\n   traffic engineering schemes dealing with network performance\n\
    \   optimization at the systems level as \"macro-TE\" and the schemes that\n \
    \  optimize at the individual resource level as \"micro-TE.\"  Under\n   certain\
    \ circumstances, the system level performance can be derived\n   from the resource\
    \ level performance using appropriate rules of\n   composition, depending upon\
    \ the particular performance measures of\n   interest.\n   Another fundamental\
    \ class of problems concerns how to effectively\n   optimize network performance.\
    \  Performance optimization may entail\n   translating solutions to specific traffic\
    \ engineering problems into\n   network configurations.  Optimization may also\
    \ entail some degree of\n   resource management control, routing control, and/or\
    \ capacity\n   augmentation.\n   As noted previously, congestion is an undesirable\
    \ phenomena in\n   operational networks.  Therefore, the next subsection addresses\
    \ the\n   issue of congestion and its ramifications within the problem context\n\
    \   of Internet traffic engineering.\n"
- title: 2.3.1 Congestion and its Ramifications
  contents:
  - "2.3.1 Congestion and its Ramifications\n   Congestion is one of the most significant\
    \ problems in an operational\n   IP context.  A network element is said to be\
    \ congested if it\n   experiences sustained overload over an interval of time.\
    \  Congestion\n   almost always results in degradation of service quality to end\
    \ users.\n   Congestion control schemes can include demand side policies and\n\
    \   supply side policies.  Demand side policies may restrict access to\n   congested\
    \ resources and/or dynamically regulate the demand to\n   alleviate the overload\
    \ situation.  Supply side policies may expand or\n   augment network capacity\
    \ to better accommodate offered traffic.\n   Supply side policies may also re-allocate\
    \ network resources by\n   redistributing traffic over the infrastructure.  Traffic\n\
    \   redistribution and resource re-allocation serve to increase the\n   'effective\
    \ capacity' seen by the demand.\n   The emphasis of this memo is primarily on\
    \ congestion management\n   schemes falling within the scope of the network, rather\
    \ than on\n   congestion management systems dependent upon sensitivity and\n \
    \  adaptivity from end-systems.  That is, the aspects that are\n   considered\
    \ in this memo with respect to congestion management are\n   those solutions that\
    \ can be provided by control entities operating on\n   the network and by the\
    \ actions of network administrators and network\n   operations systems.\n"
- title: 2.4 Solution Context
  contents:
  - "2.4 Solution Context\n   The solution context for Internet traffic engineering\
    \ involves\n   analysis, evaluation of alternatives, and choice between alternative\n\
    \   courses of action.  Generally the solution context is predicated on\n   making\
    \ reasonable inferences about the current or future state of the\n   network,\
    \ and subsequently making appropriate decisions that may\n   involve a preference\
    \ between alternative sets of action.  More\n   specifically, the solution context\
    \ demands reasonable estimates of\n   traffic workload, characterization of network\
    \ state, deriving\n   solutions to traffic engineering problems which may be implicitly\
    \ or\n   explicitly formulated, and possibly instantiating a set of control\n\
    \   actions.  Control actions may involve the manipulation of parameters\n   associated\
    \ with routing, control over tactical capacity acquisition,\n   and control over\
    \ the traffic management functions.\n   The following list of instruments may\
    \ be applicable to the solution\n   context of Internet traffic engineering.\n\
    \      (1)   A set of policies, objectives, and requirements (which may\n    \
    \        be context dependent) for network performance evaluation and\n      \
    \      performance  optimization.\n      (2)   A collection of online and possibly\
    \ offline tools and\n            mechanisms for measurement, characterization,\
    \ modeling, and\n            control of Internet traffic and control over the\
    \ placement\n            and allocation of network resources, as well as control\
    \ over\n            the mapping or distribution of traffic onto the\n        \
    \    infrastructure.\n      (3)   A set of constraints on the operating environment,\
    \ the\n            network protocols, and the traffic engineering system\n   \
    \         itself.\n      (4)   A set of quantitative and qualitative techniques\
    \ and\n            methodologies for abstracting, formulating, and solving\n \
    \           traffic engineering problems.\n      (5)   A set of administrative\
    \ control parameters which may be\n            manipulated through a Configuration\
    \ Management (CM) system.\n            The CM system itself may include a configuration\
    \ control\n            subsystem, a configuration repository, a configuration\n\
    \            accounting subsystem, and a configuration auditing\n            subsystem.\n\
    \      (6)   A set of guidelines for network performance evaluation,\n       \
    \     performance optimization, and performance improvement.\n   Derivation of\
    \ traffic characteristics through measurement and/or\n   estimation is very useful\
    \ within the realm of the solution space for\n   traffic engineering.  Traffic\
    \ estimates can be derived from customer\n   subscription information, traffic\
    \ projections, traffic models, and\n   from actual empirical measurements.  The\
    \ empirical measurements may\n   be performed at the traffic aggregate level or\
    \ at the flow level in\n   order to derive traffic statistics at various levels\
    \ of detail.\n   Measurements at the flow level or on small traffic aggregates\
    \ may be\n   performed at edge nodes, where traffic enters and leaves the network.\n\
    \   Measurements at large traffic aggregate levels may be performed\n   within\
    \ the core of the network where potentially numerous traffic\n   flows may be\
    \ in transit concurrently.\n   To conduct performance studies and to support planning\
    \ of existing\n   and future networks, a routing analysis may be performed to\
    \ determine\n   the path(s) the routing protocols will choose for various traffic\n\
    \   demands, and to ascertain the utilization of network resources as\n   traffic\
    \ is routed through the network.  The routing analysis should\n   capture the\
    \ selection of paths through the network, the assignment of\n   traffic across\
    \ multiple feasible routes, and the multiplexing of IP\n   traffic over traffic\
    \ trunks (if such constructs exists) and over the\n   underlying network infrastructure.\
    \  A network topology model is a\n   necessity for routing analysis.  A network\
    \ topology model may be\n   extracted from network architecture documents, from\
    \ network designs,\n   from information contained in router configuration files,\
    \ from\n   routing databases, from routing tables, or from automated tools that\n\
    \   discover and depict network topology information.  Topology\n   information\
    \ may also be derived from servers that monitor network\n   state, and from servers\
    \ that perform provisioning functions.\n   Routing in operational IP networks\
    \ can be administratively controlled\n   at various levels of abstraction including\
    \ the manipulation of BGP\n   attributes and manipulation of IGP metrics.  For\
    \ path oriented\n   technologies such as MPLS, routing can be further controlled\
    \ by the\n   manipulation of relevant traffic engineering parameters, resource\n\
    \   parameters, and administrative policy constraints.  Within the\n   context\
    \ of MPLS, the path of an explicit label switched path (LSP)\n   can be computed\
    \ and established in various ways including: (1)\n   manually, (2) automatically\
    \ online using constraint-based routing\n   processes implemented on label switching\
    \ routers, and (3)\n   automatically offline using constraint-based routing entities\n\
    \   implemented on external traffic engineering support systems.\n"
- title: 2.4.1 Combating the Congestion Problem
  contents:
  - "2.4.1 Combating the Congestion Problem\n   Minimizing congestion is a significant\
    \ aspect of Internet traffic\n   engineering.  This subsection gives an overview\
    \ of the general\n   approaches that have been used or proposed to combat congestion\n\
    \   problems.\n   Congestion management policies can be categorized based upon\
    \ the\n   following criteria (see e.g., [YARE95] for a more detailed taxonomy\n\
    \   of congestion control schemes): (1) Response time scale which can be\n   characterized\
    \ as long, medium, or short; (2) reactive versus\n   preventive which relates\
    \ to congestion control and congestion\n   avoidance; and (3) supply side versus\
    \ demand side congestion\n   management schemes.  These aspects are discussed\
    \ in the following\n   paragraphs.\n   (1) Congestion Management based on Response\
    \ Time Scales\n   - Long (weeks to months): Capacity planning works over a relatively\n\
    \   long time scale to expand network capacity based on estimates or\n   forecasts\
    \ of future traffic demand and traffic distribution.  Since\n   router and link\
    \ provisioning take time and are generally expensive,\n   these upgrades are typically\
    \ carried out in the weeks-to-months or\n   even years time scale.\n   - Medium\
    \ (minutes to days): Several control policies fall within the\n   medium time\
    \ scale category.  Examples include: (1) Adjusting IGP\n   and/or BGP parameters\
    \ to route traffic away or towards certain\n   segments of the network; (2) Setting\
    \ up and/or adjusting some\n   explicitly routed label switched paths (ER-LSPs)\
    \ in MPLS networks to\n   route some traffic trunks away from possibly congested\
    \ resources or\n   towards possibly more favorable routes; (3) re-configuring\
    \ the\n   logical topology of the network to make it correlate more closely\n\
    \   with the spatial traffic distribution using for example some\n   underlying\
    \ path-oriented technology such as MPLS LSPs, ATM PVCs, or\n   optical channel\
    \ trails.  Many of these adaptive medium time scale\n   response schemes rely\
    \ on a measurement system that monitors changes\n   in traffic distribution, traffic\
    \ shifts, and network resource\n   utilization and subsequently provides feedback\
    \ to the online and/or\n   offline traffic engineering mechanisms and tools which\
    \ employ this\n   feedback information to trigger certain control actions to occur\n\
    \   within the network.  The traffic engineering mechanisms and tools can\n  \
    \ be implemented in a distributed fashion or in a centralized fashion,\n   and\
    \ may have a hierarchical structure or a flat structure.  The\n   comparative\
    \ merits of distributed and centralized control structures\n   for networks are\
    \ well known.  A centralized scheme may have global\n   visibility into the network\
    \ state and may produce potentially more\n   optimal solutions.  However, centralized\
    \ schemes are prone to single\n   points of failure and may not scale as well\
    \ as distributed schemes.\n   Moreover, the information utilized by a centralized\
    \ scheme may be\n   stale and may not reflect the actual state of the network.\
    \  It is not\n   an objective of this memo to make a recommendation between\n\
    \   distributed and centralized schemes.  This is a choice that network\n   administrators\
    \ must make based on their specific needs.\n   - Short (picoseconds to minutes):\
    \ This category includes packet level\n   processing functions and events on the\
    \ order of several round trip\n   times.  It includes router mechanisms such as\
    \ passive and active\n   buffer management.  These mechanisms are used to control\
    \ congestion\n   and/or signal congestion to end systems so that they can adaptively\n\
    \   regulate the rate at which traffic is injected into the network.  One\n  \
    \ of the most popular active queue management schemes, especially for\n   TCP\
    \ traffic, is Random Early Detection (RED) [FLJA93], which supports\n   congestion\
    \ avoidance by controlling the average queue size.  During\n   congestion (but\
    \ before the queue is filled), the RED scheme chooses\n   arriving packets to\
    \ \"mark\" according to a probabilistic algorithm\n   which takes into account\
    \ the average queue size.  For a router that\n   does not utilize explicit congestion\
    \ notification (ECN) see e.g.,\n   [FLOY94], the marked packets can simply be\
    \ dropped to signal the\n   inception of congestion to end systems.  On the other\
    \ hand, if the\n   router supports ECN, then it can set the ECN field in the packet\n\
    \   header.  Several variations of RED have been proposed to support\n   different\
    \ drop precedence levels in multi-class environments [RFC-\n   2597], e.g., RED\
    \ with In and Out (RIO) and Weighted RED.  There is\n   general consensus that\
    \ RED provides congestion avoidance performance\n   which is not worse than traditional\
    \ Tail-Drop (TD) queue management\n   (drop arriving packets only when the queue\
    \ is full).  Importantly,\n   however, RED reduces the possibility of global synchronization\
    \ and\n   improves fairness among different TCP sessions.  However, RED by\n \
    \  itself can not prevent congestion and unfairness caused by sources\n   unresponsive\
    \ to RED, e.g., UDP traffic and some misbehaved greedy\n   connections.  Other\
    \ schemes have been proposed to improve the\n   performance and fairness in the\
    \ presence of unresponsive traffic.\n   Some of these schemes were proposed as\
    \ theoretical frameworks and are\n   typically not available in existing commercial\
    \ products.  Two such\n   schemes are Longest Queue Drop (LQD) and Dynamic Soft\
    \ Partitioning\n   with Random Drop (RND) [SLDC98].\n   (2) Congestion Management:\
    \ Reactive versus Preventive Schemes\n   - Reactive: reactive (recovery) congestion\
    \ management policies react\n   to existing congestion problems to improve it.\
    \  All the policies\n   described in the long and medium time scales above can\
    \ be categorized\n   as being reactive especially if the policies are based on\
    \ monitoring\n   and identifying existing congestion problems, and on the initiation\n\
    \   of relevant actions to ease a situation.\n   - Preventive: preventive (predictive/avoidance)\
    \ policies take\n   proactive action to prevent congestion based on estimates\
    \ and\n   predictions of future potential congestion problems.  Some of the\n\
    \   policies described in the long and medium time scales fall into this\n   category.\
    \  They do not necessarily respond immediately to existing\n   congestion problems.\
    \  Instead forecasts of traffic demand and\n   workload distribution are considered\
    \ and action may be taken to\n   prevent potential congestion problems in the\
    \ future.  The schemes\n   described in the short time scale (e.g., RED and its\
    \ variations, ECN,\n   LQD, and RND) are also used for congestion avoidance since\
    \ dropping\n   or marking packets before queues actually overflow would trigger\n\
    \   corresponding TCP sources to slow down.\n   (3) Congestion Management: Supply\
    \ Side versus Demand Side Schemes\n   - Supply side: supply side congestion management\
    \ policies increase\n   the effective capacity available to traffic in order to\
    \ control or\n   obviate congestion.  This can be accomplished by augmenting capacity.\n\
    \   Another way to accomplish this is to minimize congestion by having a\n   relatively\
    \ balanced distribution of traffic over the network.  For\n   example, capacity\
    \ planning should aim to provide a physical topology\n   and associated link bandwidths\
    \ that match estimated traffic workload\n   and traffic distribution based on\
    \ forecasting (subject to budgetary\n   and other constraints).  However, if actual\
    \ traffic distribution does\n   not match the topology derived from capacity panning\
    \ (due to\n   forecasting errors or facility constraints for example), then the\n\
    \   traffic can be mapped onto the existing topology using routing\n   control\
    \ mechanisms, using path oriented technologies (e.g., MPLS LSPs\n   and optical\
    \ channel trails) to modify the logical topology, or by\n   using some other load\
    \ redistribution mechanisms.\n   - Demand side: demand side congestion management\
    \ policies control or\n   regulate the offered traffic to alleviate congestion\
    \ problems.  For\n   example, some of the short time scale mechanisms described\
    \ earlier\n   (such as RED and its variations, ECN, LQD, and RND) as well as\n\
    \   policing and rate shaping mechanisms attempt to regulate the offered\n   load\
    \ in various ways.  Tariffs may also be applied as a demand side\n   instrument.\
    \  To date, however, tariffs have not been used as a means\n   of demand side\
    \ congestion management within the Internet.\n   In summary, a variety of mechanisms\
    \ can be used to address congestion\n   problems in IP networks.  These mechanisms\
    \ may operate at multiple\n   time-scales.\n"
- title: 2.5 Implementation and Operational Context
  contents:
  - "2.5 Implementation and Operational Context\n   The operational context of Internet\
    \ traffic engineering is\n   characterized by constant change which occur at multiple\
    \ levels of\n   abstraction.  The implementation context demands effective planning,\n\
    \   organization, and execution.  The planning aspects may involve\n   determining\
    \ prior sets of actions to achieve desired objectives.\n   Organizing involves\
    \ arranging and assigning responsibility to the\n   various components of the\
    \ traffic engineering system and coordinating\n   the activities to accomplish\
    \ the desired TE objectives.  Execution\n   involves measuring and applying corrective\
    \ or perfective actions to\n   attain and maintain desired TE goals.\n"
- title: 3.0 Traffic Engineering Process Model(s)
  contents:
  - "3.0 Traffic Engineering Process Model(s)\n   This section describes a generic\
    \ process model that captures the high\n   level practical aspects of Internet\
    \ traffic engineering in an\n   operational context.  The process model is described\
    \ as a sequence of\n   actions that a traffic engineer, or more generally a traffic\n\
    \   engineering system, must perform to optimize the performance of an\n   operational\
    \ network (see also [RFC-2702, AWD2]).  The process model\n   described here represents\
    \ the broad activities common to most traffic\n   engineering methodologies although\
    \ the details regarding how traffic\n   engineering is executed may differ from\
    \ network to network.  This\n   process model may be enacted explicitly or implicitly,\
    \ by an\n   automaton and/or by a human.\n   The traffic engineering process model\
    \ is iterative [AWD2].  The four\n   phases of the process model described below\
    \ are repeated continually.\n   The first phase of the TE process model is to\
    \ define the relevant\n   control policies that govern the operation of the network.\
    \  These\n   policies may depend upon many factors including the prevailing\n\
    \   business model, the network cost structure, the operating\n   constraints,\
    \ the utility model, and optimization criteria.\n   The second phase of the process\
    \ model is a feedback mechanism\n   involving the acquisition of measurement data\
    \ from the operational\n   network.  If empirical data is not readily available\
    \ from the\n   network, then synthetic workloads may be used instead which reflect\n\
    \   either the prevailing or the expected workload of the network.\n   Synthetic\
    \ workloads may be derived by estimation or extrapolation\n   using prior empirical\
    \ data.  Their derivation may also be obtained\n   using mathematical models of\
    \ traffic characteristics or other means.\n   The third phase of the process model\
    \ is to analyze the network state\n   and to characterize traffic workload.  Performance\
    \ analysis may be\n   proactive and/or reactive.  Proactive performance analysis\
    \ identifies\n   potential problems that do not exist, but could manifest in the\n\
    \   future.  Reactive performance analysis identifies existing problems,\n   determines\
    \ their cause through diagnosis, and evaluates alternative\n   approaches to remedy\
    \ the problem, if necessary.  A number of\n   quantitative and qualitative techniques\
    \ may be used in the analysis\n   process, including modeling based analysis and\
    \ simulation.  The\n   analysis phase of the process model may involve investigating\
    \ the\n   concentration and distribution of traffic across the network or\n  \
    \ relevant subsets of the network, identifying the characteristics of\n   the\
    \ offered traffic workload, identifying existing or potential\n   bottlenecks,\
    \ and identifying network pathologies such as ineffective\n   link placement,\
    \ single points of failures, etc.  Network pathologies\n   may result from many\
    \ factors including inferior network architecture,\n   inferior network design,\
    \ and configuration problems.  A traffic\n   matrix may be constructed as part\
    \ of the analysis process.  Network\n   analysis may also be descriptive or prescriptive.\n\
    \   The fourth phase of the TE process model is the performance\n   optimization\
    \ of the network.  The performance optimization phase\n   involves a decision\
    \ process which selects and implements a set of\n   actions from a set of alternatives.\
    \  Optimization actions may include\n   the use of appropriate techniques to either\
    \ control the offered\n   traffic or to control the distribution of traffic across\
    \ the network.\n   Optimization actions may also involve adding additional links\
    \ or\n   increasing link capacity, deploying additional hardware such as\n   routers\
    \ and switches, systematically adjusting parameters associated\n   with routing\
    \ such as IGP metrics and BGP attributes, and adjusting\n   traffic management\
    \ parameters.  Network performance optimization may\n   also involve starting\
    \ a network planning process to improve the\n   network architecture, network\
    \ design, network capacity, network\n   technology, and the configuration of network\
    \ elements to accommodate\n   current and future growth.\n"
- title: 3.1 Components of the Traffic Engineering Process Model
  contents:
  - "3.1 Components of the Traffic Engineering Process Model\n   The key components\
    \ of the traffic engineering process model include a\n   measurement subsystem,\
    \ a modeling and analysis subsystem, and an\n   optimization subsystem.  The following\
    \ subsections examine these\n   components as they apply to the traffic engineering\
    \ process model.\n"
- title: 3.2 Measurement
  contents:
  - "3.2 Measurement\n   Measurement is crucial to the traffic engineering function.\
    \  The\n   operational state of a network can be conclusively determined only\n\
    \   through measurement.  Measurement is also critical to the\n   optimization\
    \ function because it provides feedback data which is used\n   by traffic engineering\
    \ control subsystems.  This data is used to\n   adaptively optimize network performance\
    \ in response to events and\n   stimuli originating within and outside the network.\
    \  Measurement is\n   also needed to determine the quality of network services\
    \ and to\n   evaluate the effectiveness of traffic engineering policies.\n   Experience\
    \ suggests that measurement is most effective when acquired\n   and applied systematically.\n\
    \   When developing a measurement system to support the traffic\n   engineering\
    \ function in IP networks, the following questions should\n   be carefully considered:\
    \ Why is measurement needed in this particular\n   context? What parameters are\
    \ to be measured?  How should the\n   measurement be accomplished?  Where should\
    \ the measurement be\n   performed? When should the measurement be performed?\
    \  How frequently\n   should the monitored variables be measured?  What level\
    \ of\n   measurement accuracy and reliability is desirable? What level of\n  \
    \ measurement accuracy and reliability is realistically attainable? To\n   what\
    \ extent can the measurement system permissibly interfere with the\n   monitored\
    \ network components and variables? What is the acceptable\n   cost of measurement?\
    \ The answers to these questions will determine\n   the measurement tools and\
    \ methodologies appropriate in any given\n   traffic engineering context.\n  \
    \ It should also be noted that there is a distinction between\n   measurement\
    \ and evaluation.  Measurement provides raw data concerning\n   state parameters\
    \ and variables of monitored network elements.\n   Evaluation utilizes the raw\
    \ data to make inferences regarding the\n   monitored system.\n   Measurement\
    \ in support of the TE function can occur at different\n   levels of abstraction.\
    \  For example, measurement can be used to\n   derive packet level characteristics,\
    \ flow level characteristics, user\n   or customer level characteristics, traffic\
    \ aggregate characteristics,\n   component level characteristics, and network\
    \ wide characteristics.\n"
- title: 3.3 Modeling, Analysis, and Simulation
  contents:
  - "3.3 Modeling, Analysis, and Simulation\n   Modeling and analysis are important\
    \ aspects of Internet traffic\n   engineering.  Modeling involves constructing\
    \ an abstract or physical\n   representation which depicts relevant traffic characteristics\
    \ and\n   network attributes.\n   A network model is an abstract representation\
    \ of the network which\n   captures relevant network features, attributes, and\
    \ characteristics,\n   such as link and nodal attributes and constraints.  A network\
    \ model\n   may facilitate analysis and/or simulation which can be used to\n \
    \  predict network performance under various conditions as well as to\n   guide\
    \ network expansion plans.\n   In general, Internet traffic engineering models\
    \ can be classified as\n   either structural or behavioral.  Structural models\
    \ focus on the\n   organization of the network and its components.  Behavioral\
    \ models\n   focus on the dynamics of the network and the traffic workload.\n\
    \   Modeling for Internet traffic engineering may also be formal or\n   informal.\n\
    \   Accurate behavioral models for traffic sources are particularly\n   useful\
    \ for analysis.  Development of behavioral traffic source models\n   that are\
    \ consistent with empirical data obtained from operational\n   networks is a major\
    \ research topic in Internet traffic engineering.\n   These source models should\
    \ also be tractable and amenable to\n   analysis.  The topic of source models\
    \ for IP traffic is a research\n   topic and is therefore outside the scope of\
    \ this document.  Its\n   importance, however, must be emphasized.\n   Network\
    \ simulation tools are extremely useful for traffic\n   engineering.  Because\
    \ of the complexity of realistic quantitative\n   analysis of network behavior,\
    \ certain aspects of network performance\n   studies can only be conducted effectively\
    \ using simulation.  A good\n   network simulator can be used to mimic and visualize\
    \ network\n   characteristics under various conditions in a safe and non-disruptive\n\
    \   manner.  For example, a network simulator may be used to depict\n   congested\
    \ resources and hot spots, and to provide hints regarding\n   possible solutions\
    \ to network performance problems.  A good simulator\n   may also be used to validate\
    \ the effectiveness of planned solutions\n   to network issues without the need\
    \ to tamper with the operational\n   network, or to commence an expensive network\
    \ upgrade which may not\n   achieve the desired objectives.  Furthermore, during\
    \ the process of\n   network planning, a network simulator may reveal pathologies\
    \ such as\n   single points of failure which may require additional redundancy,\
    \ and\n   potential bottlenecks and hot spots which may require additional\n \
    \  capacity.\n   Routing simulators are especially useful in large networks. \
    \ A\n   routing simulator may identify planned links which may not actually\n\
    \   be used to route traffic by the existing routing protocols.\n   Simulators\
    \ can also be used to conduct scenario based and\n   perturbation based analysis,\
    \ as well as sensitivity studies.\n   Simulation results can be used to initiate\
    \ appropriate actions in\n   various ways.  For example, an important application\
    \ of network\n   simulation tools is to investigate and identify how best to make\
    \ the\n   network evolve and grow, in order to accommodate projected future\n\
    \   demands.\n"
- title: 3.4 Optimization
  contents:
  - "3.4 Optimization\n   Network performance optimization involves resolving network\
    \ issues by\n   transforming such issues into concepts that enable a solution,\n\
    \   identification of a solution, and implementation of the solution.\n   Network\
    \ performance optimization can be corrective or perfective.  In\n   corrective\
    \ optimization, the goal is to remedy a problem that has\n   occurred or that\
    \ is incipient.  In perfective optimization, the goal\n   is to improve network\
    \ performance even when explicit problems do not\n   exist and are not anticipated.\n\
    \   Network performance optimization is a continual process, as noted\n   previously.\
    \  Performance optimization iterations may consist of\n   real-time optimization\
    \ sub-processes and non-real-time network\n   planning sub-processes.  The difference\
    \ between real-time\n   optimization and network planning is primarily in the\
    \ relative time-\n   scale in which they operate and in the granularity of actions.\
    \  One\n   of the objectives of a real-time optimization sub-process is to\n \
    \  control the mapping and distribution of traffic over the existing\n   network\
    \ infrastructure to avoid and/or relieve congestion, to assure\n   satisfactory\
    \ service delivery, and to optimize resource utilization.\n   Real-time optimization\
    \ is needed because random incidents such as\n   fiber cuts or shifts in traffic\
    \ demand will occur irrespective of how\n   well a network is designed.  These\
    \ incidents can cause congestion and\n   other problems to manifest in an operational\
    \ network.  Real-time\n   optimization must solve such problems in small to medium\
    \ time-scales\n   ranging from micro-seconds to minutes or hours.  Examples of\
    \ real-\n   time optimization include queue management, IGP/BGP metric tuning,\n\
    \   and using technologies such as MPLS explicit LSPs to change the paths\n  \
    \ of some traffic trunks [XIAO].\n   One of the functions of the network planning\
    \ sub-process is to\n   initiate actions to systematically evolve the architecture,\n\
    \   technology, topology, and capacity of a network.  When a problem\n   exists\
    \ in the network, real-time optimization should provide an\n   immediate remedy.\
    \  Because a prompt response is necessary, the real-\n   time solution may not\
    \ be the best possible solution.  Network\n   planning may subsequently be needed\
    \ to refine the solution and\n   improve the situation.  Network planning is also\
    \ required to expand\n   the network to support traffic growth and changes in\
    \ traffic\n   distribution over time.  As previously noted, a change in the\n\
    \   topology and/or capacity of the network may be the outcome of network\n  \
    \ planning.\n   Clearly, network planning and real-time performance optimization\
    \ are\n   mutually complementary activities.  A well-planned and designed\n  \
    \ network makes real-time optimization easier, while a systematic\n   approach\
    \ to real-time network performance optimization allows network\n   planning to\
    \ focus on long term issues rather than tactical\n   considerations.  Systematic\
    \ real-time network performance\n   optimization also provides valuable inputs\
    \ and insights toward\n   network planning.\n   Stability is an important consideration\
    \ in real-time network\n   performance optimization.  This aspect will be repeatedly\
    \ addressed\n   throughout this memo.\n"
- title: 4.0 Historical Review and Recent Developments
  contents:
  - "4.0 Historical Review and Recent Developments\n   This section briefly reviews\
    \ different traffic engineering approaches\n   proposed and implemented in telecommunications\
    \ and computer networks.\n   The discussion is not intended to be comprehensive.\
    \  It is primarily\n   intended to illuminate pre-existing perspectives and prior\
    \ art\n   concerning traffic engineering in the Internet and in legacy\n   telecommunications\
    \ networks.\n"
- title: 4.1 Traffic Engineering in Classical Telephone Networks
  contents:
  - "4.1 Traffic Engineering in Classical Telephone Networks\n   This subsection presents\
    \ a brief overview of traffic engineering in\n   telephone networks which often\
    \ relates to the way user traffic is\n   steered from an originating node to the\
    \ terminating node.  This\n   subsection presents a brief overview of this topic.\
    \  A detailed\n   description of the various routing strategies applied in telephone\n\
    \   networks is included in the book by G. Ash [ASH2].\n   The early telephone\
    \ network relied on static hierarchical routing,\n   whereby routing patterns\
    \ remained fixed independent of the state of\n   the network or time of day. \
    \ The hierarchy was intended to\n   accommodate overflow traffic, improve network\
    \ reliability via\n   alternate routes, and prevent call looping by employing\
    \ strict\n   hierarchical rules.  The network was typically over-provisioned since\n\
    \   a given fixed route had to be dimensioned so that it could carry user\n  \
    \ traffic during a busy hour of any busy day.  Hierarchical routing in\n   the\
    \ telephony network was found to be too rigid upon the advent of\n   digital switches\
    \ and stored program control which were able to manage\n   more complicated traffic\
    \ engineering rules.\n   Dynamic routing was introduced to alleviate the routing\
    \ inflexibility\n   in the static hierarchical routing so that the network would\
    \ operate\n   more efficiently.  This resulted in significant economic gains\n\
    \   [HUSS87].  Dynamic routing typically reduces the overall loss\n   probability\
    \ by 10 to 20 percent (compared to static hierarchical\n   routing).  Dynamic\
    \ routing can also improve network resilience by\n   recalculating routes on a\
    \ per-call basis and periodically updating\n   routes.\n   There are three main\
    \ types of dynamic routing in the telephone\n   network.  They are time-dependent\
    \ routing, state-dependent routing\n   (SDR), and event dependent routing (EDR).\n\
    \   In time-dependent routing, regular variations in traffic loads (such\n   as\
    \ time of day or day of week) are exploited in pre-planned routing\n   tables.\
    \  In state-dependent routing, routing tables are updated\n   online according\
    \ to the current state of the network (e.g., traffic\n   demand, utilization,\
    \ etc.).  In event dependent routing, routing\n   changes are incepted by events\
    \ (such as call setups encountering\n   congested or blocked links) whereupon\
    \ new paths are searched out\n   using learning models.  EDR methods are real-time\
    \ adaptive, but they\n   do not require global state information as does SDR.\
    \  Examples of EDR\n   schemes include the dynamic alternate routing (DAR) from\
    \ BT, the\n   state-and-time dependent routing (STR) from NTT, and the success-to-\n\
    \   the-top (STT) routing from AT&T.\n   Dynamic non-hierarchical routing (DNHR)\
    \ is an example of dynamic\n   routing that was introduced in the AT&T toll network\
    \ in the 1980's to\n   respond to time-dependent information such as regular load\
    \ variations\n   as a function of time.  Time-dependent information in terms of\
    \ load\n   may be divided into three time scales: hourly, weekly, and yearly.\n\
    \   Correspondingly, three algorithms are defined to pre-plan the routing\n  \
    \ tables.  The network design algorithm operates over a year-long\n   interval\
    \ while the demand servicing algorithm operates on a weekly\n   basis to fine\
    \ tune link sizes and routing tables to correct forecast\n   errors on the yearly\
    \ basis.  At the smallest time scale, the routing\n   algorithm is used to make\
    \ limited adjustments based on daily traffic\n   variations.  Network design and\
    \ demand servicing are computed using\n   offline calculations.  Typically, the\
    \ calculations require extensive\n   searches on possible routes.  On the other\
    \ hand, routing may need\n   online calculations to handle crankback.  DNHR adopts\
    \ a \"two-link\"\n   approach whereby a path can consist of two links at most.\
    \  The\n   routing algorithm presents an ordered list of route choices between\n\
    \   an originating switch and a terminating switch.  If a call overflows,\n  \
    \ a via switch (a tandem exchange between the originating switch and\n   the terminating\
    \ switch) would send a crankback signal to the\n   originating switch.  This switch\
    \ would then select the next route,\n   and so on, until there are no alternative\
    \ routes available in which\n   the call is blocked.\n"
- title: 4.2 Evolution of Traffic Engineering in Packet Networks
  contents:
  - "4.2 Evolution of Traffic Engineering in Packet Networks\n   This subsection reviews\
    \ related prior work that was intended to\n   improve the performance of data\
    \ networks.  Indeed, optimization of\n   the performance of data networks started\
    \ in the early days of the\n   ARPANET.  Other early commercial networks such\
    \ as SNA also recognized\n   the importance of performance optimization and service\n\
    \   differentiation.\n   In terms of traffic management, the Internet has been\
    \ a best effort\n   service environment until recently.  In particular, very limited\n\
    \   traffic management capabilities existed in IP networks to provide\n   differentiated\
    \ queue management and scheduling services to packets\n   belonging to different\
    \ classes.\n   In terms of routing control, the Internet has employed distributed\n\
    \   protocols for intra-domain routing.  These protocols are highly\n   scalable\
    \ and resilient.  However, they are based on simple algorithms\n   for path selection\
    \ which have very limited functionality to allow\n   flexible control of the path\
    \ selection process.\n   In the following subsections, the evolution of practical\
    \ traffic\n   engineering mechanisms in IP networks and its predecessors are\n\
    \   reviewed.\n"
- title: 4.2.1 Adaptive Routing in the ARPANET
  contents:
  - "4.2.1 Adaptive Routing in the ARPANET\n   The early ARPANET recognized the importance\
    \ of adaptive routing where\n   routing decisions were based on the current state\
    \ of the network\n   [MCQ80].  Early minimum delay routing approaches forwarded\
    \ each\n   packet to its destination along a path for which the total estimated\n\
    \   transit time was the smallest.  Each node maintained a table of\n   network\
    \ delays, representing the estimated delay that a packet would\n   experience\
    \ along a given path toward its destination.  The minimum\n   delay table was\
    \ periodically transmitted by a node to its neighbors.\n   The shortest path,\
    \ in terms of hop count, was also propagated to give\n   the connectivity information.\n\
    \   One drawback to this approach is that dynamic link metrics tend to\n   create\
    \ \"traffic magnets\" causing congestion to be shifted from one\n   location of\
    \ a network to another location, resulting in oscillation\n   and network instability.\n"
- title: 4.2.2 Dynamic Routing in the Internet
  contents:
  - "4.2.2 Dynamic Routing in the Internet\n   The Internet evolved from the APARNET\
    \ and adopted dynamic routing\n   algorithms with distributed control to determine\
    \ the paths that\n   packets should take en-route to their destinations.  The\
    \ routing\n   algorithms are adaptations of shortest path algorithms where costs\n\
    \   are based on link metrics.  The link metric can be based on static or\n  \
    \ dynamic quantities.  The link metric based on static quantities may\n   be assigned\
    \ administratively according to local criteria.  The link\n   metric based on\
    \ dynamic quantities may be a function of a network\n   congestion measure such\
    \ as delay or packet loss.\n   It was apparent early that static link metric assignment\
    \ was\n   inadequate because it can easily lead to unfavorable scenarios in\n\
    \   which some links become congested while others remain lightly loaded.\n  \
    \ One of the many reasons for the inadequacy of static link metrics is\n   that\
    \ link metric assignment was often done without considering the\n   traffic matrix\
    \ in the network.  Also, the routing protocols did not\n   take traffic attributes\
    \ and capacity constraints into account when\n   making routing decisions.  This\
    \ results in traffic concentration\n   being localized in subsets of the network\
    \ infrastructure and\n   potentially causing congestion.  Even if link metrics\
    \ are assigned in\n   accordance with the traffic matrix, unbalanced loads in\
    \ the network\n   can still occur due to a number factors including:\n      -\
    \  Resources may not be deployed in the most optimal locations\n         from\
    \ a routing perspective.\n      -  Forecasting errors in traffic volume and/or\
    \ traffic\n         distribution.\n      -  Dynamics in traffic matrix due to\
    \ the temporal nature of\n         traffic patterns, BGP policy change from peers,\
    \ etc.\n   The inadequacy of the legacy Internet interior gateway routing system\n\
    \   is one of the factors motivating the interest in path oriented\n   technology\
    \ with explicit routing and constraint-based routing\n   capability such as MPLS.\n"
- title: 4.2.3 ToS Routing
  contents:
  - "4.2.3 ToS Routing\n   Type-of-Service (ToS) routing involves different routes\
    \ going to the\n   same destination with selection dependent upon the ToS field\
    \ of an IP\n   packet [RFC-2474].  The ToS classes may be classified as low delay\n\
    \   and high throughput.  Each link is associated with multiple link\n   costs\
    \ and each link cost is used to compute routes for a particular\n   ToS.  A separate\
    \ shortest path tree is computed for each ToS.  The\n   shortest path algorithm\
    \ must be run for each ToS resulting in very\n   expensive computation.  Classical\
    \ ToS-based routing is now outdated\n   as the IP header field has been replaced\
    \ by a Diffserv field.\n   Effective traffic engineering is difficult to perform\
    \ in classical\n   ToS-based routing because each class still relies exclusively\
    \ on\n   shortest path routing which results in localization of traffic\n   concentration\
    \ within the network.\n"
- title: 4.2.4 Equal Cost Multi-Path
  contents:
  - "4.2.4 Equal Cost Multi-Path\n   Equal Cost Multi-Path (ECMP) is another technique\
    \ that attempts to\n   address the deficiency in the Shortest Path First (SPF)\
    \ interior\n   gateway routing systems [RFC-2328].  In the classical SPF algorithm,\n\
    \   if two or more shortest paths exist to a given destination, the\n   algorithm\
    \ will choose one of them.  The algorithm is modified\n   slightly in ECMP so\
    \ that if two or more equal cost shortest paths\n   exist between two nodes, the\
    \ traffic between the nodes is distributed\n   among the multiple equal-cost paths.\
    \  Traffic distribution across the\n   equal-cost paths is usually performed in\
    \ one of two ways: (1)\n   packet-based in a round-robin fashion, or (2) flow-based\
    \ using\n   hashing on source and destination IP addresses and possibly other\n\
    \   fields of the IP header.  The first approach can easily cause out-\n   of-order\
    \ packets while the second approach is dependent upon the\n   number and distribution\
    \ of flows.  Flow-based load sharing may be\n   unpredictable in an enterprise\
    \ network where the number of flows is\n   relatively small and less heterogeneous\
    \ (for example, hashing may not\n   be uniform), but it is generally effective\
    \ in core public networks\n   where the number of flows is large and heterogeneous.\n\
    \   In ECMP, link costs are static and bandwidth constraints are not\n   considered,\
    \ so ECMP attempts to distribute the traffic as equally as\n   possible among\
    \ the equal-cost paths independent of the congestion\n   status of each path.\
    \  As a result, given two equal-cost paths, it is\n   possible that one of the\
    \ paths will be more congested than the other.\n   Another drawback of ECMP is\
    \ that load sharing cannot be achieved on\n   multiple paths which have non-identical\
    \ costs.\n"
- title: 4.2.5 Nimrod
  contents:
  - "4.2.5 Nimrod\n   Nimrod is a routing system developed to provide heterogeneous\
    \ service\n   specific routing in the Internet, while taking multiple constraints\n\
    \   into account [RFC-1992].  Essentially, Nimrod is a link state routing\n  \
    \ protocol which supports path oriented packet forwarding.  It uses the\n   concept\
    \ of maps to represent network connectivity and services at\n   multiple levels\
    \ of abstraction.  Mechanisms are provided to allow\n   restriction of the distribution\
    \ of routing information.\n   Even though Nimrod did not enjoy deployment in the\
    \ public Internet, a\n   number of key concepts incorporated into the Nimrod architecture,\n\
    \   such as explicit routing which allows selection of paths at\n   originating\
    \ nodes, are beginning to find applications in some recent\n   constraint-based\
    \ routing initiatives.\n"
- title: 4.3 Overlay Model
  contents:
  - "4.3 Overlay Model\n   In the overlay model, a virtual-circuit network, such as\
    \ ATM, frame\n   relay, or WDM, provides virtual-circuit connectivity between\
    \ routers\n   that are located at the edges of a virtual-circuit cloud.  In this\n\
    \   mode, two routers that are connected through a virtual circuit see a\n   direct\
    \ adjacency between themselves independent of the physical route\n   taken by\
    \ the virtual circuit through the ATM, frame relay, or WDM\n   network.  Thus,\
    \ the overlay model essentially decouples the logical\n   topology that routers\
    \ see from the physical topology that the ATM,\n   frame relay, or WDM network\
    \ manages.  The overlay model based on ATM\n   or frame relay enables a network\
    \ administrator or an automaton to\n   employ traffic engineering concepts to\
    \ perform path optimization by\n   re-configuring or rearranging the virtual circuits\
    \ so that a virtual\n   circuit on a congested or sub-optimal physical link can\
    \ be re-routed\n   to a less congested or more optimal one.  In the overlay model,\n\
    \   traffic engineering is also employed to establish relationships\n   between\
    \ the traffic management parameters (e.g., PCR, SCR, and MBS\n   for ATM) of the\
    \ virtual-circuit technology and the actual traffic\n   that traverses each circuit.\
    \  These relationships can be established\n   based upon known or projected traffic\
    \ profiles, and some other\n   factors.\n   The overlay model using IP over ATM\
    \ requires the management of two\n   separate networks with different technologies\
    \ (IP and ATM) resulting\n   in increased operational complexity and cost.  In\
    \ the fully-meshed\n   overlay model, each router would peer to every other router\
    \ in the\n   network, so that the total number of adjacencies is a quadratic\n\
    \   function of the number of routers.  Some of the issues with the\n   overlay\
    \ model are discussed in [AWD2].\n"
- title: 4.4 Constrained-Based Routing
  contents:
  - "4.4 Constrained-Based Routing\n   Constraint-based routing refers to a class\
    \ of routing systems that\n   compute routes through a network subject to the\
    \ satisfaction of a set\n   of constraints and requirements.  In the most general\
    \ setting,\n   constraint-based routing may also seek to optimize overall network\n\
    \   performance while minimizing costs.\n   The constraints and requirements may\
    \ be imposed by the network itself\n   or by administrative policies.  Constraints\
    \ may include bandwidth,\n   hop count, delay, and policy instruments such as\
    \ resource class\n   attributes.  Constraints may also include domain specific\
    \ attributes\n   of certain network technologies and contexts which impose\n \
    \  restrictions on the solution space of the routing function.  Path\n   oriented\
    \ technologies such as MPLS have made constraint-based routing\n   feasible and\
    \ attractive in public IP networks.\n   The concept of constraint-based routing\
    \ within the context of MPLS\n   traffic engineering requirements in IP networks\
    \ was first defined in\n   [RFC-2702].\n   Unlike QoS routing (for example, see\
    \ [RFC-2386] and [MA]) which\n   generally addresses the issue of routing individual\
    \ traffic flows to\n   satisfy prescribed flow based QoS requirements subject\
    \ to network\n   resource availability, constraint-based routing is applicable\
    \ to\n   traffic aggregates as well as flows and may be subject to a wide\n  \
    \ variety of constraints which may include policy restrictions.\n"
- title: 4.5 Overview of Other IETF Projects Related to Traffic Engineering
  contents:
  - "4.5 Overview of Other IETF Projects Related to Traffic Engineering\n   This subsection\
    \ reviews a number of IETF activities pertinent to\n   Internet traffic engineering.\
    \  These activities are primarily\n   intended to evolve the IP architecture to\
    \ support new service\n   definitions which allow preferential or differentiated\
    \ treatment to\n   be accorded to certain types of traffic.\n"
- title: 4.5.1 Integrated Services
  contents:
  - "4.5.1 Integrated Services\n   The IETF Integrated Services working group developed\
    \ the integrated\n   services (Intserv) model.  This model requires resources,\
    \ such as\n   bandwidth and buffers, to be reserved a priori for a given traffic\n\
    \   flow to ensure that the quality of service requested by the traffic\n   flow\
    \ is satisfied.  The integrated services model includes additional\n   components\
    \ beyond those used in the best-effort model such as packet\n   classifiers, packet\
    \ schedulers, and admission control.  A packet\n   classifier is used to identify\
    \ flows that are to receive a certain\n   level of service.  A packet scheduler\
    \ handles the scheduling of\n   service to different packet flows to ensure that\
    \ QoS commitments are\n   met.  Admission control is used to determine whether\
    \ a router has the\n   necessary resources to accept a new flow.\n   Two services\
    \ have been defined under the Integrated Services model:\n   guaranteed service\
    \ [RFC-2212] and controlled-load service [RFC-2211].\n   The guaranteed service\
    \ can be used for applications requiring bounded\n   packet delivery time.  For\
    \ this type of application, data that is\n   delivered to the application after\
    \ a pre-defined amount of time has\n   elapsed is usually considered worthless.\
    \  Therefore, guaranteed\n   service was intended to provide a firm quantitative\
    \ bound on the\n   end-to-end packet delay for a flow.  This is accomplished by\n\
    \   controlling the queuing delay on network elements along the data flow\n  \
    \ path.  The guaranteed service model does not, however, provide\n   bounds on\
    \ jitter (inter-arrival times between consecutive packets).\n   The controlled-load\
    \ service can be used for adaptive applications\n   that can tolerate some delay\
    \ but are sensitive to traffic overload\n   conditions.  This type of application\
    \ typically functions\n   satisfactorily when the network is lightly loaded but\
    \ its performance\n   degrades significantly when the network is heavily loaded.\n\
    \   Controlled-load service, therefore, has been designed to provide\n   approximately\
    \ the same service as best-effort service in a lightly\n   loaded network regardless\
    \ of actual network conditions.  Controlled-\n   load service is described qualitatively\
    \ in that no target values of\n   delay or loss are specified.\n   The main issue\
    \ with the Integrated Services model has been\n   scalability [RFC-2998], especially\
    \ in large public IP networks which\n   may potentially have millions of active\
    \ micro-flows in transit\n   concurrently.\n   A notable feature of the Integrated\
    \ Services model is that it\n   requires explicit signaling of QoS requirements\
    \ from end systems to\n   routers [RFC-2753].  The Resource Reservation Protocol\
    \ (RSVP)\n   performs this signaling function and is a critical component of the\n\
    \   Integrated Services model.  The RSVP protocol is described next.\n"
- title: 4.5.2 RSVP
  contents:
  - "4.5.2 RSVP\n   RSVP is a soft state signaling protocol [RFC-2205].  It supports\n\
    \   receiver initiated establishment of resource reservations for both\n   multicast\
    \ and unicast flows.  RSVP was originally developed as a\n   signaling protocol\
    \ within the integrated services framework for\n   applications to communicate\
    \ QoS requirements to the network and for\n   the network to reserve relevant\
    \ resources to satisfy the QoS\n   requirements [RFC-2205].\n   Under RSVP, the\
    \ sender or source node sends a PATH message to the\n   receiver with the same\
    \ source and destination addresses as the\n   traffic which the sender will generate.\
    \  The PATH message contains:\n   (1) a sender Tspec specifying the characteristics\
    \ of the traffic, (2)\n   a sender Template specifying the format of the traffic,\
    \ and (3) an\n   optional Adspec which is used to support the concept of one pass\
    \ with\n   advertising\" (OPWA) [RFC-2205].  Every intermediate router along the\n\
    \   path forwards the PATH Message to the next hop determined by the\n   routing\
    \ protocol.  Upon receiving a PATH Message, the receiver\n   responds with a RESV\
    \ message which includes a flow descriptor used to\n   request resource reservations.\
    \  The RESV message travels to the\n   sender or source node in the opposite direction\
    \ along the path that\n   the PATH message traversed.  Every intermediate router\
    \ along the path\n   can reject or accept the reservation request of the RESV\
    \ message.  If\n   the request is rejected, the rejecting router will send an\
    \ error\n   message to the receiver and the signaling process will terminate.\
    \  If\n   the request is accepted, link bandwidth and buffer space are\n   allocated\
    \ for the flow and the related flow state information is\n   installed in the\
    \ router.\n   One of the issues with the original RSVP specification was\n   Scalability.\
    \  This is because reservations were required for micro-\n   flows, so that the\
    \ amount of state maintained by network elements\n   tends to increase linearly\
    \ with the number of micro-flows.  These\n   issues are described in [RFC-2961].\n\
    \   Recently, RSVP has been modified and extended in several ways to\n   mitigate\
    \ the scaling problems.  As a result, it is becoming a\n   versatile signaling\
    \ protocol for the Internet.  For example, RSVP has\n   been extended to reserve\
    \ resources for aggregation of flows, to set\n   up MPLS explicit label switched\
    \ paths, and to perform other signaling\n   functions within the Internet.  There\
    \ are also a number of proposals\n   to reduce the amount of refresh messages\
    \ required to maintain\n   established RSVP sessions [RFC-2961].\n   A number\
    \ of IETF working groups have been engaged in activities\n   related to the RSVP\
    \ protocol.  These include the original RSVP\n   working group, the MPLS working\
    \ group, the Resource Allocation\n   Protocol working group, and the Policy Framework\
    \ working group.\n"
- title: 4.5.3 Differentiated Services
  contents:
  - "4.5.3 Differentiated Services\n   The goal of the Differentiated Services (Diffserv)\
    \ effort within the\n   IETF is to devise scalable mechanisms for categorization\
    \ of traffic\n   into behavior aggregates, which ultimately allows each behavior\n\
    \   aggregate to be treated differently, especially when there is a\n   shortage\
    \ of resources such as link bandwidth and buffer space [RFC-\n   2475].  One of\
    \ the primary motivations for the Diffserv effort was to\n   devise alternative\
    \ mechanisms for service differentiation in the\n   Internet that mitigate the\
    \ scalability issues encountered with the\n   Intserv model.\n   The IETF Diffserv\
    \ working group has defined a Differentiated Services\n   field in the IP header\
    \ (DS field).  The DS field consists of six bits\n   of the part of the IP header\
    \ formerly known as TOS octet.  The DS\n   field is used to indicate the forwarding\
    \ treatment that a packet\n   should receive at a node [RFC-2474].  The Diffserv\
    \ working group has\n   also standardized a number of Per-Hop Behavior (PHB) groups.\
    \  Using\n   the PHBs, several classes of services can be defined using different\n\
    \   classification, policing, shaping, and scheduling rules.\n   For an end-user\
    \ of network services to receive Differentiated\n   Services from its Internet\
    \ Service Provider (ISP), it may be\n   necessary for the user to have a Service\
    \ Level Agreement (SLA) with\n   the ISP.  An SLA may explicitly or implicitly\
    \ specify a Traffic\n   Conditioning Agreement (TCA) which defines classifier\
    \ rules as well\n   as metering, marking, discarding, and shaping rules.\n   Packets\
    \ are classified, and possibly policed and shaped at the\n   ingress to a Diffserv\
    \ network.  When a packet traverses the boundary\n   between different Diffserv\
    \ domains, the DS field of the packet may be\n   re-marked according to existing\
    \ agreements between the domains.\n   Differentiated Services allows only a finite\
    \ number of service\n   classes to be indicated by the DS field.  The main advantage\
    \ of the\n   Diffserv approach relative to the Intserv model is scalability.\n\
    \   Resources are allocated on a per-class basis and the amount of state\n   information\
    \ is proportional to the number of classes rather than to\n   the number of application\
    \ flows.\n   It should be obvious from the previous discussion that the Diffserv\n\
    \   model essentially deals with traffic management issues on a per hop\n   basis.\
    \  The Diffserv control model consists of a collection of\n   micro-TE control\
    \ mechanisms.  Other traffic engineering capabilities,\n   such as capacity management\
    \ (including routing control), are also\n   required in order to deliver acceptable\
    \ service quality in Diffserv\n   networks.  The concept of Per Domain Behaviors\
    \ has been introduced to\n   better capture the notion of differentiated services\
    \ across a\n   complete domain [RFC-3086].\n"
- title: 4.5.4 MPLS
  contents:
  - "4.5.4 MPLS\n   MPLS is an advanced forwarding scheme which also includes extensions\n\
    \   to conventional IP control plane protocols.  MPLS extends the\n   Internet\
    \ routing model and enhances packet forwarding and path\n   control [RFC-3031].\n\
    \   At the ingress to an MPLS domain, label switching routers (LSRs)\n   classify\
    \ IP packets into forwarding equivalence classes (FECs) based\n   on a variety\
    \ of factors, including, e.g., a combination of the\n   information carried in\
    \ the IP header of the packets and the local\n   routing information maintained\
    \ by the LSRs.  An MPLS label is then\n   prepended to each packet according to\
    \ their forwarding equivalence\n   classes.  In a non-ATM/FR environment, the\
    \ label is 32 bits long and\n   contains a 20-bit label field, a 3-bit experimental\
    \ field (formerly\n   known as Class-of-Service or CoS field), a 1-bit label stack\n\
    \   indicator and an 8-bit TTL field.  In an ATM (FR) environment, the\n   label\
    \ consists of information encoded in the VCI/VPI (DLCI) field.\n   An MPLS capable\
    \ router (an LSR) examines the label and possibly the\n   experimental field and\
    \ uses this information to make packet\n   forwarding decisions.\n   An LSR makes\
    \ forwarding decisions by using the label prepended to\n   packets as the index\
    \ into a local next hop label forwarding entry\n   (NHLFE).  The packet is then\
    \ processed as specified in the NHLFE.\n   The incoming label may be replaced\
    \ by an outgoing label, and the\n   packet may be switched to the next LSR.  This\
    \ label-switching process\n   is very similar to the label (VCI/VPI) swapping\
    \ process in ATM\n   networks.  Before a packet leaves an MPLS domain, its MPLS\
    \ label may\n   be removed.  A Label Switched Path (LSP) is the path between an\n\
    \   ingress LSRs and an egress LSRs through which a labeled packet\n   traverses.\
    \  The path of an explicit LSP is defined at the originating\n   (ingress) node\
    \ of the LSP.  MPLS can use a signaling protocol such as\n   RSVP or LDP to set\
    \ up LSPs.\n   MPLS is a very powerful technology for Internet traffic engineering\n\
    \   because it supports explicit LSPs which allow constraint-based\n   routing\
    \ to be implemented efficiently in IP networks [AWD2].  The\n   requirements for\
    \ traffic engineering over MPLS are described in\n   [RFC-2702].  Extensions to\
    \ RSVP to support instantiation of explicit\n   LSP are discussed in [RFC-3209].\
    \  Extensions to LDP, known as CR-LDP,\n   to support explicit LSPs are presented\
    \ in [JAM].\n"
- title: 4.5.5 IP Performance Metrics
  contents:
  - "4.5.5 IP Performance Metrics\n   The IETF IP Performance Metrics (IPPM) working\
    \ group has been\n   developing a set of standard metrics that can be used to\
    \ monitor the\n   quality, performance, and reliability of Internet services.\
    \  These\n   metrics can be applied by network operators, end-users, and\n   independent\
    \ testing groups to provide users and service providers\n   with a common understanding\
    \ of the performance and reliability of the\n   Internet component 'clouds' they\
    \ use/provide [RFC-2330].  The\n   criteria for performance metrics developed\
    \ by the IPPM WG are\n   described in [RFC-2330].  Examples of performance metrics\
    \ include\n   one-way packet\n   loss [RFC-2680], one-way delay [RFC-2679], and\
    \ connectivity measures\n   between two nodes [RFC-2678].  Other metrics include\
    \ second-order\n   measures of packet loss and delay.\n   Some of the performance\
    \ metrics specified by the IPPM WG are useful\n   for specifying Service Level\
    \ Agreements (SLAs).  SLAs are sets of\n   service level objectives negotiated\
    \ between users and service\n   providers, wherein each objective is a combination\
    \ of one or more\n   performance metrics, possibly subject to certain constraints.\n"
- title: 4.5.6 Flow Measurement
  contents:
  - "4.5.6 Flow Measurement\n   The IETF Real Time Flow Measurement (RTFM) working\
    \ group has produced\n   an architecture document defining a method to specify\
    \ traffic flows\n   as well as a number of components for flow measurement (meters,\
    \ meter\n   readers, manager) [RFC-2722].  A flow measurement system enables\n\
    \   network traffic flows to be measured and analyzed at the flow level\n   for\
    \ a variety of purposes.  As noted in RFC 2722, a flow measurement\n   system\
    \ can be very useful in the following contexts: (1)\n   understanding the behavior\
    \ of existing networks, (2) planning for\n   network development and expansion,\
    \ (3) quantification of network\n   performance, (4) verifying the quality of\
    \ network service, and (5)\n   attribution of network usage to users.\n   A flow\
    \ measurement system consists of meters, meter readers, and\n   managers.  A meter\
    \ observes packets passing through a measurement\n   point, classifies them into\
    \ certain groups, accumulates certain usage\n   data (such as the number of packets\
    \ and bytes for each group), and\n   stores the usage data in a flow table.  A\
    \ group may represent a user\n   application, a host, a network, a group of networks,\
    \ etc.  A meter\n   reader gathers usage data from various meters so it can be\
    \ made\n   available for analysis.  A manager is responsible for configuring and\n\
    \   controlling meters and meter readers.  The instructions received by a\n  \
    \ meter from a manager include flow specification, meter control\n   parameters,\
    \ and sampling techniques.  The instructions received by a\n   meter reader from\
    \ a manager include the address of the meter whose\n   date is to be collected,\
    \ the frequency of data collection, and the\n   types of flows to be collected.\n"
- title: 4.5.7 Endpoint Congestion Management
  contents:
  - "4.5.7 Endpoint Congestion Management\n   [RFC-3124] is intended to provide a\
    \ set of congestion control\n   mechanisms that transport protocols can use. \
    \ It is also intended to\n   develop mechanisms for unifying congestion control\
    \ across a subset of\n   an endpoint's active unicast connections (called a congestion\
    \ group).\n   A congestion manager continuously monitors the state of the path\
    \ for\n   each congestion group under its control.  The manager uses that\n  \
    \ information to instruct a scheduler on how to partition bandwidth\n   among\
    \ the connections of that congestion group.\n"
- title: 4.6 Overview of ITU Activities Related to Traffic Engineering
  contents:
  - "4.6 Overview of ITU Activities Related to Traffic Engineering\n   This section\
    \ provides an overview of prior work within the ITU-T\n   pertaining to traffic\
    \ engineering in traditional telecommunications\n   networks.\n   ITU-T Recommendations\
    \ E.600 [ITU-E600], E.701 [ITU-E701], and E.801\n   [ITU-E801] address traffic\
    \ engineering issues in traditional\n   telecommunications networks.  Recommendation\
    \ E.600 provides a\n   vocabulary for describing traffic engineering concepts,\
    \ while E.701\n   defines reference connections, Grade of Service (GOS), and traffic\n\
    \   parameters for ISDN.  Recommendation E.701 uses the concept of a\n   reference\
    \ connection to identify representative cases of different\n   types of connections\
    \ without describing the specifics of their actual\n   realizations by different\
    \ physical means.  As defined in\n   Recommendation E.600, \"a connection is an\
    \ association of resources\n   providing means for communication between two or\
    \ more devices in, or\n   attached to, a telecommunication network.\"  Also, E.600\
    \ defines \"a\n   resource as any set of physically or conceptually identifiable\n\
    \   entities within a telecommunication network, the use of which can be\n   unambiguously\
    \ determined\" [ITU-E600].  There can be different types\n   of connections as\
    \ the number and types of resources in a connection\n   may vary.\n   Typically,\
    \ different network segments are involved in the path of a\n   connection.  For\
    \ example, a connection may be local, national, or\n   international.  The purposes\
    \ of reference connections are to clarify\n   and specify traffic performance\
    \ issues at various interfaces between\n   different network domains.  Each domain\
    \ may consist of one or more\n   service provider networks.\n   Reference connections\
    \ provide a basis to define grade of service\n   (GoS) parameters related to traffic\
    \ engineering within the ITU-T\n   framework.  As defined in E.600, \"GoS refers\
    \ to a number of traffic\n   engineering variables which are used to provide a\
    \ measure of the\n   adequacy of a group of resources under specified conditions.\"\
    \  These\n   GoS variables may be probability of loss, dial tone, delay, etc.\n\
    \   They are essential for network internal design and operation as well\n   as\
    \ for component performance specification.\n   GoS is different from quality of\
    \ service (QoS) in the ITU framework.\n   QoS is the performance perceivable by\
    \ a telecommunication service\n   user and expresses the user's degree of satisfaction\
    \ of the service.\n   QoS parameters focus on performance aspects observable at\
    \ the service\n   access points and network interfaces, rather than their causes\
    \ within\n   the network.  GoS, on the other hand, is a set of network oriented\n\
    \   measures which characterize the adequacy of a group of resources\n   under\
    \ specified conditions.  For a network to be effective in serving\n   its users,\
    \ the values of both GoS and QoS parameters must be related,\n   with GoS parameters\
    \ typically making a major contribution to the QoS.\n   Recommendation E.600 stipulates\
    \ that a set of GoS parameters must be\n   selected and defined on an end-to-end\
    \ basis for each major service\n   category provided by a network to assist the\
    \ network provider with\n   improving efficiency and effectiveness of the network.\
    \  Based on a\n   selected set of reference connections, suitable target values\
    \ are\n   assigned to the selected GoS parameters under normal and high load\n\
    \   conditions.  These end-to-end GoS target values are then apportioned\n   to\
    \ individual resource components of the reference connections for\n   dimensioning\
    \ purposes.\n"
- title: 4.7 Content Distribution
  contents:
  - "4.7 Content Distribution\n   The Internet is dominated by client-server interactions,\
    \ especially\n   Web traffic (in the future, more sophisticated media servers\
    \ may\n   become dominant).  The location and performance of major information\n\
    \   servers has a significant impact on the traffic patterns within the\n   Internet\
    \ as well as on the perception of service quality by end\n   users.\n   A number\
    \ of dynamic load balancing techniques have been devised to\n   improve the performance\
    \ of replicated information servers.  These\n   techniques can cause spatial traffic\
    \ characteristics to become more\n   dynamic in the Internet because information\
    \ servers can be\n   dynamically picked based upon the location of the clients,\
    \ the\n   location of the servers, the relative utilization of the servers, the\n\
    \   relative performance of different networks, and the relative\n   performance\
    \ of different parts of a network.  This process of\n   assignment of distributed\
    \ servers to clients is called Traffic\n   Directing.  It functions at the application\
    \ layer.\n   Traffic Directing schemes that allocate servers in multiple\n   geographically\
    \ dispersed locations to clients may require empirical\n   network performance\
    \ statistics to make more effective decisions.  In\n   the future, network measurement\
    \ systems may need to provide this type\n   of information.  The exact parameters\
    \ needed are not yet defined.\n   When congestion exists in the network, Traffic\
    \ Directing and Traffic\n   Engineering systems should act in a coordinated manner.\
    \  This topic\n   is for further study.\n   The issues related to location and\
    \ replication of information\n   servers, particularly web servers, are important\
    \ for Internet traffic\n   engineering because these servers contribute a substantial\
    \ proportion\n   of Internet traffic.\n"
- title: 5.0 Taxonomy of Traffic Engineering Systems
  contents:
  - "5.0 Taxonomy of Traffic Engineering Systems\n   This section presents a short\
    \ taxonomy of traffic engineering\n   systems.  A taxonomy of traffic engineering\
    \ systems can be\n   constructed based on traffic engineering styles and views\
    \ as listed\n   below:\n      - Time-dependent vs State-dependent vs Event-dependent\n\
    \      - Offline vs Online\n      - Centralized vs Distributed\n      - Local\
    \ vs Global Information\n      - Prescriptive vs Descriptive\n      - Open Loop\
    \ vs Closed Loop\n      - Tactical vs Strategic\n   These classification systems\
    \ are described in greater detail in the\n   following subsections of this document.\n"
- title: 5.1 Time-Dependent Versus State-Dependent Versus Event Dependent
  contents:
  - "5.1 Time-Dependent Versus State-Dependent Versus Event Dependent\n   Traffic\
    \ engineering methodologies can be classified as time-\n   dependent, or state-dependent,\
    \ or event-dependent.  All TE schemes\n   are considered to be dynamic in this\
    \ document.  Static TE implies\n   that no traffic engineering methodology or\
    \ algorithm is being\n   applied.\n   In the time-dependent TE, historical information\
    \ based on periodic\n   variations in traffic, (such as time of day), is used\
    \ to pre-program\n   routing plans and other TE control mechanisms.  Additionally,\n\
    \   customer subscription or traffic projection may be used.  Pre-\n   programmed\
    \ routing plans typically change on a relatively long time\n   scale (e.g., diurnal).\
    \  Time-dependent algorithms do not attempt to\n   adapt to random variations\
    \ in traffic or changing network conditions.\n   An example of a time-dependent\
    \ algorithm is a global centralized\n   optimizer where the input to the system\
    \ is a traffic matrix and\n   multi-class QoS requirements as described [MR99].\n\
    \   State-dependent TE adapts the routing plans for packets based on the\n   current\
    \ state of the network.  The current state of the network\n   provides additional\
    \ information on variations in actual traffic\n   (i.e., perturbations from regular\
    \ variations) that could not be\n   predicted using historical information.  Constraint-based\
    \ routing is\n   an example of state-dependent TE operating in a relatively long\
    \ time\n   scale.  An example operating in a relatively short time scale is a\n\
    \   load-balancing algorithm described in [MATE].\n   The state of the network\
    \ can be based on parameters such as\n   utilization, packet delay, packet loss,\
    \ etc.  These parameters can be\n   obtained in several ways.  For example, each\
    \ router may flood these\n   parameters periodically or by means of some kind\
    \ of trigger to other\n   routers.  Another approach is for a particular router\
    \ performing\n   adaptive TE to send probe packets along a path to gather the\
    \ state of\n   that path.  Still another approach is for a management system to\n\
    \   gather relevant information from network elements.\n   Expeditious and accurate\
    \ gathering and distribution of state\n   information is critical for adaptive\
    \ TE due to the dynamic nature of\n   network conditions.  State-dependent algorithms\
    \ may be applied to\n   increase network efficiency and resilience.  Time-dependent\n\
    \   algorithms are more suitable for predictable traffic variations.  On\n   the\
    \ other hand, state-dependent algorithms are more suitable for\n   adapting to\
    \ the prevailing network state.\n   Event-dependent TE methods can also be used\
    \ for TE path selection.\n   Event-dependent TE methods are distinct from time-dependent\
    \ and\n   state-dependent TE methods in the manner in which paths are selected.\n\
    \   These algorithms are adaptive and distributed in nature and typically\n  \
    \ use learning models to find good paths for TE in a network.  While\n   state-dependent\
    \ TE models typically use available-link-bandwidth\n   (ALB) flooding for TE path\
    \ selection, event-dependent TE methods do\n   not require ALB flooding.  Rather,\
    \ event-dependent TE methods\n   typically search out capacity by learning models,\
    \ as in the success-\n   to-the-top (STT) method.  ALB flooding can be resource\
    \ intensive,\n   since it requires link bandwidth to carry LSAs, processor capacity\
    \ to\n   process LSAs, and the overhead can limit area/autonomous system (AS)\n\
    \   size.  Modeling results suggest that event-dependent TE methods could\n  \
    \ lead to a reduction in ALB flooding overhead without loss of network\n   throughput\
    \ performance [ASH3].\n"
- title: 5.2 Offline Versus Online
  contents:
  - "5.2 Offline Versus Online\n   Traffic engineering requires the computation of\
    \ routing plans.  The\n   computation may be performed offline or online.  The\
    \ computation can\n   be done offline for scenarios where routing plans need not\
    \ be\n   executed in real-time.  For example, routing plans computed from\n  \
    \ forecast information may be computed offline.  Typically, offline\n   computation\
    \ is also used to perform extensive searches on multi-\n   dimensional solution\
    \ spaces.\n   Online computation is required when the routing plans must adapt\
    \ to\n   changing network conditions as in state-dependent algorithms.  Unlike\n\
    \   offline computation (which can be computationally demanding), online\n   computation\
    \ is geared toward relative simple and fast calculations to\n   select routes,\
    \ fine-tune the allocations of resources, and perform\n   load balancing.\n"
- title: 5.3 Centralized Versus Distributed
  contents:
  - "5.3 Centralized Versus Distributed\n   Centralized control has a central authority\
    \ which determines routing\n   plans and perhaps other TE control parameters on\
    \ behalf of each\n   router.  The central authority collects the network-state\
    \ information\n   from all routers periodically and returns the routing information\
    \ to\n   the routers.  The routing update cycle is a critical parameter\n   directly\
    \ impacting the performance of the network being controlled.\n   Centralized control\
    \ may need high processing power and high bandwidth\n   control channels.\n  \
    \ Distributed control determines route selection by each router\n   autonomously\
    \ based on the routers view of the state of the network.\n   The network state\
    \ information may be obtained by the router using a\n   probing method or distributed\
    \ by other routers on a periodic basis\n   using link state advertisements.  Network\
    \ state information may also\n   be disseminated under exceptional conditions.\n"
- title: 5.4 Local Versus Global
  contents:
  - "5.4 Local Versus Global\n   Traffic engineering algorithms may require local\
    \ or global network-\n   state information.\n   Local information pertains to\
    \ the state of a portion of the domain.\n   Examples include the bandwidth and\
    \ packet loss rate of a particular\n   path.  Local state information may be sufficient\
    \ for certain\n   instances of distributed-controlled TEs.\n   Global information\
    \ pertains to the state of the entire domain\n   undergoing traffic engineering.\
    \  Examples include a global traffic\n   matrix and loading information on each\
    \ link throughout the domain of\n   interest.  Global state information is typically\
    \ required with\n   centralized control.  Distributed TE systems may also need\
    \ global\n   information in some cases.\n"
- title: 5.5 Prescriptive Versus Descriptive
  contents:
  - "5.5 Prescriptive Versus Descriptive\n   TE systems may also be classified as\
    \ prescriptive or descriptive.\n   Prescriptive traffic engineering evaluates\
    \ alternatives and\n   recommends a course of action.  Prescriptive traffic engineering\
    \ can\n   be further categorized as either corrective or perfective.\n   Corrective\
    \ TE prescribes a course of action to address an existing or\n   predicted anomaly.\
    \  Perfective TE prescribes a course of action to\n   evolve and improve network\
    \ performance even when no anomalies are\n   evident.\n   Descriptive traffic\
    \ engineering, on the other hand, characterizes the\n   state of the network and\
    \ assesses the impact of various policies\n   without recommending any particular\
    \ course of action.\n"
- title: 5.6 Open-Loop Versus Closed-Loop
  contents:
  - "5.6 Open-Loop Versus Closed-Loop\n   Open-loop traffic engineering control is\
    \ where control action does\n   not use feedback information from the current\
    \ network state.  The\n   control action may use its own local information for\
    \ accounting\n   purposes, however.\n   Closed-loop traffic engineering control\
    \ is where control action\n   utilizes feedback information from the network state.\
    \  The feedback\n   information may be in the form of historical information or\
    \ current\n   measurement.\n"
- title: 5.7 Tactical vs Strategic
  contents:
  - "5.7 Tactical vs Strategic\n   Tactical traffic engineering aims to address specific\
    \ performance\n   problems (such as hot-spots) that occur in the network from\
    \ a\n   tactical perspective, without consideration of overall strategic\n   imperatives.\
    \  Without proper planning and insights, tactical TE tends\n   to be ad hoc in\
    \ nature.\n   Strategic traffic engineering approaches the TE problem from a more\n\
    \   organized and systematic perspective, taking into consideration the\n   immediate\
    \ and longer term consequences of specific policies and\n   actions.\n"
- title: 6.0 Recommendations for Internet Traffic Engineering
  contents:
  - "6.0 Recommendations for Internet Traffic Engineering\n   This section describes\
    \ high level recommendations for traffic\n   engineering in the Internet.  These\
    \ recommendations are presented in\n   general terms.\n   The recommendations\
    \ describe the capabilities needed to solve a\n   traffic engineering problem\
    \ or to achieve a traffic engineering\n   objective.  Broadly speaking, these\
    \ recommendations can be\n   categorized as either functional and non-functional\
    \ recommendations.\n   Functional recommendations for Internet traffic engineering\
    \ describe\n   the functions that a traffic engineering system should perform.\n\
    \   These functions are needed to realize traffic engineering objectives\n   by\
    \ addressing traffic engineering problems.\n   Non-functional recommendations\
    \ for Internet traffic engineering\n   relate to the quality attributes or state\
    \ characteristics of a\n   traffic engineering system.  These recommendations\
    \ may contain\n   conflicting assertions and may sometimes be difficult to quantify\n\
    \   precisely.\n"
- title: 6.1 Generic Non-functional Recommendations
  contents:
  - "6.1 Generic Non-functional Recommendations\n   The generic non-functional recommendations\
    \ for Internet traffic\n   engineering include: usability, automation, scalability,\
    \ stability,\n   visibility, simplicity, efficiency, reliability, correctness,\n\
    \   maintainability, extensibility, interoperability, and security.  In a\n  \
    \ given context, some of these recommendations may be critical while\n   others\
    \ may be optional.  Therefore, prioritization may be required\n   during the development\
    \ phase of a traffic engineering system (or\n   components thereof) to tailor\
    \ it to a specific operational context.\n   In the following paragraphs, some\
    \ of the aspects of the non-\n   functional recommendations for Internet traffic\
    \ engineering are\n   summarized.\n   Usability: Usability is a human factor aspect\
    \ of traffic engineering\n   systems.  Usability refers to the ease with which\
    \ a traffic\n   engineering system can be deployed and operated.  In general,\
    \ it is\n   desirable to have a TE system that can be readily deployed in an\n\
    \   existing network.  It is also desirable to have a TE system that is\n   easy\
    \ to operate and maintain.\n   Automation: Whenever feasible, a traffic engineering\
    \ system should\n   automate as many traffic engineering functions as possible\
    \ to\n   minimize the amount of human effort needed to control and analyze\n \
    \  operational networks.  Automation is particularly imperative in large\n   scale\
    \ public networks because of the high cost of the human aspects\n   of network\
    \ operations and the high risk of network problems caused by\n   human errors.\
    \  Automation may entail the incorporation of automatic\n   feedback and intelligence\
    \ into some components of the traffic\n   engineering system.\n   Scalability:\
    \ Contemporary public networks are growing very fast with\n   respect to network\
    \ size and traffic volume.  Therefore, a TE system\n   should be scalable to remain\
    \ applicable as the network evolves.  In\n   particular, a TE system should remain\
    \ functional as the network\n   expands with regard to the number of routers and\
    \ links, and with\n   respect to the traffic volume.  A TE system should have\
    \ a scalable\n   architecture, should not adversely impair other functions and\n\
    \   processes in a network element, and should not consume too much\n   network\
    \ resources when collecting and distributing state information\n   or when exerting\
    \ control.\n   Stability: Stability is a very important consideration in traffic\n\
    \   engineering systems that respond to changes in the state of the\n   network.\
    \  State-dependent traffic engineering methodologies typically\n   mandate a tradeoff\
    \ between responsiveness and stability.  It is\n   strongly recommended that when\
    \ tradeoffs are warranted between\n   responsiveness and stability, that the tradeoff\
    \ should be made in\n   favor of stability (especially in public IP backbone networks).\n\
    \   Flexibility: A TE system should be flexible to allow for changes in\n   optimization\
    \ policy.  In particular, a TE system should provide\n   sufficient configuration\
    \ options so that a network administrator can\n   tailor the TE system to a particular\
    \ environment.  It may also be\n   desirable to have both online and offline TE\
    \ subsystems which can be\n   independently enabled and disabled.  TE systems\
    \ that are used in\n   multi-class networks should also have options to support\
    \ class based\n   performance evaluation and optimization.\n   Visibility: As\
    \ part of the TE system, mechanisms should exist to\n   collect statistics from\
    \ the network and to analyze these statistics\n   to determine how well the network\
    \ is functioning.  Derived statistics\n   such as traffic matrices, link utilization,\
    \ latency, packet loss, and\n   other performance measures of interest which are\
    \ determined from\n   network measurements can be used as indicators of prevailing\
    \ network\n   conditions.  Other examples of status information which should be\n\
    \   observed include existing functional routing information\n   (additionally,\
    \ in the context of MPLS existing LSP routes), etc.\n   Simplicity: Generally,\
    \ a TE system should be as simple as possible.\n   More importantly, the TE system\
    \ should be relatively easy to use\n   (i.e., clean, convenient, and intuitive\
    \ user interfaces).  Simplicity\n   in user interface does not necessarily imply\
    \ that the TE system will\n   use naive algorithms.  When complex algorithms and\
    \ internal\n   structures are used, such complexities should be hidden as much\
    \ as\n   possible from the network administrator through the user interface.\n\
    \   Interoperability: Whenever feasible, traffic engineering systems and\n   their\
    \ components should be developed with open standards based\n   interfaces to allow\
    \ interoperation with other systems and components.\n   Security: Security is\
    \ a critical consideration in traffic engineering\n   systems.  Such traffic engineering\
    \ systems typically exert control\n   over certain functional aspects of the network\
    \ to achieve the desired\n   performance objectives.  Therefore, adequate measures\
    \ must be taken\n   to safeguard the integrity of the traffic engineering system.\n\
    \   Adequate measures must also be taken to protect the network from\n   vulnerabilities\
    \ that originate from security breaches and other\n   impairments within the traffic\
    \ engineering system.\n   The remainder of this section will focus on some of\
    \ the high level\n   functional recommendations for traffic engineering.\n"
- title: 6.2 Routing Recommendations
  contents:
  - "6.2 Routing Recommendations\n   Routing control is a significant aspect of Internet\
    \ traffic\n   engineering.  Routing impacts many of the key performance measures\n\
    \   associated with networks, such as throughput, delay, and utilization.\n  \
    \ Generally, it is very difficult to provide good service quality in a\n   wide\
    \ area network without effective routing control.  A desirable\n   routing system\
    \ is one that takes traffic characteristics and network\n   constraints into account\
    \ during route selection while maintaining\n   stability.\n   Traditional shortest\
    \ path first (SPF) interior gateway protocols are\n   based on shortest path algorithms\
    \ and have limited control\n   capabilities for traffic engineering [RFC-2702,\
    \ AWD2].  These\n   limitations include :\n   1. The well known issues with pure\
    \ SPF protocols, which do not take\n      network constraints and traffic characteristics\
    \ into account\n      during route selection.  For example, since IGPs always\
    \ use the\n      shortest paths (based on administratively assigned link metrics)\n\
    \      to forward traffic, load sharing cannot be accomplished among\n      paths\
    \ of different costs.  Using shortest paths to forward traffic\n      conserves\
    \ network resources, but may cause the following problems:\n      1) If traffic\
    \ from a source to a destination exceeds the capacity\n      of a link along the\
    \ shortest path, the link (hence the shortest\n      path) becomes congested while\
    \ a longer path between these two\n      nodes may be under-utilized; 2) the shortest\
    \ paths from different\n      sources can overlap at some links.  If the total\
    \ traffic from the\n      sources exceeds the capacity of any of these links,\
    \ congestion\n      will occur.  Problems can also occur because traffic demand\n\
    \      changes over time but network topology and routing configuration\n    \
    \  cannot be changed as rapidly.  This causes the network topology\n      and\
    \ routing configuration to become sub-optimal over time, which\n      may result\
    \ in persistent congestion problems.\n   2. The Equal-Cost Multi-Path (ECMP) capability\
    \ of SPF IGPs supports\n      sharing of traffic among equal cost paths between\
    \ two nodes.\n      However, ECMP attempts to divide the traffic as equally as\n\
    \      possible among the equal cost shortest paths.  Generally, ECMP\n      does\
    \ not support configurable load sharing ratios among equal cost\n      paths.\
    \  The result is that one of the paths may carry\n      significantly more traffic\
    \ than other paths because it may also\n      carry traffic from other sources.\
    \  This situation can result in\n      congestion along the path that carries\
    \ more traffic.\n   3. Modifying IGP metrics to control traffic routing tends\
    \ to have\n      network-wide effect.  Consequently, undesirable and unanticipated\n\
    \      traffic shifts can be triggered as a result.  Recent work\n      described\
    \ in Section 8.0 may be capable of better control [FT00,\n      FT01].\n   Because\
    \ of these limitations, new capabilities are needed to enhance\n   the routing\
    \ function in IP networks.  Some of these capabilities have\n   been described\
    \ elsewhere and are summarized below.\n   Constraint-based routing is desirable\
    \ to evolve the routing\n   architecture of IP networks, especially public IP\
    \ backbones with\n   complex topologies [RFC-2702].  Constraint-based routing\
    \ computes\n   routes to fulfill requirements subject to constraints.  Constraints\n\
    \   may include bandwidth, hop count, delay, and administrative policy\n   instruments\
    \ such as resource class attributes [RFC-2702, RFC-2386].\n   This makes it possible\
    \ to select routes that satisfy a given set of\n   requirements subject to network\
    \ and administrative policy\n   constraints.  Routes computed through constraint-based\
    \ routing are\n   not necessarily the shortest paths.  Constraint-based routing\
    \ works\n   best with path oriented technologies that support explicit routing,\n\
    \   such as MPLS.\n   Constraint-based routing can also be used as a way to redistribute\n\
    \   traffic onto the infrastructure (even for best effort traffic).  For\n   example,\
    \ if the bandwidth requirements for path selection and\n   reservable bandwidth\
    \ attributes of network links are appropriately\n   defined and configured, then\
    \ congestion problems caused by uneven\n   traffic distribution may be avoided\
    \ or reduced.  In this way, the\n   performance and efficiency of the network\
    \ can be improved.\n   A number of enhancements are needed to conventional link\
    \ state IGPs,\n   such as OSPF and IS-IS, to allow them to distribute additional\
    \ state\n   information required for constraint-based routing.  These extensions\n\
    \   to OSPF were described in [KATZ] and to IS-IS in [SMIT].\n   Essentially,\
    \ these enhancements require the propagation of additional\n   information in\
    \ link state advertisements.  Specifically, in addition\n   to normal link-state\
    \ information, an enhanced IGP is required to\n   propagate topology state information\
    \ needed for constraint-based\n   routing.  Some of the additional topology state\
    \ information include\n   link attributes such as reservable bandwidth and link\
    \ resource class\n   attribute (an administratively specified property of the\
    \ link).  The\n   resource class attribute concept was defined in [RFC-2702].\
    \  The\n   additional topology state information is carried in new TLVs and\n\
    \   sub-TLVs in IS-IS, or in the Opaque LSA in OSPF [SMIT, KATZ].\n   An enhanced\
    \ link-state IGP may flood information more frequently than\n   a normal IGP.\
    \  This is because even without changes in topology,\n   changes in reservable\
    \ bandwidth or link affinity can trigger the\n   enhanced IGP to initiate flooding.\
    \  A tradeoff is typically required\n   between the timeliness of the information\
    \ flooded and the flooding\n   frequency to avoid excessive consumption of link\
    \ bandwidth and\n   computational resources, and more importantly, to avoid instability.\n\
    \   In a TE system, it is also desirable for the routing subsystem to\n   make\
    \ the load splitting ratio among multiple paths (with equal cost\n   or different\
    \ cost) configurable.  This capability gives network\n   administrators more flexibility\
    \ in the control of traffic\n   distribution across the network.  It can be very\
    \ useful for\n   avoiding/relieving congestion in certain situations.  Examples\
    \ can be\n   found in [XIAO].\n   The routing system should also have the capability\
    \ to control the\n   routes of subsets of traffic without affecting the routes\
    \ of other\n   traffic if sufficient resources exist for this purpose.  This\n\
    \   capability allows a more refined control over the distribution of\n   traffic\
    \ across the network.  For example, the ability to move traffic\n   from a source\
    \ to a destination away from its original path to another\n   path (without affecting\
    \ other traffic paths) allows traffic to be\n   moved from resource-poor network\
    \ segments to resource-rich segments.\n   Path oriented technologies such as MPLS\
    \ inherently support this\n   capability as discussed in [AWD2].\n   Additionally,\
    \ the routing subsystem should be able to select\n   different paths for different\
    \ classes of traffic (or for different\n   traffic behavior aggregates) if the\
    \ network supports multiple classes\n   of service (different behavior aggregates).\n"
- title: 6.3 Traffic Mapping Recommendations
  contents:
  - "6.3 Traffic Mapping Recommendations\n   Traffic mapping pertains to the assignment\
    \ of traffic workload onto\n   pre-established paths to meet certain requirements.\
    \  Thus, while\n   constraint-based routing deals with path selection, traffic\
    \ mapping\n   deals with the assignment of traffic to established paths which\
    \ may\n   have been selected by constraint-based routing or by some other\n  \
    \ means.  Traffic mapping can be performed by time-dependent or state-\n   dependent\
    \ mechanisms, as described in Section 5.1.\n   An important aspect of the traffic\
    \ mapping function is the ability to\n   establish multiple paths between an originating\
    \ node and a\n   destination node, and the capability to distribute the traffic\n\
    \   between the two nodes across the paths according to some policies.  A\n  \
    \ pre-condition for this scheme is the existence of flexible mechanisms\n   to\
    \ partition traffic and then assign the traffic partitions onto the\n   parallel\
    \ paths.  This requirement was noted in [RFC-2702].  When\n   traffic is assigned\
    \ to multiple parallel paths, it is recommended\n   that special care should be\
    \ taken to ensure proper ordering of\n   packets belonging to the same application\
    \ (or micro-flow) at the\n   destination node of the parallel paths.\n   As a\
    \ general rule, mechanisms that perform the traffic mapping\n   functions should\
    \ aim to map the traffic onto the network\n   infrastructure to minimize congestion.\
    \  If the total traffic load\n   cannot be accommodated, or if the routing and\
    \ mapping functions\n   cannot react fast enough to changing traffic conditions,\
    \ then a\n   traffic mapping system may rely on short time scale congestion\n\
    \   control mechanisms (such as queue management, scheduling, etc.) to\n   mitigate\
    \ congestion.  Thus, mechanisms that perform the traffic\n   mapping functions\
    \ should complement existing congestion control\n   mechanisms.  In an operational\
    \ network, it is generally desirable to\n   map the traffic onto the infrastructure\
    \ such that intra-class and\n   inter-class resource contention are minimized.\n\
    \   When traffic mapping techniques that depend on dynamic state feedback\n  \
    \ (e.g., MATE and such like) are used, special care must be taken to\n   guarantee\
    \ network stability.\n"
- title: 6.4 Measurement Recommendations
  contents:
  - "6.4 Measurement Recommendations\n   The importance of measurement in traffic\
    \ engineering has been\n   discussed throughout this document.  Mechanisms should\
    \ be provided to\n   measure and collect statistics from the network to support\
    \ the\n   traffic engineering function.  Additional capabilities may be needed\n\
    \   to help in the analysis of the statistics.  The actions of these\n   mechanisms\
    \ should not adversely affect the accuracy and integrity of\n   the statistics\
    \ collected.  The mechanisms for statistical data\n   acquisition should also\
    \ be able to scale as the network evolves.\n   Traffic statistics may be classified\
    \ according to long-term or\n   short-term time scales.  Long-term time scale\
    \ traffic statistics are\n   very useful for traffic engineering.  Long-term time\
    \ scale traffic\n   statistics may capture or reflect periodicity in network workload\n\
    \   (such as hourly, daily, and weekly variations in traffic profiles) as\n  \
    \ well as traffic trends.  Aspects of the monitored traffic statistics\n   may\
    \ also depict class of service characteristics for a network\n   supporting multiple\
    \ classes of service.  Analysis of the long-term\n   traffic statistics MAY yield\
    \ secondary statistics such as busy hour\n   characteristics, traffic growth patterns,\
    \ persistent congestion\n   problems, hot-spot, and imbalances in link utilization\
    \ caused by\n   routing anomalies.\n   A mechanism for constructing traffic matrices\
    \ for both long-term and\n   short-term traffic statistics should be in place.\
    \  In multi-service\n   IP networks, the traffic matrices may be constructed for\
    \ different\n   service classes.  Each element of a traffic matrix represents\
    \ a\n   statistic of traffic flow between a pair of abstract nodes.  An\n   abstract\
    \ node may represent a router, a collection of routers, or a\n   site in a VPN.\n\
    \   Measured traffic statistics should provide reasonable and reliable\n   indicators\
    \ of the current state of the network on the short-term\n   scale.  Some short\
    \ term traffic statistics may reflect link\n   utilization and link congestion\
    \ status.  Examples of congestion\n   indicators include excessive packet delay,\
    \ packet loss, and high\n   resource utilization.  Examples of mechanisms for\
    \ distributing this\n   kind of information include SNMP, probing techniques,\
    \ FTP, IGP link\n   state advertisements, etc.\n"
- title: 6.5 Network Survivability
  contents:
  - "6.5 Network Survivability\n   Network survivability refers to the capability\
    \ of a network to\n   maintain service continuity in the presence of faults. \
    \ This can be\n   accomplished by promptly recovering from network impairments\
    \ and\n   maintaining the required QoS for existing services after recovery.\n\
    \   Survivability has become an issue of great concern within the\n   Internet\
    \ community due to the increasing demands to carry mission\n   critical traffic,\
    \ real-time traffic, and other high priority traffic\n   over the Internet.  Survivability\
    \ can be addressed at the device\n   level by developing network elements that\
    \ are more reliable; and at\n   the network level by incorporating redundancy\
    \ into the architecture,\n   design, and operation of networks.  It is recommended\
    \ that a\n   philosophy of robustness and survivability should be adopted in the\n\
    \   architecture, design, and operation of traffic engineering that\n   control\
    \ IP networks (especially public IP networks).  Because\n   different contexts\
    \ may demand different levels of survivability, the\n   mechanisms developed to\
    \ support network survivability should be\n   flexible so that they can be tailored\
    \ to different needs.\n   Failure protection and restoration capabilities have\
    \ become available\n   from multiple layers as network technologies have continued\
    \ to\n   improve.  At the bottom of the layered stack, optical networks are\n\
    \   now capable of providing dynamic ring and mesh restoration\n   functionality\
    \ at the wavelength level as well as traditional\n   protection functionality.\
    \  At the SONET/SDH layer survivability\n   capability is provided with Automatic\
    \ Protection Switching (APS) as\n   well as self-healing ring and mesh architectures.\
    \  Similar\n   functionality is provided by layer 2 technologies such as ATM\n\
    \   (generally with slower mean restoration times).  Rerouting is\n   traditionally\
    \ used at the IP layer to restore service following link\n   and node outages.\
    \  Rerouting at the IP layer occurs after a period of\n   routing convergence\
    \ which may require seconds to minutes to complete.\n   Some new developments\
    \ in the MPLS context make it possible to achieve\n   recovery at the IP layer\
    \ prior to convergence [SHAR].\n   To support advanced survivability requirements,\
    \ path-oriented\n   technologies such a MPLS can be used to enhance the survivability\
    \ of\n   IP networks in a potentially cost effective manner.  The advantages\n\
    \   of path oriented technologies such as MPLS for IP restoration becomes\n  \
    \ even more evident when class based protection and restoration\n   capabilities\
    \ are required.\n   Recently, a common suite of control plane protocols has been\
    \ proposed\n   for both MPLS and optical transport networks under the acronym\n\
    \   Multi-protocol Lambda Switching [AWD1].  This new paradigm of Multi-\n   protocol\
    \ Lambda Switching will support even more sophisticated mesh\n   restoration capabilities\
    \ at the optical layer for the emerging IP\n   over WDM network architectures.\n\
    \   Another important aspect regarding multi-layer survivability is that\n   technologies\
    \ at different layers provide protection and restoration\n   capabilities at different\
    \ temporal granularities (in terms of time\n   scales) and at different bandwidth\
    \ granularity (from packet-level to\n   wavelength level).  Protection and restoration\
    \ capabilities can also\n   be sensitive to different service classes and different\
    \ network\n   utility models.\n   The impact of service outages varies significantly\
    \ for different\n   service classes depending upon the effective duration of the\
    \ outage.\n   The duration of an outage can vary from milliseconds (with minor\n\
    \   service impact) to seconds (with possible call drops for IP telephony\n  \
    \ and session time-outs for connection oriented transactions) to\n   minutes and\
    \ hours (with potentially considerable social and business\n   impact).\n   Coordinating\
    \ different protection and restoration capabilities across\n   multiple layers\
    \ in a cohesive manner to ensure network survivability\n   is maintained at reasonable\
    \ cost is a challenging task.  Protection\n   and restoration coordination across\
    \ layers may not always be\n   feasible, because networks at different layers\
    \ may belong to\n   different administrative domains.\n   The following paragraphs\
    \ present some of the general recommendations\n   for protection and restoration\
    \ coordination.\n   -  Protection and restoration capabilities from different\
    \ layers\n   should be coordinated whenever feasible and appropriate to provide\n\
    \   network survivability in a flexible and cost effective manner.\n   Minimization\
    \ of function duplication across layers is one way to\n   achieve the coordination.\
    \  Escalation of alarms and other fault\n   indicators from lower to higher layers\
    \ may also be performed in a\n   coordinated manner.  A temporal order of restoration\
    \ trigger timing\n   at different layers is another way to coordinate multi-layer\n\
    \   protection/restoration.\n   -  Spare capacity at higher layers is often regarded\
    \ as working\n   traffic at lower layers.  Placing protection/restoration functions\
    \ in\n   many layers may increase redundancy and robustness, but it should not\n\
    \   result in significant and avoidable inefficiencies in network\n   resource\
    \ utilization.\n   -  It is generally desirable to have protection and restoration\n\
    \   schemes that are bandwidth efficient.\n   -  Failure notification throughout\
    \ the network should be timely and\n   reliable.\n   -  Alarms and other fault\
    \ monitoring and reporting capabilities\n   should be provided at appropriate\
    \ layers.\n"
- title: 6.5.1 Survivability in MPLS Based Networks
  contents:
  - "6.5.1 Survivability in MPLS Based Networks\n   MPLS is an important emerging\
    \ technology that enhances IP networks in\n   terms of features, capabilities,\
    \ and services.  Because MPLS is\n   path-oriented, it can potentially provide\
    \ faster and more predictable\n   protection and restoration capabilities than\
    \ conventional hop by hop\n   routed IP systems.  This subsection describes some\
    \ of the basic\n   aspects and recommendations for MPLS networks regarding protection\n\
    \   and restoration.  See [SHAR] for a more comprehensive discussion on\n   MPLS\
    \ based recovery.\n   Protection types for MPLS networks can be categorized as\
    \ link\n   protection, node protection, path protection, and segment protection.\n\
    \   -  Link Protection: The objective for link protection is to protect\n    \
    \  an LSP from a given link failure.  Under link protection, the path\n      of\
    \ the protection or backup LSP (the secondary LSP) is disjoint\n      from the\
    \ path of the working or operational LSP at the particular\n      link over which\
    \ protection is required.  When the protected link\n      fails, traffic on the\
    \ working LSP is switched over to the\n      protection LSP at the head-end of\
    \ the failed link.  This is a\n      local repair method which can be fast.  It\
    \ might be more\n      appropriate in situations where some network elements along\
    \ a\n      given path are less reliable than others.\n   -  Node Protection: The\
    \ objective of LSP node protection is to\n      protect an LSP from a given node\
    \ failure.  Under node protection,\n      the path of the protection LSP is disjoint\
    \ from the path of the\n      working LSP at the particular node to be protected.\
    \  The secondary\n      path is also disjoint from the primary path at all links\n\
    \      associated with the node to be protected.  When the node fails,\n     \
    \ traffic on the working LSP is switched over to the protection LSP\n      at\
    \ the upstream LSR directly connected to the failed node.\n   -  Path Protection:\
    \ The goal of LSP path protection is to protect an\n      LSP from failure at\
    \ any point along its routed path.  Under path\n      protection, the path of\
    \ the protection LSP is completely disjoint\n      from the path of the working\
    \ LSP.  The advantage of path\n      protection is that the backup LSP protects\
    \ the working LSP from\n      all possible link and node failures along the path,\
    \ except for\n      failures that might occur at the ingress and egress LSRs,\
    \ or for\n      correlated failures that might impact both working and backup\n\
    \      paths simultaneously.  Additionally, since the path selection is\n    \
    \  end-to-end, path protection might be more efficient in terms of\n      resource\
    \ usage than link or node protection.  However, path\n      protection may be\
    \ slower than link and node protection in general.\n   -  Segment Protection:\
    \ An MPLS domain may be partitioned into\n      multiple protection domains whereby\
    \ a failure in a protection\n      domain is rectified within that domain.  In\
    \ cases where an LSP\n      traverses multiple protection domains, a protection\
    \ mechanism\n      within a domain only needs to protect the segment of the LSP\
    \ that\n      lies within the domain.  Segment protection will generally be\n\
    \      faster than path protection because recovery generally occurs\n      closer\
    \ to the fault.\n"
- title: 6.5.2 Protection Option
  contents:
  - "6.5.2 Protection Option\n   Another issue to consider is the concept of protection\
    \ options.  The\n   protection option uses the notation m:n protection, where\
    \ m is the\n   number of protection LSPs used to protect n working LSPs.  Feasible\n\
    \   protection options follow.\n   -  1:1: one working LSP is protected/restored\
    \ by one protection LSP.\n   -  1:n: one protection LSP is used to protect/restore\
    \ n working LSPs.\n   -  n:1: one working LSP is protected/restored by n protection\
    \ LSPs,\n      possibly with configurable load splitting ratio.  When more than\n\
    \      one protection LSP is used, it may be desirable to share the\n      traffic\
    \ across the protection LSPs when the working LSP fails to\n      satisfy the\
    \ bandwidth requirement of the traffic trunk associated\n      with the working\
    \ LSP.  This may be especially useful when it is\n      not feasible to find one\
    \ path that can satisfy the bandwidth\n      requirement of the primary LSP.\n\
    \   -  1+1: traffic is sent concurrently on both the working LSP and the\n   \
    \   protection LSP.  In this case, the egress LSR selects one of the\n      two\
    \ LSPs based on a local traffic integrity decision process,\n      which compares\
    \ the traffic received from both the working and the\n      protection LSP and\
    \ identifies discrepancies.  It is unlikely that\n      this option would be used\
    \ extensively in IP networks due to its\n      resource utilization inefficiency.\
    \  However, if bandwidth becomes\n      plentiful and cheap, then this option\
    \ might become quite viable\n      and attractive in IP networks.\n"
- title: 6.6 Traffic Engineering in Diffserv Environments
  contents:
  - "6.6 Traffic Engineering in Diffserv Environments\n   This section provides an\
    \ overview of the traffic engineering features\n   and recommendations that are\
    \ specifically pertinent to Differentiated\n   Services (Diffserv) [RFC-2475]\
    \ capable IP networks.\n   Increasing requirements to support multiple classes\
    \ of traffic, such\n   as best effort and mission critical data, in the Internet\
    \ calls for\n   IP networks to differentiate traffic according to some criteria,\
    \ and\n   to accord preferential treatment to certain types of traffic.  Large\n\
    \   numbers of flows can be aggregated into a few behavior aggregates\n   based\
    \ on some criteria in terms of common performance requirements in\n   terms of\
    \ packet loss ratio, delay, and jitter; or in terms of common\n   fields within\
    \ the IP packet headers.\n   As Diffserv evolves and becomes deployed in operational\
    \ networks,\n   traffic engineering will be critical to ensuring that SLAs defined\n\
    \   within a given Diffserv service model are met.  Classes of service\n   (CoS)\
    \ can be supported in a Diffserv environment by concatenating\n   per-hop behaviors\
    \ (PHBs) along the routing path, using service\n   provisioning mechanisms, and\
    \ by appropriately configuring edge\n   functionality such as traffic classification,\
    \ marking, policing, and\n   shaping.  PHB is the forwarding behavior that a packet\
    \ receives at a\n   DS node (a Diffserv-compliant node).  This is accomplished\
    \ by means\n   of buffer management and packet scheduling mechanisms.  In this\n\
    \   context, packets belonging to a class are those that are members of a\n  \
    \ corresponding ordering aggregate.\n   Traffic engineering can be used as a compliment\
    \ to Diffserv\n   mechanisms to improve utilization of network resources, but\
    \ not as a\n   necessary element in general.  When traffic engineering is used,\
    \ it\n   can be operated on an aggregated basis across all service classes\n \
    \  [RFC-3270] or on a per service class basis.  The former is used to\n   provide\
    \ better distribution of the aggregate traffic load over the\n   network resources.\
    \  (See [RFC-3270] for detailed mechanisms to\n   support aggregate traffic engineering.)\
    \  The latter case is discussed\n   below since it is specific to the Diffserv\
    \ environment, with so\n   called Diffserv-aware traffic engineering [DIFF_TE].\n\
    \   For some Diffserv networks, it may be desirable to control the\n   performance\
    \ of some service classes by enforcing certain\n   relationships between the traffic\
    \ workload contributed by each\n   service class and the amount of network resources\
    \ allocated or\n   provisioned for that service class.  Such relationships between\n\
    \   demand and resource allocation can be enforced using a combination\n   of,\
    \ for example: (1) traffic engineering mechanisms on a per service\n   class basis\
    \ that enforce the desired relationship between the amount\n   of traffic contributed\
    \ by a given service class and the resources\n   allocated to that class, and\
    \ (2) mechanisms that dynamically adjust\n   the resources allocated to a given\
    \ service class to relate to the\n   amount of traffic contributed by that service\
    \ class.\n   It may also be desirable to limit the performance impact of high\n\
    \   priority traffic on relatively low priority traffic.  This can be\n   achieved\
    \ by, for example, controlling the percentage of high priority\n   traffic that\
    \ is routed through a given link.  Another way to\n   accomplish this is to increase\
    \ link capacities appropriately so that\n   lower priority traffic can still enjoy\
    \ adequate service quality.\n   When the ratio of traffic workload contributed\
    \ by different service\n   classes vary significantly from router to router, it\
    \ may not suffice\n   to rely exclusively on conventional IGP routing protocols\
    \ or on\n   traffic engineering mechanisms that are insensitive to different\n\
    \   service classes.  Instead, it may be desirable to perform traffic\n   engineering,\
    \ especially routing control and mapping functions, on a\n   per service class\
    \ basis.  One way to accomplish this in a domain that\n   supports both MPLS and\
    \ Diffserv is to define class specific LSPs and\n   to map traffic from each class\
    \ onto one or more LSPs that correspond\n   to that service class.  An LSP corresponding\
    \ to a given service class\n   can then be routed and protected/restored in a\
    \ class dependent\n   manner, according to specific policies.\n   Performing traffic\
    \ engineering on a per class basis may require\n   certain per-class parameters\
    \ to be distributed.  Note that it is\n   common to have some classes share some\
    \ aggregate constraint (e.g.,\n   maximum bandwidth requirement) without enforcing\
    \ the constraint on\n   each individual class.  These classes then can be grouped\
    \ into a\n   class-type and per-class-type parameters can be distributed instead\n\
    \   to improve scalability.  It also allows better bandwidth sharing\n   between\
    \ classes in the same class-type.  A class-type is a set of\n   classes that satisfy\
    \ the following two conditions:\n   1) Classes in the same class-type have common\
    \ aggregate requirements\n   to satisfy required performance levels.\n   2) There\
    \ is no requirement to be enforced at the level of individual\n   class in the\
    \ class-type.  Note that it is still possible,\n   nevertheless, to implement\
    \ some priority policies for classes in the\n   same class-type to permit preferential\
    \ access to the class-type\n   bandwidth through the use of preemption priorities.\n\
    \   An example of the class-type can be a low-loss class-type that\n   includes\
    \ both AF1-based and AF2-based Ordering Aggregates.  With such\n   a class-type,\
    \ one may implement some priority policy which assigns\n   higher preemption priority\
    \ to AF1-based traffic trunks over AF2-based\n   ones, vice versa, or the same\
    \ priority.\n   See [DIFF-TE] for detailed requirements on Diffserv-aware traffic\n\
    \   engineering.\n"
- title: 6.7 Network Controllability
  contents:
  - "6.7 Network Controllability\n   Off-line (and on-line) traffic engineering considerations\
    \ would be of\n   limited utility if the network could not be controlled effectively\
    \ to\n   implement the results of TE decisions and to achieve desired network\n\
    \   performance objectives.  Capacity augmentation is a coarse grained\n   solution\
    \ to traffic engineering issues.  However, it is simple and\n   may be advantageous\
    \ if bandwidth is abundant and cheap or if the\n   current or expected network\
    \ workload demands it.  However, bandwidth\n   is not always abundant and cheap,\
    \ and the workload may not always\n   demand additional capacity.  Adjustments\
    \ of administrative weights\n   and other parameters associated with routing protocols\
    \ provide finer\n   grained control, but is difficult to use and imprecise because\
    \ of the\n   routing interactions that occur across the network.  In certain\n\
    \   network contexts, more flexible, finer grained approaches which\n   provide\
    \ more precise control over the mapping of traffic to routes\n   and over the\
    \ selection and placement of routes may be appropriate and\n   useful.\n   Control\
    \ mechanisms can be manual (e.g., administrative\n   configuration), partially-automated\
    \ (e.g., scripts) or fully-\n   automated (e.g., policy based management systems).\
    \  Automated\n   mechanisms are particularly required in large scale networks.\n\
    \   Multi-vendor interoperability can be facilitated by developing and\n   deploying\
    \ standardized management\n   systems (e.g., standard MIBs) and policies (PIBs)\
    \ to support the\n   control functions required to address traffic engineering\
    \ objectives\n   such as load distribution and protection/restoration.\n   Network\
    \ control functions should be secure, reliable, and stable as\n   these are often\
    \ needed to operate correctly in times of network\n   impairments (e.g., during\
    \ network congestion or security attacks).\n"
- title: 7.0 Inter-Domain Considerations
  contents:
  - "7.0 Inter-Domain Considerations\n   Inter-domain traffic engineering is concerned\
    \ with the performance\n   optimization for traffic that originates in one administrative\
    \ domain\n   and terminates in a different one.\n   Traffic exchange between autonomous\
    \ systems in the Internet occurs\n   through exterior gateway protocols.  Currently,\
    \ BGP [BGP4] is the\n   standard exterior gateway protocol for the Internet. \
    \ BGP provides a\n   number of attributes and capabilities (e.g., route filtering)\
    \ that\n   can be used for inter-domain traffic engineering.  More specifically,\n\
    \   BGP permits the control of routing information and traffic exchange\n   between\
    \ Autonomous Systems (AS's) in the Internet.  BGP incorporates\n   a sequential\
    \ decision process which calculates the degree of\n   preference for various routes\
    \ to a given destination network.  There\n   are two fundamental aspects to inter-domain\
    \ traffic engineering using\n   BGP:\n   -  Route Redistribution: controlling\
    \ the import and export of routes\n      between AS's, and controlling the redistribution\
    \ of routes between\n      BGP and other protocols within an AS.\n   -  Best path\
    \ selection: selecting the best path when there are\n      multiple candidate\
    \ paths to a given destination network.  Best\n      path selection is performed\
    \ by the BGP decision process based on a\n      sequential procedure, taking a\
    \ number of different considerations\n      into account.  Ultimately, best path\
    \ selection under BGP boils\n      down to selecting preferred exit points out\
    \ of an AS towards\n      specific destination networks.  The BGP path selection\
    \ process can\n      be influenced by manipulating the attributes associated with\
    \ the\n      BGP decision process.  These attributes include: NEXT-HOP, WEIGHT\n\
    \      (Cisco proprietary which is also implemented by some other\n      vendors),\
    \ LOCAL-PREFERENCE, AS-PATH, ROUTE-ORIGIN, MULTI-EXIT-\n      DESCRIMINATOR (MED),\
    \ IGP METRIC, etc.\n   Route-maps provide the flexibility to implement complex\
    \ BGP policies\n   based on pre-configured logical conditions.  In particular,\
    \ Route-\n   maps can be used to control import and export policies for incoming\n\
    \   and outgoing routes, control the redistribution of routes between BGP\n  \
    \ and other protocols, and influence the selection of best paths by\n   manipulating\
    \ the attributes associated with the BGP decision process.\n   Very complex logical\
    \ expressions that implement various types of\n   policies can be implemented\
    \ using a combination of Route-maps, BGP-\n   attributes, Access-lists, and Community\
    \ attributes.\n   When looking at possible strategies for inter-domain TE with\
    \ BGP, it\n   must be noted that the outbound traffic exit point is controllable,\n\
    \   whereas the interconnection point where inbound traffic is received\n   from\
    \ an EBGP peer typically is not, unless a special arrangement is\n   made with\
    \ the peer sending the traffic.  Therefore, it is up to each\n   individual network\
    \ to implement sound TE strategies that deal with\n   the efficient delivery of\
    \ outbound traffic from one's customers to\n   one's peering points.  The vast\
    \ majority of TE policy is based upon a\n   \"closest exit\" strategy, which offloads\
    \ interdomain traffic at the\n   nearest outbound peer point towards the destination\
    \ autonomous\n   system.  Most methods of manipulating the point at which inbound\n\
    \   traffic enters a network from an EBGP peer (inconsistent route\n   announcements\
    \ between peering points, AS pre-pending, and sending\n   MEDs) are either ineffective,\
    \ or not accepted in the peering\n   community.\n   Inter-domain TE with BGP is\
    \ generally effective, but it is usually\n   applied in a trial-and-error fashion.\
    \  A systematic approach for\n   inter-domain traffic engineering is yet to be\
    \ devised.\n   Inter-domain TE is inherently more difficult than intra-domain\
    \ TE\n   under the current Internet architecture.  The reasons for this are\n\
    \   both technical and administrative.  Technically, while topology and\n   link\
    \ state information are helpful for mapping traffic more\n   effectively, BGP\
    \ does not propagate such information across domain\n   boundaries for stability\
    \ and scalability reasons.  Administratively,\n   there are differences in operating\
    \ costs and network capacities\n   between domains.  Generally, what may be considered\
    \ a good solution\n   in one domain may not necessarily be a good solution in\
    \ another\n   domain.  Moreover, it would generally be considered inadvisable\
    \ for\n   one domain to permit another domain to influence the routing and\n \
    \  management of traffic in its network.\n   MPLS TE-tunnels (explicit LSPs) can\
    \ potentially add a degree of\n   flexibility in the selection of exit points\
    \ for inter-domain routing.\n   The concept of relative and absolute metrics can\
    \ be applied to this\n   purpose.  The idea is that if BGP attributes are defined\
    \ such that\n   the BGP decision process depends on IGP metrics to select exit\
    \ points\n   for inter-domain traffic, then some inter-domain traffic destined\
    \ to\n   a given peer network can be made to prefer a specific exit point by\n\
    \   establishing a TE-tunnel between the router making the selection to\n   the\
    \ peering point via a TE-tunnel and assigning the TE-tunnel a\n   metric which\
    \ is smaller than the IGP cost to all other peering\n   points.  If a peer accepts\
    \ and processes MEDs, then a similar MPLS\n   TE-tunnel based scheme can be applied\
    \ to cause certain entrance\n   points to be preferred by setting MED to be an\
    \ IGP cost, which has\n   been modified by the tunnel metric.\n   Similar to intra-domain\
    \ TE, inter-domain TE is best accomplished when\n   a traffic matrix can be derived\
    \ to depict the volume of traffic from\n   one autonomous system to another.\n\
    \   Generally, redistribution of inter-domain traffic requires\n   coordination\
    \ between peering partners.  An export policy in one\n   domain that results in\
    \ load redistribution across peer points with\n   another domain can significantly\
    \ affect the local traffic matrix\n   inside the domain of the peering partner.\
    \  This, in turn, will affect\n   the intra-domain TE due to changes in the spatial\
    \ distribution of\n   traffic.  Therefore, it is mutually beneficial for peering\
    \ partners\n   to coordinate with each other before attempting any policy changes\n\
    \   that may result in significant shifts in inter-domain traffic.  In\n   certain\
    \ contexts, this coordination can be quite challenging due to\n   technical and\
    \ non- technical reasons.\n   It is a matter of speculation as to whether MPLS,\
    \ or similar\n   technologies, can be extended to allow selection of constrained\
    \ paths\n   across domain boundaries.\n"
- title: 8.0 Overview of Contemporary TE Practices in Operational IP Networks
  contents:
  - "8.0 Overview of Contemporary TE Practices in Operational IP Networks\n   This\
    \ section provides an overview of some contemporary traffic\n   engineering practices\
    \ in IP networks.  The focus is primarily on the\n   aspects that pertain to the\
    \ control of the routing function in\n   operational contexts.  The intent here\
    \ is to provide an overview of\n   the commonly used practices.  The discussion\
    \ is not intended to be\n   exhaustive.\n   Currently, service providers apply\
    \ many of the traffic engineering\n   mechanisms discussed in this document to\
    \ optimize the performance of\n   their IP networks.  These techniques include\
    \ capacity planning for\n   long time scales, routing control using IGP metrics\
    \ and MPLS for\n   medium time scales, the overlay model also for medium time\
    \ scales,\n   and traffic management mechanisms for short time scale.\n   When\
    \ a service provider plans to build an IP network, or expand the\n   capacity\
    \ of an existing network, effective capacity planning should\n   be an important\
    \ component of the process.  Such plans may take the\n   following aspects into\
    \ account: location of new nodes if any,\n   existing and predicted traffic patterns,\
    \ costs, link capacity,\n   topology, routing design, and survivability.\n   Performance\
    \ optimization of operational networks is usually an\n   ongoing process in which\
    \ traffic statistics, performance parameters,\n   and fault indicators are continually\
    \ collected from the network.\n   This empirical data is then analyzed and used\
    \ to trigger various\n   traffic engineering mechanisms.  Tools that perform what-if\
    \ analysis\n   can also be used to assist the TE process by allowing various\n\
    \   scenarios to be reviewed before a new set of configurations are\n   implemented\
    \ in the operational network.\n   Traditionally, intra-domain real-time TE with\
    \ IGP is done by\n   increasing the OSPF or IS-IS metric of a congested link until\
    \ enough\n   traffic has been diverted from that link.  This approach has some\n\
    \   limitations as discussed in Section 6.2.  Recently, some new intra-\n   domain\
    \ TE approaches/tools have been proposed\n   [RR94][FT00][FT01][WANG].  Such approaches/tools\
    \ take traffic matrix,\n   network topology, and network performance objective(s)\
    \ as input, and\n   produce some link metrics and possibly some unequal load-sharing\n\
    \   ratios to be set at the head-end routers of some ECMPs as output.\n   These\
    \ new progresses open new possibility for intra-domain TE with\n   IGP to be done\
    \ in a more systematic way.\n   The overlay model (IP over ATM or IP over Frame\
    \ relay) is another\n   approach which is commonly used in practice [AWD2].  The\
    \ IP over ATM\n   technique is no longer viewed favorably due to recent advances\
    \ in\n   MPLS and router hardware technology.\n   Deployment of MPLS for traffic\
    \ engineering applications has commenced\n   in some service provider networks.\
    \  One operational scenario is to\n   deploy MPLS in conjunction with an IGP (IS-IS-TE\
    \ or OSPF-TE) that\n   supports the traffic engineering extensions, in conjunction\
    \ with\n   constraint-based routing for explicit route computations, and a\n \
    \  signaling protocol (e.g., RSVP-TE or CRLDP) for LSP instantiation.\n   In contemporary\
    \ MPLS traffic engineering contexts, network\n   administrators specify and configure\
    \ link attributes and resource\n   constraints such as maximum reservable bandwidth\
    \ and resource class\n   attributes for links (interfaces) within the MPLS domain.\
    \  A link\n   state protocol that supports TE extensions (IS-IS-TE or OSPF-TE)\
    \ is\n   used to propagate information about network topology and link\n   attribute\
    \ to all routers in the routing area.  Network administrators\n   also specify\
    \ all the LSPs that are to originate each router.  For\n   each LSP, the network\
    \ administrator specifies the destination node\n   and the attributes of the LSP\
    \ which indicate the requirements that to\n   be satisfied during the path selection\
    \ process.  Each router then\n   uses a local constraint-based routing process\
    \ to compute explicit\n   paths for all LSPs originating from it.  Subsequently,\
    \ a signaling\n   protocol is used to instantiate the LSPs.  By assigning proper\n\
    \   bandwidth values to links and LSPs, congestion caused by uneven\n   traffic\
    \ distribution can generally be avoided or mitigated.\n   The bandwidth attributes\
    \ of LSPs used for traffic engineering can be\n   updated periodically.  The basic\
    \ concept is that the bandwidth\n   assigned to an LSP should relate in some manner\
    \ to the bandwidth\n   requirements of traffic that actually flows through the\
    \ LSP.  The\n   traffic attribute of an LSP can be modified to accommodate traffic\n\
    \   growth and persistent traffic shifts.  If network congestion occurs\n   due\
    \ to some unexpected events, existing LSPs can be rerouted to\n   alleviate the\
    \ situation or network administrator can configure new\n   LSPs to divert some\
    \ traffic to alternative paths.  The reservable\n   bandwidth of the congested\
    \ links can also be reduced to force some\n   LSPs to be rerouted to other paths.\n\
    \   In an MPLS domain, a traffic matrix can also be estimated by\n   monitoring\
    \ the traffic on LSPs.  Such traffic statistics can be used\n   for a variety\
    \ of purposes including network planning and network\n   optimization.  Current\
    \ practice suggests that deploying an MPLS\n   network consisting of hundreds\
    \ of routers and thousands of LSPs is\n   feasible.  In summary, recent deployment\
    \ experience suggests that\n   MPLS approach is very effective for traffic engineering\
    \ in IP\n   networks [XIAO].\n   As mentioned previously in Section 7.0, one usually\
    \ has no direct\n   control over the distribution of inbound traffic.  Therefore,\
    \ the\n   main goal of contemporary inter-domain TE is to optimize the\n   distribution\
    \ of outbound traffic between multiple inter-domain links.\n   When operating\
    \ a global network, maintaining the ability to operate\n   the network in a regional\
    \ fashion where desired, while continuing to\n   take advantage of the benefits\
    \ of a global network, also becomes an\n   important objective.\n   Inter-domain\
    \ TE with BGP usually begins with the placement of\n   multiple peering interconnection\
    \ points in locations that have high\n   peer density, are in close proximity\
    \ to originating/terminating\n   traffic locations on one's own network, and are\
    \ lowest in cost.\n   There are generally several locations in each region of\
    \ the world\n   where the vast majority of major networks congregate and\n   interconnect.\
    \  Some location-decision problems that arise in\n   association with inter-domain\
    \ routing are discussed in [AWD5].\n   Once the locations of the interconnects\
    \ are determined, and circuits\n   are implemented, one decides how best to handle\
    \ the routes heard from\n   the peer, as well as how to propagate the peers' routes\
    \ within one's\n   own network.  One way to engineer outbound traffic flows on\
    \ a network\n   with many EBGP peers is to create a hierarchy of peers.  Generally,\n\
    \   the Local Preferences of all peers are set to the same value so that\n   the\
    \ shortest AS paths will be chosen to forward traffic.  Then, by\n   over-writing\
    \ the inbound MED metric (Multi-exit-discriminator metric,\n   also referred to\
    \ as \"BGP metric\".  Both terms are used\n   interchangeably in this document)\
    \ with BGP metrics to routes received\n   at different peers, the hierarchy can\
    \ be formed.  For example, all\n   Local Preferences can be set to 200, preferred\
    \ private peers can be\n   assigned a BGP metric of 50, the rest of the private\
    \ peers can be\n   assigned a BGP metric of 100, and public peers can be assigned\
    \ a BGP\n   metric of 600.  \"Preferred\" peers might be defined as those peers\n\
    \   with whom the most available capacity exists, whose customer base is\n   larger\
    \ in comparison to other peers, whose interconnection costs are\n   the lowest,\
    \ and with whom upgrading existing capacity is the easiest.\n   In a network with\
    \ low utilization at the edge, this works well.  The\n   same concept could be\
    \ applied to a network with higher edge\n   utilization by creating more levels\
    \ of BGP metrics between peers,\n   allowing for more granularity in selecting\
    \ the exit points for\n   traffic bound for a dual homed customer on a peer's\
    \ network.\n   By only replacing inbound MED metrics with BGP metrics, only equal\n\
    \   AS-Path length routes' exit points are being changed.  (The BGP\n   decision\
    \ considers Local Preference first, then AS-Path length, and\n   then BGP metric).\
    \  For example, assume a network has two possible\n   egress points, peer A and\
    \ peer B.  Each peer has 40% of the\n   Internet's routes exclusively on its network,\
    \ while the remaining 20%\n   of the Internet's routes are from customers who\
    \ dual home between A\n   and B.  Assume that both peers have a Local Preference\
    \ of 200 and a\n   BGP metric of 100.  If the link to peer A is congested, increasing\n\
    \   its BGP metric while leaving the Local Preference at 200 will ensure\n   that\
    \ the 20% of total routes belonging to dual homed customers will\n   prefer peer\
    \ B as the exit point.  The previous example would be used\n   in a situation\
    \ where all exit points to a given peer were close to\n   congestion levels, and\
    \ traffic needed to be shifted away from that\n   peer entirely.\n   When there\
    \ are multiple exit points to a given peer, and only one of\n   them is congested,\
    \ it is not necessary to shift traffic away from the\n   peer entirely, but only\
    \ from the one congested circuit.  This can be\n   achieved by using passive IGP-metrics,\
    \ AS-path filtering, or prefix\n   filtering.\n   Occasionally, more drastic changes\
    \ are needed, for example, in\n   dealing with a \"problem peer\" who is difficult\
    \ to work with on\n   upgrades or is charging high prices for connectivity to\
    \ their\n   network.  In that case, the Local Preference to that peer can be\n\
    \   reduced below the level of other peers.  This effectively reduces the\n  \
    \ amount of traffic sent to that peer to only originating traffic\n   (assuming\
    \ no transit providers are involved).  This type of change\n   can affect a large\
    \ amount of traffic, and is only used after other\n   methods have failed to provide\
    \ the desired results.\n   Although it is not much of an issue in regional networks,\
    \ the\n   propagation of a peer's routes back through the network must be\n  \
    \ considered when a network is peering on a global scale.  Sometimes,\n   business\
    \ considerations can influence the choice of BGP policies in a\n   given context.\
    \  For example, it may be imprudent, from a business\n   perspective, to operate\
    \ a global network and provide full access to\n   the global customer base to\
    \ a small network in a particular country.\n   However, for the purpose of providing\
    \ one's own customers with\n   quality service in a particular region, good connectivity\
    \ to that\n   in-country network may still be necessary.  This can be achieved\
    \ by\n   assigning a set of communities at the edge of the network, which have\n\
    \   a known behavior when routes tagged with those communities are\n   propagating\
    \ back through the core.  Routes heard from local peers\n   will be prevented\
    \ from propagating back to the global network,\n   whereas routes learned from\
    \ larger peers may be allowed to propagate\n   freely throughout the entire global\
    \ network.  By implementing a\n   flexible community strategy, the benefits of\
    \ using a single global AS\n   Number (ASN) can be realized, while the benefits\
    \ of operating\n   regional networks can also be taken advantage of.  An alternative\
    \ to\n   doing this is to use different ASNs in different regions, with the\n\
    \   consequence that the AS path length for routes announced by that\n   service\
    \ provider will increase.\n"
- title: 9.0 Conclusion
  contents:
  - "9.0 Conclusion\n   This document described principles for traffic engineering\
    \ in the\n   Internet.  It presented an overview of some of the basic issues\n\
    \   surrounding traffic engineering in IP networks.  The context of TE\n   was\
    \ described, a TE process models and a taxonomy of TE styles were\n   presented.\
    \  A brief historical review of pertinent developments\n   related to traffic\
    \ engineering was provided.  A survey of\n   contemporary TE techniques in operational\
    \ networks was presented.\n   Additionally, the document specified a set of generic\
    \ requirements,\n   recommendations, and options for Internet traffic engineering.\n"
- title: 10.0 Security Considerations
  contents:
  - "10.0 Security Considerations\n   This document does not introduce new security\
    \ issues.\n"
- title: 11.0 Acknowledgments
  contents:
  - "11.0 Acknowledgments\n   The authors would like to thank Jim Boyle for inputs\
    \ on the\n   recommendations section, Francois Le Faucheur for inputs on Diffserv\n\
    \   aspects, Blaine Christian for inputs on measurement, Gerald Ash for\n   inputs\
    \ on routing in telephone networks and for text on event-\n   dependent TE methods,\
    \ Steven Wright for inputs on network\n   controllability, and Jonathan Aufderheide\
    \ for inputs on inter-domain\n   TE with BGP.  Special thanks to Randy Bush for\
    \ proposing the TE\n   taxonomy based on \"tactical vs strategic\" methods.  The\
    \ subsection\n   describing an \"Overview of ITU Activities Related to Traffic\n\
    \   Engineering\" was adapted from a contribution by Waisum Lai.  Useful\n   feedback\
    \ and pointers to relevant materials were provided by J. Noel\n   Chiappa.  Additional\
    \ comments were provided by Glenn Grotefeld during\n   the working last call process.\
    \  Finally, the authors would like to\n   thank Ed Kern, the TEWG co-chair, for\
    \ his comments and support.\n"
- title: 12.0 References
  contents:
  - "12.0 References\n   [ASH2]      J. Ash, Dynamic Routing in Telecommunications\
    \ Networks,\n               McGraw Hill, 1998.\n   [ASH3]      Ash, J., \"TE &\
    \ QoS Methods for IP-, ATM-, & TDM-Based\n               Networks\", Work in Progress,\
    \ March 2001.\n   [AWD1]      D. Awduche and Y. Rekhter, \"Multiprocotol Lambda\n\
    \               Switching:  Combining MPLS Traffic Engineering Control\n     \
    \          with Optical Crossconnects\", IEEE Communications\n               Magazine,\
    \ March 2001.\n   [AWD2]      D. Awduche, \"MPLS and Traffic Engineering in IP\n\
    \               Networks\", IEEE Communications Magazine, Dec. 1999.\n   [AWD5]\
    \      D. Awduche et al, \"An Approach to Optimal Peering Between\n          \
    \     Autonomous Systems in the Internet\", International\n               Conference\
    \ on Computer Communications and Networks\n               (ICCCN'98), Oct. 1998.\n\
    \   [CRUZ]      R. L. Cruz, \"A Calculus for Network Delay, Part II:\n       \
    \        Network Analysis\", IEEE Transactions on Information\n              \
    \ Theory, vol. 37, pp.  132-141, 1991.\n   [DIFF-TE]   Le Faucheur, F., Nadeau,\
    \ T., Tatham, M., Telkamp, T.,\n               Cooper, D., Boyle, J., Lai, W.,\
    \ Fang, L., Ash, J., Hicks,\n               P., Chui, A., Townsend, W. and D.\
    \ Skalecki, \"Requirements\n               for support of Diff-Serv-aware MPLS\
    \ Traffic Engineering\",\n               Work in Progress, May 2001.\n   [ELW95]\
    \     A. Elwalid, D. Mitra and R.H. Wentworth, \"A New Approach\n            \
    \   for Allocating Buffers and Bandwidth to Heterogeneous,\n               Regulated\
    \ Traffic in an ATM Node\", IEEE IEEE Journal on\n               Selected Areas\
    \ in Communications, 13:6, pp. 1115-1127,\n               Aug. 1995.\n   [FGLR]\
    \      A. Feldmann, A. Greenberg, C. Lund, N. Reingold, and J.\n             \
    \  Rexford, \"NetScope: Traffic Engineering for IP Networks\",\n             \
    \  IEEE Network Magazine, 2000.\n   [FLJA93]    S. Floyd and V. Jacobson, \"Random\
    \ Early Detection\n               Gateways for Congestion Avoidance\", IEEE/ACM\
    \ Transactions\n               on Networking, Vol. 1 Nov. 4., p. 387-413, Aug.\
    \ 1993.\n   [FLOY94]    S. Floyd, \"TCP and Explicit Congestion Notification\"\
    , ACM\n               Computer Communication Review, V. 24, No. 5, p. 10-23,\n\
    \               Oct. 1994.\n   [FT00]      B. Fortz and M. Thorup, \"Internet\
    \ Traffic Engineering by\n               Optimizing OSPF Weights\", IEEE INFOCOM\
    \ 2000, Mar. 2000.\n   [FT01]      B. Fortz and M. Thorup, \"Optimizing OSPF/IS-IS\
    \ Weights in\n               a Changing World\",\n               www.research.att.com/~mthorup/PAPERS/papers.html.\n\
    \   [HUSS87]    B.R. Hurley, C.J.R. Seidl and W.F. Sewel, \"A Survey of\n    \
    \           Dynamic Routing Methods for Circuit-Switched Traffic\",\n        \
    \       IEEE Communication Magazine, Sep. 1987.\n   [ITU-E600]  ITU-T Recommendation\
    \ E.600, \"Terms and Definitions of\n               Traffic Engineering\", Mar.\
    \ 1993.\n   [ITU-E701]  ITU-T Recommendation E.701, \"Reference Connections for\n\
    \               Traffic Engineering\", Oct. 1993.\n   [ITU-E801]  ITU-T Recommendation\
    \ E.801, \"Framework for Service\n               Quality Agreement\", Oct. 1996.\n\
    \   [JAM]       Jamoussi, B., Editior, Andersson, L., Collon, R. and R.\n    \
    \           Dantu, \"Constraint-Based LSP Setup using LDP\", RFC 3212,\n     \
    \          January 2002.\n   [KATZ]      Katz, D., Yeung, D. and K. Kompella,\
    \ \"Traffic Engineering\n               Extensions to OSPF\", Work in Progress,\
    \ February 2001.\n   [LNO96]     T. Lakshman, A. Neidhardt, and T. Ott, \"The\
    \ Drop from\n               Front Strategy in TCP over ATM and its Interworking\
    \ with\n               other Control Features\", Proc. INFOCOM'96, p. 1242-1250,\n\
    \               1996.\n   [MA]        Q. Ma, \"Quality of Service Routing in Integrated\
    \ Services\n               Networks\", PhD Dissertation, CMU-CS-98-138, CMU, 1998.\n\
    \   [MATE]      A. Elwalid, C. Jin, S. Low, and I. Widjaja, \"MATE: MPLS\n   \
    \            Adaptive Traffic Engineering\", Proc. INFOCOM'01, Apr.\n        \
    \       2001.\n   [MCQ80]     J.M. McQuillan, I. Richer, and E.C. Rosen, \"The\
    \ New\n               Routing Algorithm for the ARPANET\", IEEE.  Trans. on\n\
    \               Communications, vol. 28, no. 5, pp. 711-719, May 1980.\n   [MR99]\
    \      D. Mitra and K.G. Ramakrishnan, \"A Case Study of\n               Multiservice,\
    \ Multipriority Traffic Engineering Design\n               for Data Networks\"\
    , Proc. Globecom'99, Dec 1999.\n   [RFC-1458]  Braudes, R. and S. Zabele, \"Requirements\
    \ for Multicast\n               Protocols\", RFC 1458, May 1993.\n   [RFC-1771]\
    \  Rekhter, Y. and T. Li, \"A Border Gateway Protocol 4\n               (BGP-4)\"\
    , RFC 1771, March 1995.\n   [RFC-1812]  Baker, F., \"Requirements for IP Version\
    \ 4 Routers\", STD\n               4, RFC 1812, June 1995.\n   [RFC-1992]  Castineyra,\
    \ I., Chiappa, N. and M. Steenstrup, \"The\n               Nimrod Routing Architecture\"\
    , RFC 1992, August 1996.\n   [RFC-1997]  Chandra, R., Traina, P. and T. Li, \"\
    BGP Community\n               Attributes\", RFC 1997, August 1996.\n   [RFC-1998]\
    \  Chen, E. and T. Bates, \"An Application of the BGP\n               Community\
    \ Attribute in Multi-home Routing\", RFC 1998,\n               August 1996.\n\
    \   [RFC-2205]  Braden, R., Zhang, L., Berson, S., Herzog, S. and S.\n       \
    \        Jamin, \"Resource Reservation Protocol (RSVP) - Version 1\n         \
    \      Functional Specification\", RFC 2205, September 1997.\n   [RFC-2211]  Wroclawski,\
    \ J., \"Specification of the Controlled-Load\n               Network Element Service\"\
    , RFC 2211, September 1997.\n   [RFC-2212]  Shenker, S., Partridge, C. and R.\
    \ Guerin, \"Specification\n               of Guaranteed Quality of Service\",\
    \ RFC 2212, September\n               1997.\n   [RFC-2215]  Shenker, S. and J.\
    \ Wroclawski, \"General Characterization\n               Parameters for Integrated\
    \ Service Network Elements\", RFC\n               2215, September 1997.\n   [RFC-2216]\
    \  Shenker, S. and J. Wroclawski, \"Network Element Service\n               Specification\
    \ Template\", RFC 2216, September 1997.\n   [RFC-2328]  Moy, J., \"OSPF Version\
    \ 2\", STD 54, RFC 2328, July 1997.\n   [RFC-2330]  Paxson, V., Almes, G., Mahdavi,\
    \ J. and M. Mathis,\n               \"Framework for IP Performance Metrics\",\
    \ RFC 2330, May\n               1998.\n   [RFC-2386]  Crawley, E., Nair, R., Rajagopalan,\
    \ B. and H. Sandick, \"A\n               Framework for QoS-based Routing in the\
    \ Internet\", RFC\n               2386, August 1998.\n   [RFC-2474]  Nichols,\
    \ K., Blake, S., Baker, F. and D. Black,\n               \"Definition of the Differentiated\
    \ Services Field (DS\n               Field) in the IPv4 and IPv6 Headers\", RFC\
    \ 2474, December\n               1998.\n   [RFC-2475]  Blake, S., Black, D., Carlson,\
    \ M., Davies, E., Wang, Z.\n               and W. Weiss, \"An Architecture for\
    \ Differentiated\n               Services\", RFC 2475, December 1998.\n   [RFC-2597]\
    \  Heinanen, J., Baker, F., Weiss, W. and J. Wroclawski,\n               \"Assured\
    \ Forwarding PHB Group\", RFC 2597, June 1999.\n   [RFC-2678]  Mahdavi, J. and\
    \ V. Paxson, \"IPPM Metrics for Measuring\n               Connectivity\", RFC\
    \ 2678, September 1999.\n   [RFC-2679]  Almes, G., Kalidindi, S. and M. Zekauskas,\
    \ \"A One-way\n               Delay Metric for IPPM\", RFC 2679, September 1999.\n\
    \   [RFC-2680]  Almes, G., Kalidindi, S. and M. Zekauskas, \"A One-way\n     \
    \          Packet Loss Metric for IPPM\", RFC 2680, September 1999.\n   [RFC-2702]\
    \  Awduche, D., Malcolm, J., Agogbua, J., O'Dell, M. and J.\n               McManus,\
    \ \"Requirements for Traffic Engineering over\n               MPLS\", RFC 2702,\
    \ September 1999.\n   [RFC-2722]  Brownlee, N., Mills, C. and G. Ruth, \"Traffic\
    \ Flow\n               Measurement: Architecture\", RFC 2722, October 1999.\n\
    \   [RFC-2753]  Yavatkar, R., Pendarakis, D. and R. Guerin, \"A Framework\n  \
    \             for Policy-based Admission Control\", RFC 2753, January\n      \
    \         2000.\n   [RFC-2961]  Berger, L., Gan, D., Swallow, G., Pan, P., Tommasi,\
    \ F.\n               and S. Molendini, \"RSVP Refresh Overhead Reduction\n   \
    \            Extensions\", RFC 2961, April 2000.\n   [RFC-2998]  Bernet, Y., Ford,\
    \ P., Yavatkar, R., Baker, F., Zhang, L.,\n               Speer, M., Braden, R.,\
    \ Davie, B., Wroclawski, J. and E.\n               Felstaine, \"A Framework for\
    \ Integrated Services Operation\n               over Diffserv Networks\", RFC\
    \ 2998, November 2000.\n   [RFC-3031]  Rosen, E., Viswanathan, A. and R. Callon,\
    \ \"Multiprotocol\n               Label Switching Architecture\", RFC 3031, January\
    \ 2001.\n   [RFC-3086]  Nichols, K. and B. Carpenter, \"Definition of\n      \
    \         Differentiated Services Per Domain Behaviors and Rules\n           \
    \    for their Specification\", RFC 3086, April 2001.\n   [RFC-3124]  Balakrishnan,\
    \ H. and S. Seshan, \"The Congestion Manager\",\n               RFC 3124, June\
    \ 2001.\n   [RFC-3209]  Awduche, D., Berger, L., Gan, D., Li, T., Srinivasan,\
    \ V.\n               and G. Swallow, \"RSVP-TE: Extensions to RSVP for LSP\n \
    \              Tunnels\", RFC 3209, December 2001.\n   [RFC-3210]  Awduche, D.,\
    \ Hannan, A. and X. Xiao, \"Applicability\n               Statement for Extensions\
    \ to RSVP for LSP-Tunnels\", RFC\n               3210, December 2001.\n   [RFC-3213]\
    \  Ash, J., Girish, M., Gray, E., Jamoussi, B. and G.\n               Wright,\
    \ \"Applicability Statement for CR-LDP\", RFC 3213,\n               January 2002.\n\
    \   [RFC-3270]  Le Faucheur, F., Wu, L., Davie, B., Davari, S., Vaahanen,\n  \
    \             P., Krishnan, R., Cheval, P. and J. Heinanen, \"Multi-\n       \
    \        Protocol Label Switching (MPLS) Support of Differentiated\n         \
    \      Services\", RFC 3270, April 2002.\n   [RR94]      M.A. Rodrigues and K.G.\
    \ Ramakrishnan, \"Optimal Routing in\n               Shortest Path Networks\"\
    , ITS'94, Rio de Janeiro, Brazil.\n   [SHAR]      Sharma, V., Crane, B., Owens,\
    \ K., Huang, C., Hellstrand,\n               F., Weil, J., Anderson, L., Jamoussi,\
    \ B., Cain, B.,\n               Civanlar, S. and A. Chui, \"Framework for MPLS\
    \ Based\n               Recovery\", Work in Progress.\n   [SLDC98]    B. Suter,\
    \ T. Lakshman, D. Stiliadis, and A. Choudhury,\n               \"Design Considerations\
    \ for Supporting TCP with Per-flow\n               Queueing\", Proc. INFOCOM'98,\
    \ p. 299-306, 1998.\n   [SMIT]      Smit, H. and T. Li, \"IS-IS extensions for\
    \ Traffic\n               Engineering\", Work in Progress.\n   [WANG]      Y.\
    \ Wang, Z. Wang, L. Zhang, \"Internet traffic engineering\n               without\
    \ full mesh overlaying\", Proceedings of\n               INFOCOM'2001, April 2001.\n\
    \   [XIAO]      X. Xiao, A. Hannan, B. Bailey, L. Ni, \"Traffic\n            \
    \   Engineering with MPLS in the Internet\", IEEE Network\n               magazine,\
    \ Mar. 2000.\n   [YARE95]    C. Yang and A. Reddy, \"A Taxonomy for Congestion\
    \ Control\n               Algorithms in Packet Switching Networks\", IEEE Network\n\
    \               Magazine, p.  34-45, 1995.\n"
- title: 13.0 Authors' Addresses
  contents:
  - "13.0 Authors' Addresses\n   Daniel O. Awduche\n   Movaz Networks\n   7926 Jones\
    \ Branch Drive, Suite 615\n   McLean, VA 22102\n   Phone: 703-298-5291\n   EMail:\
    \ awduche@movaz.com\n   Angela Chiu\n   Celion Networks\n   1 Sheila Dr., Suite\
    \ 2\n   Tinton Falls, NJ 07724\n   Phone: 732-747-9987\n   EMail: angela.chiu@celion.com\n\
    \   Anwar Elwalid\n   Lucent Technologies\n   Murray Hill, NJ 07974\n   Phone:\
    \ 908 582-7589\n   EMail: anwar@lucent.com\n   Indra Widjaja\n   Bell Labs, Lucent\
    \ Technologies\n   600 Mountain Avenue\n   Murray Hill, NJ 07974\n   Phone: 908\
    \ 582-0435\n   EMail: iwidjaja@research.bell-labs.com\n   XiPeng Xiao\n   Redback\
    \ Networks\n   300 Holger Way\n   San Jose, CA 95134\n   Phone: 408-750-5217\n\
    \   EMail: xipeng@redback.com\n"
- title: 14.0  Full Copyright Statement
  contents:
  - "14.0  Full Copyright Statement\n   Copyright (C) The Internet Society (2002).\
    \  All Rights Reserved.\n   This document and translations of it may be copied\
    \ and furnished to\n   others, and derivative works that comment on or otherwise\
    \ explain it\n   or assist in its implementation may be prepared, copied, published\n\
    \   and distributed, in whole or in part, without restriction of any\n   kind,\
    \ provided that the above copyright notice and this paragraph are\n   included\
    \ on all such copies and derivative works.  However, this\n   document itself\
    \ may not be modified in any way, such as by removing\n   the copyright notice\
    \ or references to the Internet Society or other\n   Internet organizations, except\
    \ as needed for the purpose of\n   developing Internet standards in which case\
    \ the procedures for\n   copyrights defined in the Internet Standards process\
    \ must be\n   followed, or as required to translate it into languages other than\n\
    \   English.\n   The limited permissions granted above are perpetual and will\
    \ not be\n   revoked by the Internet Society or its successors or assigns.\n \
    \  This document and the information contained herein is provided on an\n   \"\
    AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE\
    \ DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED\
    \ TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE\
    \ ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A\
    \ PARTICULAR PURPOSE.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
