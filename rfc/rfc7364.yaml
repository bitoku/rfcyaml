- title: __initial_text__
  contents:
  - '         Problem Statement: Overlays for Network Virtualization

    '
- title: Abstract
  contents:
  - "Abstract\n   This document describes issues associated with providing multi-\n\
    \   tenancy in large data center networks and how these issues may be\n   addressed\
    \ using an overlay-based network virtualization approach.  A\n   key multi-tenancy\
    \ requirement is traffic isolation so that one\n   tenant's traffic is not visible\
    \ to any other tenant.  Another\n   requirement is address space isolation so\
    \ that different tenants can\n   use the same address space within different virtual\
    \ networks.\n   Traffic and address space isolation is achieved by assigning one\
    \ or\n   more virtual networks to each tenant, where traffic within a virtual\n\
    \   network can only cross into another virtual network in a controlled\n   fashion\
    \ (e.g., via a configured router and/or a security gateway).\n   Additional functionality\
    \ is required to provision virtual networks,\n   associating a virtual machine's\
    \ network interface(s) with the\n   appropriate virtual network and maintaining\
    \ that association as the\n   virtual machine is activated, migrated, and/or deactivated.\
    \  Use of\n   an overlay-based approach enables scalable deployment on large\n\
    \   network infrastructures.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7364.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2014 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................4\n\
    \   2. Terminology .....................................................6\n  \
    \ 3. Problem Areas ...................................................6\n    \
    \  3.1. Need for Dynamic Provisioning ..............................6\n      3.2.\
    \ Virtual Machine Mobility Limitations .......................7\n      3.3. Inadequate\
    \ Forwarding Table Sizes ..........................7\n      3.4. Need to Decouple\
    \ Logical and Physical Configuration ........7\n      3.5. Need for Address Separation\
    \ between Virtual Networks .......8\n      3.6. Need for Address Separation between\
    \ Virtual Networks and ...8\n      3.7. Optimal Forwarding .........................................9\n\
    \   4. Using Network Overlays to Provide Virtual Networks .............10\n  \
    \    4.1. Overview of Network Overlays ..............................10\n    \
    \  4.2. Communication between Virtual and Non-virtualized\n           Networks\
    \ ..................................................12\n      4.3. Communication\
    \ between Virtual Networks ....................12\n      4.4. Overlay Design Characteristics\
    \ ............................13\n      4.5. Control-Plane Overlay Networking\
    \ Work Areas ...............14\n      4.6. Data-Plane Work Areas .....................................15\n\
    \   5. Related IETF and IEEE Work .....................................15\n  \
    \    5.1. BGP/MPLS IP VPNs ..........................................16\n    \
    \  5.2. BGP/MPLS Ethernet VPNs ....................................16\n      5.3.\
    \ 802.1 VLANs ...............................................17\n      5.4. IEEE\
    \ 802.1aq -- Shortest Path Bridging ....................17\n      5.5. VDP .......................................................17\n\
    \      5.6. ARMD ......................................................18\n  \
    \    5.7. TRILL .....................................................18\n    \
    \  5.8. L2VPNs ....................................................18\n      5.9.\
    \ Proxy Mobile IP ...........................................19\n      5.10. LISP\
    \ .....................................................19\n   6. Summary ........................................................19\n\
    \   7. Security Considerations ........................................19\n  \
    \ 8. References .....................................................20\n    \
    \  8.1. Normative Reference .......................................20\n      8.2.\
    \ Informative References ....................................20\n   Acknowledgments\
    \ ...................................................22\n   Contributors ......................................................22\n\
    \   Authors' Addresses ................................................23\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Data centers are increasingly being consolidated and outsourced\
    \ in an\n   effort to improve the deployment time of applications and reduce\n\
    \   operational costs.  This coincides with an increasing demand for\n   compute,\
    \ storage, and network resources from applications.  In order\n   to scale compute,\
    \ storage, and network resources, physical resources\n   are being abstracted\
    \ from their logical representation, in what is\n   referred to as server, storage,\
    \ and network virtualization.\n   Virtualization can be implemented in various\
    \ layers of computer\n   systems or networks.\n   The demand for server virtualization\
    \ is increasing in data centers.\n   With server virtualization, each physical\
    \ server supports multiple\n   virtual machines (VMs), each running its own operating\
    \ system,\n   middleware, and applications.  Virtualization is a key enabler of\n\
    \   workload agility, i.e., allowing any server to host any application\n   and\
    \ providing the flexibility of adding, shrinking, or moving\n   services within\
    \ the physical infrastructure.  Server virtualization\n   provides numerous benefits,\
    \ including higher utilization, increased\n   security, reduced user downtime,\
    \ reduced power usage, etc.\n   Multi-tenant data centers are taking advantage\
    \ of the benefits of\n   server virtualization to provide a new kind of hosting,\
    \ a virtual\n   hosted data center.  Multi-tenant data centers are ones where\n\
    \   individual tenants could belong to a different company (in the case\n   of\
    \ a public provider) or a different department (in the case of an\n   internal\
    \ company data center).  Each tenant has the expectation of a\n   level of security\
    \ and privacy separating their resources from those\n   of other tenants.  For\
    \ example, one tenant's traffic must never be\n   exposed to another tenant, except\
    \ through carefully controlled\n   interfaces, such as a security gateway (e.g.,\
    \ a firewall).\n   To a tenant, virtual data centers are similar to their physical\n\
    \   counterparts, consisting of end stations attached to a network,\n   complete\
    \ with services such as load balancers and firewalls.  But\n   unlike a physical\
    \ data center, Tenant Systems connect to a virtual\n   network (VN).  To Tenant\
    \ Systems, a virtual network looks like a\n   normal network (e.g., providing\
    \ an Ethernet or L3 service), except\n   that the only end stations connected\
    \ to the virtual network are those\n   belonging to a tenant's specific virtual\
    \ network.\n   A tenant is the administrative entity on whose behalf one or more\n\
    \   specific virtual network instances and their associated services\n   (whether\
    \ virtual or physical) are managed.  In a cloud environment, a\n   tenant would\
    \ correspond to the customer that is using a particular\n   virtual network. \
    \ However, a tenant may also find it useful to create\n   multiple different virtual\
    \ network instances.  Hence, there is a one-\n   to-many mapping between tenants\
    \ and virtual network instances.  A\n   single tenant may operate multiple individual\
    \ virtual network\n   instances, each associated with a different service.\n \
    \  How a virtual network is implemented does not generally matter to the\n   tenant;\
    \ what matters is that the service provided (Layer 2 (L2) or\n   Layer 3 (L3))\
    \ has the right semantics, performance, etc.  It could be\n   implemented via\
    \ a pure routed network, a pure bridged network, or a\n   combination of bridged\
    \ and routed networks.  A key requirement is\n   that each individual virtual\
    \ network instance be isolated from other\n   virtual network instances, with\
    \ traffic crossing from one virtual\n   network to another only when allowed by\
    \ policy.\n   For data center virtualization, two key issues must be addressed.\n\
    \   First, address space separation between tenants must be supported.\n   Second,\
    \ it must be possible to place (and migrate) VMs anywhere in\n   the data center,\
    \ without restricting VM addressing to match the\n   subnet boundaries of the\
    \ underlying data center network.\n   This document outlines problems encountered\
    \ in scaling the number of\n   isolated virtual networks in a data center.  Furthermore,\
    \ the\n   document presents issues associated with managing those virtual\n  \
    \ networks in relation to operations, such as virtual network creation/\n   deletion\
    \ and end-node membership change.  Finally, this document\n   makes the case that\
    \ an overlay-based approach has a number of\n   advantages over traditional, non-overlay\
    \ approaches.  The purpose of\n   this document is to identify the set of issues\
    \ that any solution has\n   to address in building multi-tenant data centers.\
    \  With this\n   approach, the goal is to allow the construction of standardized,\n\
    \   interoperable implementations to allow the construction of multi-\n   tenant\
    \ data centers.\n   This document is the problem statement for the \"Network\n\
    \   Virtualization over Layer 3\" (NVO3) Working Group.  NVO3 is focused\n   on\
    \ the construction of overlay networks that operate over an IP (L3)\n   underlay\
    \ transport network.  NVO3 expects to provide both L2 service\n   and IP service\
    \ to Tenant Systems (though perhaps as two different\n   solutions).  Some deployments\
    \ require an L2 service, others an L3\n   service, and some may require both.\n\
    \   Section 2 gives terminology.  Section 3 describes the problem space\n   details.\
    \  Section 4 describes overlay networks in more detail.\n   Section 5 reviews\
    \ related and further work, and Section 6 closes with\n   a summary.\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   This document uses the same terminology as [RFC7365].  In\
    \ addition,\n   this document use the following terms.\n   Overlay Network:  A\
    \ virtual network in which the separation of\n      tenants is hidden from the\
    \ underlying physical infrastructure.\n      That is, the underlying transport\
    \ network does not need to know\n      about tenancy separation to correctly forward\
    \ traffic.  IEEE 802.1\n      Provider Backbone Bridging (PBB) [IEEE-802.1Q] is\
    \ an example of an\n      L2 overlay network.  PBB uses MAC-in-MAC encapsulation\
    \ (where\n      \"MAC\" refers to \"Media Access Control\"), and the underlying\n\
    \      transport network forwards traffic using only the Backbone MAC\n      (B-MAC)\
    \ and Backbone VLAN Identifier (B-VID) in the outer header.\n      The underlay\
    \ transport network is unaware of the tenancy\n      separation provided by, for\
    \ example, a 24-bit Backbone Service\n      Instance Identifier (I-SID).\n   C-VLAN:\
    \  This document refers to Customer VLANs (C-VLANs) as\n      implemented by many\
    \ routers, i.e., an L2 virtual network\n      identified by a Customer VLAN Identifier\
    \ (C-VID).  An end station\n      (e.g., a VM) in this context that is part of\
    \ an L2 virtual network\n      will effectively belong to a C-VLAN.  Within an\
    \ IEEE 802.1Q-2011\n      network, other tags may be used as well, but such usage\
    \ is\n      generally not visible to the end station.  Section 5.3 provides\n\
    \      more details on VLANs defined by [IEEE-802.1Q].\n   This document uses\
    \ the phrase \"virtual network instance\" with its\n   ordinary meaning to represent\
    \ an instance of a virtual network.  Its\n   usage may differ from the \"VNI\"\
    \ acronym defined in the framework\n   document [RFC7365].  The \"VNI\" acronym\
    \ is not used in this document.\n"
- title: 3.  Problem Areas
  contents:
  - "3.  Problem Areas\n   The following subsections describe aspects of multi-tenant\
    \ data\n   center networking that pose problems for network infrastructure.\n\
    \   Different problem aspects may arise based on the network architecture\n  \
    \ and scale.\n"
- title: 3.1.  Need for Dynamic Provisioning
  contents:
  - "3.1.  Need for Dynamic Provisioning\n   Some service providers offer services\
    \ to multiple customers whereby\n   services are dynamic and the resources assigned\
    \ to support them must\n   be able to change quickly as demand changes.  In current\
    \ systems, it\n   can be difficult to provision resources for individual tenants\
    \ (e.g.,\n   QoS) in such a way that provisioned properties migrate automatically\n\
    \   when services are dynamically moved around within the data center to\n   optimize\
    \ workloads.\n"
- title: 3.2.  Virtual Machine Mobility Limitations
  contents:
  - "3.2.  Virtual Machine Mobility Limitations\n   A key benefit of server virtualization\
    \ is virtual machine (VM)\n   mobility.  A VM can be migrated from one server\
    \ to another live,\n   i.e., while continuing to run and without needing to shut\
    \ down and\n   restart at the new location.  A key requirement for live migration\
    \ is\n   that a VM retain critical network state at its new location,\n   including\
    \ its IP and MAC address(es).  Preservation of MAC addresses\n   may be necessary,\
    \ for example, when software licenses are bound to\n   MAC addresses.  More generally,\
    \ any change in the VM's MAC addresses\n   resulting from a move would be visible\
    \ to the VM and thus potentially\n   result in unexpected disruptions.  Retaining\
    \ IP addresses after a\n   move is necessary to prevent existing transport connections\
    \ (e.g.,\n   TCP) from breaking and needing to be restarted.\n   In data center\
    \ networks, servers are typically assigned IP addresses\n   based on their physical\
    \ location, for example, based on the Top-of-\n   Rack (ToR) switch for the server\
    \ rack or the C-VLAN configured to the\n   server.  Servers can only move to other\
    \ locations within the same IP\n   subnet.  This constraint is not problematic\
    \ for physical servers,\n   which move infrequently, but it restricts the placement\
    \ and movement\n   of VMs within the data center.  Any solution for a scalable\
    \ multi-\n   tenant data center must allow a VM to be placed (or moved) anywhere\n\
    \   within the data center without being constrained by the subnet\n   boundary\
    \ concerns of the host servers.\n"
- title: 3.3.  Inadequate Forwarding Table Sizes
  contents:
  - "3.3.  Inadequate Forwarding Table Sizes\n   Today's virtualized environments\
    \ place additional demands on the\n   forwarding tables of forwarding nodes in\
    \ the physical infrastructure.\n   The core problem is that location independence\
    \ results in specific\n   end state information being propagated into the forwarding\
    \ system\n   (e.g., /32 host routes in IPv4 networks or MAC addresses in IEEE\n\
    \   802.3 Ethernet networks).  In L2 networks, for instance, instead of\n   just\
    \ one address per server, the network infrastructure may have to\n   learn addresses\
    \ of the individual VMs (which could range in the\n   hundreds per server).  This\
    \ increases the demand on a forwarding\n   node's table capacity compared to non-virtualized\
    \ environments.\n"
- title: 3.4.  Need to Decouple Logical and Physical Configuration
  contents:
  - "3.4.  Need to Decouple Logical and Physical Configuration\n   Data center operators\
    \ must be able to achieve high utilization of\n   server and network capacity.\
    \  For efficient and flexible allocation,\n   operators should be able to spread\
    \ a virtual network instance across\n   servers in any rack in the data center.\
    \  It should also be possible\n   to migrate compute workloads to any server anywhere\
    \ in the network\n   while retaining the workload's addresses.\n   In networks\
    \ of many types (e.g., IP subnets, MPLS VPNs, VLANs, etc.),\n   moving servers\
    \ elsewhere in the network may require expanding the\n   scope of a portion of\
    \ the network (e.g., subnet, VPN, VLAN, etc.)\n   beyond its original boundaries.\
    \  While this can be done, it requires\n   potentially complex network configuration\
    \ changes and may, in some\n   cases (e.g., a VLAN or L2VPN), conflict with the\
    \ desire to bound the\n   size of broadcast domains.  In addition, when VMs migrate,\
    \ the\n   physical network (e.g., access lists) may need to be reconfigured,\n\
    \   which can be time consuming and error prone.\n   An important use case is\
    \ cross-pod expansion.  A pod typically\n   consists of one or more racks of servers\
    \ with associated network and\n   storage connectivity.  A tenant's virtual network\
    \ may start off on a\n   pod and, due to expansion, require servers/VMs on other\
    \ pods,\n   especially the case when other pods are not fully utilizing all their\n\
    \   resources.  This use case requires that virtual networks span\n   multiple\
    \ pods in order to provide connectivity to all of the tenants'\n   servers/VMs.\
    \  Such expansion can be difficult to achieve when tenant\n   addressing is tied\
    \ to the addressing used by the underlay network or\n   when the expansion requires\
    \ that the scope of the underlying C-VLAN\n   expand beyond its original pod boundary.\n"
- title: 3.5.  Need for Address Separation between Virtual Networks
  contents:
  - "3.5.  Need for Address Separation between Virtual Networks\n   Individual tenants\
    \ need control over the addresses they use within a\n   virtual network.  But\
    \ it can be problematic when different tenants\n   want to use the same addresses\
    \ or even if the same tenant wants to\n   reuse the same addresses in different\
    \ virtual networks.\n   Consequently, virtual networks must allow tenants to use\
    \ whatever\n   addresses they want without concern for what addresses are being\
    \ used\n   by other tenants or other virtual networks.\n"
- title: 3.6.  Need for Address Separation between Virtual Networks and
  contents:
  - "3.6.  Need for Address Separation between Virtual Networks and\n      Infrastructure\n\
    \   As in the previous case, a tenant needs to be able to use whatever\n   addresses\
    \ it wants in a virtual network independent of what addresses\n   the underlying\
    \ data center network is using.  Tenants (and the\n   underlay infrastructure\
    \ provider) should be able use whatever\n   addresses make sense for them without\
    \ having to worry about address\n   collisions between addresses used by tenants\
    \ and those used by the\n   underlay data center network.\n"
- title: 3.7.  Optimal Forwarding
  contents:
  - "3.7.  Optimal Forwarding\n   Another problem area relates to the optimal forwarding\
    \ of traffic\n   between peers that are not connected to the same virtual network.\n\
    \   Such forwarding happens when a host on a virtual network communicates\n  \
    \ with a host not on any virtual network (e.g., an Internet host) as\n   well\
    \ as when a host on a virtual network communicates with a host on\n   a different\
    \ virtual network.  A virtual network may have two (or\n   more) gateways for\
    \ forwarding traffic onto and off of the virtual\n   network, and the optimal\
    \ choice of which gateway to use may depend on\n   the set of available paths\
    \ between the communicating peers.  The set\n   of available gateways may not\
    \ be equally \"close\" to a given\n   destination.  The issue appears both when\
    \ a VM is initially\n   instantiated on a virtual network or when a VM migrates\
    \ or is moved\n   to a different location.  After a migration, for instance, a\
    \ VM's\n   best-choice gateway for such traffic may change, i.e., the VM may get\n\
    \   better service by switching to the \"closer\" gateway, and this may\n   improve\
    \ the utilization of network resources.\n   IP implementations in network endpoints\
    \ typically do not distinguish\n   between multiple routers on the same subnet\
    \ -- there may only be a\n   single default gateway in use, and any use of multiple\
    \ routers\n   usually considers all of them to be one hop away.  Routing protocol\n\
    \   functionality is constrained by the requirement to cope with these\n   endpoint\
    \ limitations -- for example, the Virtual Router Redundancy\n   Protocol (VRRP)\
    \ has one router serve as the master to handle all\n   outbound traffic.  This\
    \ problem can be particularly acute when the\n   virtual network spans multiple\
    \ data centers, as a VM is likely to\n   receive significantly better service\
    \ when forwarding external traffic\n   through a local router compared to using\
    \ a router at a remote data\n   center.\n   The optimal forwarding problem applies\
    \ to both outbound and inbound\n   traffic.  For outbound traffic, the choice\
    \ of outbound router\n   determines the path of outgoing traffic from the VM,\
    \ which may be\n   sub-optimal after a VM move.  For inbound traffic, the location\
    \ of\n   the VM within the IP subnet for the VM is not visible to the routers\n\
    \   beyond the virtual network.  Thus, the routing infrastructure will\n   have\
    \ no information as to which of the two externally visible\n   gateways leading\
    \ into the virtual network would be the better choice\n   for reaching a particular\
    \ VM.\n   The issue is further complicated when middleboxes (e.g., load\n   balancers,\
    \ firewalls, etc.) must be traversed.  Middleboxes may have\n   session state\
    \ that must be preserved for ongoing communication, and\n   traffic must continue\
    \ to flow through the middlebox, regardless of\n   which router is \"closest\"\
    .\n"
- title: 4.  Using Network Overlays to Provide Virtual Networks
  contents:
  - "4.  Using Network Overlays to Provide Virtual Networks\n   Virtual networks are\
    \ used to isolate a tenant's traffic from that of\n   other tenants (or even traffic\
    \ within the same tenant network that\n   requires isolation).  There are two\
    \ main characteristics of virtual\n   networks:\n   1.  Virtual networks isolate\
    \ the address space used in one virtual\n       network from the address space\
    \ used by another virtual network.\n       The same network addresses may be used\
    \ in different virtual\n       networks at the same time.  In addition, the address\
    \ space used\n       by a virtual network is independent from that used by the\n\
    \       underlying physical network.\n   2.  Virtual networks limit the scope\
    \ of packets sent on the virtual\n       network.  Packets sent by Tenant Systems\
    \ attached to a virtual\n       network are delivered as expected to other Tenant\
    \ Systems on that\n       virtual network and may exit a virtual network only\
    \ through\n       controlled exit points, such as a security gateway.  Likewise,\n\
    \       packets sourced from outside of the virtual network may enter the\n  \
    \     virtual network only through controlled entry points, such as a\n      \
    \ security gateway.\n"
- title: 4.1.  Overview of Network Overlays
  contents:
  - "4.1.  Overview of Network Overlays\n   To address the problems described in Section\
    \ 3, a network overlay\n   approach can be used.\n   The idea behind an overlay\
    \ is quite straightforward.  Each virtual\n   network instance is implemented\
    \ as an overlay.  The original packet\n   is encapsulated by the first-hop network\
    \ device, called a Network\n   Virtualization Edge (NVE), and tunneled to a remote\
    \ NVE.  The\n   encapsulation identifies the destination of the device that will\n\
    \   perform the decapsulation (i.e., the egress NVE for the tunneled\n   packet)\
    \ before delivering the original packet to the endpoint.  The\n   rest of the\
    \ network forwards the packet based on the encapsulation\n   header and can be\
    \ oblivious to the payload that is carried inside.\n   Overlays are based on what\
    \ is commonly known as a \"map-and-encap\"\n   architecture.  When processing\
    \ and forwarding packets, three distinct\n   and logically separable steps take\
    \ place:\n   1.  The first-hop overlay device implements a mapping operation that\n\
    \       determines where the encapsulated packet should be sent to reach\n   \
    \    its intended destination VM.  Specifically, the mapping function\n      \
    \ maps the destination address (either L2 or L3) of a packet\n       received\
    \ from a VM into the corresponding destination address of\n       the egress NVE\
    \ device.  The destination address will be the\n       underlay address of the\
    \ NVE device doing the decapsulation and is\n       an IP address.\n   2.  Once\
    \ the mapping has been determined, the ingress overlay NVE\n       device encapsulates\
    \ the received packet within an overlay header.\n   3.  The final step is to actually\
    \ forward the (now encapsulated)\n       packet to its destination.  The packet\
    \ is forwarded by the\n       underlay (i.e., the IP network) based entirely on\
    \ its outer\n       address.  Upon receipt at the destination, the egress overlay\
    \ NVE\n       device decapsulates the original packet and delivers it to the\n\
    \       intended recipient VM.\n   Each of the above steps is logically distinct,\
    \ though an\n   implementation might combine them for efficiency or other reasons.\n\
    \   It should be noted that in L3 BGP/VPN terminology, the above steps\n   are\
    \ commonly known as \"forwarding\" or \"virtual forwarding\".\n   The first-hop\
    \ NVE device can be a traditional switch or router or the\n   virtual switch residing\
    \ inside a hypervisor.  Furthermore, the\n   endpoint can be a VM, or it can be\
    \ a physical server.  Examples of\n   architectures based on network overlays\
    \ include BGP/MPLS IP VPNs\n   [RFC4364], Transparent Interconnection of Lots\
    \ of Links (TRILL)\n   [RFC6325], the Locator/ID Separation Protocol (LISP) [RFC6830],\
    \ and\n   Shortest Path Bridging (SPB) [IEEE-802.1aq].\n   In the data plane,\
    \ an overlay header provides a place to carry either\n   the virtual network identifier\
    \ or an identifier that is locally\n   significant to the edge device.  In both\
    \ cases, the identifier in the\n   overlay header specifies which specific virtual\
    \ network the data\n   packet belongs to.  Since both routed and bridged semantics\
    \ can be\n   supported by a virtual data center, the original packet carried\n\
    \   within the overlay header can be an Ethernet frame or just the IP\n   packet.\n\
    \   A key aspect of overlays is the decoupling of the \"virtual\" MAC and/\n \
    \  or IP addresses used by VMs from the physical network infrastructure\n   and\
    \ the infrastructure IP addresses used by the data center.  If a VM\n   changes\
    \ location, the overlay edge devices simply update their\n   mapping tables to\
    \ reflect the new location of the VM within the data\n   center's infrastructure\
    \ space.  Because an overlay network is used, a\n   VM can now be located anywhere\
    \ in the data center that the overlay\n   reaches without regard to traditional\
    \ constraints imposed by the\n   underlay network, such as the C-VLAN scope or\
    \ the IP subnet scope.\n   Multi-tenancy is supported by isolating the traffic\
    \ of one virtual\n   network instance from traffic of another.  Traffic from one\
    \ virtual\n   network instance cannot be delivered to another instance without\n\
    \   (conceptually) exiting the instance and entering the other instance\n   via\
    \ an entity (e.g., a gateway) that has connectivity to both virtual\n   network\
    \ instances.  Without the existence of a gateway entity, tenant\n   traffic remains\
    \ isolated within each individual virtual network\n   instance.\n   Overlays are\
    \ designed to allow a set of VMs to be placed within a\n   single virtual network\
    \ instance, whether that virtual network\n   provides a bridged network or a routed\
    \ network.\n"
- title: 4.2.  Communication between Virtual and Non-virtualized Networks
  contents:
  - "4.2.  Communication between Virtual and Non-virtualized Networks\n   Not all\
    \ communication will be between devices connected to\n   virtualized networks.\
    \  Devices using overlays will continue to access\n   devices and make use of\
    \ services on non-virtualized networks, whether\n   in the data center, the public\
    \ Internet, or at remote/branch\n   campuses.  Any virtual network solution must\
    \ be capable of\n   interoperating with existing routers, VPN services, load balancers,\n\
    \   intrusion-detection services, firewalls, etc., on external networks.\n   Communication\
    \ between devices attached to a virtual network and\n   devices connected to non-virtualized\
    \ networks is handled\n   architecturally by having specialized gateway devices\
    \ that receive\n   packets from a virtualized network, decapsulate them, process\
    \ them as\n   regular (i.e., non-virtualized) traffic, and finally forward them\
    \ on\n   to their appropriate destination (and vice versa).\n   A wide range of\
    \ implementation approaches are possible.  Overlay\n   gateway functionality could\
    \ be combined with other network\n   functionality into a network device that\
    \ implements the overlay\n   functionality and then forwards traffic between other\
    \ internal\n   components that implement functionality such as full router service,\n\
    \   load balancing, firewall support, VPN gateway, etc.\n"
- title: 4.3.  Communication between Virtual Networks
  contents:
  - "4.3.  Communication between Virtual Networks\n   Communication between devices\
    \ on different virtual networks is\n   handled architecturally by adding specialized\
    \ interconnect\n   functionality among the otherwise isolated virtual networks.\
    \  For a\n   virtual network providing an L2 service, such interconnect\n   functionality\
    \ could be IP forwarding configured as part of the\n   \"default gateway\" for\
    \ each virtual network.  For a virtual network\n   providing L3 service, the interconnect\
    \ functionality could be IP\n   forwarding configured as part of routing between\
    \ IP subnets, or it\n   could be based on configured inter-virtual-network traffic\
    \ policies.\n   In both cases, the implementation of the interconnect functionality\n\
    \   could be distributed across the NVEs and could be combined with other\n  \
    \ network functionality (e.g., load balancing and firewall support)\n   that is\
    \ applied to traffic forwarded between virtual networks.\n"
- title: 4.4.  Overlay Design Characteristics
  contents:
  - "4.4.  Overlay Design Characteristics\n   Below are some of the characteristics\
    \ of environments that must be\n   taken into account by the overlay technology.\n\
    \   1.  Highly distributed systems: The overlay should work in an\n       environment\
    \ where there could be many thousands of access\n       switches (e.g., residing\
    \ within the hypervisors) and many more\n       Tenant Systems (e.g., VMs) connected\
    \ to them.  This leads to a\n       distributed mapping system that puts a low\
    \ overhead on the\n       overlay tunnel endpoints.\n   2.  Many highly distributed\
    \ virtual networks with sparse membership:\n       Each virtual network could\
    \ be highly dispersed inside the data\n       center.  Also, along with expectation\
    \ of many virtual networks,\n       the number of Tenant Systems connected to\
    \ any one virtual network\n       is expected to be relatively low; therefore,\
    \ the percentage of\n       NVEs participating in any given virtual network would\
    \ also be\n       expected to be low.  For this reason, efficient delivery of\n\
    \       multi-destination traffic within a virtual network instance\n       should\
    \ be taken into consideration.\n   3.  Highly dynamic Tenant Systems: Tenant Systems\
    \ connected to\n       virtual networks can be very dynamic, both in terms of\n\
    \       creation/deletion/power-on/power-off and in terms of mobility\n      \
    \ from one access device to another.\n   4.  Be incrementally deployable, without\
    \ necessarily requiring major\n       upgrade of the entire network: The first-hop\
    \ device (or end\n       system) that adds and removes the overlay header may\
    \ require new\n       software and may require new hardware (e.g., for improved\n\
    \       performance).  The rest of the network should not need to change\n   \
    \    just to enable the use of overlays.\n   5.  Work with existing data center\
    \ network deployments without\n       requiring major changes in operational or\
    \ other practices: For\n       example, some data centers have not enabled multicast\
    \ beyond\n       link-local scope.  Overlays should be capable of leveraging\n\
    \       underlay multicast support where appropriate, but not require its\n  \
    \     enablement in order to use an overlay solution.\n   6.  Network infrastructure\
    \ administered by a single administrative\n       domain: This is consistent with\
    \ operation within a data center,\n       and not across the Internet.\n"
- title: 4.5.  Control-Plane Overlay Networking Work Areas
  contents:
  - "4.5.  Control-Plane Overlay Networking Work Areas\n   There are three specific\
    \ and separate potential work areas in the\n   area of control-plane protocols\
    \ needed to realize an overlay\n   solution.  The areas correspond to different\
    \ possible \"on-the-wire\"\n   protocols, where distinct entities interact with\
    \ each other.\n   One area of work concerns the address dissemination protocol\
    \ an NVE\n   uses to build and maintain the mapping tables it uses to deliver\n\
    \   encapsulated packets to their proper destination.  One approach is to\n  \
    \ build mapping tables entirely via learning (as is done in 802.1\n   networks).\
    \  Another approach is to use a specialized control-plane\n   protocol.  While\
    \ there are some advantages to using or leveraging an\n   existing protocol for\
    \ maintaining mapping tables, the fact that large\n   numbers of NVEs will likely\
    \ reside in hypervisors places constraints\n   on the resources (CPU and memory)\
    \ that can be dedicated to such\n   functions.\n   From an architectural perspective,\
    \ one can view the address-mapping\n   dissemination problem as having two distinct\
    \ and separable\n   components.  The first component consists of a back-end Network\n\
    \   Virtualization Authority (NVA) that is responsible for distributing\n   and\
    \ maintaining the mapping information for the entire overlay\n   system.  For\
    \ this document, we use the term \"NVA\" to refer to an\n   entity that supplies\
    \ answers, without regard to how it knows the\n   answers it is providing.  The\
    \ second component consists of the on-\n   the-wire protocols an NVE uses when\
    \ interacting with the NVA.\n   The first two areas of work are thus: describing\
    \ the NVA function and\n   defining NVA-NVE interactions.\n   The back-end NVA\
    \ could provide high performance, high resiliency,\n   failover, etc., and could\
    \ be implemented in significantly different\n   ways.  For example, one model\
    \ uses a traditional, centralized\n   \"directory-based\" database, using replicated\
    \ instances for\n   reliability and failover.  A second model involves using and\
    \ possibly\n   extending an existing routing protocol (e.g., BGP, IS-IS, etc.).\
    \  To\n   support different architectural models, it is useful to have one\n \
    \  standard protocol for the NVE-NVA interaction while allowing\n   different\
    \ protocols and architectural approaches for the NVA itself.\n   Separating the\
    \ two allows NVEs to transparently interact with\n   different types of NVAs,\
    \ i.e., either of the two architectural models\n   described above.  Having separate\
    \ protocols could also allow for a\n   simplified NVE that only interacts with\
    \ the NVA for the mapping table\n   entries it needs and allows the NVA (and its\
    \ associated protocols) to\n   evolve independently over time with minimal impact\
    \ to the NVEs.\n   A third work area considers the attachment and detachment of\
    \ VMs (or\n   Tenant Systems [RFC7365], more generally) from a specific virtual\n\
    \   network instance.  When a VM attaches, the NVE associates the VM with\n  \
    \ a specific overlay for the purposes of tunneling traffic sourced from\n   or\
    \ destined to the VM.  When a VM disconnects, the NVE should notify\n   the NVA\
    \ that the Tenant System to NVE address mapping is no longer\n   valid.  In addition,\
    \ if this VM was the last remaining member of the\n   virtual network, then the\
    \ NVE can also terminate any tunnels used to\n   deliver tenant multi-destination\
    \ packets within the VN to the NVE.\n   In the case where an NVE and hypervisor\
    \ are on separate physical\n   devices separated by an access network, a standardized\
    \ protocol may\n   be needed.\n   In summary, there are three areas of potential\
    \ work.  The first area\n   concerns the implementation of the NVA function itself\
    \ and any\n   protocols it needs (e.g., if implemented in a distributed fashion).\n\
    \   A second area concerns the interaction between the NVA and NVEs.  The\n  \
    \ third work area concerns protocols associated with attaching and\n   detaching\
    \ a VM from a particular virtual network instance.  All three\n   work areas are\
    \ important to the development of scalable,\n   interoperable solutions.\n"
- title: 4.6.  Data-Plane Work Areas
  contents:
  - "4.6.  Data-Plane Work Areas\n   The data plane carries encapsulated packets for\
    \ Tenant Systems.  The\n   data-plane encapsulation header carries a VN Context\
    \ identifier\n   [RFC7365] for the virtual network to which the data packet belongs.\n\
    \   Numerous encapsulation or tunneling protocols already exist that can\n   be\
    \ leveraged.  In the absence of strong and compelling justification,\n   it would\
    \ not seem necessary or helpful to develop yet another\n   encapsulation format\
    \ just for NVO3.\n"
- title: 5.  Related IETF and IEEE Work
  contents:
  - "5.  Related IETF and IEEE Work\n   The following subsections discuss related\
    \ IETF and IEEE work.  These\n   subsections are not meant to provide complete\
    \ coverage of all IETF\n   and IEEE work related to data centers, and the descriptions\
    \ should\n   not be considered comprehensive.  Each area aims to address\n   particular\
    \ limitations of today's data center networks.  In all\n   areas, scaling is a\
    \ common theme as are multi-tenancy and VM\n   mobility.  Comparing and evaluating\
    \ the work result and progress of\n   each work area listed is out of the scope\
    \ of this document.  The\n   intent of this section is to provide a reference\
    \ to the interested\n   readers.  Note that NVO3 is scoped to running over an\
    \ IP/L3 underlay\n   network.\n"
- title: 5.1.  BGP/MPLS IP VPNs
  contents:
  - "5.1.  BGP/MPLS IP VPNs\n   BGP/MPLS IP VPNs [RFC4364] support multi-tenancy,\
    \ VPN traffic\n   isolation, address overlapping, and address separation between\n\
    \   tenants and network infrastructure.  The BGP/MPLS control plane is\n   used\
    \ to distribute the VPN labels and the tenant IP addresses that\n   identify the\
    \ tenants (or to be more specific, the particular VPN/\n   virtual network) and\
    \ tenant IP addresses.  Deployment of enterprise\n   L3 VPNs has been shown to\
    \ scale to thousands of VPNs and millions of\n   VPN prefixes.  BGP/MPLS IP VPNs\
    \ are currently deployed in some large\n   enterprise data centers.  The potential\
    \ limitation for deploying BGP/\n   MPLS IP VPNs in data center environments is\
    \ the practicality of using\n   BGP in the data center, especially reaching into\
    \ the servers or\n   hypervisors.  There may be computing workforce skill set\
    \ issues,\n   equipment support issues, and potential new scaling challenges.\
    \  A\n   combination of BGP and lighter-weight IP signaling protocols, e.g.,\n\
    \   the Extensible Messaging and Presence Protocol (XMPP), has been\n   proposed\
    \ to extend the solutions into the data center environment\n   [END-SYSTEM] while\
    \ taking advantage of built-in VPN features with its\n   rich policy support;\
    \ it is especially useful for inter-tenant\n   connectivity.\n"
- title: 5.2.  BGP/MPLS Ethernet VPNs
  contents:
  - "5.2.  BGP/MPLS Ethernet VPNs\n   Ethernet Virtual Private Networks (E-VPNs) [EVPN]\
    \ provide an emulated\n   L2 service in which each tenant has its own Ethernet\
    \ network over a\n   common IP or MPLS infrastructure.  A BGP/MPLS control plane\
    \ is used\n   to distribute the tenant MAC addresses and the MPLS labels that\n\
    \   identify the tenants and tenant MAC addresses.  Within the BGP/MPLS\n   control\
    \ plane, a 32-bit Ethernet tag is used to identify the\n   broadcast domains (VLANs)\
    \ associated with a given L2 VLAN service\n   instance, and these Ethernet tags\
    \ are mapped to VLAN IDs understood\n   by the tenant at the service edges.  This\
    \ means that any VLAN-based\n   limitation on the customer site is associated\
    \ with an individual\n   tenant service edge, enabling a much higher level of\
    \ scalability.\n   Interconnection between tenants is also allowed in a controlled\n\
    \   fashion.\n   VM mobility [MOBILITY] introduces the concept of a combined L2/L3\
    \ VPN\n   service in order to support the mobility of individual virtual\n   machines\
    \ (VMs) between data centers connected over a common IP or\n   MPLS infrastructure.\n"
- title: 5.3.  802.1 VLANs
  contents:
  - "5.3.  802.1 VLANs\n   VLANs are a well-understood construct in the networking\
    \ industry,\n   providing an L2 service via a physical network in which tenant\n\
    \   forwarding information is part of the physical network\n   infrastructure.\
    \  A VLAN is an L2 bridging construct that provides the\n   semantics of virtual\
    \ networks mentioned above: a MAC address can be\n   kept unique within a VLAN,\
    \ but it is not necessarily unique across\n   VLANs.  Traffic scoped within a\
    \ VLAN (including broadcast and\n   multicast traffic) can be kept within the\
    \ VLAN it originates from.\n   Traffic forwarded from one VLAN to another typically\
    \ involves router\n   (L3) processing.  The forwarding table lookup operation\
    \ may be keyed\n   on {VLAN, MAC address} tuples.\n   VLANs are a pure L2 bridging\
    \ construct, and VLAN identifiers are\n   carried along with data frames to allow\
    \ each forwarding point to know\n   what VLAN the frame belongs to.  Various types\
    \ of VLANs are available\n   today and can be used for network virtualization,\
    \ even together.  The\n   C-VLAN, Service VLAN (S-VLAN), and Backbone VLAN (B-VLAN)\
    \ IDs\n   [IEEE-802.1Q] are 12 bits.  The 24-bit I-SID [IEEE-802.1aq] allows\n\
    \   the support of more than 16 million virtual networks.\n"
- title: 5.4.  IEEE 802.1aq -- Shortest Path Bridging
  contents:
  - "5.4.  IEEE 802.1aq -- Shortest Path Bridging\n   Shortest Path Bridging (SPB)\
    \ [IEEE-802.1aq] is an overlay based on\n   IS-IS that operates over L2 Ethernets.\
    \  SPB supports multipathing and\n   addresses a number of shortcomings in the\
    \ original Ethernet Spanning\n   Tree Protocol.  Shortest Path Bridging Mac (SPBM)\
    \ uses IEEE 802.1ah\n   PBB (MAC-in-MAC) encapsulation and supports a 24-bit I-SID,\
    \ which can\n   be used to identify virtual network instances.  SPBM provides\
    \ multi-\n   pathing and supports easy virtual network creation or update.\n \
    \  SPBM extends IS-IS in order to perform link-state routing among core\n   SPBM\
    \ nodes, obviating the need for bridge learning for communication\n   among core\
    \ SPBM nodes.  Learning is still used to build and maintain\n   the mapping tables\
    \ of edge nodes to encapsulate Tenant System traffic\n   for transport across\
    \ the SPBM core.\n   SPB is compatible with all other 802.1 standards and thus\
    \ allows\n   leveraging of other features, e.g., VSI Discovery Protocol (VDP),\n\
    \   Operations, Administration, and Maintenance (OAM), or scalability\n   solutions.\n"
- title: 5.5.  VDP
  contents:
  - "5.5.  VDP\n   VDP is the Virtual Station Interface (VSI) Discovery and\n   Configuration\
    \ Protocol specified by IEEE P802.1Qbg [IEEE-802.1Qbg].\n   VDP is a protocol\
    \ that supports the association of a VSI with a port.\n   VDP is run between the\
    \ end station (e.g., a server running a\n   hypervisor) and its adjacent switch\
    \ (i.e., the device on the edge of\n   the network).  VDP is used, for example,\
    \ to communicate to the switch\n   that a virtual machine (virtual station) is\
    \ moving, i.e., designed\n   for VM migration.\n"
- title: 5.6.  ARMD
  contents:
  - "5.6.  ARMD\n   The Address Resolution for Massive numbers of hosts in the Data\n\
    \   center (ARMD) WG examined data center scaling issues with a focus on\n   address\
    \ resolution and developed a problem statement document\n   [RFC6820].  While\
    \ an overlay-based approach may address some of the\n   \"pain points\" that were\
    \ raised in ARMD (e.g., better support for\n   multi-tenancy), analysis will be\
    \ needed to understand the scaling\n   trade-offs of an overlay-based approach\
    \ compared with existing\n   approaches.  On the other hand, existing IP-based\
    \ approaches such as\n   proxy ARP may help mitigate some concerns.\n"
- title: 5.7.  TRILL
  contents:
  - "5.7.  TRILL\n   TRILL is a network protocol that provides an Ethernet L2 service\
    \ to\n   end systems and is designed to operate over any L2 link type.  TRILL\n\
    \   establishes forwarding paths using IS-IS routing and encapsulates\n   traffic\
    \ within its own TRILL header.  TRILL, as originally defined,\n   supports only\
    \ the standard (and limited) 12-bit C-VID identifier.\n   Work to extend TRILL\
    \ to support more than 4094 VLANs has recently\n   completed and is defined in\
    \ [RFC7172]\n"
- title: 5.8.  L2VPNs
  contents:
  - "5.8.  L2VPNs\n   The IETF has specified a number of approaches for connecting\
    \ L2\n   domains together as part of the L2VPN Working Group.  That group,\n \
    \  however, has historically been focused on provider-provisioned L2\n   VPNs,\
    \ where the service provider participates in management and\n   provisioning of\
    \ the VPN.  In addition, much of the target environment\n   for such deployments\
    \ involves carrying L2 traffic over WANs.  Overlay\n   approaches as discussed\
    \ in this document are intended be used within\n   data centers where the overlay\
    \ network is managed by the data center\n   operator rather than by an outside\
    \ party.  While overlays can run\n   across the Internet as well, they will extend\
    \ well into the data\n   center itself (e.g., up to and including hypervisors)\
    \ and include\n   large numbers of machines within the data center itself.\n \
    \  Other L2VPN approaches, such as the Layer 2 Tunneling Protocol (L2TP)\n   [RFC3931]\
    \ require significant tunnel state at the encapsulating and\n   decapsulating\
    \ endpoints.  Overlays require less tunnel state than\n   other approaches, which\
    \ is important to allow overlays to scale to\n   hundreds of thousands of endpoints.\
    \  It is assumed that smaller\n   switches (i.e., virtual switches in hypervisors\
    \ or the adjacent\n   devices to which VMs connect) will be part of the overlay\
    \ network and\n   be responsible for encapsulating and decapsulating packets.\n"
- title: 5.9.  Proxy Mobile IP
  contents:
  - "5.9.  Proxy Mobile IP\n   Proxy Mobile IP [RFC5213] [RFC5844] makes use of the\
    \ Generic Routing\n   Encapsulation (GRE) Key Field [RFC5845] [RFC6245], but not\
    \ in a way\n   that supports multi-tenancy.\n"
- title: 5.10.  LISP
  contents:
  - "5.10.  LISP\n   LISP [RFC6830] essentially provides an IP-over-IP overlay where\
    \ the\n   internal addresses are end station identifiers and the outer IP\n  \
    \ addresses represent the location of the end station within the core\n   IP network\
    \ topology.  The LISP overlay header uses a 24-bit Instance\n   ID used to support\
    \ overlapping inner IP addresses.\n"
- title: 6.  Summary
  contents:
  - "6.  Summary\n   This document has argued that network virtualization using overlays\n\
    \   addresses a number of issues being faced as data centers scale in\n   size.\
    \  In addition, careful study of current data center problems is\n   needed for\
    \ development of proper requirements and standard solutions.\n   This document\
    \ identifies three potential control protocol work areas.\n   The first involves\
    \ a back-end NVA and how it learns and distributes\n   the mapping information\
    \ NVEs use when processing tenant traffic.  A\n   second involves the protocol\
    \ an NVE would use to communicate with the\n   back-end NVA to obtain the mapping\
    \ information.  The third potential\n   work concerns the interactions that take\
    \ place when a VM attaches or\n   detaches from a specific virtual network instance.\n\
    \   There are a number of approaches that provide some, if not all, of\n   the\
    \ desired semantics of virtual networks.  Each approach needs to be\n   analyzed\
    \ in detail to assess how well it satisfies the requirements.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   Because this document describes the problem space\
    \ associated with the\n   need for virtualization of networks in complex, large-scale,\
    \ data-\n   center networks, it does not itself introduce any security risks.\n\
    \   However, it is clear that security concerns need to be a\n   consideration\
    \ of any solutions proposed to address this problem\n   space.\n   Solutions will\
    \ need to address both data-plane and control-plane\n   security concerns.\n \
    \  In the data plane, isolation of virtual network traffic from other\n   virtual\
    \ networks is a primary concern -- for NVO3, this isolation may\n   be based on\
    \ VN identifiers that are not involved in underlay network\n   packet forwarding\
    \ between overlay edges (NVEs).  Use of a VN\n   identifier in the overlay reduces\
    \ the underlay network's role in\n   isolating virtual networks by comparison\
    \ to approaches where VN\n   identifiers are involved in packet forwarding (e.g.,\
    \ 802.1 VLANs as\n   described in Section 5.3).\n   In addition to isolation,\
    \ assurances against spoofing, snooping,\n   transit modification and denial of\
    \ service are examples of other\n   important data-plane considerations.  Some\
    \ limited environments may\n   even require confidentiality.\n   In the control\
    \ plane, the primary security concern is ensuring that\n   an unauthorized party\
    \ does not compromise the control-plane protocol\n   in ways that improperly impact\
    \ the data plane.  Some environments may\n   also be concerned about confidentiality\
    \ of the control plane.\n   More generally, denial-of-service concerns may also\
    \ be a\n   consideration.  For example, a tenant on one virtual network could\n\
    \   consume excessive network resources in a way that degrades services\n   for\
    \ other tenants on other virtual networks.\n"
- title: 8.  References
  contents:
  - '8.  References

    '
- title: 8.1.  Normative Reference
  contents:
  - "8.1.  Normative Reference\n   [RFC7365]  Lasserre, M., Balus, F., Morin, T.,\
    \ Bitar, N., and Y.\n              Rekhter, \"Framework for Data Center (DC) Network\n\
    \              Virtualization\", RFC 7365, October 2014,\n              <http://www.rfc-editor.org/info/rfc7365>.\n"
- title: 8.2.  Informative References
  contents:
  - "8.2.  Informative References\n   [END-SYSTEM]\n              Marques, P., Fang,\
    \ L., Sheth, N., Napierala, M., and N.\n              Bitar, \"BGP-signaled end-system\
    \ IP/VPNs\", Work in\n              Progress, draft-ietf-l3vpn-end-system-04,\
    \ October 2014.\n   [EVPN]     Sajassi, A., Aggarwal, R., Bitar, N., Isaac, A.,\
    \ and J.\n              Uttaro, \"BGP MPLS Based Ethernet VPN\", Work in Progress,\n\
    \              draft-ietf-l2vpn-evpn-10, October 2014.\n   [IEEE-802.1Q]\n   \
    \           IEEE, \"IEEE Standard for Local and metropolitan area\n          \
    \    networks -- Media Access Control (MAC) Bridges and Virtual\n            \
    \  Bridged Local Area Networks\", IEEE 802.1Q-2011, August\n              2011,\
    \ <http://standards.ieee.org/getieee802/\n              download/802.1Q-2011.pdf>.\n\
    \   [IEEE-802.1Qbg]\n              IEEE, \"IEEE Standard for Local and metropolitan\
    \ area\n              networks -- Media Access Control (MAC) Bridges and Virtual\n\
    \              Bridged Local Area Networks -- Amendment 21: Edge Virtual\n   \
    \           Bridging\", IEEE 802.1Qbg-2012, July 2012,\n              <http://standards.ieee.org/getieee802/\n\
    \              download/802.1Qbg-2012.pdf>.\n   [IEEE-802.1aq]\n             \
    \ IEEE, \"IEEE Standard for Local and metropolitan area\n              networks\
    \ -- Media Access Control (MAC) Bridges and Virtual\n              Bridged Local\
    \ Area Networks -- Amendment 20: Shortest Path\n              Bridging\", IEEE\
    \ 802.1aq, June 2012,\n              <http://standards.ieee.org/getieee802/\n\
    \              download/802.1aq-2012.pdf>.\n   [MOBILITY] Aggarwal, R., Rekhter,\
    \ Y., Henderickx, W., Shekhar, R.,\n              Fang, L., and A. Sajassi, \"\
    Data Center Mobility based on\n              E-VPN, BGP/MPLS IP VPN, IP Routing\
    \ and NHRP\", Work in\n              Progress, draft-raggarwa-data-center-mobility-07,\
    \ June\n              2014.\n   [RFC3931]  Lau, J., Townsley, M., and I. Goyret,\
    \ \"Layer Two Tunneling\n              Protocol - Version 3 (L2TPv3)\", RFC 3931,\
    \ March 2005,\n              <http://www.rfc-editor.org/info/rfc3931>.\n   [RFC4364]\
    \  Rosen, E. and Y. Rekhter, \"BGP/MPLS IP Virtual Private\n              Networks\
    \ (VPNs)\", RFC 4364, February 2006,\n              <http://www.rfc-editor.org/info/rfc4364>.\n\
    \   [RFC5213]  Gundavelli, S., Leung, K., Devarapalli, V., Chowdhury, K.,\n  \
    \            and B. Patil, \"Proxy Mobile IPv6\", RFC 5213, August 2008,\n   \
    \           <http://www.rfc-editor.org/info/rfc5213>.\n   [RFC5844]  Wakikawa,\
    \ R. and S. Gundavelli, \"IPv4 Support for Proxy\n              Mobile IPv6\"\
    , RFC 5844, May 2010,\n              <http://www.rfc-editor.org/info/rfc5844>.\n\
    \   [RFC5845]  Muhanna, A., Khalil, M., Gundavelli, S., and K. Leung,\n      \
    \        \"Generic Routing Encapsulation (GRE) Key Option for Proxy\n        \
    \      Mobile IPv6\", RFC 5845, June 2010,\n              <http://www.rfc-editor.org/info/rfc5845>.\n\
    \   [RFC6245]  Yegani, P., Leung, K., Lior, A., Chowdhury, K., and J.\n      \
    \        Navali, \"Generic Routing Encapsulation (GRE) Key Extension\n       \
    \       for Mobile IPv4\", RFC 6245, May 2011,\n              <http://www.rfc-editor.org/info/rfc6245>.\n\
    \   [RFC6325]  Perlman, R., Eastlake, D., Dutt, D., Gai, S., and A.\n        \
    \      Ghanwani, \"Routing Bridges (RBridges): Base Protocol\n              Specification\"\
    , RFC 6325, July 2011,\n              <http://www.rfc-editor.org/info/6325>.\n\
    \   [RFC6820]  Narten, T., Karir, M., and I. Foo, \"Address Resolution\n     \
    \         Problems in Large Data Center Networks\", RFC 6820, January\n      \
    \        2013, <http://www.rfc-editor.org/info/rfc6820>.\n   [RFC6830]  Farinacci,\
    \ D., Fuller, V., Meyer, D., and D. Lewis, \"The\n              Locator/ID Separation\
    \ Protocol (LISP)\", RFC 6830, January\n              2013, <http://www.rfc-editor.org/info/rfc6830>.\n\
    \   [RFC7172]  Eastlake, D., Zhang, M., Agarwal, P., Perlman, R., and D.\n   \
    \           Dutt, \"Transparent Interconnection of Lots of Links\n           \
    \   (TRILL): Fine-Grained Labeling\", RFC 7172, May 2014,\n              <http://www.rfc-editor.org/info/rfc7172>.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   Helpful comments and improvements to this document have come\
    \ from Lou\n   Berger, John Drake, Ilango Ganga, Ariel Hendel, Vinit Jain, Petr\n\
    \   Lapukhov, Thomas Morin, Benson Schliesser, Qin Wu, Xiaohu Xu, Lucy\n   Yong,\
    \ and many others on the NVO3 mailing list.\n   Special thanks to Janos Farkas\
    \ for his persistence and numerous\n   detailed comments related to the lack of\
    \ precision in the text\n   relating to IEEE 802.1 technologies.\n"
- title: Contributors
  contents:
  - "Contributors\n   Dinesh Dutt and Murari Sridharin were original co-authors of\
    \ the\n   Internet-Draft that led to the BoF that formed the NVO3 WG.  That\n\
    \   original draft eventually became the basis for this document.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Thomas Narten (editor)\n   IBM\n   Research Triangle Park,\
    \ NC\n   United States\n   EMail: narten@us.ibm.com\n   Eric Gray (editor)\n \
    \  Ericsson\n   EMail: eric.gray@ericsson.com\n   David Black\n   EMC Corporation\n\
    \   176 South Street\n   Hopkinton, MA  01748\n   United States\n   EMail: david.black@emc.com\n\
    \   Luyuan Fang\n   Microsoft\n   5600 148th Ave NE\n   Redmond, WA  98052\n \
    \  United States\n   EMail: lufang@microsoft.com\n   Lawrence Kreeger\n   Cisco\n\
    \   170 W. Tasman Avenue\n   San Jose, CA  95134\n   United States\n   EMail:\
    \ kreeger@cisco.com\n   Maria Napierala\n   AT&T\n   200 S. Laurel Avenue\n  \
    \ Middletown, NJ  07748\n   United States\n   EMail: mnapierala@att.com\n"
