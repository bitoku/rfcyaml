- title: __initial_text__
  contents:
  - "           Lightweight Directory Access Protocol (version 3)\n              \
    \         Replication Requirements\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2002).  All Rights Reserved.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document discusses the fundamental requirements for replication\n\
    \   of data accessible via the Lightweight Directory Access Protocol\n   (version\
    \ 3) (LDAPv3).  It is intended to be a gathering place for\n   general replication\
    \ requirements needed to provide interoperability\n   between informational directories.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   7    References....................................................13\n\
    \   A    Appendix A - Usage Scenarios..................................15\n  \
    \ A.1  Extranet Example..............................................15\n   A.2\
    \  Consolidation Example.........................................15\n   A.3  Replication\
    \ Heterogeneous Deployment Example..................16\n   A.4  Shared Name Space\
    \ Example.....................................16\n   A.5  Supplier Initiated Replication................................16\n\
    \   A.6  Consumer Initiated Replication................................17\n  \
    \ A.7  Prioritized attribute replication.............................17\n   A.8\
    \  Bandwidth issues..............................................17\n   A.9  Interoperable\
    \ Administration and Management...................18\n   A.10 Enterprise Directory\
    \ Replication Mesh.........................18\n   A.11 Failure of the Master in\
    \ a Master-Slave Replicated Directory..19\n   A.12 Failure of a Directory Holding\
    \ Critical Service Information...19\n   B    Appendix B - Rationale........................................20\n\
    \   B.1  Meta-Data Implications........................................20\n  \
    \ B.2  Order of Transfer for Replicating Data........................20\n   B.3\
    \  Schema Mismatches and Replication.............................21\n   B.4  Detecting\
    \ and Repairing Inconsistencies Among Replicas........22\n   B.5  Some Test Cases\
    \ for Conflict Resolution in Multi-Master\n        Replication...................................................23\n\
    \   B.6  Data Confidentiality and Data Integrity During Replication....27\n  \
    \ B.7  Failover in Single-Master Systems.............................27\n   B.8\
    \  Including Operational Attributes in Atomic Operations.........29\n        Authors'\
    \ Addresses............................................30\n        Full Copyright\
    \ Statement......................................31\n"
- title: 1  Introduction
  contents:
  - "1  Introduction\n   Distributing directory information throughout the network\
    \ provides a\n   two-fold benefit: (1) it increases the reliability of the directory\n\
    \   through fault tolerance, and (2) it brings the directory content\n   closer\
    \ to the clients using the data.  LDAP's success as an access\n   protocol for\
    \ directory information is driving the need to distribute\n   LDAP directory content\
    \ within the enterprise and Internet.\n   Currently, LDAP does not define a replication\
    \ mechanism, and mentions\n   LDAP shadow servers (see [RFC2251]) in passing.\
    \  A standard mechanism\n   for directory replication in a multi-vendor environment\
    \ is critical\n   to the continued success of LDAP in the market place.\n   This\
    \ document sets out the requirements for replication between\n   multiple LDAP\
    \ servers.  While RFC 2251 and RFC 2252 [RFC2252] set\n   forth the standards\
    \ for communication between LDAP clients and\n   servers there are additional\
    \ requirements for server-to-server\n   communication.  Some of these are covered\
    \ here.\n   This document first introduces the terminology to be used, then\n\
    \   presents the different replication models being considered.\n   Requirements\
    \ follow, along with security considerations.  The\n   reasoning that leads to\
    \ the requirements is presented in the\n   Appendices.  This was done to provide\
    \ a clean separation of the\n   requirements from their justification.\n   The\
    \ key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\
    \   SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n\
    \   document are to be interpreted as described in [RFC2119].\n"
- title: 2  Terminology
  contents:
  - "2  Terminology\n   The following terms are used in this document:\n   Anonymous\
    \ Replication - Replication where the endpoints are\n   identified to each other\
    \ but not authenticated.  Also known as\n   \"unauthenticated replication\".\n\
    \   Area of replication - A whole or portion of a Directory Information\n   Tree\
    \ (DIT) that makes up a distinct unit of data to be replicated.\n   An area of\
    \ replication is defined by a replication base entry and\n   includes all or some\
    \ of the depending entries contained therein on a\n   single server.  It divides\
    \ directory data into partitions whose\n   propagation behavior may be independently\
    \ configured from other\n   partitions.  Areas of replication may overlap or be\
    \ nested.  This is\n   a subset of the definition of a \"replicated area\" in\
    \ X.525 [X.525].\n   Atomic operation - A set of changes to directory data which\
    \ the LDAP\n   standards guarantee will be treated as a unit; all changes will\
    \ be\n   made or all the changes will fail.\n   Atomicity Information - Information\
    \ about atomic operations passed as\n   part of replication.\n   Conflict - A\
    \ situation that arises when changes are made to the same\n   directory data on\
    \ different directory servers before replication can\n   synchronize the data\
    \ on the servers.  When the servers do\n   synchronize, they have inconsistent\
    \ data - a conflict.\n   Conflict resolution - Deterministic procedures used to\
    \ resolve change\n   information conflicts that may arise during replication.\n\
    \   Critical OID - Attributes or object classes defined in the\n   replication\
    \ agreement as being critical to the operation of the\n   system.  Changes affecting\
    \ critical OIDs cause immediate initiation\n   of a replica cycle.  An example\
    \ of a critical OID might be a password\n   or certificate.\n   Fractional replication\
    \ - The capability to filter a subset of\n   attributes for replication.\n   Incremental\
    \ Update - An update that contains only those attributes or\n   entries that have\
    \ changed.\n   Master Replica - A replica that may be directly updated via LDAP\n\
    \   operations.  In a Master-Slave Replication system, the Master Replica\n  \
    \ is the only directly updateable replica in the replica-group.\n   Master-Slave,\
    \ or Single Master Replication - A replication model that\n   assumes only one\
    \ server, the master, allows LDAP write access to the\n   replicated data.  Note\
    \ that Master-Slave replication can be\n   considered a proper subset of multi-master\
    \ replication.\n   Meta-Data - Data collected by the replication system that describes\n\
    \   the status/state of replication.\n   Multi-Master Replication - A replication\
    \ model where entries can be\n   written and updated on any of several master\
    \ replica copies without\n   requiring communication with other master replicas\
    \ before the write\n   or update is performed.\n   One-way Replication  - The\
    \ process of synchronization in a single\n   direction where the authoritative\
    \ source information is provided to a\n   replica.\n   Partial Replication - Partial\
    \ Replication is Fractional Replication,\n   Sparse Replication, or both.\n  \
    \ Propagation Behavior - The behavior of the synchronization process\n   between\
    \ a consumer and a supplier.\n   Replica - An instance of an area of replication\
    \ on a server.\n   Replica-Group - The servers that hold instances of a particular\
    \ area\n   of replication.  A server may be part of several replica-groups.\n\
    \   Replica (or Replication) Cycle - The interval during which update\n   information\
    \ is exchanged between two or more replicas.  It begins\n   during an attempt\
    \ to push data to, or pull data from, another replica\n   or set of replicas,\
    \ and ends when the data has successfully been\n   exchanged or an error is encountered.\n\
    \   Replication - The process of synchronizing data distributed across\n   directory\
    \ servers and rectifying update conflicts.\n   Replication Agreement - A collection\
    \ of information describing the\n   parameters of replication between two or more\
    \ servers in a replica-\n   group.\n   Replication Base Entry - The distinguished\
    \ name of the root vertex of\n   a replicated area.\n   Replication Initiation\
    \ Conflict - A Replication Initiation Conflict\n   is a situation where two sources\
    \ want to update the same replica at\n   the same time.\n   Replication Session\
    \ - A session set up between two servers in a\n   replica-group to pass update\
    \ information as part of a replica cycle.\n   Slave (or Read-Only) Replica - A\
    \ replica that cannot be directly\n   updated via LDAP requests.  Changes may\
    \ only be made via replication\n   from a master replica.  Read-only replicas\
    \ may occur in both single-\n   and multi-master systems.\n   Sparse Replication\
    \ - The capability to filter some subset of entries\n   (other than a complete\
    \ collection) of an area of replication.\n   Topology - The shape of the directed\
    \ graph describing the\n   relationships between replicas.\n   Two-way Replication\
    \ - The process of synchronization where change\n   information flows bi-directionally\
    \ between two replicas.\n   Unauthenticated Replication - See Anonymous Replication.\n\
    \   Update Propagation - Protocol-based process by which directory\n   replicas\
    \ are reconciled.\n"
- title: 3  The Models
  contents:
  - "3  The Models\n   The objective is to provide an interoperable, LDAPv3 directory\n\
    \   synchronization protocol that is simple, efficient and flexible;\n   supporting\
    \ both multi-master and master-slave replication.  The\n   protocol must meet\
    \ the needs of both the Internet and enterprise\n   environments.\n   There are\
    \ five data consistency models.\n   Model 1: Transactional Consistency -- Environments\
    \ that exhibit all\n   four of the ACID properties (Atomicity, Consistency, Isolation,\n\
    \   Durability) [ACID].\n   Model 2: Eventual (or Transient) Consistency -- Environments\
    \ where\n   definite knowledge of the topology is provided through predetermined\n\
    \   replication agreements.  Examples include X.500 Directories (the\n   X.500\
    \ model is single-master only) [X.501, X.525], Bayou [XEROX], and\n   NDS (Novell\
    \ Directory Services) [NDS].  In this model, every update\n   propagates to every\
    \ replica that it can reach via a path of stepwise\n   eventual connectivity.\n\
    \   Model 3: Limited Effort Eventual (or Probabilistic) Consistency --\n   Environments\
    \ that provide a statistical probability of convergence\n   with knowledge of\
    \ topology.  An example is the Xerox Clearinghouse\n   [XEROX2].  This model is\
    \ similar to \"Eventual Consistency\", except\n   where replicas may purge updates.\
    \  Purging drops propagation changes\n   when some replica time boundary is exceeded,\
    \ thus leaving some\n   changes replicated to only a portion of the topology.\
    \  Transactional\n   consistency is not preserved, though some weaker constraints\
    \ on\n   consistency are available.\n   Model 4: Loosest Consistency -- Environments\
    \ where information is\n   provided from an opportunistic or simple cache until\
    \ stale.  Complete\n   knowledge of topology may not be shared among all replicas.\n\
    \   Model 5: Ad hoc -- A copy of a data store where no follow up checks\n   are\
    \ made for the accuracy/freshness of the data.\n   Consistency models 1, 2 and\
    \ 3 involve the use of prearranged\n   replication agreements among servers. \
    \ While model 1 may simplify\n   support for atomicity in multi-master systems,\
    \ the added complexity\n   of the distributed 2-phase commit required for Model\
    \ 1 is\n   significant; therefor, model 1 will not be considered at this time.\n\
    \   Models 4 and 5 involve unregistered replicas that \"pull\" updates from\n\
    \   another directory server without that server's knowledge.  These\n   models\
    \ violate a directory's security policies.\n   Models 2 and 3 illustrate two replication\
    \ scenarios that must be\n   handled:  policy configuration through security management\
    \ parameters\n   (model 2), and hosting relatively static data and address information\n\
    \   as in white-pages applications (model 3).  Therefore, replication\n   requirements\
    \ are presented for models 2 and 3.\n   Interoperability among directories using\
    \ LDAP replication may be\n   limited for implementations that add semantics beyond\
    \ those specified\n   by the LDAP core documents (RFC 2251-2256, 2829, 2830).\
    \  In addition,\n   the \"core\" specifications include numerous features which\
    \ are not\n   mandatory-to-implement (e.g., RECOMMENDED or OPTIONAL).  There are\n\
    \   also numerous elective extensions.  Thus LDAP replication\n   interoperability\
    \ between independent implementations of LDAP which\n   support different options\
    \ may be limited.  Use of applicability\n   statements to improve interoperability\
    \ in particular application\n   spaces is RECOMMENDED.\n"
- title: 4  Requirements
  contents:
  - '4  Requirements

    '
- title: 4.1 General
  contents:
  - "4.1 General\n   G1.  LDAP Replication MUST support models 2 (Eventual Consistency)\n\
    \        and 3 (Limited Effort Eventual Consistency) above.\n   G2.  LDAP Replication\
    \ SHOULD NOT preclude support for model 1\n        (Transactional Consistency)\
    \ in the future.\n   G3.  LDAP replication SHOULD have minimal impact on system\n\
    \        performance.\n   G4.  The LDAP Replication Standard SHOULD NOT limit\
    \ the replication\n        transaction rate.\n   G5.  The LDAP replication standard\
    \ SHOULD NOT limit the size of an\n        area of replication or a replica.\n\
    \   G6.  Meta-data collected by the LDAP replication mechanism MUST NOT\n    \
    \    grow without bound.\n   G7.  All policy and state data pertaining to replication\
    \ MUST be\n        accessible via LDAP.\n   G8.  LDAP replication MUST be capable\
    \ of replicating the following:\n        - all userApplication attribute types\n\
    \        - all directoryOperation and distributedOperation attribute\n       \
    \   types defined in the LDAP \"core\" specifications (RFCs 2251-\n          2256,\
    \ 2829-2830)\n        - attribute subtypes\n        - attribute description options\
    \ (e.g., \";binary\" and Language\n          Tags [RFC2596])\n   G9.  LDAP replication\
    \ SHOULD support replication of\n        directoryOperation and distributedOperation\
    \ attribute types\n        defined in standards track LDAP extensions.\n   G10.\
    \ LDAP replication MUST NOT support replication of dsaOperation\n        attribute\
    \ types as such attributes are DSA-specific.\n   G11. The LDAP replication system\
    \ should limit impact on the network\n        by minimizing the number of messages\
    \ and the amount of traffic\n        sent.\n"
- title: 4.2 Model
  contents:
  - "4.2 Model\n   M1.  The model MUST support the following triggers for initiation\
    \ of\n        a replica cycle:\n        a) A configurable set of scheduled times\n\
    \        b) Periodically, with a configurable period between replica\n       \
    \    cycles\n        c) A configurable maximum amount of time between replica\
    \ cycles\n        d) A configurable number of accumulated changes\n        e)\
    \ Change in the value of a critical OID\n        f) As the result of an automatic\
    \ rescheduling after a\n           replication initiation conflict\n        g)\
    \ A manual request for immediate replication\n        With the exception of manual\
    \ request, the specific trigger(s)\n        and related parameters for a given\
    \ server MUST be identified in\n        a well-known place defined by the standard,\
    \ e.g., the\n        Replication Agreement(s).\n   M2.  The replication model\
    \ MUST support both master-slave and multi-\n        master relationships.\n \
    \  M3.  An attribute in an entry MUST eventually converge to the same\n      \
    \  set of values in every replica holding that entry.\n   M4.  LDAP replication\
    \ MUST encompass schema definitions, attribute\n        names and values, access\
    \ control information, knowledge\n        information, and name space information.\n\
    \   M5.  LDAP replication MUST NOT require that all copies of the\n        replicated\
    \ information be complete, but MAY require that at\n        least one copy be\
    \ complete.  The model MUST support Partial\n        Replicas.\n   M6.  The determination\
    \ of which OIDs are critical MUST be\n        configurable in the replication\
    \ agreement.\n   M7.  The parameters of the replication process among the members\
    \ of\n        the replica-group, including access parameters, necessary\n    \
    \    authentication credentials, assurances of confidentiality\n        (encryption),\
    \ and area(s) of replication MUST be defined in a\n        standard location (e.g.,\
    \ the replication agreements).\n   M8.  The replication agreements SHOULD accommodate\
    \ multiple servers\n        receiving the same area of replication under a single\
    \ predefined\n        agreement.\n   M9.  LDAP replication MUST provide scalability\
    \ to both enterprise and\n        Internet environments, e.g., an LDAP server\
    \ must be able to\n        provide replication services to replicas within an\
    \ enterprise as\n        well as across the Internet.\n   M10. While different\
    \ directory implementations can support\n        different/extended schema, schema\
    \ mismatches between two\n        replicating servers MUST be handled.  One way\
    \ of handling such\n        mismatches might be to raise an error condition.\n\
    \   M11. There MUST be a facility that can update, or totally refresh, a\n   \
    \     replica-group from a standard data format, such as LDIF format\n       \
    \ [RFC2849].\n   M12. An update received by a consumer more than once MUST NOT\
    \ produce\n        a different outcome than if the update were received only once.\n"
- title: 4.3 Protocol
  contents:
  - "4.3 Protocol\n   P1.  The replication protocol MUST provide for recovery and\n\
    \        rescheduling of a replication session due to replication\n        initiation\
    \ conflicts (e.g., consumer busy replicating with other\n        servers) and\
    \ or loss of connection (e.g., supplier cannot reach\n        a replica).\n  \
    \ P2.  LDUP replication SHOULD NOT send an update to a consumer if the\n     \
    \   consumer has previously acknowledged that update.\n   P3.  The LDAP replication\
    \ protocol MUST allow for full update to\n        facilitate replica initialization\
    \ and reset loading utilizing a\n        standardized format such as LDIF [RFC2849]\
    \ format.\n   P4.  Incremental replication MUST be allowed.\n   P5.  The replication\
    \ protocol MUST allow either a master or slave\n        replica to initiate the\
    \ replication process.\n   P6.  The protocol MUST preserve atomicity of LDAP operations\
    \ as\n        defined in RFC2251 [RFC2251].  In a multi-master environment\n \
    \       this may lead to an unresolvable conflict.  MM5 and MM6 discuss\n    \
    \    how to handle this situation.\n   P7.  The protocol MUST support a mechanism\
    \ to report schema\n        mismatches between replicas discovered during a replication\n\
    \        session.\n"
- title: 4.4 Schema
  contents:
  - "4.4 Schema\n   SC1.  A standard way to determine what replicas are held on a\
    \ server\n         MUST be defined.\n   SC2.  A standard schema for representing\
    \ replication agreements MUST\n         be defined.\n   SC3.  The semantics associated\
    \ with modifying the attributes of\n         replication agreements MUST be defined.\n\
    \   SC4.  A standard method for determining the location of replication\n    \
    \     agreements MUST be defined.\n   SC5.  A standard schema for publishing state\
    \ information about a\n         given replica MUST be defined.\n   SC6.  A standard\
    \ method for determining the location of replica state\n         information MUST\
    \ be defined.\n   SC7.  It MUST be possible for appropriately authorized\n   \
    \      administrators, regardless of their network location, to access\n     \
    \    replication agreements in the DIT.\n   SC8.  Replication agreements of all\
    \ servers containing replicated\n         information MUST be accessible via LDAP.\n\
    \   SC9.  An entry MUST be uniquely identifiable throughout its lifetime.\n"
- title: 4.5 Single Master
  contents:
  - "4.5 Single Master\n   SM1.  A Single Master system SHOULD provide a fast method\
    \ of\n         promoting a slave replica to become the master replica.\n   SM2.\
    \  The master replica in a Single Master system SHOULD send all\n         changes\
    \ to read-only replicas in the order in which the master\n         applied them.\n"
- title: 4.6 Multi-Master
  contents:
  - "4.6 Multi-Master\n   MM1.  The replication protocol SHOULD NOT saturate the network\
    \ with\n         redundant or unnecessary entry replication.\n   MM2.  The initiator\
    \ MUST be allowed to determine whether it will\n         become a consumer or\
    \ supplier during the synchronization\n         startup process.\n   MM3.  During\
    \ a replica cycle, it MUST be possible for the two servers\n         to switch\
    \ between the consumer and supplier roles.\n   MM4.  When multiple master replicas\
    \ want to start a replica cycle\n         with the same replica at the same time,\
    \ the model MUST have an\n         automatic and deterministic mechanism for resolving\
    \ or avoiding\n         replication initiation conflict.\n   MM5.  Multi-master\
    \ replication MUST NOT lose information during\n         replication.  If conflict\
    \ resolution would result in the loss\n         of directory information, the\
    \ replication process MUST store\n         that information, notify the administrator\
    \ of the nature of the\n         conflict and the information that was lost, and\
    \ provide a\n         mechanism for possible override by the administrator.\n\
    \   MM6.  Multi-master replication MUST support convergence of the values\n  \
    \       of attributes and entries.  Convergence may result in an event\n     \
    \    as described in MM5.\n   MM7.  Multi-master conflict resolution MUST NOT\
    \ depend on the in-\n         order arrival of changes at a replica to assure\
    \ eventual\n         convergence.\n   MM8.  Multi-master replication MUST support\
    \ read-only replicas as\n         well as read-write replicas.\n"
- title: 4.7 Administration and Management
  contents:
  - "4.7 Administration and Management\n   AM1.  Replication agreements MUST allow\
    \ the initiation of a replica\n         cycle to be administratively postponed\
    \ to a more convenient\n         period.\n   AM2.  Each copy of a replica MUST\
    \ maintain audit history information\n         of which servers it has replicated\
    \ with and which servers have\n         replicated with it.\n   AM3.  Access to\
    \ replication agreements, topologies, and policy\n         attributes MUST be\
    \ provided through LDAP.\n   AM4.  The capability to check the differences between\
    \ two replicas\n         for the same information SHOULD be provided.\n   AM5.\
    \  A mechanism to fix differences between replicas without\n         triggering\
    \ new replica cycles SHOULD be provided.\n   AM6.  The sequence of updates to\
    \ access control information (ACI) and\n         the data controlled by that ACI\
    \ MUST be maintained by\n         replication.\n   AM7.  It MUST be possible to\
    \ add a 'blank' replica to a replica-\n         group, and force a full update\
    \ from (one of) the Master(s), for\n         the purpose of adding a new directory\
    \ server to the system.\n   AM8.  Vendors SHOULD provide tools to audit schema\
    \ compatibility\n         within a potential replica-group.\n"
- title: 4.8 Security
  contents:
  - "4.8 Security\n   The terms \"data confidentiality\" and \"data integrity\" are\
    \ defined in\n   the Internet Security Glossary [RFC2828].\n   S1.  The protocol\
    \ MUST support mutual authentication of the source\n        and the replica directories\
    \ during initialization of a\n        replication session.\n   S2.  The protocol\
    \ MUST support mutual verification of authorization\n        of the source to\
    \ send and the replica to receive replicated data\n        during initialization\
    \ of a replication session.\n   S3.  The protocol MUST also support the initialization\
    \ of anonymous\n        replication sessions.\n   S4.  The replication protocol\
    \ MUST support transfer of data with data\n        integrity and data confidentiality.\n\
    \   S5.  The replication protocol MUST support the ability during\n        initialization\
    \ of a replication session for an authenticated\n        source and replica to\
    \ mutually decide to disable data integrity\n        and data confidentiality\
    \ within the context of and for the\n        duration of that particular replication\
    \ session.\n   S6.  To promote interoperability, there MUST be a mandatory-to-\n\
    \        implement data confidentiality mechanism.\n   S7.  The transport for\
    \ administrative access MUST permit assurance of\n        the integrity and confidentiality\
    \ of all data transferred.\n   S8.  To support data integrity, there must be a\
    \ mandatory-to-\n        implement data integrity mechanism.\n"
- title: 5  Security Considerations
  contents:
  - "5  Security Considerations\n   This document includes security requirements (listed\
    \ in section 4.8\n   above) for the replication model and protocol.  As noted\
    \ in Section\n   3, interoperability may be impacted when replicating among servers\n\
    \   that implement non-standard extensions to basic LDAP semantics.\n   Security-related\
    \ and general LDAP interoperability will be\n   significantly impacted by the\
    \ degree of consistency with which\n   implementations support existing and future\
    \ standards detailing LDAP\n   security models, such as a future standard LDAP\
    \ access control model.\n"
- title: 6  Acknowledgements
  contents:
  - "6  Acknowledgements\n   This document is based on input from IETF members interested\
    \ in LDUP\n   Replication.\n"
- title: 7  References
  contents:
  - "7  References\n   [ACID]    T. Haerder, A. Reuter, \"Principles of Transaction-Oriented\n\
    \             Database Recovery\", Computing Surveys, Vol. 15, No. 4\n       \
    \      (December 1983), pp. 287-317.\n   [NDS]     Novell, \"NDS Technical Overview\"\
    , 104-000223-001,\n             http://developer.novell.com/ndk/doc/ndslib/dsov_enu/data/\n\
    \             h6tvg4z7.html, September, 2000.\n   [RFC2119] Bradner, S., \"Key\
    \ Words for Use in RFCs to Indicate\n             Requirement Levels\", BCP 14,\
    \ RFC 2119, March 1997.\n   [RFC2251] Wahl, M., Howes, T. and S. Kille, \"Lightweight\
    \ Directory\n             Access Protocol\", RFC 2251, December 1997.\n   [RFC2252]\
    \ Wahl, M., Coulbeck, A., Howes, T. and S. Kille,\n             \"Lightweight\
    \ Directory Access Protocol (v3): Attribute\n             Syntax Definitions\"\
    , RFC 2252, December 1997.\n   [RFC2253] Kille, S., Wahl, M. and T. Howes, \"\
    Lightweight Directory\n             Access Protocol (v3): UTF-8 String Representation\
    \ of\n             Distinguished Names\", RFC 2253, December 1997.\n   [RFC2254]\
    \ Howes, T., \"The String Representation of LDAP Search\n             Filters\"\
    , RFC 2254, December 1997.\n   [RFC2255] Howes, T. and M. Smith, \"The LDAP URL\
    \ Format\", RFC 2255,\n             December 1997.\n   [RFC2256] Wahl, M., \"\
    A Summary of the X.500(96) User Schema for use\n             with LDAPv3\", RFC\
    \ 2256, December 1997.\n   [RFC2596] Wahl, M. and T. Howes, \"Use of Language\
    \ Codes in LDAP\", RFC\n             2596, May 1999.\n   [RFC2828] Shirey, R.\
    \ \"Internet Security Glossary\", FYI 36, RFC 2828,\n             May 2000.\n\
    \   [RFC2829] Wahl, M., Alvestrand, H., Hodges, J. and R. Morgan,\n          \
    \   \"Authentication Methods for LDAP\", RFC 2829, May 2000.\n   [RFC2830] Hodges,\
    \ J., Morgan, R. and M. Wahl, \"Lightweight Directory\n             Access Protocol\
    \ (v3): Extension for Transport Layer\n             Security\", RFC 2830, May\
    \ 2000.\n   [RFC2849] Good, G., \"The LDAP Data Interchange Format (LDIF)\", RFC\n\
    \             2849, June 2000.\n   [X.501]   ITU-T Recommendation X.501 (1993),\
    \ | ISO/IEC 9594-2: 1993,\n             Information Technology - Open Systems\
    \ Interconnection - The\n             Directory: Models.\n   [X.525]   ITU-T Recommendation\
    \ X.525 (1997), | ISO/IEC 9594-9: 1997,\n             Information Technology -\
    \ Open Systems Interconnection - The\n             Directory: Replication.\n \
    \  [XEROX]   C. Hauser, \"Managing update conflicts in Bayou, a weakly\n     \
    \        connected replicated storage system\". Palo Alto, CA: Xerox\n       \
    \      PARC, Computer Science Laboratory; 1995 August; CSL-95-4.\n   [XEROX2]\
    \  Alan D. Demers, Mark Gealy, Daniel Greene, Carl Hauser,\n             Wesley\
    \ Irish, John Larson, Sue Manning, Scott Shenker,\n             Howard Sturgis,\
    \ Daniel Swinehart, Douglas Terry, Don Woods,\n             \"Epidemic Algorithms\
    \ for Replicated Database Maintenance\".\n             Palo Alto, CA, Xerox PARC,\
    \ January 1989.\n"
- title: A. APPENDIX A - Usage Scenarios
  contents:
  - "A. APPENDIX A - Usage Scenarios\n   The following directory deployment examples\
    \ are intended to validate\n   our replication requirements.  A heterogeneous\
    \ set of directory\n   implementations is assumed for all the cases below.  This\
    \ material is\n   intended as background; no requirements are presented in this\n\
    \   Appendix.\n"
- title: A.1. Extranet Example
  contents:
  - "A.1. Extranet Example\n   A company has a trading partner with whom it wishes\
    \ to share\n   directory information.  This information may be as simple as a\n\
    \   corporate telephone directory, or as complex as an extranet workflow\n   application.\
    \  For performance reasons, the company wishes to place a\n   replica of its directory\
    \ within the Partner Company, rather than\n   exposing its directory beyond its\
    \ firewall.\n   The requirements that follow from this scenario are:\n   - One-way\
    \ replication, single mastered.\n   - Authentication of clients.\n   - Common\
    \ access control and access control identification.\n   - Secure transmission\
    \ of updates.\n   - Selective attribute replication (Fractional Replication),\
    \ so that\n     only partial entries can be replicated.\n"
- title: A.2. Consolidation Example
  contents:
  - "A.2. Consolidation Example\n   Company A acquires company B.  Each company has\
    \ an existing\n   directory.\n   During the transition period, as the organizations\
    \ are merged, both\n   directory services must coexist.  Company A may wish to\
    \ attach\n   company B's directory to its own.\n   The requirements that follow\
    \ from this scenario are:\n   - Multi-Master replication.\n   - Common access\
    \ control model. Access control model identification.\n   - Secure transmission\
    \ of updates.\n   - Replication between DITs with potentially differing schema.\n"
- title: A.3. Replication Heterogeneous Deployment Example
  contents:
  - "A.3. Replication Heterogeneous Deployment Example\n   An organization may choose\
    \ to deploy directory implementations from\n   multiple vendors, to enjoy the\
    \ distinguishing benefits of each.\n   In this case, multi-master replication\
    \ is required to ensure that the\n   multiple replicas of the DIT are synchronized.\
    \  Some vendors may\n   provide directory clients, which are tied to their own\
    \ directory\n   service.\n   The requirements that follow from this scenario are:\n\
    \   - Multi-Master replication\n   - Common access control model and access control\
    \ model\n     identification.\n   - Secure transmission of updates.\n   - Replication\
    \ among DITs with potentially differing schemas.\n"
- title: A.4. Shared Name Space Example
  contents:
  - "A.4. Shared Name Space Example\n   Two organizations may choose to cooperate\
    \ on some venture and need a\n   shared name space to manage their operation.\
    \  Both organizations will\n   require administrative rights over the shared name\
    \ space.\n   The requirements that follow from this scenario are:\n   - Multi-Master\
    \ replication.\n   - Common access control model and access control model\n  \
    \   identification.\n   - Secure transmission of updates.\n"
- title: A.5. Supplier Initiated Replication
  contents:
  - "A.5. Supplier Initiated Replication\n   This is a single master environment that\
    \ maintains a number of\n   replicas of the DIT by pushing changes based on a\
    \ defined schedule.\n   The requirements that follow from this scenario are:\n\
    \   - Single-master environment.\n   - Supplier-initiated replication.\n   - Secure\
    \ transmission of updates.\n"
- title: A.6. Consumer Initiated Replication
  contents:
  - "A.6. Consumer Initiated Replication\n   Again a single mastered replication topology,\
    \ but the slave replica\n   initiates the replication exchange rather than the\
    \ master.  An\n   example of this is a replica that resides on a laptop computer\
    \ that\n   may run disconnected for a period of time.\n   The requirements that\
    \ follow from this scenario are:\n   - Single-master environment.\n   - Consumer\
    \ initiated replication.\n   - Open scheduling (anytime).\n"
- title: A.7. Prioritized attribute replication
  contents:
  - "A.7. Prioritized attribute replication\n   The password attribute can provide\
    \ an example of the requirement for\n   prioritized attribute replication.  A\
    \ user is working in Utah and the\n   administrator resides in California.  The\
    \ user has forgotten his\n   password.  So the user calls or emails the administrator\
    \ to request a\n   new password.  The administrator provides the updated password\
    \ (a\n   change).\n   Under normal conditions, the directory replicates to a number\
    \ of\n   different locations overnight.  But corporate security policy states\n\
    \   that passwords are critical and the new value must be available\n   immediately\
    \ (e.g., shortly) after any change.  Replication needs to\n   occur immediately\
    \ for critical attributes/entries.\n   The requirements that follow from this\
    \ scenario are:\n   - Incremental replication of changes.\n   - Immediate replication\
    \ on change of certain attributes.\n   - Replicate based on time/attribute semantics.\n"
- title: A.8. Bandwidth issues
  contents:
  - "A.8. Bandwidth issues\n   The replication of Server (A) R/W replica (a) in Kathmandu\
    \ is handled\n   via a dial up phone link to Paris where server (B) R/W replica\
    \ of (a)\n   resides.  Server (C) R/W replica of (a) is connected by a T1\n  \
    \ connection to server (B).  Each connection has a different\n   performance characteristic.\n\
    \   The requirements that follow from this scenario are:\n   - Minimize repetitive\
    \ updates when replicating from multiple\n     replication paths.\n   - Incremental\
    \ replication of changes.\n   - Provide replication cycles to delay and/or retry\
    \ when connections\n     cannot be reached.\n   - Allowances for consumer initiated\
    \ or supplier initiated\n     replication.\n"
- title: A.9. Interoperable Administration and Management
  contents:
  - "A.9. Interoperable Administration and Management\n   The administrator with administrative\
    \ authority of the corporate\n   directory which is replicated by numerous geographically\
    \ dispersed\n   LDAP servers from different vendors notices that the replication\n\
    \   process is not completing correctly as the change log is continuing\n   to\
    \ grow and/or error messages inform him.  The administrator uses his\n   $19.95\
    \ RepCo LDAP directory replication diagnostic tools to look at\n   Root DSE replica\
    \ knowledge on server 17 and determines that server 42\n   made by LDAP'RUS Inc.\
    \ is not replicating properly due to an object\n   conflict.  Using his Repco\
    \ Remote repair tools he connects to server\n   42 and resolves the conflict on\
    \ the remote server.\n   The requirements that follow from this scenario are:\n\
    \   - Provide replication audit history.\n   - Provide mechanisms for managing\
    \ conflict resolution.\n   - Provide LDAP access to predetermined agreements,\
    \ topology and\n     policy attributes.\n   - Provide operations for comparing\
    \ replica's content for validity.\n   - Provide LDAP access to status and audit\
    \ information.\n"
- title: A.10. Enterprise Directory Replication Mesh
  contents:
  - "A.10. Enterprise Directory Replication Mesh\n   A Corporation builds a mesh of\
    \ directory servers within the\n   enterprise utilizing LDAP servers from various\
    \ vendors.  Five servers\n   are holding the same area of replication.  The predetermined\n\
    \   replication agreement(s) for the enterprise mesh are under a single\n   management,\
    \ and the security domain allows a single predetermined\n   replication agreement\
    \ to manage the 5 servers' replication.\n   The requirements that follow from\
    \ this scenario are:\n   - One predefined replication agreement that manages a\
    \ single area of\n     replication that is held on numerous servers.\n   - Common\
    \ support of replication management knowledge across vendor\n     implementation.\n\
    \   - Rescheduling and continuation of a replication cycle when one\n     server\
    \ in a replica-group is busy and/or unavailable.\n"
- title: A.11. Failure of the Master in a Master-Slave Replicated Directory
  contents:
  - "A.11. Failure of the Master in a Master-Slave Replicated Directory\n   A company\
    \ has a corporate directory that is used by the corporate\n   email system.  The\
    \ directory is held on a mesh of servers from\n   several vendors.  A corporate\
    \ relocation results in the closing of\n   the location where the master copy\
    \ of the directory is located.\n   Employee information (such as mailbox locations\
    \ and employee\n   certificate information) must be kept up to date or mail cannot\
    \ be\n   delivered.\n   The requirements that follow from this scenario are:\n\
    \   - An existing slave replica must be \"promote-able\" to become the new\n \
    \    master.\n   - The \"promotion\" must be done without significant downtime,\
    \ since\n     updates to the directory will continue.\n"
- title: A.12. Failure of a Directory Holding Critical Service Information
  contents:
  - "A.12. Failure of a Directory Holding Critical Service Information\n   An ISP\
    \ uses a policy management system that uses a directory as the\n   policy data\
    \ repository.  The directory is replicated in several\n   different sites on different\
    \ vendors' products to avoid single points\n   of failure.  It is imperative that\
    \ the directory be available and be\n   updateable even if one site is disconnected\
    \ from the network.\n   Changes to the data must be traceable, and it must be\
    \ possible to\n   determine how changes made from different sites interacted.\n\
    \   The requirements that follow from this scenario are:\n   - Multi-master replication.\n\
    \   - Ability to reschedule replication sessions.\n   - Support for manual review\
    \ and override of replication conflict\n     resolution.\n"
- title: B. APPENDIX B - Rationale
  contents:
  - "B. APPENDIX B - Rationale\n   This Appendix gives some of the background behind\
    \ the requirements.\n   It is included to help the protocol designers understand\
    \ the thinking\n   behind some of the requirements and to present some of the\
    \ issues\n   that should be considered during design.  With the exception of\n\
    \   section B.8, which contains a suggested requirement for the update to\n  \
    \ RFC 2251, this Appendix does not state any formal requirements.\n"
- title: B.1. Meta-Data Implications
  contents:
  - "B.1. Meta-Data Implications\n   Requirement G4 states that meta-data must not\
    \ grow without bound.\n   This implies that meta-data must, at some point, be\
    \ purged from the\n   system.  This, in turn, raises concerns about stability.\
    \  Purging\n   meta-data before all replicas have been updated may lead to\n \
    \  incomplete replication of change information and inconsistencies\n   among\
    \ replicas.  Therefore, care must be taken setting up the rules\n   for purging\
    \ meta-data from the system while still ensuring that\n   meta-data will not grow\
    \ forever.\n"
- title: B.2. Order of Transfer for Replicating Data
  contents:
  - "B.2. Order of Transfer for Replicating Data\n   Situations may arise where it\
    \ would be beneficial to replicate data\n   out-of-order (e.g., send data to consumer\
    \ replicas in a different\n   order than it was processed at the supplier replica).\
    \  One such case\n   might occur if a large bulk load was done on the master server\
    \ in a\n   single-master environment and then a single change to a critical OID\n\
    \   (a password change, for example) was then made.  Rather than wait for\n  \
    \ all the bulk data to be sent to the replicas, the password change\n   might\
    \ be moved to the head of the queue and be sent before all the\n   bulk data was\
    \ transferred.  Other cases where this might be\n   considered are schema changes\
    \ or changes to critical policy data\n   stored in the directory.\n   While there\
    \ are practical benefits to allowing out-of-order transfer,\n   there are some\
    \ negative consequences as well.  Once out-of-order\n   transfers are permitted,\
    \ all receiving replicas must be prepared to\n   deal with data and schema conflicts\
    \ that might arise.\n   As an example, assume that schema changes are critical\
    \ and must be\n   moved to the front of the replication queue.  Now assume that\
    \ a\n   schema change deletes an attribute for some object class.  It is\n   possible\
    \ that some of the operations ahead of the schema change in\n   the queue are\
    \ operations to delete values of the soon-to-be-deleted\n   attribute so that\
    \ the schema change can be done with no problems.  If\n   the schema change moves\
    \ to the head of the queue, the consumer\n   servers might have to delete an attribute\
    \ that still has values, and\n   then receive requests to delete the values of\
    \ an attribute that is no\n   longer defined.\n   In the multi-master case, similar\
    \ situations can arise when\n   simultaneous changes are made to different replicas.\
    \  Thus, multi-\n   master systems must have conflict resolution algorithms in\
    \ place to\n   handle such situations.  But in the single-master case conflict\n\
    \   resolution is not needed unless the master is allowed to send data\n   out-of-order.\
    \  This is the reasoning behind requirement SM2, which\n   recommends that data\
    \ always be sent in order in single-master\n   replication.\n   Note that even\
    \ with this restriction, the concept of a critical OID\n   is still useful in\
    \ single-master replication.  An example of its\n   utility can be found in section\
    \ A.7.\n"
- title: B.3. Schema Mismatches and Replication
  contents:
  - "B.3. Schema Mismatches and Replication\n   Multi-vendor environments are the\
    \ primary area of interest for LDAP\n   replication standards.  Some attention\
    \ must thus be paid to the issue\n   of schema mismatches, since they can easily\
    \ arise when vendors\n   deliver slightly different base schema with their directory\
    \ products.\n   Even when both products meet the requirements of the standards\n\
    \   [RFC2252], the vendors may have included additional attributes or\n   object\
    \ classes with their products.  When two different vendors'\n   products attempt\
    \ to replicate, these additions can cause schema\n   mismatches.  Another potential\
    \ cause of schema mismatches is\n   discussed in section A.3.\n   There are only\
    \ a few possible responses when a mismatch is\n   discovered.\n   - Raise an error\
    \ condition and ignore the data.  This should always\n     be allowed and is the\
    \ basis for requirement P8 and the comment on\n     M10.\n   - Map/convert the\
    \ data to the form required by the consuming replica.\n     A system may choose\
    \ this course; requirement M10 is intended to\n     allow this option.  The extent\
    \ of the conversion is up to the\n     implementation; in the extreme it could\
    \ support use of the\n     replication protocol in meta-directories.\n   - Quietly\
    \ ignore (do not store on the consumer replica and do not\n     raise an error\
    \ condition) any data that does not conform to the\n     schema at the consumer.\n\
    \   Requirement M10 is intended to exclude the last option.\n   Requirement AM8\
    \ suggests that vendors should provide tools to help\n   discover schema mismatches\
    \ when replication is being set up.  But\n   schema will change after the initial\
    \ setup, so the replication system\n   must be prepared to handle unexpected mismatches.\n\
    \   Normal IETF practice in protocol implementation suggests that one be\n   strict\
    \ in what one sends and be flexible in what one receives.  The\n   parallel in\
    \ this case is that a supplier should be prepared to\n   receive an error notification\
    \ for any schema mismatch, but a consumer\n   may choose to do a conversion instead.\n\
    \   The other option that can be considered in this situation is the use\n   of\
    \ fractional replication.  If replication is set up so only the\n   common attributes\
    \ are replicated, mismatches can be avoided.\n   One additional consideration\
    \ here is replication of the schema\n   itself.  M4 requires that it be possible\
    \ to replicate schema.  If a\n   consumer replica is doing conversion, extreme\
    \ care should be taken if\n   schema elements are replicated since some attributes\
    \ are intended to\n   have different definitions on different replicas.\n   For\
    \ fractional replication, the protocol designers and implementors\n   should give\
    \ careful consideration to the way they handle schema\n   replication.  Some options\
    \ for schema replication include:\n   - All schema elements are replicated.\n\
    \   - Schema elements are replicated only if they are used by attributes\n   \
    \  that are being replicated.\n   - Schema are manually configured on the servers\
    \ involved in\n     fractional replication; schema elements are not replicated\
    \ via the\n     protocol.\n"
- title: B.4. Detecting and Repairing Inconsistencies Among Replicas
  contents:
  - "B.4. Detecting and Repairing Inconsistencies Among Replicas\n   Despite the best\
    \ efforts of designers, implementors, and operators,\n   inconsistencies will\
    \ occasionally crop up among replicas in\n   production directories.  Tools will\
    \ be needed to detect and to\n   correct these inconsistencies.\n   A special\
    \ client may accomplish detection through periodic\n   comparisons of replicas.\
    \  This client would typically read two\n   replicas of the same replication base\
    \ entry and compare the answers,\n   possibly by BINDing to each of the two replicas\
    \ to be compared and\n   reading them both.  In cases where the directory automatically\n\
    \   reroutes some requests (e.g., chaining), mechanisms to force access\n   to\
    \ a particular replica should be supplied.\n   Alternatively, the server could\
    \ support a special request to handle\n   this situation.  A client would invoke\
    \ an operation at some server.\n   It would cause that server to extract the contents\
    \ from some other\n   server it has a replication agreement with and report the\
    \ differences\n   back to the client as the result.\n   If an inconsistency is\
    \ found, it needs to be repaired.  To determine\n   the appropriate repair, the\
    \ administrator will need access to the\n   replication history to figure out\
    \ how the inconsistency occurred and\n   what the correct repair should be.\n\
    \   When a repair is made, it should be restricted to the replica that\n   needs\
    \ to be fixed; the repair should not cause new replication events\n   to be started.\
    \  This may require special tools to change the local\n   data store without triggering\
    \ replication.\n   Requirements AM2, AM4, and AM5 address these needs.\n"
- title: B.5. Some Test Cases for Conflict Resolution in Multi-Master Replication
  contents:
  - "B.5. Some Test Cases for Conflict Resolution in Multi-Master Replication\n  \
    \ Use of multi-master replication inevitably leads to the possibility\n   that\
    \ incompatible changes will be made simultaneously on different\n   servers. \
    \ In such cases, conflict resolution algorithms must be\n   applied.\n   As a\
    \ guiding principle, conflict resolution should avoid surprising\n   the user.\
    \  One way to do this is to adopt the principle that, to the\n   extent possible,\
    \ conflict resolution should mimic the situation that\n   would happen if there\
    \ were a single server where all the requests\n   were handled.\n   While this\
    \ is a useful guideline, there are some situations where it\n   is impossible\
    \ to implement.  Some of these cases are examined in this\n   section.  In particular,\
    \ there are some cases where data will be\n   \"lost\" in multi-master replication\
    \ that would not be lost in a\n   single-server configuration.\n   In the examples\
    \ below, assume that there are three replicas, A, B,\n   and C.  All three replicas\
    \ are updateable.  Changes are made to\n   replicas A and B before replication\
    \ allows either replica to see the\n   change made on the other.  In discussion\
    \ of the multi-master cases,\n   we assume that the change to A takes precedence\
    \ using whatever rules\n   are in force for conflict resolution.\n"
- title: B.5.1. Create-Create
  contents:
  - "B.5.1. Create-Create\n   A user creates a new entry with distinguished name DN\
    \ on A.  At the\n   same time, a different user adds an entry with the same distinguished\n\
    \   name on B.\n   In the single-server case, one of the create operations would\
    \ have\n   occurred before the other, and the second request would have failed.\n\
    \   In the multi-master case, each create was successful on its\n   originating\
    \ server.  The problem is not detected until replication\n   takes place.  When\
    \ a replication request to create a DN that already\n   exists arrives at one\
    \ of the servers, conflict resolution is invoked.\n   (Note that the two requests\
    \ can be distinguished even though they\n   have the same DN because every entry\
    \ has some sort of unique\n   identifier per requirement SC9.)\n   As noted above,\
    \ in these discussions we assume that the change from\n   replica A has priority\
    \ based on the conflict resolution algorithm.\n   Whichever change arrives first,\
    \ requirement MM6 says that the values\n   from replica A must be those in place\
    \ on all replicas at the end of\n   the replication cycle.  Requirement MM5 states\
    \ that the system cannot\n   quietly ignore the values from replica B.\n   The\
    \ values from replica B might be logged with some notice to the\n   administrators,\
    \ or they might be added to the DIT with a machine\n   generated DN (again with\
    \ notice to the administrators).  If they are\n   stored with a machine generated\
    \ DN, the same DN must be used on all\n   servers in the replica-group (otherwise\
    \ requirement M3 would be\n   violated).  Note that in the case where the entry\
    \ in question is a\n   container, storage with a machine generated DN provides\
    \ a place where\n   descendent entries may be stored if any descendents were generated\n\
    \   before the replication cycle was completed.\n   In any case, some mechanism\
    \ must be provided to allow the\n   administrator to reverse the conflict resolution\
    \ algorithm and force\n   the values originally created on B into place on all\
    \ replicas if\n   desired.\n"
- title: B.5.2. Rename-Rename
  contents:
  - "B.5.2. Rename-Rename\n   On replica A, an entry with distinguished name DN1 is\
    \ renamed to DN.\n   At the same time on replica B, an entry with distinguished\
    \ name DN2\n   is renamed to DN.\n   In the single-server case, one rename operation\
    \ would occur before\n   the other and the second would fail since the target\
    \ name already\n   exists.\n   In the multi-master case, each rename was successful\
    \ on its\n   originating server.  Assuming that the change on A has priority in\n\
    \   the conflict resolution sense, DN will be left with the values from\n   DN1\
    \ in all replicas and DN1 will no longer exist in any replica.  The\n   question\
    \ is what happens to DN2 and its original values.\n   Requirement MM5 states that\
    \ these values must be stored somewhere.\n   They might be logged, they might\
    \ be left in the DIT as the values of\n   DN2, or they might be left in the DIT\
    \ as the values of some machine\n   generated DN.  Leaving them as the values\
    \ of DN2 is attractive since\n   it is the same as the single-server case, but\
    \ if a new DN2 has\n   already been created before the replica cycle finishes,\
    \ there are\n   some very complex cases to resolve.  Any of the solutions described\n\
    \   in this paragraph would be consistent with requirement MM5.\n"
- title: B.5.3. Locking Based on Atomicity of ModifyRequest
  contents:
  - "B.5.3. Locking Based on Atomicity of ModifyRequest\n   There is an entry with\
    \ distinguished name DN that contains attributes\n   X, Y, and Z.  The value of\
    \ X is 1.  On replica A, a ModifyRequest is\n   processed which includes modifications\
    \ to change that value of X from\n   1 to 0 and to set the value of Y to \"USER1\"\
    .  At the same time,\n   replica B processes a ModifyRequest which includes modifications\
    \ to\n   change the value of X from 1 to 0 and to set the value of Y to\n   \"\
    USER2\" and the value of Z to 42.  The application in this case is\n   using X\
    \ as a lock and is depending on the atomic nature of\n   ModifyRequests to provide\
    \ mutual exclusion for lock access.\n   In the single-server case, the two operations\
    \ would have occurred\n   sequentially.  Since a ModifyRequest is atomic, the\
    \ entire first\n   operation would succeed.  The second ModifyRequest would fail,\
    \ since\n   the value of X would be 0 when it was attempted, and the modification\n\
    \   changing X from 1 to 0 would thus fail.  The atomicity rule would\n   cause\
    \ all other modifications in the ModifyRequest to fail as well.\n   In the multi-master\
    \ case, it is inevitable that at least some of the\n   changes will be reversed\
    \ despite the use of the lock.  Assuming the\n   changes from A have priority\
    \ per the conflict resolution algorithm,\n   the value of X should be 0 and the\
    \ value of Y should be \"USER1\" The\n   interesting question is the value of\
    \ Z at the end of the replication\n   cycle.  If it is 42, the atomicity constraint\
    \ on the change from B\n   has been violated.  But for it to revert to its previous\
    \ value,\n   grouping information must be retained and it is not clear when that\n\
    \   information can be safely discarded.  Thus, requirement G6 may be\n   violated.\n"
- title: B.5.4. General Principles
  contents:
  - "B.5.4. General Principles\n   With multi-master replication there are a number\
    \ of cases where a\n   user or application will complete a sequence of operations\
    \ with a\n   server but those actions are later \"undone\" because someone else\n\
    \   completed a conflicting set of operations at another server.\n   To some extent,\
    \ this can happen in any multi-user system.  If a user\n   changes the value of\
    \ an attribute and later reads it back,\n   intervening operations by another\
    \ user may have changed the value.\n   In the multi-master case, the problem is\
    \ worsened, since techniques\n   used to resolve the problem in the single-server\
    \ case won't work as\n   shown in the examples above.\n   The major question here\
    \ is one of intended use.  In LDAP standards\n   work, it has long been said that\
    \ replication provides \"loose\n   consistency\" among replicas.  At several IETF\
    \ meetings and on the\n   mailing list, usage examples from finance where locking\
    \ is required\n   have been declared poor uses for LDAP.  Requirement G1 is consistent\n\
    \   with this history.  But if loose consistency is the goal, the locking\n  \
    \ example above is an inappropriate use of LDAP, at least in a\n   replicated\
    \ environment.\n"
- title: B.5.5. Avoiding the Problem
  contents:
  - "B.5.5. Avoiding the Problem\n   The examples above discuss some of the most difficult\
    \ problems that\n   can arise in multi-master replication.  While they can be\
    \ dealt with,\n   dealing with them is difficult and can lead to situations that\
    \ are\n   quite confusing to the application and to users.\n   The common characteristics\
    \ of the examples are:\n   - Several directory users/applications are changing\
    \ the same data.\n   - They are changing the data before previous changes have\
    \ replicated.\n   - They are using different directory servers to make these changes.\n\
    \   - They are changing data that are parts of a distinguished name or\n     they\
    \ are using ModifyRequest to both read and write a given\n     attribute value\
    \ in a single atomic request.\n   If any one of these conditions is reversed,\
    \ the types of problems\n   described above will not occur.  There are many useful\
    \ applications\n   of multi-master directories where at least one of the above\n\
    \   conditions does not occur.  For cases where all four do occur,\n   application\
    \ designers should be aware of the possible consequences.\n"
- title: B.6. Data Confidentiality and Data Integrity During Replication
  contents:
  - "B.6. Data Confidentiality and Data Integrity During Replication\n   Directories\
    \ will frequently hold proprietary information.  Policy\n   information, name\
    \ and address information, and customer lists can be\n   quite proprietary and\
    \ are likely to be stored in directories.  Such\n   data must be protected against\
    \ intercept or modification during\n   replication.\n   In some cases, the network\
    \ environment (e.g., a private network) may\n   provide sufficient data confidentiality\
    \ and integrity for the\n   application.  In other cases, the data in the directory\
    \ may be public\n   and not require protection.  For these reasons data confidentiality\n\
    \   and integrity were not made requirements for all replication\n   sessions.\
    \  But there are a substantial number of applications that\n   will need data\
    \ confidentiality and integrity for replication, so\n   there is a requirement\
    \ (S4) that the protocol allow for data\n   confidentiality and integrity in those\
    \ cases where they are needed.\n   Typically, the policy on the use of confidentiality\
    \ and integrity\n   measures would be held in the replication agreement per requirement\n\
    \   M7.\n   This leaves the question of what mechanism(s) to use.  While this\
    \ is\n   ultimately a design/implementation decision, replication across\n   different\
    \ vendors' directory products is an important goal of the\n   LDAP replication\
    \ work at the IETF.  If different vendors choose to\n   support different data\
    \ confidentiality and integrity mechanisms, the\n   advantages of a standard replication\
    \ protocol would be lost.  Thus\n   there is a requirement (S6) for mandatory-to-implement\
    \ data\n   confidentiality and integrity mechanisms.\n   Anonymous replication\
    \ (requirement S3) is supported since it may be\n   useful in the same sorts of\
    \ situations where data integrity and data\n   confidentiality protection are\
    \ not needed.\n"
- title: B.7. Failover in Single-Master Systems
  contents:
  - "B.7. Failover in Single-Master Systems\n   In a single-master system, all modifications\
    \ must originate at the\n   master.  The master is therefore a single point of\
    \ failure for\n   modifications.  This can cause concern when high availability\
    \ is a\n   requirement for the directory system.\n   One way to reduce the problem\
    \ is to provide a failover process that\n   converts a slave replica to master\
    \ when the original master fails.\n   The time required to execute the failover\
    \ process then becomes a\n   major factor in availability of the system as a whole.\n\
    \   Factors that designers and implementors should consider when working\n   on\
    \ failover include:\n   - If the master replica contains control information or\
    \ meta-data\n     that is not part of the slave replica(s), this information will\n\
    \     have to be inserted into the slave that is being \"promoted\" to\n     master\
    \ as part of the failover process.  Since the old master is\n     presumably unavailable\
    \ at this point, it may be difficult to obtain\n     this data.  For example,\
    \ if the master holds the status information\n     of all replicas, but each slave\
    \ replica only holds its own status\n     information, failover would require\
    \ that the new master get the\n     status of all existing replicas, presumably\
    \ from those replicas.\n     Similar issues could arise for replication agreements\
    \ if the master\n     is the only system that holds a complete set.\n   - If data\
    \ privacy mechanisms (e.g., encryption) are in use during\n     replication, the\
    \ new master would need to have the necessary key\n     information to talk to\
    \ all of the slave replicas.\n   - It is not only the new master that needs to\
    \ be reconfigured.  The\n     slaves also need to have their configurations updated\
    \ so they know\n     where updates should come from and where they should refer\n\
    \     modifications.\n   - The failover mechanism should be able to handle a situation\
    \ where\n     the old master is \"broken\" but not \"dead\".  The slave replicas\n\
    \     should ignore updates from the old master after failover is\n     initiated.\n\
    \   - The old master will eventually be repaired and returned to the\n     replica-group.\
    \  It might join the group as a slave and pick up the\n     changes it has \"\
    missed\" from the new master, or there might be some\n     mechanism to bring\
    \ it into sync with the new master and then let it\n     take over as master.\
    \  Some resynchronization mechanism will be\n     needed.\n   - Availability would\
    \ be maximized if the whole failover process could\n     be automated (e.g., failover\
    \ is initiated by an external system\n     when it determines that the original\
    \ master is not functioning\n     properly).\n"
- title: B.8. Including Operational Attributes in Atomic Operations
  contents:
  - "B.8. Including Operational Attributes in Atomic Operations\n   LDAPv3 [RFC2251]\
    \ declares that some operations are atomic (e.g., all\n   of the modifications\
    \ in a single ModifyRequest).  It also defines\n   several operational attributes\
    \ that store information about when\n   changes are made to the directory (createTimestamp,\
    \ etc.) and which\n   ID was responsible for a given change (modifiersName, etc.).\n\
    \   Currently, there is no statement in RFC2251 requiring that changes to\n  \
    \ these operational attributes be atomic with the changes to the data.\n   It\
    \ is RECOMMENDED that this requirement be added during the revision\n   of RFC2251.\
    \  In the interim, replication SHOULD treat these\n   operations as though such\
    \ a requirement were in place.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Russel F. Weiser\n   Digital Signature Trust Co.\n   1095\
    \ East 2100 South\n   Suite #201\n   Salt Lake City, UT 84106\n   Phone: +1 801\
    \ 326 5421\n   Fax:  +1 801 326 5421\n   EMail: rweiser@trustdst.com\n   Ellen\
    \ J. Stokes\n   IBM\n   11400 Burnet Rd.\n   Austin, TX  78758\n   Phone: +1 512\
    \ 436 9098\n   Fax: +1 512 436 1193\n   EMail: stokese@us.ibm.com\n   Ryan D.\
    \ Moats\n   Lemur Networks\n   15621 Drexel Circle\n   Omaha, NE  68135\n   Phone:\
    \ +1 402 894 9456\n   EMail: rmoats@lemurnetworks.net\n   Richard V. Huber\n \
    \  Room C3-3B30\n   AT&T Laboratories\n   200 Laurel Avenue South\n   Middletown,\
    \ NJ  07748\n   Phone: +1 732 420 2632\n   Fax: +1 732 368 1690\n   EMail: rvh@att.com\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The Internet Society (2002).  All\
    \ Rights Reserved.\n   This document and translations of it may be copied and\
    \ furnished to\n   others, and derivative works that comment on or otherwise explain\
    \ it\n   or assist in its implementation may be prepared, copied, published\n\
    \   and distributed, in whole or in part, without restriction of any\n   kind,\
    \ provided that the above copyright notice and this paragraph are\n   included\
    \ on all such copies and derivative works.  However, this\n   document itself\
    \ may not be modified in any way, such as by removing\n   the copyright notice\
    \ or references to the Internet Society or other\n   Internet organizations, except\
    \ as needed for the purpose of\n   developing Internet standards in which case\
    \ the procedures for\n   copyrights defined in the Internet Standards process\
    \ must be\n   followed, or as required to translate it into languages other than\n\
    \   English.\n   The limited permissions granted above are perpetual and will\
    \ not be\n   revoked by the Internet Society or its successors or assigns.\n \
    \  This document and the information contained herein is provided on an\n   \"\
    AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE\
    \ DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED\
    \ TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE\
    \ ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A\
    \ PARTICULAR PURPOSE.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
