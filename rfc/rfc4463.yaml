- title: __initial_text__
  contents:
  - "                A Media Resource Control Protocol (MRCP)\n              Developed\
    \ by Cisco, Nuance, and Speechworks\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2006).\n"
- title: IESG Note
  contents:
  - "IESG Note\n   This RFC is not a candidate for any level of Internet Standard.\
    \  The\n   IETF disclaims any knowledge of the fitness of this RFC for any\n \
    \  purpose and in particular notes that the decision to publish is not\n   based\
    \ on IETF review for such things as security, congestion control,\n   or inappropriate\
    \ interaction with deployed protocols.  The RFC Editor\n   has chosen to publish\
    \ this document at its discretion.  Readers of\n   this document should exercise\
    \ caution in evaluating its value for\n   implementation and deployment.  See\
    \ RFC 3932 for more information.\n   Note that this document uses a MIME type\
    \ 'application/mrcp' which has\n   not been registered with the IANA, and is therefore\
    \ not recognized as\n   a standard IETF MIME type.  The historical value of this\
    \ document as\n   an ancestor to ongoing standardization in this space, however,\
    \ makes\n   the publication of this document meaningful.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document describes a Media Resource Control Protocol (MRCP)\
    \ that\n   was developed jointly by Cisco Systems, Inc., Nuance Communications,\n\
    \   and Speechworks, Inc.  It is published as an RFC as input for further\n  \
    \ IETF development in this area.\n   MRCP controls media service resources like\
    \ speech synthesizers,\n   recognizers, signal generators, signal detectors, fax\
    \ servers, etc.,\n   over a network.  This protocol is designed to work with streaming\n\
    \   protocols like RTSP (Real Time Streaming Protocol) or SIP (Session\n   Initiation\
    \ Protocol), which help establish control connections to\n   external media streaming\
    \ devices, and media delivery mechanisms like\n   RTP (Real Time Protocol).\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \   2. Architecture ....................................................4\n  \
    \    2.1. Resources and Services .....................................4\n    \
    \  2.2. Server and Resource Addressing .............................5\n   3. MRCP\
    \ Protocol Basics ............................................5\n      3.1. Establishing\
    \ Control Session and Media Streams .............5\n      3.2. MRCP over RTSP\
    \ .............................................6\n      3.3. Media Streams and\
    \ RTP Ports ................................8\n   4. Notational Conventions ..........................................8\n\
    \   5. MRCP Specification ..............................................9\n  \
    \    5.1. Request ...................................................10\n    \
    \  5.2. Response ..................................................10\n      5.3.\
    \ Event .....................................................12\n      5.4. Message\
    \ Headers ...........................................12\n   6. Media Server ...................................................19\n\
    \      6.1. Media Server Session ......................................19\n  \
    \ 7. Speech Synthesizer Resource ....................................21\n    \
    \  7.1. Synthesizer State Machine .................................22\n      7.2.\
    \ Synthesizer Methods .......................................22\n      7.3. Synthesizer\
    \ Events ........................................23\n      7.4. Synthesizer Header\
    \ Fields .................................23\n      7.5. Synthesizer Message Body\
    \ ..................................29\n      7.6. SET-PARAMS ................................................32\n\
    \      7.7. GET-PARAMS ................................................32\n  \
    \    7.8. SPEAK .....................................................33\n    \
    \  7.9. STOP ......................................................34\n      7.10.\
    \ BARGE-IN-OCCURRED ........................................35\n      7.11. PAUSE\
    \ ....................................................37\n      7.12. RESUME ...................................................37\n\
    \      7.13. CONTROL ..................................................38\n  \
    \    7.14. SPEAK-COMPLETE ...........................................40\n    \
    \  7.15. SPEECH-MARKER ............................................41\n   8. Speech\
    \ Recognizer Resource .....................................42\n      8.1. Recognizer\
    \ State Machine ..................................42\n      8.2. Recognizer Methods\
    \ ........................................42\n      8.3. Recognizer Events .........................................43\n\
    \      8.4. Recognizer Header Fields ..................................43\n  \
    \    8.5. Recognizer Message Body ...................................51\n    \
    \  8.6. SET-PARAMS ................................................56\n      8.7.\
    \ GET-PARAMS ................................................56\n      8.8. DEFINE-GRAMMAR\
    \ ............................................57\n      8.9. RECOGNIZE .................................................60\n\
    \      8.10. STOP .....................................................63\n  \
    \    8.11. GET-RESULT ...............................................64\n    \
    \  8.12. START-OF-SPEECH ..........................................64\n      8.13.\
    \ RECOGNITION-START-TIMERS .................................65\n      8.14. RECOGNITON-COMPLETE\
    \ ......................................65\n      8.15. DTMF Detection ...........................................67\n\
    \   9. Future Study ...................................................67\n  \
    \ 10. Security Considerations .......................................67\n   11.\
    \ RTSP-Based Examples ...........................................67\n   12. Informative\
    \ References ........................................74\n   Appendix A. ABNF Message\
    \ Definitions ..............................76\n   Appendix B. Acknowledgements\
    \ ......................................84\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The Media Resource Control Protocol (MRCP) is designed to\
    \ provide a\n   mechanism for a client device requiring audio/video stream processing\n\
    \   to control processing resources on the network.  These media\n   processing\
    \ resources may be speech recognizers (a.k.a. Automatic-\n   Speech-Recognition\
    \ (ASR) engines), speech synthesizers (a.k.a. Text-\n   To-Speech (TTS) engines),\
    \ fax, signal detectors, etc.  MRCP allows\n   implementation of distributed Interactive\
    \ Voice Response platforms,\n   for example VoiceXML [6] interpreters.  The MRCP\
    \ protocol defines the\n   requests, responses, and events needed to control the\
    \ media\n   processing resources.  The MRCP protocol defines the state machine\n\
    \   for each resource and the required state transitions for each request\n  \
    \ and server-generated event.\n   The MRCP protocol does not address how the control\
    \ session is\n   established with the server and relies on the Real Time Streaming\n\
    \   Protocol (RTSP) [2] to establish and maintain the session.  The\n   session\
    \ control protocol is also responsible for establishing the\n   media connection\
    \ from the client to the network server.  The MRCP\n   protocol and its messaging\
    \ is designed to be carried over RTSP or\n   another protocol as a MIME-type similar\
    \ to the Session Description\n   Protocol (SDP) [5].\n   The key words \"MUST\"\
    , \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD\
    \ NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n   document are to\
    \ be interpreted as described in RFC 2119 [8].\n"
- title: 2.  Architecture
  contents:
  - "2.  Architecture\n   The system consists of a client that requires media streams\
    \ generated\n   or needs media streams processed and a server that has the resources\n\
    \   or devices to process or generate the streams.  The client\n   establishes\
    \ a control session with the server for media processing\n   using a protocol\
    \ such as RTSP.  This will also set up and establish\n   the RTP stream between\
    \ the client and the server or another RTP\n   endpoint.  Each resource needed\
    \ in processing or generating the\n   stream is addressed or referred to by a\
    \ URL.  The client can now use\n   MRCP messages to control the media resources\
    \ and affect how they\n   process or generate the media stream.\n     |--------------------|\n\
    \     ||------------------||                   |----------------------|\n    \
    \ || Application Layer||                   ||--------------------||\n     ||------------------||\
    \                   || TTS  | ASR  | Fax  ||\n     ||  ASR/TTS API     ||    \
    \               ||Plugin|Plugin|Plugin||\n     ||------------------||        \
    \           ||  on  |  on  |  on  ||\n     ||    MRCP Core     ||            \
    \       || MRCP | MRCP | MRCP ||\n     ||  Protocol Stack  ||                \
    \   ||--------------------||\n     ||------------------||                   ||\
    \   RTSP Stack       ||\n     ||   RTSP Stack     ||                   ||    \
    \                ||\n     ||------------------||                   ||--------------------||\n\
    \     ||   TCP/IP Stack   ||========IP=========||  TCP/IP Stack      ||\n    \
    \ ||------------------||                   ||--------------------||\n     |--------------------|\
    \                   |----------------------|\n        MRCP client            \
    \                 Real-time Streaming MRCP\n                                 \
    \                media server\n"
- title: 2.1.  Resources and Services
  contents:
  - "2.1.  Resources and Services\n   The server is set up to offer a certain set\
    \ of resources and services\n   to the client.  These resources are of 3 types.\n\
    \   Transmission Resources\n   These are resources that are capable of generating\
    \ real-time streams,\n   like signal generators that generate tones and sounds\
    \ of certain\n   frequencies and patterns, and speech synthesizers that generate\n\
    \   spoken audio streams, etc.\n   Reception Resources\n   These are resources\
    \ that receive and process streaming data like\n   signal detectors and speech\
    \ recognizers.\n   Dual Mode Resources\n   These are resources that both send\
    \ and receive data like a fax\n   resource, capable of sending or receiving fax\
    \ through a two-way RTP\n   stream.\n"
- title: 2.2.  Server and Resource Addressing
  contents:
  - "2.2.  Server and Resource Addressing\n   The server as a whole is addressed using\
    \ a container URL, and the\n   individual resources the server has to offer are\
    \ reached by\n   individual resource URLs within the container URL.\n   RTSP Example:\n\
    \   A media server or container URL like,\n     rtsp://mediaserver.com/media/\n\
    \   may contain one or more resource URLs of the form,\n     rtsp://mediaserver.com/media/speechrecognizer/\n\
    \     rtsp://mediaserver.com/media/speechsynthesizer/\n     rtsp://mediaserver.com/media/fax/\n"
- title: 3.  MRCP Protocol Basics
  contents:
  - "3.  MRCP Protocol Basics\n   The message format for MRCP is text based, with\
    \ mechanisms to carry\n   embedded binary data.  This allows data like recognition\
    \ grammars,\n   recognition results, synthesizer speech markup, etc., to be carried\n\
    \   in the MRCP message between the client and the server resource.  The\n   protocol\
    \ does not address session control management, media\n   management, reliable\
    \ sequencing, and delivery or server or resource\n   addressing.  These are left\
    \ to a protocol like SIP or RTSP.  MRCP\n   addresses the issue of controlling\
    \ and communicating with the\n   resource processing the stream, and defines the\
    \ requests, responses,\n   and events needed to do that.\n"
- title: 3.1.  Establishing Control Session and Media Streams
  contents:
  - "3.1.  Establishing Control Session and Media Streams\n   The control session\
    \ between the client and the server is established\n   using a protocol like RTSP.\
    \  This protocol will also set up the\n   appropriate RTP streams between the\
    \ server and the client, allocating\n   ports and setting up transport parameters\
    \ as needed.  Each control\n   session is identified by a unique session-id. \
    \ The format, usage, and\n   life cycle of the session-id is in accordance with\
    \ the RTSP protocol.\n   The resources within the session are addressed by the\
    \ individual\n   resource URLs.\n   The MRCP protocol is designed to work with\
    \ and tunnel through another\n   protocol like RTSP, and augment its capabilities.\
    \  MRCP relies on\n   RTSP headers for sequencing, reliability, and addressing\
    \ to make sure\n   that messages get delivered reliably and in the correct order\
    \ and to\n   the right resource.  The MRCP messages are carried in the RTSP\n\
    \   message body.  The media server delivers the MRCP message to the\n   appropriate\
    \ resource or device by looking at the session-level\n   message headers and URL\
    \ information.  Another protocol, such as SIP\n   [4], could be used for tunneling\
    \ MRCP messages.\n"
- title: 3.2.  MRCP over RTSP
  contents:
  - "3.2.  MRCP over RTSP\n   RTSP supports both TCP and UDP mechanisms for the client\
    \ to talk to\n   the server and is differentiated by the RTSP URL.  All MRCP based\n\
    \   media servers MUST support TCP for transport and MAY support UDP.\n   In RTSP,\
    \ the ANNOUNCE method/response MUST be used to carry MRCP\n   request/responses\
    \ between the client and the server.  MRCP messages\n   MUST NOT be communicated\
    \ in the RTSP SETUP or TEARDOWN messages.\n   Currently all RTSP messages are\
    \ request/responses and there is no\n   support for asynchronous events in RTSP.\
    \  This is because RTSP was\n   designed to work over TCP or UDP and, hence, could\
    \ not assume\n   reliability in the underlying protocol.  Hence, when using MRCP\
    \ over\n   RTSP, an asynchronous event from the MRCP server is packaged in a\n\
    \   server-initiated ANNOUNCE method/response communication.  A future\n   RTSP\
    \ extension to send asynchronous events from the server to the\n   client would\
    \ provide an alternate vehicle to carry such asynchronous\n   MRCP events from\
    \ the server.\n   An RTSP session is created when an RTSP SETUP message is sent\
    \ from\n   the client to a server and is addressed to a server URL or any one\
    \ of\n   its resource URLs without specifying a session-id.  The server will\n\
    \   establish a session context and will respond with a session-id to the\n  \
    \ client.  This sequence will also set up the RTP transport parameters\n   between\
    \ the client and the server, and then the server will be ready\n   to receive\
    \ or send media streams.  If the client wants to attach an\n   additional resource\
    \ to an existing session, the client should send\n   that session's ID in the\
    \ subsequent SETUP message.\n   When a media server implementing MRCP over RTSP\
    \ receives a PLAY,\n   RECORD, or PAUSE RTSP method from an MRCP resource URL,\
    \ it should\n   respond with an RTSP 405 \"Method not Allowed\" response.  For\
    \ these\n   resources, the only allowed RTSP methods are SETUP, TEARDOWN,\n  \
    \ DESCRIBE, and ANNOUNCE.\n   Example 1:\n   C->S:  ANNOUNCE rtsp://media.server.com/media/synthesizer\
    \ RTSP/1.0\n          CSeq:4\n          Session:12345678\n          Content-Type:application/mrcp\n\
    \          Content-Length:223\n          SPEAK 543257 MRCP/1.0\n          Voice-gender:neutral\n\
    \          Voice-category:teenager\n          Prosody-volume:medium\n        \
    \  Content-Type:application/synthesis+ssml\n          Content-Length:104\n   \
    \       <?xml version=\"1.0\"?>\n          <speak>\n           <paragraph>\n \
    \            <sentence>You have 4 new messages.</sentence>\n             <sentence>The\
    \ first is from <say-as\n             type=\"name\">Stephanie Williams</say-as>\n\
    \             and arrived at <break/>\n             <say-as type=\"time\">3:45pm</say-as>.</sentence>\n\
    \             <sentence>The subject is <prosody\n             rate=\"-20%\">ski\
    \ trip</prosody></sentence>\n           </paragraph>\n          </speak>\n   S->C:\
    \  RTSP/1.0 200 OK\n          CSeq: 4\n          Session:12345678\n          RTP-Info:url=rtsp://media.server.com/media/synthesizer;\n\
    \                    seq=9810092;rtptime=3450012\n          Content-Type:application/mrcp\n\
    \          Content-Length:52\n          MRCP/1.0 543257 200 IN-PROGRESS\n   S->C:\
    \  ANNOUNCE rtsp://media.server.com/media/synthesizer RTSP/1.0\n          CSeq:6\n\
    \          Session:12345678\n          Content-Type:application/mrcp\n       \
    \   Content-Length:123\n          SPEAK-COMPLETE 543257 COMPLETE MRCP/1.0\n  \
    \ C->S:  RTSP/1.0 200 OK\n          CSeq:6\n   For the sake of brevity, most examples\
    \ from here on show only the\n   MRCP messages and do not show the RTSP message\
    \ and headers in which\n   they are tunneled.  Also, RTSP messages such as response\
    \ that are not\n   carrying an MRCP message are also left out.\n"
- title: 3.3.  Media Streams and RTP Ports
  contents:
  - "3.3.  Media Streams and RTP Ports\n   A single set of RTP/RTCP ports is negotiated\
    \ and shared between the\n   MRCP client and server when multiple media processing\
    \ resources, such\n   as automatic speech recognition (ASR) engines and text to\
    \ speech\n   (TTS) engines, are used for a single session.  The individual\n \
    \  resource instances allocated on the server under a common session\n   identifier\
    \ will feed from/to that single RTP stream.\n   The client can send multiple media\
    \ streams towards the server,\n   differentiated by using different synchronized\
    \ source (SSRC)\n   identifier values.  Similarly the server can use multiple\n\
    \   Synchronized Source (SSRC) identifier values to differentiate media\n   streams\
    \ originating from the individual transmission resource URLs if\n   more than\
    \ one exists.  The individual resources may, on the other\n   hand, work together\
    \ to send just one stream to the client.  This is\n   up to the implementation\
    \ of the media server.\n"
- title: 4.  Notational Conventions
  contents:
  - "4.  Notational Conventions\n   Since many of the definitions and syntax are identical\
    \ to HTTP/1.1,\n   this specification only points to the section where they are\
    \ defined\n   rather than copying it.  For brevity, [HX.Y] refers to Section X.Y\
    \ of\n   the current HTTP/1.1 specification (RFC 2616 [1]).\n   All the mechanisms\
    \ specified in this document are described in both\n   prose and an augmented\
    \ Backus-Naur form (ABNF) similar to that used\n   in [H2.1].  It is described\
    \ in detail in RFC 4234 [3].\n   The ABNF provided along with the descriptive\
    \ text is informative in\n   nature and may not be complete.  The complete message\
    \ format in ABNF\n   form is provided in Appendix A and is the normative format\n\
    \   definition.\n"
- title: 5.  MRCP Specification
  contents:
  - "5.  MRCP Specification\n   The MRCP PDU is textual using an ISO 10646 character\
    \ set in the UTF-8\n   encoding (RFC 3629 [12]) to allow many different languages\
    \ to be\n   represented.  However, to assist in compact representations, MRCP\n\
    \   also allows other character sets such as ISO 8859-1 to be used when\n   desired.\
    \  The MRCP protocol headers and field names use only the\n   US-ASCII subset\
    \ of UTF-8.  Internationalization only applies to\n   certain fields like grammar,\
    \ results, speech markup, etc., and not to\n   MRCP as a whole.\n   Lines are\
    \ terminated by CRLF, but receivers SHOULD be prepared to\n   also interpret CR\
    \ and LF by themselves as line terminators.  Also,\n   some parameters in the\
    \ PDU may contain binary data or a record\n   spanning multiple lines.  Such fields\
    \ have a length value associated\n   with the parameter, which indicates the number\
    \ of octets immediately\n   following the parameter.\n   The whole MRCP PDU is\
    \ encoded in the body of the session level\n   message as a MIME entity of type\
    \ application/mrcp.  The individual\n   MRCP messages do not have addressing information\
    \ regarding which\n   resource the request/response are to/from.  Instead, the\
    \ MRCP message\n   relies on the header of the session level message carrying\
    \ it to\n   deliver the request to the appropriate resource, or to figure out\
    \ who\n   the response or event is from.\n   The MRCP message set consists of\
    \ requests from the client to the\n   server, responses from the server to the\
    \ client and asynchronous\n   events from the server to the client.  All these\
    \ messages consist of\n   a start-line, one or more header fields (also known\
    \ as \"headers\"), an\n   empty line (i.e., a line with nothing preceding the\
    \ CRLF) indicating\n   the end of the header fields, and an optional message body.\n\
    \          generic-message =   start-line\n                              message-header\n\
    \                              CRLF\n                              [ message-body\
    \ ]\n          message-body    =   *OCTET\n          start-line      =   request-line\
    \ / status-line / event-line\n   The message-body contains resource-specific and\
    \ message-specific data\n   that needs to be carried between the client and server\
    \ as a MIME\n   entity.  The information contained here and the actual MIME-types\n\
    \   used to carry the data are specified later when addressing the\n   specific\
    \ messages.\n   If a message contains data in the message body, the header fields\n\
    \   will contain content-headers indicating the MIME-type and encoding of\n  \
    \ the data in the message body.\n"
- title: 5.1.  Request
  contents:
  - "5.1.  Request\n   An MRCP request consists of a Request line followed by zero\
    \ or more\n   parameters as part of the message headers and an optional message\n\
    \   body containing data specific to the request message.\n   The Request message\
    \ from a client to the server includes, within the\n   first line, the method\
    \ to be applied, a method tag for that request,\n   and the version of protocol\
    \ in use.\n     request-line   =    method-name SP request-id SP\n           \
    \              mrcp-version CRLF\n   The request-id field is a unique identifier\
    \ created by the client and\n   sent to the server.  The server resource should\
    \ use this identifier\n   in its response to this request.  If the request does\
    \ not complete\n   with the response, future asynchronous events associated with\
    \ this\n   request MUST carry the request-id.\n     request-id    =    1*DIGIT\n\
    \   The method-name field identifies the specific request that the client\n  \
    \ is making to the server.  Each resource supports a certain list of\n   requests\
    \ or methods that can be issued to it, and will be addressed\n   in later sections.\n\
    \     method-name    =    synthesizer-method\n                    /    recognizer-method\n\
    \   The mrcp-version field is the MRCP protocol version that is being\n   used\
    \ by the client.\n     mrcp-version   =    \"MRCP\" \"/\" 1*DIGIT \".\" 1*DIGIT\n"
- title: 5.2.  Response
  contents:
  - "5.2.  Response\n   After receiving and interpreting the request message, the\
    \ server\n   resource responds with an MRCP response message.  It consists of\
    \ a\n   status line optionally followed by a message body.\n     response-line\
    \  =    mrcp-version SP request-id SP status-code SP\n                       \
    \  request-state CRLF\n   The mrcp-version field used here is similar to the one\
    \ used in the\n   Request Line and indicates the version of MRCP protocol running\
    \ on\n   the server.\n   The request-id used in the response MUST match the one\
    \ sent in the\n   corresponding request message.\n   The status-code field is\
    \ a 3-digit code representing the success or\n   failure or other status of the\
    \ request.\n   The request-state field indicates if the job initiated by the Request\n\
    \   is PENDING, IN-PROGRESS, or COMPLETE.  The COMPLETE status means that\n  \
    \ the Request was processed to completion and that there will be no\n   more events\
    \ from that resource to the client with that request-id.\n   The PENDING status\
    \ means that the job has been placed on a queue and\n   will be processed in first-in-first-out\
    \ order.  The IN-PROGRESS\n   status means that the request is being processed\
    \ and is not yet\n   complete.  A PENDING or IN-PROGRESS status indicates that\
    \ further\n   Event messages will be delivered with that request-id.\n     request-state\
    \    =  \"COMPLETE\"\n                      /  \"IN-PROGRESS\"\n             \
    \         /  \"PENDING\"\n"
- title: 5.2.1.  Status Codes
  contents:
  - "5.2.1.  Status Codes\n   The status codes are classified under the Success(2XX)\
    \ codes and the\n   Failure(4XX) codes.\n"
- title: 5.2.1.1.  Success 2xx
  contents:
  - "5.2.1.1.  Success 2xx\n      200       Success\n      201       Success with\
    \ some optional parameters ignored.\n"
- title: 5.2.1.2.  Failure 4xx
  contents:
  - "5.2.1.2.  Failure 4xx\n      401       Method not allowed\n      402       Method\
    \ not valid in this state\n      403       Unsupported Parameter\n      404  \
    \     Illegal Value for Parameter\n      405       Not found (e.g., Resource URI\
    \ not initialized\n                or doesn't exist)\n      406       Mandatory\
    \ Parameter Missing\n      407       Method or Operation Failed (e.g., Grammar\
    \ compilation\n                failed in the recognizer.  Detailed cause codes\
    \ MAY BE\n                available through a resource specific header field.)\n\
    \      408       Unrecognized or unsupported message entity\n      409       Unsupported\
    \ Parameter Value\n      421-499   Resource specific Failure codes\n"
- title: 5.3.  Event
  contents:
  - "5.3.  Event\n   The server resource may need to communicate a change in state\
    \ or the\n   occurrence of a certain event to the client.  These messages are\
    \ used\n   when a request does not complete immediately and the response returns\n\
    \   a status of PENDING or IN-PROGRESS.  The intermediate results and\n   events\
    \ of the request are indicated to the client through the event\n   message from\
    \ the server.  Events have the request-id of the request\n   that is in progress\
    \ and is generating these events and status value.\n   The status value is COMPLETE\
    \ if the request is done and this was the\n   last event, else it is IN-PROGRESS.\n\
    \     event-line       =  event-name SP request-id SP request-state SP\n     \
    \                    mrcp-version CRLF\n   The mrcp-version used here is identical\
    \ to the one used in the\n   Request/Response Line and indicates the version of\
    \ MRCP protocol\n   running on the server.\n   The request-id used in the event\
    \ should match the one sent in the\n   request that caused this event.\n   The\
    \ request-state indicates if the Request/Command causing this event\n   is complete\
    \ or still in progress, and is the same as the one\n   mentioned in Section 5.2.\
    \  The final event will contain a COMPLETE\n   status indicating the completion\
    \ of the request.\n   The event-name identifies the nature of the event generated\
    \ by the\n   media resource.  The set of valid event names are dependent on the\n\
    \   resource generating it, and will be addressed in later sections.\n     event-name\
    \       =  synthesizer-event\n                      /  recognizer-event\n"
- title: 5.4.  Message Headers
  contents:
  - "5.4.  Message Headers\n   MRCP header fields, which include general-header (Section\
    \ 5.4) and\n   resource-specific-header (Sections 7.4 and 8.4), follow the same\n\
    \   generic format as that given in Section 2.1 of RFC 2822 [7].  Each\n   header\
    \ field consists of a name followed by a colon (\":\") and the\n   field value.\
    \  Field names are case-insensitive.  The field value MAY\n   be preceded by any\
    \ amount of linear whitespace (LWS), though a single\n   SP is preferred.  Header\
    \ fields can be extended over multiple lines\n   by preceding each extra line\
    \ with at least one SP or HT.\n          message-header =    1*(generic-header\
    \ / resource-header)\n   The order in which header fields with differing field\
    \ names are\n   received is not significant.  However, it is \"good practice\"\
    \ to send\n   general-header fields first, followed by request-header or response-\n\
    \   header fields, and ending with the entity-header fields.\n   Multiple message-header\
    \ fields with the same field-name MAY be\n   present in a message if and only\
    \ if the entire field value for that\n   header field is defined as a comma-separated\
    \ list (i.e., #(values)).\n   It MUST be possible to combine the multiple header\
    \ fields into one\n   \"field-name:field-value\" pair, without changing the semantics\
    \ of the\n   message, by appending each subsequent field-value to the first, each\n\
    \   separated by a comma.  Therefore, the order in which header fields\n   with\
    \ the same field-name are received is significant to the\n   interpretation of\
    \ the combined field value, and thus a proxy MUST NOT\n   change the order of\
    \ these field values when a message is forwarded.\n   Generic Headers\n     generic-header\
    \      =    active-request-id-list\n                         /    proxy-sync-id\n\
    \                         /    content-id\n                         /    content-type\n\
    \                         /    content-length\n                         /    content-base\n\
    \                         /    content-location\n                         /  \
    \  content-encoding\n                         /    cache-control\n           \
    \              /    logging-tag\n   All headers in MRCP will be case insensitive,\
    \ consistent with HTTP\n   and RTSP protocol header definitions.\n"
- title: 5.4.1.  Active-Request-Id-List
  contents:
  - "5.4.1.  Active-Request-Id-List\n   In a request, this field indicates the list\
    \ of request-ids to which\n   it should apply.  This is useful when there are\
    \ multiple Requests\n   that are PENDING or IN-PROGRESS and you want this request\
    \ to apply to\n   one or more of these specifically.\n   In a response, this field\
    \ returns the list of request-ids that the\n   operation modified or were in progress\
    \ or just completed.  There\n   could be one or more requests that returned a\
    \ request-state of\n   PENDING or IN-PROGRESS.  When a method affecting one or\
    \ more PENDING\n   or IN-PROGRESS requests is sent from the client to the server,\
    \ the\n   response MUST contain the list of request-ids that were affected in\n\
    \   this header field.\n   The active-request-id-list is only used in requests\
    \ and responses,\n   not in events.\n   For example, if a STOP request with no\
    \ active-request-id-list is sent\n   to a synthesizer resource (a wildcard STOP)\
    \ that has one or more\n   SPEAK requests in the PENDING or IN-PROGRESS state,\
    \ all SPEAK\n   requests MUST be cancelled, including the one IN-PROGRESS.  In\n\
    \   addition, the response to the STOP request would contain the\n   request-id\
    \ of all the SPEAK requests that were terminated in the\n   active-request-id-list.\
    \  In this case, no SPEAK-COMPLETE or\n   RECOGNITION-COMPLETE events will be\
    \ sent for these terminated\n   requests.\n     active-request-id-list  =  \"\
    Active-Request-Id-List\" \":\" request-id\n                                 *(\"\
    ,\" request-id) CRLF\n"
- title: 5.4.2.  Proxy-Sync-Id
  contents:
  - "5.4.2.  Proxy-Sync-Id\n   When any server resource generates a barge-in-able\
    \ event, it will\n   generate a unique Tag and send it as a header field in an\
    \ event to\n   the client.  The client then acts as a proxy to the server resource\n\
    \   and sends a BARGE-IN-OCCURRED method (Section 7.10) to the\n   synthesizer\
    \ server resource with the Proxy-Sync-Id it received from\n   the server resource.\
    \  When the recognizer and synthesizer resources\n   are part of the same session,\
    \ they may choose to work together to\n   achieve quicker interaction and response.\
    \  Here, the proxy-sync-id\n   helps the resource receiving the event, proxied\
    \ by the client, to\n   decide if this event has been processed through a direct\
    \ interaction\n   of the resources.\n     proxy-sync-id    =  \"Proxy-Sync-Id\"\
    \ \":\" 1*ALPHA CRLF\n"
- title: 5.4.3.  Accept-Charset
  contents:
  - "5.4.3.  Accept-Charset\n   See [H14.2].  This specifies the acceptable character\
    \ set for\n   entities returned in the response or events associated with this\n\
    \   request.  This is useful in specifying the character set to use in\n   the\
    \ Natural Language Semantics Markup Language (NLSML) results of a\n   RECOGNITON-COMPLETE\
    \ event.\n"
- title: 5.4.4.  Content-Type
  contents:
  - "5.4.4.  Content-Type\n   See [H14.17].  Note that the content types suitable\
    \ for MRCP are\n   restricted to speech markup, grammar, recognition results,\
    \ etc., and\n   are specified later in this document.  The multi-part content\
    \ type\n   \"multi-part/mixed\" is supported to communicate multiple of the above\n\
    \   mentioned contents, in which case the body parts cannot contain any\n   MRCP\
    \ specific headers.\n"
- title: 5.4.5.  Content-Id
  contents:
  - "5.4.5.  Content-Id\n   This field contains an ID or name for the content, by\
    \ which it can be\n   referred to.  The definition of this field conforms to RFC\
    \ 2392 [14],\n   RFC 2822 [7], RFC 2046 [13] and is needed in multi-part messages.\
    \  In\n   MRCP whenever the content needs to be stored, by either the client or\n\
    \   the server, it is stored associated with this ID.  Such content can\n   be\
    \ referenced during the session in URI form using the session:URI\n   scheme described\
    \ in a later section.\n"
- title: 5.4.6.  Content-Base
  contents:
  - "5.4.6.  Content-Base\n   The content-base entity-header field may be used to\
    \ specify the base\n   URI for resolving relative URLs within the entity.\n  \
    \   content-base      = \"Content-Base\" \":\" absoluteURI CRLF\n   Note, however,\
    \ that the base URI of the contents within the entity-\n   body may be redefined\
    \ within that entity-body.  An example of this\n   would be a multi-part MIME\
    \ entity, which in turn can have multiple\n   entities within it.\n"
- title: 5.4.7.  Content-Encoding
  contents:
  - "5.4.7.  Content-Encoding\n   The content-encoding entity-header field is used\
    \ as a modifier to the\n   media-type.  When present, its value indicates what\
    \ additional\n   content coding has been applied to the entity-body, and thus\
    \ what\n   decoding mechanisms must be applied in order to obtain the media-type\n\
    \   referenced by the content-type header field.  Content-encoding is\n   primarily\
    \ used to allow a document to be compressed without losing\n   the identity of\
    \ its underlying media type.\n          content-encoding =  \"Content-Encoding\"\
    \ \":\"\n                              *WSP content-coding\n                 \
    \             *(*WSP \",\" *WSP content-coding *WSP )\n                      \
    \        CRLF\n          content-coding   =  token\n          token          \
    \  =  1*(alphanum / \"-\" / \".\" / \"!\" / \"%\" / \"*\"\n                  \
    \            / \"_\" / \"+\" / \"`\" / \"'\" / \"~\" )\n   Content coding is defined\
    \ in [H3.5].  An example of its use is\n     Content-Encoding:gzip\n   If multiple\
    \ encodings have been applied to an entity, the content\n   codings MUST be listed\
    \ in the order in which they were applied.\n"
- title: 5.4.8.  Content-Location
  contents:
  - "5.4.8.  Content-Location\n   The content-location entity-header field MAY BE\
    \ used to supply the\n   resource location for the entity enclosed in the message\
    \ when that\n   entity is accessible from a location separate from the requested\n\
    \   resource's URI.\n     content-location =  \"Content-Location\" \":\" ( absoluteURI\
    \ /\n                             relativeURI ) CRLF\n   The content-location\
    \ value is a statement of the location of the\n   resource corresponding to this\
    \ particular entity at the time of the\n   request.  The media server MAY use\
    \ this header field to optimize\n   certain operations.  When providing this header\
    \ field, the entity\n   being sent should not have been modified from what was\
    \ retrieved from\n   the content-location URI.\n   For example, if the client\
    \ provided a grammar markup inline, and it\n   had previously retrieved it from\
    \ a certain URI, that URI can be\n   provided as part of the entity, using the\
    \ content-location header\n   field.  This allows a resource like the recognizer\
    \ to look into its\n   cache to see if this grammar was previously retrieved,\
    \ compiled, and\n   cached.  In which case, it might optimize by using the previously\n\
    \   compiled grammar object.\n   If the content-location is a relative URI, the\
    \ relative URI is\n   interpreted relative to the content-base URI.\n"
- title: 5.4.9.  Content-Length
  contents:
  - "5.4.9.  Content-Length\n   This field contains the length of the content of the\
    \ message body\n   (i.e., after the double CRLF following the last header field).\n\
    \   Unlike HTTP, it MUST be included in all messages that carry content\n   beyond\
    \ the header portion of the message.  If it is missing, a\n   default value of\
    \ zero is assumed.  It is interpreted according to\n   [H14.13].\n"
- title: 5.4.10.  Cache-Control
  contents:
  - "5.4.10.  Cache-Control\n   If the media server plans on implementing caching,\
    \ it MUST adhere to\n   the cache correctness rules of HTTP 1.1 (RFC2616), when\
    \ accessing and\n   caching HTTP URI.  In particular, the expires and cache-control\n\
    \   headers of the cached URI or document must be honored and will always\n  \
    \ take precedence over the Cache-Control defaults set by this header\n   field.\
    \  The cache-control directives are used to define the default\n   caching algorithms\
    \ on the media server for the session or request.\n   The scope of the directive\
    \ is based on the method it is sent on.  If\n   the directives are sent on a SET-PARAMS\
    \ method, it SHOULD apply for\n   all requests for documents the media server\
    \ may make in that session.\n   If the directives are sent on any other messages,\
    \ they MUST only\n   apply to document requests the media server needs to make\
    \ for that\n   method.  An empty cache-control header on the GET-PARAMS method\
    \ is a\n   request for the media server to return the current cache-control\n\
    \   directives setting on the server.\n          cache-control  =    \"Cache-Control\"\
    \ \":\" *WSP cache-directive\n                              *( *WSP \",\" *WSP\
    \ cache-directive *WSP )\n                              CRLF\n          cache-directive\
    \ =   \"max-age\" \"=\" delta-seconds\n                          /   \"max-stale\"\
    \ \"=\" delta-seconds\n                          /   \"min-fresh\" \"=\" delta-seconds\n\
    \          delta-seconds       = 1*DIGIT\n   Here, delta-seconds is a time value\
    \ to be specified as an integer\n   number of seconds, represented in decimal,\
    \ after the time that the\n   message response or data was received by the media\
    \ server.\n   These directives allow the media server to override the basic\n\
    \   expiration mechanism.\n   max-age\n      Indicates that the client is OK with\
    \ the media server using a\n      response whose age is no greater than the specified\
    \ time in\n      seconds.  Unless a max-stale directive is also included, the\n\
    \      client is not willing to accept the media server using a stale\n      response.\n\
    \   min-fresh\n      Indicates that the client is willing to accept the media\
    \ server\n      using a response whose freshness lifetime is no less than its\n\
    \      current age plus the specified time in seconds.  That is, the\n      client\
    \ wants the media server to use a response that will still be\n      fresh for\
    \ at least the specified number of seconds.\n   max-stale\n      Indicates that\
    \ the client is willing to accept the media server\n      using a response that\
    \ has exceeded its expiration time.  If max-\n      stale is assigned a value,\
    \ then the client is willing to accept\n      the media server using a response\
    \ that has exceeded its expiration\n      time by no more than the specified number\
    \ of seconds.  If no value\n      is assigned to max-stale, then the client is\
    \ willing to accept the\n      media server using a stale response of any age.\n\
    \   The media server cache MAY BE requested to use stale response/data\n   without\
    \ validation, but only if this does not conflict with any\n   \"MUST\"-level requirements\
    \ concerning cache validation (e.g., a\n   \"must-revalidate\" cache-control directive)\
    \ in the HTTP 1.1\n   specification pertaining the URI.\n   If both the MRCP cache-control\
    \ directive and the cached entry on the\n   media server include \"max-age\" directives,\
    \ then the lesser of the two\n   values is used for determining the freshness\
    \ of the cached entry for\n   that request.\n"
- title: 5.4.11.  Logging-Tag
  contents:
  - "5.4.11.  Logging-Tag\n   This header field MAY BE sent as part of a SET-PARAMS/GET-PARAMS\n\
    \   method to set the logging tag for logs generated by the media server.\n  \
    \ Once set, the value persists until a new value is set or the session\n   is\
    \ ended.  The MRCP server should provide a mechanism to subset its\n   output\
    \ logs so that system administrators can examine or extract only\n   the log file\
    \ portion during which the logging tag was set to a\n   certain value.\n   MRCP\
    \ clients using this feature should take care to ensure that no\n   two clients\
    \ specify the same logging tag.  In the event that two\n   clients specify the\
    \ same logging tag, the effect on the MRCP server's\n   output logs in undefined.\n\
    \     logging-tag    =    \"Logging-Tag\" \":\" 1*ALPHA CRLF\n"
- title: 6.  Media Server
  contents:
  - "6.  Media Server\n   The capability of media server resources can be found using\
    \ the RTSP\n   DESCRIBE mechanism.  When a client issues an RTSP DESCRIBE method\
    \ for\n   a media resource URI, the media server response MUST contain an SDP\n\
    \   description in its body describing the capabilities of the media\n   server\
    \ resource.  The SDP description MUST contain at a minimum the\n   media header\
    \ (m-line) describing the codec and other media related\n   features it supports.\
    \  It MAY contain another SDP header as well, but\n   support for it is optional.\n\
    \   The usage of SDP messages in the RTSP message body and its\n   application\
    \ follows the SIP RFC 2543 [4], but is limited to media-\n   related negotiation\
    \ and description.\n"
- title: 6.1.  Media Server Session
  contents:
  - "6.1.  Media Server Session\n   As discussed in Section 3.2, a client/server should\
    \ share one RTSP\n   session-id for the different resources it may use under the\
    \ same\n   session.  The client MUST allocate a set of client RTP/RTCP ports for\n\
    \   a new session and MUST NOT send a Session-ID in the SETUP message for\n  \
    \ the first resource.  The server then creates a Session-ID and\n   allocates\
    \ a set of server RTP/RTCP ports and responds to the SETUP\n   message.\n   If\
    \ the client wants to open more resources with the same server under\n   the same\
    \ session, it will send the session-id (that it got in the\n   earlier SETUP response)\
    \ in the SETUP for the new resource.  A SETUP\n   message with an existing session-id\
    \ tells the server that this new\n   resource will feed from/into the same RTP/RTCP\
    \ stream of that\n   existing session.\n   If the client wants to open a resource\
    \ from a media server that is\n   not where the first resource came from, it will\
    \ send separate SETUP\n   requests with no session-id header field in them.  Each\
    \ server will\n   allocate its own session-id and return it in the response. \
    \ Each of\n   them will also come back with their own set of RTP/RTCP ports. \
    \ This\n   would be the case when the synthesizer engine and the recognition\n\
    \   engine are on different servers.\n   The RTSP SETUP method SHOULD contain\
    \ an SDP description of the media\n   stream being set up.  The RTSP SETUP response\
    \ MUST contain an SDP\n   description of the media stream that it expects to receive\
    \ and send\n   on that session.\n   The SDP description in the SETUP method from\
    \ the client SHOULD\n   describe the required media parameters like codec, Named\
    \ Signaling\n   Event (NSE) payload types, etc.  This could have multiple media\n\
    \   headers (i.e., m-lines) to allow the client to provide the media\n   server\
    \ with more than one option to choose from.\n   The SDP description in the SETUP\
    \ response should reflect the media\n   parameters that the media server will\
    \ be using for the stream.  It\n   should be within the choices that were specified\
    \ in the SDP of the\n   SETUP method, if one was provided.\n   Example:\n    \
    \ C->S:\n       SETUP rtsp://media.server.com/recognizer/ RTSP/1.0\n       CSeq:1\n\
    \       Transport:RTP/AVP;unicast;client_port=46456-46457\n       Content-Type:application/sdp\n\
    \       Content-Length:190\n       v=0\n       o=- 123 456 IN IP4 10.0.0.1\n \
    \      s=Media Server\n       p=+1-888-555-1212\n       c=IN IP4 0.0.0.0\n   \
    \    t=0 0\n       m=audio 46456 RTP/AVP 0 96\n       a=rtpmap:0 pcmu/8000\n \
    \      a=rtpmap:96 telephone-event/8000\n       a=fmtp:96 0-15\n     S->C:\n \
    \      RTSP/1.0 200 OK\n       CSeq:1\n       Session:0a030258_00003815_3bc4873a_0001_0000\n\
    \       Transport:RTP/AVP;unicast;client_port=46456-46457;\n                 \
    \ server_port=46460-46461\n       Content-Length:190\n       Content-Type:application/sdp\n\
    \       v=0\n       o=- 3211724219 3211724219 IN IP4 10.3.2.88\n       s=Media\
    \ Server\n       c=IN IP4 0.0.0.0\n       t=0 0\n       m=audio 46460 RTP/AVP\
    \ 0 96\n       a=rtpmap:0 pcmu/8000\n       a=rtpmap:96 telephone-event/8000\n\
    \       a=fmtp:96 0-15\n   If an SDP description was not provided in the RTSP\
    \ SETUP method, then\n   the media server may decide on parameters of the stream\
    \ but MUST\n   specify what it chooses in the SETUP response.  An SDP announcement\n\
    \   is only returned in a response to a SETUP message that does not\n   specify\
    \ a Session.  That is, the server will not return an SDP\n   announcement for\
    \ the synthesizer SETUP of a session already\n   established with a recognizer.\n\
    \     C->S:\n       SETUP rtsp://media.server.com/recognizer/ RTSP/1.0\n     \
    \  CSeq:1\n       Transport:RTP/AVP;unicast;client_port=46498\n     S->C:\n  \
    \     RTSP/1.0 200 OK\n       CSeq:1\n       Session:0a030258_000039dc_3bc48a13_0001_0000\n\
    \       Transport:RTP/AVP;unicast; client_port=46498;\n                  server_port=46502-46503\n\
    \       Content-Length:193\n       Content-Type:application/sdp\n       v=0\n\
    \       o=- 3211724947 3211724947 IN IP4 10.3.2.88\n       s=Media Server\n  \
    \     c=IN IP4 0.0.0.0\n       t=0 0\n       m=audio 46502 RTP/AVP 0 101\n   \
    \    a=rtpmap:0 pcmu/8000\n       a=rtpmap:101 telephone-event/8000\n       a=fmtp:101\
    \ 0-15\n"
- title: 7.  Speech Synthesizer Resource
  contents:
  - "7.  Speech Synthesizer Resource\n   This resource is capable of converting text\
    \ provided by the client\n   and generating a speech stream in real-time.  Depending\
    \ on the\n   implementation and capability of this resource, the client can\n\
    \   control parameters like voice characteristics, speaker speed, etc.\n   The\
    \ synthesizer resource is controlled by MRCP requests from the\n   client.  Similarly,\
    \ the resource can respond to these requests or\n   generate asynchronous events\
    \ to the server to indicate certain\n   conditions during the processing of the\
    \ stream.\n"
- title: 7.1.  Synthesizer State Machine
  contents:
  - "7.1.  Synthesizer State Machine\n   The synthesizer maintains states because\
    \ it needs to correlate MRCP\n   requests from the client.  The state transitions\
    \ shown below describe\n   the states of the synthesizer and reflect the request\
    \ at the head of\n   the queue.  A SPEAK request in the PENDING state can be deleted\
    \ or\n   stopped by a STOP request and does not affect the state of the\n   resource.\n\
    \        Idle                   Speaking                  Paused\n        State\
    \                  State                     State\n        |                \
    \       |                          |\n        |----------SPEAK------->|      \
    \           |--------|\n        |<------STOP------------|             CONTROL\
    \      |\n        |<----SPEAK-COMPLETE----|                 |------->|\n     \
    \   |<----BARGE-IN-OCCURRED-|                          |\n        |          \
    \    |--------|                          |\n        |          CONTROL      |-----------PAUSE--------->|\n\
    \        |              |------->|<----------RESUME---------|\n        |     \
    \                  |               |----------|\n        |                   \
    \    |              PAUSE       |\n        |                       |         \
    \      |--------->|\n        |              |--------|----------|            \
    \   |\n        |     BARGE-IN-OCCURRED |      SPEECH-MARKER       |\n        |\
    \              |------->|<---------|               |\n        |----------|   \
    \         |             |------------|\n        |         STOP          |    \
    \      SPEAK           |\n        |          |            |             |----------->|\n\
    \        |<---------|                                       |\n        |<-------------------STOP--------------------------|\n"
- title: 7.2.  Synthesizer Methods
  contents:
  - "7.2.  Synthesizer Methods\n   The synthesizer supports the following methods.\n\
    \     synthesizer-method  =  \"SET-PARAMS\"\n                         /  \"GET-PARAMS\"\
    \n                         /  \"SPEAK\"\n                         /  \"STOP\"\n\
    \                         /  \"PAUSE\"\n                         /  \"RESUME\"\
    \n                         /  \"BARGE-IN-OCCURRED\"\n                        \
    \ /  \"CONTROL\"\n"
- title: 7.3.  Synthesizer Events
  contents:
  - "7.3.  Synthesizer Events\n   The synthesizer may generate the following events.\n\
    \     synthesizer-event   =  \"SPEECH-MARKER\"\n                         /  \"\
    SPEAK-COMPLETE\"\n"
- title: 7.4.  Synthesizer Header Fields
  contents:
  - "7.4.  Synthesizer Header Fields\n   A synthesizer message may contain header\
    \ fields containing request\n   options and information to augment the Request,\
    \ Response, or Event of\n   the message with which it is associated.\n     synthesizer-header\
    \  =  jump-target       ; Section 7.4.1\n                         /  kill-on-barge-in\
    \  ; Section 7.4.2\n                         /  speaker-profile   ; Section 7.4.3\n\
    \                         /  completion-cause  ; Section 7.4.4\n             \
    \            /  voice-parameter   ; Section 7.4.5\n                         /\
    \  prosody-parameter ; Section 7.4.6\n                         /  vendor-specific\
    \   ; Section 7.4.7\n                         /  speech-marker     ; Section 7.4.8\n\
    \                         /  speech-language   ; Section 7.4.9\n             \
    \            /  fetch-hint        ; Section 7.4.10\n                         /\
    \  audio-fetch-hint  ; Section 7.4.11\n                         /  fetch-timeout\
    \     ; Section 7.4.12\n                         /  failed-uri        ; Section\
    \ 7.4.13\n                         /  failed-uri-cause  ; Section 7.4.14\n   \
    \                      /  speak-restart     ; Section 7.4.15\n               \
    \          /  speak-length      ; Section 7.4.16\n     Parameter           Support\
    \        Methods/Events/Response\n     jump-target         MANDATORY      SPEAK,\
    \ CONTROL\n     logging-tag         MANDATORY      SET-PARAMS, GET-PARAMS\n  \
    \   kill-on-barge-in    MANDATORY      SPEAK\n     speaker-profile     OPTIONAL\
    \       SET-PARAMS, GET-PARAMS,\n                                        SPEAK,\
    \ CONTROL\n     completion-cause    MANDATORY      SPEAK-COMPLETE\n     voice-parameter\
    \     MANDATORY      SET-PARAMS, GET-PARAMS,\n                               \
    \         SPEAK, CONTROL\n     prosody-parameter   MANDATORY      SET-PARAMS,\
    \ GET-PARAMS,\n                                        SPEAK, CONTROL\n     vendor-specific\
    \     MANDATORY      SET-PARAMS, GET-PARAMS\n     speech-marker       MANDATORY\
    \      SPEECH-MARKER\n     speech-language     MANDATORY      SET-PARAMS, GET-PARAMS,\
    \ SPEAK\n     fetch-hint          MANDATORY      SET-PARAMS, GET-PARAMS, SPEAK\n\
    \     audio-fetch-hint    MANDATORY      SET-PARAMS, GET-PARAMS, SPEAK\n     fetch-timeout\
    \       MANDATORY      SET-PARAMS, GET-PARAMS, SPEAK\n     failed-uri        \
    \  MANDATORY      Any\n     failed-uri-cause    MANDATORY      Any\n     speak-restart\
    \       MANDATORY      CONTROL\n     speak-length        MANDATORY      SPEAK,\
    \ CONTROL\n"
- title: 7.4.1.  Jump-Target
  contents:
  - "7.4.1.  Jump-Target\n   This parameter MAY BE specified in a CONTROL method and\
    \ controls the\n   jump size to move forward or rewind backward on an active SPEAK\n\
    \   request.  A + or - indicates a relative value to what is being\n   currently\
    \ played.  This MAY BE specified in a SPEAK request to\n   indicate an offset\
    \ into the speech markup that the SPEAK request\n   should start speaking from.\
    \  The different speech length units\n   supported are dependent on the synthesizer\
    \ implementation.  If it\n   does not support a unit or the operation, the resource\
    \ SHOULD respond\n   with a status code of 404 \"Illegal or Unsupported value\
    \ for\n   parameter\".\n     jump-target         =    \"Jump-Size\" \":\" speech-length-value\
    \ CRLF\n     speech-length-value =    numeric-speech-length\n                \
    \         /    text-speech-length\n     text-speech-length  =    1*ALPHA SP \"\
    Tag\"\n     numeric-speech-length=   (\"+\" / \"-\") 1*DIGIT SP\n            \
    \                  numeric-speech-unit\n     numeric-speech-unit =    \"Second\"\
    \n                         /    \"Word\"\n                         /    \"Sentence\"\
    \n                         /    \"Paragraph\"\n"
- title: 7.4.2.  Kill-On-Barge-In
  contents:
  - "7.4.2.  Kill-On-Barge-In\n   This parameter MAY BE sent as part of the SPEAK\
    \ method to enable\n   kill-on-barge-in support.  If enabled, the SPEAK method\
    \ is\n   interrupted by DTMF input detected by a signal detector resource or\n\
    \   by the start of speech sensed or recognized by the speech recognizer\n   resource.\n\
    \     kill-on-barge-in    =    \"Kill-On-Barge-In\" \":\" boolean-value CRLF\n\
    \     boolean-value       =    \"true\" / \"false\"\n   If the recognizer or signal\
    \ detector resource is on, the same server\n   as the synthesizer, the server\
    \ should be intelligent enough to\n   recognize their interactions by their common\
    \ RTSP session-id and work\n   with each other to provide kill-on-barge-in support.\
    \  The client\n   needs to send a BARGE-IN-OCCURRED method to the synthesizer\
    \ resource\n   when it receives a barge-in-able event from the synthesizer resource\n\
    \   or signal detector resource.  These resources MAY BE local or\n   distributed.\
    \  If this field is not specified, the value defaults to\n   \"true\".\n"
- title: 7.4.3.  Speaker Profile
  contents:
  - "7.4.3.  Speaker Profile\n   This parameter MAY BE part of the SET-PARAMS/GET-PARAMS\
    \ or SPEAK\n   request from the client to the server and specifies the profile\
    \ of\n   the speaker by a URI, which may be a set of voice parameters like\n \
    \  gender, accent, etc.\n     speaker-profile     =    \"Speaker-Profile\" \"\
    :\" uri CRLF\n"
- title: 7.4.4.  Completion Cause
  contents:
  - "7.4.4.  Completion Cause\n   This header field MUST be specified in a SPEAK-COMPLETE\
    \ event coming\n   from the synthesizer resource to the client.  This indicates\
    \ the\n   reason behind the SPEAK request completion.\n     completion-cause \
    \   =    \"Completion-Cause\" \":\" 1*DIGIT SP 1*ALPHA\n                     \
    \        CRLF\n   Cause-Code  Cause-Name     Description\n     000       normal\
    \         SPEAK completed normally.\n     001       barge-in       SPEAK request\
    \ was terminated because\n                              of barge-in.\n     002\
    \       parse-failure  SPEAK request terminated because of a\n               \
    \               failure to parse the speech markup text.\n     003       uri-failure\
    \    SPEAK request terminated because, access\n                              to\
    \ one of the URIs failed.\n     004       error          SPEAK request terminated\
    \ prematurely due\n                              to synthesizer error.\n     005\
    \       language-unsupported\n                              Language not supported.\n"
- title: 7.4.5.  Voice-Parameters
  contents:
  - "7.4.5.  Voice-Parameters\n   This set of parameters defines the voice of the\
    \ speaker.\n     voice-parameter     =    \"Voice-\" voice-param-name \":\"\n\
    \                              voice-param-value CRLF\n   voice-param-name is\
    \ any one of the attribute names under the voice\n   element specified in W3C's\
    \ Speech Synthesis Markup Language\n   Specification [9].  The voice-param-value\
    \ is any one of the value\n   choices of the corresponding voice element attribute\
    \ specified in the\n   above section.\n   These header fields MAY BE sent in SET-PARAMS/GET-PARAMS\
    \ request to\n   define/get default values for the entire session or MAY BE sent\
    \ in\n   the SPEAK request to define default values for that speak request.\n\
    \   Furthermore, these attributes can be part of the speech text marked\n   up\
    \ in Speech Synthesis Markup Language (SSML).\n   These voice parameter header\
    \ fields can also be sent in a CONTROL\n   method to affect a SPEAK request in\
    \ progress and change its behavior\n   on the fly.  If the synthesizer resource\
    \ does not support this\n   operation, it should respond back to the client with\
    \ a status of\n   unsupported.\n"
- title: 7.4.6.  Prosody-Parameters
  contents:
  - "7.4.6.  Prosody-Parameters\n   This set of parameters defines the prosody of\
    \ the speech.\n     prosody-parameter   =    \"Prosody-\" prosody-param-name \"\
    :\"\n                              prosody-param-value CRLF\n   prosody-param-name\
    \ is any one of the attribute names under the\n   prosody element specified in\
    \ W3C's Speech Synthesis Markup Language\n   Specification [9].  The prosody-param-value\
    \ is any one of the value\n   choices of the corresponding prosody element attribute\
    \ specified in\n   the above section.\n   These header fields MAY BE sent in SET-PARAMS/GET-PARAMS\
    \ request to\n   define/get default values for the entire session or MAY BE sent\
    \ in\n   the SPEAK request to define default values for that speak request.\n\
    \   Furthermore, these attributes can be part of the speech text marked\n   up\
    \ in SSML.\n   The prosody parameter header fields in the SET-PARAMS or SPEAK\n\
    \   request only apply if the speech data is of type text/plain and does\n   not\
    \ use a speech markup format.\n   These prosody parameter header fields MAY also\
    \ be sent in a CONTROL\n   method to affect a SPEAK request in progress and to\
    \ change its\n   behavior on the fly.  If the synthesizer resource does not support\n\
    \   this operation, it should respond back to the client with a status of\n  \
    \ unsupported.\n"
- title: 7.4.7.  Vendor-Specific Parameters
  contents:
  - "7.4.7.  Vendor-Specific Parameters\n   This set of headers allows for the client\
    \ to set vendor-specific\n   parameters.\n     vendor-specific         = \"Vendor-Specific-Parameters\"\
    \ \":\"\n                               vendor-specific-av-pair\n            \
    \                   *[\";\" vendor-specific-av-pair] CRLF\n     vendor-specific-av-pair\
    \ = vendor-av-pair-name \"=\"\n                               vendor-av-pair-value\n\
    \   This header MAY BE sent in the SET-PARAMS/GET-PARAMS method and is\n   used\
    \ to set vendor-specific parameters on the server side.  The\n   vendor-av-pair-name\
    \ can be any vendor-specific field name and\n   conforms to the XML vendor-specific\
    \ attribute naming convention.  The\n   vendor-av-pair-value is the value to set\
    \ the attribute to and needs\n   to be quoted.\n   When asking the server to get\
    \ the current value of these parameters,\n   this header can be sent in the GET-PARAMS\
    \ method with the list of\n   vendor-specific attribute names to get separated\
    \ by a semicolon.\n"
- title: 7.4.8.  Speech Marker
  contents:
  - "7.4.8.  Speech Marker\n   This header field contains a marker tag that may be\
    \ embedded in the\n   speech data.  Most speech markup formats provide mechanisms\
    \ to embed\n   marker fields between speech texts.  The synthesizer will generate\n\
    \   SPEECH-MARKER events when it reaches these marker fields.  This field\n  \
    \ SHOULD be part of the SPEECH-MARKER event and will contain the marker\n   tag\
    \ values.\n     speech-marker =          \"Speech-Marker\" \":\" 1*ALPHA CRLF\n"
- title: 7.4.9.  Speech Language
  contents:
  - "7.4.9.  Speech Language\n   This header field specifies the default language\
    \ of the speech data\n   if it is not specified in the speech data.  The value\
    \ of this header\n   field should follow RFC 3066 [16] for its values.  This MAY\
    \ occur in\n   SPEAK, SET-PARAMS, or GET-PARAMS request.\n     speech-language\
    \          =    \"Speech-Language\" \":\" 1*ALPHA CRLF\n"
- title: 7.4.10.  Fetch Hint
  contents:
  - "7.4.10.  Fetch Hint\n   When the synthesizer needs to fetch documents or other\
    \ resources like\n   speech markup or audio files, etc., this header field controls\
    \ URI\n   access properties.  This defines when the synthesizer should retrieve\n\
    \   content from the server.  A value of \"prefetch\" indicates a file may\n \
    \  be downloaded when the request is received, whereas \"safe\" indicates\n  \
    \ a file that should only be downloaded when actually needed.  The\n   default\
    \ value is \"prefetch\".  This header field MAY occur in SPEAK,\n   SET-PARAMS,\
    \ or GET-PARAMS requests.\n     fetch-hint               =    \"Fetch-Hint\" \"\
    :\" 1*ALPHA CRLF\n"
- title: 7.4.11.  Audio Fetch Hint
  contents:
  - "7.4.11.  Audio Fetch Hint\n   When the synthesizer needs to fetch documents or\
    \ other resources like\n   speech audio files, etc., this header field controls\
    \ URI access\n   properties.  This defines whether or not the synthesizer can\
    \ attempt\n   to optimize speech by pre-fetching audio.  The value is either \"\
    safe\"\n   to say that audio is only fetched when it is needed, never before;\n\
    \   \"prefetch\" to permit, but not require the platform to pre-fetch the\n  \
    \ audio; or \"stream\" to allow it to stream the audio fetches.  The\n   default\
    \ value is \"prefetch\".  This header field MAY occur in SPEAK,\n   SET-PARAMS,\
    \ or GET-PARAMS requests.\n     audio-fetch-hint         =    \"Audio-Fetch-Hint\"\
    \ \":\" 1*ALPHA CRLF\n"
- title: 7.4.12.  Fetch Timeout
  contents:
  - "7.4.12.  Fetch Timeout\n   When the synthesizer needs to fetch documents or other\
    \ resources like\n   speech audio files, etc., this header field controls URI\
    \ access\n   properties.  This defines the synthesizer timeout for resources the\n\
    \   media server may need to fetch from the network.  This is specified\n   in\
    \ milliseconds.  The default value is platform-dependent.  This\n   header field\
    \ MAY occur in SPEAK, SET-PARAMS, or GET-PARAMS.\n     fetch-timeout         \
    \   =    \"Fetch-Timeout\" \":\" 1*DIGIT CRLF\n"
- title: 7.4.13.  Failed URI
  contents:
  - "7.4.13.  Failed URI\n   When a synthesizer method needs a synthesizer to fetch\
    \ or access a\n   URI, and the access fails, the media server SHOULD provide the\
    \ failed\n   URI in this header field in the method response.\n     failed-uri\
    \               =    \"Failed-URI\" \":\" Url CRLF\n"
- title: 7.4.14.  Failed URI Cause
  contents:
  - "7.4.14.  Failed URI Cause\n   When a synthesizer method needs a synthesizer to\
    \ fetch or access a\n   URI, and the access fails, the media server SHOULD provide\
    \ the URI\n   specific or protocol-specific response code through this header\
    \ field\n   in the method response.  This field has been defined as alphanumeric\n\
    \   to accommodate all protocols, some of which might have a response\n   string\
    \ instead of a numeric response code.\n     failed-uri-cause         =    \"Failed-URI-Cause\"\
    \ \":\" 1*ALPHA CRLF\n"
- title: 7.4.15.  Speak Restart
  contents:
  - "7.4.15.  Speak Restart\n   When a CONTROL jump backward request is issued to\
    \ a currently\n   speaking synthesizer resource and the jumps beyond the start\
    \ of the\n   speech, the current SPEAK request re-starts from the beginning of\
    \ its\n   speech data and the response to the CONTROL request would contain\n\
    \   this header indicating a restart.  This header MAY occur in the\n   CONTROL\
    \ response.\n     speak-restart       =    \"Speak-Restart\" \":\" boolean-value\
    \ CRLF\n"
- title: 7.4.16.  Speak Length
  contents:
  - "7.4.16.  Speak Length\n   This parameter MAY BE specified in a CONTROL method\
    \ to control the\n   length of speech to speak, relative to the current speaking\
    \ point in\n   the currently active SPEAK request.  A \"-\" value is illegal in\
    \ this\n   field.  If a field with a Tag unit is specified, then the media must\n\
    \   speak until the tag is reached or the SPEAK request complete,\n   whichever\
    \ comes first.  This MAY BE specified in a SPEAK request to\n   indicate the length\
    \ to speak in the speech data and is relative to\n   the point in speech where\
    \ the SPEAK request starts.  The different\n   speech length units supported are\
    \ dependent on the synthesizer\n   implementation.  If it does not support a unit\
    \ or the operation, the\n   resource SHOULD respond with a status code of 404\
    \ \"Illegal or\n   Unsupported value for parameter\".\n     speak-length     \
    \   =    \"Speak-Length\" \":\" speech-length-value\n                        \
    \      CRLF\n"
- title: 7.5.  Synthesizer Message Body
  contents:
  - "7.5.  Synthesizer Message Body\n   A synthesizer message may contain additional\
    \ information associated\n   with the Method, Response, or Event in its message\
    \ body.\n"
- title: 7.5.1.  Synthesizer Speech Data
  contents:
  - "7.5.1.  Synthesizer Speech Data\n   Marked-up text for the synthesizer to speak\
    \ is specified as a MIME\n   entity in the message body.  The message to be spoken\
    \ by the\n   synthesizer can be specified inline (by embedding the data in the\n\
    \   message body) or by reference (by providing the URI to the data).  In\n  \
    \ either case, the data and the format used to markup the speech needs\n   to\
    \ be supported by the media server.\n   All media servers MUST support plain text\
    \ speech data and W3C's\n   Speech Synthesis Markup Language [9] at a minimum\
    \ and, hence, MUST\n   support the MIME types text/plain and application/synthesis+ssml\
    \ at a\n   minimum.\n   If the speech data needs to be specified by URI reference,\
    \ the MIME\n   type text/uri-list is used to specify the one or more URIs that\
    \ will\n   list what needs to be spoken.  If a list of speech URIs is specified,\n\
    \   speech data provided by each URI must be spoken in the order in which\n  \
    \ the URI are specified.\n   If the data to be spoken consists of a mix of URI\
    \ and inline speech\n   data, the multipart/mixed MIME-type is used and embedded\
    \ with the\n   MIME-blocks for text/uri-list, application/synthesis+ssml or\n\
    \   text/plain.  The character set and encoding used in the speech data\n   may\
    \ be specified according to standard MIME-type definitions.  The\n   multi-part\
    \ MIME-block can contain actual audio data in .wav or Sun\n   audio format.  This\
    \ is used when the client has audio clips that it\n   may have recorded, then\
    \ stored in memory or a local device, and that\n   it currently needs to play\
    \ as part of the SPEAK request.  The audio\n   MIME-parts can be sent by the client\
    \ as part of the multi-part MIME-\n   block.  This audio will be referenced in\
    \ the speech markup data that\n   will be another part in the multi-part MIME-block\
    \ according to the\n   multipart/mixed MIME-type specification.\n   Example 1:\n\
    \       Content-Type:text/uri-list\n       Content-Length:176\n       http://www.cisco.com/ASR-Introduction.sml\n\
    \       http://www.cisco.com/ASR-Document-Part1.sml\n       http://www.cisco.com/ASR-Document-Part2.sml\n\
    \       http://www.cisco.com/ASR-Conclusion.sml\n   Example 2:\n       Content-Type:application/synthesis+ssml\n\
    \       Content-Length:104\n       <?xml version=\"1.0\"?>\n       <speak>\n \
    \      <paragraph>\n                <sentence>You have 4 new messages.</sentence>\n\
    \                <sentence>The first is from <say-as\n                type=\"\
    name\">Stephanie Williams</say-as>\n                and arrived at <break/>\n\
    \                <say-as type=\"time\">3:45pm</say-as>.</sentence>\n         \
    \       <sentence>The subject is <prosody\n                rate=\"-20%\">ski trip</prosody></sentence>\n\
    \       </paragraph>\n       </speak>\n   Example 3:\n       Content-Type:multipart/mixed;\
    \ boundary=\"--break\"\n       --break\n       Content-Type:text/uri-list\n  \
    \     Content-Length:176\n       http://www.cisco.com/ASR-Introduction.sml\n \
    \      http://www.cisco.com/ASR-Document-Part1.sml\n       http://www.cisco.com/ASR-Document-Part2.sml\n\
    \       http://www.cisco.com/ASR-Conclusion.sml\n       --break\n       Content-Type:application/synthesis+ssml\n\
    \       Content-Length:104\n       <?xml version=\"1.0\"?>\n       <speak>\n \
    \      <paragraph>\n                <sentence>You have 4 new messages.</sentence>\n\
    \                <sentence>The first is from <say-as\n                type=\"\
    name\">Stephanie Williams</say-as>\n                and arrived at <break/>\n\
    \                <say-as type=\"time\">3:45pm</say-as>.</sentence>\n         \
    \       <sentence>The subject is <prosody\n                rate=\"-20%\">ski trip</prosody></sentence>\n\
    \       </paragraph>\n       </speak>\n        --break\n"
- title: 7.6.  SET-PARAMS
  contents:
  - "7.6.  SET-PARAMS\n   The SET-PARAMS method, from the client to server, tells\
    \ the\n   synthesizer resource to define default synthesizer context\n   parameters,\
    \ like voice characteristics and prosody, etc.  If the\n   server accepted and\
    \ set all parameters, it MUST return a Response-\n   Status of 200.  If it chose\
    \ to ignore some optional parameters, it\n   MUST return 201.\n   If some of the\
    \ parameters being set are unsupported or have illegal\n   values, the server\
    \ accepts and sets the remaining parameters and MUST\n   respond with a Response-Status\
    \ of 403 or 404, and MUST include in the\n   response the header fields that could\
    \ not be set.\n   Example:\n     C->S:SET-PARAMS 543256 MRCP/1.0\n         Voice-gender:female\n\
    \         Voice-category:adult\n         Voice-variant:3\n     S->C:MRCP/1.0 543256\
    \ 200 COMPLETE\n"
- title: 7.7.  GET-PARAMS
  contents:
  - "7.7.  GET-PARAMS\n   The GET-PARAMS method, from the client to server, asks the\n\
    \   synthesizer resource for its current synthesizer context parameters,\n   like\
    \ voice characteristics and prosody, etc.  The client SHOULD send\n   the list\
    \ of parameters it wants to read from the server by listing a\n   set of empty\
    \ parameter header fields.  If a specific list is not\n   specified then the server\
    \ SHOULD return all the settable parameters\n   including vendor-specific parameters\
    \ and their current values.  The\n   wild card use can be very intensive as the\
    \ number of settable\n   parameters can be large depending on the vendor.  Hence,\
    \ it is\n   RECOMMENDED that the client does not use the wildcard GET-PARAMS\n\
    \   operation very often.\n   Example:\n     C->S:GET-PARAMS 543256 MRCP/1.0\n\
    \          Voice-gender:\n          Voice-category:\n          Voice-variant:\n\
    \          Vendor-Specific-Parameters:com.mycorp.param1;\n                   \
    \   com.mycorp.param2\n     S->C:MRCP/1.0 543256 200 COMPLETE\n          Voice-gender:female\n\
    \          Voice-category:adult\n          Voice-variant:3\n          Vendor-Specific-Parameters:com.mycorp.param1=\"\
    Company Name\";\n                         com.mycorp.param2=\"124324234@mycorp.com\"\
    \n"
- title: 7.8.  SPEAK
  contents:
  - "7.8.  SPEAK\n   The SPEAK method from the client to the server provides the\n\
    \   synthesizer resource with the speech text and initiates speech\n   synthesis\
    \ and streaming.  The SPEAK method can carry voice and\n   prosody header fields\
    \ that define the behavior of the voice being\n   synthesized, as well as the\
    \ actual marked-up text to be spoken.  If\n   specific voice and prosody parameters\
    \ are specified as part of the\n   speech markup text, it will take precedence\
    \ over the values specified\n   in the header fields and those set using a previous\
    \ SET-PARAMS\n   request.\n   When applying voice parameters, there are 3 levels\
    \ of scope.  The\n   highest precedence are those specified within the speech\
    \ markup text,\n   followed by those specified in the header fields of the SPEAK\
    \ request\n   and, hence, apply for that SPEAK request only, followed by the\n\
    \   session default values that can be set using the SET-PARAMS request\n   and\
    \ apply for the whole session moving forward.\n   If the resource is idle and\
    \ the SPEAK request is being actively\n   processed, the resource will respond\
    \ with a success status code and a\n   request-state of IN-PROGRESS.\n   If the\
    \ resource is in the speaking or paused states (i.e., it is in\n   the middle\
    \ of processing a previous SPEAK request), the status\n   returns success and\
    \ a request-state of PENDING.  This means that this\n   SPEAK request is in queue\
    \ and will be processed after the currently\n   active SPEAK request is completed.\n\
    \   For the synthesizer resource, this is the only request that can\n   return\
    \ a request-state of IN-PROGRESS or PENDING.  When the text to\n   be synthesized\
    \ is complete, the resource will issue a SPEAK-COMPLETE\n   event with the request-id\
    \ of the SPEAK message and a request-state of\n   COMPLETE.\n   Example:\n   \
    \  C->S:SPEAK 543257 MRCP/1.0\n          Voice-gender:neutral\n          Voice-category:teenager\n\
    \          Prosody-volume:medium\n          Content-Type:application/synthesis+ssml\n\
    \          Content-Length:104\n          <?xml version=\"1.0\"?>\n          <speak>\n\
    \          <paragraph>\n            <sentence>You have 4 new messages.</sentence>\n\
    \            <sentence>The first is from <say-as\n            type=\"name\">Stephanie\
    \ Williams</say-as>\n            and arrived at <break/>\n            <say-as\
    \ type=\"time\">3:45pm</say-as>.</sentence>\n            <sentence>The subject\
    \ is <prosody\n            rate=\"-20%\">ski trip</prosody></sentence>\n     \
    \     </paragraph>\n          </speak>\n     S->C:MRCP/1.0 543257 200 IN-PROGRESS\n\
    \     S->C:SPEAK-COMPLETE 543257 COMPLETE MRCP/1.0\n          Completion-Cause:000\
    \ normal\n"
- title: 7.9.  STOP
  contents:
  - "7.9.  STOP\n   The STOP method from the client to the server tells the resource\
    \ to\n   stop speaking if it is speaking something.\n   The STOP request can be\
    \ sent with an active-request-id-list header\n   field to stop the zero or more\
    \ specific SPEAK requests that may be in\n   queue and return a response code\
    \ of 200(Success).  If no active-\n   request-id-list header field is sent in\
    \ the STOP request, it will\n   terminate all outstanding SPEAK requests.\n  \
    \ If a STOP request successfully terminated one or more PENDING or\n   IN-PROGRESS\
    \ SPEAK requests, then the response message body contains\n   an active-request-id-list\
    \ header field listing the SPEAK request-ids\n   that were terminated.  Otherwise,\
    \ there will be no active-request-\n   id-list header field in the response. \
    \ No SPEAK-COMPLETE events will\n   be sent for these terminated requests.\n \
    \  If a SPEAK request that was IN-PROGRESS and speaking was stopped, the\n   next\
    \ pending SPEAK request, if any, would become IN-PROGRESS and move\n   to the\
    \ speaking state.\n   If a SPEAK request that was IN-PROGRESS and in the paused\
    \ state was\n   stopped, the next pending SPEAK request, if any, would become\n\
    \   IN-PROGRESS and move to the paused state.\n   Example:\n     C->S:SPEAK 543258\
    \ MRCP/1.0\n          Content-Type:application/synthesis+ssml\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <speak>\n          <paragraph>\n\
    \            <sentence>You have 4 new messages.</sentence>\n            <sentence>The\
    \ first is from <say-as\n            type=\"name\">Stephanie Williams</say-as>\n\
    \            and arrived at <break/>\n            <say-as type=\"time\">3:45pm</say-as>.</sentence>\n\
    \            <sentence>The subject is <prosody\n            rate=\"-20%\">ski\
    \ trip</prosody></sentence>\n          </paragraph>\n          </speak>\n    \
    \ S->C:MRCP/1.0 543258 200 IN-PROGRESS\n     C->S:STOP 543259 200 MRCP/1.0\n \
    \    S->C:MRCP/1.0 543259 200 COMPLETE\n          Active-Request-Id-List:543258\n"
- title: 7.10.  BARGE-IN-OCCURRED
  contents:
  - "7.10.  BARGE-IN-OCCURRED\n   The BARGE-IN-OCCURRED method is a mechanism for\
    \ the client to\n   communicate a barge-in-able event it detects to the speech\
    \ resource.\n   This event is useful in two scenarios,\n   1.  The client has\
    \ detected some events like DTMF digits or other\n       barge-in-able events\
    \ and wants to communicate that to the\n       synthesizer.\n   2.  The recognizer\
    \ resource and the synthesizer resource are in\n       different servers.  In\
    \ which case the client MUST act as a Proxy\n       and receive event from the\
    \ recognition resource, and then send a\n       BARGE-IN-OCCURRED method to the\
    \ synthesizer.  In such cases, the\n       BARGE-IN-OCCURRED method would also\
    \ have a proxy-sync-id header\n       field received from the resource generating\
    \ the original event.\n   If a SPEAK request is active with kill-on-barge-in enabled,\
    \ and the\n   BARGE-IN-OCCURRED event is received, the synthesizer should stop\n\
    \   streaming out audio.  It should also terminate any speech requests\n   queued\
    \ behind the current active one, irrespective of whether they\n   have barge-in\
    \ enabled or not.  If a barge-in-able prompt was playing\n   and it was terminated,\
    \ the response MUST contain the request-ids of\n   all SPEAK requests that were\
    \ terminated in its active-request-id-\n   list.  There will be no SPEAK-COMPLETE\
    \ events generated for these\n   requests.\n   If the synthesizer and the recognizer\
    \ are on the same server, they\n   could be optimized for a quicker kill-on-barge-in\
    \ response by having\n   them interact directly based on a common RTSP session-id.\
    \  In these\n   cases, the client MUST still proxy the recognition event through\
    \ a\n   BARGE-IN-OCCURRED method, but the synthesizer resource may have\n   already\
    \ stopped and sent a SPEAK-COMPLETE event with a barge-in\n   completion cause\
    \ code.  If there were no SPEAK requests terminated as\n   a result of the BARGE-IN-OCCURRED\
    \ method, the response would still be\n   a 200 success, but MUST not contain\
    \ an active-request-id-list header\n   field.\n     C->S:SPEAK 543258 MRCP/1.0\n\
    \          Voice-gender:neutral\n          Voice-category:teenager\n         \
    \ Prosody-volume:medium\n          Content-Type:application/synthesis+ssml\n \
    \         Content-Length:104\n          <?xml version=\"1.0\"?>\n          <speak>\n\
    \          <paragraph>\n            <sentence>You have 4 new messages.</sentence>\n\
    \            <sentence>The first is from <say-as\n            type=\"name\">Stephanie\
    \ Williams</say-as>\n            and arrived at <break/>\n            <say-as\
    \ type=\"time\">3:45pm</say-as>.</sentence>\n            <sentence>The subject\
    \ is <prosody\n            rate=\"-20%\">ski trip</prosody></sentence>\n     \
    \     </paragraph>\n          </speak>\n     S->C:MRCP/1.0 543258 200 IN-PROGRESS\n\
    \     C->S:BARGE-IN-OCCURRED 543259 200 MRCP/1.0\n          Proxy-Sync-Id:987654321\n\
    \     S->C:MRCP/1.0 543259 200 COMPLETE\n          Active-Request-Id-List:543258\n"
- title: 7.11.  PAUSE
  contents:
  - "7.11.  PAUSE\n   The PAUSE method from the client to the server tells the resource\
    \ to\n   pause speech, if it is speaking something.  If a PAUSE method is\n  \
    \ issued on a session when a SPEAK is not active, the server SHOULD\n   respond\
    \ with a status of 402 or \"Method not valid in this state\".  If\n   a PAUSE\
    \ method is issued on a session when a SPEAK is active and\n   paused, the server\
    \ SHOULD respond with a status of 200 or \"Success\".\n   If a SPEAK request was\
    \ active, the server MUST return an active-\n   request-id-list header with the\
    \ request-id of the SPEAK request that\n   was paused.\n     C->S:SPEAK 543258\
    \ MRCP/1.0\n          Voice-gender:neutral\n          Voice-category:teenager\n\
    \          Prosody-volume:medium\n          Content-Type:application/synthesis+ssml\n\
    \          Content-Length:104\n          <?xml version=\"1.0\"?>\n          <speak>\n\
    \          <paragraph>\n            <sentence>You have 4 new messages.</sentence>\n\
    \            <sentence>The first is from <say-as\n            type=\"name\">Stephanie\
    \ Williams</say-as>\n            and arrived at <break/>\n            <say-as\
    \ type=\"time\">3:45pm</say-as>.</sentence>\n            <sentence>The subject\
    \ is <prosody\n            rate=\"-20%\">ski trip</prosody></sentence>\n     \
    \     </paragraph>\n          </speak>\n     S->C:MRCP/1.0 543258 200 IN-PROGRESS\n\
    \     C->S:PAUSE 543259 MRCP/1.0\n     S->C:MRCP/1.0 543259 200 COMPLETE\n   \
    \       Active-Request-Id-List:543258\n"
- title: 7.12.  RESUME
  contents:
  - "7.12.  RESUME\n   The RESUME method from the client to the server tells a paused\n\
    \   synthesizer resource to continue speaking.  If a RESUME method is\n   issued\
    \ on a session when a SPEAK is not active, the server SHOULD\n   respond with\
    \ a status of 402 or \"Method not valid in this state\".  If\n   a RESUME method\
    \ is issued on a session when a SPEAK is active and\n   speaking (i.e., not paused),\
    \ the server SHOULD respond with a status\n   of 200 or \"Success\".  If a SPEAK\
    \ request was active, the server MUST\n   return an active-request-id-list header\
    \ with the request-id of the\n   SPEAK request that was resumed\n   Example:\n\
    \     C->S:SPEAK 543258 MRCP/1.0\n          Voice-gender:neutral\n          Voice-category:teenager\n\
    \          Prosody-volume:medium\n          Content-Type:application/synthesis+ssml\n\
    \          Content-Length:104\n          <?xml version=\"1.0\"?>\n          <speak>\n\
    \          <paragraph>\n              <sentence>You have 4 new messages.</sentence>\n\
    \              <sentence>The first is from <say-as\n              type=\"name\"\
    >Stephanie Williams</say-as>\n              and arrived at <break/>\n        \
    \      <say-as type=\"time\">3:45pm</say-as>.</sentence>\n              <sentence>The\
    \ subject is <prosody\n              rate=\"-20%\">ski trip</prosody></sentence>\n\
    \          </paragraph>\n          </speak>\n     S->C:MRCP/1.0 543258 200 IN-PROGRESS\n\
    \     C->S:PAUSE 543259 MRCP/1.0\n     S->C:MRCP/1.0 543259 200 COMPLETE\n   \
    \       Active-Request-Id-List:543258\n     C->S:RESUME 543260 MRCP/1.0\n    \
    \ S->C:MRCP/1.0 543260 200 COMPLETE\n          Active-Request-Id-List:543258\n"
- title: 7.13.  CONTROL
  contents:
  - "7.13.  CONTROL\n   The CONTROL method from the client to the server tells a synthesizer\n\
    \   that is speaking to modify what it is speaking on the fly.  This\n   method\
    \ is used to make the synthesizer jump forward or backward in\n   what it is being\
    \ spoken, change speaker rate and speaker parameters,\n   etc.  It affects the\
    \ active or IN-PROGRESS SPEAK request.  Depending\n   on the implementation and\
    \ capability of the synthesizer resource, it\n   may allow this operation or one\
    \ or more of its parameters.\n   When a CONTROL to jump forward is issued and\
    \ the operation goes\n   beyond the end of the active SPEAK method's text, the\
    \ request\n   succeeds.  A SPEAK-COMPLETE event follows the response to the CONTROL\n\
    \   method.  If there are more SPEAK requests in the queue, the\n   synthesizer\
    \ resource will continue to process the next SPEAK method.\n   When a CONTROL\
    \ to jump backwards is issued and the operation jumps to\n   the beginning of\
    \ the speech data of the active SPEAK request, the\n   response to the CONTROL\
    \ request contains the speak-restart header.\n   These two behaviors can be used\
    \ to rewind or fast-forward across\n   multiple speech requests, if the client\
    \ wants to break up a speech\n   markup text into multiple SPEAK requests.\n \
    \  If a SPEAK request was active when the CONTROL method was received,\n   the\
    \ server MUST return an active-request-id-list header with the\n   Request-id\
    \ of the SPEAK request that was active.\n   Example:\n     C->S:SPEAK 543258 MRCP/1.0\n\
    \          Voice-gender:neutral\n          Voice-category:teenager\n         \
    \ Prosody-volume:medium\n          Content-Type:application/synthesis+ssml\n \
    \         Content-Length:104\n          <?xml version=\"1.0\"?>\n          <speak>\n\
    \          <paragraph>\n            <sentence>You have 4 new messages.</sentence>\n\
    \            <sentence>The first is from <say-as\n            type=\"name\">Stephanie\
    \ Williams</say-as>\n            and arrived at <break/>\n            <say-as\
    \ type=\"time\">3:45pm</say-as>.</sentence>\n            <sentence>The subject\
    \ is <prosody\n            rate=\"-20%\">ski trip</prosody></sentence>\n     \
    \     </paragraph>\n          </speak>\n     S->C:MRCP/1.0 543258 200 IN-PROGRESS\n\
    \     C->S:CONTROL 543259 MRCP/1.0\n          Prosody-rate:fast\n     S->C:MRCP/1.0\
    \ 543259 200 COMPLETE\n          Active-Request-Id-List:543258\n     C->S:CONTROL\
    \ 543260 MRCP/1.0\n          Jump-Size:-15 Words\n     S->C:MRCP/1.0 543260 200\
    \ COMPLETE\n          Active-Request-Id-List:543258\n"
- title: 7.14.  SPEAK-COMPLETE
  contents:
  - "7.14.  SPEAK-COMPLETE\n   This is an Event message from the synthesizer resource\
    \ to the client\n   indicating that the SPEAK request was completed.  The request-id\n\
    \   header field WILL match the request-id of the SPEAK request that\n   initiated\
    \ the speech that just completed.  The request-state field\n   should be COMPLETE\
    \ indicating that this is the last Event with that\n   request-id, and that the\
    \ request with that request-id is now\n   complete.  The completion-cause header\
    \ field specifies the cause code\n   pertaining to the status and reason of request\
    \ completion such as the\n   SPEAK completed normally or because of an error or\
    \ kill-on-barge-in,\n   etc.\n   Example:\n     C->S:SPEAK 543260 MRCP/1.0\n \
    \         Voice-gender:neutral\n          Voice-category:teenager\n          Prosody-volume:medium\n\
    \          Content-Type:application/synthesis+ssml\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <speak>\n          <paragraph>\n\
    \            <sentence>You have 4 new messages.</sentence>\n            <sentence>The\
    \ first is from <say-as\n            type=\"name\">Stephanie Williams</say-as>\n\
    \            and arrived at <break/>\n            <say-as type=\"time\">3:45pm</say-as>.</sentence>\n\
    \            <sentence>The subject is <prosody\n            rate=\"-20%\">ski\
    \ trip</prosody></sentence>\n          </paragraph>\n          </speak>\n    \
    \ S->C:MRCP/1.0 543260 200 IN-PROGRESS\n     S->C:SPEAK-COMPLETE 543260 COMPLETE\
    \ MRCP/1.0\n          Completion-Cause:000 normal\n"
- title: 7.15.  SPEECH-MARKER
  contents:
  - "7.15.  SPEECH-MARKER\n   This is an event generated by the synthesizer resource\
    \ to the client\n   when it hits a marker tag in the speech markup it is currently\n\
    \   processing.  The request-id field in the header matches the SPEAK\n   request\
    \ request-id that initiated the speech.  The request-state\n   field should be\
    \ IN-PROGRESS as the speech is still not complete and\n   there is more to be\
    \ spoken.  The actual speech marker tag hit,\n   describing where the synthesizer\
    \ is in the speech markup, is returned\n   in the speech-marker header field.\n\
    \   Example:\n     C->S:SPEAK 543261 MRCP/1.0\n          Voice-gender:neutral\n\
    \          Voice-category:teenager\n          Prosody-volume:medium\n        \
    \  Content-Type:application/synthesis+ssml\n          Content-Length:104\n   \
    \       <?xml version=\"1.0\"?>\n          <speak>\n          <paragraph>\n  \
    \          <sentence>You have 4 new messages.</sentence>\n            <sentence>The\
    \ first is from <say-as\n            type=\"name\">Stephanie Williams</say-as>\n\
    \            and arrived at <break/>\n            <say-as type=\"time\">3:45pm</say-as>.</sentence>\n\
    \            <mark name=\"here\"/>\n            <sentence>The subject is\n   \
    \            <prosody rate=\"-20%\">ski trip</prosody>\n            </sentence>\n\
    \            <mark name=\"ANSWER\"/>\n          </paragraph>\n          </speak>\n\
    \     S->C:MRCP/1.0 543261 200 IN-PROGRESS\n     S->C:SPEECH-MARKER 543261 IN-PROGRESS\
    \ MRCP/1.0\n          Speech-Marker:here\n     S->C:SPEECH-MARKER 543261 IN-PROGRESS\
    \ MRCP/1.0\n          Speech-Marker:ANSWER\n     S->C:SPEAK-COMPLETE 543261 COMPLETE\
    \ MRCP/1.0\n          Completion-Cause:000 normal\n"
- title: 8.  Speech Recognizer Resource
  contents:
  - "8.  Speech Recognizer Resource\n   The speech recognizer resource is capable\
    \ of receiving an incoming\n   voice stream and providing the client with an interpretation\
    \ of what\n   was spoken in textual form.\n"
- title: 8.1.  Recognizer State Machine
  contents:
  - "8.1.  Recognizer State Machine\n   The recognizer resource is controlled by MRCP\
    \ requests from the\n   client.  Similarly, the resource can respond to these\
    \ requests or\n   generate asynchronous events to the server to indicate certain\n\
    \   conditions during the processing of the stream.  Hence, the\n   recognizer\
    \ maintains states to correlate MRCP requests from the\n   client.  The state\
    \ transitions are described below.\n        Idle                   Recognizing\
    \               Recognized\n        State                  State             \
    \        State\n         |                       |                          |\n\
    \         |---------RECOGNIZE---->|---RECOGNITION-COMPLETE-->|\n         |<------STOP------------|<-----RECOGNIZE-----------|\n\
    \         |                       |                          |\n         |   \
    \                    |              |-----------|\n         |              |--------|\
    \       GET-RESULT         |\n         |       START-OF-SPEECH |             \
    \ |---------->|\n         |------------| |------->|                          |\n\
    \         |            |          |----------|               |\n         |   \
    \   DEFINE-GRAMMAR   | RECOGNITION-START-TIMERS |\n         |<-----------|   \
    \       |<---------|               |\n         |                       |     \
    \                     |\n         |                       |                  \
    \        |\n         |-------|               |                          |\n  \
    \       |      STOP             |                          |\n         |<------|\
    \               |                          |\n         |                     \
    \                             |\n         |<-------------------STOP--------------------------|\n\
    \         |<-------------------DEFINE-GRAMMAR----------------|\n"
- title: 8.2.  Recognizer Methods
  contents:
  - "8.2.  Recognizer Methods\n   The recognizer supports the following methods.\n\
    \     recognizer-method   =    SET-PARAMS\n                         /    GET-PARAMS\n\
    \                         /    DEFINE-GRAMMAR\n                         /    RECOGNIZE\n\
    \                         /    GET-RESULT\n                         /    RECOGNITION-START-TIMERS\n\
    \                         /    STOP\n"
- title: 8.3.  Recognizer Events
  contents:
  - "8.3.  Recognizer Events\n   The recognizer may generate the following events.\n\
    \     recognizer-event    =    START-OF-SPEECH\n                        /    RECOGNITION-COMPLETE\n"
- title: 8.4.  Recognizer Header Fields
  contents:
  - "8.4.  Recognizer Header Fields\n   A recognizer message may contain header fields\
    \ containing request\n   options and information to augment the Method, Response,\
    \ or Event\n   message it is associated with.\n     recognizer-header   =    confidence-threshold\
    \     ; Section 8.4.1\n                         /    sensitivity-level       \
    \ ; Section 8.4.2\n                         /    speed-vs-accuracy        ; Section\
    \ 8.4.3\n                         /    n-best-list-length       ; Section 8.4.4\n\
    \                         /    no-input-timeout         ; Section 8.4.5\n    \
    \                     /    recognition-timeout      ; Section 8.4.6\n        \
    \                 /    waveform-url             ; Section 8.4.7\n            \
    \             /    completion-cause         ; Section 8.4.8\n                \
    \         /    recognizer-context-block ; Section 8.4.9\n                    \
    \     /    recognizer-start-timers  ; Section 8.4.10\n                       \
    \  /    vendor-specific          ; Section 8.4.11\n                         /\
    \    speech-complete-timeout  ; Section 8.4.12\n                         /   \
    \ speech-incomplete-timeout; Section 8.4.13\n                         /    dtmf-interdigit-timeout\
    \  ; Section 8.4.14\n                         /    dtmf-term-timeout        ;\
    \ Section 8.4.15\n                         /    dtmf-term-char           ; Section\
    \ 8.4.16\n                         /    fetch-timeout            ; Section 8.4.17\n\
    \                         /    failed-uri               ; Section 8.4.18\n   \
    \                      /    failed-uri-cause         ; Section 8.4.19\n      \
    \                   /    save-waveform            ; Section 8.4.20\n         \
    \                /    new-audio-channel        ; Section 8.4.21\n            \
    \             /    speech-language          ; Section 8.4.22\n     Parameter \
    \               Support   Methods/Events\n     confidence-threshold     MANDATORY\
    \ SET-PARAMS, RECOGNIZE\n                                        GET-RESULT\n\
    \     sensitivity-level        Optional  SET-PARAMS, GET-PARAMS,\n           \
    \                             RECOGNIZE\n     speed-vs-accuracy        Optional\
    \  SET-PARAMS, GET-PARAMS,\n                                        RECOGNIZE\n\
    \     n-best-list-length       Optional  SET-PARAMS, GET-PARAMS,\n           \
    \                             RECOGNIZE, GET-RESULT\n     no-input-timeout   \
    \      MANDATORY SET-PARAMS, GET-PARAMS,\n                                   \
    \     RECOGNIZE\n     recognition-timeout      MANDATORY SET-PARAMS, GET-PARAMS,\n\
    \                                        RECOGNIZE\n     waveform-url        \
    \     MANDATORY RECOGNITION-COMPLETE\n     completion-cause         MANDATORY\
    \ DEFINE-GRAMMAR, RECOGNIZE,\n                                        RECOGNITON-COMPLETE\n\
    \     recognizer-context-block Optional  SET-PARAMS, GET-PARAMS\n     recognizer-start-timers\
    \  MANDATORY RECOGNIZE\n     vendor-specific          MANDATORY SET-PARAMS, GET-PARAMS\n\
    \     speech-complete-timeout  MANDATORY SET-PARAMS, GET-PARAMS\n            \
    \                            RECOGNIZE\n     speech-incomplete-timeout MANDATORY\
    \ SET-PARAMS, GET-PARAMS\n                                        RECOGNIZE\n\
    \     dtmf-interdigit-timeout  MANDATORY SET-PARAMS, GET-PARAMS\n            \
    \                            RECOGNIZE\n     dtmf-term-timeout        MANDATORY\
    \ SET-PARAMS, GET-PARAMS\n                                        RECOGNIZE\n\
    \     dtmf-term-char           MANDATORY SET-PARAMS, GET-PARAMS\n            \
    \                            RECOGNIZE\n     fetch-timeout            MANDATORY\
    \ SET-PARAMS, GET-PARAMS\n                                        RECOGNIZE, DEFINE-GRAMMAR\n\
    \     failed-uri               MANDATORY DEFINE-GRAMMAR response,\n          \
    \                              RECOGNITION-COMPLETE\n     failed-uri-cause   \
    \      MANDATORY DEFINE-GRAMMAR response,\n                                  \
    \      RECOGNITION-COMPLETE\n     save-waveform            MANDATORY SET-PARAMS,\
    \ GET-PARAMS,\n                                        RECOGNIZE\n     new-audio-channel\
    \        MANDATORY RECOGNIZE\n     speech-language          MANDATORY SET-PARAMS,\
    \ GET-PARAMS,\n                                        RECOGNIZE, DEFINE-GRAMMAR\n"
- title: 8.4.1.  Confidence Threshold
  contents:
  - "8.4.1.  Confidence Threshold\n   When a recognition resource recognizes or matches\
    \ a spoken phrase\n   with some portion of the grammar, it associates a confidence\
    \ level\n   with that conclusion.  The confidence-threshold parameter tells the\n\
    \   recognizer resource what confidence level should be considered a\n   successful\
    \ match.  This is an integer from 0-100 indicating the\n   recognizer's confidence\
    \ in the recognition.  If the recognizer\n   determines that its confidence in\
    \ all its recognition results is less\n   than the confidence threshold, then\
    \ it MUST return no-match as the\n   recognition result.  This header field MAY\
    \ occur in RECOGNIZE, SET-\n   PARAMS, or GET-PARAMS.  The default value for this\
    \ field is platform\n   specific.\n     confidence-threshold =    \"Confidence-Threshold\"\
    \ \":\" 1*DIGIT CRLF\n"
- title: 8.4.2.  Sensitivity Level
  contents:
  - "8.4.2.  Sensitivity Level\n   To filter out background noise and not mistake\
    \ it for speech, the\n   recognizer may support a variable level of sound sensitivity.\
    \  The\n   sensitivity-level parameter allows the client to set this value on\n\
    \   the recognizer.  This header field MAY occur in RECOGNIZE, SET-\n   PARAMS,\
    \ or GET-PARAMS.  A higher value for this field means higher\n   sensitivity.\
    \  The default value for this field is platform specific.\n     sensitivity-level\
    \   =    \"Sensitivity-Level\" \":\" 1*DIGIT CRLF\n"
- title: 8.4.3.  Speed Vs Accuracy
  contents:
  - "8.4.3.  Speed Vs Accuracy\n   Depending on the implementation and capability\
    \ of the recognizer\n   resource, it may be tunable towards Performance or Accuracy.\
    \  Higher\n   accuracy may mean more processing and higher CPU utilization, meaning\n\
    \   less calls per media server and vice versa.  This parameter on the\n   resource\
    \ can be tuned by the speed-vs-accuracy header.  This header\n   field MAY occur\
    \ in RECOGNIZE, SET-PARAMS, or GET-PARAMS.  A higher\n   value for this field\
    \ means higher speed.  The default value for this\n   field is platform specific.\n\
    \     speed-vs-accuracy   =     \"Speed-Vs-Accuracy\" \":\" 1*DIGIT CRLF\n"
- title: 8.4.4.  N Best List Length
  contents:
  - "8.4.4.  N Best List Length\n   When the recognizer matches an incoming stream\
    \ with the grammar, it\n   may come up with more than one alternative match because\
    \ of\n   confidence levels in certain words or conversation paths.  If this\n\
    \   header field is not specified, by default, the recognition resource\n   will\
    \ only return the best match above the confidence threshold.  The\n   client,\
    \ by setting this parameter, could ask the recognition resource\n   to send it\
    \ more than 1 alternative.  All alternatives must still be\n   above the confidence-threshold.\
    \  A value greater than one does not\n   guarantee that the recognizer will send\
    \ the requested number of\n   alternatives.  This header field MAY occur in RECOGNIZE,\
    \ SET-PARAMS,\n   or GET-PARAMS.  The minimum value for this field is 1.  The\
    \ default\n   value for this field is 1.\n     n-best-list-length  =    \"N-Best-List-Length\"\
    \ \":\" 1*DIGIT CRLF\n"
- title: 8.4.5.  No Input Timeout
  contents:
  - "8.4.5.  No Input Timeout\n   When recognition is started and there is no speech\
    \ detected for a\n   certain period of time, the recognizer can send a RECOGNITION-\n\
    \   COMPLETE event to the client and terminate the recognition operation.\n  \
    \ The no-input-timeout header field can set this timeout value.  The\n   value\
    \ is in milliseconds.  This header field MAY occur in RECOGNIZE,\n   SET-PARAMS,\
    \ or GET-PARAMS.  The value for this field ranges from 0 to\n   MAXTIMEOUT, where\
    \ MAXTIMEOUT is platform specific.  The default value\n   for this field is platform\
    \ specific.\n     no-input-timeout    =    \"No-Input-Timeout\" \":\" 1*DIGIT\
    \ CRLF\n"
- title: 8.4.6.  Recognition Timeout
  contents:
  - "8.4.6.  Recognition Timeout\n   When recognition is started and there is no match\
    \ for a certain\n   period of time, the recognizer can send a RECOGNITION-COMPLETE\
    \ event\n   to the client and terminate the recognition operation.  The\n   recognition-timeout\
    \ parameter field sets this timeout value.  The\n   value is in milliseconds.\
    \  The value for this field ranges from 0 to\n   MAXTIMEOUT, where MAXTIMEOUT\
    \ is platform specific.  The default value\n   is 10 seconds.  This header field\
    \ MAY occur in RECOGNIZE, SET-PARAMS\n   or GET-PARAMS.\n     recognition-timeout\
    \ =    \"Recognition-Timeout\" \":\" 1*DIGIT CRLF\n"
- title: 8.4.7.  Waveform URL
  contents:
  - "8.4.7.  Waveform URL\n   If the save-waveform header field is set to true, the\
    \ recognizer MUST\n   record the incoming audio stream of the recognition into\
    \ a file and\n   provide a URI for the client to access it.  This header MUST\
    \ be\n   present in the RECOGNITION-COMPLETE event if the save-waveform header\n\
    \   field was set to true.  The URL value of the header MUST be NULL if\n   there\
    \ was some error condition preventing the server from recording.\n   Otherwise,\
    \ the URL generated by the server SHOULD be globally unique\n   across the server\
    \ and all its recognition sessions.  The URL SHOULD\n   BE available until the\
    \ session is torn down.\n     waveform-url        =    \"Waveform-URL\" \":\"\
    \ Url CRLF\n"
- title: 8.4.8.  Completion Cause
  contents:
  - "8.4.8.  Completion Cause\n   This header field MUST be part of a RECOGNITION-COMPLETE\
    \ event coming\n   from the recognizer resource to the client.  This indicates\
    \ the\n   reason behind the RECOGNIZE method completion.  This header field\n\
    \   MUST BE sent in the DEFINE-GRAMMAR and RECOGNIZE responses, if they\n   return\
    \ with a failure status and a COMPLETE state.\n     Cause-Code     Cause-Name\
    \     Description\n       000           success       RECOGNIZE completed with\
    \ a match or\n                                   DEFINE-GRAMMAR succeeded in\n\
    \                                   downloading and compiling the\n          \
    \                         grammar\n       001           no-match      RECOGNIZE\
    \ completed, but no match\n                                   was found\n    \
    \   002          no-input-timeout\n                                   RECOGNIZE\
    \ completed without a match\n                                   due to a no-input-timeout\n\
    \       003          recognition-timeout\n                                   RECOGNIZE\
    \ completed without a match\n                                   due to a recognition-timeout\n\
    \       004           gram-load-failure\n                                   RECOGNIZE\
    \ failed due grammar load\n                                   failure.\n     \
    \  005           gram-comp-failure\n                                   RECOGNIZE\
    \ failed due to grammar\n                                   compilation failure.\n\
    \       006           error         RECOGNIZE request terminated\n           \
    \                        prematurely due to a recognizer\n                   \
    \                error.\n       007           speech-too-early\n             \
    \                      RECOGNIZE request terminated because\n                \
    \                   speech was too early.\n       008           too-much-speech-timeout\n\
    \                                   RECOGNIZE request terminated because\n   \
    \                                speech was too long.\n       009           uri-failure\
    \   Failure accessing a URI.\n       010           language-unsupported\n    \
    \                               Language not supported.\n"
- title: 8.4.9.  Recognizer Context Block
  contents:
  - "8.4.9.  Recognizer Context Block\n   This parameter MAY BE sent as part of the\
    \ SET-PARAMS or GET-PARAMS\n   request.  If the GET-PARAMS method contains this\
    \ header field with no\n   value, then it is a request to the recognizer to return\
    \ the\n   recognizer context block.  The response to such a message MAY contain\n\
    \   a recognizer context block as a message entity.  If the server\n   returns\
    \ a recognizer context block, the response MUST contain this\n   header field\
    \ and its value MUST match the content-id of that entity.\n   If the SET-PARAMS\
    \ method contains this header field, it MUST contain\n   a message entity containing\
    \ the recognizer context data, and a\n   content-id matching this header field.\n\
    \   This content-id should match the content-id that came with the\n   context\
    \ data during the GET-PARAMS operation.\n     recognizer-context-block =    \"\
    Recognizer-Context-Block\" \":\"\n                                   1*ALPHA CRLF\n"
- title: 8.4.10.  Recognition Start Timers
  contents:
  - "8.4.10.  Recognition Start Timers\n   This parameter MAY BE sent as part of the\
    \ RECOGNIZE request.  A value\n   of false tells the recognizer to start recognition,\
    \ but not to start\n   the no-input timer yet.  The recognizer should not start\
    \ the timers\n   until the client sends a RECOGNITION-START-TIMERS request to\
    \ the\n   recognizer.  This is useful in the scenario when the recognizer and\n\
    \   synthesizer engines are not part of the same session.  Here, when a\n   kill-on-barge-in\
    \ prompt is being played, you want the RECOGNIZE\n   request to be simultaneously\
    \ active so that it can detect and\n   implement kill-on-barge-in.  But at the\
    \ same time, you don't want the\n   recognizer to start the no-input timers until\
    \ the prompt is finished.\n   The default value is \"true\".\n     recognizer-start-timers\
    \  =    \"Recognizer-Start-Timers\" \":\"\n                                  \
    \ boolean-value CRLF\n"
- title: 8.4.11.  Vendor Specific Parameters
  contents:
  - "8.4.11.  Vendor Specific Parameters\n   This set of headers allows the client\
    \ to set Vendor Specific\n   parameters.\n   This header can be sent in the SET-PARAMS\
    \ method and is used to set\n   vendor-specific parameters on the server.  The\
    \ vendor-av-pair-name\n   can be any vendor-specific field name and conforms to\
    \ the XML\n   vendor-specific attribute naming convention.  The vendor-av-pair-\n\
    \   value is the value to set the attribute to, and needs to be quoted.\n   When\
    \ asking the server to get the current value of these parameters,\n   this header\
    \ can be sent in the GET-PARAMS method with the list of\n   vendor-specific attribute\
    \ names to get separated by a semicolon.\n   This header field MAY occur in SET-PARAMS\
    \ or GET-PARAMS.\n"
- title: 8.4.12.  Speech Complete Timeout
  contents:
  - "8.4.12.  Speech Complete Timeout\n   This header field specifies the length of\
    \ silence required following\n   user speech before the speech recognizer finalizes\
    \ a result (either\n   accepting it or throwing a nomatch event).  The speech-complete-\n\
    \   timeout value is used when the recognizer currently has a complete\n   match\
    \ of an active grammar, and specifies how long it should wait for\n   more input\
    \ before declaring a match.  By contrast, the incomplete\n   timeout is used when\
    \ the speech is an incomplete match to an active\n   grammar.  The value is in\
    \ milliseconds.\n     speech-complete-timeout = \"Speech-Complete-Timeout\" \"\
    :\"\n                               1*DIGIT CRLF\n   A long speech-complete-timeout\
    \ value delays the result completion\n   and, therefore, makes the computer's\
    \ response slow.  A short speech-\n   complete-timeout may lead to an utterance\
    \ being broken up\n   inappropriately.  Reasonable complete timeout values are\
    \ typically in\n   the range of 0.3 seconds to 1.0 seconds.  The value for this\
    \ field\n   ranges from 0 to MAXTIMEOUT, where MAXTIMEOUT is platform specific.\n\
    \   The default value for this field is platform specific.  This header\n   field\
    \ MAY occur in RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n"
- title: 8.4.13.  Speech Incomplete Timeout
  contents:
  - "8.4.13.  Speech Incomplete Timeout\n   This header field specifies the required\
    \ length of silence following\n   user speech, after which a recognizer finalizes\
    \ a result.  The\n   incomplete timeout applies when the speech prior to the silence\
    \ is an\n   incomplete match of all active grammars.  In this case, once the\n\
    \   timeout is triggered, the partial result is rejected (with a nomatch\n   event).\
    \  The value is in milliseconds.  The value for this field\n   ranges from 0 to\
    \ MAXTIMEOUT, where MAXTIMEOUT is platform specific.\n   The default value for\
    \ this field is platform specific.\n     speech-incomplete-timeout = \"Speech-Incomplete-Timeout\"\
    \ \":\"\n                                 1*DIGIT CRLF\n   The speech-incomplete-timeout\
    \ also applies when the speech prior to\n   the silence is a complete match of\
    \ an active grammar, but where it is\n   possible to speak further and still match\
    \ the grammar.  By contrast,\n   the complete timeout is used when the speech\
    \ is a complete match to\n   an active grammar and no further words can be spoken.\n\
    \   A long speech-incomplete-timeout value delays the result completion\n   and,\
    \ therefore, makes the computer's response slow.  A short speech-\n   incomplete-timeout\
    \ may lead to an utterance being broken up\n   inappropriately.\n   The speech-incomplete-timeout\
    \ is usually longer than the speech-\n   complete-timeout to allow users to pause\
    \ mid-utterance (for example,\n   to breathe).  This header field MAY occur in\
    \ RECOGNIZE, SET-PARAMS,\n   or GET-PARAMS.\n"
- title: 8.4.14.  DTMF Interdigit Timeout
  contents:
  - "8.4.14.  DTMF Interdigit Timeout\n   This header field specifies the inter-digit\
    \ timeout value to use when\n   recognizing DTMF input.  The value is in milliseconds.\
    \  The value for\n   this field ranges from 0 to MAXTIMEOUT, where MAXTIMEOUT\
    \ is platform\n   specific.  The default value is 5 seconds.  This header field\
    \ MAY\n   occur in RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n     dtmf-interdigit-timeout\
    \ = \"DTMF-Interdigit-Timeout\" \":\"\n                               1*DIGIT\
    \ CRLF\n"
- title: 8.4.15.  DTMF Term Timeout
  contents:
  - "8.4.15.  DTMF Term Timeout\n   This header field specifies the terminating timeout\
    \ to use when\n   recognizing DTMF input.  The value is in milliseconds.  The\
    \ value for\n   this field ranges from 0 to MAXTIMEOUT, where MAXTIMEOUT is platform\n\
    \   specific.  The default value is 10 seconds.  This header field MAY\n   occur\
    \ in RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n     dtmf-term-timeout   =    \"DTMF-Term-Timeout\"\
    \ \":\" 1*DIGIT CRLF\n"
- title: 8.4.16.  DTMF-Term-Char
  contents:
  - "8.4.16.  DTMF-Term-Char\n   This header field specifies the terminating DTMF\
    \ character for DTMF\n   input recognition.  The default value is NULL which is\
    \ specified as\n   an empty header field.  This header field MAY occur in RECOGNIZE,\n\
    \   SET-PARAMS, or GET-PARAMS.\n     dtmf-term-char      =    \"DTMF-Term-Char\"\
    \ \":\" CHAR CRLF\n"
- title: 8.4.17.  Fetch Timeout
  contents:
  - "8.4.17.  Fetch Timeout\n   When the recognizer needs to fetch grammar documents,\
    \ this header\n   field controls URI access properties.  This defines the recognizer\n\
    \   timeout for completing the fetch of the resources the media server\n   needs\
    \ from the network.  The value is in milliseconds.  The value for\n   this field\
    \ ranges from 0 to MAXTIMEOUT, where MAXTIMEOUT is platform\n   specific.  The\
    \ default value for this field is platform specific.\n   This header field MAY\
    \ occur in RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n"
- title: 8.4.18.  Failed URI
  contents:
  - "8.4.18.  Failed URI\n   When a recognizer method needs a recognizer to fetch\
    \ or access a URI,\n   and the access fails, the media server SHOULD provide the\
    \ failed URI\n   in this header field in the method response.\n"
- title: 8.4.19.  Failed URI Cause
  contents:
  - "8.4.19.  Failed URI Cause\n   When a recognizer method needs a recognizer to\
    \ fetch or access a URI,\n   and the access fails, the media server SHOULD provide\
    \ the URI-\n   specific or protocol-specific response code through this header\
    \ field\n   in the method response.  This field has been defined as alphanumeric\n\
    \   to accommodate all protocols, some of which might have a response\n   string\
    \ instead of a numeric response code.\n"
- title: 8.4.20.  Save Waveform
  contents:
  - "8.4.20.  Save Waveform\n   This header field allows the client to indicate to\
    \ the recognizer\n   that it MUST save the audio stream that was recognized. \
    \ The\n   recognizer MUST then record the recognized audio and make it\n   available\
    \ to the client in the form of a URI returned in the\n   waveform-uri header field\
    \ in the RECOGNITION-COMPLETE event.  If\n   there was an error in recording the\
    \ stream or the audio clip is\n   otherwise not available, the recognizer MUST\
    \ return an empty\n   waveform-uri header field.  The default value for this fields\
    \ is\n   \"false\".\n     save-waveform       =    \"Save-Waveform\" \":\" boolean-value\
    \ CRLF\n"
- title: 8.4.21.  New Audio Channel
  contents:
  - "8.4.21.  New Audio Channel\n   This header field MAY BE specified in a RECOGNIZE\
    \ message and allows\n   the client to tell the media server that, from that point\
    \ on, it will\n   be sending audio data from a new audio source, channel, or speaker.\n\
    \   If the recognition resource had collected any line statistics or\n   information,\
    \ it MUST discard it and start fresh for this RECOGNIZE.\n   This helps in the\
    \ case where the client MAY want to reuse an open\n   recognition session with\
    \ the media server for multiple telephone\n   calls.\n     new-audio-channel \
    \  =    \"New-Audio-Channel\" \":\" boolean-value CRLF\n"
- title: 8.4.22.  Speech Language
  contents:
  - "8.4.22.  Speech Language\n   This header field specifies the language of recognition\
    \ grammar data\n   within a session or request, if it is not specified within\
    \ the data.\n   The value of this header field should follow RFC 3066 [16] for\
    \ its\n   values.  This MAY occur in DEFINE-GRAMMAR, RECOGNIZE, SET-PARAMS, or\n\
    \   GET-PARAMS request.\n"
- title: 8.5.  Recognizer Message Body
  contents:
  - "8.5.  Recognizer Message Body\n   A recognizer message may carry additional data\
    \ associated with the\n   method, response, or event.  The client may send the\
    \ grammar to be\n   recognized in DEFINE-GRAMMAR or RECOGNIZE requests.  When\
    \ the grammar\n   is sent in the DEFINE-GRAMMAR method, the server should be able\
    \ to\n   download compile and optimize the grammar.  The RECOGNIZE request\n \
    \  MUST contain a list of grammars that need to be active during the\n   recognition.\
    \  The server resource may send the recognition results in\n   the RECOGNITION-COMPLETE\
    \ event or the GET-RESULT response.  This data\n   will be carried in the message\
    \ body of the corresponding MRCP\n   message.\n"
- title: 8.5.1.  Recognizer Grammar Data
  contents:
  - "8.5.1.  Recognizer Grammar Data\n   Recognizer grammar data from the client to\
    \ the server can be provided\n   inline or by reference.  Either way, they are\
    \ carried as MIME\n   entities in the message body of the MRCP request message.\
    \  The\n   grammar specified inline or by reference specifies the grammar used\n\
    \   to match in the recognition process and this data is specified in one\n  \
    \ of the standard grammar specification formats like W3C's XML or ABNF\n   or\
    \ Sun's Java Speech Grammar Format, etc.  All media servers MUST\n   support W3C's\
    \ XML based grammar markup format [11] (MIME-type\n   application/grammar+xml)\
    \ and SHOULD support the ABNF form (MIME-type\n   application/grammar).\n   When\
    \ a grammar is specified in-line in the message, the client MUST\n   provide a\
    \ content-id for that grammar as part of the content headers.\n   The server MUST\
    \ store the grammar associated with that content-id for\n   the duration of the\
    \ session.  A stored grammar can be overwritten by\n   defining a new grammar\
    \ with the same content-id.  Grammars that have\n   been associated with a content-id\
    \ can be referenced through a special\n   \"session:\" URI scheme.\n   Example:\n\
    \     session:help@root-level.store\n   If grammar data needs to be specified\
    \ by external URI reference, the\n   MIME-type text/uri-list is used to list the\
    \ one or more URI that will\n   specify the grammar data.  All media servers MUST\
    \ support the HTTP\n   URI access mechanism.\n   If the data to be defined consists\
    \ of a mix of URI and inline grammar\n   data, the multipart/mixed MIME-type is\
    \ used and embedded with the\n   MIME-blocks for text/uri-list, application/grammar\
    \ or\n   application/grammar+xml.  The character set and encoding used in the\n\
    \   grammar data may be specified according to standard MIME-type\n   definitions.\n\
    \   When more than one grammar URI or inline grammar block is specified\n   in\
    \ a message body of the RECOGNIZE request, it is an active list of\n   grammar\
    \ alternatives to listen.  The ordering of the list implies the\n   precedence\
    \ of the grammars, with the first grammar in the list having\n   the highest precedence.\n\
    \   Example 1:\n       Content-Type:application/grammar+xml\n       Content-Id:request1@form-level.store\n\
    \       Content-Length:104\n       <?xml version=\"1.0\"?>\n       <!-- the default\
    \ grammar language is US English -->\n       <grammar xml:lang=\"en-US\" version=\"\
    1.0\">\n       <!-- single language attachment to tokens -->\n       <rule id=\"\
    yes\">\n                  <one-of>\n                      <item xml:lang=\"fr-CA\"\
    >oui</item>\n                      <item xml:lang=\"en-US\">yes</item>\n     \
    \             </one-of>\n          </rule>\n       <!-- single language attachment\
    \ to a rule expansion -->\n          <rule id=\"request\">\n                 \
    \ may I speak to\n                  <one-of xml:lang=\"fr-CA\">\n            \
    \          <item>Michel Tremblay</item>\n                      <item>Andre Roy</item>\n\
    \                  </one-of>\n          </rule>\n          <!-- multiple language\
    \ attachment to a token -->\n          <rule id=\"people1\">\n               \
    \   <token lexicon=\"en-US,fr-CA\"> Robert </token>\n          </rule>\n     \
    \     <!-- the equivalent single-language attachment expansion -->\n         \
    \ <rule id=\"people2\">\n                  <one-of>\n                      <item\
    \ xml:lang=\"en-US\">Robert</item>\n                      <item xml:lang=\"fr-CA\"\
    >Robert</item>\n                  </one-of>\n          </rule>\n          </grammar>\n\
    \   Example 2:\n      Content-Type:text/uri-list\n      Content-Length:176\n \
    \     session:help@root-level.store\n      http://www.cisco.com/Directory-Name-List.grxml\n\
    \      http://www.cisco.com/Department-List.grxml\n      http://www.cisco.com/TAC-Contact-List.grxml\n\
    \      session:menu1@menu-level.store\n   Example 3:\n      Content-Type:multipart/mixed;\
    \ boundary=\"--break\"\n      --break\n      Content-Type:text/uri-list\n    \
    \  Content-Length:176\n      http://www.cisco.com/Directory-Name-List.grxml\n\
    \      http://www.cisco.com/Department-List.grxml\n      http://www.cisco.com/TAC-Contact-List.grxml\n\
    \      --break\n      Content-Type:application/grammar+xml\n      Content-Id:request1@form-level.store\n\
    \      Content-Length:104\n      <?xml version=\"1.0\"?>\n      <!-- the default\
    \ grammar language is US English -->\n      <grammar xml:lang=\"en-US\" version=\"\
    1.0\">\n      <!-- single language attachment to tokens -->\n      <rule id=\"\
    yes\">\n                  <one-of>\n                      <item xml:lang=\"fr-CA\"\
    >oui</item>\n                      <item xml:lang=\"en-US\">yes</item>\n     \
    \             </one-of>\n         </rule>\n      <!-- single language attachment\
    \ to a rule expansion -->\n         <rule id=\"request\">\n                  may\
    \ I speak to\n                  <one-of xml:lang=\"fr-CA\">\n                \
    \      <item>Michel Tremblay</item>\n                      <item>Andre Roy</item>\n\
    \                  </one-of>\n         </rule>\n         <!-- multiple language\
    \ attachment to a token -->\n         <rule id=\"people1\">\n                \
    \  <token lexicon=\"en-US,fr-CA\"> Robert </token>\n         </rule>\n       \
    \  <!-- the equivalent single-language attachment expansion -->\n         <rule\
    \ id=\"people2\">\n                  <one-of>\n                      <item xml:lang=\"\
    en-US\">Robert</item>\n                      <item xml:lang=\"fr-CA\">Robert</item>\n\
    \                  </one-of>\n         </rule>\n         </grammar>\n       --break\n"
- title: 8.5.2.  Recognizer Result Data
  contents:
  - "8.5.2.  Recognizer Result Data\n   Recognition result data from the server is\
    \ carried in the MRCP\n   message body of the RECOGNITION-COMPLETE event or the\
    \ GET-RESULT\n   response message as MIME entities.  All media servers MUST support\n\
    \   W3C's Natural Language Semantics Markup Language (NLSML) [10] as the\n   default\
    \ standard for returning recognition results back to the\n   client, and hence\
    \ MUST support the MIME-type application/x-nlsml.\n   Example 1:\n      Content-Type:application/x-nlsml\n\
    \      Content-Length:104\n      <?xml version=\"1.0\"?>\n      <result grammar=\"\
    http://theYesNoGrammar\">\n          <interpretation>\n              <instance>\n\
    \                  <myApp:yes_no>\n                      <response>yes</response>\n\
    \                  </myApp:yes_no>\n              </instance>\n              <input>ok</input>\n\
    \          </interpretation>\n      </result>\n"
- title: 8.5.3.  Recognizer Context Block
  contents:
  - "8.5.3.  Recognizer Context Block\n   When the client has to change recognition\
    \ servers within a call, this\n   is a block of data that the client MAY collect\
    \ from the first media\n   server and provide to the second media server.  This\
    \ may be because\n   the client needs different language support or because the\
    \ media\n   server issued an RTSP RE-DIRECT.  Here, the first recognizer may have\n\
    \   collected acoustic and other data during its recognition.  When we\n   switch\
    \ recognition servers, communicating this data may allow the\n   second recognition\
    \ server to provide better recognition based on the\n   acoustic data collected\
    \ by the previous recognizer.  This block of\n   data is vendor-specific and MUST\
    \ be carried as MIME-type\n   application/octets in the body of the message.\n\
    \   This block of data is communicated in the SET-PARAMS and GET-PARAMS\n   method/response\
    \ messages.  In the GET-PARAMS method, if an empty\n   recognizer-context-block\
    \ header field is present, then the recognizer\n   should return its vendor-specific\
    \ context block in the message body\n   as a MIME-entity with a specific content-id.\
    \  The content-id value\n   should also be specified in the recognizer-context-block\
    \ header field\n   in the GET-PARAMS response.  The SET-PARAMS request wishing\
    \ to\n   provide this vendor-specific data should send it in the message body\n\
    \   as a MIME-entity with the same content-id that it received from the\n   GET-PARAMS.\
    \  The content-id should also be sent in the recognizer-\n   context-block header\
    \ field of the SET-PARAMS message.\n   Each automatic speech recognition (ASR)\
    \ vendor choosing to use this\n   mechanism to handoff recognizer context data\
    \ among its servers should\n   distinguish its vendor-specific block of data from\
    \ other vendors by\n   choosing a unique content-id that they should recognize.\n"
- title: 8.6.  SET-PARAMS
  contents:
  - "8.6.  SET-PARAMS\n   The SET-PARAMS method, from the client to the server, tells\
    \ the\n   recognizer resource to set and modify recognizer context parameters\n\
    \   like recognizer characteristics, result detail level, etc.  In the\n   following\
    \ sections some standard parameters are discussed.  If the\n   server resource\
    \ does not recognize an OPTIONAL parameter, it MUST\n   ignore that field.  Many\
    \ of the parameters in the SET-PARAMS method\n   can also be used in another method\
    \ like the RECOGNIZE method.  But\n   the difference is that when you set something\
    \ like the sensitivity-\n   level using the SET-PARAMS, it applies for all future\
    \ requests,\n   whenever applicable.  On the other hand, when you pass sensitivity-\n\
    \   level in a RECOGNIZE request, it applies only to that request.\n   Example:\n\
    \     C->S:SET-PARAMS 543256 MRCP/1.0\n          Sensitivity-Level:20\n      \
    \    Recognition-Timeout:30\n          Confidence-Threshold:85\n     S->C:MRCP/1.0\
    \ 543256 200 COMPLETE\n"
- title: 8.7.  GET-PARAMS
  contents:
  - "8.7.  GET-PARAMS\n   The GET-PARAMS method, from the client to the server, asks\
    \ the\n   recognizer resource for its current default parameters, like\n   sensitivity-level,\
    \ n-best-list-length, etc.  The client can request\n   specific parameters from\
    \ the server by sending it one or more empty\n   parameter headers with no values.\
    \  The server should then return the\n   settings for those specific parameters\
    \ only.  When the client does\n   not send a specific list of empty parameter\
    \ headers, the recognizer\n   should return the settings for all parameters. \
    \ The wild card use can\n   be very intensive as the number of settable parameters\
    \ can be large\n   depending on the vendor.  Hence, it is RECOMMENDED that the\
    \ client\n   does not use the wildcard GET-PARAMS operation very often.\n   Example:\n\
    \     C->S:GET-PARAMS 543256 MRCP/1.0\n          Sensitivity-Level:\n        \
    \  Recognition-Timeout:\n          Confidence-threshold:\n     S->C:MRCP/1.0 543256\
    \ 200 COMPLETE\n          Sensitivity-Level:20\n          Recognition-Timeout:30\n\
    \          Confidence-Threshold:85\n"
- title: 8.8.  DEFINE-GRAMMAR
  contents:
  - "8.8.  DEFINE-GRAMMAR\n   The DEFINE-GRAMMAR method, from the client to the server,\
    \ provides a\n   grammar and tells the server to define, download if needed, and\n\
    \   compile the grammar.\n   If the server resource is in the recognition state,\
    \ the DEFINE-\n   GRAMMAR request MUST respond with a failure status.\n   If the\
    \ resource is in the idle state and is able to successfully load\n   and compile\
    \ the grammar, the status MUST return a success code and\n   the request-state\
    \ MUST be COMPLETE.\n   If the recognizer could not define the grammar for some\
    \ reason, say\n   the download failed or the grammar failed to compile, or the\
    \ grammar\n   was in an unsupported form, the MRCP response for the DEFINE-GRAMMAR\n\
    \   method MUST contain a failure status code of 407, and a completion-\n   cause\
    \ header field describing the failure reason.\n   Example:\n     C->S:DEFINE-GRAMMAR\
    \ 543257 MRCP/1.0\n          Content-Type:application/grammar+xml\n          Content-Id:request1@form-level.store\n\
    \          Content-Length:104\n          <?xml version=\"1.0\"?>\n          <!--\
    \ the default grammar language is US English -->\n          <grammar xml:lang=\"\
    en-US\" version=\"1.0\">\n          <!-- single language attachment to tokens\
    \ -->\n          <rule id=\"yes\">\n              <one-of>\n                 \
    \ <item xml:lang=\"fr-CA\">oui</item>\n                  <item xml:lang=\"en-US\"\
    >yes</item>\n              </one-of>\n          </rule>\n          <!-- single\
    \ language attachment to a rule expansion -->\n          <rule id=\"request\"\
    >\n              may I speak to\n              <one-of xml:lang=\"fr-CA\">\n \
    \                 <item>Michel Tremblay</item>\n                  <item>Andre\
    \ Roy</item>\n              </one-of>\n          </rule>\n          </grammar>\n\
    \     S->C:MRCP/1.0 543257 200 COMPLETE\n          Completion-Cause:000 success\n\
    \     C->S:DEFINE-GRAMMAR 543258 MRCP/1.0\n          Content-Type:application/grammar+xml\n\
    \          Content-Id:helpgrammar@root-level.store\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <!-- the default grammar language\
    \ is US English -->\n          <grammar xml:lang=\"en-US\" version=\"1.0\">\n\
    \          <rule id=\"request\">\n              I need help\n          </rule>\n\
    \          </grammar>\n     S->C:MRCP/1.0 543258 200 COMPLETE\n          Completion-Cause:000\
    \ success\n     C->S:DEFINE-GRAMMAR 543259 MRCP/1.0\n          Content-Type:application/grammar+xml\n\
    \          Content-Id:request2@field-level.store\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <!-- the default grammar language\
    \ is US English -->\n          <grammar xml:lang=\"en-US\" version=\"1.0\">\n\
    \          <rule id=\"request\">\n              I need help\n          </rule>\n\
    \     S->C:MRCP/1.0 543258 200 COMPLETE\n          Completion-Cause:000 success\n\
    \     C->S:DEFINE-GRAMMAR 543259 MRCP/1.0\n          Content-Type:application/grammar+xml\n\
    \          Content-Id:request2@field-level.store\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n               <grammar xml:lang=\"en\">\n\
    \               <import uri=\"session:politeness@form-level.store\"\n        \
    \               name=\"polite\"/>\n               <rule id=\"basicCmd\" scope=\"\
    public\">\n               <example> please move the window </example>\n      \
    \         <example> open a file </example>\n               <ruleref import=\"\
    polite#startPolite\"/>\n               <ruleref uri=\"#command\"/>\n         \
    \      <ruleref import=\"polite#endPolite\"/>\n               </rule>\n      \
    \         <rule id=\"command\">\n               <ruleref uri=\"#action\"/> <ruleref\
    \ uri=\"#object\"/>\n               </rule>\n               <rule id=\"action\"\
    >\n                    <choice>\n                    <item weight=\"10\" tag=\"\
    OPEN\">   open </item>\n                    <item weight=\"2\"  tag=\"CLOSE\"\
    >  close </item>\n                    <item weight=\"1\"  tag=\"DELETE\"> delete\
    \ </item>\n                    <item weight=\"1\"  tag=\"MOVE\">   move </item>\n\
    \                    </choice>\n               </rule>\n               <rule id=\"\
    object\">\n               <count number=\"optional\">\n                    <choice>\n\
    \                         <item> the </item>\n                         <item>\
    \ a </item>\n                    </choice>\n               </count>\n        \
    \       <choice>\n                    <item> window </item>\n                \
    \    <item> file </item>\n                    <item> menu </item>\n          \
    \     </choice>\n               </rule>\n               </grammar>\n     S->C:MRCP/1.0\
    \ 543259 200 COMPLETE\n          Completion-Cause:000 success\n     C->S:RECOGNIZE\
    \ 543260 MRCP/1.0\n          N-Best-List-Length:2\n          Content-Type:text/uri-list\n\
    \          Content-Length:176\n          session:request1@form-level.store\n \
    \         session:request2@field-level.store\n          session:helpgramar@root-level.store\n\
    \     S->C:MRCP/1.0 543260 200 IN-PROGRESS\n     S->C:START-OF-SPEECH 543260 IN-PROGRESS\
    \ MRCP/1.0\n     S->C:RECOGNITION-COMPLETE 543260 COMPLETE MRCP/1.0\n        \
    \  Completion-Cause:000 success\n          Waveform-URL:http://web.media.com/session123/audio.wav\n\
    \          Content-Type:applicationt/x-nlsml\n          Content-Length:276\n \
    \         <?xml version=\"1.0\"?>\n          <result x-model=\"http://IdentityModel\"\
    \n            xmlns:xf=\"http://www.w3.org/2000/xforms\"\n            grammar=\"\
    session:request1@form-level.store\">\n               <interpretation>\n      \
    \              <xf:instance name=\"Person\">\n                      <Person>\n\
    \                          <Name> Andre Roy </Name>\n                      </Person>\n\
    \                    </xf:instance>\n                    <input>   may I speak\
    \ to Andre Roy </input>\n               </interpretation>\n          </result>\n"
- title: 8.9.  RECOGNIZE
  contents:
  - "8.9.  RECOGNIZE\n   The RECOGNIZE method from the client to the server tells\
    \ the\n   recognizer to start recognition and provides it with a grammar to\n\
    \   match for.  The RECOGNIZE method can carry parameters to control the\n   sensitivity,\
    \ confidence level, and the level of detail in results\n   provided by the recognizer.\
    \  These parameters override the current\n   defaults set by a previous SET-PARAMS\
    \ method.\n   If the resource is in the recognition state, the RECOGNIZE request\n\
    \   MUST respond with a failure status.\n   If the resource is in the Idle state\
    \ and was able to successfully\n   start the recognition, the server MUST return\
    \ a success code and a\n   request-state of IN-PROGRESS.  This means that the\
    \ recognizer is\n   active and that the client should expect further events with\
    \ this\n   request-id.\n   If the resource could not start a recognition, it MUST\
    \ return a\n   failure status code of 407 and contain a completion-cause header\n\
    \   field describing the cause of failure.\n   For the recognizer resource, this\
    \ is the only request that can return\n   request-state of IN-PROGRESS, meaning\
    \ that recognition is in\n   progress.  When the recognition completes by matching\
    \ one of the\n   grammar alternatives or by a time-out without a match or for\
    \ some\n   other reason, the recognizer resource MUST send the client a\n   RECOGNITON-COMPLETE\
    \ event with the result of the recognition and a\n   request-state of COMPLETE.\n\
    \   For large grammars that can take a long time to compile and for\n   grammars\
    \ that are used repeatedly, the client could issue a DEFINE-\n   GRAMMAR request\
    \ with the grammar ahead of time.  In such a case, the\n   client can issue the\
    \ RECOGNIZE request and reference the grammar\n   through the \"session:\" special\
    \ URI.  This also applies in general if\n   the client wants to restart recognition\
    \ with a previous inline\n   grammar.\n   Note that since the audio and the messages\
    \ are carried over separate\n   communication paths there may be a race condition\
    \ between the start\n   of the flow of audio and the receipt of the RECOGNIZE\
    \ method.  For\n   example, if audio flow is started by the client at the same\
    \ time as\n   the RECOGNIZE method is sent, either the audio or the RECOGNIZE\
    \ will\n   arrive at the recognizer first.  As another example, the client may\n\
    \   chose to continuously send audio to the Media server and signal the\n   Media\
    \ server to recognize using the RECOGNIZE method.  A number of\n   mechanisms\
    \ exist to resolve this condition and the mechanism chosen\n   is left to the\
    \ implementers of recognizer Media servers.\n   Example:\n     C->S:RECOGNIZE\
    \ 543257 MRCP/1.0\n          Confidence-Threshold:90\n          Content-Type:application/grammar+xml\n\
    \          Content-Id:request1@form-level.store\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <!-- the default grammar language\
    \ is US English -->\n          <grammar xml:lang=\"en-US\" version=\"1.0\">\n\
    \          <!-- single language attachment to tokens -->\n          <rule id=\"\
    yes\">\n                   <one-of>\n                            <item xml:lang=\"\
    fr-CA\">oui</item>\n                            <item xml:lang=\"en-US\">yes</item>\n\
    \                   </one-of>\n               </rule>\n          <!-- single language\
    \ attachment to a rule expansion -->\n               <rule id=\"request\">\n \
    \                  may I speak to\n                   <one-of xml:lang=\"fr-CA\"\
    >\n                            <item>Michel Tremblay</item>\n                \
    \            <item>Andre Roy</item>\n                   </one-of>\n          \
    \     </rule>\n            </grammar>\n     S->C:MRCP/1.0 543257 200 IN-PROGRESS\n\
    \     S->C:START-OF-SPEECH 543257 IN-PROGRESS MRCP/1.0\n     S->C:RECOGNITION-COMPLETE\
    \ 543257 COMPLETE MRCP/1.0\n          Completion-Cause:000 success\n         \
    \ Waveform-URL:http://web.media.com/session123/audio.wav\n          Content-Type:application/x-nlsml\n\
    \          Content-Length:276\n          <?xml version=\"1.0\"?>\n          <result\
    \ x-model=\"http://IdentityModel\"\n            xmlns:xf=\"http://www.w3.org/2000/xforms\"\
    \n            grammar=\"session:request1@form-level.store\">\n              <interpretation>\n\
    \                  <xf:instance name=\"Person\">\n                      <Person>\n\
    \                          <Name> Andre Roy </Name>\n                      </Person>\n\
    \                  </xf:instance>\n                    <input>   may I speak to\
    \ Andre Roy </input>\n              </interpretation>\n          </result>\n"
- title: 8.10.  STOP
  contents:
  - "8.10.  STOP\n   The STOP method from the client to the server tells the resource\
    \ to\n   stop recognition if one is active.  If a RECOGNIZE request is active\n\
    \   and the STOP request successfully terminated it, then the response\n   header\
    \ contains an active-request-id-list header field containing the\n   request-id\
    \ of the RECOGNIZE request that was terminated.  In this\n   case, no RECOGNITION-COMPLETE\
    \ event will be sent for the terminated\n   request.  If there was no recognition\
    \ active, then the response MUST\n   NOT contain an active-request-id-list header\
    \ field.  Either\n   way,method the response MUST contain a status of 200(Success).\n\
    \   Example:\n     C->S:RECOGNIZE 543257 MRCP/1.0\n          Confidence-Threshold:90\n\
    \          Content-Type:application/grammar+xml\n          Content-Id:request1@form-level.store\n\
    \          Content-Length:104\n          <?xml version=\"1.0\"?>\n          <!--\
    \ the default grammar language is US English -->\n          <grammar xml:lang=\"\
    en-US\" version=\"1.0\">\n          <!-- single language attachment to tokens\
    \ -->\n          <rule id=\"yes\">\n                   <one-of>\n            \
    \                <item xml:lang=\"fr-CA\">oui</item>\n                       \
    \     <item xml:lang=\"en-US\">yes</item>\n                   </one-of>\n    \
    \           </rule>\n          <!-- single language attachment to a rule expansion\
    \ -->\n               <rule id=\"request\">\n                   may I speak to\n\
    \                   <one-of xml:lang=\"fr-CA\">\n                            <item>Michel\
    \ Tremblay</item>\n                            <item>Andre Roy</item>\n      \
    \             </one-of>\n               </rule>\n          </grammar>\n     S->C:MRCP/1.0\
    \ 543257 200 IN-PROGRESS\n     C->S:STOP 543258 200 MRCP/1.0\n     S->C:MRCP/1.0\
    \ 543258 200 COMPLETE\n          Active-Request-Id-List:543257\n"
- title: 8.11.  GET-RESULT
  contents:
  - "8.11.  GET-RESULT\n   The GET-RESULT method from the client to the server can\
    \ be issued\n   when the recognizer is in the recognized state.  This request\
    \ allows\n   the client to retrieve results for a completed recognition.  This\
    \ is\n   useful if the client decides it wants more alternatives or more\n   information.\
    \  When the media server receives this request, it should\n   re-compute and return\
    \ the results according to the recognition\n   constraints provided in the GET-RESULT\
    \ request.\n   The GET-RESULT request could specify constraints like a different\n\
    \   confidence-threshold, or n-best-list-length.  This feature is\n   optional\
    \ and the automatic speech recognition (ASR) engine may return\n   a status of\
    \ unsupported feature.\n   Example:\n     C->S:GET-RESULT 543257 MRCP/1.0\n  \
    \        Confidence-Threshold:90\n     S->C:MRCP/1.0 543257 200 COMPLETE\n   \
    \       Content-Type:application/x-nlsml\n          Content-Length:276\n     \
    \     <?xml version=\"1.0\"?>\n          <result x-model=\"http://IdentityModel\"\
    \n            xmlns:xf=\"http://www.w3.org/2000/xforms\"\n            grammar=\"\
    session:request1@form-level.store\">\n              <interpretation>\n       \
    \           <xf:instance name=\"Person\">\n                      <Person>\n  \
    \                        <Name> Andre Roy </Name>\n                      </Person>\n\
    \                  </xf:instance>\n                            <input>   may I\
    \ speak to Andre Roy </input>\n              </interpretation>\n          </result>\n"
- title: 8.12.  START-OF-SPEECH
  contents:
  - "8.12.  START-OF-SPEECH\n   This is an event from the recognizer to the client\
    \ indicating that it\n   has detected speech.  This event is useful in implementing\
    \ kill-on-\n   barge-in scenarios when the synthesizer resource is in a different\n\
    \   session than the recognizer resource and, hence, is not aware of an\n   incoming\
    \ audio source.  In these cases, it is up to the client to act\n   as a proxy\
    \ and turn around and issue the BARGE-IN-OCCURRED method to\n   the synthesizer\
    \ resource.  The recognizer resource also sends a\n   unique proxy-sync-id in\
    \ the header for this event, which is sent to\n   the synthesizer in the BARGE-IN-OCCURRED\
    \ method to the synthesizer.\n   This event should be generated irrespective of\
    \ whether the\n   synthesizer and recognizer are in the same media server or not.\n"
- title: 8.13.  RECOGNITION-START-TIMERS
  contents:
  - "8.13.  RECOGNITION-START-TIMERS\n   This request is sent from the client to the\
    \ recognition resource when\n   it knows that a kill-on-barge-in prompt has finished\
    \ playing.  This\n   is useful in the scenario when the recognition and synthesizer\n\
    \   engines are not in the same session.  Here, when a kill-on-barge-in\n   prompt\
    \ is being played, you want the RECOGNIZE request to be\n   simultaneously active\
    \ so that it can detect and implement kill-on-\n   barge-in.  But at the same\
    \ time, you don't want the recognizer to\n   start the no-input timers until the\
    \ prompt is finished.  The\n   parameter recognizer-start-timers header field\
    \ in the RECOGNIZE\n   request will allow the client to say if the timers should\
    \ be started\n   or not.  The recognizer should not start the timers until the\
    \ client\n   sends a RECOGNITION-START-TIMERS method to the recognizer.\n"
- title: 8.14.  RECOGNITON-COMPLETE
  contents:
  - "8.14.  RECOGNITON-COMPLETE\n   This is an Event from the recognizer resource\
    \ to the client\n   indicating that the recognition completed.  The recognition\
    \ result is\n   sent in the MRCP body of the message.  The request-state field\
    \ MUST\n   be COMPLETE indicating that this is the last event with that\n   request-id,\
    \ and that the request with that request-id is now\n   complete.  The recognizer\
    \ context still holds the results and the\n   audio waveform input of that recognition\
    \ until the next RECOGNIZE\n   request is issued.  A URL to the audio waveform\
    \ MAY BE returned to\n   the client in a waveform-url header field in the RECOGNITION-COMPLETE\n\
    \   event.  The client can use this URI to retrieve or playback the\n   audio.\n\
    \   Example:\n     C->S:RECOGNIZE 543257 MRCP/1.0\n          Confidence-Threshold:90\n\
    \          Content-Type:application/grammar+xml\n          Content-Id:request1@form-level.store\n\
    \          Content-Length:104\n          <?xml version=\"1.0\"?>\n          <!--\
    \ the default grammar language is US English -->\n          <grammar xml:lang=\"\
    en-US\" version=\"1.0\">\n          <!-- single language attachment to tokens\
    \ -->\n          <rule id=\"yes\">\n                   <one-of>\n            \
    \                <item xml:lang=\"fr-CA\">oui</item>\n                       \
    \     <item xml:lang=\"en-US\">yes</item>\n                   </one-of>\n    \
    \           </rule>\n          <!-- single language attachment to a rule expansion\
    \ -->\n               <rule id=\"request\">\n                   may I speak to\n\
    \                   <one-of xml:lang=\"fr-CA\">\n                            <item>Michel\
    \ Tremblay</item>\n                            <item>Andre Roy</item>\n      \
    \             </one-of>\n               </rule>\n          </grammar>\n     S->C:MRCP/1.0\
    \ 543257 200 IN-PROGRESS\n     S->C:START-OF-SPEECH 543257 IN-PROGRESS MRCP/1.0\n\
    \     S->C:RECOGNITION-COMPLETE 543257 COMPLETE MRCP/1.0\n          Completion-Cause:000\
    \ success\n          Waveform-URL:http://web.media.com/session123/audio.wav\n\
    \          Content-Type:application/x-nlsml\n          Content-Length:276\n  \
    \        <?xml version=\"1.0\"?>\n          <result x-model=\"http://IdentityModel\"\
    \n            xmlns:xf=\"http://www.w3.org/2000/xforms\"\n            grammar=\"\
    session:request1@form-level.store\">\n              <interpretation>\n       \
    \           <xf:instance name=\"Person\">\n                      <Person>\n  \
    \                        <Name> Andre Roy </Name>\n                      </Person>\n\
    \                  </xf:instance>\n                            <input>   may I\
    \ speak to Andre Roy </input>\n              </interpretation>\n          </result>\n"
- title: 8.15.  DTMF Detection
  contents:
  - "8.15.  DTMF Detection\n   Digits received as DTMF tones will be delivered to\
    \ the automatic\n   speech recognition (ASR) engine in the RTP stream according\
    \ to RFC\n   2833 [15].  The automatic speech recognizer (ASR) needs to support\n\
    \   RFC 2833 [15] to recognize digits.  If it does not support RFC 2833\n   [15],\
    \ it will have to process the audio stream and extract the audio\n   tones from\
    \ it.\n"
- title: 9.  Future Study
  contents:
  - "9.  Future Study\n   Various sections of the recognizer could be distributed\
    \ into Digital\n   Signal Processors (DSPs) on the Voice Browser/Gateway or IP\
    \ Phones.\n   For instance, the gateway might perform voice activity detection\
    \ to\n   reduce network bandwidth and CPU requirement of the automatic speech\n\
    \   recognition (ASR) server.  Such extensions are deferred for further\n   study\
    \ and will not be addressed in this document.\n"
- title: 10.  Security Considerations
  contents:
  - "10.  Security Considerations\n   The MRCP protocol may carry sensitive information\
    \ such as account\n   numbers, passwords, etc.  For this reason it is important\
    \ that the\n   client have the option of secure communication with the server\
    \ for\n   both the control messages as well as the media, though the client is\n\
    \   not required to use it.  If all MRCP communications happens in a\n   trusted\
    \ domain behind a firewall, this may not be necessary.  If the\n   client or server\
    \ is deployed in an insecure network, communication\n   happening across this\
    \ insecure network needs to be protected.  In\n   such cases, the following additional\
    \ security functionality MUST be\n   supported on the MRCP server.  MRCP servers\
    \ MUST implement Transport\n   Layer Security (TLS) to secure the RTSP communication,\
    \ i.e., the RTSP\n   stack SHOULD support the rtsps: URI form.  MRCP servers MUST\
    \ support\n   Secure Real-Time Transport Protocol (SRTP) as an option to send\
    \ and\n   receive media.\n"
- title: 11.  RTSP-Based Examples
  contents:
  - "11.  RTSP-Based Examples\n   The following is an example of a typical session\
    \ of speech synthesis\n   and recognition between a client and the server.\n \
    \  Opening the synthesizer.  This is the first resource for this\n   session.\
    \  The server and client agree on a single Session ID 12345678\n   and set of\
    \ RTP/RTCP ports on both sides.\n     C->S:SETUP rtsp://media.server.com/media/synthesizer\
    \ RTSP/1.0\n          CSeq:2\n          Transport:RTP/AVP;unicast;client_port=46456-46457\n\
    \          Content-Type:application/sdp\n          Content-Length:190\n      \
    \    v=0\n          o=- 123 456 IN IP4 10.0.0.1\n          s=Media Server\n  \
    \        p=+1-888-555-1212\n          c=IN IP4 0.0.0.0\n          t=0 0\n    \
    \      m=audio 0 RTP/AVP 0 96\n          a=rtpmap:0 pcmu/8000\n          a=rtpmap:96\
    \ telephone-event/8000\n          a=fmtp:96 0-15\n     S->C:RTSP/1.0 200 OK\n\
    \          CSeq:2\n          Transport:RTP/AVP;unicast;client_port=46456-46457;\n\
    \                    server_port=46460-46461\n          Session:12345678\n   \
    \       Content-Length:190\n          Content-Type:application/sdp\n         \
    \ v=0\n          o=- 3211724219 3211724219 IN IP4 10.3.2.88\n          s=Media\
    \ Server\n          c=IN IP4 0.0.0.0\n          t=0 0\n          m=audio 46460\
    \ RTP/AVP 0 96\n          a=rtpmap:0 pcmu/8000\n          a=rtpmap:96 telephone-event/8000\n\
    \          a=fmtp:96 0-15\n   Opening a recognizer resource.  Uses the existing\
    \ session ID and\n   ports.\n     C->S:SETUP rtsp://media.server.com/media/recognizer\
    \ RTSP/1.0\n          CSeq:3\n          Transport:RTP/AVP;unicast;client_port=46456-46457;\n\
    \                     mode=record;ttl=127\n          Session:12345678\n     S->C:RTSP/1.0\
    \ 200 OK\n          CSeq:3\n          Transport:RTP/AVP;unicast;client_port=46456-46457;\n\
    \                     server_port=46460-46461;mode=record;ttl=127\n          Session:12345678\n\
    \   An ANNOUNCE message with the MRCP SPEAK request initiates speech.\n     C->S:ANNOUNCE\
    \ rtsp://media.server.com/media/synthesizer RTSP/1.0\n          CSeq:4\n     \
    \     Session:12345678\n          Content-Type:application/mrcp\n          Content-Length:456\n\
    \          SPEAK 543257 MRCP/1.0\n          Kill-On-Barge-In:false\n         \
    \ Voice-gender:neutral\n          Voice-category:teenager\n          Prosody-volume:medium\n\
    \          Content-Type:application/synthesis+ssml\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <speak>\n          <paragraph>\n\
    \                   <sentence>You have 4 new messages.</sentence>\n          \
    \         <sentence>The first is from <say-as\n                   type=\"name\"\
    >Stephanie Williams</say-as> <mark\n          name=\"Stephanie\"/>\n         \
    \          and arrived at <break/>\n                   <say-as type=\"time\">3:45pm</say-as>.</sentence>\n\
    \                   <sentence>The subject is <prosody\n                   rate=\"\
    -20%\">ski trip</prosody></sentence>\n          </paragraph>\n          </speak>\n\
    \     S->C:RTSP/1.0 200 OK\n          CSeq:4\n          Session:12345678\n   \
    \       RTP-Info:url=rtsp://media.server.com/media/synthesizer;\n            \
    \         seq=9810092;rtptime=3450012\n          Content-Type:application/mrcp\n\
    \          Content-Length:456\n          MRCP/1.0 543257 200 IN-PROGRESS\n   The\
    \ synthesizer hits the special marker in the message to be spoken\n   and faithfully\
    \ informs the client of the event.\n     S->C:ANNOUNCE rtsp://media.server.com/media/synthesizer\
    \ RTSP/1.0\n          CSeq:5\n          Session:12345678\n          Content-Type:application/mrcp\n\
    \          Content-Length:123\n          SPEECH-MARKER 543257 IN-PROGRESS MRCP/1.0\n\
    \          Speech-Marker:Stephanie\n     C->S:RTSP/1.0 200 OK\n          CSeq:5\n\
    \   The synthesizer finishes with the SPEAK request.\n     S->C:ANNOUNCE rtsp://media.server.com/media/synthesizer\
    \ RTSP/1.0\n          CSeq:6\n          Session:12345678\n          Content-Type:application/mrcp\n\
    \          Content-Length:123\n          SPEAK-COMPLETE 543257 COMPLETE MRCP/1.0\n\
    \     C->S:RTSP/1.0 200 OK\n          CSeq:6\n   The recognizer is issued a request\
    \ to listen for the customer\n   choices.\n     C->S:ANNOUNCE rtsp://media.server.com/media/recognizer\
    \ RTSP/1.0\n          CSeq:7\n          Session:12345678\n          RECOGNIZE\
    \ 543258 MRCP/1.0\n          Content-Type:application/grammar+xml\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <!-- the default grammar language\
    \ is US English -->\n          <grammar xml:lang=\"en-US\" version=\"1.0\">\n\
    \          <!-- single language attachment to a rule expansion -->\n         \
    \      <rule id=\"request\">\n                   Can I speak to\n            \
    \       <one-of xml:lang=\"fr-CA\">\n                            <item>Michel\
    \ Tremblay</item>\n                            <item>Andre Roy</item>\n      \
    \             </one-of>\n               </rule>\n          </grammar>\n     S->C:RTSP/1.0\
    \ 200 OK\n          CSeq:7\n          Content-Type:application/mrcp\n        \
    \  Content-Length:123\n          MRCP/1.0 543258 200 IN-PROGRESS\n   The client\
    \ issues the next MRCP SPEAK method in an ANNOUNCE message,\n   asking the user\
    \ the question.  It is generally RECOMMENDED when\n   playing a prompt to the\
    \ user with kill-on-barge-in and asking for\n   input, that the client issue the\
    \ RECOGNIZE request ahead of the SPEAK\n   request for optimum performance and\
    \ user experience.  This way, it is\n   guaranteed that the recognizer is online\
    \ before the prompt starts\n   playing and the user's speech will not be truncated\
    \ at the beginning\n   (especially for power users).\n     C->S:ANNOUNCE rtsp://media.server.com/media/synthesizer\
    \ RTSP/1.0\n          CSeq:8 Session:12345678 Content-Type:application/mrcp\n\
    \          Content-Length:733\n          SPEAK 543259 MRCP/1.0\n          Kill-On-Barge-In:true\n\
    \          Content-Type:application/synthesis+ssml\n          Content-Length:104\n\
    \          <?xml version=\"1.0\"?>\n          <speak>\n          <paragraph>\n\
    \                   <sentence>Welcome to ABC corporation.</sentence>\n       \
    \            <sentence>Who would you like Talk to.</sentence>\n          </paragraph>\n\
    \          </speak>\n     S->C:RTSP/1.0 200 OK\n          CSeq:8\n          Content-Type:application/mrcp\n\
    \          Content-Length:123\n          MRCP/1.0 543259 200 IN-PROGRESS\n   Since\
    \ the last SPEAK request had Kill-On-Barge-In set to \"true\", the\n   message\
    \ synthesizer is interrupted when the user starts speaking, and\n   the client\
    \ is notified.\n   Now, since the recognition and synthesizer resources are in\
    \ the same\n   session, they worked with each other to deliver kill-on-barge-in.\
    \  If\n   the resources were in different sessions, it would have taken a few\n\
    \   more messages before the client got the SPEAK-COMPLETE event from the\n  \
    \ synthesizer resource.  Whether the synthesizer and recognizer are in\n   the\
    \ same session or not, the recognizer MUST generate the START-OF-\n   SPEECH event\
    \ to the client.\n   The client should have then blindly turned around and issued\
    \ a\n   BARGE-IN-OCCURRED method to the synthesizer resource.  The\n   synthesizer,\
    \ if kill-on-barge-in was enabled on the current SPEAK\n   request, would have\
    \ then interrupted it and issued SPEAK-COMPLETE\n   event to the client.  In this\
    \ example, since the synthesizer and\n   recognizer are in the same session, the\
    \ client did not issue the\n   BARGE-IN-OCCURRED method to the synthesizer and\
    \ assumed that kill-\n   on-barge-in was implemented between the two resources\
    \ in the same\n   session and worked.\n   The completion-cause code differentiates\
    \ if this is normal completion\n   or a kill-on-barge-in interruption.\n     S->C:ANNOUNCE\
    \ rtsp://media.server.com/media/recognizer RTSP/1.0\n          CSeq:9\n      \
    \    Session:12345678\n          Content-Type:application/mrcp\n          Content-Length:273\n\
    \          START-OF-SPEECH 543258 IN-PROGRESS MRCP/1.0\n     C->S:RTSP/1.0 200\
    \ OK\n          CSeq:9\n     S->C:ANNOUNCE rtsp://media.server.com/media/synthesizer\
    \ RTSP/1.0\n          CSeq:10\n          Session:12345678\n          Content-Type:application/mrcp\n\
    \          Content-Length:273\n          SPEAK-COMPLETE 543259 COMPLETE MRCP/1.0\n\
    \          Completion-Cause:000 normal\n     C->S:RTSP/1.0 200 OK\n          CSeq:10\n\
    \   The recognition resource matched the spoken stream to a grammar and\n   generated\
    \ results.  The result of the recognition is returned by the\n   server as part\
    \ of the RECOGNITION-COMPLETE event.\n     S->C:ANNOUNCE rtsp://media.server.com/media/recognizer\
    \ RTSP/1.0\n          CSeq:11\n          Session:12345678\n          Content-Type:application/mrcp\n\
    \          Content-Length:733\n          RECOGNITION-COMPLETE 543258 COMPLETE\
    \ MRCP/1.0\n          Completion-Cause:000 success\n          Waveform-URL:http://web.media.com/session123/audio.wav\n\
    \          Content-Type:application/x-nlsml\n          Content-Length:104\n  \
    \        <?xml version=\"1.0\"?>\n          <result x-model=\"http://IdentityModel\"\
    \n            xmlns:xf=\"http://www.w3.org/2000/xforms\"\n            grammar=\"\
    session:request1@form-level.store\">\n              <interpretation>\n       \
    \           <xf:instance name=\"Person\">\n                      <Person>\n  \
    \                        <Name> Andre Roy </Name>\n                      </Person>\n\
    \                  </xf:instance>\n                            <input>   may I\
    \ speak to Andre Roy </input>\n              </interpretation>\n          </result>\n\
    \     C->S:RTSP/1.0 200 OK\n          CSeq:11\n     C->S:TEARDOWN rtsp://media.server.com/media/synthesizer\
    \ RTSP/1.0\n          CSeq:12\n          Session:12345678\n     S->C:RTSP/1.0\
    \ 200 OK\n          CSeq:12\n   We are done with the resources and are tearing\
    \ them down.  When the\n   last of the resources for this session are released,\
    \ the Session-ID\n   and the RTP/RTCP ports are also released.\n     C->S:TEARDOWN\
    \ rtsp://media.server.com/media/recognizer RTSP/1.0\n          CSeq:13\n     \
    \     Session:12345678\n     S->C:RTSP/1.0 200 OK\n          CSeq:13\n"
- title: 12.  Informative References
  contents:
  - "12.  Informative References\n   [1]   Fielding, R., Gettys, J., Mogul, J., Frystyk.\
    \ H., Masinter, L.,\n         Leach, P., and T. Berners-Lee, \"Hypertext transfer\
    \ protocol --\n         HTTP/1.1\", RFC 2616, June 1999.\n   [2]   Schulzrinne,\
    \ H., Rao, A., and R. Lanphier, \"Real Time Streaming\n         Protocol (RTSP)\"\
    , RFC 2326, April 1998\n   [3]   Crocker, D. and P. Overell, \"Augmented BNF for\
    \ Syntax\n         Specifications: ABNF\", RFC 4234, October 2005.\n   [4]   Rosenberg,\
    \ J., Schulzrinne, H., Camarillo, G., Johnston, A.,\n         Peterson, J., Sparks,\
    \ R., Handley, M., and E. Schooler, \"SIP:\n         Session Initiation Protocol\"\
    , RFC 3261, June 2002.\n   [5]   Handley, M. and V. Jacobson, \"SDP: Session Description\n\
    \         Protocol\", RFC 2327, April 1998.\n   [6]   World Wide Web Consortium,\
    \ \"Voice Extensible Markup Language\n         (VoiceXML) Version 2.0\", W3C Candidate\
    \ Recommendation, March\n         2004.\n   [7]   Resnick, P., \"Internet Message\
    \ Format\", RFC 2822, April 2001.\n   [8]   Bradner, S., \"Key words for use in\
    \ RFCs to Indicate Requirement\n         Levels\", BCP 14, RFC 2119, March 1997.\n\
    \   [9]   World Wide Web Consortium, \"Speech Synthesis Markup Language\n    \
    \     (SSML) Version 1.0\", W3C Candidate Recommendation, September\n        \
    \ 2004.\n   [10]  World Wide Web Consortium, \"Natural Language Semantics Markup\n\
    \         Language (NLSML) for the Speech Interface Framework\", W3C\n       \
    \  Working Draft, 30 May 2001.\n   [11]  World Wide Web Consortium, \"Speech Recognition\
    \ Grammar\n         Specification Version 1.0\", W3C Candidate Recommendation,\
    \ March\n         2004.\n   [12]  Yergeau, F., \"UTF-8, a transformation format\
    \ of ISO 10646\", STD\n         63, RFC 3629, November 2003.\n   [13]  Freed,\
    \ N. and N. Borenstein, \"Multipurpose Internet Mail\n         Extensions (MIME)\
    \ Part Two: Media Types\", RFC 2046, November\n         1996.\n   [14]  Levinson,\
    \ E., \"Content-ID and Message-ID Uniform Resource\n         Locators\", RFC 2392,\
    \ August 1998.\n   [15]  Schulzrinne, H. and S. Petrack, \"RTP Payload for DTMF\
    \ Digits,\n         Telephony Tones and Telephony Signals\", RFC 2833, May 2000.\n\
    \   [16]  Alvestrand, H., \"Tags for the Identification of Languages\", BCP\n\
    \         47, RFC 3066, January 2001.\n"
- title: Appendix A.  ABNF Message Definitions
  contents:
  - "Appendix A.  ABNF Message Definitions\n   ALPHA          =  %x41-5A / %x61-7A\
    \   ; A-Z / a-z\n   CHAR           =  %x01-7F     ; any 7-bit US-ASCII character,\n\
    \                                 ;    excluding NUL\n   CR             =  %x0D\
    \        ; carriage return\n   CRLF           =  CR LF       ; Internet standard\
    \ newline\n   DIGIT          =  %x30-39     ; 0-9\n   DQUOTE         =  %x22 \
    \       ; \" (Double Quote)\n   HEXDIG         =  DIGIT / \"A\" / \"B\" / \"C\"\
    \ / \"D\" / \"E\" / \"F\"\n   HTAB           =  %x09        ; horizontal tab\n\
    \   LF             =  %x0A        ; linefeed\n   OCTET          =  %x00-FF   \
    \  ; 8 bits of data\n   SP             =  %x20        ; space\n   WSP        \
    \    =  SP / HTAB   ; white space\n   LWS            =  [*WSP CRLF] 1*WSP ; linear\
    \ whitespace\n   SWS            =  [LWS] ; sep whitespace\n   UTF8-NONASCII  =\
    \  %xC0-DF 1UTF8-CONT\n                  /  %xE0-EF 2UTF8-CONT\n             \
    \     /  %xF0-F7 3UTF8-CONT\n                  /  %xF8-Fb 4UTF8-CONT\n       \
    \           /  %xFC-FD 5UTF8-CONT\n   UTF8-CONT      =  %x80-BF\n   param    \
    \      =  *pchar\n   quoted-string  =  SWS DQUOTE *(qdtext / quoted-pair )\n \
    \                    DQUOTE\n   qdtext         =  LWS / %x21 / %x23-5B / %x5D-7E\n\
    \                     / UTF8-NONASCII\n   quoted-pair    =  \"\\\" (%x00-09 /\
    \ %x0B-0C\n                     / %x0E-7F)\n   token          =  1*(alphanum /\
    \ \"-\" / \".\" / \"!\" / \"%\" / \"*\"\n                      / \"_\" / \"+\"\
    \ / \"`\" / \"'\" / \"~\" )\n   reserved       =  \";\" / \"/\" / \"?\" / \":\"\
    \ / \"@\" / \"&\" / \"=\"\n                     / \"+\" / \"$\" / \",\"\n   mark\
    \           =  \"-\" / \"_\" / \".\" / \"!\" / \"~\" / \"*\" / \"'\"\n       \
    \              / \"(\" / \")\"\n   unreserved     =  alphanum / mark\n   char\
    \           =  unreserved / escaped /\n                     \":\" / \"@\" / \"\
    &\" / \"=\" / \"+\" / \"$\" / \",\"\n   alphanum       =  ALPHA / DIGIT\n   escaped\
    \        =  \"%\" HEXDIG HEXDIG\n   absoluteURI    =  scheme \":\" ( hier-part\
    \ / opaque-part )\n   relativeURI    =  ( net-path / abs-path / rel-path )\n \
    \                    [ \"?\" query ]\n   hier-part      =  ( net-path / abs-path\
    \ ) [ \"?\" query ]\n   net-path       =  \"//\" authority [ abs-path ]\n   abs-path\
    \       =  \"/\" path-segments\n   rel-path       =  rel-segment [ abs-path ]\n\
    \   rel-segment    =  1*( unreserved / escaped / \";\" / \"@\"\n             \
    \        / \"&\" / \"=\" / \"+\" / \"$\" / \",\" )\n   opaque-part    =  uric-no-slash\
    \ *uric\n   uric           =  reserved / unreserved / escaped\n   uric-no-slash\
    \  =  unreserved / escaped / \";\" / \"?\" / \":\"\n                     / \"\
    @\" / \"&\" / \"=\" / \"+\" / \"$\" / \",\"\n   path-segments  =  segment *( \"\
    /\" segment )\n   segment        =  *pchar *( \";\" param )\n   scheme       \
    \  =  ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" )\n   authority      =  srvr\
    \ / reg-name\n   srvr           =  [ [ userinfo \"@\" ] hostport ]\n   reg-name\
    \       =  1*( unreserved / escaped / \"$\" / \",\"\n                     / \"\
    ;\" / \":\" / \"@\" / \"&\" / \"=\" / \"+\" )\n   query          =  *uric\n  \
    \ userinfo       =  ( user ) [ \":\" password ] \"@\"\n   user           =  1*(\
    \ unreserved / escaped\n                       / user-unreserved )\n   user-unreserved\
    \  =  \"&\" / \"=\" / \"+\" / \"$\" / \",\" / \";\"\n                       /\
    \ \"?\" / \"/\"\n   password         =  *( unreserved / escaped /\n          \
    \             \"&\" / \"=\" / \"+\" / \"$\" / \",\" )\n   hostport         = \
    \ host [ \":\" port ]\n   host             =  hostname / IPv4address / IPv6reference\n\
    \   hostname         =  *( domainlabel \".\" ) toplabel [ \".\" ]\n   domainlabel\
    \      =  alphanum\n                       / alphanum *( alphanum / \"-\" ) alphanum\n\
    \   toplabel       =    ALPHA / ALPHA *( alphanum / \"-\" )\n                \
    \       alphanum\n   IPv4address    =    1*3DIGIT \".\" 1*3DIGIT \".\" 1*3DIGIT\
    \ \".\"\n                       1*3DIGIT\n   IPv6reference  =    \"[\" IPv6address\
    \ \"]\"\n   IPv6address    =    hexpart [ \":\" IPv4address ]\n   hexpart    \
    \    =    hexseq / hexseq \"::\" [ hexseq ] / \"::\"\n                       [\
    \ hexseq ]\n   hexseq         =    hex4 *( \":\" hex4)\n   hex4           =  \
    \  1*4HEXDIG\n   port           =    1*DIGIT\n   generic-message =   start-line\n\
    \                       message-header\n                       CRLF\n        \
    \               [ message-body ]\n   message-body   =    *OCTET\n   start-line\
    \     =    request-line / status-line / event-line\n   request-line   =    method-name\
    \ SP request-id SP\n                                 mrcp-version CRLF\n   status-line\
    \    =    mrcp-version SP request-id SP\n                       status-code SP\
    \ request-state CRLF\n   event-line     =    event-name SP request-id SP\n   \
    \                    request-state SP mrcp-version CRLF\n   message-header = \
    \   1*(generic-header / resource-header)\n   generic-header =    active-request-id-list\n\
    \                  /    proxy-sync-id\n                  /    content-id\n   \
    \               /    content-type\n                  /    content-length\n   \
    \               /    content-base\n                  /    content-location\n \
    \                 /    content-encoding\n                  /    cache-control\n\
    \                  /    logging-tag\n   ; -- content-id is as defined in RFC 2392\
    \ and RFC 2046\n   mrcp-version   =    \"MRCP\" \"/\" 1*DIGIT \".\" 1*DIGIT\n\
    \   request-id     =    1*DIGIT\n   status-code    =    1*DIGIT\n   active-request-id-list\
    \ =  \"Active-Request-Id-List\" \":\"\n                            request-id\
    \ *(\",\" request-id) CRLF\n   proxy-sync-id  =    \"Proxy-Sync-Id\" \":\" 1*ALPHA\
    \ CRLF\n   content-length =    \"Content-Length\" \":\" 1*DIGIT CRLF\n   content-base\
    \   =    \"Content-Base\" \":\" absoluteURI CRLF\n   content-type   =    \"Content-Type\"\
    \ \":\" media-type\n   media-type     =    type \"/\" subtype *( \";\" parameter\
    \ )\n   type           =    token\n   subtype        =    token\n   parameter\
    \      =    attribute \"=\" value\n   attribute      =    token\n   value    \
    \      =    token / quoted-string\n   content-encoding =  \"Content-Encoding\"\
    \ \":\"\n                       *WSP content-coding\n                       *(*WSP\
    \ \",\" *WSP content-coding *WSP )\n                       CRLF\n   content-coding\
    \   =  token\n   content-location =  \"Content-Location\" \":\"\n            \
    \           ( absoluteURI / relativeURI )  CRLF\n   cache-control  =    \"Cache-Control\"\
    \ \":\"\n                       *WSP cache-directive\n                       *(\
    \ *WSP \",\" *WSP cache-directive *WSP )\n                       CRLF\n   cache-directive\
    \ =   \"max-age\" \"=\" delta-seconds\n                   /   \"max-stale\" \"\
    =\" delta-seconds\n                   /   \"min-fresh\" \"=\" delta-seconds\n\
    \   logging-tag    =    \"Logging-Tag\" \":\" 1*ALPHA CRLF\n   resource-header\
    \ =   recognizer-header\n                       /    synthesizer-header\n   method-name\
    \    =    synthesizer-method\n                       /    recognizer-method\n\
    \   event-name     =    synthesizer-event\n                       /    recognizer-event\n\
    \   request-state  =    \"COMPLETE\"\n                  /    \"IN-PROGRESS\"\n\
    \                  /    \"PENDING\"\n   synthesizer-method = \"SET-PARAMS\"\n\
    \                  /    \"GET-PARAMS\"\n                  /    \"SPEAK\"\n   \
    \               /    \"STOP\"\n                  /    \"PAUSE\"\n            \
    \      /    \"RESUME\"\n                  /    \"BARGE-IN-OCCURRED\"\n       \
    \           /    \"CONTROL\"\n   synthesizer-event = \"SPEECH-MARKER\"\n     \
    \             /    \"SPEAK-COMPLETE\"\n   synthesizer-header =     jump-target\n\
    \                      /     kill-on-barge-in\n                      /     speaker-profile\n\
    \                      /     completion-cause\n                      /     voice-parameter\n\
    \                      /     prosody-parameter\n                      /     vendor-specific\n\
    \                      /     speech-marker\n                      /     speech-language\n\
    \                      /     fetch-hint\n                      /     audio-fetch-hint\n\
    \                      /     fetch-timeout\n                      /     failed-uri\n\
    \                      /     failed-uri-cause\n                      /     speak-restart\n\
    \                      /     speak-length\n   recognizer-method = \"SET-PARAMS\"\
    \n                      /    \"GET-PARAMS\"\n                      /    \"DEFINE-GRAMMAR\"\
    \n                      /    \"RECOGNIZE\"\n                      /    \"GET-RESULT\"\
    \n                      /    \"RECOGNITION-START-TIMERS\"\n                  \
    \    /    \"STOP\"\n   recognizer-event  =      \"START-OF-SPEECH\"\n        \
    \             /      \"RECOGNITION-COMPLETE\"\n   recognizer-header =      confidence-threshold\n\
    \                     /      sensitivity-level\n                     /      speed-vs-accuracy\n\
    \                     /      n-best-list-length\n                     /      no-input-timeout\n\
    \                     /      recognition-timeout\n                     /     \
    \ waveform-url\n                     /      completion-cause\n               \
    \      /      recognizer-context-block\n                     /      recognizer-start-timers\n\
    \                     /      vendor-specific\n                     /      speech-complete-timeout\n\
    \                     /      speech-incomplete-timeout\n                     /\
    \      dtmf-interdigit-timeout\n                     /      dtmf-term-timeout\n\
    \                     /      dtmf-term-char\n                     /      fetch-timeout\n\
    \                     /      failed-uri\n                     /      failed-uri-cause\n\
    \                     /      save-waveform\n                     /      new-audio-channel\n\
    \                     /      speech-language\n   jump-target       =  \"Jump-Size\"\
    \ \":\" speech-length-value CRLF\n   speech-length-value =    numeric-speech-length\n\
    \                     /      text-speech-length\n   text-speech-length =     1*ALPHA\
    \ SP \"Tag\"\n   numeric-speech-length =(\"+\" / \"-\") 1*DIGIT SP\n         \
    \              numeric-speech-unit\n   numeric-speech-unit =    \"Second\"\n \
    \                      /    \"Word\"\n                       /    \"Sentence\"\
    \n                       /    \"Paragraph\"\n   delta-seconds  =    1*DIGIT\n\
    \   kill-on-barge-in =  \"Kill-On-Barge-In\" \":\" boolean-value CRLF\n   boolean-value\
    \  =    \"true\" / \"false\"\n   speaker-profile =    \"Speaker-Profile\" \":\"\
    \ absoluteURI CRLF\n   completion-cause =  \"Completion-Cause\" \":\" 1*DIGIT\
    \ SP\n                       1*ALPHA CRLF\n   voice-parameter =   \"Voice-\" voice-param-name\
    \ \":\"\n                       voice-param-value CRLF\n   voice-param-name =\
    \  1*ALPHA\n   voice-param-value = 1*alphanum\n   prosody-parameter = \"Prosody-\"\
    \ prosody-param-name \":\"\n                        prosody-param-value CRLF\n\
    \   prosody-param-name =     1*ALPHA\n   prosody-param-value = 1*alphanum\n  \
    \ vendor-specific =   \"Vendor-Specific-Parameters\" \":\"\n                 \
    \     vendor-specific-av-pair\n                       *[\";\" vendor-specific-av-pair]\
    \ CRLF\n   vendor-specific-av-pair = vendor-av-pair-name \"=\"\n             \
    \                vendor-av-pair-value\n   vendor-av-pair-name = 1*ALPHA\n   vendor-av-pair-value\
    \ = 1*alphanum\n   speech-marker  =    \"Speech-Marker\" \":\" 1*ALPHA CRLF\n\
    \   speech-language =   \"Speech-Language\" \":\" 1*ALPHA CRLF\n   fetch-hint\
    \     =    \"Fetch-Hint\" \":\" 1*ALPHA CRLF\n   audio-fetch-hint =  \"Audio-Fetch-Hint\"\
    \ \":\" 1*ALPHA CRLF\n   fetch-timeout  =    \"Fetch-Timeout\" \":\" 1*DIGIT CRLF\n\
    \   failed-uri     =    \"Failed-URI\" \":\" absoluteURI CRLF\n   failed-uri-cause\
    \ =  \"Failed-URI-Cause\" \":\" 1*ALPHA CRLF\n   speak-restart  =    \"Speak-Restart\"\
    \ \":\" boolean-value CRLF\n   speak-length   =    \"Speak-Length\" \":\" speech-length-value\n\
    \                       CRLF\n   confidence-threshold =   \"Confidence-Threshold\"\
    \ \":\"\n                            1*DIGIT CRLF\n   sensitivity-level = \"Sensitivity-Level\"\
    \ \":\" 1*DIGIT CRLF\n   speed-vs-accuracy = \"Speed-Vs-Accuracy\" \":\" 1*DIGIT\
    \ CRLF\n   n-best-list-length = \"N-Best-List-Length\" \":\" 1*DIGIT CRLF\n  \
    \ no-input-timeout =  \"No-Input-Timeout\" \":\" 1*DIGIT CRLF\n   recognition-timeout\
    \ = \"Recognition-Timeout\" \":\" 1*DIGIT CRLF\n   waveform-url   =    \"Waveform-URL\"\
    \ \":\" absoluteURI CRLF\n   recognizer-context-block = \"Recognizer-Context-Block\"\
    \ \":\"\n                       1*ALPHA CRLF\n   recognizer-start-timers = \"\
    Recognizer-Start-Timers\" \":\"\n                       boolean-value CRLF\n \
    \  speech-complete-timeout = \"Speech-Complete-Timeout\" \":\"\n             \
    \          1*DIGIT CRLF\n   speech-incomplete-timeout = \"Speech-Incomplete-Timeout\"\
    \ \":\"\n                       1*DIGIT CRLF\n   dtmf-interdigit-timeout = \"\
    DTMF-Interdigit-Timeout\" \":\"\n                             1*DIGIT CRLF\n \
    \  dtmf-term-timeout = \"DTMF-Term-Timeout\" \":\" 1*DIGIT CRLF\n   dtmf-term-char\
    \ =    \"DTMF-Term-Char\" \":\" CHAR CRLF\n   save-waveform  =    \"Save-Waveform\"\
    \ \":\" boolean-value CRLF\n   new-audio-channel = \"New-Audio-Channel\" \":\"\
    \n                       boolean-value CRLF\n"
- title: Appendix B.  Acknowledgements
  contents:
  - "Appendix B.  Acknowledgements\n   Andre Gillet (Nuance Communications)\n   Andrew\
    \ Hunt (SpeechWorks)\n   Aaron Kneiss (SpeechWorks)\n   Kristian Finlator (SpeechWorks)\n\
    \   Martin Dragomirecky (Cisco Systems, Inc.)\n   Pierre Forgues (Nuance Communications)\n\
    \   Suresh Kaliannan (Cisco Systems, Inc.)\n   Corey Stohs (Cisco Systems, Inc.)\n\
    \   Dan Burnett (Nuance Communications)\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Saravanan Shanmugham\n   Cisco Systems, Inc.\n   170 W.\
    \ Tasman Drive\n   San Jose, CA 95134\n   EMail: sarvi@cisco.com\n   Peter Monaco\n\
    \   Nuasis Corporation\n   303 Bryant St.\n   Mountain View, CA 94041\n   EMail:\
    \ peter.monaco@nuasis.com\n   Brian Eberman\n   Speechworks, Inc.\n   695 Atlantic\
    \ Avenue\n   Boston, MA 02111\n   EMail: brian.eberman@speechworks.com\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The Internet Society (2006).\n   This\
    \ document is subject to the rights, licenses and restrictions\n   contained in\
    \ BCP 78 and at www.rfc-editor.org/copyright.html, and\n   except as set forth\
    \ therein, the authors retain all their rights.\n   This document and the information\
    \ contained herein are provided on an\n   \"AS IS\" basis and THE CONTRIBUTOR,\
    \ THE ORGANIZATION HE/SHE REPRESENTS\n   OR IS SPONSORED BY (IF ANY), THE INTERNET\
    \ SOCIETY AND THE INTERNET\n   ENGINEERING TASK FORCE DISCLAIM ALL WARRANTIES,\
    \ EXPRESS OR IMPLIED,\n   INCLUDING BUT NOT LIMITED TO ANY WARRANTY THAT THE USE\
    \ OF THE\n   INFORMATION HEREIN WILL NOT INFRINGE ANY RIGHTS OR ANY IMPLIED\n\
    \   WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.\n"
- title: Intellectual Property
  contents:
  - "Intellectual Property\n   The IETF takes no position regarding the validity or\
    \ scope of any\n   Intellectual Property Rights or other rights that might be\
    \ claimed to\n   pertain to the implementation or use of the technology described\
    \ in\n   this document or the extent to which any license under such rights\n\
    \   might or might not be available; nor does it represent that it has\n   made\
    \ any independent effort to identify any such rights.  Information\n   on the\
    \ procedures with respect to rights in RFC documents can be\n   found in BCP 78\
    \ and BCP 79.\n   Copies of IPR disclosures made to the IETF Secretariat and any\n\
    \   assurances of licenses to be made available, or the result of an\n   attempt\
    \ made to obtain a general license or permission for the use of\n   such proprietary\
    \ rights by implementers or users of this\n   specification can be obtained from\
    \ the IETF on-line IPR repository at\n   http://www.ietf.org/ipr.\n   The IETF\
    \ invites any interested party to bring to its attention any\n   copyrights, patents\
    \ or patent applications, or other proprietary\n   rights that may cover technology\
    \ that may be required to implement\n   this standard.  Please address the information\
    \ to the IETF at\n   ietf-ipr@ietf.org.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is provided by the IETF\n\
    \   Administrative Support Activity (IASA).\n"
