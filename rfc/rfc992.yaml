- title: __initial_text__
  contents:
  - "       On Communication Support for Fault Tolerant Process Groups\n         \
    \            K. P. Birman and T. A. Joseph\n             Dept. of Computer Science,\
    \ Cornell University\n                           Ithaca, N.Y. 14853\n        \
    \                      607-255-9199\n"
- title: 1. Status of this Memo.
  contents:
  - "1. Status of this Memo.\n   This memo describes a collection of multicast communication\
    \ primi-\n   tives integrated with a mechanism for handling process failure and\n\
    \   recovery.  These primitives facilitate the implementation of fault-\n   tolerant\
    \ process groups, which can be used to provide distributed\n   services in an\
    \ environment subject to non-malicious crash failures.\n   Unlike other process\
    \ group approaches, such as Cheriton's \"host\n   groups\" (RFC's 966, 988, [Cheriton]),\
    \ our approach provides powerful\n   guarantees about the behavior of the communication\
    \ subsystem when\n   process group membership is changing dynamically, for example\
    \ due to\n   process or site failures, recoveries, or migration of a process from\n\
    \   one site to another.  Our approach also addresses delivery ordering\n   issues\
    \ that arise when multiple clients communicate with a process\n   group concurrently,\
    \ or a single client transmits multiple multicast\n   messages to a group without\
    \ pausing to wait until each is received.\n   Moreover, the cost of the approach\
    \ is low.  An implementation is be-\n   ing undertaken at Cornell as part of the\
    \ ISIS project.\n   Here, we argue that the form of \"best effort\" reliability\
    \ provided by\n   host groups may not address the requirements of those researchers\
    \ who\n   are building fault tolerant software.  Our basic premise is that re-\n\
    \   liable handling of failures, recoveries, and dynamic process migra-\n   tion\
    \ are important aspects of programming in distributed environ-\n   ments, and\
    \ that communication support that provides unpredictable\n   behavior in the presence\
    \ of such events places an unacceptable burden\n   of complexity on higher level\
    \ application software.  This complexity\n   does not arise when using the fault-tolerant\
    \ process group alterna-\n   tive.\n   This memo summarizes our approach and briefly\
    \ contrasts it with other\n   process group approaches.  For a detailed discussion,\
    \ together with\n   figures that clarify the details of the approach, readers\
    \ are re-\n   ferred to the papers cited below.\n   Distribution of this memo\
    \ is unlimited.\n"
- title: 2. Acknowledgments
  contents:
  - "2. Acknowledgments\n   This memo was adopted from a paper presented at the Asilomar\
    \ workshop\n   on fault-tolerant distributed computing, March 1986, and summarizes\n\
    \   material from a technical report that was issued by Cornell Universi-\n  \
    \ ty, Dept. of Computer Science, in August 1985, which will appear in\n   ACM\
    \ Transactions on Computer Systems in February 1987 [Birman-b].\n   Copies of\
    \ these paper, and other relevant papers, are available on\n   request from the\
    \ author: Dept. of Computer Science, Cornell Universi-\n   ty, Ithaca, New York\
    \ 14853. (birman@gvax.cs.cornell.edu).  The ISIS\n   project also maintains a\
    \ mailing list.  To be added to this list,\n   contact M. Schmizzi (schiz@gvax.cs.cornell.edu).\n\
    \   This work was supported by the Defense Advanced Research Projects\n   Agency\
    \ (DoD) under ARPA order 5378, Contract MDA903-85-C-0124, and by\n   the National\
    \ Science Foundation under grant DCR-8412582.  The views,\n   opinions and findings\
    \ contained in this report are those of the au-\n   thors and should not be construed\
    \ as an official Department of De-\n   fense position, policy, or decision.\n"
- title: 3. Introduction
  contents:
  - "3. Introduction\n   At Cornell, we recently completed a prototype of the ISIS\
    \ system,\n   which transforms abstract type specifications into fault-tolerant\n\
    \   distributed implementations, while insulating users from the mechan-\n   isms\
    \ by which fault-tolerance is achieved.  This version of ISIS, re-\n   ported\
    \ in [Birman-a], supports transactional resilient objects as a\n   basic programming\
    \ abstraction.  Our current work undertakes to pro-\n   vide a much broader range\
    \ of fault-tolerant programming mechanisms,\n   including fault-tolerant distributed\
    \ bulletin boards [Birman-c] and\n   fault-tolerant remote procedure calls on\
    \ process groups [Birman-b].\n   The approach to communication that we report\
    \ here arose as part of\n   this new version of the ISIS system.\n   Unreliable\
    \ communication primitives, such as the multicast group com-\n   munication primitives\
    \ proposed in RFC's 966 and 988 and in [Cheri-\n   ton], leave some uncertainty\
    \ in the delivery status of a message when\n   failures and other exceptional\
    \ events occur during communication.\n   Instead, a form of \"best effort\" delivery\
    \ is provided, but with the\n   possibility that some member of a group of processes\
    \ did not receive\n   the message if the group membership was changing just as\
    \ communica-\n   tion took place.  When we tried to use this sort of primitive\
    \ in our\n   original work on ISIS, which must behave reliably in the presence\
    \ of\n   such events, we had to address this aspect at an application level.\n\
    \   The resulting software was complex, difficult to reason about, and\n   filled\
    \ with obscure bugs, and we were eventually forced to abandon\n   the entire approach\
    \ as infeasible.\n   A wide range of reliable communication primitives have been\
    \ proposed\n   in the literature, and we became convinced that by using them,\
    \ the\n   complexity of our software could be greatly reduced.  These range\n\
    \   from reliable and atomic broadcast [Chang] [Cristian] [Schneider] to\n   Byzantine\
    \ agreement [Strong].  For several reasons, however, the ex-\n   isting work does\
    \ not solve the problem at hand.  The most obvious is\n   that they do not provide\
    \ a mechanism for sending a message to all the\n   members of a group when the\
    \ membership is changing dynamically (the\n   \"group addressing\" problem). \
    \ In addition, one can identify delivery\n   ordering issues and questions regarding\
    \ the detection of communica-\n   tion failures that should be handled within\
    \ the broadcast mechanism.\n   These motivate a careful reexamination of the entire\
    \ reliable broad-\n   cast problem.\n   The multicast primitives we report here\
    \ are designed to respect\n   several sorts of ordering constraints, and have\
    \ cost and latency that\n   varies depending on the nature of the constraint required\
    \ [Birman-b]\n   [Joseph-a] [Joseph-b].  Failure and recovery are integrated into\
    \ the\n   communication subsystem by treating these events as a special sort of\n\
    \   multicast issued on behalf of a process that has failed or recovered.\n  \
    \ The primitives are presented in the context of fault tolerant process\n   groups:\
    \ groups of processes that cooperate to implement some distri-\n   buted algorithm\
    \ or service, and which need to see consistent order-\n   ings of system events\
    \ in order to achieve mutually consistent\n   behavior.  Such groups are similar\
    \ to the host groups of the V system\n   and the ones described in RFC's 966 and\
    \ 988, but provide guarantees\n   of consistency in just the situations where\
    \ a host group provides a\n   \"best effort\" delivery which may sometimes be\
    \ erroneous.\n   It is helpful to think of our primitives as providing a logical\
    \ or\n   \"virtual\" form of reliability: rather than addressing physical\n  \
    \ delivery issues, they ensure that a client will never observe a sys-\n   tem\
    \ state \"inconsistent\" with the assumption that reliable delivery\n   has occurred.\
    \  Readers familiar with serializability theory may want\n   to think of this\
    \ as a weaker analog: in serializability, one allows\n   interleaved executions\
    \ of operations provided that the resulting sys-\n   tem state is consistent with\
    \ the assumption that execution was\n   sequential.  Similarly, reliable communication\
    \ primitives permit de-\n   viations from the reliable delivery abstraction provided\
    \ that the\n   resulting system state is indistinguishable from one in which reli-\n\
    \   able delivery actually did occur.\n   Using our primitives, the ISIS system\
    \ achieved both high levels of\n   concurrency and suprisingly good performance.\
    \  Equally important, its\n   structure was made suprisingly simple, making it\
    \ feasible to reason\n   about the correctness of the algorithms that are needed\
    \ to maintain\n   high availability even when failures, recoveries, or process\
    \ migra-\n   tion occurs.  More recently, we have applied the same approach to\
    \ a\n   variety of other problems in distributed computing, and even designed\n\
    \   a consistent, fault tolerant, distributed bulletin board data struc-\n   ture\
    \ (a generalized version of the blackboards used in artificial in-\n   telligence\
    \ programs), with equally good results [Birman-c].  Thus, we\n   feel that the\
    \ approach has been shown to work in a variety of set-\n   tings where unreliable\
    \ primitives simply could not be used.\n   In the remainder of this memo we summarize\
    \ the issues and alterna-\n   tives that the designer of a distributed system\
    \ is presented with,\n   focusing on two styles of support for fault-tolerant\
    \ computing: re-\n   mote procedure calls coupled with a transactional execution\
    \ facility,\n   such as is used in the ARGUS system [Liskov], and the fault-tolerant\n\
    \   process group mechanism mentioned above.  We argue that transactional\n  \
    \ interactions are too restrictive to support the sort of mechanism\n   needed,\
    \ and then show how our primitives can be used to provide such\n   a mechanism.\
    \  We conclude by speculating on future directions in\n   which this work might\
    \ be taken.\n"
- title: 4. Issues in fault-tolerance
  contents:
  - "4. Issues in fault-tolerance\n   The difficulty of constructing fault-tolerant\
    \ distributed software\n   can be traced to a number of interrelated issues. \
    \ The list that fol-\n   lows is not exhaustive, but attempts to touch on the\
    \ principal con-\n   siderations that must be addressed in any such system:\n\
    \      [1]Synchronization.  Distributed systems offer the potential for\n    \
    \  large amounts of concurrency, and it is usually desirable to\n      operate\
    \ at as high a level of concurrency as possible.  However,\n      when we move\
    \ from a sequential execution environment to a con-\n      current one, it becomes\
    \ necessary to synchronize actions that may\n      conflict in their access to\
    \ shared data or entail communication\n      with overlapping sets of processes.\
    \  Thus, a mechanism is needed\n      for ordering conflicting events.  Additional\
    \ problems that can\n      arise in this context include deadlock avoidance or\
    \ detection,\n      livelock avoidance, etc.\n      [2]Failure detection.  It\
    \ is usually necessary for a fault-\n      tolerant application to have a consistent\
    \ picture of which com-\n      ponents fail, and in what order. Timeout, the most\
    \ common mechan-\n      ism for detecting failure, is unsatisfactory, because\
    \ there are\n      many situations in which a healthy component can timeout with\n\
    \      respect to one component without this being detected by some\n      another.\
    \  Failure detection under more rigorous requirements\n      requires an agreement\
    \ protocol that is related to Byzantine agree-\n      ment [Strong] [Hadzilacos].\
    \  Regardless of how this problem is\n      solved, some sort of reliable failure\
    \ detection mechanism will be\n      needed in any fault-tolerant distributed\
    \ system.\n      [3] Consistency.  When a group of processes cooperate in a distri-\n\
    \      buted system, it is necessary to ensure that the operational\n      processes\
    \ have consistent views of the state of the group as a\n      whole.  For example,\
    \ if process p believes that some property A\n      holds, and on the basis of\
    \ this interacts with process q, the\n      state of q should not contradict the\
    \ fact that p believes A to be\n      true.  This problem is closely related to\
    \ notions of knowledge and\n      consistency in distributed systems [Halpern]\
    \ [Lamport].  In our\n      context, A will often be the assertion that a multicast\
    \ has been\n      received by q, or that q saw some sequence of events occur in\
    \ the\n      same order as did p.  Thus, it is necessary to be able to specify\n\
    \      the precise consistency constraints on a distributed software sys-\n  \
    \    tem, and system support should be available to facilitate the\n      attainment\
    \ of these constraints.\n      [4] Serializability.  Many distributed systems\
    \ are partitioned\n      into data manager processes, which implement shared variables,\
    \ and\n      transaction manager processes, which issue requests to data\n   \
    \   managers [Bernstein].  If transaction managers can execute con-\n      currently,\
    \ it is desirable to ensure that transactions produce\n      serializable outcomes\
    \ [Eswaren] [Papadimitrou].  Serializability\n      is increasingly viewed as\
    \ an important property in \"object-\n      oriented\" distributed systems that\
    \ package services as abstract\n      objects with which clients communicate by\
    \ remote procedure calls\n      (RPC).  On the other hand, there are systems for\
    \ which serializa-\n      bility is either too strong a constraint, or simply\
    \ inappropriate.\n      Thus, one needs a way to achieve serializability in applications\n\
    \      where it will be needed, without imposing system-wide restrictions\n  \
    \    that would prevent the design of software subsystems for which\n      serializability\
    \ is not needed.\n   Jointly, these problems render the design of fault-tolerant\
    \ distri-\n   buted software daunting in the absence of adequate support.  The\n\
    \   correctness of any proposed design and of its implementation become\n   serious,\
    \ if not insurmountable, concerns.  In Sec. 7, we will show\n   how the primitives\
    \ of Sec. 6 provide simple ways to overcome all of\n   these issues.\n"
- title: 5. Existing alternatives
  contents:
  - "5. Existing alternatives\n   If one rules out \"unreliable\" communication mechanisms,\
    \ there are\n   basically two fault-tolerant alternatives that can be pursued.\n\
    \   The first approach is to provide mechanisms for transactional\n   interactions\
    \ between processes that communicate using remote pro-\n   cedure calls [Birrell].\
    \  This has lead to work on nested transactions\n   (due to nested RPC's) [Moss],\
    \ support for transactions at the\n   language level [Liskov], transactions within\
    \ an operating systems\n   kernel [Spector] [Allchin] [Popek] [Lazowska], and\
    \ transactional\n   access to higher-level replicated services, such as resilient\
    \ objects\n   in ISIS or relations in database systems.  The primitives in a tran-\n\
    \   sactional system provide mechanisms for distributing the request that\n  \
    \ initiates the transaction, accessing data (which may be replicated),\n   performing\
    \ concurrency control, and implementing commit or abort.\n   Additional mechanisms\
    \ are normally needed for orphan termination,\n   deadlock detection, etc.  The\
    \ issue then arises of how these mechan-\n   isms should themselves be implemented.\n\
    \   Our work in ISIS leads us to believe that whereas transactions are\n   easily\
    \ implemented on top of fault-tolerant process groups -- we have\n   done so --\
    \ the converse is much harder.  Moreover, transactions\n   represent a relatively\
    \ heavy-weight solution to the problems surveyed\n   in the previous section,\
    \ and might impose an unacceptable overhead on\n   subsystems that need to run\
    \ non-transactionally, for example because\n   a pair of concurrent processes\
    \ needs to interact on a frequent basis.\n   (We are not saying that \"transactional\"\
    \ mechanisms such as cobegins\n   and toplevel actions can't solve this problem,\
    \ but just that they\n   yield a solution that is awkward and costly).  This sort\
    \ of reasoning\n   has lead us to focus on non-transactional interaction mechanisms,\
    \ and\n   to treat transactions as a special class of mechanisms used when\n \
    \  processes that have been designed to employ a transactional protocol\n   interact.\n\
    \   The second approach involves the provision of a communication primi-\n   tive,\
    \ such as atomic broadcast, which can be used as the framework on\n   which higher\
    \ level algorithms are designed.  Such a primitive seeks\n   to deliver messages\
    \ reliably to some set of destinations, despite the\n   possibility that failures\
    \ might occur during the execution of the\n   protocol.  Above, we termed this\
    \ the fault tolerant process group\n   approach, since it lends itself to the\
    \ organization of cooperating\n   processes into groups, as described in the introduction.\
    \  Process\n   groups are an extremely flexible abstraction, and have been employed\n\
    \   in the V Kernel [Cheriton] and in UNIX, and more recently in the ISIS\n  \
    \ system.  A proposal to provide Internet support for host groups was\n   raised\
    \ in RFC's 966 and 988.  However, the idea of adapting the pro-\n   cess group\
    \ approach to work reliably in an environment subject to the\n   sorts of exception\
    \ events and concurrency cited in the previous sec-\n   tion seems to be new.\n\
    \   As noted earlier, existing reliable communication protocols do not\n   address\
    \ the requirements of fault-tolerant process groups.  For exam-\n   ple, in [Schneider],\
    \ an implementation of a reliable multicast primi-\n   tive is described.  Such\
    \ a primitive ensures that a designated mes-\n   sage will be transmitted from\
    \ one site to all other operational sites\n   in a system; if a failure occurs\
    \ but any site has received the mes-\n   sage, all will eventually do so.  [Chang]\
    \ and [Cristian] describe\n   implementations for atomic broadcast, which is a\
    \ reliable broadcast\n   (sent to all sites in a system) with the additional property\
    \ that\n   messages are delivered in the same order at all overlapping destina-\n\
    \   tions, and this order preserves the transmission order if messages\n   originate\
    \ in a single site.\n   Atomic broadcast is a powerful abstraction, and essentially\
    \ the same\n   behavior is provided by one of the multicast primitives we discuss\
    \ in\n   the next section.  However, it has several drawbacks which made us\n\
    \   hesitant to adopt it as the only primitive in the system.  Most seri-\n  \
    \ ous is the latency that is incurred in order to satisfy the delivery\n   ordering\
    \ property.  Without delving deeply into the implementations,\n   which are based\
    \ on a token scheme in [Chang] and an acknowledgement\n   protocol in [Schneider],\
    \ we observe that the delaying of certain mes-\n   sages is fundamental to the\
    \ establishment of a unique global delivery\n   ordering; indeed, it is easy to\
    \ prove on knowledge theoretic grounds\n   that this must always be the case.\
    \  In [Chang] a primary goal is to\n   minimize the number of messages sent, and\
    \ the protocol given performs\n   extremely well in this regard.  However, a delay\
    \ occurs while waiting\n   for tokens to arrive and the delivery latency that\
    \ results may be\n   high.  [Cristian] assumes that clocks are closely synchronized\
    \ and\n   that message transit times are bounded by well-known constants, and\n\
    \   uses this to derive atomic broadcast protocols tolerant of increas-\n   ingly\
    \ severe classes of failures.  The protocols explicitly delay\n   delivery to\
    \ achieve the desired global ordering on multicasts.  For\n   reasons discussed\
    \ below, this tends to result in high latency in typ-\n   ical local networking\
    \ environments.  An additional drawback of the\n   atomic broadcast protocols\
    \ is that no mechanism is provided for\n   ensuring that all processes observe\
    \ the same sequence of failures and\n   recoveries, or for ensuring that failures\
    \ and recoveries are ordered\n   relative to ongoing multicasts.  Since this problem\
    \ arises in any\n   setting where one process monitors another, we felt it should\
    \ be\n   addressed at the same level as the communication protocol.  Finally,\n\
    \   one wants a group oriented multicast protocol, not a site oriented\n   broadcast,\
    \ and this issue must be resolved too.\n"
- title: 6. Our multicast primitives
  contents:
  - "6. Our multicast primitives\n   We now describe three multicast protocols - GBCAST,\
    \ ABCAST, and\n   CBCAST - for transmitting a message reliably from a sender process\
    \ to\n   some set of destination processes.  Details of the protocols and\n  \
    \ their correctness proofs can be found in [Birman-b].  The protocols\n   ensure\
    \ \"all or nothing\" behavior: if any destination receives a mes-\n   sage, then\
    \ unless it fails, all destinations will receive it.  Group\n   addressing is\
    \ discussed in Sec. 6.5.\n   The failure model that one adopts has a considerable\
    \ impact on the\n   structure of the resulting system.  We adopted the model of\
    \ fail-stop\n   processors [Schneider]: when failures occur, a processor simply\
    \ stops\n   (crashes), as do all the processes executing on it.  We also assume\n\
    \   that individual processes can crash, and that this is detected when\n   it\
    \ occurs by a monitoring mechanism present at each site.  Further\n   assumptions\
    \ are sometimes made about the availability of synchronized\n   realtime clocks.\
    \  Here, we adopt the position that although reason-\n   ably accurate elapsed-time\
    \ clocks may be available, closely synchron-\n   ized clocks probably will not\
    \ be.  For example, the 60Hz \"line\"\n   clocks commonly used on current workstations\
    \ are only accurate to\n   16ms.  On the other hand, 4-8ms inter-site message\
    \ transit times are\n   common and 1-2ms are reported increasingly often.  Thus,\
    \ it is impos-\n   sible to synchronize clocks to better than 32-48ms, enough\
    \ time for a\n   pair of sites to exchange between 4 and 50 messages.  Even with\n\
    \   advancing technology, it seems safe to assume that clock skew will\n   remain\
    \ \"large\" when compared to inter-site message transmission\n   speed.  In particular,\
    \ this argues against time-based protocols such\n   as the one used in [Cristian]\n\
    \   6.1 The GBCAST primitive\n       GBCAST (group multicast) is the most constrained,\
    \ and costly, of\n       the three primitives.  It is used to transmit information\
    \ about\n       failures and recoveries to members of a process group.  A recov-\n\
    \       ering member uses GBCAST to inform the operational ones that it\n    \
    \   has become available.  Additionally, when a member fails, the\n       system\
    \ arranges for a GBCAST to be issued to group members on its\n       behalf, informing\
    \ them of its failure.  Arguments to GBCAST are a\n       message and a process\
    \ group identifier, which is translated into\n       a set of destinations as\
    \ described below (Sec. 6.5).\n       Our GBCAST protocol ensures that if any\
    \ process receives a multi-\n       cast B before receiving a GBCAST G, then all\
    \ overlapping destina-\n       tions will receive B before G <1> This is true\
    \ regardless of the\n       type of multicast involved.  Moreover, when a failure\
    \ occurs, the\n       corresponding GBCAST message is delivered after any other\
    \ multi-\n       casts from the failed process.  Each member can therefore main-\n\
    \       tain a VIEW listing the membership of the process group, updating\n  \
    \     it when a GBCAST is received.  Although VIEW's are not updated\n       simultaneously\
    \ in real time, all members observe the same\n       sequence of VIEW changes.\
    \  Since, GBCAST's are ordered relative\n       to all other multicasts, all members\
    \ receiving a given multicast\n       will have the same value of VIEW when they\
    \ receive it.\n       Notice that GBCAST also provides a convenient way to change\
    \ other\n       global properties of a group \"atomically\".  In our work, we\
    \ have\n       used GBCAST to dynamically change a ranking on the members of a\n\
    \       group, to request that group members establish checkpoints for\n     \
    \  use if recovery is needed after all failure, and to implement\n       process\
    \ migration.  In each case, the ordering of GBCAST relative\n       to other events\
    \ that makes it possible to perform the desired\n       action without running\
    \ any additional protocol.  Other uses for\n       GBCAST will no doubt emerge\
    \ as our research continues.\n       Members of a process group can also use the\
    \ value of VIEW to pick\n       a strategy for processing an incoming request,\
    \ or to react to\n       failure or recovery without having to run any special\
    \ protocol\n       first.  Since the GBCAST ordering is the same everywhere, their\n\
    \       actions will all be consistent.  Notice that when all the members\n  \
    \     of a process group may have failed, GBCAST also provides an inex-\n    \
    \   pensive way to determine the last site that failed: process group\n      \
    \ members simply log each value of VIEW that becomes defined on\n       stable\
    \ storage before using it; a simplified version of the algo-\n       rithm in\
    \ [Skeen-a] can then be executed when recovering from\n       failure.\n   6.2\
    \ The ABCAST primitive\n       The GBCAST primitive is too costly to be used for\
    \ general commun-\n       ication between process group members.  This motivates\
    \ the intro-\n       duction of weaker (less ordered) primitives, which might\
    \ be used\n       in situations where a total order on multicast messages is not\n\
    \       necessary.  Our second primitive, ABCAST (atomic multicast),\n       satisfies\
    \ such a weaker constraint.  Specifically, it is often\n       desired that if\
    \ two multicasts are received in some order at a\n       common destination site,\
    \ they be received in that order at all\n       other common destinations, even\
    \ if this order was not predeter-\n       mined.  For example, if a process group\
    \ is being used to maintain\n       a replicated queue and ABCAST is used to transmit\
    \ queue opera-\n       tions to all copies, the operations will be done in the\
    \ same\n       order everywhere, hence the copies of the queue will remain mutu-\n\
    \       ally consistent.  The primitive ABCAST(msg, label, dests) pro-\n     \
    \  vides this behavior.  Two ABCAST's having the same label are\n       delivered\
    \ in the same order at all common destinations.\n   6.3 The CBCAST primitive\n\
    \       Our third primitive, CBCAST (causal multicast), is weakest in the\n  \
    \     sense that it involves less distributed synchronization then\n       GBCAST\
    \ or ABCAST.  CBCAST(msg, dests) atomically delivers msg to\n       each operational\
    \ dest.  The CBCAST protocol ensures that if two\n       multicasts are potentially\
    \ causally dependent on another, then\n       the former is delivered after the\
    \ latter at all overlapping des-\n       tinations.  A multicast B' is potentially\
    \ causally dependent on a\n       multicast B if both multicasts originate from\
    \ the same process,\n       and B' is sent after B, or if there exists a chain\
    \ of message\n       transmissions and receptions or local events by which knowledge\n\
    \       could have been transferred from the process that issued B to the\n  \
    \     process that issued B' [Lamport].  For causally independent mul-\n     \
    \  ticasts, the delivery ordering is not constrained.\n       CBCAST is valuable\
    \ in systems like ISIS, where concurrency con-\n       trol algorithms are used\
    \ to synchronize concurrent computations.\n       In these systems, if two processes\
    \ communicate concurrently with\n       the same process the messages are almost\
    \ always independent ones\n       that can be processed in any order: otherwise,\
    \ concurrency con-\n       trol would have caused one to pause until the other\
    \ was finished.\n       On the other hand, order is clearly important within a\
    \ causally\n       linked series of multicasts, and it is precisely this sort\
    \ of\n       order that CBCAST respects.\n   6.4 Other multicast primitives\n\
    \       A weaker multicast primitive is reliable multicast, which pro-\n     \
    \  vides all-or-nothing delivery, but no ordering properties.  The\n       formulation\
    \ of CBCAST in [Birman-b] actually includes a mechanism\n       for performing\
    \ multicasts of this sort, hence no special\n       primitive is needed for the\
    \ purpose.  Additionally, there may be\n       situations in which ABCAST protocols\
    \ that also satisfy a CBCAST\n       ordering property would be valuable.  Our\
    \ ABCAST primitive could\n       be changed to respect such a rule, and we made\
    \ use of a multicast\n       primitive that is simultaneously causal and atomic\
    \ in our work on\n       consistent shared bulletin boards ([Birman-c]).  For\
    \ simplicity,\n       the presentation here assumes that ABCAST is completely\
    \ orthogo-\n       nal to CBCAST, but a simple way to build an efficient \"causal\n\
    \       atomic\" multicast is described in our full-length paper.  The\n     \
    \  cost of this protocol is only slightly higher than that of\n       ABCAST.\n\
    \   6.5 Group addressing protocol\n       Since group membership can change dynamically,\
    \ it may be diffi-\n       cult for a process to compute a list of destinations\
    \ to which a\n       message should be sent, for example, as is needed to perform\
    \ a\n       GBCAST.  In [Birman-b] we report on a protocol for ensuring that\n\
    \       a given multicast will be delivered to all members of a process\n    \
    \   group in the same view.  This view is either the view that was\n       operative\
    \ when the message transmission was initiated, or a view\n       that was defined\
    \ subsequently.  The algorithm is a simple itera-\n       tive one that costs\
    \ nothing unless the group membership changes,\n       and permits the caching\
    \ of possibly inaccurate membership infor-\n       mation near processes that\
    \ might want to communicate with a\n       group.  Using the protocol, a flexible\
    \ message addressing scheme\n       can readily be supported.\n       Iterative\
    \ addressing is only required when the process transmit-\n       ting a message\
    \ has an inaccurate copy of the process group view.\n       In the implementation\
    \ we are now building, this would rarely be\n       the case, and iteration is\
    \ never needed if the view is known to\n       be accurate.  Thus, iterated delivery\
    \ should be very infrequent.\n   6.6 Synchronous versus asynchronous multicast\
    \ abstractions\n       Many systems employ RPC internally, as a lowest level primitive\n\
    \       for interaction between processes.  It should be evident that all\n  \
    \     of our multicast primitives can be used to implement replicated\n      \
    \ remote procedure calls [Cooper]: the caller would simply pause\n       until\
    \ replies have been received from all the participants\n       (observation of\
    \ a failure constitutes a reply in this case).  We\n       term such a use of\
    \ the primitives synchronous, to distinguish it\n       from from an asynchronous\
    \ multicast in which no replies, or just\n       one reply, suffices.\n      \
    \ In our work on ISIS, GBCAST and ABCAST are normally invoked syn-\n       chronously,\
    \ to implement a remote procedure call by one member of\n       an object on all\
    \ the members of its process group.  However,\n       CBCAST, which is the most\
    \ frequently used overall, is almost\n       never invoked synchronously.  Asynchronous\
    \ CBCAST's are the\n       primary source of concurrency in ISIS: although the\
    \ delivery ord-\n       ering is assured, transmission can be delayed to enable\
    \ a message\n       to be piggybacked on another, or to schedule IO within the\
    \ system\n       as a whole.  While the system cannot defer an asynchronous multi-\n\
    \       cast indefinitely, the ability to defer it a little, without\n       delaying\
    \ some computation by doing so, permits load to be\n       smoothed.  Since CBCAST\
    \ respects the delivery orderings on which\n       a computation might depend,\
    \ and is ordered with respect to\n       failures, the concurrency introduced\
    \ does not complicate higher\n       level algorithms.  Moreover, the protocol\
    \ itself is extremely\n       cheap.\n       A problem is introduced by our decision\
    \ to allow asynchronous\n       multicasts: the atomic reception property must\
    \ now be extended to\n       address causally related sequences of asynchronous\
    \ messages.  If\n       a failure were to result in some multicasts being delivered\
    \ to\n       all their destinations but others that precede them not being\n \
    \      delivered anywhere, inconsistency might result even if the desti-\n   \
    \    nations do not overlap.  We therefore extend the atomicity pro-\n       perty\
    \ as follows.  If process t receives a message m from process\n       s, and s\
    \ subsequently fails, then unless t fails as well, all\n       messages m' that\
    \ s received prior to its failure must be\n       delivered to their remaining\
    \ operational destinations.  This is\n       because the state of t may now depend\
    \ on the contents of any such\n       m', hence the system state could become\
    \ inconsistent if the\n       delivery of m' were not completed.  The costs of\
    \ the protocols\n       are not affected by this change.\n       A second problem\
    \ arises when the user-level implications of this\n       atomicity rule are considered.\
    \  In the event of a failure, any\n       suffix of a sequence of aysnchronous\
    \ multicasts could be lost and\n       the system state would still be internally\
    \ consistent.  A process\n       that is about to take some action that may leave\
    \ an externally\n       visible side-effect will need a way to pause until it\
    \ is\n       guaranteed that such multicasts have actually been delivered.\n \
    \      For this purpose, a flush primitive is provided.  Occasional\n       calls\
    \ to flush do not eliminate the benefit of using CBCAST asyn-\n       chronously.\
    \  Unless the system has built up a considerable back-\n       log of undelivered\
    \ multicast messages, which should be rare,\n       flush will only pause while\
    \ transmission of the last few multi-\n       casts complete.\n"
- title: 7. Using the primitives
  contents:
  - "7. Using the primitives\n   The reliable communication primitives described above\
    \ lead to simple\n   solutions for the problems cited in Sec. 4:\n       [1] \
    \ Synchronization.  Many synchronization problems are subsumed\n       into the\
    \ primitives themselves.  For example, consider the use of\n       GBCAST to implement\
    \ recovery.  A recovering process would issue a\n       GBCAST to the process\
    \ group members, requesting that state\n       information be transferred to it.\
    \  In addition to sending the\n       current state of the group to the recovering\
    \ process, group\n       members update the process group view at this time. \
    \ Subsequent\n       messages to the group will be delivered to the recovered\
    \ process,\n       with all necessary synchronization being provided by the ordering\n\
    \       properties of GBCAST.  In situations where other forms of syn-\n     \
    \  chronization are needed, ABCAST provides a simple way to ensure\n       that\
    \ several processes take actions in the same order, and this\n       form of low-level\
    \ synchronization simplifies a number of higher-\n       level synchronization\
    \ problems.  For example, if ABCAST is used\n       to do P() and V() operations\
    \ on a distributed semaphore, the\n       order of operations on the semaphore\
    \ is set by the ABCAST, hence\n       all the managers of the semaphore see these\
    \ operations in a fixed\n       order.\n       [2]  Failure detection.  Consistent\
    \ failure (and recovery) detec-\n       tion are trivial using our primitives:\
    \ a process simply waits for\n       the appropriate process group view to change.\
    \  This facilitates\n       the implementation of algorithms in which one processes\
    \ monitors\n       the status of another process.  A process that acts on the\
    \ basis\n       of a process group view change does so with the assurance that\n\
    \       other group members will (eventually) observe the same event and\n   \
    \    will take consistent actions.\n       [3]  Consistency.  We believe that\
    \ consistency is generally\n       expressible as a set of atomicity and ordering\
    \ constraints on\n       message delivery, particularly causal ones of the sort\
    \ provided\n       by CBCAST.  Our primitives permit a process to specify the\
    \ com-\n       munication properties needed to achieve a desired form of con-\n\
    \       sistency.  Continued research will be needed to understand pre-\n    \
    \   cisely how to pick the weakest primitive in a designated situa-\n       tion.\n\
    \       [4]  Serializability.  To achieve serializability, one implements\n  \
    \     a concurrency control algorithm and then forces computations to\n      \
    \ respect the serialization order that this algorithm choses.  The\n       ABCAST\
    \ primitive, as observed above, is a powerful tool for\n       establishing an\
    \ order between concurrent events, e.g. by lock\n       acquisition.  Having established\
    \ such an order, CBCAST can be\n       used to distribute information about the\
    \ computation and also its\n       termination (commit or abort).  Any process\
    \ that observes the\n       commit or abort of a computation will only be able\
    \ to interact\n       with data managers that have received messages preceding\
    \ the com-\n       mit or abort, hence a highly asynchronous transactional execution\n\
    \       results.  If a process running a computation fails, this is\n       detected\
    \ when a failure GBCAST is received instead of the commit.\n       Thus, executions\
    \ are simple and quite deterministic.\n       If commit is conditional, CBCAST\
    \ would be used to first interro-\n       gate participants to learn if they are\
    \ prepared to commit, and\n       then to transmit the commit or abort decision\
    \ (the usual two-\n       phase commit).  On the other hand, conditional commits\
    \ can often\n       be avoided using our approach.  A method for building transac-\n\
    \       tions that will roll-forward after failure after failure is dis-\n   \
    \    cussed in more detail in [Birman-a] [Joseph-a] [Joseph-b].  Other\n     \
    \  forms of concurrency control, such as timestamp generation, can\n       similarly\
    \ be implemented using ABCAST and CBCAST.  We view tran-\n       sactional data\
    \ storage as an application-level concern, which can\n       be handled using\
    \ a version stack approach or a multi-version\n       store, or any other appropriate\
    \ mechanism.\n"
- title: 8. Implementation
  contents:
  - "8. Implementation\n   The communication primitives can be built in layers, starting\
    \ with a\n   bare network providing unreliable Internet datagrams.  The software\n\
    \   structure is, however, less mature and more complex than the one sug-\n  \
    \ gested in RFC's 966 and 988.  For example, at this stage of our\n   research\
    \ we do not understand how to optimize our protocols to the\n   same extent as\
    \ for the unreliable host multicast approach described\n   in those RFC's.  Thus,\
    \ the implementation we describe here should be\n   understood to be a prototype.\
    \  A particularly intriguing question,\n   which we are investigating actively,\
    \ concerns the use of a \"best\n   effort\" ethernet or Internet multicast as\
    \ a tool to optimize the\n   implementation of our protocols.\n   Our basic approach\
    \ is to view large area networks as a set of clus-\n   ters of sites interconnected\
    \ by high speed LAN devices and intercon-\n   nected by slower long-haul links.\
    \  We first provide protocols for use\n   within clusters, and then extend them\
    \ to run between clusters too.\n   Network partitioning can be tolerated at all\
    \ levels of the hierarchy\n   in the sense that no incorrect actions can result\
    \ after network par-\n   titioning, although our approach will sometimes block\
    \ until the par-\n   tition is repaired.  Our protocols also tend to block within\
    \ a clus-\n   ter while the list of operational sites for that cluster is being\n\
    \   changed.  In normal LAN's, this happens infrequently (during site\n   failure\
    \ or recovery), and would not pose a problem.  (In failure\n   intensive applications,\
    \ alternative protocols might be needed to\n   address this issue).\n   The lowest\
    \ level of our software uses a site-to-site acknowledgement\n   protocol to convert\
    \ the unreliable packet transport this into a\n   sequenced, error-free message\
    \ abstraction, using timeouts to detect\n   apparent failures.  TCP can also be\
    \ used for this purpose, provided\n   that a \"filter\" is placed on the incoming\
    \ message stream and certain\n   types of messages are handled specially.  An\
    \ agreement protocol is\n   then used to order the site-failures and recoveries\
    \ consistently.  If\n   timeouts cause a failure to be detected erroneously, the\
    \ protocol\n   forces the affected site to undergo recovery.\n   Built on this\
    \ is a layer that supports the primitives themselves.\n   CBCAST has a very light-weight\
    \ implementation, based on the idea of\n   flooding the system with copies of\
    \ a message: Each process buffers\n   copies of any messages needed to ensure\
    \ the consistency of its view\n   of the system.  If message m is delivered to\
    \ process p, and m is\n   potentially causally dependent on a message m prime,\
    \ then a copy of m\n   prime is sent to p as well (duplicates are discarded).\
    \  A garbage\n   collector deletes superfluous copies after a message has reached\
    \ all\n   its destinations.  By using extensive piggybacking and a simple\n  \
    \ scheduling algorithm to control message transmission, the cost of a\n   CBCAST\
    \ is kept low -- often, less than one packet per destination.\n   ABCAST employs\
    \ a two-phase protocol based on one suggested to us by\n   Skeen [Skeen-b].  This\
    \ protocol has higher latency than CBCAST\n   because delivery can only occur\
    \ during the second phase; ABCAST is\n   thus inherently synchronous.  In ISIS,\
    \ however, ABCAST is used\n   rarely; we believe that this would be the case in\
    \ other systems as\n   well.  GBCAST is implemented using a two-phase protocol\
    \ similar to\n   the one for ABCAST, but with an additional mechanism that flushes\n\
    \   messages from a failed process before delivering the GBCAST announc-\n   ing\
    \ the failure.  Although GBCAST is slower than ABCAST or CBCAST, it\n   is used\
    \ rarely enough so that performance is probably less of an\n   issue here -- and\
    \ in any case, even GBCAST could be tuned to give\n   very high throughput.  Preliminary\
    \ performance figures appear in\n   [Birman-b].\n   Although satisfactory performance\
    \ should be possible using an imple-\n   mentation that sits on top of a conventional\
    \ Internet mechanism, it\n   should be noted that to achieve really high rates\
    \ of communication\n   the layers of software described above must reside in the\
    \ kernel,\n   because they run on behalf of large numbers of clients, run fre-\n\
    \   quently, and tend to execute for very brief periods before doing I/O\n   and\
    \ pausing.  A non-kernel implementation will thus incur high\n   scheduling and\
    \ context switching overhead.  Additionally, it is not\n   at all clear how to\
    \ use ethernet style broadcast mechanisms to optim-\n   ize the performance of\
    \ this sort of protocol, although it should be\n   possible.  We view this as\
    \ an interesting area for research.\n   A forthcoming paper will describe higher\
    \ level software that we are\n   building on top of the basic fault-tolerant process\
    \ group mechanism\n   described above.\n"
- title: 9. Conclusions
  contents:
  - "9. Conclusions\n   The experience of implementing a substantial fault-tolerant\
    \ system\n   left us with insights into the properties to be desired from a com-\n\
    \   munication subsystem.  In particular, we became convinced that to\n   build\
    \ a reliable distributed system, one must start with a reliable\n   communication\
    \ subsystem.  The multicast primitives described in this\n   memo present a simple\
    \ interface, achieve a high level of concurrency,\n   can be used in both local\
    \ and wide area networks, and are applicable\n   to software ranging from distributed\
    \ database systems to the fault-\n   tolerant objects and bulletin boards provided\
    \ by ISIS.  Because they\n   are integrated with failure handling mechanisms and\
    \ respect desired\n   event orderings, they introduce a desirable form of determinism\
    \ into\n   distributed computation without compromising efficiency.  A conse-\n\
    \   quence is that high-level algorithms are greatly simplified, reducing\n  \
    \ the probability of error.  We believe that this is a very promising\n   and\
    \ practical approach to building large fault-tolerant distributed\n   systems,\
    \ and it is the only one we know of that leads to a rigorous\n   form of confidence\
    \ in the resulting software.\n"
- title: 'NOTES:'
  contents:
  - "NOTES:\n   <1> A problem arises if a process p fails without receiving some mes-\n\
    \   sage after that message has already been delivered to some other pro-\n  \
    \ cess q: q's VIEW when it received the message would show p to be\n   operational;\
    \ hence, q will assume that p received the message,\n   although p is physically\
    \ incapable of doing so.  However, the state\n   of the system is now equivalent\
    \ to one in which p did receive the\n   message, but failed before acting on it.\
    \  In effect, there exists an\n   interpretation of the actual system state that\
    \ is consistent with q's\n   assumption.  Thus, GBCAST satisfies the sort of logical\
    \ delivery pro-\n   perty cited in the introduction.\n"
- title: 10. References
  contents:
  - '10. References

    '
- title: '[RFC966] Deering, S. and Cheriton, D.  Host groups: A multicast exten-'
  contents:
  - "[RFC966] Deering, S. and Cheriton, D.  Host groups: A multicast exten-\n    \
    \  sion to the internet protocol.  Stanford University, December\n      1985.\n"
- title: '[RFC988] Deering, S.  Host extensions for IP multicasting.  Stanford'
  contents:
  - "[RFC988] Deering, S.  Host extensions for IP multicasting.  Stanford\n      University,\
    \ July 1986.\n"
- title: '[Allchin] Allchin, J., McKendry, M.  Synchronization and recovery of'
  contents:
  - "[Allchin] Allchin, J., McKendry, M.  Synchronization and recovery of\n      actions.\
    \  Proc. 2nd ACM SIGACT/SIGOPS Principles of Distributed\n      Computing, Montreal,\
    \ Canada, 1983.\n"
- title: '[Babaoglu] Babaoglu, O., Drummond, R.  The streets of Byzantium: Network'
  contents:
  - "[Babaoglu] Babaoglu, O., Drummond, R.  The streets of Byzantium: Network\n  \
    \    architectures for fast reliable multicast.  IEEE Trans. on\n      Software\
    \ Engineering TSE-11, 6 (June 1985).\n"
- title: '[Bernstein] Bernstein, P., Goodman, N.  Concurrency control algorithms'
  contents:
  - "[Bernstein] Bernstein, P., Goodman, N.  Concurrency control algorithms\n    \
    \  for replicated database systems.  ACM Computing Surveys 13, 2\n      (June\
    \ 1981), 185-222.\n"
- title: '[Birman-a] Birman, K.  Replication and fault-tolerance in the ISIS sys-'
  contents:
  - "[Birman-a] Birman, K.  Replication and fault-tolerance in the ISIS sys-\n   \
    \   tem.  Proc. 10th ACM SIGOPS Symposium on Operating Systems Princi-\n     \
    \ ples.  Orcas Island, Washington, Dec. 1985, 79-86.\n"
- title: '[Birman-b] Birman, K., Joseph, T.  Reliable communication in the pres-'
  contents:
  - "[Birman-b] Birman, K., Joseph, T.  Reliable communication in the pres-\n    \
    \  ence of failures.  Dept. of Computer Science, Cornell Univ., TR\n      85-694,\
    \ Aug. 1985.  To appear in ACM TOCS (Feb. 1987).\n"
- title: '[Birman-c] Birman, K., Joseph, T., Stephenson, P.  Programming with'
  contents:
  - "[Birman-c] Birman, K., Joseph, T., Stephenson, P.  Programming with\n      fault\
    \ tolerant bulletin boards in asynchronous distributed sys-\n      tems.  Dept.\
    \ of Computer Science, Cornell Univ., TR 85-788, Aug.\n      1986.\n"
- title: '[Birrell] Birrell, A., Nelson, B.  Implementing remote procedure calls.'
  contents:
  - "[Birrell] Birrell, A., Nelson, B.  Implementing remote procedure calls.\n   \
    \   ACM Transactions on Computer Systems 2, 1 (Feb. 1984), 39-59.\n"
- title: '[Chang] Chang, J., Maxemchuck, M. Reliable multicast protocols.  ACM'
  contents:
  - "[Chang] Chang, J., Maxemchuck, M. Reliable multicast protocols.  ACM\n      TOCS\
    \ 2, 3 (Aug. 1984), 251-273.\n"
- title: '[Cheriton] Cheriton, D. The V Kernel: A software base for distributed'
  contents:
  - "[Cheriton] Cheriton, D. The V Kernel: A software base for distributed\n     \
    \ systems.  IEEE Software 1 12, (1984), 19-43.\n"
- title: '[Cooper] Cooper, E. Replicated procedure call.  Proc. 3rd ACM Symposium'
  contents:
  - "[Cooper] Cooper, E. Replicated procedure call.  Proc. 3rd ACM Symposium\n   \
    \   on Principles of Distributed Computing., August 1984, 220-232.\n      (May\
    \ 1985).\n"
- title: '[Cristian] Cristian, F. et al Atomic multicast: From simple diffusion to'
  contents:
  - "[Cristian] Cristian, F. et al Atomic multicast: From simple diffusion to\n  \
    \    Byzantine agreement.  IBM Technical Report RJ 4540 (48668), Oct.\n      1984.\n"
- title: '[Eswaren] Eswaren, K.P., et al The notion of consistency and predicate'
  contents:
  - "[Eswaren] Eswaren, K.P., et al The notion of consistency and predicate\n    \
    \  locks in a database system.  Comm. ACM 19, 11 (Nov. 1976), 624-\n      633.\n"
- title: '[Hadzilacos] Hadzilacos, V.  Byzantine agreement under restricted types'
  contents:
  - "[Hadzilacos] Hadzilacos, V.  Byzantine agreement under restricted types\n   \
    \   of failures (not telling the truth is different from telling of\n      lies).\
    \  Tech. ARep. TR-19-83, Aiken Comp. Lab., Harvard University\n      (June 1983).\n"
- title: '[Halpern] Halpern, J., and Moses, Y.  Knowledge and common knowledge in'
  contents:
  - "[Halpern] Halpern, J., and Moses, Y.  Knowledge and common knowledge in\n   \
    \   a distributed environment.  Tech. Report RJ-4421, IBM San Jose\n      Research\
    \ Laboratory, 1984.\n"
- title: '[Joseph-a] Joseph, T.  Low cost management of replicated data.  Ph.D.'
  contents:
  - "[Joseph-a] Joseph, T.  Low cost management of replicated data.  Ph.D.\n     \
    \ dissertation, Dept. of Computer Science, Cornell Univ., Ithaca\n      (Dec.\
    \ 1985).\n"
- title: '[Joseph-b] Joseph, T., Birman, K.  Low cost management of replicated'
  contents:
  - "[Joseph-b] Joseph, T., Birman, K.  Low cost management of replicated\n      data\
    \ in fault-tolerant distributed systems.  ACM TOCS 4, 1 (Feb\n      1986), 54-70.\n"
- title: '[Lamport] Lamport, L.  Time, clocks, and the ordering of events in a'
  contents:
  - "[Lamport] Lamport, L.  Time, clocks, and the ordering of events in a\n      distributed\
    \ system.  CACM 21, 7, July 1978, 558-565.\n"
- title: '[Lazowska] Lazowska, E. et al The architecture of the EDEN system.'
  contents:
  - "[Lazowska] Lazowska, E. et al The architecture of the EDEN system.\n      Proc.\
    \ 8th Symposium on Operating Systems Principles, Dec. 1981,\n      148-159.\n"
- title: '[Liskov] Liskov, B., Scheifler, R. Guardians and actions: Linguistic'
  contents:
  - "[Liskov] Liskov, B., Scheifler, R. Guardians and actions: Linguistic\n      support\
    \ for robust, distributed programs.  ACM TOPLAS 5, 3 (July\n      1983), 381-404.\n"
- title: '[Moss] Moss, E.  Nested transactions: An approach to reliable, distri-'
  contents:
  - "[Moss] Moss, E.  Nested transactions: An approach to reliable, distri-\n    \
    \  buted computing.  Ph.D. thesis, MIT Dept of EECS, TR 260, April\n      1981.\n"
- title: '[Papadimitrou] Papadimitrou, C.  The serializability of concurrent data-'
  contents:
  - "[Papadimitrou] Papadimitrou, C.  The serializability of concurrent data-\n  \
    \    base updates.  JACM 26, 4 (Oct. 1979), 631-653.\n"
- title: '[Popek] Popek, G. et al.  Locus: A network transparent, high reliability'
  contents:
  - "[Popek] Popek, G. et al.  Locus: A network transparent, high reliability\n  \
    \    distributed system.  Proc. 8th Symposium on Operating Systems\n      Principles,\
    \ Dec. 1981, 169-177.\n"
- title: '[Schlicting] Schlicting, R, Schneider, F.  Fail-stop processors: An'
  contents:
  - "[Schlicting] Schlicting, R, Schneider, F.  Fail-stop processors: An\n      approach\
    \ to designing fault-tolerant distributed computing sys-\n      tems.  ACM TOCS\
    \ 1, 3, August 1983, 222-238.\n"
- title: '[Schneider] Schneider, F., Gries, D., Schlicting, R.  Reliable multicast'
  contents:
  - "[Schneider] Schneider, F., Gries, D., Schlicting, R.  Reliable multicast\n  \
    \    protocols.  Science of computer programming 3, 2 (March 1984).\n"
- title: '[Skeen-a] Skeen, D.  Determining the last process to fail.  ACM TOCS 3,'
  contents:
  - "[Skeen-a] Skeen, D.  Determining the last process to fail.  ACM TOCS 3,\n   \
    \   1, Feb. 1985, 15-30.\n"
- title: '[Skeen-b] Skeen, D.  A reliable multicast protocol.  Unpublished.'
  contents:
  - '[Skeen-b] Skeen, D.  A reliable multicast protocol.  Unpublished.

    '
- title: '[Spector] Spector, A., et al  Distributed transactions for reliable sys-'
  contents:
  - "[Spector] Spector, A., et al  Distributed transactions for reliable sys-\n  \
    \    tems.  Proc. 10th ACM SIGOPS Symposium on Operating Systems Prin-\n     \
    \ ciples, Dec. 1985, 127-146.\n"
- title: '[Strong] Strong, H.R., Dolev, D. Byzantine agreement. Digest of papers,'
  contents:
  - "[Strong] Strong, H.R., Dolev, D. Byzantine agreement. Digest of papers,\n   \
    \   Spring Compcon 83, San Francisco, CA, March 1983, 77-81.\n"
