- title: __initial_text__
  contents:
  - '         Some Internet Architectural Guidelines and Philosophy

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2002).  All Rights Reserved.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document extends RFC 1958 by outlining some of the philosophical\n\
    \   guidelines to which architects and designers of Internet backbone\n   networks\
    \ should adhere.  We describe the Simplicity Principle, which\n   states that\
    \ complexity is the primary mechanism that impedes\n   efficient scaling, and\
    \ discuss its implications on the architecture,\n   design and engineering issues\
    \ found in large scale Internet\n   backbones.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction . . . . . . . . . . . . . . . . . . . .\
    \ . . . .  2\n   2. Large Systems and The Simplicity Principle . . . . . . . .\
    \ .  3\n   2.1. The End-to-End Argument and Simplicity   . . . . . . . . .  3\n\
    \   2.2. Non-linearity and Network Complexity   . . . . . . . . . .  3\n   2.2.1.\
    \ The Amplification Principle. . . . . . . . . . . . . . .  4\n   2.2.2. The Coupling\
    \ Principle . . . . . . . . . . . . . . . . .  5\n   2.3. Complexity lesson from\
    \ voice. . . . .  . . . . . . . . . .  6\n   2.4. Upgrade cost of complexity.\
    \ . . . . .  . . . . . . . . . .  7\n   3. Layering Considered Harmful. . . .\
    \ . . . . . . . . . . . . .  7\n   3.1. Optimization Considered Harmful . . .\
    \  . . . . . . . . . .  8\n   3.2. Feature Richness Considered Harmful .  . .\
    \ . . . . . . . .  9\n   3.3. Evolution of Transport Efficiency for IP.  . . .\
    \ . . . . .  9\n   3.4. Convergence Layering. . . . . . . . . . .  . . . . . .\
    \ . .  9\n   3.4.1. Note on Transport Protocol Layering. . . . . . . . . . . 11\n\
    \   3.5. Second Order Effects   . . . . . . . . . . . . . . . . . . 11\n   3.6.\
    \ Instantiating the EOSL Model with IP   . . . . . . . . . . 12\n   4. Avoid the\
    \ Universal Interworking Function. . . . . . . . . . 12\n   4.1. Avoid Control\
    \ Plane Interworking . . . . . . . . . . . . . 13\n   5. Packet versus Circuit\
    \ Switching: Fundamental Differences . . 13\n   5.1. Is PS is inherently more\
    \ efficient than CS?  . . . . . . . 13\n   5.2. Is PS simpler than CS? . . . .\
    \ . . . . . . . . . . . . . . 14\n   5.2.1. Software/Firmware Complexity . . .\
    \ . . . . . . . . . . . 15\n   5.2.2. Macro Operation Complexity . . . . . . .\
    \ . . . . . . . . 15\n   5.2.3. Hardware Complexity. . . . . . . . . . . . . .\
    \ . . . . . 15\n   5.2.4. Power. . . . . . . . . . . . . . . . . . . . . . . .\
    \ . . 16\n   5.2.5. Density. . . . . . . . . . . . . . . . . . . . . . . . . 16\n\
    \   5.2.6. Fixed versus variable costs. . . . . . . . . . . . . . . 16\n   5.2.7.\
    \ QoS. . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n   5.2.8. Flexibility.\
    \ . . . . . . . . . . . . . . . . . . . . . . 17\n   5.3. Relative Complexity\
    \  . . . . . . . . . . . . . . . . . . . 17\n   5.3.1. HBHI and the OPEX Challenge.\
    \ . . . . . . . . . . . . . . 18\n   6. The Myth of Over-Provisioning. . . . .\
    \ . . . . . . . . . . . 18\n   7. The Myth of Five Nines . . . . . . . . . . .\
    \ . . . . . . . . 19\n   8. Architectural Component Proportionality Law. . . .\
    \ . . . . . 20\n   8.1. Service Delivery Paths . . . . . . . . . . . . . . . .\
    \ . . 21\n   9. Conclusions. . . . . . . . . . . . . . . . . . . . . . . . . 21\n\
    \   10. Security Considerations . . . . . . . . . . . . . . . . . . 22\n   11.\
    \ Acknowledgments . . . . . . . . . . . . . . . . . . . . . . 23\n   12. References.\
    \ . . . . . . . . . . . . . . . . . . . . . . . . 23\n   13. Authors' Addresses.\
    \ . . . . . . . . . . . . . . . . . . . . 27\n   14. Full Copyright Statement.\
    \ . . . . . . . . . . . . . . . . . 28\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   RFC 1958 [RFC1958] describes the underlying principles of\
    \ the\n   Internet architecture.  This note extends that work by outlining some\n\
    \   of the philosophical guidelines to which architects and designers of\n   Internet\
    \ backbone networks should adhere.  While many of the areas\n   outlined in this\
    \ document may be controversial, the unifying\n   principle described here, controlling\
    \ complexity as a mechanism to\n   control costs and reliability, should not be.\
    \  Complexity in carrier\n   networks can derive from many sources.  However,\
    \ as stated in\n   [DOYLE2002], \"Complexity in most systems is driven by the\
    \ need for\n   robustness to uncertainty in their environments and component parts\n\
    \   far more than by basic functionality\".  The major thrust of this\n   document,\
    \ then, is to raise awareness about the complexity of some of\n   our current\
    \ architectures, and to examine the effect such complexity\n   will almost certainly\
    \ have on the IP carrier industry's ability to\n   succeed.\n   The rest of this\
    \ document is organized as follows: The first section\n   describes the Simplicity\
    \ Principle and its implications for the\n   design of very large systems.  The\
    \ remainder of the document outlines\n   the high-level consequences of the Simplicity\
    \ Principle and how it\n   should guide large scale network architecture and design\
    \ approaches.\n"
- title: 2.  Large Systems and The Simplicity Principle
  contents:
  - "2.  Large Systems and The Simplicity Principle\n   The Simplicity Principle,\
    \ which was perhaps first articulated by Mike\n   O'Dell, former Chief Architect\
    \ at UUNET, states that complexity is\n   the primary mechanism which impedes\
    \ efficient scaling, and as a\n   result is the primary driver of increases in\
    \ both capital\n   expenditures (CAPEX) and operational expenditures (OPEX). \
    \ The\n   implication for carrier IP networks then, is that to be successful we\n\
    \   must drive our architectures and designs toward the simplest possible\n  \
    \ solutions.\n"
- title: 2.1.  The End-to-End Argument and Simplicity
  contents:
  - "2.1.  The End-to-End Argument and Simplicity\n   The end-to-end argument, which\
    \ is described in [SALTZER] (as well as\n   in RFC 1958 [RFC1958]), contends that\
    \ \"end-to-end protocol design\n   should not rely on the maintenance of state\
    \ (i.e., information about\n   the state of the end-to-end communication) inside\
    \ the network.  Such\n   state should be maintained only in the end points, in\
    \ such a way that\n   the state can only be destroyed when the end point itself\
    \ breaks.\"\n   This property has also been related to Clark's \"fate-sharing\"\
    \ concept\n   [CLARK].  We can see that the end-to-end principle leads directly\
    \ to\n   the Simplicity Principle by examining the so-called \"hourglass\"\n \
    \  formulation of the Internet architecture [WILLINGER2002].  In this\n   model,\
    \ the thin waist of the hourglass is envisioned as the\n   (minimalist) IP layer,\
    \ and any additional complexity is added above\n   the IP layer.  In short, the\
    \ complexity of the Internet belongs at\n   the edges, and the IP layer of the\
    \ Internet should remain as simple\n   as possible.\n   Finally, note that the\
    \ End-to-End Argument does not imply that the\n   core of the Internet will not\
    \ contain and maintain state.  In fact, a\n   huge amount coarse grained state\
    \ is maintained in the Internet's core\n   (e.g., routing state).  However, the\
    \ important point here is that\n   this (coarse grained) state is almost orthogonal\
    \ to the state\n   maintained by the end-points (e.g., hosts).  It is this minimization\n\
    \   of interaction that contributes to simplicity.  As a result,\n   consideration\
    \ of \"core vs. end-point\" state interaction is crucial\n   when analyzing protocols\
    \ such as Network Address Translation (NAT),\n   which reduce the transparency\
    \ between network and hosts.\n"
- title: 2.2.  Non-linearity and Network Complexity
  contents:
  - "2.2.  Non-linearity and Network Complexity\n   Complex architectures and designs\
    \ have been (and continue to be)\n   among the most significant and challenging\
    \ barriers to building cost-\n   effective large scale IP networks.  Consider,\
    \ for example, the task\n   of building a large scale packet network.  Industry\
    \ experience has\n   shown that building such a network is a different activity\
    \ (and hence\n   requires a different skill set) than building a small to medium\
    \ scale\n   network, and as such doesn't have the same properties.  In\n   particular,\
    \ the largest networks exhibit, both in theory and in\n   practice, architecture,\
    \ design, and engineering non-linearities which\n   are not exhibited at smaller\
    \ scale.  We call this Architecture,\n   Design, and Engineering (ADE) non-linearity.\
    \  That is, systems such\n   as the Internet could be described as highly self-dissimilar,\
    \ with\n   extremely different scales and levels of abstraction [CARLSON].  The\n\
    \   ADE non-linearity property is based upon two well-known principles\n   from\
    \ non-linear systems theory [THOMPSON]:\n"
- title: 2.2.1.  The Amplification Principle
  contents:
  - "2.2.1.  The Amplification Principle\n   The Amplification Principle states that\
    \ there are non-linearities\n   which occur at large scale which do not occur\
    \ at small to medium\n   scale.\n   COROLLARY: In many large networks, even small\
    \ things can and do cause\n   huge events.  In system-theoretic terms, in large\
    \ systems such as\n   these, even small perturbations on the input to a process\
    \ can\n   destabilize the system's output.\n   An important example of the Amplification\
    \ Principle is non-linear\n   resonant amplification, which is a powerful process\
    \ that can\n   transform dynamic systems, such as large networks, in surprising\
    \ ways\n   with seemingly small fluctuations.  These small fluctuations may\n\
    \   slowly accumulate, and if they are synchronized with other cycles,\n   may\
    \ produce major changes.  Resonant phenomena are examples of non-\n   linear behavior\
    \ where small fluctuations may be amplified and have\n   influences far exceeding\
    \ their initial sizes.  The natural world is\n   filled with examples of resonant\
    \ behavior that can produce system-\n   wide changes, such as the destruction\
    \ of the Tacoma Narrows bridge\n   (due to the resonant amplification of small\
    \ gusts of wind).  Other\n   examples include the gaps in the asteroid belts and\
    \ rings of Saturn\n   which are created by non-linear resonant amplification.\
    \  Some\n   features of human behavior and most pilgrimage systems are influenced\n\
    \   by resonant phenomena involving the dynamics of the solar system,\n   such\
    \ as solar days, the 27.3 day (sidereal) and 29.5 day (synodic)\n   cycles of\
    \ the moon or the 365.25 day cycle of the sun.\n   In the Internet domain, it\
    \ has been shown that increased inter-\n   connectivity results in more complex\
    \ and often slower BGP routing\n   convergence [AHUJA].  A related result is that\
    \ a small amount of\n   inter-connectivity causes the output of a routing mesh\
    \ to be\n   significantly more complex than its input [GRIFFIN].  An important\n\
    \   method for reducing amplification is ensure that local changes have\n   only\
    \ local effect (this is as opposed to systems in which local\n   changes have\
    \ global effect).  Finally, ATM provides an excellent\n   example of an amplification\
    \ effect: if you lose one cell, you destroy\n   the entire packet (and it gets\
    \ worse, as in the absence of mechanisms\n   such as Early Packet Discard [ROMANOV],\
    \ you will continue to carry\n   the already damaged packet).\n   Another interesting\
    \ example of amplification comes from the\n   engineering domain, and is described\
    \ in [CARLSON].  They consider the\n   Boeing 777, which is a \"fly-by-wire\"\
    \ aircraft, containing as many as\n   150,000 subsystems and approximately 1000\
    \ CPUs.  What they observe is\n   that while the 777 is robust to large-scale\
    \ atmospheric disturbances,\n   turbulence boundaries, and variations in cargo\
    \ loads (to name a few),\n   it could be catastrophically disabled my microscopic\
    \ alterations in a\n   very few large CPUs (as the point out, fortunately this\
    \ is a very\n   rare occurrence).  This example illustrates the issue \"that\n\
    \   complexity can amplify small perturbations, and the design engineer\n   must\
    \ ensure such perturbations are extremely rare.\" [CARLSON]\n"
- title: 2.2.2.  The Coupling Principle
  contents:
  - "2.2.2.  The Coupling Principle\n   The Coupling Principle states that as things\
    \ get larger, they often\n   exhibit increased interdependence between components.\n\
    \   COROLLARY: The more events that simultaneously occur, the larger the\n   likelihood\
    \ that two or more will interact.  This phenomenon has also\n   been termed \"\
    unforeseen feature interaction\" [WILLINGER2002].\n   Much of the non-linearity\
    \ observed large systems is largely due to\n   coupling.  This coupling has both\
    \  horizontal and vertical\n   components.  In the context of networking, horizontal\
    \ coupling is\n   exhibited between the same protocol layer, while vertical coupling\n\
    \   occurs between layers.\n   Coupling is exhibited by a wide variety of natural\
    \ systems, including\n   plasma macro-instabilities (hydro-magnetic, e.g., kink,\
    \ fire-hose,\n   mirror, ballooning, tearing, trapped-particle effects) [NAVE],\
    \ as\n   well as various kinds of electrochemical systems (consider the custom\n\
    \   fluorescent nucleotide synthesis/nucleic acid labeling problem\n   [WARD]).\
    \  Coupling of clock physical periodicity has also been\n   observed [JACOBSON],\
    \ as well as coupling of various types of\n   biological cycles.\n   Several canonical\
    \ examples also exist in well known network systems.\n   Examples include the\
    \ synchronization of various control loops, such\n   as routing update synchronization\
    \ and TCP Slow Start synchronization\n   [FLOYD,JACOBSON].  An important result\
    \ of these observations is that\n   coupling is intimately related to synchronization.\
    \  Injecting\n   randomness into these systems is one way to reduce coupling.\n\
    \   Interestingly, in analyzing risk factors for the Public Switched\n   Telephone\
    \ Network (PSTN), Charles Perrow decomposes the complexity\n   problem along two\
    \ related axes, which he terms \"interactions\" and\n   \"coupling\" [PERROW].\
    \  Perrow cites interactions and coupling as\n   significant factors in determining\
    \ the reliability of a complex\n   system (and in particular, the PSTN).  In this\
    \ model, interactions\n   refer to the dependencies between components (linear\
    \ or non-linear),\n   while coupling refers to the flexibility in a system.  Systems\
    \ with\n   simple, linear interactions have components  that affect only other\n\
    \   components that are functionally downstream.  Complex system\n   components\
    \ interact with many other components in different and\n   possibly distant parts\
    \ of the system.  Loosely coupled systems are\n   said to have more flexibility\
    \ in time constraints, sequencing, and\n   environmental assumptions than do tightly\
    \ coupled systems.  In\n   addition, systems with complex interactions and tight\
    \ coupling are\n   likely to have unforeseen failure states (of course, complex\n\
    \   interactions permit more complications to develop and make the system\n  \
    \ hard to understand and predict); this behavior is also described in\n   [WILLINGER2002].\
    \  Tight coupling also means that the system has less\n   flexibility in recovering\
    \ from failure states.\n   The PSTN's SS7 control network provides an interesting\
    \ example of\n   what can go wrong with a tightly coupled complex system.  Outages\n\
    \   such as the well publicized 1991 outage of AT&T's SS7 demonstrates\n   the\
    \ phenomenon: the outage was caused by software bugs in the\n   switches' crash\
    \ recovery code.  In this case, one switch crashed due\n   to a hardware glitch.\
    \  When this switch came back up, it (plus a\n   reasonably probable timing event)\
    \ caused its neighbors to crash When\n   the neighboring switches came back up,\
    \ they caused their neighbors to\n   crash, and so on [NEUMANN] (the root cause\
    \ turned out to be a\n   misplaced 'break' statement; this is an excellent example\
    \ of cross-\n   layer coupling).  This phenomenon is similar to the phase-locking\
    \ of\n   weakly coupled oscillators, in which random variations in sequence\n\
    \   times plays an important role in system stability [THOMPSON].\n"
- title: 2.3.  Complexity lesson from voice
  contents:
  - "2.3.  Complexity lesson from voice\n   In the 1970s and 1980s, the voice carriers\
    \ competed by adding\n   features which drove substantial increases in the complexity\
    \ of the\n   PSTN, especially in the Class 5 switching infrastructure.  This\n\
    \   complexity was typically software-based, not hardware driven, and\n   therefore\
    \ had cost curves worse than Moore's Law.  In summary, poor\n   margins on voice\
    \ products today are due to OPEX and CAPEX costs not\n   dropping as we might\
    \ expect from simple hardware-bound\n   implementations.\n"
- title: 2.4.  Upgrade cost of complexity
  contents:
  - "2.4.  Upgrade cost of complexity\n   Consider the cost of providing new features\
    \ in a complex network.\n   The traditional voice network has little intelligence\
    \ in its edge\n   devices (phone instruments), and a very smart core.  The Internet\
    \ has\n   smart edges, computers with operating systems, applications, etc.,\n\
    \   and a simple core, which consists of a control plane and packet\n   forwarding\
    \ engines.  Adding an new Internet service is just a matter\n   of distributing\
    \ an application to the a few consenting desktops who\n   wish to use it.  Compare\
    \ this to adding a service to voice, where one\n   has to upgrade the entire core.\n"
- title: 3.  Layering Considered Harmful
  contents:
  - "3.  Layering Considered Harmful\n   There are several generic properties of layering,\
    \ or vertical\n   integration as applied to networking.  In general, a layer as\
    \ defined\n   in our context implements one or more of\n    Error Control:   \
    \  The layer makes the \"channel\" more reliable\n                       (e.g.,\
    \ reliable transport layer)\n    Flow Control:      The layer avoids flooding\
    \ slower peer (e.g.,\n                       ATM flow control)\n    Fragmentation:\
    \     Dividing large data chunks into smaller\n                       pieces,\
    \ and subsequent reassembly (e.g., TCP\n                       MSS fragmentation/reassembly)\n\
    \    Multiplexing:      Allow several higher level sessions share\n          \
    \             single lower level \"connection\" (e.g., ATM PVC)\n    Connection\
    \ Setup:  Handshaking with peer (e.g., TCP three-way\n                       handshake,\
    \ ATM ILMI)\n    Addressing/Naming: Locating, managing identifiers associated\n\
    \                       with entities (e.g., GOSSIP 2 NSAP Structure\n       \
    \                [RFC1629])\n   Layering of this type does have various conceptual\
    \ and structuring\n   advantages.  However, in the data networking context structured\n\
    \   layering implies that the functions of each layer are carried out\n   completely\
    \ before the protocol data unit is passed to the next layer.\n   This means that\
    \ the optimization of each layer has to be done\n   separately.  Such ordering\
    \ constraints are in conflict with efficient\n   implementation of data manipulation\
    \ functions.  One could accuse the\n   layered model (e.g., TCP/IP and ISO OSI)\
    \ of causing this conflict.\n   In fact, the operations of multiplexing and segmentation\
    \ both hide\n   vital information that lower layers may need to optimize their\n\
    \   performance.  For example, layer N may duplicate lower level\n   functionality,\
    \ e.g., error recovery hop-hop versus end-to-end error\n   recovery.  In addition,\
    \ different layers may need the same\n   information (e.g., time stamp): layer\
    \ N may need layer N-2\n   information (e.g., lower layer packet sizes), and the\
    \ like [WAKEMAN].\n   A related and even more ironic statement comes from Tennenhouse's\n\
    \   classic paper, \"Layered Multiplexing Considered Harmful\"\n   [TENNENHOUSE]:\
    \ \"The ATM approach to broadband networking is presently\n   being pursued within\
    \ the CCITT (and elsewhere) as the unifying\n   mechanism for the support of service\
    \ integration, rate adaptation,\n   and jitter control within the lower layers\
    \ of the network\n   architecture.  This position paper is specifically concerned\
    \ with the\n   jitter arising from the design of the \"middle\" and \"upper\"\
    \ layers\n   that operate within the end systems and relays of multi-service\n\
    \   networks (MSNs).\"\n   As a result of inter-layer dependencies, increased\
    \ layering can\n   quickly lead to violation of the Simplicity Principle.  Industry\n\
    \   experience has taught us that increased layering frequently increases\n  \
    \ complexity and hence leads to increases in OPEX, as is predicted by\n   the\
    \ Simplicity Principle.  A corollary is stated in RFC 1925\n   [RFC1925], section\
    \ 2(5):\n      \"It is always possible to agglutinate multiple separate problems\n\
    \      into a single complex interdependent solution.  In most cases\n      this\
    \ is a bad idea.\"\n   The first order conclusion then, is that horizontal (as\
    \ opposed to\n   vertical) separation may be more cost-effective and reliable\
    \ in the\n   long term.\n"
- title: 3.1.  Optimization Considered Harmful
  contents:
  - "3.1.  Optimization Considered Harmful\n   A corollary of the layering arguments\
    \ above is that optimization can\n   also be considered harmful.  In particular,\
    \ optimization introduces\n   complexity, and as well as introducing tighter coupling\
    \ between\n   components and layers.\n   An important and related effect of optimization\
    \ is described by the\n   Law of Diminishing Returns, which states that if one\
    \ factor of\n   production is increased while the others remain constant, the\
    \ overall\n   returns will relatively decrease after a certain point [SPILLMAN].\n\
    \   The implication here is that trying to squeeze out efficiency past\n   that\
    \ point only adds complexity, and hence leads to less reliable\n   systems.\n"
- title: 3.2.  Feature Richness Considered Harmful
  contents:
  - "3.2.  Feature Richness Considered Harmful\n   While adding any new feature may\
    \ be considered a gain (and in fact\n   frequently differentiates vendors of various\
    \ types of equipment), but\n   there is a danger.  The danger is in increased\
    \ system complexity.\n"
- title: 3.3.  Evolution of Transport Efficiency for IP
  contents:
  - "3.3.  Evolution of Transport Efficiency for IP\n   The evolution of transport\
    \ infrastructures for IP offers a good\n   example of how decreasing vertical\
    \ integration has lead to various\n   efficiencies.  In particular,\n    | IP\
    \ over ATM over SONET  -->\n    | IP over SONET over WDM  -->\n    | IP over WDM\n\
    \    |\n   \\|/\n   Decreasing complexity, CAPEX, OPEX\n   The key point here\
    \ is that layers are removed resulting in CAPEX and\n   OPEX efficiencies.\n"
- title: 3.4.  Convergence Layering
  contents:
  - "3.4.  Convergence Layering\n   Convergence is related to the layering concepts\
    \ described above in\n   that convergence is achieved via a \"convergence layer\"\
    .  The end\n   state of the convergence argument is the concept of Everything\
    \ Over\n   Some Layer (EOSL).  Conduit, DWDM, fiber, ATM, MPLS, and even IP have\n\
    \   all been proposed as convergence layers.  It is important to note\n   that\
    \ since layering typically drives OPEX up, we expect convergence\n   will as well.\
    \  This observation is again consistent with industry\n   experience.\n   There\
    \ are many notable examples of convergence layer failure.\n   Perhaps the most\
    \ germane example is IP over ATM.  The immediate and\n   most obvious consequence\
    \ of ATM layering is the so-called cell tax:\n   First, note that the complete\
    \ answer on ATM efficiency is that it\n   depends upon packet size distributions.\
    \  Let's assume that typical\n   Internet type traffic patterns, which tend to\
    \ have high percentages\n   of packets at 40, 44, and 552 bytes.  Recent data\
    \ [CAIDA] shows that\n   about 95% of WAN bytes and 85% of packets are TCP.  Much\
    \ of this\n   traffic is composed of 40/44 byte packets.\n   Now, consider the\
    \ case of a a DS3 backbone with PLCP turned on.  Then\n   the maximum cell rate\
    \ is 96,000 cells/sec.  If you multiply this\n   value by the number of bits in\
    \ the payload, you get: 96000 cells/sec\n   * 48 bytes/cell * 8 = 36.864 Mbps.\
    \  This, however, is unrealistic\n   since it\n   assumes perfect payload packing.\
    \  There are two other things that\n   contribute to the ATM overhead (cell tax):\
    \ The wasted padding and the\n   8 byte SNAP header.\n   It is the SNAP header\
    \ which causes most of the problems (and you\n   can't do anything about this),\
    \ forcing most small packets to consume\n   two cells, with the second cell to\
    \ be mostly empty padding (this\n   interacts really poorly with the data quoted\
    \ above, e.g., that most\n   packets are 40-44 byte TCP Ack packets).  This causes\
    \ a loss of about\n   another 16% from the 36.8 Mbps ideal throughput.\n   So\
    \ the total throughput ends up being (for a DS3):\n             DS3 Line Rate:\
    \              44.736\n             PLCP Overhead              - 4.032\n     \
    \        Per Cell Header:           - 3.840\n             SNAP Header & Padding:\
    \     - 5.900\n                                       =========\n            \
    \                             30.960 Mbps\n   Result: With a DS3 line rate of\
    \ 44.736 Mbps, the total overhead is\n   about 31%.\n   Another way to look at\
    \ this is that since a large fraction of WAN\n   traffic is comprised of TCP ACKs,\
    \ one can make a different but\n   related calculation.  IP over ATM requires:\n\
    \             IP data (40 bytes in this case)\n             8 bytes SNAP\n   \
    \          8 bytes AAL5 stuff\n             5 bytes for each cell\n          \
    \   + as much more as it takes to fill out the last cell\n   On ATM, this becomes\
    \ two cells - 106 bytes to convey 40 bytes of\n   information.  The next most\
    \ common size seems to be one of several\n   sizes in the 504-556 byte range -\
    \ 636 bytes to carry IP, TCP, and a\n   512 byte TCP payload - with messages larger\
    \ than 1000 bytes running\n   third.\n   One would imagine that 87% payload (556\
    \ byte message size) is better\n   than 37% payload (TCP Ack size), but it's not\
    \ the 95-98% that\n   customers are used to, and the predominance of TCP Acks\
    \ skews the\n   average.\n"
- title: 3.4.1.  Note on Transport Protocol Layering
  contents:
  - "3.4.1.  Note on Transport Protocol Layering\n   Protocol layering models are\
    \ frequently cast as \"X over Y\" models.\n   In these cases, protocol Y carries\
    \ protocol X's protocol data units\n   (and possibly control data) over Y's data\
    \ plane, i.e., Y is a\n   \"convergence layer\".  Examples include Frame Relay\
    \ over ATM, IP over\n   ATM, and IP over MPLS.  While X over Y layering has met\
    \ with only\n   marginal success [TENNENHOUSE,WAKEMAN], there have been a few\
    \ notable\n   instances where efficiency can be and is gained.  In particular,\
    \ \"X\n   over Y efficiencies\" can be realized when there is a kind of\n   \"\
    isomorphism\" between the X and Y (i.e., there is a small convergence\n   layer).\
    \  In these cases X's data, and possibly control traffic, are\n   \"encapsulated\"\
    \ and transported over Y.  Examples include Frame Relay\n   over ATM, and Frame\
    \ Relay, AAL5 ATM and Ethernet over L2TPv3\n   [L2TPV3]; the simplifying factors\
    \ here are that there is no\n   requirement that a shared clock be recovered by\
    \ the communicating end\n   points, and that control-plane interworking is minimized.\
    \  An\n   alternative is to interwork the X and Y's control and data planes;\n\
    \   control-plane interworking is discussed below.\n"
- title: 3.5.  Second Order Effects
  contents:
  - "3.5.  Second Order Effects\n   IP over ATM provides an excellent example of unanticipated\
    \ second\n   order effects.  In particular, Romanov and Floyd's classic study\
    \ on\n   TCP good-put [ROMANOV] on ATM showed that large UBR buffers (larger\n\
    \   than one TCP window size) are required to achieve reasonable\n   performance,\
    \ that packet discard mechanisms (such as Early Packet\n   Discard, or EPD) improve\
    \ the effective usage of the bandwidth and\n   that more elaborate service and\
    \ drop strategies than FIFO+EPD, such\n   as per VC queuing and accounting, might\
    \ be required at the bottleneck\n   to ensure both high efficiency and fairness.\
    \  Though all studies\n   clearly indicate that a buffer size not less than one\
    \ TCP window size\n   is required, the amount of extra buffer required naturally\
    \ depends on\n   the packet discard mechanism used and is still an open issue.\n\
    \   Examples of this kind of problem with layering abound in practical\n   networking.\
    \  Consider, for example, the effect of IP transport's\n   implicit assumptions\
    \ of lower layers.  In particular:\n    o Packet loss: TCP assumes that packet\
    \ losses are indications of\n      congestion, but sometimes losses are from corruption\
    \ on a wireless\n      link [RFC3115].\n    o Reordered packets: TCP assumes that\
    \ significantly reordered\n      packets are indications of congestion.  This\
    \ is not always the\n      case [FLOYD2001].\n    o Round-trip times: TCP measures\
    \ round-trip times, and assumes that\n      the lack of an acknowledgment within\
    \ a period of time based on the\n      measured round-trip time is a packet loss,\
    \ and therefore an\n      indication of congestion [KARN].\n    o Congestion control:\
    \ TCP congestion control implicitly assumes that\n      all the packets in a flow\
    \ are treated the same by the network, but\n      this is not always the case\
    \ [HANDLEY].\n"
- title: 3.6.  Instantiating the EOSL Model with IP
  contents:
  - "3.6.  Instantiating the EOSL Model with IP\n   While IP is being proposed as\
    \ a transport for almost everything, the\n   base assumption, that Everything\
    \ over IP (EOIP) will result in OPEX\n   and CAPEX efficiencies, requires critical\
    \ examination.  In\n   particular, while it is the case that many protocols can\
    \ be\n   efficiently transported over an IP network (specifically, those\n   protocols\
    \ that do not need to recover synchronization between the\n   communication end\
    \ points, such as Frame Relay, Ethernet, and AAL5\n   ATM), the Simplicity and\
    \ Layering Principles suggest that EOIP may\n   not represent the most efficient\
    \ convergence strategy for arbitrary\n   services.  Rather, a more CAPEX and OPEX\
    \ efficient convergence layer\n   might be much lower (again, this behavior is\
    \ predicted by the\n   Simplicity Principle).\n   An example of where EOIP would\
    \ not be the most OPEX and CAPEX\n   efficient transport would be in those cases\
    \ where a service or\n   protocol needed SONET-like restoration times (e.g., 50ms).\
    \  It is not\n   hard to imagine that it would cost more to build and operate\
    \ an IP\n   network with this kind of restoration and convergence property (if\n\
    \   that were even possible) than it would to build the SONET network in\n   the\
    \ first place.\n"
- title: 4.  Avoid the Universal Interworking Function
  contents:
  - "4.  Avoid the Universal Interworking Function\n   While there have been many\
    \ implementations of Universal Interworking\n   unction (UIWF), IWF approaches\
    \ have been problematic at large scale.\n   his concern is codified in the Principle\
    \ of Minimum Intervention\n   BRYANT]:\n   \"To minimise the scope of information,\
    \ and to improve the efficiency\n   of data flow through the Encapsulation Layer,\
    \ the payload should,\n   where possible, be transported as received without modification.\"\
    \n"
- title: 4.1.  Avoid Control Plane Interworking
  contents:
  - "4.1.  Avoid Control Plane Interworking\n   This corollary is best understood\
    \ in the context of the integrated\n   solutions space.  In this case, the architecture\
    \ and design\n   frequently achieves the worst of all possible worlds.  This is\
    \ due to\n   the fact that such integrated solutions perform poorly at both ends\n\
    \   of the performance/CAPEX/OPEX spectrum: the protocols with the least\n   switching\
    \ demand may have to bear the cost of the most expensive,\n   while the protocols\
    \ with the most stringent requirements often must\n   make concessions to those\
    \ with different requirements.  Add to this\n   the various control plane interworking\
    \ issues and you have a large\n   opportunity for failure.  In summary, interworking\
    \ functions should\n   be restricted to data plane interworking and encapsulations,\
    \ and\n   these functions should be carried out at the edge of the network.\n\
    \   As described above, interworking models have been successful in those\n  \
    \ cases where there is a kind of \"isomorphism\" between the layers being\n  \
    \ interworked.  The trade-off here, frequently described as the\n   \"Integrated\
    \ vs.  Ships In the Night trade-off\" has been examined at\n   various times and\
    \  at various protocol layers.  In general, there are\n   few cases in which such\
    \ integrated solutions have proven efficient.\n   Multi-protocol BGP [RFC2283]\
    \ is a subtly different but notable\n   exception.  In this case, the control\
    \ plane is  independent of the\n   format of the control data.  That is, no control\
    \ plane data\n   conversion is required, in contrast with control plane interworking\n\
    \   models such as the ATM/IP interworking envisioned by some soft-switch\n  \
    \ manufacturers, and the so-called \"PNNI-MPLS SIN\" interworking\n   [ATMMPLS].\n"
- title: '5.  Packet versus Circuit Switching: Fundamental Differences'
  contents:
  - "5.  Packet versus Circuit Switching: Fundamental Differences\n   Conventional\
    \ wisdom holds that packet switching (PS) is inherently\n   more efficient than\
    \ circuit switching (CS), primarily because of the\n   efficiencies that can be\
    \ gained by statistical multiplexing and the\n   fact that routing and forwarding\
    \ decisions are made independently in\n   a hop-by-hop fashion [[MOLINERO2002].\
    \  Further, it is widely assumed\n   that IP is simpler that circuit switching,\
    \ and hence should be more\n   economical to deploy and manage [MCK2002].  However,\
    \ if one examines\n   these and related assumptions, a different picture emerges\
    \ (see for\n   example [ODLYZKO98]).  The following sections discuss these\n \
    \  assumptions.\n"
- title: 5.1.  Is PS is inherently more efficient than CS?
  contents:
  - "5.1.  Is PS is inherently more efficient than CS?\n   It is well known that packet\
    \ switches make efficient use of scarce\n   bandwidth [BARAN].  This efficiency\
    \ is based on the statistical\n   multiplexing inherent in packet switching. \
    \ However, we continue to\n   be puzzled by what is generally believed to be the\
    \ low utilization of\n   Internet backbones.  The first question we might ask\
    \ is what is the\n   current average utilization of Internet backbones, and how\
    \ does that\n   relate to the utilization of long distance voice networks?  Odlyzko\n\
    \   and Coffman [ODLYZKO,COFFMAN] report that the average utilization of\n   links\
    \ in the IP networks was in the range between 3% and 20%\n   (corporate intranets\
    \ run in the 3% range, while commercial Internet\n   backbones run in the 15-20%\
    \ range).  On the other hand, the average\n   utilization of long haul voice lines\
    \ is about 33%.  In addition, for\n   2002, the average utilization of optical\
    \ networks (all services)\n   appears to be hovering at about 11%, while the historical\
    \ average is\n   approximately 15% [ML2002].  The question then becomes why we\
    \ see\n   such utilization levels, especially in light of the assumption that\n\
    \   PS is inherently more efficient than CS.  The reasons cited by\n   Odlyzko\
    \ and Coffman include:\n      (i).   Internet traffic is extremely asymmetric\
    \ and bursty, but\n             links are symmetric and of fixed capacity (i.e.,\
    \ don't know\n             the traffic matrix, or required link capacities);\n\
    \      (ii).  It is difficult to predict traffic growth on a link, so\n      \
    \       operators tend to add bandwidth aggressively;\n      (iii).  Falling prices\
    \ for coarser bandwidth granularity make it\n             appear more economical\
    \ to add capacity in large increments.\n   Other static factors include protocol\
    \ overhead, other kinds of\n   equipment granularity, restoration capacity, and\
    \ provisioning lag\n   time all contribute to the need to \"over-provision\" [MC2001].\n"
- title: 5.2.  Is PS simpler than CS?
  contents:
  - "5.2.  Is PS simpler than CS?\n   The end-to-end principle can be interpreted\
    \ as stating that the\n   complexity of the Internet belongs at the edges.  However,\
    \ today's\n   Internet backbone routers are extremely complex.  Further, this\n\
    \   complexity scales with line rate.  Since the relative complexity of\n   circuit\
    \ and packet switching seems to have resisted direct analysis,\n   we instead\
    \ examine several artifacts of packet and circuit switching\n   as complexity\
    \ metrics.  Among the metrics we might look at are\n   software complexity, macro\
    \ operation complexity, hardware complexity,\n   power consumption, and density.\
    \  Each of these metrics is considered\n   below.\n"
- title: 5.2.1.  Software/Firmware Complexity
  contents:
  - "5.2.1.  Software/Firmware Complexity\n   One measure of software/firmware complexity\
    \ is the number of\n   instructions required to program the device.  The typical\
    \ software\n   image for an Internet router requires between eight and ten million\n\
    \   instructions (including firmware), whereas a typical transport switch\n  \
    \ requires on average about three million instructions [MCK2002].\n   This difference\
    \ in software complexity has tended to make Internet\n   routers unreliable, and\
    \ has notable other second order effects (e.g.,\n   it may take a long time to\
    \ reboot such a router).  As another point\n   of comparison, consider that the\
    \ AT&T (Lucent) 5ESS class 5 switch,\n   which has a huge number of calling features,\
    \ requires only about\n   twice the number of lines of code as an Internet core\
    \ router [EICK].\n   Finally, since routers are as much or more software than\
    \ hardware\n   devices, another result of the code complexity is that the cost\
    \ of\n   routers benefits less from Moore's Law than less software-intensive\n\
    \   devices.  This causes a bandwidth/device trade-off that favors\n   bandwidth\
    \ more than less software-intensive devices.\n"
- title: 5.2.2.  Macro Operation Complexity
  contents:
  - "5.2.2.  Macro Operation Complexity\n   An Internet router's line card must perform\
    \ many complex operations,\n   including processing the packet header, longest\
    \ prefix match,\n   generating ICMP error messages, processing IP header options,\
    \ and\n   buffering the packet so that TCP congestion control will be effective\n\
    \   (this typically requires a buffer of size proportional to the line\n   rate\
    \ times the RTT, so a buffer will hold around 250 ms of packet\n   data).  This\
    \ doesn't include route and packet filtering, or any QoS\n   or VPN filtering.\n\
    \   On the other hand, a transport switch need only to map ingress time-\n   slots\
    \ to egress time-slots and interfaces, and therefore can be\n   considerably less\
    \ complex.\n"
- title: 5.2.3.  Hardware Complexity
  contents:
  - "5.2.3.  Hardware Complexity\n   One measure of hardware complexity is the number\
    \ of logic gates on a\n   line card [MOLINERO2002].  Consider the case of a high-speed\
    \ Internet\n   router line card: An OC192 POS router line card contains at least\
    \ 30\n   million gates in ASICs, at least one CPU, 300 Mbytes of packet\n   buffers,\
    \ 2 Mbytes of forwarding table, and 10 Mbytes of other\n   state memory.  On the\
    \ other hand, a comparable transport switch line\n   card has 7.5 million logic\
    \ gates, no CPU, no packet buffer, no\n   forwarding table, and an on-chip state\
    \ memory.  Rather, the line-card\n   of an electronic transport switch typically\
    \ contains a SONET framer,\n   a chip to map ingress time-slots to egress time-slots,\
    \ and an\n   interface to the switch fabric.\n"
- title: 5.2.4.  Power
  contents:
  - "5.2.4.  Power\n   Since transport switches have traditionally been built from\
    \ simpler\n   hardware components, they also consume less power [PMC].\n"
- title: 5.2.5.  Density
  contents:
  - "5.2.5.  Density\n   The highest capacity transport switches have about four times\
    \ the\n   capacity of an IP router [CISCO,CIENA], and sell for about one-third\n\
    \   as much per Gigabit/sec.  Optical (OOO) technology pushes this\n   complexity\
    \ difference further (e.g., tunable lasers, MEMs switches.\n   e.g., [CALIENT]),\
    \ and DWDM multiplexers provide technology to build\n   extremely high capacity,\
    \ low power transport switches.\n   A related metric is physical footprint.  In\
    \ general, by virtue of\n   their higher density, transport switches have a smaller\
    \ \"per-gigabit\"\n   physical footprint.\n"
- title: 5.2.6.  Fixed versus variable costs
  contents:
  - "5.2.6.  Fixed versus variable costs\n   Packet switching would seem to have high\
    \ variable cost, meaning that\n   it costs more to send the n-th piece of information\
    \ using packet\n   switching than it might in a circuit switched network.  Much\
    \ of this\n   advantage is due to the relatively static nature of circuit\n  \
    \ switching, e.g., circuit switching can take advantage of of pre-\n   scheduled\
    \ arrival of information to eliminate operations to be\n   performed on incoming\
    \ information.  For example, in the circuit\n   switched case, there is no need\
    \ to buffer incoming information,\n   perform loop detection, resolve next hops,\
    \ modify fields in the\n   packet header, and the like.  Finally, many circuit\
    \ switched networks\n   combine relatively static configuration with out-of-band\
    \ control\n   planes (e.g., SS7), which greatly simplifies data-plane switching.\n\
    \   The bottom line is that as data rates get large, it becomes more and\n   more\
    \ complex to switch packets, while circuit switching scales more\n   or less linearly.\n"
- title: 5.2.7.  QoS
  contents:
  - "5.2.7.  QoS\n   While the components of a complete solution for Internet QoS,\n\
    \   including call admission control, efficient packet classification,\n   and\
    \ scheduling algorithms, have been the subject of extensive\n   research and standardization\
    \ for more than 10 years, end-to-end\n   signaled QoS for the Internet has not\
    \ become a reality.\n   Alternatively, QoS has been part of the circuit switched\n\
    \   infrastructure almost from its inception.  On the other hand, QoS is\n   usually\
    \ deployed to determine queuing disciplines to be used when\n   there is insufficient\
    \ bandwidth to support traffic.  But unlike voice\n   traffic, packet drop or\
    \ severe delay may have a much more serious\n   effect on TCP traffic due to its\
    \ congestion-aware feedback loop (in\n   particular, TCP backoff/slow start).\n"
- title: 5.2.8.  Flexibility
  contents:
  - "5.2.8.  Flexibility\n   A somewhat harder to quantify metric is the inherent\
    \ flexibility of\n   the Internet.  While the Internet's flexibility has led to\
    \ its rapid\n   growth, this flexibility comes with a relatively high cost at\
    \ the\n   edge: the need for highly trained support personnel.  A standard rule\n\
    \   of thumb is that in an enterprise setting, a single support person\n   suffices\
    \ to provide telephone service for a group, while you need ten\n   computer networking\
    \ experts to serve the networking requirements of\n   the same group [ODLYZKO98A].\
    \  This phenomenon is also described in\n   [PERROW].\n"
- title: 5.3.  Relative Complexity
  contents:
  - "5.3.  Relative Complexity\n   The relative computational complexity of circuit\
    \ switching as\n   compared to packet switching has been difficult to describe\
    \ in formal\n   terms [PARK].  As such, the sections above seek to describe the\n\
    \   complexity in terms of observable artifacts.  With this in mind, it\n   is\
    \ clear that the fundamental driver producing the increased\n   complexities outlined\
    \ above is the hop-by-hop independence (HBHI)\n   inherent in the IP architecture.\
    \  This is in contrast to the end to\n   end architectures such as ATM or Frame\
    \ Relay.\n   [WILLINGER2002] describes this phenomenon in terms of the robustness\n\
    \   requirement of the original Internet design, and how this requirement\n  \
    \ has the driven complexity of the network.  In particular, they\n   describe\
    \ a \"complexity/robustness\" spiral, in which increases in\n   complexity create\
    \ further and more serious sensitivities, which then\n   requires additional robustness\
    \ (hence the spiral).\n   The important lesson of this section is that the Simplicity\n\
    \   Principle, while applicable to circuit switching as well as packet\n   switching,\
    \ is crucial in controlling the complexity (and hence OPEX\n   and CAPEX properties)\
    \ of packet networks.  This idea is reinforced by\n   the observation that while\
    \ packet switching is a younger, less mature\n   discipline than circuit switching,\
    \ the trend in packet switches is\n   toward more complex line cards, while the\
    \ complexity of circuit\n   switches appears to be scaling linearly with line\
    \ rates and aggregate\n   capacity.\n"
- title: 5.3.1.  HBHI and the OPEX Challenge
  contents:
  - "5.3.1.  HBHI and the OPEX Challenge\n   As a result of HBHI, we need to approach\
    \ IP networks in a\n   fundamentally different way than we do circuit based networks.\
    \  In\n   particular, the major OPEX challenge faced by the IP network is that\n\
    \   debugging of a large-scale IP network still requires a large degree\n   of\
    \ expertise and understanding, again due to the hop-by-hop\n   independence inherent\
    \ in a packet architecture (again, note that this\n   hop-by-hop independence\
    \ is not present in virtual circuit networks\n   such as ATM or Frame Relay).\
    \  For example, you may have to visit a\n   large set of your routers only to\
    \ discover that the problem is\n   external to your own network.  Further, the\
    \ debugging tools used to\n   diagnose problems are also complex and somewhat\
    \ primitive.  Finally,\n   IP has to deal with people having problems with their\
    \ DNS or their\n   mail or news or some new application, whereas this is usually\
    \ not the\n   case for TDM/ATM/etc.  In the case of IP, this can be eased by\n\
    \   improving automation (note that much of what we mention is customer\n   facing).\
    \  In general, there are many variables external to the\n   network that effect\
    \ OPEX.\n   Finally, it is important to note that the quantitative relationship\n\
    \   between CAPEX, OPEX, and a network's inherent complexity is not well\n   understood.\
    \  In fact, there are no agreed upon and quantitative\n   metrics for describing\
    \ a network's complexity, so a precise\n   relationship between CAPEX, OPEX, and\
    \ complexity remains elusive.\n"
- title: 6.  The Myth of Over-Provisioning
  contents:
  - "6.  The Myth of Over-Provisioning\n   As noted in [MC2001] and elsewhere, much\
    \ of the complexity we observe\n   in today's Internet is directed at increasing\
    \ bandwidth utilization.\n   As a result, the desire of network engineers to keep\
    \ network\n   utilization below 50% has been termed \"over-provisioning\".  However,\n\
    \   this use of the term over-provisioning is a misnomer.  Rather, in\n   modern\
    \ Internet backbones the unused capacity is actually protection\n   capacity.\
    \  In particular, one might view this as \"1:1 protection at\n   the IP layer\"\
    .  Viewed in this way, we see that an IP network\n   provisioned to run at 50%\
    \ utilization is no more over-provisioned\n   than the typical SONET network.\
    \  However, the important advantages\n   that accrue to an IP network provisioned\
    \ in this way include close to\n   speed of light delay and close to zero packet\
    \ loss [FRALEIGH].  These\n   benefits can been seen as a \"side-effect\" of 1:1\
    \ protection\n   provisioning.\n   There are also other, system-theoretic reasons\
    \ for providing 1:1-like\n   protection provisioning.  Most notable among these\
    \ reasons is that\n   packet-switched networks with in-band control loops can\
    \ become\n   unstable and can experience oscillations and synchronization when\n\
    \   congested.  Complex and non-linear dynamic interaction of traffic\n   means\
    \ that congestion in one part of the network will spread to other\n   parts of\
    \ the network.  When routing protocol packets are lost due to\n   congestion or\
    \ route-processor overload, it causes inconsistent\n   routing state, and this\
    \ may result in traffic loops, black holes, and\n   lost connectivity.  Thus,\
    \ while statistical multiplexing can in\n   theory yield higher network utilization,\
    \ in practice, to maintain\n   consistent performance and a reasonably stable\
    \ network, the dynamics\n   of the Internet backbones favor 1:1 provisioning and\
    \ its side effects\n   to keep the network stable and delay low.\n"
- title: 7.  The Myth of Five Nines
  contents:
  - "7.  The Myth of Five Nines\n   Paul Baran, in his classic paper, \"SOME PERSPECTIVES\
    \ ON NETWORKS--\n   PAST, PRESENT AND FUTURE\", stated that \"The tradeoff curves\
    \ between\n   cost and system reliability suggest that the most reliable systems\n\
    \   might be built of relatively unreliable and hence low cost elements,\n   if\
    \ it is system reliability at the lowest overall system cost that is\n   at issue\"\
    \ [BARAN77].\n   Today we refer to this phenomenon as \"the myth of five nines\"\
    .\n   Specifically, so-called five nines reliability in packet network\n   elements\
    \ is consider a myth for the following reasons: First, since\n   80% of unscheduled\
    \ outages are caused by people or process errors\n   [SCOTT], there is only a\
    \ 20% window in which to optimize.  Thus, in\n   order to increase component reliability,\
    \ we add complexity\n   (optimization frequently leads to complexity), which is\
    \ the root\n   cause of 80% of the unplanned outages.  This effectively narrows\
    \ the\n   20% window (i.e., you increase the likelihood of people and process\n\
    \   failure).  This phenomenon is also characterized as a\n   \"complexity/robustness\"\
    \ spiral [WILLINGER2002], in which increases in\n   complexity create further\
    \ and more serious sensitivities, which then\n   requires additional robustness,\
    \ and so on (hence the spiral).\n   The conclusion, then is that while a system\
    \ like the Internet can\n   reach five-nines-like reliability, it is undesirable\
    \ (and likely\n   impossible) to try to make any individual component, especially\
    \ the\n   most complex ones, reach that reliability standard.\n"
- title: 8.  Architectural Component Proportionality Law
  contents:
  - "8.  Architectural Component Proportionality Law\n   As noted in the previous\
    \ section, the computational complexity of\n   packet switched networks such as\
    \ the Internet has proven difficult to\n   describe in formal terms.  However,\
    \ an intuitive, high level\n   definition of architectural complexity might be\
    \ that the complexity\n   of an architecture is proportional to its number of\
    \ components, and\n   that the probability of achieving a stable implementation\
    \ of an\n   architecture is inversely proportional to its number of components.\n\
    \   As described above, components include discrete elements such as\n   hardware\
    \ elements, space and power requirements, as well as software,\n   firmware, and\
    \ the protocols they implement.\n   Stated more abstractly:\n       Let\n    \
    \     A   be a representation of architecture A,\n         |A| be number of distinct\
    \ components in the service\n             delivery path of architecture A,\n \
    \        w   be a monotonically increasing function,\n         P   be the probability\
    \ of a stable implementation of an\n             architecture, and let\n     \
    \  Then\n         Complexity(A) = O(w(|A|))\n         P(A)          = O(1/w(|A|))\n\
    \       where\n       O(f) = {g:N->R | there exists c > 0 and n such that g(n)\n\
    \       < c*f(n)}\n       [That is, O(f) comprises the set of functions g for\
    \ which\n       there exists a constant c and a number n, such that g(n) is\n\
    \       smaller or equal to c*f(n) for all n. That is, O(f) is the\n       set\
    \ of all functions that do not grow faster than f,\n       disregarding constant\
    \ factors]\n   Interestingly, the Highly Optimized Tolerance (HOT) model [HOT]\n\
    \   attempts to characterize complexity in general terms (HOT is one\n   recent\
    \ attempt to develop a general framework for the study of\n   complexity, and\
    \ is a member of a family of abstractions generally\n   termed \"the new science\
    \ of complexity\" or \"complex adaptive\n   systems\").  Tolerance, in HOT semantics,\
    \ means that \"robustness in\n   complex systems is a constrained and limited\
    \ quantity that must be\n   carefully managed and protected.\" One focus of the\
    \ HOT model is to\n   characterize heavy-tailed distributions such as Complexity(A)\
    \ in the\n   above example (other examples include forest fires, power outages,\n\
    \   and Internet traffic distributions).  In particular, Complexity(A)\n   attempts\
    \ to map the extreme heterogeneity of the parts of the system\n   (Internet),\
    \ and the effect of their organization into highly\n   structured networks, with\
    \ hierarchies and multiple scales.\n"
- title: 8.1.  Service Delivery Paths
  contents:
  - "8.1.  Service Delivery Paths\n   The Architectural Component Proportionality\
    \ Law (ACPL) states that\n   the complexity of an architecture is proportional\
    \ to its number of\n   components.\n   COROLLARY: Minimize the number of components\
    \ in a service delivery\n   path, where the service delivery path can be a protocol\
    \ path, a\n   software path, or a physical path.\n   This corollary is an important\
    \ consequence of the ACPL, as the path\n   between a customer and the desired\
    \ service is particularly sensitive\n   to the number and complexity of elements\
    \ in the path.  This is due to\n   the fact that the complexity \"smoothing\"\
    \ that we find at high levels\n   of aggregation [ZHANG] is missing as you move\
    \ closer to the edge, as\n   well as having complex interactions with backoffice\
    \ and CRM systems.\n   Examples of architectures that haven't found a market due\
    \ to this\n   effect include TINA-based CRM systems, CORBA/TINA based service\n\
    \   architectures.  The basic lesson here was that the only possibilities\n  \
    \ for deploying these systems were \"Limited scale deployments (such) as\n   in\
    \ Starvision can avoid coping with major unproven scalability\n   issues\", or\
    \ \"Otherwise need massive investments (like the carrier-\n   grade ORB built\
    \ almost from scratch)\" [TINA].  In other words, these\n   systems had complex\
    \ service delivery paths, and were too complex to\n   be feasibly deployed.\n"
- title: 9.  Conclusions
  contents:
  - "9.  Conclusions\n   This document attempts to codify long-understood Internet\n\
    \   architectural principles.  In particular, the unifying principle\n   described\
    \ here is best expressed by the Simplicity Principle, which\n   states complexity\
    \ must be controlled if one hopes to efficiently\n   scale a complex object. \
    \ The idea that simplicity itself can lead to\n   some form of optimality has\
    \ been a common theme throughout history,\n   and has been stated in many other\
    \ ways and along many dimensions.\n   For example, consider the maxim known as\
    \ Occam's Razor, which was\n   formulated by the medieval English philosopher\
    \ and Franciscan monk\n   William of Ockham (ca. 1285-1349), and states \"Pluralitas\
    \ non est\n   ponenda sine neccesitate\" or \"plurality should not be posited\
    \ without\n   necessity.\" (hence Occam's Razor is sometimes called \"the principle\n\
    \   of unnecessary plurality\" and \" the principle of simplicity\").  A\n   perhaps\
    \ more contemporary formulation of Occam's Razor states that\n   the simplest\
    \ explanation for a phenomenon is the one preferred by\n   nature.  Other formulations\
    \ of the same  idea can be found in the\n   KISS (Keep It Simple Stupid) principle\
    \ and the Principle of Least\n   Astonishment (the assertion that the most usable\
    \ system is the one\n   that least often leaves users astonished).  [WILLINGER2002]\
    \ provides\n   a more theoretical discussion of \"robustness through simplicity\"\
    , and\n   in discussing the PSTN, [KUHN87] states that in most systems, \"a\n\
    \   trade-off can be made between simplicity of interactions and\n   looseness\
    \ of coupling\".\n   When applied to packet switched network architectures, the\
    \ Simplicity\n   Principle has implications that some may consider heresy, e.g.,\
    \ that\n   highly converged approaches are likely to be less efficient than\n\
    \   \"less converged\" solutions.  Otherwise stated, the \"optimal\"\n   convergence\
    \ layer may be much lower in the protocol stack that is\n   conventionally believed.\
    \  In addition, the analysis above leads to\n   several conclusions that are contrary\
    \ to the conventional wisdom\n   surrounding  packet networking.  Perhaps most\
    \ significant is the\n   belief that packet switching is simpler than circuit\
    \ switching.  This\n   belief has lead to conclusions such as \"since packet is\
    \ simpler than\n   circuit, it must cost less to operate\".  This study finds\
    \ to the\n   contrary.  In particular, by examining the metrics described above,\n\
    \   we find that packet switching is more complex than circuit switching.\n  \
    \ Interestingly, this conclusion is borne out by the fact that\n   normalized\
    \ OPEX for data networks is typically significantly greater\n   than for voice\
    \ networks [ML2002].\n   Finally, the important conclusion of this work is that\
    \ for packet\n   networks that are of the scale of today's Internet or larger,\
    \ we must\n   strive for the simplest possible solutions if we hope to build cost\n\
    \   effective infrastructures.  This idea is eloquently stated in\n   [DOYLE2002]:\
    \ \"The evolution of protocols can lead to a\n   robustness/complexity/fragility\
    \ spiral where complexity added for\n   robustness also adds new fragilities,\
    \ which in turn leads to new and\n   thus spiraling complexities\".  This is exactly\
    \ the phenomenon that\n   the Simplicity Principle is designed to avoid.\n"
- title: 10.  Security Considerations
  contents:
  - "10.  Security Considerations\n   This document does not directly effect the security\
    \ of any existing\n   Internet protocol.  However, adherence to the Simplicity\
    \ Principle\n   does have a direct affect on our ability to implement secure systems.\n\
    \   In particular, a system's complexity grows, it becomes  more\n   difficult\
    \ to model and analyze, and hence it becomes more difficult\n   to find and understand\
    \ the security implications inherent in its\n   architecture, design, and implementation.\n"
- title: 11.  Acknowledgments
  contents:
  - "11.  Acknowledgments\n   Many of the ideas for comparing the complexity of circuit\
    \ switched\n   and packet switched networks were inspired by conversations with\
    \ Nick\n   McKeown.  Scott Bradner, David Banister, Steve Bellovin, Steward\n\
    \   Bryant, Christophe Diot, Susan Harris, Ananth Nagarajan, Andrew\n   Odlyzko,\
    \ Pete and Natalie Whiting, and Lixia Zhang made many helpful\n   comments on\
    \ early drafts of this document.\n"
- title: 12.  References
  contents:
  - "12.  References\n   [AHUJA]         \"The Impact of Internet Policy and Topology\
    \ on\n                   Delayed Routing Convergence\", Labovitz, et. al.\n  \
    \                 Infocom, 2001.\n   [ATMMPLS]       \"ATM-MPLS Interworking Migration\
    \ Complexities Issues\n                   and Preliminary Assessment\", School\
    \ of\n                   Interdisciplinary Computing and Engineering,\n      \
    \             University of Missouri-Kansas City, April 2002\n   [BARAN]     \
    \    \"On Distributed Communications\", Paul Baran, Rand\n                   Corporation\
    \ Memorandum RM-3420-PR,\n                   http://www.rand.org/publications/RM/RM3420\"\
    , August,\n                   1964.\n   [BARAN77]       \"SOME PERSPECTIVES ON\
    \ NETWORKS--PAST, PRESENT AND\n                   FUTURE\", Paul Baran,  Information\
    \ Processing 77,\n                   North-Holland Publishing Company, 1977,\n\
    \   [BRYANT]        \"Protocol Layering in PWE3\", Bryant et al, Work in\n   \
    \                Progress.\n   [CAIDA]         http://www.caida.org\n   [CALLIENT]\
    \      http://www.calient.net/home.html\n   [CARLSON]       \"Complexity and Robustness\"\
    , J.M. Carlson and John\n                   Doyle, Proc. Natl. Acad. Sci. USA,\
    \ Vol. 99, Suppl. 1,\n                   2538-2545, February 19, 2002.\n     \
    \              http://www.pnas.org/cgi/doi/10.1073/pnas.012582499\n   [CIENA]\
    \         \"CIENA Multiwave CoreDiretor\",\n                   http://www.ciena.com/downloads/products/\n\
    \                   coredirector.pdf\n   [CISCO]         http://www.cisco.com\n\
    \   [CLARK]         \"The Design Philosophy of the DARPA Internet\n          \
    \         Protocols\", D. Clark, Proc. of the ACM SIGCOMM, 1988.\n   [COFFMAN]\
    \       \"Internet Growth: Is there a 'Moores Law' for Data\n                \
    \   Traffic\", K.G. Coffman and A.M. Odlyzko, pp. 47-93,\n                   Handbook\
    \ of Massive Data Stes, J. Elli, P. M.\n                   Pardalos, and M. G.\
    \ C. Resende, Editors. Kluwer,\n                   2002.\n   [DOYLE2002]     \"\
    Robustness and the Internet: Theoretical\n                   Foundations\", John\
    \ C. Doyle, et. al. Work in\n                   Progress.\n   [EICK]         \
    \ \"Visualizing Software Changes\", S.G. Eick, et al,\n                   National\
    \ Institute of Statistical Sciences, Technical\n                   Report 113,\
    \ December 2000.\n   [MOLINERO2002]  \"TCP Switching: Exposing Circuits to IP\"\
    , Pablo\n                   Molinero-Fernandez and Nick McKeown, IEEE January,\n\
    \                   2002.\n   [FLOYD]         \"The Synchronization of Periodic\
    \ Routing Messages\",\n                   Sally Floyd and Van Jacobson, IEEE ACM\
    \ Transactions\n                   on Networking, 1994.\n   [FLOYD2001]     \"\
    A Report on Some Recent Developments in TCP\n                   Congestion Control,\
    \ IEEE Communications Magazine, S.\n                   Floyd, April 2001.\n  \
    \ [FRALEIGH]      \"Provisioning IP Backbone Networks to Support Delay-\n    \
    \               Based Service Level Agreements\", Chuck Fraleigh,\n          \
    \         Fouad Tobagi, and Christophe Diot, 2002.\n   [GRIFFIN]       \"What\
    \ is the Sound of One Route Flapping\", Timothy G.\n                   Griffin,\
    \  IPAM Workshop on Large-Scale Communication\n                   Networks: Topology,\
    \ Routing, Traffic, and Control,\n                   March, 2002.\n   [HANDLEY]\
    \       \"On Inter-layer Assumptions (A view from the\n                   Transport\
    \ Area), slides from a presentation at the\n                   IAB workshop on\
    \ Wireless Internetworking\", M.\n                   Handley,  March 2000.\n \
    \  [HOT]           J.M. Carlson and John Doyle, Phys. Rev. E 60, 1412-\n     \
    \              1427, 1999.\n   [ISO10589]      \"Intermediate System to Intermediate\
    \ System\n                   Intradomain Routing Exchange Protocol (IS-IS)\".\n\
    \   [JACOBSON]      \"Congestion Avoidance and Control\", Van Jacobson,\n    \
    \               Proceedings of ACM Sigcomm 1988, pp. 273-288.\n   [KARN]     \
    \     \"TCP vs Link Layer Retransmission\" in P. Karn et al.,\n              \
    \     Advice for Internet Subnetwork Designers, Work in\n                   Progress.\n\
    \   [KUHN87]        \"Sources of Failure in the Public Switched Telephone\n  \
    \                 Network\", D. Richard Kuhn, EEE Computer, Vol. 30, No.\n   \
    \                4, April, 1997.\n   [L2TPV3]        Lan, J., et. al., \"Layer\
    \ Two Tunneling Protocol\n                   (Version 3) -- L2TPv3\", Work in\
    \ Progress.\n   [MC2001]        \"U.S Communications Infrastructure at A Crossroads:\n\
    \                   Opportunities Amid the Gloom\", McKinsey&Company for\n   \
    \                Goldman-Sachs, August 2001.\n   [MCK2002]       Nick McKeown,\
    \ personal communication, April, 2002.\n   [ML2002]        \"Optical Systems\"\
    , Merril Lynch Technical Report,\n                   April, 2002.\n   [NAVE] \
    \         \"The influence of mode coupling on the non-linear\n               \
    \    evolution of tearing modes\", M.F.F. Nave, et al, Eur.\n                \
    \   Phys. J. D 8, 287-297.\n   [NEUMANN]       \"Cause of AT&T network failure\"\
    , Peter G. Neumann,\n                   http://catless.ncl.ac.uk/Risks/9.62.html#subj2\n\
    \   [ODLYZKO]       \"Data networks are mostly empty for good reason\",\n    \
    \               A.M. Odlyzko, IT Professional 1 (no. 2), pp. 67-69,\n        \
    \           Mar/Apr 1999.\n   [ODLYZKO98A]    \"Smart and stupid networks: Why\
    \ the Internet is like\n                   Microsoft\".  A. M. Odlyzko, ACM Networker,\
    \ 2(5),\n                   December, 1998.\n   [ODLYZKO98]     \"The economics\
    \ of the Internet: Utility, utilization,\n                   pricing, and Quality\
    \ of Service\", A.M. Odlyzko, July,\n                   1998.\n              \
    \     http://www.dtc.umn.edu/~odlyzko/doc/networks.html\n   [PARK]          \"\
    The Internet as a Complex System: Scaling,\n                   Complexity and\
    \ Control\", Kihong Park and Walter\n                   Willinger, AT&T Research,\
    \ 2002.\n   [PERROW]        \"Normal Accidents: Living with High Risk\n      \
    \             Technologies\", Basic Books, C. Perrow, New York,\n            \
    \       1984.\n   [PMC]           \"The Design of a 10 Gigabit Core Router\n \
    \                  Architecture\", PMC-Sierra, http://www.pmc-\n             \
    \      sierra.com/products/diagrams/CoreRouter_lg.html\n   [RFC1629]       Colella,\
    \ R., Callon, R., Gardner, E. and Y. Rekhter,\n                   \"Guidelines\
    \ for OSI NSAP Allocation in the Internet\",\n                   RFC 1629, May\
    \ 1994.\n   [RFC1925]       Callon, R., \"The Twelve Networking Truths\", RFC\
    \ 1925,\n                   1 April 1996.\n   [RFC1958]       Carpenter, B., Ed.,\
    \ \"Architectural principles of the\n                   Internet\", RFC 1958,\
    \ June 1996.\n   [RFC2283]       Bates, T., Chandra, R., Katz, D. and Y. Rekhter,\n\
    \                   \"Multiprotocol Extensions for BGP4\", RFC 2283,\n       \
    \            February 1998.\n   [RFC3155]       Dawkins, S., Montenegro, G., Kojo,\
    \ M. and N. Vaidya,\n                   \"End-to-end Performance Implications\
    \ of Links with\n                   Errors\", BCP 50, RFC 3155, May 2001.\n  \
    \ [ROMANOV]       \"Dynamics of TCP over ATM Networks\", A. Romanov, S.\n    \
    \               Floyd, IEEE JSAC, vol. 13, No 4, pp.633-641, May\n           \
    \        1995.\n   [SALTZER]       \"End-To-End Arguments in System Design\",\
    \ J.H.\n                   Saltzer, D.P. Reed, and D.D. Clark, ACM TOCS, Vol 2,\n\
    \                   Number 4, November 1984, pp 277-288.\n   [SCOTT]         \"\
    Making Smart Investments to Reduce Unplanned\n                   Downtime\", D.\
    \ Scott, Tactical Guidelines, TG-07-4033,\n                   Gartner Group Research\
    \ Note, March 1999.\n   [SPILLMAN]      \"The Law of Diminishing Returns:, W.\
    \ J. Spillman and\n                   E. Lang, 1924.\n   [STALLINGS]     \"Data\
    \ and Computer Communications (2nd Ed)\", William\n                   Stallings,\
    \ Maxwell Macmillan, 1989.\n   [TENNENHOUSE]   \"Layered multiplexing considered\
    \ harmful\", D.\n                   Tennenhouse, Proceedings of the IFIP Workshop\
    \ on\n                   Protocols for High-Speed Networks, Rudin ed., North\n\
    \                   Holland Publishers, May 1989.\n   [THOMPSON]      \"Nonlinear\
    \ Dynamics and Chaos\". J.M.T. Thompson and\n                   H.B. Stewart,\
    \ John Wiley and Sons, 1994, ISBN\n                   0471909602.\n   [TINA] \
    \         \"What is TINA and is it useful for the TelCos?\",\n               \
    \    Paolo Coppo, Carlo A. Licciardi, CSELT, EURESCOM\n                   Participants\
    \ in P847 (FT, IT, NT, TI)\n   [WAKEMAN]       \"Layering considered harmful\"\
    , Ian Wakeman, Jon\n                   Crowcroft, Zheng Wang, and Dejan Sirovica,\
    \ IEEE\n                   Network, January 1992, p. 7-16.\n   [WARD]        \
    \  \"Custom fluorescent-nucleotide synthesis as an\n                   alternative\
    \ method for nucleic acid labeling\",\n                   Octavian Henegariu*,\
    \ Patricia Bray-Ward and David C.\n                   Ward, Nature Biotech 18:345-348\
    \ (2000).\n   [WILLINGER2002] \"Robustness and the Internet: Design and evolution\"\
    ,\n                   Walter Willinger and John Doyle, 2002.\n   [ZHANG]     \
    \    \"Impact of Aggregation on Scaling Behavior of\n                   Internet\
    \ Backbone Traffic\", Sprint ATL Technical\n                   Report TR02-ATL-020157\
    \ Zhi-Li Zhang, Vinay Ribeiroj,\n                   Sue Moon, Christophe Diot,\
    \ February, 2002.\n"
- title: 13.  Authors' Addresses
  contents:
  - "13.  Authors' Addresses\n   Randy Bush\n   EMail: randy@psg.com\n   David Meyer\n\
    \   EMail: dmm@maoz.com\n"
- title: 14.  Full Copyright Statement
  contents:
  - "14.  Full Copyright Statement\n   Copyright (C) The Internet Society (2002).\
    \  All Rights Reserved.\n   This document and translations of it may be copied\
    \ and furnished to\n   others, and derivative works that comment on or otherwise\
    \ explain it\n   or assist in its implementation may be prepared, copied, published\n\
    \   and distributed, in whole or in part, without restriction of any\n   kind,\
    \ provided that the above copyright notice and this paragraph are\n   included\
    \ on all such copies and derivative works.  However, this\n   document itself\
    \ may not be modified in any way, such as by removing\n   the copyright notice\
    \ or references to the Internet Society or other\n   Internet organizations, except\
    \ as needed for the purpose of\n   developing Internet standards in which case\
    \ the procedures for\n   copyrights defined in the Internet Standards process\
    \ must be\n   followed, or as required to translate it into languages other than\n\
    \   English.\n   The limited permissions granted above are perpetual and will\
    \ not be\n   revoked by the Internet Society or its successors or assigns.\n \
    \  This document and the information contained herein is provided on an\n   \"\
    AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE\
    \ DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED\
    \ TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE\
    \ ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A\
    \ PARTICULAR PURPOSE.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
