- title: __initial_text__
  contents:
  - '                NFSv4.0 Migration: Specification Update

    '
- title: Abstract
  contents:
  - "Abstract\n   The migration feature of NFSv4 allows the transfer of responsibility\n\
    \   for a single file system from one server to another without\n   disruption\
    \ to clients.  Recent implementation experience has shown\n   problems in the\
    \ existing specification for this feature in NFSv4.0.\n   This document identifies\
    \ the problem areas and provides revised\n   specification text that updates the\
    \ NFSv4.0 specification in RFC\n   7530.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This\
    \ document is a product of the Internet Engineering Task Force\n   (IETF).  It\
    \ represents the consensus of the IETF community.  It has\n   received public\
    \ review and has been approved for publication by the\n   Internet Engineering\
    \ Steering Group (IESG).  Further information on\n   Internet Standards is available\
    \ in Section 2 of RFC 7841.\n   Information about the current status of this document,\
    \ any errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc7931.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2016 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n   2.  Conventions . . . . . . . . . . . . . . . . . . . . .\
    \ . . . .   3\n   3.  Definitions . . . . . . . . . . . . . . . . . . . . . .\
    \ . . .   3\n     3.1.  Terminology . . . . . . . . . . . . . . . . . . . . .\
    \ . .   3\n     3.2.  Data Type Definitions . . . . . . . . . . . . . . . . .\
    \ .   5\n   4.  Background  . . . . . . . . . . . . . . . . . . . . . . . . .\
    \   5\n   5.  Client Identity Definition  . . . . . . . . . . . . . . . . .  \
    \ 7\n     5.1.  Differences from Replaced Sections  . . . . . . . . . . .   7\n\
    \     5.2.  Client Identity Data Items  . . . . . . . . . . . . . . .   8\n  \
    \     5.2.1.  Client Identity Structure . . . . . . . . . . . . . .   9\n    \
    \   5.2.2.  Client Identity Shorthand . . . . . . . . . . . . . .  11\n     5.3.\
    \  Server Release of Client ID . . . . . . . . . . . . . . .  13\n     5.4.  Client\
    \ ID String Approaches . . . . . . . . . . . . . . .  14\n     5.5.  Non-uniform\
    \ Client ID String Approach . . . . . . . . . .  16\n     5.6.  Uniform Client\
    \ ID String Approach . . . . . . . . . . . .  16\n     5.7.  Mixing Client ID\
    \ String Approaches  . . . . . . . . . . .  18\n     5.8.  Trunking Determination\
    \ when Using Uniform Client ID\n           Strings . . . . . . . . . . . . . .\
    \ . . . . . . . . . . .  20\n     5.9.  Client ID String Construction Details\
    \ . . . . . . . . . .  26\n   6.  Locking and Multi-Server Namespace  . . . .\
    \ . . . . . . . . .  28\n     6.1.  Lock State and File System Transitions  .\
    \ . . . . . . . .  28\n       6.1.1.  Migration and State . . . . . . . . . .\
    \ . . . . . . .  29\n         6.1.1.1.  Migration and Client IDs  . . . . . .\
    \ . . . . . .  31\n         6.1.1.2.  Migration and State Owner Information .\
    \ . . . . .  32\n       6.1.2.  Replication and State . . . . . . . . . . . .\
    \ . . . .  36\n       6.1.3.  Notification of Migrated Lease  . . . . . . . .\
    \ . . .  36\n       6.1.4.  Migration and the lease_time Attribute  . . . . .\
    \ . .  39\n   7.  Server Implementation Considerations  . . . . . . . . . . .\
    \ .  39\n     7.1.  Relation of Locking State Transfer to Other Aspects of\n \
    \          File System Motion  . . . . . . . . . . . . . . . . . . .  39\n   \
    \  7.2.  Preventing Locking State Modification during Transfer . .  41\n   8.\
    \  Additional Changes  . . . . . . . . . . . . . . . . . . . . .  44\n     8.1.\
    \  Summary of Additional Changes from Previous Documents . .  45\n     8.2.  NFS4ERR_CLID_INUSE\
    \ Definition . . . . . . . . . . . . . .  45\n     8.3.  NFS4ERR_DELAY Return\
    \ from RELEASE_LOCKOWNER . . . . . . .  45\n     8.4.  Operation 35: SETCLIENTID\
    \ -- Negotiate Client ID  . . . .  46\n     8.5.  Security Considerations for\
    \ Inter-server Information\n           Transfer  . . . . . . . . . . . . . . .\
    \ . . . . . . . . .  51\n     8.6.  Security Considerations Revision  . . . .\
    \ . . . . . . . .  51\n   9.  Security Considerations . . . . . . . . . . . .\
    \ . . . . . . .  52\n   10. References  . . . . . . . . . . . . . . . . . . .\
    \ . . . . . .  52\n     10.1.  Normative References . . . . . . . . . . . . .\
    \ . . . . .  52\n     10.2.  Informative References . . . . . . . . . . . . .\
    \ . . . .  52\n   Acknowledgements  . . . . . . . . . . . . . . . . . . . . .\
    \ . . .  53\n   Authors' Addresses  . . . . . . . . . . . . . . . . . . . . .\
    \ . .  54\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This Standards Track document corrects the existing definitive\n\
    \   specification of the NFSv4.0 protocol described in [RFC7530].  Given\n   this\
    \ fact, one should take the current document into account when\n   learning about\
    \ NFSv4.0, particularly if one is concerned with issues\n   that relate to:\n\
    \   o  File system migration, particularly when it involves transparent\n    \
    \  state migration.\n   o  The construction and interpretation of the nfs_client_id4\n\
    \      structure and particularly the requirements on the id string\n      within\
    \ it, referred to below as a \"client ID string\".\n"
- title: 2.  Conventions
  contents:
  - "2.  Conventions\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\"\
    , \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and\
    \ \"OPTIONAL\" in this\n   document are to be interpreted as described in [RFC2119].\n"
- title: 3.  Definitions
  contents:
  - '3.  Definitions

    '
- title: 3.1.  Terminology
  contents:
  - "3.1.  Terminology\n   The following definitions are included to provide an appropriate\n\
    \   context for the reader.  This section is derived from Section 1.5 of\n   [RFC7530]\
    \ but has been adapted to the needs of this document.\n   Boot Instance Id:  A\
    \ boot instance id is an identifier, such as a\n      boot time, allowing two\
    \ different instances of the same client to\n      be reliably distinguished.\
    \  A boot instance id is opaque to the\n      server and is often used as the\
    \ verifier field in the\n      nfs_client_id4 structure, which identifies the\
    \ client to the\n      server.\n   Client:  A client is an entity that accesses\
    \ the NFS server's\n      resources.  The client may be an application that contains\
    \ the\n      logic to access the NFS server directly.  The client may also be\n\
    \      the traditional operating system client that provides remote file\n   \
    \   system services for a set of applications.\n      With reference to byte-range\
    \ locking, the client is also the\n      entity that maintains a set of locks\
    \ on behalf of one or more\n      applications.  This client is responsible for\
    \ crash or failure\n      recovery for those locks it manages.\n      Note that\
    \ multiple clients may share the same transport and\n      connection, and multiple\
    \ clients may exist on the same network\n      node.\n   Client ID:  A client\
    \ ID is a 64-bit quantity (in the form of a\n      clientid4) used as a unique,\
    \ shorthand reference to a particular\n      client instance, identified by a\
    \ client-supplied verifier (in the\n      form of a boot instance id) and client\
    \ ID string.  The server is\n      responsible for supplying the client ID.\n\
    \   File System:  A file system is the collection of objects on a server\n   \
    \   that share the same fsid attribute (see Section 5.8.1.9 of\n      [RFC7530]).\n\
    \   Grace Period:  A grace period is an interval of time during which the\n  \
    \    server will only grant locking requests to reclaim existing locks\n     \
    \ but not those that create new locks.  This gives clients an\n      opportunity\
    \ to re-establish locking state in response to a\n      potentially disruptive\
    \ event.  The grace period may be general to\n      help deal with server reboot,\
    \ or it may be specific to a file\n      system to deal with file system migration\
    \ when transparent state\n      migration is not provided.\n   Lease:  A lease\
    \ is an interval of time defined by the server for\n      which the client is\
    \ irrevocably granted a lock.  At the end of a\n      lease period, the lock may\
    \ be revoked if the lease has not been\n      extended.  The lock must be revoked\
    \ if a conflicting lock has been\n      granted after the lease interval.\n  \
    \    All leases granted by a server have the same fixed duration.  Note\n    \
    \  that the fixed interval duration was chosen to alleviate the\n      expense\
    \ a server would have in maintaining state about variable-\n      length leases\
    \ across server failures.\n   Lock:  The term \"lock\" is used to refer to record\
    \ (byte-range) locks\n      as well as share reservations unless specifically\
    \ stated\n      otherwise.\n   Lock-Owner:  Each byte-range lock is associated\
    \ with a specific lock-\n      owner and an open-owner.  The lock-owner consists\
    \ of a client ID\n      and an opaque owner string.  The client presents this\
    \ to the\n      server to establish the ownership of the byte-range lock as\n\
    \      needed.\n   Open-Owner:  Each open file is associated with a specific open-owner,\n\
    \      which consists of a client ID and an opaque owner string.  The\n      client\
    \ presents this to the server to establish the ownership of\n      the open as\
    \ needed.\n   Server:  A server is an entity responsible for coordinating client\n\
    \      access to a set of file systems.\n   Stateid:  A stateid is a 128-bit quantity\
    \ returned by a server that\n      uniquely identifies the open and locking states\
    \ provided by the\n      server for a specific open-owner or lock-owner/open-owner\
    \ pair for\n      a specific file and type of lock.\n   Trunking:  A situation\
    \ in which multiple physical addresses are\n      connected to the same logical\
    \ server.\n   Verifier:  A verifier is a quantity, in the form of a verifier4,\
    \ that\n      allows one party to an interaction to be aware of a\n      reinitialization\
    \ or other significant change to the state of the\n      other party.  In [RFC7530],\
    \ this term most often designates the\n      verifier field of an nfs_client_id4,\
    \ in which a boot instance id\n      is placed to allow the server to determine\
    \ when there has been a\n      client reboot, making it necessary to eliminate\
    \ locking state\n      associated with the previous instance of the same client.\n"
- title: 3.2.  Data Type Definitions
  contents:
  - "3.2.  Data Type Definitions\n   This section contains a table that shows where\
    \ data types referred to\n   in this document are defined.\n           +-----------------+--------------------------------+\n\
    \           | Item            | Section                        |\n           +-----------------+--------------------------------+\n\
    \           | cb_client4      | Section 2.2.11 in [RFC7530]    |\n           |\
    \ clientaddr4     | Section 2.2.10 in [RFC7530]    |\n           | clientid4 \
    \      | Section 2.1 in [RFC7530]       |\n           | lock_owner4     | Section\
    \ 2.2.14 in [RFC7530]    |\n           | nfs_client_id4  | Section 5.2.1 (this\
    \ document)  |\n           | open_owner4     | Section 2.2.13 in [RFC7530]   \
    \ |\n           | verifier4       | Section 2.1 in [RFC7530]       |\n       \
    \    +-----------------+--------------------------------+\n"
- title: 4.  Background
  contents:
  - "4.  Background\n   Implementation experience with transparent state migration\
    \ has\n   exposed a number of problems with the then existing specifications of\n\
    \   this feature in [RFC7530] and predecessors.  The symptoms were:\n   o  After\
    \ migration of a file system, a reboot of the associated\n      client was not\
    \ appropriately dealt with, in that the state\n      associated with the rebooting\
    \ client was not promptly freed.\n   o  Situations can arise whereby a given server\
    \ has multiple leases\n      with the same nfs_client_id4 (consisting of id and\
    \ verifier\n      fields), when the protocol clearly assumes there can be only\
    \ one.\n   o  Excessive client implementation complexity since clients have to\n\
    \      deal with situations in which a single client can wind up with its\n  \
    \    locking state with a given server divided among multiple leases\n      each\
    \ with its own clientid4.\n   An analysis of these symptoms leads to the conclusion\
    \ that existing\n   specifications have erred.  They assume that locking state,\
    \ including\n   both state ids and clientid4s, should be transferred as part of\n\
    \   transparent state migration.  The troubling symptoms arise from the\n   failure\
    \ to describe how migrating state is to be integrated with\n   existing client\
    \ definition structures on the destination server.\n   The need for the server\
    \ to appropriately merge stateids associated\n   with a common client boot instance\
    \ encounters a difficult problem.\n   The issue is that the common client practice\
    \ with regard to the\n   presentation of unique strings specifying client identity\
    \ makes it\n   essentially impossible for the client to determine whether or not\
    \ two\n   stateids, originally generated on different servers, are referable to\n\
    \   the same client.  This practice is allowed and endorsed by the\n   existing\
    \ NFSv4.0 specification [RFC7530].\n   However, upon the prototyping of clients\
    \ implementing an alternative\n   approach, it has been found that there exist\
    \ servers that do not work\n   well with these new clients.  It appears that current\
    \ circumstances,\n   in which a particular client implementation pattern had been\
    \ adopted\n   universally, have resulted in some servers not being able to\n \
    \  interoperate against alternate client implementation patterns.  As a\n   result,\
    \ we have a situation that requires careful attention to\n   untangling compatibility\
    \ issues.\n   This document updates the existing NFSv4.0 specification [RFC7530]\
    \ as\n   follows:\n   o  It makes clear that NFSv4.0 supports multiple approaches\
    \ to the\n      construction of client ID strings, including those formerly\n\
    \      endorsed by existing NFSV4.0 specifications and those currently\n     \
    \ being widely deployed.\n   o  It explains how clients can effectively use client\
    \ ID strings that\n      are presented to multiple servers.\n   o  It addresses\
    \ the potential compatibility issues that might arise\n      for clients adopting\
    \ a previously non-favored client ID string\n      construction approach including\
    \ the existence of servers that have\n      problems with the new approach.\n\
    \   o  It gives some guidance regarding the factors that might govern\n      clients'\
    \ choice of a client ID string construction approach and\n      recommends that\
    \ clients construct client ID strings in a manner\n      that supports lease merger\
    \ if they intend to support transparent\n      state migration.\n   o  It specifies\
    \ how state is to be transparently migrated, including\n      defining how state\
    \ that arrives at a new server as part of\n      migration is to be merged into\
    \ existing leases for clients\n      connected to the target server.\n   o  It\
    \ makes further clarifications and corrections to address cases\n      where the\
    \ specification text does not take proper account of the\n      issues raised\
    \ by state migration or where it has been found that\n      the existing text\
    \ is insufficiently clear.  This includes a\n      revised definition of the SETCLIENTID\
    \ operation in Section 8.4,\n      which replaces Section 16.33 in [RFC7530].\n\
    \   For a more complete explanation of the choices made in addressing\n   these\
    \ issues, see [INFO-MIGR].\n"
- title: 5.  Client Identity Definition
  contents:
  - "5.  Client Identity Definition\n   This section is a replacement for Sections\
    \ 9.1.1 and 9.1.2 in\n   [RFC7530].  The replaced sections are named \"Client\
    \ ID\" and \"Server\n   Release of Client ID\", respectively.\n   It supersedes\
    \ the replaced sections.\n"
- title: 5.1.  Differences from Replaced Sections
  contents:
  - "5.1.  Differences from Replaced Sections\n   Because of the need for greater\
    \ attention to and careful description\n   of this area, this section is much\
    \ larger than the sections it\n   replaces.  The principal changes/additions made\
    \ by this section are:\n   o  It corrects inconsistencies regarding the possible\
    \ role or non-\n      role of the client IP address in construction of client\
    \ ID\n      strings.\n   o  It clearly addresses the need to maintain a non-volatile\
    \ record\n      across reboots of client ID strings or any changeable values that\n\
    \      are used in their construction.\n   o  It provides a more complete description\
    \ of circumstances leading\n      to clientid4 invalidity and the appropriate\
    \ recovery actions.\n   o  It presents, as valid alternatives, two approaches\
    \ to client ID\n      string construction (named \"uniform\" and \"non-uniform\"\
    ) and gives\n      some implementation guidance to help implementers choose one\
    \ or\n      the other of these.\n   o  It adds a discussion of issues involved\
    \ for clients in interacting\n      with servers whose behavior is not consistent\
    \ with use of uniform\n      client ID strings.\n   o  It adds a description of\
    \ how server behavior might be used by the\n      client to determine when multiple\
    \ server IP addresses correspond\n      to the same server.\n"
- title: 5.2.  Client Identity Data Items
  contents:
  - "5.2.  Client Identity Data Items\n   The NFSv4 protocol contains a number of\
    \ protocol entities to identify\n   clients and client-based entities for locking-related\
    \ purposes:\n   o  The nfs_client_id4 structure, which uniquely identifies a specific\n\
    \      client boot instance.  That identification is presented to the\n      server\
    \ by doing a SETCLIENTID operation.  The SETCLIENTID\n      operation is described\
    \ in Section 8.4, which modifies a\n      description in Section 16.33 of [RFC7530].\n\
    \   o  The clientid4, which is returned by the server upon completion of\n   \
    \   a successful SETCLIENTID operation.  This id is used by the client\n     \
    \ to identify itself when doing subsequent locking-related\n      operations.\
    \  A clientid4 is associated with a particular lease\n      whereby a client instance\
    \ holds state on a server instance and may\n      become invalid due to client\
    \ reboot, server reboot, or other\n      circumstances.\n   o  Opaque arrays,\
    \ which are used together with the clientid4 to\n      designate within-client\
    \ entities (e.g., processes) as the owners\n      of opens (open-owners) and owners\
    \ of byte-range locks (lock-\n      owners).\n"
- title: 5.2.1.  Client Identity Structure
  contents:
  - "5.2.1.  Client Identity Structure\n   The basis of the client identification\
    \ infrastructure is encapsulated\n   in the following data structure, which also\
    \ appears in Section 9.1.1\n   of [RFC7530]:\n   struct nfs_client_id4 {\n   \
    \        verifier4       verifier;\n           opaque          id<NFS4_OPAQUE_LIMIT>;\n\
    \   };\n   The nfs_client_id4 structure uniquely defines a particular client\n\
    \   boot instance as follows:\n   o  The id field is a variable-length string\
    \ that uniquely identifies\n      a specific client.  Although it is described\
    \ here as a string and\n      is often referred to as a \"client string\", it\
    \ should be understood\n      that the protocol defines this as opaque data. \
    \ In particular,\n      those receiving such an id should not assume that it will\
    \ be in\n      the UTF-8 encoding.  Servers MUST NOT reject an nfs_client_id4\n\
    \      simply because the id string does not follow the rules of UTF-8\n     \
    \ encoding.\n      The encoding and decoding processes for this field (e.g., use\
    \ of\n      network byte order) need to result in the same internal\n      representation\
    \ whatever the endianness of the originating and\n      receiving machines.\n\
    \   o  The verifier field contains a client boot instance identifier that\n  \
    \    is used by the server to detect client reboots.  Only if the boot\n     \
    \ instance is different from that which the server has previously\n      recorded\
    \ in connection with the client (as identified by the id\n      field) does the\
    \ server cancel the client's leased state.  This\n      cancellation occurs once\
    \ it receives confirmation of the new\n      nfs_clientd4 via SETCLIENTID_CONFIRM.\
    \  The SETCLIENTID_CONFIRM\n      operation is described in Section 16.34 of [RFC7530].\n\
    \      In order to prevent the possibility of malicious destruction of\n     \
    \ the locking state associated with a client, the server MUST NOT\n      cancel\
    \ a client's leased state if the principal that established\n      the state for\
    \ a given id string is not the same as the principal\n      issuing the SETCLIENTID.\n\
    \   There are several considerations for how the client generates the id\n   string:\n\
    \   o  The string should be unique so that multiple clients do not\n      present\
    \ the same string.  The consequences of two clients\n      presenting the same\
    \ string range from one client getting an error\n      to one client having its\
    \ leased state abruptly and unexpectedly\n      canceled.\n   o  The string should\
    \ be selected so that subsequent incarnations\n      (e.g., reboots) of the same\
    \ client cause the client to present the\n      same string.  The implementer\
    \ is cautioned against an approach\n      that requires the string to be recorded\
    \ in a local file because\n      this precludes the use of the implementation\
    \ in an environment\n      where there is no local disk and all file access is\
    \ from an NFSv4\n      server.\n   o  The string MAY be different for each server\
    \ network address that\n      the client accesses rather than common to all server\
    \ network\n      addresses.\n      The considerations that might influence a client\
    \ to use different\n      strings for different network server addresses are explained\
    \ in\n      Section 5.4.\n   o  The algorithm for generating the string should\
    \ not assume that the\n      clients' network addresses will remain the same for\
    \ any set period\n      of time.  Even while the client is still running in its\
    \ current\n      incarnation, changes might occur between client incarnations.\n\
    \      Changes to the client ID string due to network address changes\n      would\
    \ result in successive SETCLIENTID operations for the same\n      client appearing\
    \ as from different clients, interfering with the\n      use of the nfs_client_id4\
    \ verifier field to cancel state\n      associated with previous boot instances\
    \ of the same client.\n      The difficulty is more severe if the client address\
    \ is the only\n      client-based information in the client ID string.  In such\
    \ a case,\n      there is a real risk that after the client gives up the network\n\
    \      address, another client, using the same algorithm, would generate\n   \
    \   a conflicting id string.  This would be likely to cause an\n      inappropriate\
    \ loss of locking state.  See Section 5.9 for detailed\n      guidance regarding\
    \ client ID string construction.\n"
- title: 5.2.2.  Client Identity Shorthand
  contents:
  - "5.2.2.  Client Identity Shorthand\n   Once a SETCLIENTID and SETCLIENTID_CONFIRM\
    \ sequence has successfully\n   completed, the client uses the shorthand client\
    \ identifier, of type\n   clientid4, instead of the longer and less compact nfs_client_id4\n\
    \   structure.  This shorthand client identifier (a client ID) is\n   assigned\
    \ by the server and should be chosen so that it will not\n   conflict with a client\
    \ ID previously assigned by the same server and,\n   to the degree practicable,\
    \ by other servers as well.  This applies\n   across server restarts or reboots.\n\
    \   Establishment of the client ID by a new incarnation of the client\n   also\
    \ has the effect of immediately breaking any leased state that a\n   previous\
    \ incarnation of the client might have had on the server, as\n   opposed to forcing\
    \ the new client incarnation to wait for the leases\n   to expire.  Breaking the\
    \ lease state amounts to the server removing\n   all locks, share reservations,\
    \ and delegation states not requested\n   using the CLAIM_DELEGATE_PREV claim\
    \ type associated with a client\n   having the same identity.  For a discussion\
    \ of delegation state\n   recovery, see Section 10.2.1 of [RFC7530].\n   Note\
    \ that the SETCLIENTID and SETCLIENTID_CONFIRM operations have a\n   secondary\
    \ purpose of establishing the information the server needs to\n   make callbacks\
    \ to the client for the purpose of supporting\n   delegations.  The client is\
    \ able to change this information via\n   SETCLIENTID and SETCLIENTID_CONFIRM\
    \ within the same incarnation of\n   the client without causing removal of the\
    \ client's leased state.\n   Distinct servers MAY assign clientid4s independently,\
    \ and they will\n   generally do so.  Therefore, a client has to be prepared to\
    \ deal with\n   multiple instances of the same clientid4 value received on distinct\n\
    \   IP addresses, denoting separate entities.  When trunking of server IP\n  \
    \ addresses is not a consideration, a client should keep track of\n   <IP-address,\
    \ clientid4> pairs, so that each pair is distinct.  For a\n   discussion of how\
    \ to address the issue in the face of possible\n   trunking of server IP addresses,\
    \ see Section 5.4.\n   Owners of opens and owners of byte-range locks are separate\
    \ entities\n   and remain separate even if the same opaque arrays are used to\n\
    \   designate owners of each.  The protocol distinguishes between open-\n   owners\
    \ (represented by open_owner4 structures) and lock-owners\n   (represented by\
    \ lock_owner4 structures).\n   Both sorts of owners consist of a clientid4 and\
    \ an opaque owner\n   string.  For each client, there is a set of distinct owner\
    \ values\n   used with that client which constitutes the set of known owners of\n\
    \   that type, for the given client.\n   Each open is associated with a specific\
    \ open-owner while each byte-\n   range lock is associated with a lock-owner and\
    \ an open-owner, the\n   latter being the open-owner associated with the open\
    \ file under which\n   the LOCK operation was done.\n   When a clientid4 is presented\
    \ to a server and that clientid4 is not\n   valid, the server will reject the\
    \ request with an error that depends\n   on the reason for clientid4 invalidity.\
    \  The error\n   NFS4ERR_ADMIN_REVOKED is returned when the invalidation is the\
    \ result\n   of administrative action.  When the clientid4 is unrecognizable,\
    \ the\n   error NFS4ERR_STALE_CLIENTID or NFS4ERR_EXPIRED may be returned.  An\n\
    \   unrecognizable clientid4 can occur for a number of reasons:\n   o  A server\
    \ reboot causing loss of the server's knowledge of the\n      client.  (Always\
    \ returns NFS4ERR_STALE_CLIENTID.)\n   o  Client error sending an incorrect clientid4\
    \ or a valid clientid4\n      to the wrong server.  (May return either error.)\n\
    \   o  Loss of lease state due to lease expiration.  (Always returns\n      NFS4ERR_EXPIRED.)\n\
    \   o  Client or server error causing the server to believe that the\n      client\
    \ has rebooted (i.e., receiving a SETCLIENTID with an\n      nfs_client_id4 that\
    \ has a matching id string and a non-matching\n      boot instance id as the verifier).\
    \  (May return either error.)\n   o  Migration of all state under the associated\
    \ lease causes its non-\n      existence to be recognized on the source server.\
    \  (Always returns\n      NFS4ERR_STALE_CLIENTID.)\n   o  Merger of state under\
    \ the associated lease with another lease\n      under a different client ID causes\
    \ the clientid4 serving as the\n      source of the merge to cease being recognized\
    \ on its server.\n      (Always returns NFS4ERR_STALE_CLIENTID.)\n   In the event\
    \ of a server reboot, loss of lease state due to lease\n   expiration, or administrative\
    \ revocation of a clientid4, the client\n   must obtain a new clientid4 by use\
    \ of the SETCLIENTID operation and\n   then proceed to any other necessary recovery\
    \ for the server reboot\n   case (see Section 9.6.2 in [RFC7530]).  In cases of\
    \ server or client\n   error resulting in a clientid4 becoming unusable, use of\
    \ SETCLIENTID\n   to establish a new lease is desirable as well.\n   In cases\
    \ in which loss of server knowledge of a clientid4 is the\n   result of migration,\
    \ different recovery procedures are required.  See\n   Section 6.1.1 for details.\
    \  Note that in cases in which there is any\n   uncertainty about which sort of\
    \ handling is applicable, the\n   distinguishing characteristic is that in reboot-like\
    \ cases, the\n   clientid4 and all associated stateids cease to exist while in\n\
    \   migration-related cases, the clientid4 ceases to exist while the\n   stateids\
    \ are still valid.\n   The client must also employ the SETCLIENTID operation when\
    \ it\n   receives an NFS4ERR_STALE_STATEID error using a stateid derived from\n\
    \   its current clientid4, since this indicates a situation, such as a\n   server\
    \ reboot that has invalidated the existing clientid4 and\n   associated stateids\
    \ (see Section 9.1.5 in [RFC7530] for details).\n   See the detailed descriptions\
    \ of SETCLIENTID (in Section 8.4) and\n   SETCLIENTID_CONFIRM (in Section 16.34\
    \ of [RFC7530]) for a complete\n   specification of these operations.\n"
- title: 5.3.  Server Release of Client ID
  contents:
  - "5.3.  Server Release of Client ID\n   If the server determines that the client\
    \ holds no associated state\n   for its clientid4, the server may choose to release\
    \ that clientid4.\n   The server may make this choice for an inactive client so\
    \ that\n   resources are not consumed by those intermittently active clients.\n\
    \   If the client contacts the server after this release, the server must\n  \
    \ ensure the client receives the appropriate error so that it will use\n   the\
    \ SETCLIENTID/SETCLIENTID_CONFIRM sequence to establish a new\n   identity.  It\
    \ should be clear that the server must be very hesitant\n   to release a client\
    \ ID since the resulting work on the client to\n   recover from such an event\
    \ will be the same burden as if the server\n   had failed and restarted.  Typically,\
    \ a server would not release a\n   client ID unless there had been no activity\
    \ from that client for many\n   minutes.\n   Note that if the id string in a SETCLIENTID\
    \ request is properly\n   constructed, and if the client takes care to use the\
    \ same principal\n   for each successive use of SETCLIENTID, then, barring an\
    \ active\n   denial-of-service attack, NFS4ERR_CLID_INUSE should never be\n  \
    \ returned.\n   However, client bugs, server bugs, or perhaps a deliberate change\
    \ of\n   the principal owner of the id string (such as may occur in the case\n\
    \   in which a client changes security flavors, and under the new flavor,\n  \
    \ there is no mapping to the previous owner) will in rare cases result\n   in\
    \ NFS4ERR_CLID_INUSE.\n   In situations in which there is an apparent change of\
    \ principal, when\n   the server gets a SETCLIENTID specifying a client ID string\
    \ for which\n   the server has a clientid4 that currently has no state, or for\
    \ which\n   it has state, but where the lease has expired, the server MUST allow\n\
    \   the SETCLIENTID rather than returning NFS4ERR_CLID_INUSE.  The server\n  \
    \ MUST then confirm the new client ID if followed by the appropriate\n   SETCLIENTID_CONFIRM.\n"
- title: 5.4.  Client ID String Approaches
  contents:
  - "5.4.  Client ID String Approaches\n   One particular aspect of the construction\
    \ of the nfs_client_id4\n   string has proved recurrently troublesome.  The client\
    \ has a choice\n   of:\n   o  Presenting the same id string to multiple server\
    \ addresses.  This\n      is referred to as the \"uniform client ID string approach\"\
    \ and is\n      discussed in Section 5.6.\n   o  Presenting different id strings\
    \ to multiple server addresses.\n      This is referred to as the \"non-uniform\
    \ client ID string approach\"\n      and is discussed in Section 5.5.\n   Note\
    \ that implementation considerations, including compatibility with\n   existing\
    \ servers, may make it desirable for a client to use both\n   approaches, based\
    \ on configuration information, such as mount\n   options.  This issue will be\
    \ discussed in Section 5.7.\n   Construction of the client ID string has arisen\
    \ as a difficult issue\n   because of the way in which the NFS protocols have\
    \ evolved.  It is\n   useful to consider two points in that evolution.\n   o \
    \ NFSv3 as a stateless protocol had no need to identify the state\n      shared\
    \ by a particular client-server pair (see [RFC1813]).  Thus,\n      there was\
    \ no need to consider the question of whether a set of\n      requests come from\
    \ the same client or whether two server IP\n      addresses are connected to the\
    \ same server.  As the environment\n      was one in which the user supplied the\
    \ target server IP address as\n      part of incorporating the remote file system\
    \ in the client's file\n      namespace, there was no occasion to take note of\
    \ server trunking.\n      Within a stateless protocol, the situation was symmetrical.\
    \  The\n      client has no server identity information, and the server has no\n\
    \      client identity information.\n   o  NFSv4.1 is a stateful protocol with\
    \ full support for client and\n      server identity determination (see [RFC5661]).\
    \  This enables the\n      server to be aware when two requests come from the\
    \ same client\n      (they are on sessions sharing a clientid4) and the client\
    \ to be\n      aware when two server IP addresses are connected to the same\n\
    \      server.  Section 2.10.5.1 of [RFC5661] explains how the client is\n   \
    \   able to assure itself that the connections are to the same logical\n     \
    \ server.\n   NFSv4.0 is unfortunately halfway between these two.  It introduced\n\
    \   new requirements such as the need to identify specific clients and\n   client\
    \ instances without addressing server identity issues.  The two\n   client ID\
    \ string approaches have arisen in attempts to deal with the\n   changing requirements\
    \ of the protocol as implementation has\n   proceeded, and features that were\
    \ not very substantial in early\n   implementations of NFSv4.0 became more substantial\
    \ as implementation\n   proceeded.\n   o  In the absence of any implementation\
    \ of features related to\n      fs_locations (replication, referral, and migration),\
    \ the situation\n      is very similar to that of NFSv3 (see Section 8.1 and the\n\
    \      subsections within Section 8.4 of [RFC7530] for discussion of\n      these\
    \ features).  In this case, locking state has been added, but\n      there is\
    \ no need for concern about the provision of accurate\n      client and server\
    \ identity determination.  This is the situation\n      that gave rise to the\
    \ non-uniform client ID string approach.\n   o  In the presence of replication\
    \ and referrals, the client may have\n      occasion to take advantage of knowledge\
    \ of server trunking\n      information.  Even more important, transparent state\
    \ migration, by\n      transferring state among servers, causes difficulties for\
    \ the non-\n      uniform client ID string approach, in that the two different\n\
    \      client ID strings sent to different IP addresses may wind up being\n  \
    \    processed by the same logical server, adding confusion.\n   o  A further\
    \ consideration is that client implementations typically\n      provide NFSv4.1\
    \ by augmenting their existing NFSv4.0\n      implementation, not by providing\
    \ two separate implementations.\n      Thus, the more NFSv4.0 and NFSv4.1 can\
    \ work alike, the less\n      complex the clients are.  This is a key reason why\
    \ those\n      implementing NFSv4.0 clients might prefer using the uniform client\n\
    \      string model, even if they have chosen not to provide\n      fs_locations-related\
    \ features in their NFSv4.0 client.\n   Both approaches have to deal with the\
    \ asymmetry in client and server\n   identity information between client and server.\
    \  Each seeks to make\n   the client's and the server's views match.  In the process,\
    \ each\n   encounters some combination of inelegant protocol features and/or\n\
    \   implementation difficulties.  The choice of which to use is up to the\n  \
    \ client implementer, and the sections below try to give some useful\n   guidance.\n"
- title: 5.5.  Non-uniform Client ID String Approach
  contents:
  - "5.5.  Non-uniform Client ID String Approach\n   The non-uniform client ID string\
    \ approach is an attempt to handle\n   these matters in NFSv4.0 client implementations\
    \ in as NFSv3-like a\n   way as possible.\n   For a client using the non-uniform\
    \ approach, all internal recording\n   of clientid4 values is to include, whether\
    \ explicitly or implicitly,\n   the server IP address so that one always has an\
    \ <IP-address,\n   clientid4> pair.  Two such pairs from different servers are\
    \ always\n   distinct even when the clientid4 values are the same, as they may\n\
    \   occasionally be.  In this approach, such equality is always treated\n   as\
    \ simple happenstance.\n   Making the client ID string different on different\
    \ server IP\n   addresses results in a situation in which a server has no way\
    \ of\n   tying together information from the same client, when the client\n  \
    \ accesses multiple server IP addresses.  As a result, it will treat a\n   single\
    \ client as multiple clients with separate leases for each\n   server network\
    \ address.  Since there is no way in the protocol for\n   the client to determine\
    \ if two network addresses are connected to the\n   same server, the resulting\
    \ lack of knowledge is symmetrical and can\n   result in simpler client implementations\
    \ in which there is a single\n   clientid4/lease per server network address.\n\
    \   Support for migration, particularly with transparent state migration,\n  \
    \ is more complex in the case of non-uniform client ID strings.  For\n   example,\
    \ migration of a lease can result in multiple leases for the\n   same client accessing\
    \ the same server addresses, vitiating many of\n   the advantages of this approach.\
    \  Therefore, client implementations\n   that support migration with transparent\
    \ state migration are likely to\n   experience difficulties using the non-uniform\
    \ client ID string\n   approach and should not do so, except where it is necessary\
    \ for\n   compatibility with existing server implementations (for details of\n\
    \   arranging use of multiple client ID string approaches, see\n   Section 5.7).\n"
- title: 5.6.  Uniform Client ID String Approach
  contents:
  - "5.6.  Uniform Client ID String Approach\n   When the client ID string is kept\
    \ uniform, the server has the basis\n   to have a single clientid4/lease for each\
    \ distinct client.  The\n   problem that has to be addressed is the lack of explicit\
    \ server\n   identity information, which was made available in NFSv4.1.\n   When\
    \ the same client ID string is given to multiple IP addresses, the\n   client\
    \ can determine whether two IP addresses correspond to a single\n   server, based\
    \ on the server's behavior.  This is the inverse of the\n   strategy adopted for\
    \ the non-uniform approach in which different\n   server IP addresses are told\
    \ about different clients, simply to\n   prevent a server from manifesting behavior\
    \ that is inconsistent with\n   there being a single server for each IP address,\
    \ in line with the\n   traditions of NFS.  So, to compare:\n   o  In the non-uniform\
    \ approach, servers are told about different\n      clients because, if the server\
    \ were to use accurate client\n      identity information, two IP addresses on\
    \ the same server would\n      behave as if they were talking to the same client,\
    \ which might\n      prove disconcerting to a client not expecting such behavior.\n\
    \   o  In the uniform approach, the servers are told about there being a\n   \
    \   single client, which is, after all, the truth.  Then, when the\n      server\
    \ uses this information, two IP addresses on the same server\n      will behave\
    \ as if they are talking to the same client, and this\n      difference in behavior\
    \ allows the client to infer the server IP\n      address trunking configuration,\
    \ even though NFSv4.0 does not\n      explicitly provide this information.\n \
    \     The approach given in the section below shows one example of how\n     \
    \ this might be done.\n   The uniform client ID string approach makes it necessary\
    \ to exercise\n   more care in the definition of the boot instance id sent as\
    \ the\n   verifier field in an nfs_client_id4:\n   o  In [RFC7530], the client\
    \ is told to change the verifier field\n      value when reboot occurs, but there\
    \ is no explicit statement as to\n      the converse, so that any requirement\
    \ to keep the verifier field\n      constant unless rebooting is only present\
    \ by implication.\n   o  Many existing clients change the boot instance id every\
    \ time they\n      destroy and recreate the data structure that tracks an\n  \
    \    <IP-address, clientid4> pair.  This might happen if the last mount\n    \
    \  of a particular server is removed, and then a fresh mount is\n      created.\
    \  Also, note that this might result in each <IP-address,\n      clientid4> pair\
    \ having its own boot instance id that is\n      independent of the others.\n\
    \   o  Within the uniform client ID string approach, an nfs_client_id4\n     \
    \ designates a globally known client instance, so that the verifier\n      field\
    \ should change if and only if a new client instance is\n      created, typically\
    \ as a result of a reboot.\n      Clients using the uniform client ID string approach\
    \ are therefore\n      well advised to use a verifier established only once for\
    \ each\n      reboot, typically at reboot time.\n   The following are advantages\
    \ for the implementation of using the\n   uniform client ID string approach:\n\
    \   o  Clients can take advantage of server trunking (and clustering with\n  \
    \    single-server-equivalent semantics) to increase bandwidth or\n      reliability.\n\
    \   o  There are advantages in state management so that, for example, one\n  \
    \    never has a delegation under one clientid4 revoked because of a\n      reference\
    \ to the same file from the same client under a different\n      clientid4.\n\
    \   o  The uniform client ID string approach allows the server to do any\n   \
    \   necessary automatic lease merger in connection with transparent\n      state\
    \ migration, without requiring any client involvement.  This\n      consideration\
    \ is of sufficient weight to cause us to recommend use\n      of the uniform client\
    \ ID string approach for clients supporting\n      transparent state migration.\n\
    \   The following implementation considerations might cause issues for\n   client\
    \ implementations.\n   o  This approach is considerably different from the non-uniform\n\
    \      approach, which most client implementations have been following.\n    \
    \  Until substantial implementation experience is obtained with this\n      approach,\
    \ reluctance to embrace something so new is to be\n      expected.\n   o  Mapping\
    \ between server network addresses and leases is more\n      complicated in that\
    \ it is no longer a one-to-one mapping.\n   Another set of relevant considerations\
    \ relate to privacy concerns,\n   which users of the client might have in that\
    \ use of the uniform\n   client ID string approach would enable multiple servers\
    \ acting in\n   concert to determine when multiple requests received at different\n\
    \   times derive from the same NFSv4.0 client.  For example, this might\n   enable\
    \ determination that multiple distinct user identities in fact\n   are likely\
    \ to correspond to requests made by the same person, even\n   when those requests\
    \ are directed to different servers.\n   How to balance these considerations depends\
    \ on implementation goals.\n"
- title: 5.7.  Mixing Client ID String Approaches
  contents:
  - "5.7.  Mixing Client ID String Approaches\n   As noted above, a client that needs\
    \ to use the uniform client ID\n   string approach (e.g., to support migration)\
    \ may also need to support\n   existing servers with implementations that do not\
    \ work properly in\n   this case.\n   Some examples of such server issues include:\n\
    \   o  Some existing NFSv4.0 server implementations of IP address\n      failover\
    \ depend on clients' use of a non-uniform client ID string\n      approach.  In\
    \ particular, when a server supports both its own IP\n      address and one failed\
    \ over from a partner server, it may have\n      separate sets of state applicable\
    \ to the two IP addresses, owned\n      by different servers but residing on a\
    \ single one.\n      In this situation, some servers have relied on clients' use\
    \ of the\n      non-uniform client ID string approach, as suggested but not\n\
    \      mandated by [RFC7530], to keep these sets of state separate, and\n    \
    \  they will have problems handling clients using the uniform client\n      ID\
    \ string approach, in that such clients will see changes in\n      trunking relationships\
    \ whenever server failover and giveback\n      occur.\n   o  Some existing servers\
    \ incorrectly return NFS4ERR_CLID_INUSE simply\n      because there already exists\
    \ a clientid4 for the same client,\n      established using a different IP address.\
    \  This causes difficulty\n      for a multihomed client using the uniform client\
    \ ID string\n      approach.\n      Although this behavior is not correct, such\
    \ servers still exist,\n      and this specification should give clients guidance\
    \ about dealing\n      with the situation, as well as making the correct behavior\
    \ clear.\n   In order to support use of these sorts of servers, the client can\
    \ use\n   different client ID string approaches for different mounts, in order\n\
    \   to assure that:\n   o  The uniform client ID string approach is used when\
    \ accessing\n      servers that may return NFS4ERR_MOVED and when the client wishes\n\
    \      to enable transparent state migration.\n   o  The non-uniform client ID\
    \ string approach is used when accessing\n      servers whose implementations\
    \ make them incompatible with the\n      uniform client ID string approach.\n\
    \   Since the client cannot easily determine which of the above are true,\n  \
    \ implementations are likely to rely on user-specified mount options to\n   select\
    \ the appropriate approach to use, in cases in which a client\n   supports simultaneous\
    \ use of multiple approaches.  Choice of a\n   default to use in such cases is\
    \ up to the client implementation.\n   In the case in which the same server has\
    \ multiple mounts, and both\n   approaches are specified for the same server,\
    \ the client could have\n   multiple clientid4s corresponding to the same server,\
    \ one for each\n   approach, and would then have to keep these separate.\n"
- title: 5.8.  Trunking Determination when Using Uniform Client ID Strings
  contents:
  - "5.8.  Trunking Determination when Using Uniform Client ID Strings\n   This section\
    \ provides an example of how trunking determination could\n   be done by a client\
    \ following the uniform client ID string approach\n   (whether this is used for\
    \ all mounts or not).  Clients need not\n   follow this procedure, but implementers\
    \ should make sure that the\n   issues dealt with by this procedure are all properly\
    \ addressed.\n   It is best to clarify here the various possible purposes of trunking\n\
    \   determination and the corresponding requirements as to server\n   behavior.\
    \  The following points should be noted:\n   o  The primary purpose of the trunking\
    \ determination algorithm is to\n      make sure that, if the server treats client\
    \ requests on two IP\n      addresses as part of the same client, the client will\
    \ not be\n      surprised and encounter disconcerting server behavior, as\n  \
    \    mentioned in Section 5.6.  Such behavior could occur if the client\n    \
    \  were unaware that all of its client requests for the two IP\n      addresses\
    \ were being handled as part of a single client talking to\n      a single server.\n\
    \   o  A second purpose is to be able to use knowledge of trunking\n      relationships\
    \ for better performance, etc.\n   o  If a server were to give out distinct clientid4s\
    \ in response to\n      receiving the same nfs_client_id4 on different network\
    \ addresses,\n      and acted as if these were separate clients, the primary purpose\n\
    \      of trunking determination would be met, as long as the server did\n   \
    \   not treat them as part of the same client.  In this case, the\n      server\
    \ would be acting, with regard to that client, as if it were\n      two distinct\
    \ servers.  This would interfere with the secondary\n      purpose of trunking\
    \ determination, but there is nothing the client\n      can do about that.\n \
    \  o  Suppose a server were to give such a client two different\n      clientid4s\
    \ but act as if they were one.  That is the only way that\n      the server could\
    \ behave in a way that would defeat the primary\n      purpose of the trunking\
    \ determination algorithm.\n      Servers MUST NOT behave that way.\n   For a\
    \ client using the uniform approach, clientid4 values are treated\n   as important\
    \ information in determining server trunking patterns.\n   For two different IP\
    \ addresses to return the same clientid4 value is\n   a necessary, though not\
    \ a sufficient condition for them to be\n   considered as connected to the same\
    \ server.  As a result, when two\n   different IP addresses return the same clientid4,\
    \ the client needs to\n   determine, using the procedure given below or otherwise,\
    \ whether the\n   IP addresses are connected to the same server.  For such clients,\
    \ all\n   internal recording of clientid4 values needs to include, whether\n \
    \  explicitly or implicitly, identification of the server from which the\n   clientid4\
    \ was received so that one always has a (server, clientid4)\n   pair.  Two such\
    \ pairs from different servers are always considered\n   distinct even when the\
    \ clientid4 values are the same, as they may\n   occasionally be.\n   In order\
    \ to make this approach work, the client must have certain\n   information accessible\
    \ for each nfs_client_id4 used by the uniform\n   approach (only one in general).\
    \  The client needs to maintain a list\n   of all server IP addresses, together\
    \ with the associated clientid4\n   values, SETCLIENTID principals, and authentication\
    \ flavors.  As a\n   part of the associated data structures, there should be the\
    \ ability\n   to mark a server IP structure as having the same server as another\n\
    \   and to mark an IP address as currently unresolved.  One way to do\n   this\
    \ is to allow each such entry to point to another with the pointer\n   value being\
    \ one of:\n   o  A pointer to another entry for an IP address associated with\
    \ the\n      same server, where that IP address is the first one referenced to\n\
    \      access that server.\n   o  A pointer to the current entry if there is no\
    \ earlier IP address\n      associated with the same server, i.e., where the current\
    \ IP\n      address is the first one referenced to access that server.  The\n\
    \      text below refers to such an IP address as the lead IP address for\n  \
    \    a given server.\n   o  The value NULL if the address's server identity is\
    \ currently\n      unresolved.\n   In order to keep the above information current,\
    \ in the interests of\n   the most effective trunking determination, RENEWs should\
    \ be\n   periodically done on each server.  However, even if this is not done,\n\
    \   the primary purpose of the trunking determination algorithm, to\n   prevent\
    \ confusion due to trunking hidden from the client, will be\n   achieved.\n  \
    \ Given this apparatus, when a SETCLIENTID is done and a clientid4\n   returned,\
    \ the data structure can be searched for a matching clientid4\n   and if such\
    \ is found, further processing can be done to determine\n   whether the clientid4\
    \ match is accidental, or the result of trunking.\n   In this algorithm, when\
    \ SETCLIENTID is done initially, it will use\n   the common nfs_client_id4 and\
    \ specify the current target IP address\n   as callback.cb_location within the\
    \ callback parameters.  We call the\n   clientid4 and SETCLIENTID verifier returned\
    \ by this operation XC and\n   XV, respectively.\n   This choice of callback parameters\
    \ is provisional and reflects the\n   client's preferences in the event that the\
    \ IP address is not trunked\n   with other IP addresses.  The algorithm is constructed\
    \ so that only\n   the appropriate callback parameters, reflecting observed trunking\n\
    \   patterns, are actually confirmed.\n   Note that when the client has done previous\
    \ SETCLIENTIDs to any IP\n   addresses, with more than one principal or authentication\
    \ flavor, one\n   has the possibility of receiving NFS4ERR_CLID_INUSE, since it\
    \ is not\n   yet known which of the connections with existing IP addresses might\n\
    \   be trunked with the current one.  In the event that the SETCLIENTID\n   fails\
    \ with NFS4ERR_CLID_INUSE, one must try all other combinations of\n   principals\
    \ and authentication flavors currently in use, and\n   eventually one will be\
    \ correct and not return NFS4ERR_CLID_INUSE.\n   Note that at this point, no SETCLIENTID_CONFIRM\
    \ has yet been done.\n   This is because the SETCLIENTID just done has either\
    \ established a\n   new clientid4 on a previously unknown server or changed the\
    \ callback\n   parameters on a clientid4 associated with some already known server.\n\
    \   Given it is undesirable to confirm something that should not happen,\n   what\
    \ is to be done next depends on information about existing\n   clientid4s.\n \
    \  o  If no matching clientid4 is found, the IP address X and clientid4\n    \
    \  XC are added to the list and considered as having no existing\n      known\
    \ IP addresses trunked with it.  The IP address is marked as a\n      lead IP\
    \ address for a new server.  A SETCLIENTID_CONFIRM is done\n      using XC and\
    \ XV.\n   o  If a matching clientid4 is found that is marked unresolved,\n   \
    \   processing on the new IP address is suspended.  In order to\n      simplify\
    \ processing, there can only be one unresolved IP address\n      for any given\
    \ clientid4.\n   o  If one or more matching clientid4s are found, none of which\
    \ are\n      marked unresolved, the new IP address X is entered and marked\n \
    \     unresolved.  A SETCLIENTID_CONFIRM is done to X using XC and XV.\n   When,\
    \ as a result of encountering the last of the three cases shown\n   above, an\
    \ unresolved IP address exists, further processing is\n   required.  After applying\
    \ the steps below to each of the lead IP\n   addresses with a matching clientid4,\
    \ the address will have been\n   resolved: It may have been determined to be part\
    \ of an already known\n   server as a new IP address to be added to an existing\
    \ set of IP\n   addresses for that server.  Otherwise, it will be recognized as\
    \ a new\n   server.  At the point at which this determination is made, the\n \
    \  unresolved indication is cleared and any suspended SETCLIENTID\n   processing\
    \ is restarted.\n   For each lead IP address IPn with a clientid4 matching XC,\
    \ the\n   following steps are done.  Because the Remote Procedure Call (RPC) to\n\
    \   do a SETCLIENTID could take considerable time, it is desirable for\n   the\
    \ client to perform these operations in parallel.  Note that\n   because the clientid4\
    \ is a 64-bit value, the number of such IP\n   addresses that would need to be\
    \ tested is expected to be quite small,\n   even when the client is interacting\
    \ with many NFSv4.0 servers.  Thus,\n   while parallel processing is desirable,\
    \ it is not necessary.\n   o  If the principal for IPn does not match that for\
    \ X, the IP address\n      is skipped, since it is impossible for IPn and X to\
    \ be trunked in\n      these circumstances.  If the principal does match but the\n\
    \      authentication flavor does not, the authentication flavor already\n   \
    \   used should be used for address X as well.  This will avoid any\n      possibility\
    \ that NFS4ERR_CLID_INUSE will be returned for the\n      SETCLIENTID and SETCLIENTID_CONFIRM\
    \ to be done below, as long as\n      the server(s) at IP addresses IPn and X\
    \ is correctly implemented.\n   o  A SETCLIENTID is done to update the callback\
    \ parameters to reflect\n      the possibility that X will be marked as associated\
    \ with the\n      server whose lead IP address is IPn.  The specific callback\n\
    \      parameters chosen, in terms of cb_client4 and callback_ident, are\n   \
    \   up to the client and should reflect its preferences as to callback\n     \
    \ handling for the common clientid4, in the event that X and IPn are\n      trunked\
    \ together.  When a SETCLIENTID is done on IP address IPn, a\n      setclientid_confirm\
    \ value (in the form of a verifier4) is\n      returned, which will be referred\
    \ to as SCn.\n      Note that the NFSv4.0 specification requires the server to\
    \ make\n      sure that such verifiers are very unlikely to be regenerated.\n\
    \      Given that it is already highly unlikely that the clientid4 XC is\n   \
    \   duplicated by distinct servers, the probability that SCn is\n      duplicated\
    \ as well has to be considered vanishingly small.  Note\n      also that the callback\
    \ update procedure can be repeated multiple\n      times to reduce the probability\
    \ of further spurious matches.\n   o  The setclientid_confirm value SCn is saved\
    \ for later use in\n      confirming the SETCLIENTID done to IPn.\n   Once the\
    \ SCn values are gathered up by the procedure above, they are\n   each tested\
    \ by being used as the verifier for a SETCLIENTID_CONFIRM\n   operation directed\
    \ to the original IP address X, whose trunking\n   relationships are to be determined.\
    \  These RPC operations may be done\n   in parallel.\n   There are a number of\
    \ things that should be noted at this point.\n   o  The SETCLIENTID operations\
    \ done on the various IPn addresses in\n      the procedure above will never be\
    \ confirmed by SETCLIENTID_CONFIRM\n      operations directed to the various IPn\
    \ addresses.  If these\n      callback updates are to be confirmed, they will\
    \ be confirmed by\n      SETCLIENTID_CONFIRM operations directed at the original\
    \ IP address\n      X, which can only happen if SCn was generated by an IPn that\
    \ was\n      trunked with X, allowing the SETCLIENTID to be successfully\n   \
    \   confirmed and allowing us to infer the existence of that trunking\n      relationship.\n\
    \   o  The number of successful SETCLIENTID_CONFIRM operations done\n      should\
    \ never be more than one.  If both SCn and SCm are accepted\n      by X, then\
    \ it indicates that both IPn and IPm are trunked with X,\n      but that is only\
    \ possible if IPn and IPm are trunked together.\n      Since these two addresses\
    \ were earlier recognized as not trunked\n      together, this should be impossible,\
    \ if the servers in question\n      are implemented correctly.\n   Further processing\
    \ depends on the success or failure of the various\n   SETCLIENTD_CONFIRM operations\
    \ done in the step above.\n   o  If the setclientid_confirm value generated by\
    \ a particular IPn is\n      accepted on X, then X and IPn are recognized as connected\
    \ to the\n      same server, and the entry for X is marked as associated with\
    \ IPn.\n   o  If none of the confirm operations are accepted, then X is\n    \
    \  recognized as a distinct server.  Its callback parameters will\n      remain\
    \ as the ones established by the original SETCLIENTID.\n   In either of the cases,\
    \ the entry is considered resolved and\n   processing can be restarted for IP\
    \ addresses whose clientid4 matched\n   XC but whose resolution had been deferred.\n\
    \   The procedure described above must be performed so as to exclude the\n   possibility\
    \ that multiple SETCLIENTIDs done to different server IP\n   addresses and returning\
    \ the same clientid4 might \"race\" in such a\n   fashion that there is no explicit\
    \ determination of whether they\n   correspond to the same server.  The following\
    \ possibilities for\n   serialization are all valid, and implementers may choose\
    \ among them\n   based on a tradeoff between performance and complexity.  They\
    \ are\n   listed in order of increasing parallelism:\n   o  An NFSv4.0 client\
    \ might serialize all instances of SETCLIENTID/\n      SETCLIENTID_CONFIRM processing,\
    \ either directly or by serializing\n      mount operations involving use of NFSv4.0.\
    \  While doing so will\n      prevent the races mentioned above, this degree of\
    \ serialization\n      can cause performance issues when there is a high volume\
    \ of mount\n      operations.\n   o  One might instead serialize the period of\
    \ processing that begins\n      when the clientid4 received from the server is\
    \ processed and ends\n      when all trunking determination for that server is\
    \ completed.\n      This prevents the races mentioned above, without adding to\
    \ delay\n      except when trunking determination is common.\n   o  One might\
    \ avoid much of the serialization implied above, by\n      allowing trunking determination\
    \ for distinct clientid4 values to\n      happen in parallel, with serialization\
    \ of trunking determination\n      happening independently for each distinct clientid4\
    \ value.\n   The procedure above has made no explicit mention of the possibility\n\
    \   that server reboot can occur at any time.  To address this\n   possibility,\
    \ the client should make sure the following steps are\n   taken:\n   o  When a\
    \ SETCLIENTID_CONFIRM is rejected by a given IPn, the client\n      should be\
    \ aware of the possibility that the rejection is due to XC\n      (rather than\
    \ XV) being invalid.  This situation can be addressed\n      by doing a RENEW\
    \ specifying XC directed to the IP address X.  If\n      that operation succeeds,\
    \ then the rejection is to be acted on\n      normally since either XV is invalid\
    \ on IPn or XC has become\n      invalid on IPn while it is valid on X, showing\
    \ that IPn and X are\n      not trunked.  If, on the other hand, XC is not valid\
    \ on X, then\n      the trunking detection process should be restarted once a\
    \ new\n      client ID is established on X.\n   o  In the event of a reboot detected\
    \ on any server-lead IP, the set\n      of IP addresses associated with the server\
    \ should not change, and\n      state should be re-established for the lease as\
    \ a whole, using all\n      available connected server IP addresses.  It is prudent\
    \ to verify\n      connectivity by doing a RENEW using the new clientid4 on each\
    \ such\n      server address before using it, however.\n   Another situation not\
    \ discussed explicitly above is the possibility\n   that a SETCLIENTID done to\
    \ one of the IPn addresses might take so\n   long that it is necessary to time\
    \ out the operation, to prevent\n   unacceptably delaying the MOUNT operation.\
    \  One simple possibility is\n   to simply fail the MOUNT at this point.  Because\
    \ the average number\n   of IP addresses that might have to be tested is quite\
    \ small, this\n   will not greatly increase the probability of MOUNT failure.\
    \  Other\n   possible approaches are:\n   o  If the IPn has sufficient state in\
    \ existence, the existing\n      stateids and sequence values might be validated\
    \ by being used on\n      IP address X.  In the event of success, X and IPn should\
    \ be\n      considered trunked together.\n      What constitutes \"sufficient\"\
    \ state in this context is an\n      implementation decision that is affected\
    \ by the implementer's\n      willingness to fail the MOUNT in an uncertain case\
    \ and the\n      strength of the state verification procedure implemented.\n \
    \  o  If IPn has no locking state in existence, X could be recorded as a\n   \
    \   lead IP address on a provisional basis, subject to trunking being\n      tested\
    \ again, once IPn starts becoming responsive.  To avoid\n      confusion between\
    \ IPn and X, and the need to merge distinct state\n      corpora for X and IPn\
    \ at a later point, this retest of trunking\n      should occur after RENEWs on\
    \ IPn are responded to and before\n      establishing any new state for either\
    \ IPn as a separate server or\n      for IPn considered as a server address trunked\
    \ with X.\n   o  The client locking-related code could be made more tolerant of\n\
    \      what would otherwise be considered anomalous results due to an\n      unrecognized\
    \ trunking relationship.  The client could use the\n      appearance of behavior\
    \ explainable by a previously unknown\n      trunking relationship as the cue\
    \ to consider the addresses as\n      trunked.\n      This choice has a lot of\
    \ complexity associated with it, and it is\n      likely that few implementations\
    \ will use it.  When the set of\n      locking state on IPn is small (e.g., a\
    \ single stateid) but not\n      empty, most client implementations are likely\
    \ to either fail the\n      MOUNT or implement a more stringent verification procedure\
    \ using\n      the existing stateid on IPn as a basis to generate further state\n\
    \      as raw material for the trunking verification process.\n"
- title: 5.9.  Client ID String Construction Details
  contents:
  - "5.9.  Client ID String Construction Details\n   This section gives more detailed\
    \ guidance on client ID string\n   construction.  The guidance in this section\
    \ will cover cases in which\n   either the uniform or the non-uniform approach\
    \ to the client ID\n   string is used.\n   Note that among the items suggested\
    \ for inclusion, there are many\n   that may conceivably change.  In order for\
    \ the client ID string to\n   remain valid in such circumstances, the client SHOULD\
    \ either:\n   o  Use a saved copy of such value rather than the changeable value\n\
    \      itself, or\n   o  Save the constructed client ID string rather than constructing\
    \ it\n      anew at SETCLIENTID time, based on unchangeable parameters and\n \
    \     saved copies of changeable data items.\n   A file is not always a valid\
    \ choice to store such information, given\n   the existence of diskless clients.\
    \  In such situations, whatever\n   facilities exist for a client to store configuration\
    \ information such\n   as boot arguments should be used.\n   Given the considerations\
    \ listed in Section 5.2.1, an id string would\n   be one that includes as its\
    \ basis:\n   o  An identifier uniquely associated with the node on which the\n\
    \      client is running.\n   o  For a user-level NFSv4.0 client, it should contain\
    \ additional\n      information to distinguish the client from a kernel-based\
    \ client\n      and from other user-level clients running on the same node, such\n\
    \      as a universally unique identifier (UUID).\n   o  Where the non-uniform\
    \ approach is to be used, the IP address of\n      the server.\n   o  Additional\
    \ information that tends to be unique, such as one or\n      more of:\n      *\
    \  The timestamp of when the NFSv4 software was first installed on\n         the\
    \ client (though this is subject to the previously mentioned\n         caution\
    \ about using information that is stored in a file,\n         because the file\
    \ might only be accessible over NFSv4).\n      *  A true random number, generally\
    \ established once and saved.\n   With regard to the identifier associated with\
    \ the node on which the\n   client is running, the following possibilities are\
    \ likely candidates.\n   o  The client machine's serial number.\n   o  The client's\
    \ IP address.  Note that this SHOULD be treated as a\n      changeable value.\n\
    \   o  A Media Access Control (MAC) address.  Note that this also should\n   \
    \   be considered a changeable value because of the possibility of\n      configuration\
    \ changes.\n   Privacy concerns may be an issue if some of the items above (e.g.,\n\
    \   machine serial number and MAC address) are used.  When it is\n   necessary\
    \ to use such items to ensure uniqueness, application of a\n   one-way hash function\
    \ is desirable.  When the non-uniform approach is\n   used, that hash function\
    \ should be applied to all of the components\n   chosen as a unit rather than\
    \ to particular individual elements.\n"
- title: 6.  Locking and Multi-Server Namespace
  contents:
  - "6.  Locking and Multi-Server Namespace\n   This section contains a replacement\
    \ for Section 9.14 of [RFC7530],\n   \"Migration, Replication, and State\".\n\
    \   The replacement is in Section 6.1 and supersedes the replaced\n   section.\n\
    \   The changes made can be briefly summarized as follows:\n   o  Adding text\
    \ to address the case of stateid conflict on migration.\n   o  Specifying that\
    \ when leases are moved, as a result of file system\n      migration, they are\
    \ to be merged with leases on the destination\n      server that are connected\
    \ to the same client.\n   o  Adding text that deals with the case of a clientid4\
    \ being changed\n      on state transfer as a result of conflict with an existing\n\
    \      clientid4.\n   o  Adding a section describing how information associated\
    \ with open-\n      owners and lock-owners is to be managed with regard to migration.\n\
    \   o  The description of handling of the NFS4ERR_LEASE_MOVED has been\n     \
    \ rewritten for greater clarity.\n"
- title: 6.1.  Lock State and File System Transitions
  contents:
  - "6.1.  Lock State and File System Transitions\n   File systems may transition\
    \ to a different server in several\n   circumstances:\n   o  Responsibility for\
    \ handling a given file system is transferred to\n      a new server via migration.\n\
    \   o  A client may choose to use an alternate server (e.g., in response\n   \
    \   to server unresponsiveness) in the context of file system\n      replication.\n\
    \   In such cases, the appropriate handling of state shared between the\n   client\
    \ and server (i.e., locks, leases, stateids, and client IDs) is\n   as described\
    \ below.  The handling differs between migration and\n   replication.\n   If a\
    \ server replica or a server immigrating a file system agrees to,\n   or is expected\
    \ to, accept opaque values from the client that\n   originated from another server,\
    \ then it is a wise implementation\n   practice for the servers to encode the\
    \ \"opaque\" values in network\n   byte order (i.e., in a big-endian format).\
    \  When doing so, servers\n   acting as replicas or immigrating file systems will\
    \ be able to parse\n   values like stateids, directory cookies, filehandles, etc.,\
    \ even if\n   their native byte order is different from that of other servers\n\
    \   cooperating in the replication and migration of the file system.\n"
- title: 6.1.1.  Migration and State
  contents:
  - "6.1.1.  Migration and State\n   In the case of migration, the servers involved\
    \ in the migration of a\n   file system should transfer all server state associated\
    \ with the\n   migrating file system from source to the destination server.  If\n\
    \   state is transferred, this MUST be done in a way that is transparent\n   to\
    \ the client.  This state transfer will ease the client's transition\n   when\
    \ a file system migration occurs.  If the servers are successful\n   in transferring\
    \ all state, the client will continue to use stateids\n   assigned by the original\
    \ server.  Therefore, the new server must\n   recognize these stateids as valid\
    \ and treat them as representing the\n   same locks as they did on the source\
    \ server.\n   In this context, the phrase \"the same locks\" means that:\n   o\
    \  They are associated with the same file.\n   o  They represent the same types\
    \ of locks, whether opens,\n      delegations, advisory byte-range locks, or mandatory\
    \ byte-range\n      locks.\n   o  They have the same lock particulars, including\
    \ such things as\n      access modes, deny modes, and byte ranges.\n   o  They\
    \ are associated with the same owner string(s).\n   If transferring stateids from\
    \ server to server would result in a\n   conflict for an existing stateid for\
    \ the destination server with the\n   existing client, transparent state migration\
    \ MUST NOT happen for that\n   client.  Servers participating in using transparent\
    \ state migration\n   should coordinate their stateid assignment policies to make\
    \ this\n   situation unlikely or impossible.  The means by which this might be\n\
    \   done, like all of the inter-server interactions for migration, are\n   not\
    \ specified by the NFS version 4.0 protocol (neither in [RFC7530]\n   nor this\
    \ update).\n   A client may determine the disposition of migrated state by using\
    \ a\n   stateid associated with the migrated state on the new server.\n   o  If\
    \ the stateid is not valid and an error NFS4ERR_BAD_STATEID is\n      received,\
    \ either transparent state migration has not occurred or\n      the state was\
    \ purged due to a mismatch in the verifier (i.e., the\n      boot instance id).\n\
    \   o  If the stateid is valid, transparent state migration has occurred.\n  \
    \ Since responsibility for an entire file system is transferred with a\n   migration\
    \ event, there is no possibility that conflicts will arise on\n   the destination\
    \ server as a result of the transfer of locks.\n   The servers may choose not\
    \ to transfer the state information upon\n   migration.  However, this choice\
    \ is discouraged, except where\n   specific issues such as stateid conflicts make\
    \ it necessary.  When a\n   server implements migration and it does not transfer\
    \ state\n   information, it MUST provide a file-system-specific grace period,\
    \ to\n   allow clients to reclaim locks associated with files in the migrated\n\
    \   file system.  If it did not do so, clients would have to re-obtain\n   locks,\
    \ with no assurance that a conflicting lock was not granted\n   after the file\
    \ system was migrated and before the lock was re-\n   obtained.\n   In the case\
    \ of migration without state transfer, when the client\n   presents state information\
    \ from the original server (e.g., in a RENEW\n   operation or a READ operation\
    \ of zero length), the client must be\n   prepared to receive either NFS4ERR_STALE_CLIENTID\
    \ or\n   NFS4ERR_BAD_STATEID from the new server.  The client should then\n  \
    \ recover its state information as it normally would in response to a\n   server\
    \ failure.  The new server must take care to allow for the\n   recovery of state\
    \ information as it would in the event of server\n   restart.\n   In those situations\
    \ in which state has not been transferred, as shown\n   by a return of NFS4ERR_BAD_STATEID,\
    \ the client may attempt to reclaim\n   locks in order to take advantage of cases\
    \ in which the destination\n   server has set up a file-system-specific grace\
    \ period in support of\n   the migration.\n"
- title: 6.1.1.1.  Migration and Client IDs
  contents:
  - "6.1.1.1.  Migration and Client IDs\n   The handling of clientid4 values is similar\
    \ to that for stateids.\n   However, there are some differences that derive from\
    \ the fact that a\n   clientid4 is an object that spans multiple file systems\
    \ while a\n   stateid is inherently limited to a single file system.\n   The clientid4\
    \ and nfs_client_id4 information (id string and boot\n   instance id) will be\
    \ transferred with the rest of the state\n   information, and the destination\
    \ server should use that information\n   to determine appropriate clientid4 handling.\
    \  Although the\n   destination server may make state stored under an existing\
    \ lease\n   available under the clientid4 used on the source server, the client\n\
    \   should not assume that this is always so.  In particular,\n   o  If there\
    \ is an existing lease with an nfs_client_id4 that matches\n      a migrated lease\
    \ (same id string and verifier), the server SHOULD\n      merge the two, making\
    \ the union of the sets of stateids available\n      under the clientid4 for the\
    \ existing lease.  As part of the lease\n      merger, the expiration time of\
    \ the lease will reflect renewal done\n      within either of the ancestor leases\
    \ (and so will reflect the\n      latest of the renewals).\n   o  If there is\
    \ an existing lease with an nfs_client_id4 that\n      partially matches a migrated\
    \ lease (same id string and a different\n      (boot) verifier), the server MUST\
    \ eliminate one of the two,\n      possibly invalidating one of the ancestor clientid4s.\
    \  Since boot\n      instance ids are not ordered, the later lease renewal time\
    \ will\n      prevail.\n   o  If the destination server already has the transferred\
    \ clientid4 in\n      use for another purpose, it is free to substitute a different\n\
    \      clientid4 and associate that with the transferred nfs_client_id4.\n   When\
    \ leases are not merged, the transfer of state should result in\n   creation of\
    \ a confirmed client record with empty callback information\n   but matching the\
    \ {v, x, c} with v and x derived from the transferred\n   client information and\
    \ c chosen by the destination server.  For a\n   description of this notation,\
    \ see Section 8.4.5\n   In such cases, the client SHOULD re-establish new callback\n\
    \   information with the new server as soon as possible, according to\n   sequences\
    \ described in sections \"Operation 35: SETCLIENTID --\n   Negotiate Client ID\"\
    \ and \"Operation 36: SETCLIENTID_CONFIRM --\n   Confirm Client ID\".  This ensures\
    \ that server operations are not\n   delayed due to an inability to recall delegations\
    \ and prevents the\n   unwanted revocation of existing delegations.  The client\
    \ can\n   determine the new clientid4 (the value c) from the response to\n   SETCLIENTID.\n\
    \   The client can use its own information about leases with the\n   destination\
    \ server to see if lease merger should have happened.  When\n   there is any ambiguity,\
    \ the client MAY use the above procedure to set\n   the proper callback information\
    \ and find out, as part of the process,\n   the correct value of its clientid4\
    \ with respect to the server in\n   question.\n"
- title: 6.1.1.2.  Migration and State Owner Information
  contents:
  - "6.1.1.2.  Migration and State Owner Information\n   In addition to stateids,\
    \ the locks they represent, and client\n   identity information, servers also\
    \ need to transfer information\n   related to the current status of open-owners\
    \ and lock-owners.\n   This information includes:\n   o  The sequence number of\
    \ the last operation associated with the\n      particular owner.\n   o  Sufficient\
    \ information regarding the results of the last operation\n      to allow reissued\
    \ operations to be correctly responded to.\n   When individual open-owners and\
    \ lock-owners have only been used in\n   connection with a particular file system,\
    \ the server SHOULD transfer\n   this information together with the lock state.\
    \  The owner ceases to\n   exist on the source server and is reconstituted on\
    \ the destination\n   server.  This will happen in the case of clients that have\
    \ been\n   written to isolate each owner to a specific file system, but it may\n\
    \   happen for other clients as well.\n   Note that when servers take this approach\
    \ for all owners whose state\n   is limited to the particular file system being\
    \ migrated, doing so\n   will not cause difficulties for clients not adhering\
    \ to an approach\n   in which owners are isolated to particular file systems.\
    \  As long as\n   the client recognizes the loss of transferred state, the protocol\n\
    \   allows the owner in question to disappear, and the client may have to\n  \
    \ deal with an owner confirmation request that would not have occurred\n   in\
    \ the absence of the migration.\n   When migration occurs and the source server\
    \ discovers an owner whose\n   state includes the migrated file system but other\
    \ file systems as\n   well, it cannot transfer the associated owner state.  Instead,\
    \ the\n   existing owner state stays in place, but propagation of owner state\n\
    \   is done as specified below:\n   o  When the current seqid for an owner represents\
    \ an operation\n      associated with the file system being migrated, owner status\n\
    \      SHOULD be propagated to the destination file system.\n   o  When the current\
    \ seqid for an owner does not represent an\n      operation associated with the\
    \ file system being migrated, owner\n      status MAY be propagated to the destination\
    \ file system.\n   o  When the owner in question has never been used for an operation\n\
    \      involving the migrated file system, the owner information SHOULD\n    \
    \  NOT be propagated to the destination file system.\n   Note that a server may\
    \ obey all of the conditions above without the\n   overhead of keeping track of\
    \ a set of file systems that any\n   particular owner has been associated with.\
    \  Consider a situation in\n   which the source server has decided to keep lock-related\
    \ state\n   associated with a file system fixed, preparatory to propagating it\
    \ to\n   the destination file system.  If a client is free to create new locks\n\
    \   associated with existing owners on other file systems, the owner\n   information\
    \ may be propagated to the destination file system, even\n   though, at the time\
    \ the file system migration is recognized by the\n   client to have occurred,\
    \ the last operation associated with the owner\n   may not be associated with\
    \ the migrating file system.\n   When a source server propagates owner-related\
    \ state associated with\n   owners that span multiple file systems, it will propagate\
    \ the owner\n   sequence value to the destination server, while retaining it on\
    \ the\n   source server, as long as there exists state associated with the\n \
    \  owner.  When owner information is propagated in this way, source and\n   destination\
    \ servers start with the same owner sequence value that is\n   then updated independently,\
    \ as the client makes owner-related\n   requests to the servers.  Note that each\
    \ server will have some period\n   in which the associated sequence value for\
    \ an owner is identical to\n   the one transferred as part of migration.  At those\
    \ times, when a\n   server receives a request with a matching owner sequence value,\
    \ it\n   MUST NOT respond with the associated stored response if the\n   associated\
    \ file system is not, when the reissued request is received,\n   part of the set\
    \ of file systems handled by that server.\n   One sort of case may require more\
    \ complex handling.  When multiple\n   file systems are migrated, in sequence,\
    \ to a specific destination\n   server, an owner may be migrated to a destination\
    \ server, on which it\n   was already present, leading to the issue of how the\
    \ resident owner\n   information and that being newly migrated are to be reconciled.\n\
    \   If file system migration encounters a situation where owner\n   information\
    \ needs to be merged, it MAY decline to transfer such\n   state, even if it chooses\
    \ to handle other cases in which locks for a\n   given owner are spread among\
    \ multiple file systems.\n   As a way of understanding the situations that need\
    \ to be addressed\n   when owner information needs to be merged, consider the\
    \ following\n   scenario:\n   o  There is client C and two servers, X and Y. \
    \ There are two\n      clientid4s designating C, which are referred to as CX and\
    \ CY.\n   o  Initially, server X supports file systems F1, F2, F3, and F4.\n \
    \     These will be migrated, one at a time, to server Y.\n   o  While these migrations\
    \ are proceeding, the client makes locking\n      requests for file systems F1\
    \ through F4 on behalf of owner O\n      (either a lock-owner or an open-owner),\
    \ with each request going to\n      X or Y depending on where the relevant file\
    \ system is being\n      supported at the time the request is made.\n   o  Once\
    \ the first migration event occurs, client C will maintain two\n      instances\
    \ for owner O, one for each server.\n   o  It is always possible that C may make\
    \ a request of server X\n      relating to owner O, and before receiving a response,\
    \ it finds the\n      target file system has moved to Y and needs to reissue the\
    \ request\n      to server Y.\n   o  At the same time, C may make a request of\
    \ server Y relating to\n      owner O, and this too may encounter a lost-response\
    \ situation.\n   As a result of such merger situations, the server will need to\n\
    \   provide support for dealing with retransmission of owner-sequenced\n   requests\
    \ that diverge from the typical model in which there is\n   support for retransmission\
    \ of replies only for a request whose\n   sequence value exactly matches the last\
    \ one sent.  In some\n   situations, there may be two requests, each of which\
    \ had the last\n   sequence when it was issued.  As a result of migration and\
    \ owner\n   merger, one of those will no longer be the last by sequence.\n   When\
    \ servers do support such merger of owner information on the\n   destination server,\
    \ the following rules are to be adhered to:\n   o  When an owner sequence value\
    \ is propagated to a destination server\n      where it already exists, the resulting\
    \ sequence value is to be the\n      greater of the one present on the destination\
    \ server and the one\n      being propagated as part of migration.\n   o  In the\
    \ event that an owner sequence value on a server represents a\n      request applying\
    \ to a file system currently present on the server,\n      it is not to be rendered\
    \ invalid simply because that sequence\n      value is changed as a result of\
    \ owner information propagation as\n      part of file system migration.  Instead,\
    \ it is retained until it\n      can be deduced that the client in question has\
    \ received the reply.\n   As a result of the operation of these rules, there are\
    \ three ways in\n   which there can be more reply data than what is typically\
    \ present,\n   i.e., data for a single request per owner whose sequence is the\
    \ last\n   one received, where the next sequence to be used is one beyond that.\n\
    \   o  When the owner sequence value for a migrating file system is\n      greater\
    \ than the corresponding value on the destination server,\n      the last request\
    \ for the owner in effect at the destination server\n      needs to be retained,\
    \ even though it is no longer one less than\n      the next sequence to be received.\n\
    \   o  When the owner sequence value for a migrating file system is less\n   \
    \   than the corresponding value on the destination server, the\n      sequence\
    \ number for last request for the owner in effect on the\n      migrating file\
    \ system needs to be retained, even though it is no\n      longer than one less\
    \ the next sequence to be received.\n   o  When the owner sequence value for a\
    \ migrating file system is equal\n      to the corresponding value on the destination\
    \ server, one has two\n      different \"last\" requests that both must be retained.\
    \  The next\n      sequence value to be used is one beyond the sequence value\
    \ shared\n      by these two requests.\n   Here are some guidelines as to when\
    \ servers can drop such additional\n   reply data, which is created as part of\
    \ owner information migration.\n   o  The server SHOULD NOT drop this information\
    \ simply because it\n      receives a new sequence value for the owner in question,\
    \ since\n      that request may have been issued before the client was aware of\n\
    \      the migration event.\n   o  The server SHOULD drop this information if\
    \ it receives a new\n      sequence value for the owner in question, and the request\
    \ relates\n      to the same file system.\n   o  The server SHOULD drop the part\
    \ of this information that relates\n      to non-migrated file systems if it receives\
    \ a new sequence value\n      for the owner in question, and the request relates\
    \ to a non-\n      migrated file system.\n   o  The server MAY drop this information\
    \ when it receives a new\n      sequence value for the owner in question for a\
    \ considerable period\n      of time (more than one or two lease periods) after\
    \ the migration\n      occurs.\n"
- title: 6.1.2.  Replication and State
  contents:
  - "6.1.2.  Replication and State\n   Since client switch-over in the case of replication\
    \ is not under\n   server control, the handling of state is different.  In this\
    \ case,\n   leases, stateids, and client IDs do not have validity across a\n \
    \  transition from one server to another.  The client must re-establish\n   its\
    \ locks on the new server.  This can be compared to the re-\n   establishment\
    \ of locks by means of reclaim-type requests after a\n   server reboot.  The difference\
    \ is that the server has no provision to\n   distinguish requests reclaiming locks\
    \ from those obtaining new locks\n   or to defer the latter.  Thus, a client re-establishing\
    \ a lock on the\n   new server (by means of a LOCK or OPEN request) may have the\
    \ requests\n   denied due to a conflicting lock.  Since replication is intended\
    \ for\n   read-only use of file systems, such denial of locks should not pose\n\
    \   large difficulties in practice.  When an attempt to re-establish a\n   lock\
    \ on a new server is denied, the client should treat the situation\n   as if its\
    \ original lock had been revoked.\n"
- title: 6.1.3.  Notification of Migrated Lease
  contents:
  - "6.1.3.  Notification of Migrated Lease\n   A file system can be migrated to another\
    \ server while a client that\n   has state related to that file system is not\
    \ actively submitting\n   requests to it.  In this case, the migration is reported\
    \ to the\n   client during lease renewal.  Lease renewal can occur either\n  \
    \ explicitly via a RENEW operation or implicitly when the client\n   performs\
    \ a lease-renewing operation on another file system on that\n   server.\n   In\
    \ order for the client to schedule renewal of leases that may have\n   been relocated\
    \ to the new server, the client must find out about\n   lease relocation before\
    \ those leases expire.  Similarly, when\n   migration occurs but there has not\
    \ been transparent state migration,\n   the client needs to find out about the\
    \ change soon enough to be able\n   to reclaim the lock within the destination\
    \ server's grace period.  To\n   accomplish this, all operations that implicitly\
    \ renew leases for a\n   client (such as OPEN, CLOSE, READ, WRITE, RENEW, LOCK,\
    \ and others)\n   will return the error NFS4ERR_LEASE_MOVED if responsibility\
    \ for any\n   of the leases to be renewed has been transferred to a new server.\n\
    \   Note that when the transfer of responsibility leaves remaining state\n   for\
    \ that lease on the source server, the lease is renewed just as it\n   would have\
    \ been in the NFS4ERR_OK case, despite returning the error.\n   The transfer of\
    \ responsibility happens when the server receives a\n   GETATTR(fs_locations)\
    \ from the client for each file system for which\n   a lease has been moved to\
    \ a new server.  Normally, it does this after\n   receiving an NFS4ERR_MOVED for\
    \ an access to the file system, but the\n   server is not required to verify that\
    \ this happens in order to\n   terminate the return of NFS4ERR_LEASE_MOVED.  By\
    \ convention, the\n   compounds containing GETATTR(fs_locations) SHOULD include\
    \ an appended\n   RENEW operation to permit the server to identify the client\
    \ getting\n   the information.\n   Note that the NFS4ERR_LEASE_MOVED error is\
    \ required only when\n   responsibility for at least one stateid has been affected.\
    \  In the\n   case of a null lease, where the only associated state is a clientid4,\n\
    \   an NFS4ERR_LEASE_MOVED error SHOULD NOT be generated.\n   Upon receiving the\
    \ NFS4ERR_LEASE_MOVED error, a client that supports\n   file system migration\
    \ MUST perform the necessary GETATTR operation\n   for each of the file systems\
    \ containing state that have been\n   migrated, so it gives the server evidence\
    \ that it is aware of the\n   migration of the file system.  Once the client has\
    \ done this for all\n   migrated file systems on which the client holds state,\
    \ the server\n   MUST resume normal handling of stateful requests from that client.\n\
    \   One way in which clients can do this efficiently in the presence of\n   large\
    \ numbers of file systems is described below.  This approach\n   divides the process\
    \ into two phases: one devoted to finding the\n   migrated file systems, and the\
    \ second devoted to doing the necessary\n   GETATTRs.\n   The client can find\
    \ the migrated file systems by building and issuing\n   one or more COMPOUND requests,\
    \ each consisting of a set of PUTFH/\n   GETFH pairs, each pair using a filehandle\
    \ in one of the file systems\n   in question.  All such COMPOUND requests can\
    \ be done in parallel.\n   The successful completion of such a request indicates\
    \ that none of\n   the file systems interrogated have been migrated while termination\n\
    \   with NFS4ERR_MOVED indicates that the file system getting the error\n   has\
    \ migrated while those interrogated before it in the same COMPOUND\n   have not.\
    \  Those whose interrogation follows the error remain in an\n   uncertain state\
    \ and can be interrogated by restarting the requests\n   from after the point\
    \ at which NFS4ERR_MOVED was returned or by\n   issuing a new set of COMPOUND\
    \ requests for the file systems that\n   remain in an uncertain state.\n   Once\
    \ the migrated file systems have been found, all that is needed is\n   for the\
    \ client to give evidence to the server that it is aware of the\n   migrated status\
    \ of file systems found by this process, by\n   interrogating the fs_locations\
    \ attribute for a filehandle within each\n   of the migrated file systems.  The\
    \ client can do this by building and\n   issuing one or more COMPOUND requests,\
    \ each of which consists of a\n   set of PUTFH operations, each followed by a\
    \ GETATTR of the\n   fs_locations attribute.  A RENEW is necessary to enable the\n\
    \   operations to be associated with the lease returning\n   NFS4ERR_LEASE_MOVED.\
    \  Once the client has done this for all migrated\n   file systems on which the\
    \ client holds state, the server will resume\n   normal handling of stateful requests\
    \ from that client.\n   In order to support legacy clients that do not handle\
    \ the\n   NFS4ERR_LEASE_MOVED error correctly, the server SHOULD time out after\n\
    \   a wait of at least two lease periods, at which time it will resume\n   normal\
    \ handling of stateful requests from all clients.  If a client\n   attempts to\
    \ access the migrated files, the server MUST reply with\n   NFS4ERR_MOVED.  In\
    \ this situation, it is likely that the client would\n   find its lease expired,\
    \ although a server may use \"courtesy\" locks\n   (as described in Section 9.6.3.1\
    \ of [RFC7530]) to mitigate the issue.\n   When the client receives an NFS4ERR_MOVED\
    \ error, the client can\n   follow the normal process to obtain the destination\
    \ server\n   information (through the fs_locations attribute) and perform renewal\n\
    \   of those leases on the new server.  If the server has not had state\n   transferred\
    \ to it transparently, the client will receive either\n   NFS4ERR_STALE_CLIENTID\
    \ or NFS4ERR_STALE_STATEID from the new server,\n   as described above.  The client\
    \ can then recover state information as\n   it does in the event of server failure.\n\
    \   Aside from recovering from a migration, there are other reasons a\n   client\
    \ may wish to retrieve fs_locations information from a server.\n   When a server\
    \ becomes unresponsive, for example, a client may use\n   cached fs_locations\
    \ data to discover an alternate server hosting the\n   same file system data.\
    \  A client may periodically request\n   fs_locations data from a server in order\
    \ to keep its cache of\n   fs_locations data fresh.\n   Since a GETATTR(fs_locations)\
    \ operation would be used for refreshing\n   cached fs_locations data, a server\
    \ could mistake such a request as\n   indicating recognition of an NFS4ERR_LEASE_MOVED\
    \ condition.\n   Therefore, a compound that is not intended to signal that a client\n\
    \   has recognized a migrated lease SHOULD be prefixed with a guard\n   operation\
    \ that fails with NFS4ERR_MOVED if the filehandle being\n   queried is no longer\
    \ present on the server.  The guard can be as\n   simple as a GETFH operation.\n\
    \   Though unlikely, it is possible that the target of such a compound\n   could\
    \ be migrated in the time after the guard operation is executed\n   on the server\
    \ but before the GETATTR(fs_locations) operation is\n   encountered.  When a client\
    \ issues a GETATTR(fs_locations) operation\n   as part of a compound not intended\
    \ to signal recognition of a\n   migrated lease, it SHOULD be prepared to process\
    \ fs_locations data in\n   the reply that shows the current location of the file\
    \ system is gone.\n"
- title: 6.1.4.  Migration and the lease_time Attribute
  contents:
  - "6.1.4.  Migration and the lease_time Attribute\n   In order that the client may\
    \ appropriately manage its leases in the\n   case of migration, the destination\
    \ server must establish proper\n   values for the lease_time attribute.\n   When\
    \ state is transferred transparently, that state should include\n   the correct\
    \ value of the lease_time attribute.  The lease_time\n   attribute on the destination\
    \ server must never be less than that on\n   the source since this would result\
    \ in premature expiration of leases\n   granted by the source server.  Upon migration\
    \ in which state is\n   transferred transparently, the client is under no obligation\
    \ to\n   refetch the lease_time attribute and may continue to use the value\n\
    \   previously fetched (on the source server).\n   In the case in which lease\
    \ merger occurs as part of state transfer,\n   the lease_time attribute of the\
    \ destination lease remains in effect.\n   The client can simply renew that lease\
    \ with its existing lease_time\n   attribute.  State in the source lease is renewed\
    \ at the time of\n   transfer so that it cannot expire, as long as the destination\
    \ lease\n   is appropriately renewed.\n   If state has not been transferred transparently\
    \ (i.e., the client\n   needs to reclaim or re-obtain its locks), the client should\
    \ fetch the\n   value of lease_time on the new (i.e., destination) server, and\
    \ use it\n   for subsequent locking requests.  However, the server must respect\
    \ a\n   grace period at least as long as the lease_time on the source server,\n\
    \   in order to ensure that clients have ample time to reclaim their\n   locks\
    \ before potentially conflicting non-reclaimed locks are granted.\n   The means\
    \ by which the new server obtains the value of lease_time on\n   the old server\
    \ is left to the server implementations.  It is not\n   specified by the NFS version\
    \ 4.0 protocol.\n"
- title: 7.  Server Implementation Considerations
  contents:
  - "7.  Server Implementation Considerations\n   This section provides suggestions\
    \ to help server implementers deal\n   with issues involved in the transparent\
    \ transfer of file-system-\n   related data between servers.  Servers are not\
    \ obliged to follow\n   these suggestions but should be sure that their approach\
    \ to the\n   issues handle all the potential problems addressed below.\n"
- title: 7.1.  Relation of Locking State Transfer to Other Aspects of File System
  contents:
  - "7.1.  Relation of Locking State Transfer to Other Aspects of File System\n  \
    \    Motion\n   In many cases, state transfer will be part of a larger function\n\
    \   wherein the contents of a file system are transferred from server to\n   server.\
    \  Although specifics will vary with the implementation, the\n   relation between\
    \ the transfer of persistent file data and metadata\n   and the transfer of state\
    \ will typically be described by one of the\n   cases below.\n   o  In some implementations,\
    \ access to the on-disk contents of a file\n      system can be transferred from\
    \ server to server by making the\n      storage devices on which the file system\
    \ resides physically\n      accessible from multiple servers, and transferring\
    \ the right and\n      responsibility for handling that file system from server\
    \ to\n      server.\n      In such implementations, the transfer of locking state\
    \ happens on\n      its own, as described in Section 7.2.  The transfer of physical\n\
    \      access to the file system happens after the locking state is\n      transferred\
    \ and before any subsequent access to the file system.\n      In cases where such\
    \ transfer is not instantaneous, there will be a\n      period in which all operations\
    \ on the file system are held off,\n      either by having the operations themselves\
    \ return NFS4ERR_DELAY\n      or, where this is not allowed, by using the techniques\
    \ described\n      below in Section 7.2.\n   o  In other implementations, file\
    \ system data and metadata must be\n      copied from the server where they have\
    \ existed to the destination\n      server.  Because of the typical amounts of\
    \ data involved, it is\n      generally not practical to hold off access to the\
    \ file system\n      while this transfer is going on.  Normal access to the file\n\
    \      system, including modifying operations, will generally happen\n      while\
    \ the transfer is going on.\n      Eventually, the file system copying process\
    \ will complete.  At\n      this point, there will be two valid copies of the\
    \ file system, one\n      on each of the source and destination servers.  Servers\
    \ may\n      maintain that state of affairs by making sure that each\n      modification\
    \ to file system data is done on both the source and\n      destination servers.\n\
    \      Although the transfer of locking state can begin before the above\n   \
    \   state of affairs is reached, servers will often wait until it is\n      arrived\
    \ at to begin transfer of locking state.  Once the transfer\n      of locking\
    \ state is completed, as described in the section below,\n      clients may be\
    \ notified of the migration event and access the\n      destination file system\
    \ on the destination server.\n   o  Another case in which file system data and\
    \ metadata must be copied\n      from server to server involves a variant of the\
    \ pattern above.  In\n      cases in which a single file system moves between\
    \ or among a small\n      set of servers, it will transition to a server on which\
    \ a previous\n      instantiation of that same file system existed before.  In\
    \ such\n      cases, it is often more efficient to update the previous file\n\
    \      system instance to reflect changes made while the active file\n      system\
    \ was residing elsewhere rather than copying the file system\n      data anew.\n\
    \      In such cases, the copying of file system data and metadata is\n      replaced\
    \ by a process that validates each visible file system\n      object, copying\
    \ new objects and updating those that have changed\n      since the file system\
    \ was last present on the destination server.\n      Although this process is\
    \ generally shorter than a complete copy,\n      it is generally long enough that\
    \ it is not practical to hold off\n      access to the file system while this\
    \ update is going on.\n      Eventually, the file system updating process will\
    \ complete.  At\n      this point, there will be two valid copies of the file\
    \ system, one\n      on each of the source and destination servers.  Servers may\n\
    \      maintain that state of affairs just as is done in the previous\n      case.\
    \  Similarly, the transfer of locking state, once it is\n      complete, allows\
    \ the clients to be notified of the migration event\n      and access the destination\
    \ file system on the destination server.\n"
- title: 7.2.  Preventing Locking State Modification during Transfer
  contents:
  - "7.2.  Preventing Locking State Modification during Transfer\n   When transferring\
    \ locking state from the source to a destination\n   server, there will be occasions\
    \ when the source server will need to\n   prevent operations that modify the state\
    \ being transferred.  For\n   example, if the locking state at time T is sent\
    \ to the destination\n   server, any state change that occurs on the source server\
    \ after that\n   time but before the file system transfer is made effective will\
    \ mean\n   that the state on the destination server will differ from that on the\n\
    \   source server, which matches what the client would expect to see.\n   In general,\
    \ a server can prevent some set of server-maintained data\n   from changing by\
    \ returning NFS4ERR_DELAY on operations that attempt\n   to change that data.\
    \  In the case of locking state for NFSv4.0, there\n   are two specific issues\
    \ that might interfere:\n   o  Returning NFS4ERR_DELAY will not prevent state\
    \ from changing in\n      that owner-based sequence values will still change,\
    \ even though\n      NFS4ERR_DELAY is returned.  For example, OPEN and LOCK will\
    \ change\n      state (in the form of owner seqid values) even when they return\n\
    \      NFS4ERR_DELAY.\n   o  Some operations that modify locking state are not\
    \ allowed to\n      return NFS4ERR_DELAY (i.e., OPEN_CONFIRM, RELEASE_LOCKOWNER,\
    \ and\n      RENEW).\n   Note that the first problem and most instances of the\
    \ second can be\n   addressed by returning NFS4ERR_DELAY on the operations that\
    \ establish\n   a filehandle within the target as one of the filehandles associated\n\
    \   with the request, i.e., as either the current or saved filehandle.\n   This\
    \ would require returning NFS4ERR_DELAY under the following\n   circumstances:\n\
    \   o  On a PUTFH that specifies a filehandle within the target file\n      system.\n\
    \   o  On a LOOKUP or LOOKUPP that crosses into the target file system.\n   As\
    \ a result of doing this, OPEN_CONFIRM is dealt with, leaving only\n   RELEASE_LOCKOWNER\
    \ and RENEW still to be dealt with.\n   Note that if the server establishes and\
    \ maintains a situation in\n   which no request has, as either the current or\
    \ saved filehandle, a\n   filehandle within the target file system, no special\
    \ handling of\n   SAVEFH or RESTOREFH is required.  Thus, the fact that these\n\
    \   operations cannot return NFS4ERR_DELAY is not a problem since neither\n  \
    \ will establish a filehandle in the target file system as the current\n   filehandle.\n\
    \   If the server is to establish the situation described above, it may\n   have\
    \ to take special note of long-running requests that started\n   before state\
    \ migration.  Part of any solution to this issue will\n   involve distinguishing\
    \ two separate points in time at which handling\n   for the target file system\
    \ will change.  Let us distinguish:\n   o  A time T after which the previously\
    \ mentioned operations will\n      return NFS4ERR_DELAY.\n   o  A later time T'\
    \ at which the server can consider file system\n      locking state fixed, making\
    \ it possible for it to be sent to the\n      destination server.\n   For a server\
    \ to decide on T', it must ensure that requests started\n   before T cannot change\
    \ target file system locking state, given that\n   all those started after T are\
    \ dealt with by returning NFS4ERR_DELAY\n   upon setting filehandles within the\
    \ target file system.  Among the\n   ways of doing this are:\n   o  Keeping track\
    \ of the earliest request started that is still in\n      execution (for example,\
    \ by keeping a list of active requests\n      ordered by request start time).\
    \  Requests that started before and\n      are still in progress at time T may\
    \ potentially affect the locking\n      state; once the starting time of the earliest-started\
    \ active\n      request is later than T, the starting time of the first such\n\
    \      request can be chosen as T' by the server since any request in\n      progress\
    \ after T' started after time T.  Accordingly, it would not\n      have been allowed\
    \ to change locking state for the migrating file\n      system and would have\
    \ returned NFS4ERR_DELAY had it tried to make\n      a change.\n   o  Keeping\
    \ track of the count of requests started before time T that\n      have a filehandle\
    \ within the target file system as either the\n      current or saved filehandle.\
    \  The server can then define T' to be\n      the first time after T at which\
    \ the count is zero.\n   The set of operations that change locking state include\
    \ two that\n   cannot be dealt with by the above approach, because they are not\n\
    \   specific to a particular file system and do not use a current\n   filehandle\
    \ as an implicit parameter.\n   o  RENEW can be dealt with by applying the renewal\
    \ to state for non-\n      transitioning file systems.  The effect of renewal\
    \ for the\n      transitioning file system can be ignored, as long as the servers\n\
    \      make sure that the lease on the destination server has an\n      expiration\
    \ time that is no earlier than the latest renewal done on\n      the source server.\
    \  This can be easily accomplished by making the\n      lease expiration on the\
    \ destination server equal to the time in\n      which the state transfer was\
    \ completed plus the lease period.\n   o  RELEASE_LOCKOWNER can be handled by\
    \ propagating the fact of the\n      lock-owner deletion (e.g., by using an RPC)\
    \ to the destination\n      server.  Such a propagation RPC can be done as part\
    \ of the\n      operation, or the existence of the deletion can be recorded\n\
    \      locally and propagation of owner deletions to the destination\n      server\
    \ done as a batch later.  In either case, the actual\n      deletions on the destination\
    \ server have to be delayed until all\n      of the other state information has\
    \ been transferred.\n      Alternatively, RELEASE_LOCKOWNER can be dealt with\
    \ by returning\n      NFS4ERR_DELAY.  In order to avoid compatibility issues for\
    \ clients\n      not prepared to accept NFS4ERR_DELAY in response to\n      RELEASE_LOCKOWNER,\
    \ care must be exercised.  (See Section 8.3 for\n      details.)\n   The approach\
    \ outlined above, wherein NFS4ERR_DELAY is returned based\n   primarily on the\
    \ use of current and saved filehandles in the file\n   system, prevents all reference\
    \ to the transitioning file system\n   rather than limiting the delayed operations\
    \ to those that change\n   locking state on the transitioning file system.  Because\
    \ of this,\n   servers may choose to limit the time during which this broad approach\n\
    \   is used by adopting a layered approach to the issue.\n   o  During the preparatory\
    \ phase, operations that change, create, or\n      destroy locks or modify the\
    \ valid set of stateids will return\n      NFS4ERR_DELAY.  During this phase,\
    \ owner-associated seqids may\n      change, and the identity of the file system\
    \ associated with the\n      last request for a given owner may change as well.\
    \  Also,\n      RELEASE_LOCKOWNER operations may be processed without returning\n\
    \      NFS4ERR_DELAY as long as the fact of the lock-owner deletion is\n     \
    \ recorded locally for later transmission.\n   o  During the restrictive phase,\
    \ operations that change locking state\n      for the file system in transition\
    \ are prevented by returning\n      NFS4ERR_DELAY on any attempt to make a filehandle\
    \ within that file\n      system either the current or saved filehandle for a\
    \ request.\n      RELEASE_LOCKOWNER operations may return NFS4ERR_DELAY, but if\
    \ they\n      are processed, the lock-owner deletion needs to be communicated\n\
    \      immediately to the destination server.\n   A possible sequence would be\
    \ the following.\n   o  The server enters the preparatory phase for the transitioning\
    \ file\n      system.\n   o  At this point, locking state, including stateids,\
    \ locks, and owner\n      strings, is transferred to the destination server. \
    \ The seqids\n      associated with owners are either not transferred or transferred\n\
    \      on a provisional basis, subject to later change.\n   o  After the above\
    \ has been transferred, the server may enter the\n      restrictive phase for\
    \ the file system.\n   o  At this point, the updated seqid values may be sent\
    \ to the\n      destination server.\n      Reporting regarding pending owner deletions\
    \ (as a result of\n      RELEASE_LOCKOWNER operations) can be communicated at\
    \ the same\n      time.\n   o  Once it is known that all of this information has\
    \ been transferred\n      to the destination server, and there are no pending\n\
    \      RELEASE_LOCKOWNER notifications outstanding, the source server may\n  \
    \    treat the file system transition as having occurred and return\n      NFS4ERR_MOVED\
    \ when an attempt is made to access it.\n"
- title: 8.  Additional Changes
  contents:
  - "8.  Additional Changes\n   This section contains a number of items that relate\
    \ to the changes in\n   the section above, but which, for one reason or another,\
    \ exist in\n   different portions of the specification to be updated.\n"
- title: 8.1.  Summary of Additional Changes from Previous Documents
  contents:
  - "8.1.  Summary of Additional Changes from Previous Documents\n   Summarized here\
    \ are all the remaining changes, not included in the\n   two main sections.\n\
    \   o  New definition of the error NFS4ERR_CLID_INUSE, appearing in\n      Section\
    \ 8.2.  This replaces the definition in Section 13.1.10.1 in\n      [RFC7530].\n\
    \   o  A revision of the error definitions section to allow\n      RELEASE_LOCKOWNER\
    \ to return NFS4ERR_DELAY, with appropriate\n      constraints to assure interoperability\
    \ with clients not expecting\n      this error to be returned.  These changes\
    \ are discussed in\n      Section 8.2 and modify the error tables in Sections\
    \ 13.2 and 13.4\n      in [RFC7530].\n   o  A revised description of SETCLIENTID,\
    \ appearing in Section 8.4.\n      This brings the description into sync with\
    \ the rest of the\n      specification regarding NFS4ERR_CLID_INUSE.  The revised\n\
    \      description replaces the one in Section 16.33 of [RFC7530].\n   o  Some\
    \ security-related changes appear in Sections 8.5 and 8.6.  The\n      Security\
    \ Considerations section of this document (Section 9)\n      describes the effect\
    \ on the corresponding section (Section 19) in\n      [RFC7530].\n"
- title: 8.2.  NFS4ERR_CLID_INUSE Definition
  contents:
  - "8.2.  NFS4ERR_CLID_INUSE Definition\n   The definition of this error is now as\
    \ follows:\n      The SETCLIENTID operation has found that the id string within\
    \ the\n      specified nfs_client_id4 was previously presented with a different\n\
    \      principal and that client instance currently holds an active\n      lease.\
    \  A server MAY return this error if the same principal is\n      used, but a\
    \ change in authentication flavor gives good reason to\n      reject the new SETCLIENTID\
    \ operation as not bona fide.\n"
- title: 8.3.  NFS4ERR_DELAY Return from RELEASE_LOCKOWNER
  contents:
  - "8.3.  NFS4ERR_DELAY Return from RELEASE_LOCKOWNER\n   The existing error tables\
    \ should be considered modified to allow\n   NFS4ERR_DELAY to be returned by RELEASE_LOCKOWNER.\
    \  However, the\n   scope of this addition is limited and is not to be considered\
    \ as\n   making this error return generally acceptable.\n   It needs to be made\
    \ clear that servers may not return this error to\n   clients not prepared to\
    \ support file system migration.  Such clients\n   may be following the error\
    \ specifications in [RFC7530] and so might\n   not expect NFS4ERR_DELAY to be\
    \ returned on RELEASE_LOCKOWNER.\n   The following constraint applies to this\
    \ additional error return, as\n   if it were a note appearing together with the\
    \ newly allowed error\n   code:\n      In order to make server state fixed for\
    \ a file system being\n      migrated, a server MAY return NFS4ERR_DELAY in response\
    \ to a\n      RELEASE_LOCKOWNER that will affect locking state being propagated\n\
    \      to a destination server.  The source server MUST NOT do so unless\n   \
    \   it is likely that it will later return NFS4ERR_MOVED for the file\n      system\
    \ in question.\n      In the context of lock-owner release, the set of file systems,\n\
    \      such that server state being made fixed can result in\n      NFS4ERR_DELAY,\
    \ must include the file system on which the operation\n      associated with the\
    \ current lock-owner seqid was performed.\n      In addition, this set may include\
    \ other file systems on which an\n      operation associated with an earlier seqid\
    \ for the current lock-\n      owner seqid was performed, since servers will have\
    \ to deal with\n      the issue of an owner being used in succession for multiple\
    \ file\n      systems.\n      Thus, if a client is prepared to receive NFS4ERR_MOVED\
    \ after\n      creating state associated with a given file system, it also needs\n\
    \      to be prepared to receive NFS4ERR_DELAY in response to\n      RELEASE_LOCKOWNER,\
    \ if it has used that owner in connection with a\n      file on that file system.\n"
- title: '8.4.  Operation 35: SETCLIENTID -- Negotiate Client ID'
  contents:
  - '8.4.  Operation 35: SETCLIENTID -- Negotiate Client ID

    '
- title: 8.4.1.  SYNOPSIS
  contents:
  - "8.4.1.  SYNOPSIS\n     client, callback, callback_ident -> clientid, setclientid_confirm\n"
- title: 8.4.2.  ARGUMENT
  contents:
  - "8.4.2.  ARGUMENT\n   struct SETCLIENTID4args {\n           nfs_client_id4  client;\n\
    \           cb_client4      callback;\n           uint32_t        callback_ident;\n\
    \   };\n"
- title: 8.4.3.  RESULT
  contents:
  - "8.4.3.  RESULT\n   struct SETCLIENTID4resok {\n           clientid4       clientid;\n\
    \           verifier4       setclientid_confirm;\n   };\n   union SETCLIENTID4res\
    \ switch (nfsstat4 status) {\n    case NFS4_OK:\n            SETCLIENTID4resok\
    \      resok4;\n    case NFS4ERR_CLID_INUSE:\n            clientaddr4    client_using;\n\
    \   default:\n            void;\n   };\n"
- title: 8.4.4.  DESCRIPTION
  contents:
  - "8.4.4.  DESCRIPTION\n   The client uses the SETCLIENTID operation to notify the\
    \ server of its\n   intention to use a particular client identifier, callback,\
    \ and\n   callback_ident for subsequent requests that entail creating lock,\n\
    \   share reservation, and delegation state on the server.  Upon\n   successful\
    \ completion, the server will return a shorthand client ID\n   that, if confirmed\
    \ via a separate step, will be used in subsequent\n   file locking and file open\
    \ requests.  Confirmation of the client ID\n   must be done via the SETCLIENTID_CONFIRM\
    \ operation to return the\n   client ID and setclientid_confirm values, as verifiers,\
    \ to the\n   server.  The reason why two verifiers are necessary is that it is\n\
    \   possible to use SETCLIENTID and SETCLIENTID_CONFIRM to modify the\n   callback\
    \ and callback_ident information but not the shorthand client\n   ID.  In that\
    \ event, the setclientid_confirm value is effectively the\n   only verifier.\n\
    \   The callback information provided in this operation will be used if\n   the\
    \ client is provided an open delegation at a future point.\n   Therefore, the\
    \ client must correctly reflect the program and port\n   numbers for the callback\
    \ program at the time SETCLIENTID is used.\n   The callback_ident value is used\
    \ by the server on the callback.  The\n   client can leverage the callback_ident\
    \ to eliminate the need for more\n   than one callback RPC program number, while\
    \ still being able to\n   determine which server is initiating the callback.\n"
- title: 8.4.5.  IMPLEMENTATION
  contents:
  - "8.4.5.  IMPLEMENTATION\n   To specify the implementation of SETCLIENTID, the\
    \ following notations\n   are used.\n   Let:\n   x  be the value of the client.id\
    \ subfield of the SETCLIENTID4args\n      structure.\n   v  be the value of the\
    \ client.verifier subfield of the\n      SETCLIENTID4args structure.\n   c  be\
    \ the value of the client ID field returned in the\n      SETCLIENTID4resok structure.\n\
    \   k  represent the value combination of the callback and callback_ident\n  \
    \    fields of the SETCLIENTID4args structure.\n   s  be the setclientid_confirm\
    \ value returned in the SETCLIENTID4resok\n      structure.\n   { v, x, c, k,\
    \ s }  be a quintuple for a client record.  A client\n      record is confirmed\
    \ if there has been a SETCLIENTID_CONFIRM\n      operation to confirm it.  Otherwise,\
    \ it is unconfirmed.  An\n      unconfirmed record is established by a SETCLIENTID\
    \ call.\n"
- title: 8.4.5.1.  IMPLEMENTATION (Preparatory Phase)
  contents:
  - "8.4.5.1.  IMPLEMENTATION (Preparatory Phase)\n   Since SETCLIENTID is a non-idempotent\
    \ operation, our treatment\n   assumes use of a duplicate request cache (DRC).\
    \  For a discussion of\n   the DRC, see Section 9.1.7 of [RFC7530].\n   When the\
    \ server gets a SETCLIENTID { v, x, k } request, it first does\n   a number of\
    \ preliminary checks as listed below before proceeding to\n   the main part of\
    \ SETCLIENTID processing.\n   o  It first looks up the request in the DRC.  If\
    \ there is a hit, it\n      returns the result cached in the DRC.  The server\
    \ does NOT remove\n      client state (locks, shares, delegations) nor does it\
    \ modify any\n      recorded callback and callback_ident information for client\
    \ { x }.\n      The server now proceeds to the main part of SETCLIENTID.\n   o\
    \  Otherwise (i.e., in the case of any DRC miss), the server takes\n      the\
    \ client ID string x and searches for confirmed client records\n      for x that\
    \ the server may have recorded from previous SETCLIENTID\n      calls.  If there\
    \ are no such records, or if all such records have\n      a recorded principal\
    \ that matches that of the current request's\n      principal, then the preparatory\
    \ phase proceeds as follows.\n      *  If there is a confirmed client record with\
    \ a matching client ID\n         string and a non-matching principal, the server\
    \ checks the\n         current state of the associated lease.  If there is no\n\
    \         associated state for the lease, or the lease has expired, the\n    \
    \     server proceeds to the main part of SETCLIENTID.\n      *  Otherwise, the\
    \ server is being asked to do a SETCLIENTID for a\n         client by a non-matching\
    \ principal while there is active state.\n         In this case, the server rejects\
    \ the SETCLIENTID request\n         returning an NFS4ERR_CLID_INUSE error, since\
    \ use of a single\n         client with multiple principals is not allowed.  Note\
    \ that even\n         though the previously used clientaddr4 is returned with\
    \ this\n         error, the use of the same id string with multiple clientaddr4s\n\
    \         is not prohibited, while its use with multiple principals is\n     \
    \    prohibited.\n"
- title: 8.4.5.2.  IMPLEMENTATION (Main Phase)
  contents:
  - "8.4.5.2.  IMPLEMENTATION (Main Phase)\n   If the SETCLIENTID has not been dealt\
    \ with by DRC processing, and has\n   not been rejected with an NFS4ERR_CLID_INUSE\
    \ error, then the main\n   part of SETCLIENTID processing proceeds, as described\
    \ below.\n   o  The server checks if it has recorded a confirmed record for {\
    \ v,\n      x, c, l, s }, where l may or may not equal k.  If so, and since\n\
    \      the id verifier v of the request matches that which is confirmed\n    \
    \  and recorded, the server treats this as a probable callback\n      information\
    \ update and records an unconfirmed { v, x, c, k, t }\n      and leaves the confirmed\
    \ { v, x, c, l, s } in place, such that t\n      != s.  It does not matter if\
    \ k equals l or not.  Any pre-existing\n      unconfirmed { v, x, c, *, * } is\
    \ removed.\n      The server returns { c, t }.  It is indeed returning the old\n\
    \      clientid4 value c, because the client apparently only wants to\n      update\
    \ callback value k to value l.  It's possible this request is\n      one from\
    \ the Byzantine router that has stale callback information,\n      but this is\
    \ not a problem.  The callback information update is\n      only confirmed if\
    \ followed up by a SETCLIENTID_CONFIRM { c, t }.\n      The server awaits confirmation\
    \ of k via SETCLIENTID_CONFIRM { c, t\n      }.\n      The server does NOT remove\
    \ client (lock/share/delegation) state\n      for x.\n   o  The server has previously\
    \ recorded a confirmed { u, x, c, l, s }\n      record such that v != u, l may\
    \ or may not equal k, and has not\n      recorded any unconfirmed { *, x, *, *,\
    \ * } record for x.  The\n      server records an unconfirmed { v, x, d, k, t\
    \ } (d != c, t != s).\n      The server returns { d, t }.\n      The server awaits\
    \ confirmation of { d, k } via SETCLIENTID_CONFIRM\n      { d, t }.\n      The\
    \ server does NOT remove client (lock/share/delegation) state\n      for x.\n\
    \   o  The server has previously recorded a confirmed { u, x, c, l, s }\n    \
    \  record such that v != u, l may or may not equal k, and recorded an\n      unconfirmed\
    \ { w, x, d, m, t } record such that c != d, t != s, m\n      may or may not equal\
    \ k, m may or may not equal l, and k may or may\n      not equal l.  Whether w\
    \ == v or w != v makes no difference.  The\n      server simply removes the unconfirmed\
    \ { w, x, d, m, t } record and\n      replaces it with an unconfirmed { v, x,\
    \ e, k, r } record, such\n      that e != d, e != c, r != t, r != s.\n      The\
    \ server returns { e, r }.\n      The server awaits confirmation of { e, k } via\
    \ SETCLIENTID_CONFIRM\n      { e, r }.\n      The server does NOT remove client\
    \ (lock/share/delegation) state\n      for x.\n   o  The server has no confirmed\
    \ { *, x, *, *, * } for x.  It may or\n      may not have recorded an unconfirmed\
    \ { u, x, c, l, s }, where l\n      may or may not equal k, and u may or may not\
    \ equal v.  Any\n      unconfirmed record { u, x, c, l, * }, regardless whether\
    \ u == v or\n      l == k, is replaced with an unconfirmed record { v, x, d, k,\
    \ t }\n      where d != c, t != s.\n      The server returns { d, t }.\n     \
    \ The server awaits confirmation of { d, k } via SETCLIENTID_CONFIRM\n      {\
    \ d, t }.  The server does NOT remove client (lock/share/\n      delegation) state\
    \ for x.\n   The server generates the clientid and setclientid_confirm values\
    \ and\n   must take care to ensure that these values are extremely unlikely to\n\
    \   ever be regenerated.\n"
- title: 8.5.  Security Considerations for Inter-server Information Transfer
  contents:
  - "8.5.  Security Considerations for Inter-server Information Transfer\n   Although\
    \ the means by which the source and destination server\n   communicate is not\
    \ specified by NFSv4.0, the following security-\n   related considerations for\
    \ inter-server communication should be\n   noted.\n   o  Communication between\
    \ source and destination servers needs to be\n      carried out in a secure manner,\
    \ with protection against deliberate\n      modification of data in transit provided\
    \ by using either a private\n      network or a security mechanism that ensures\
    \ integrity.  In many\n      cases, privacy will also be required, requiring a\
    \ strengthened\n      security mechanism if a private network is not used.\n \
    \  o  Effective implementation of the file system migration function\n      requires\
    \ that a trust relationship exist between source and\n      destination servers.\
    \  The details of that trust relationship\n      depend on the specifics of the\
    \ inter-server transfer protocol,\n      which is outside the scope of this specification.\n\
    \   o  The source server may communicate to the destination server\n      security-related\
    \ information in order to allow it to more\n      rigorously validate clients'\
    \ identity.  For example, the\n      destination server might reject a SETCLIENTID\
    \ done with a\n      different principal or with a different IP address than was\
    \ done\n      previously by the client on the source server.  However, the\n \
    \     destination server MUST NOT use this information to allow any\n      operation\
    \ to be performed by the client that would not be allowed\n      otherwise.\n"
- title: 8.6.  Security Considerations Revision
  contents:
  - "8.6.  Security Considerations Revision\n   The penultimate paragraph of Section\
    \ 19 of [RFC7530] should be\n   revised to read as follows:\n      Because the\
    \ operations SETCLIENTID/SETCLIENTID_CONFIRM are\n      responsible for the release\
    \ of client state, it is imperative that\n      the principal used for these operations\
    \ be checked against and\n      match the previous use of these operations.  In\
    \ addition, use of\n      integrity protection is desirable on the SETCLIENTID\
    \ operation, to\n      prevent an attack whereby a change in the boot instance\
    \ id\n      (verifier) forces an undesired loss of client state.  See\n      Section\
    \ 5 for further discussion.\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   The security considerations of [RFC7530] remain\
    \ appropriate with the\n   exception of the modification to the penultimate paragraph\
    \ specified\n   in Section 8.6 of this document and the addition of the material\
    \ in\n   Section 8.5.\n"
- title: 10.  References
  contents:
  - '10.  References

    '
- title: 10.1.  Normative References
  contents:
  - "10.1.  Normative References\n   [RFC2119]  Bradner, S., \"Key words for use in\
    \ RFCs to Indicate\n              Requirement Levels\", BCP 14, RFC 2119,\n  \
    \            DOI 10.17487/RFC2119, March 1997,\n              <http://www.rfc-editor.org/info/rfc2119>.\n\
    \   [RFC7530]  Haynes, T., Ed. and D. Noveck, Ed., \"Network File System\n   \
    \           (NFS) Version 4 Protocol\", RFC 7530, DOI 10.17487/RFC7530,\n    \
    \          March 2015, <http://www.rfc-editor.org/info/rfc7530>.\n"
- title: 10.2.  Informative References
  contents:
  - "10.2.  Informative References\n   [INFO-MIGR]\n              Noveck, D., Ed.,\
    \ Shivam, P., Lever, C., and B. Baker,\n              \"NFSv4 migration: Implementation\
    \ experience and spec\n              issues to resolve\", Work in Progress, draft-ietf-nfsv4-\n\
    \              migration-issues-09, February 2016.\n   [RFC1813]  Callaghan, B.,\
    \ Pawlowski, B., and P. Staubach, \"NFS\n              Version 3 Protocol Specification\"\
    , RFC 1813,\n              DOI 10.17487/RFC1813, June 1995,\n              <http://www.rfc-editor.org/info/rfc1813>.\n\
    \   [RFC5661]  Shepler, S., Ed., Eisler, M., Ed., and D. Noveck, Ed.,\n      \
    \        \"Network File System (NFS) Version 4 Minor Version 1\n             \
    \ Protocol\", RFC 5661, DOI 10.17487/RFC5661, January 2010,\n              <http://www.rfc-editor.org/info/rfc5661>.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   The editor and authors of this document gratefully acknowledge\
    \ the\n   contributions of Trond Myklebust of Primary Data and Robert Thurlow\n\
    \   of Oracle.  We also thank Tom Haynes of Primary Data and Spencer\n   Shepler\
    \ of Microsoft for their guidance and suggestions.\n   Special thanks go to members\
    \ of the Oracle Solaris NFS team,\n   especially Rick Mesta and James Wahlig,\
    \ for their work implementing\n   an NFSv4.0 migration prototype and identifying\
    \ many of the issues\n   addressed here.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   David Noveck (editor)\n   Hewlett Packard Enterprise\n\
    \   165 Dascomb Road\n   Andover, MA  01810\n   United States of America\n   Phone:\
    \ +1 978 474 2011\n   Email: davenoveck@gmail.com\n   Piyush Shivam\n   Oracle\
    \ Corporation\n   5300 Riata Park Ct.\n   Austin, TX  78727\n   United States\
    \ of America\n   Phone: +1 512 401 1019\n   Email: piyush.shivam@oracle.com\n\
    \   Charles Lever\n   Oracle Corporation\n   1015 Granger Avenue\n   Ann Arbor,\
    \ MI  48104\n   United States of America\n   Phone: +1 734 274 2396\n   Email:\
    \ chuck.lever@oracle.com\n   Bill Baker\n   Oracle Corporation\n   5300 Riata\
    \ Park Ct.\n   Austin, TX  78727\n   United States of America\n   Phone: +1 512\
    \ 401 1081\n   Email: bill.baker@oracle.com\n"
