- title: __initial_text__
  contents:
  - ''
- title: Independent Submission                                     E. Rosen, Ed.
  contents:
  - "Independent Submission                                     E. Rosen, Ed.\n  \
    \     Cisco Systems' Solution for Multicast in BGP/MPLS IP VPNs\n"
- title: Abstract
  contents:
  - "Abstract\n   This document describes the MVPN (Multicast in BGP/MPLS IP VPNs)\n\
    \   solution designed and deployed by Cisco Systems.  The procedures\n   specified\
    \ in this document are largely a subset of the generalized\n   MVPN framework\
    \ recently standardized by the IETF.  However, as the\n   deployment of the procedures\
    \ specified herein predates the\n   publication of IETF standards (in some cases\
    \ by over five years), an\n   implementation based on these procedures differs\
    \ in some respects\n   from a fully standards-compliant implementation.  These\
    \ differences\n   are pointed out in the document.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for the historical record.\n   This document defines a Historic\
    \ Document for the Internet community.\n   This is a contribution to the RFC Series,\
    \ independently of any other\n   RFC stream.  The RFC Editor has chosen to publish\
    \ this document at\n   its discretion and makes no statement about its value for\n\
    \   implementation or deployment.  Documents approved for publication by\n   the\
    \ RFC Editor are not a candidate for any level of Internet\n   Standard; see Section\
    \ 2 of RFC 5741.\n   Information about the current status of this document, any\
    \ errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc6037.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2010 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Specification of Requirements ..............................3\n  \
    \    1.2. Scaling Multicast State Information in the Network Core ....3\n    \
    \  1.3. Overview ...................................................4\n   2. Multicast\
    \ VRFs ..................................................6\n   3. Multicast Domains\
    \ ...............................................7\n      3.1. Model of Operation\
    \ .........................................7\n   4. Multicast Tunnels ...............................................7\n\
    \      4.1. Ingress PEs ................................................8\n  \
    \    4.2. Egress PEs .................................................8\n    \
    \  4.3. Tunnel Destination Address(es) .............................8\n      4.4.\
    \ Auto-Discovery .............................................8\n           4.4.1.\
    \ MDT-SAFI ...........................................10\n      4.5. Which PIM\
    \ Variant to Use ..................................10\n      4.6. Inter-AS MDT\
    \ Construction .................................11\n           4.6.1. The PIM\
    \ MVPN Join Attribute ........................11\n                  4.6.1.1. Definition\
    \ ................................11\n                  4.6.1.2. Usage .....................................12\n\
    \      4.7. Encapsulation in GRE ......................................13\n  \
    \    4.8. MTU .......................................................14\n    \
    \  4.9. TTL .......................................................14\n      4.10.\
    \ Differentiated Services ..................................14\n      4.11. Avoiding\
    \ Conflict with Internet Multicast ................14\n   5. The PIM C-Instance\
    \ and the MT ..................................15\n      5.1. PIM C-Instance Control\
    \ Packets ............................15\n      5.2. PIM C-Instance RPF Determination\
    \ ..........................15\n           5.2.1. Connector Attribute ................................16\n\
    \   6. Data MDT: Optimizing Flooding ..................................17\n  \
    \    6.1. Limitation of Multicast Domain ............................17\n    \
    \  6.2. Signaling Data MDTs .......................................17\n      6.3.\
    \ Use of SSM for Data MDTs ..................................19\n   7. Packet\
    \ Formats and Constants ...................................20\n      7.1. MDT\
    \ TLV ...................................................20\n      7.2. MDT Join\
    \ TLV for IPv4 Streams .............................20\n      7.3. MDT Join TLV\
    \ for IPv6 Streams .............................21\n      7.4. Multiple MDT Join\
    \ TLVs per Datagram .......................22\n      7.5. Constants .................................................22\n\
    \   8. IANA Considerations ............................................23\n  \
    \ 9. Security Considerations ........................................23\n   10.\
    \ Acknowledgments ...............................................23\n   11. References\
    \ ....................................................24\n      11.1. Normative\
    \ References .....................................24\n      11.2. Informative\
    \ References ...................................24\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document describes the MVPN (Multicast in BGP/MPLS\
    \ IP VPNs)\n   solution designed and deployed by Cisco Systems.  This document\
    \ is\n   being made available for the record and as a reference for\n   interoperating\
    \ with deployed implementations.  This document is a\n   technical specification\
    \ and should not be used to infer the current\n   or future plans of Cisco Systems.\n\
    \   The procedures specified in this document are largely a subset of the\n  \
    \ generalized MVPN framework defined in [MVPN].  However, as this\n   document\
    \ specifies an implementation that precedes the\n   standardization of [MVPN]\
    \ by several years, it does differ in a few\n   respects from a fully standards-compliant\
    \ implementation.  These\n   differences are pointed out where they occur.\n \
    \  The base specification for BGP/MPLS IP VPNs [RFC4364] does not\n   provide\
    \ a way for IP multicast data or control traffic to travel from\n   one VPN site\
    \ to another.  This document extends that specification by\n   specifying the\
    \ necessary protocols and procedures for support of IP\n   multicast.\n   This\
    \ specification presupposes that:\n   1. Protocol Independent Multicast (PIM)\
    \ [PIM-SM], running over either\n      IPv4 or IPv6, is the multicast routing\
    \ protocol used within the\n      VPN,\n   2. PIM, running over IPv4, is the multicast\
    \ routing protocol used\n      within the service-provider (SP) network, and\n\
    \   3. the SP network supports native IPv4 multicast forwarding.\n   Familiarity\
    \ with the terminology and procedures of [RFC4364] is\n   presupposed.  Familiarity\
    \ with [PIM-SM] is also presupposed.\n"
- title: 1.1.  Specification of Requirements
  contents:
  - "1.1.  Specification of Requirements\n   The key words \"MUST\", \"MUST NOT\"\
    , \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\"\
    , \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in [RFC2119].\n"
- title: 1.2.  Scaling Multicast State Information in the Network Core
  contents:
  - "1.2.  Scaling Multicast State Information in the Network Core\n   The BGP/MPLS\
    \ IP VPN service of [RFC4364] provides a VPN with\n   \"optimal\" unicast routing\
    \ through the SP backbone, in that a packet\n   follows the \"shortest path\"\
    \ across the backbone, as determined by the\n   backbone's own routing algorithm.\
    \  This optimal routing is provided\n   without requiring the \"P routers\" (routers\
    \ in the provider backbone,\n   other than the \"provider edge\" or \"PE\" routers)\
    \ to maintain any\n   routing information that is specific to a VPN; indeed, the\
    \ P routers\n   do not maintain any per-VPN state at all.\n   Unfortunately, optimal\
    \ multicast routing cannot be provided without\n   requiring the P routers to\
    \ maintain some VPN-specific state\n   information.  Optimal multicast routing\
    \ would require that one or\n   more multicast distribution trees be created in\
    \ the backbone for each\n   multicast group that is in use.  If a particular multicast\
    \ group from\n   within a VPN is using source-based distribution trees, optimal\n\
    \   routing requires that there be one distribution tree for each\n   transmitter\
    \ of that group.  If shared trees are being used, one tree\n   for each group\
    \ is still required.  Each such tree requires state in\n   some set of the P routers,\
    \ with the amount of state being\n   proportional to the number of multicast transmitters.\
    \  The reason\n   there needs to be at least one distribution tree per multicast\
    \ group\n   is that each group may have a different set of receivers; multicast\n\
    \   routing algorithms generally go to great lengths to ensure that a\n   multicast\
    \ packet will not be sent to a node that is not on the path\n   to a receiver.\n\
    \   Given that an SP generally supports many VPNs, where each VPN may\n   have\
    \ many multicast groups, and each multicast group may have many\n   transmitters,\
    \ it is not scalable to have one or more distribution\n   trees for each multicast\
    \ group.  The SP has no control whatsoever\n   over the number of multicast groups\
    \ and transmitters that exist in\n   the VPNs, and it is difficult to place any\
    \ bound on these numbers.\n   In order to have a scalable multicast solution for\
    \ BGP/MPLS IP VPNs,\n   the amount of state maintained by the P routers needs\
    \ to be\n   proportional to something that IS under the control of the SP.  This\n\
    \   specification describes such a solution.  In this solution, the\n   amount\
    \ of state maintained in the P routers is proportional only to\n   the number\
    \ of VPNs that run over the backbone; the amount of state in\n   the P routers\
    \ is NOT sensitive to the number of multicast groups or\n   to the number of multicast\
    \ transmitters within the VPNs.  To achieve\n   this scalability, the optimality\
    \ of the multicast routes is reduced.\n   A PE that is not on the path to any\
    \ receiver of a particular\n   multicast group may still receive multicast packets\
    \ for that group,\n   and if so, will have to discard them.  The SP does, however,\
    \ have\n   control over the tradeoff between optimal routing and scalability.\n"
- title: 1.3.  Overview
  contents:
  - "1.3.  Overview\n   An SP determines whether a particular VPN is multicast-enabled.\
    \  If\n   it is, it corresponds to a \"Multicast Domain\".  A PE that attaches\
    \ to\n   a particular multicast-enabled VPN is said to belong to the\n   corresponding\
    \ Multicast Domain.  For each Multicast Domain, there is\n   a default multicast\
    \ distribution tree (\"MDT\") through the backbone,\n   connecting ALL of the\
    \ PEs that belong to that Multicast Domain.  A\n   given PE may be in as many\
    \ Multicast Domains as there are VPNs\n   attached to that PE.  However, each\
    \ Multicast Domain has its own MDT.\n   The MDTs are created by running PIM in\
    \ the backbone, and in general\n   an MDT also includes P routers on the paths\
    \ between the PE routers.\n   In a departure from the usual multicast tree distribution\
    \ procedures,\n   the Default MDT for a Multicast Domain is constructed automatically\n\
    \   as the PEs in the domain come up.  Construction of the Default MDT\n   does\
    \ not depend on the existence of multicast traffic in the domain;\n   it will\
    \ exist before any such multicast traffic is seen.  Default\n   MDTs correspond\
    \ to the Multidirectional Inclusive P-Multicast Service\n   Interfaces (\"MI-PMSIs\"\
    ) of [MVPN].\n   In BGP/MPLS IP VPNs, each CE (\"Customer Edge\", see [RFC4364])\
    \ router\n   is a unicast routing adjacency of a PE router, but CE routers at\n\
    \   different sites do NOT become unicast routing adjacencies of each\n   other.\
    \  This important characteristic is retained for multicast\n   routing -- a CE\
    \ router becomes a PIM adjacency of a PE router, but CE\n   routers at different\
    \ sites do NOT become PIM adjacencies of each\n   other.  Multicast packets from\
    \ within a VPN are received from a CE\n   router by an ingress PE router.  The\
    \ ingress PE encapsulates the\n   multicast packets and (initially) forwards them\
    \ along the Default MDT\n   to all the PE routers connected to sites of the given\
    \ VPN.  Every PE\n   router attached to a site of the given VPN thus receives\
    \ all\n   multicast packets from within that VPN.  If a particular PE router is\n\
    \   not on the path to any receiver of that multicast group, the PE\n   simply\
    \ discards that packet.\n   If a large amount of traffic is being sent to a particular\
    \ multicast\n   group, but that group does not have receivers at all the VPN sites,\n\
    \   it can be wasteful to forward that group's traffic along the Default\n   MDT.\
    \  Therefore, we also specify a method for establishing individual\n   MDTs for\
    \ specific multicast groups.  We call these \"Data MDTs\".  A\n   Data MDT delivers\
    \ VPN data traffic for a particular multicast group\n   only to those PE routers\
    \ that are on the path to receivers of that\n   multicast group.  Using a Data\
    \ MDT has the benefit of reducing the\n   amount of multicast traffic on the backbone,\
    \ as well as reducing the\n   load on some of the PEs; it has the disadvantage\
    \ of increasing the\n   amount of state that must be maintained by the P routers.\
    \  The SP has\n   complete control over this tradeoff.  Data MDTs correspond to\
    \ the\n   Selective PMSI (\"S-PMSIs\") of [MVPN].\n   This solution requires the\
    \ SP to deploy appropriate protocols and\n   procedures, but is transparent to\
    \ the SP's customers.  An enterprise\n   that uses PIM-based multicasting in its\
    \ network can migrate from a\n   private network to a BGP/MPLS IP VPN service,\
    \ while continuing to use\n   whatever multicast router configurations it was\
    \ previously using; no\n   changes need be made to CE routers or to other routers\
    \ at customer\n   sites.  For instance, any dynamic Rendezvous Point (\"RP\")-discovery\n\
    \   procedures that are already in use may be left in place.\n"
- title: 2.  Multicast VRFs
  contents:
  - "2.  Multicast VRFs\n   The notion of a VPN Routing and Forwarding table (\"VRF\"\
    ), defined in\n   [RFC4364], is extended to include multicast routing entries\
    \ as well\n   as unicast routing entries.\n   Each VRF has its own multicast routing\
    \ table.  When a multicast data\n   or control packet is received from a particular\
    \ CE device, multicast\n   routing is done in the associated VRF.\n   Each PE\
    \ router runs a number of instances of PIM - Sparse Mode\n   (PIM-SM), as many\
    \ as one per VRF.  In each instance of PIM-SM, the PE\n   maintains a PIM adjacency\
    \ with each of the PIM-capable CE routers\n   associated with that VRF.  The multicast\
    \ routing table created by\n   each instance is specific to the corresponding\
    \ VRF.  We will refer to\n   these PIM instances as \"VPN-specific PIM instances\"\
    , or \"PIM\n   C-instances\".\n   Each PE router also runs a \"provider-wide\"\
    \ instance of PIM-SM (a \"PIM\n   P-instance\"), in which it has a PIM adjacency\
    \ with each of its IGP\n   neighbors (i.e., with P routers), but NOT with any\
    \ CE routers, and\n   not with other PE routers (unless they happen to be adjacent\
    \ in the\n   SP's network).  The P routers also run the P-instance of PIM, but\
    \ do\n   NOT run a C-instance.\n   In order to help clarify when we are speaking\
    \ of the PIM P-instance\n   and when we are speaking of a PIM C-instance, we will\
    \ also apply the\n   prefixes \"P-\" and \"C-\" respectively to control messages,\
    \ addresses,\n   etc.  Thus, a P-Join would be a PIM Join that is processed by\
    \ the PIM\n   P-instance, and a C-Join would be a PIM Join that is processed by\
    \ a\n   C-instance.  A P-group address would be a group address in the SP's\n\
    \   address space, and a C-group address would be a group address in a\n   VPN's\
    \ address space.\n"
- title: 3.  Multicast Domains
  contents:
  - '3.  Multicast Domains

    '
- title: 3.1.  Model of Operation
  contents:
  - "3.1.  Model of Operation\n   A Multicast Domain (\"MD\") is essentially a set\
    \ of VRFs associated\n   with interfaces that can send multicast traffic to each\
    \ other.  From\n   the standpoint of a PIM C-instance, a Multicast Domain is equivalent\n\
    \   to a multi-access interface.  The PE routers in a given MD become PIM\n  \
    \ adjacencies of each other in the PIM C-instance.\n   Each multicast VRF is assigned\
    \ to one MD.  Each MD is configured with\n   a distinct, multicast P-group address,\
    \ called the \"Default MDT group\n   address\".  This address is used to build\
    \ the Default MDT for the MD.\n   When a PE router needs to send PIM C-instance\
    \ control traffic to the\n   other PE routers in the MD, it encapsulates the control\
    \ traffic, with\n   its own IPv4 address as the source IP address and the Default\
    \ MDT\n   group address as the destination IP address.  Note that the Default\n\
    \   MDT is part of the PIM P-instance, whereas the PEs that communicate\n   over\
    \ the Default MDT are PIM adjacencies in a C-instance.  Within the\n   C-instance,\
    \ the Default MDT appears to be a multi-access network to\n   which all the PEs\
    \ are attached.  This is discussed in more detail in\n   Section 4.\n   The Default\
    \ MDT does not only carry the PIM control traffic of the\n   MD's PIM C-instance.\
    \  It also, by default, carries the multicast data\n   traffic of the C-instance.\
    \  In some cases, though, multicast data\n   traffic in a particular MD will be\
    \ sent on a Data MDT rather than on\n   the Default MDT.  The use of Data MDTs\
    \ is described in Section 6.\n   Note that, if an MDT (Default or Data) is set\
    \ up using the ASM (\"Any-\n   Source Multicast\") Service Model, the MDT (Default\
    \ or Data) must have\n   a P-group address that is \"globally unique\" (more precisely,\
    \ unique\n   over the set of SP networks carrying the multicast traffic of the\n\
    \   corresponding MD).  If the MDT is set up using the SSM (\"Source-\n   Specific\
    \ Multicast\") model, the P-group address of an MDT only needs\n   to be unique\
    \ relative to the source of the MDT (however, see\n   Section 4.4).  Nevertheless,\
    \ some implementations require the same\n   SSM group address to be assigned to\
    \ all the PEs.  Interoperability\n   with those implementations requires conformance\
    \ to this restriction.\n"
- title: 4.  Multicast Tunnels
  contents:
  - "4.  Multicast Tunnels\n   An MD can be thought of as a set of PE routers connected\
    \ by a\n   multicast tunnel (\"MT\").  From the perspective of a VPN-specific\
    \ PIM\n   instance, an MT is a single multi-access interface.  In the SP\n   network,\
    \ a single MT is realized as a Default MDT combined with zero\n   or more Data\
    \ MDTs.\n"
- title: 4.1.  Ingress PEs
  contents:
  - "4.1.  Ingress PEs\n   An ingress PE is a PE router that is either directly connected\
    \ to the\n   multicast sender in the VPN, or via a CE router.  When the multicast\n\
    \   sender starts transmitting, and if there are receivers (or a PIM RP)\n   behind\
    \ other PE routers in the common MD, the ingress PE becomes the\n   transmitter\
    \ of either the Default MDT group or a Data MDT group in\n   the SP network.\n"
- title: 4.2.  Egress PEs
  contents:
  - "4.2.  Egress PEs\n   A PE router with a VRF configured in an MD becomes a receiver\
    \ of the\n   Default MDT group for that MD.  A PE router may also join a Data\
    \ MDT\n   group if it has a VPN-specific PIM instance in which it is forwarding\n\
    \   to one of its attached sites traffic for a particular C-group, and\n   that\
    \ particular C-group has been associated with that particular Data\n   MDT.  When\
    \ a PE router joins any P-group used for encapsulating VPN\n   multicast traffic,\
    \ the PE router becomes one of the endpoints of the\n   corresponding MT.\n  \
    \ When a packet is received from an MT, the receiving PE derives the MD\n   from\
    \ the destination address, which is a P-group address, of the\n   received packet.\
    \  The packet is then passed to the corresponding\n   multicast VRF and VPN-specific\
    \ PIM instance for further processing.\n"
- title: 4.3.  Tunnel Destination Address(es)
  contents:
  - "4.3.  Tunnel Destination Address(es)\n   An MT is an IP tunnel for which the\
    \ destination address is a P-group\n   address.  However, an MT is not limited\
    \ to using only one P-group\n   address for encapsulation.  Based on the payload\
    \ VPN multicast\n   traffic, it can choose to use the Default MDT group address,\
    \ or one\n   of the Data MDT group addresses (as described in Section 6 of this\n\
    \   document), allowing the MT to reach a different set of PE routers in\n   the\
    \ common MD.\n"
- title: 4.4.  Auto-Discovery
  contents:
  - "4.4.  Auto-Discovery\n   Any of the variants of PIM may be used to set up the\
    \ Default MDT:\n   PIM-SM, Bidirectional PIM [BIDIR], or PIM-Source-Specific Multicast\n\
    \   (PIM-SSM) [SSM].  Except in the case of PIM-SSM, the PEs need only\n   know\
    \ the proper P-group address in order to begin setting up the\n   Default MDTs.\
    \  The PEs will then discover each others' addresses by\n   virtue of receiving\
    \ PIM control traffic, e.g., PIM Hellos, sourced\n   (and encapsulated) by each\
    \ other.\n   However, in the case of PIM-SSM, the necessary MDTs for an MD cannot\n\
    \   be set up until each PE in the MD knows the source address of each of\n  \
    \ the other PEs in that same MD.  This information needs to be auto-\n   discovered.\n\
    \   A new BGP address family, the MDT-Subsequent Address Family\n   Identifier\
    \ (\"MDT-SAFI\"), is defined.  The Network Layer Reachability\n   Information\
    \ (NLRI) for this address family consists of a Route\n   Distinguisher (RD), an\
    \ IPv4 unicast address, and a multicast group\n   address.  A given PE router\
    \ in a given MD constructs an NLRI in this\n   family from:\n   -  Its own IPv4\
    \ address.  If it has several, it uses the one that it\n      will be placing\
    \ in the IP Source Address field of multicast\n      packets that it will be sending\
    \ over the MDT.\n   -  An RD that has been assigned to the MD.\n   -  The P-group\
    \ address, an IPv4 multicast address that is to be used\n      as the IP Destination\
    \ Address field of multicast packets that will\n      be sent over the MDT.\n\
    \   When a PE distributes this NLRI via BGP, it may include a Route\n   Target\
    \ (RT) Extended Communities attribute.  This RT must be an\n   \"Import RT\" [RFC4364]\
    \ of each VRF in the MD.  The ordinary BGP\n   distribution procedures used by\
    \ [RFC4364] will then ensure that each\n   PE learns the MDT-SAFI \"address\"\
    \ of each of the other PEs in the MD,\n   and that the learned MDT-SAFI addresses\
    \ get associated with the right\n   VRFs.\n   If a PE receives an MDT-SAFI NLRI\
    \ that does not have an RT attribute,\n   the P-group address from the NLRI has\
    \ to be used to associate the\n   NLRI with a particular VRF.  In this case, each\
    \ Multicast Domain must\n   be associated with a unique P-address, even if PIM-SSM\
    \ is used.\n   However, finding a unique P-address for a multi-provider multicast\n\
    \   group may be difficult.\n   In order to facilitate the deployment of multi-provider\
    \ Multicast\n   Domains, this specification REQUIRES the use of the MDT-SAFI NLRI\n\
    \   (even if PIM-SSM is not used to set up the Default MDT).  This\n   specification\
    \ also REQUIRES that an implementation be capable of\n   using PIM-SSM to set\
    \ up the Default MDT.\n   In [MVPN], the MDT-SAFI is replaced by the Intra-Autonomous-System\n\
    \   Inclusive-PMSI auto-discovery (\"Intra-AS I-PMSI A-D\") route.  The\n   latter\
    \ is a generalized version of the MDT-SAFI, which allows the\n   \"Default MDTs\"\
    \ and \"Data MDTs\" to be implemented as MPLS P2MP LSPs\n   (\"Point-to-Multipoint\
    \ Label Switched Paths\") or MP2MP LSPs\n   (\"Multipoint-to-Multipoint Label\
    \ Switched Paths\"), as well as by\n   PIM-created multicast distribution trees.\
    \  In the latter case, the\n   Intra-AS A-D routes carry the same information\
    \ that the MDT-SAFI\n   does, though with a different encoding.\n   The Intra-AS\
    \ A-D routes also carry Route Targets, and so may be\n   distributed in the same\
    \ manner as unicast routes, including being\n   distributed inter-AS.  (Despite\
    \ their name, the inter-AS distribution\n   of Intra-AS I-PMSI A-D routes is sometimes\
    \ necessary in [MVPN].)\n   The encoding of the MDT-SAFI is specified in the following\n\
    \   subsection.\n"
- title: 4.4.1.  MDT-SAFI
  contents:
  - "4.4.1.  MDT-SAFI\n   BGP messages in which AFI=1 and SAFI=66 are \"MDT-SAFI\"\
    \ messages.\n   The NLRI format is the 8-byte-RD:IPv4-address followed by the\
    \ MDT\n   group address, i.e., the MP_REACH attribute for this SAFI will\n   contain\
    \ one or more tuples of the following form:\n          +-------------------------------+\n\
    \          |                               |\n          |  RD:IPv4-address (12\
    \ octets)  |\n          |                               |\n          +-------------------------------+\n\
    \          |    Group Address (4 octets)   |\n          +-------------------------------+\n\
    \   The IPv4 address identifies the PE that originated this route, and\n   the\
    \ RD identifies a VRF in that PE.  The group address MUST be an\n   IPv4 multicast\
    \ group address and is used to build the P-tunnels.  All\n   PEs attached to a\
    \ given MVPN MUST specify the same group address,\n   even if the group is an\
    \ SSM group.  MDT-SAFI routes do not carry RTs,\n   and the group address is used\
    \ to associate a received MDT-SAFI route\n   with a VRF.\n"
- title: 4.5.  Which PIM Variant to Use
  contents:
  - "4.5.  Which PIM Variant to Use\n   To minimize the amount of multicast routing\
    \ state maintained by the P\n   routers, the Default MDTs should be realized as\
    \ shared trees, such as\n   PIM bidirectional trees.  However, the operational\
    \ procedures for\n   assigning P-group addresses may be greatly simplified, especially\
    \ in\n   the case of multi-provider MDs, if PIM-SSM is used.\n   Data MDTs are\
    \ best realized as source trees, constructed via PIM-SSM.\n"
- title: 4.6.  Inter-AS MDT Construction
  contents:
  - "4.6.  Inter-AS MDT Construction\n   Standard PIM techniques for the construction\
    \ of source trees\n   presuppose that every router has a route to the source of\
    \ the tree.\n   However, if the source of the tree is in a different AS than a\n\
    \   particular P router, it is possible that the P router will not have a\n  \
    \ route to the source.  For example, the remote AS may be using BGP to\n   distribute\
    \ a route to the source, but a particular P router may be\n   part of a \"BGP-free\
    \ core\", in which the P routers are not aware of\n   BGP-distributed routes.\n\
    \   What is needed in this case is a way for a PE to tell PIM to\n   construct\
    \ the tree through a particular BGP speaker, the \"BGP Next\n   Hop\" for the\
    \ tree source.  This can be accomplished with a PIM\n   extension.\n   If the\
    \ PE has selected the source of the tree from the MDT SAFI\n   address family,\
    \ then it may be desirable to build the tree along the\n   route to the MDT SAFI\
    \ address, rather than along the route to the\n   corresponding IPv4 address.\
    \  This enables the inter-AS portion of the\n   tree to follow a path that is\
    \ specifically chosen for multicast\n   (i.e., it allows the inter-AS multicast\
    \ topology to be\n   \"non-congruent\" to the inter-AS unicast topology).  This\
    \ too requires\n   a PIM extension.\n   The necessary PIM extension is the PIM\
    \ MVPN Join Attribute described\n   in the following subsection.\n"
- title: 4.6.1.  The PIM MVPN Join Attribute
  contents:
  - '4.6.1.  The PIM MVPN Join Attribute

    '
- title: 4.6.1.1.  Definition
  contents:
  - "4.6.1.1.  Definition\n   In [PIM-ATTRIB], the notion of a \"Join Attribute\"\
    \ is defined, and a\n   format for included Join Attributes in PIM Join/Prune\
    \ messages is\n   specified.  We now define a new Join Attribute, which we call\
    \ the\n   \"MVPN Join Attribute\".\n   0                   1                 \
    \  2                   3\n   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4\
    \ 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |F|E|   Type    | Length        |     Proxy IP address\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \                                   |      RD\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-.......\n\
    \   The 6-bit Type field of the MVPN Join Attribute is set to 1.\n   The F-bit\
    \ is set to 0, indicating that the attribute is\n   non-transitive.\n   Rules\
    \ for setting the E-bit are given in [PIM-ATTRIB].\n   Two information fields\
    \ are carried in the MVPN Join Attribute:\n   -  Proxy IP address: The IP address\
    \ of the node towards which the PIM\n      Join/Prune message is to be forwarded.\
    \  This will either be an\n      IPv4 or an IPv6 address, depending on whether\
    \ the PIM Join/Prune\n      message itself is IPv4 or IPv6.\n   -  RD: An eight-byte\
    \ RD.  This immediately follows the proxy IP\n      address.\n   The PIM message\
    \ also carries the address of the upstream PE.\n   In the case of an intra-AS\
    \ MVPN, the proxy and the upstream PE are\n   the same.  In the case of an inter-AS\
    \ MVPN, the proxy will be the AS\n   Border Router (ASBR) that is the exit point\
    \ from the local AS on the\n   path to the upstream PE.\n"
- title: 4.6.1.2.  Usage
  contents:
  - "4.6.1.2.  Usage\n   When a PE router creates a PIM Join/Prune message in order\
    \ to set up\n   an inter-AS Default MDT, it does so as a result of having received\
    \ a\n   particular MDT-SAFI route.  It includes an MVPN Join Attribute whose\n\
    \   fields are set as follows:\n   -  If the upstream PE is in the same AS as\
    \ the local PE, then the\n      Proxy field contains the address of the upstream\
    \ PE.  Otherwise,\n      it contains the address of the BGP Next Hop on the route\
    \ to the\n      upstream PE.\n   -  The RD field contains the RD from the NLRI\
    \ of the MDT-SAFI route.\n   -  The Upstream PE field contains the address of\
    \ the PE that\n      originated the MDT-SAFI route (obtained from the NLRI of\
    \ that\n      route).\n   When a PIM router processes a PIM Join/Prune message\
    \ with an MVPN\n   Join Attribute, it first checks to see if the Proxy field contains\n\
    \   one of its own addresses.\n   If not, the router uses the proxy IP address\
    \ in order to determine\n   the Reverse Path Forwarding (RPF) interface and neighbor.\
    \  The MVPN\n   Join Attribute MUST be passed upstream, unchanged.\n   If the\
    \ proxy address is one of the router's own IP addresses, then\n   the router looks\
    \ in its BGP routing table for an MDT-SAFI route whose\n   NLRI consists of the\
    \ upstream PE address prepended with the RD from\n   the Join Attribute.  If there\
    \ is no match, the PIM message is\n   discarded.  If there is a match, the IP\
    \ address from the BGP Next Hop\n   field of the matching route is used in order\
    \ to determine the RPF\n   interface and neighbor.  When the PIM Join/Prune is\
    \ forwarded\n   upstream, the Proxy field is replaced with the address of the\
    \ BGP\n   Next Hop, and the RD and Upstream PE fields are left unchanged.\n"
- title: 4.7.  Encapsulation in GRE
  contents:
  - "4.7.  Encapsulation in GRE\n   Generic Routing Encapsulation (GRE) [GRE1701]\
    \ is used when sending\n   multicast traffic through an MDT.  The following diagram\
    \ shows the\n   progression of the packet as it enters and leaves the service-\n\
    \   provider network.\n   Packets received        Packets in transit      Packets\
    \ forwarded\n   at ingress PE           in the service-         by egress PEs\n\
    \                           provider network\n                           +---------------+\n\
    \                           |  P-IP Header  |\n                           +---------------+\n\
    \                           |      GRE      |\n   ++=============++       ++=============++\
    \       ++=============++\n   || C-IP Header ||       || C-IP Header ||      \
    \ || C-IP Header ||\n   ++=============++ >>>>> ++=============++ >>>>> ++=============++\n\
    \   || C-Payload   ||       || C-Payload   ||       || C-Payload   ||\n   ++=============++\
    \       ++=============++       ++=============++\n   The IPv4 Protocol Number\
    \ field in the P-IP Header MUST be set to 47.\n   The Protocol Type field of the\
    \ GRE Header MUST be set to 0x0800 if\n   the C-IP header is an IPv4 header; it\
    \ MUST be set to 0x86dd if the\n   C-IP header is an IPv6 header.\n   [GRE2784]\
    \ specifies an optional GRE checksum, and [GRE2890] specifies\n   optional GRE\
    \ Key and Sequence Number fields.\n   The GRE Key field is not needed because\
    \ the P-group address in the\n   delivery IP header already identifies the MD,\
    \ and thus associates the\n   VRF context, for the payload packet to be further\
    \ processed.\n   The GRE Sequence Number field is also not needed because the\n\
    \   transport layer services for the original application will be\n   provided\
    \ by the C-IP Header.\n   The use of the GRE Checksum field MUST follow [GRE2784].\n\
    \   To facilitate high-speed implementation, this document recommends\n   that\
    \ the ingress PE routers encapsulate VPN packets without setting\n   the Checksum,\
    \ Key, or Sequence Number field.\n"
- title: 4.8.  MTU
  contents:
  - "4.8.  MTU\n   Because multicast group addresses are used as tunnel destination\n\
    \   addresses, existing Path MTU discovery mechanisms cannot be used.\n   This\
    \ requires that:\n   1. The ingress PE router (one that does the encapsulation)\
    \ MUST NOT\n      set the DF (\"Don't Fragment\") bit in the outer header, and\n\
    \   2. If the \"DF\" bit is cleared in the IP header of the C-Packet,\n      fragment\
    \ the C-Packet before encapsulation if appropriate.  This\n      is very important\
    \ in practice due to the fact that the performance\n      of the reassembly function\
    \ is significantly lower than that of\n      decapsulating and forwarding packets\
    \ on today's router\n      implementations.\n"
- title: 4.9.  TTL
  contents:
  - "4.9.  TTL\n   The ingress PE should not copy the Time to Live (TTL) field from\
    \ the\n   payload IP header received from a CE router to the delivery IP\n   header.\
    \  Setting the TTL of the delivery IP header is determined by\n   the local policy\
    \ of the ingress PE router.\n"
- title: 4.10.  Differentiated Services
  contents:
  - "4.10.  Differentiated Services\n   By default, setting of the DS (\"Differentiated\
    \ Services\") field in\n   the delivery IP header should follow the guidelines\
    \ outlined in\n   [DIFF2983].  An SP may also choose to deploy any of the additional\n\
    \   mechanisms the PE routers support.\n"
- title: 4.11.  Avoiding Conflict with Internet Multicast
  contents:
  - "4.11.  Avoiding Conflict with Internet Multicast\n   If the SP is providing Internet\
    \ multicast, distinct from its VPN\n   multicast services, it must ensure that\
    \ the P-group addresses that\n   correspond to its MDs are distinct from any of\
    \ the group addresses of\n   the Internet multicasts it supports.  This is best\
    \ done by using\n   administratively scoped addresses [ADMIN-ADDR].\n   The C-group\
    \ addresses need not be distinct from either the P-group\n   addresses or the\
    \ Internet multicast addresses.\n"
- title: 5.  The PIM C-Instance and the MT
  contents:
  - "5.  The PIM C-Instance and the MT\n   If a particular VRF is in a particular\
    \ MD, the corresponding MT is\n   treated by that VRF's VPN-specific PIM instances\
    \ as a LAN interface.\n   As a result, the PEs that are adjacent on the MT will\
    \ generate and\n   process PIM control packets, such as Hello, Join/Prune, and\
    \ Assert.\n   Designated Forwarder election occurs just as it would on an actual\n\
    \   LAN interface.\n"
- title: 5.1.  PIM C-Instance Control Packets
  contents:
  - "5.1.  PIM C-Instance Control Packets\n   The PIM protocol packets are sent to\
    \ ALL-PIM-ROUTERS (224.0.0.13 for\n   IPv4 or ff02::d for IPv6) in the context\
    \ of that VRF, but when in\n   transit in the provider network, they are encapsulated\
    \ using the\n   Default MDT group configured for that MD.  This allows VPN-specific\n\
    \   PIM routes to be extended from site to site without appearing in the\n   P\
    \ routers.\n   If a PIM C-Instance control packet is an IPv6 packet, its source\n\
    \   address is the IPv4-mapped IPv6 address corresponding to the IPv4\n   address\
    \ of the PE router sending the packet.\n"
- title: 5.2.  PIM C-Instance RPF Determination
  contents:
  - "5.2.  PIM C-Instance RPF Determination\n   Although the MT is treated as a PIM-enabled\
    \ interface, unicast\n   routing is NOT run over it, and there are no unicast\
    \ routing\n   adjacencies over it.  It is therefore necessary to specify special\n\
    \   procedures for determining when the MT is to be regarded as the \"RPF\n  \
    \ Interface\" for a particular C-address.\n   When a PE needs to determine the\
    \ RPF interface of a particular\n   C-address, it looks up the C-address in the\
    \ VRF.  If the route\n   matching it is not a VPN-IP route learned from MP-BGP\
    \ as described in\n   [RFC4364], or if that route's outgoing interface is one\
    \ of the\n   interfaces associated with the VRF, then ordinary PIM procedures\
    \ for\n   determining the RPF interface apply.\n   However, if the route matching\
    \ the C-address is a VPN-IP route whose\n   outgoing interface is not one of the\
    \ interfaces associated with the\n   VRF, then PIM will consider the outgoing\
    \ interface to be the MT\n   associated with the VPN-specific PIM instance.\n\
    \   Once PIM has determined that the RPF interface for a particular\n   C-address\
    \ is the MT, it is necessary for PIM to determine the RPF\n   neighbor for that\
    \ C-address.  This will be one of the other PEs that\n   is a PIM adjacency over\
    \ the MT.\n   The BGP \"Connector\" Attribute is defined.  Whenever a PE router\n\
    \   distributes a VPN-IP address from a VRF that is part of an MD, it\n   SHOULD\
    \ distribute a Connector Attribute along with it.  The Connector\n   Attribute\
    \ specifies the MDT address family, and its value is the IP\n   address that the\
    \ PE router is using as its source IP address for the\n   multicast packets that\
    \ are encapsulated and sent over the MT.  When a\n   PE has determined that the\
    \ RPF interface for a particular C-address\n   is the MT, it looks up the Connector\
    \ Attribute that was distributed\n   along with the VPN-IP address corresponding\
    \ to that C-address.  The\n   value of this Connector Attribute is considered\
    \ to be the RPF\n   adjacency for the C-address.\n   There are older implementations\
    \ in which the Connector Attribute is\n   not present.  In this case, as long\
    \ as the \"BGP Next Hop\" for the\n   C-address is one of the PEs that is a PIM\
    \ adjacency, then that PE is\n   treated as the RPF adjacency for that C-address.\n\
    \   However, if the MD spans multiple Autonomous Systems, and an\n   \"option\
    \ b\" interconnect ([RFC4364], Section 10) is used, the BGP Next\n   Hop might\
    \ not be a PIM adjacency, and the RPF check will not succeed\n   unless the Connector\
    \ Attribute is used.\n   In [MVPN], the Connector Attribute is replaced by the\
    \ \"VRF Route\n   Import Extended Community\" attribute.  The latter is a generalized\n\
    \   version, but carries the same information as the Connector Attribute\n   does;\
    \ the encoding, however, is different.\n   The Connector Attribute is defined\
    \ in the following subsection.\n"
- title: 5.2.1.  Connector Attribute
  contents:
  - "5.2.1.  Connector Attribute\n   The Connector Attribute is an optional transitive\
    \ attribute.  Its\n   value field is formatted as follows:\n           0     \
    \              1\n           0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5\n          +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \          |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1|\n          +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \          |                               |\n          |  IPv4 Address of PE\
    \           |\n          |                               |\n          +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n"
- title: '6.  Data MDT: Optimizing Flooding'
  contents:
  - '6.  Data MDT: Optimizing Flooding

    '
- title: 6.1.  Limitation of Multicast Domain
  contents:
  - "6.1.  Limitation of Multicast Domain\n   While the procedure specified in the\
    \ previous section requires the P\n   routers to maintain multicast state, the\
    \ amount of state is bounded\n   by the number of supported VPNs.  The P routers\
    \ do NOT run any VPN-\n   specific PIM instances.\n   In particular, the use of\
    \ a single bidirectional tree per VPN scales\n   well as the number of transmitters\
    \ and receivers increases, but not\n   so well as the amount of multicast traffic\
    \ per VPN increases.\n   The multicast routing provided by this scheme is not\
    \ optimal, in that\n   a packet of a particular multicast group may be forwarded\
    \ to PE\n   routers that have no downstream receivers for that group, and which\n\
    \   hence may need to discard the packet.\n   In the simplest configuration model,\
    \ only the Default MDT group is\n   configured for each MD.  The result of the\
    \ configuration is that all\n   VPN multicast traffic, whether control or data,\
    \ will be encapsulated\n   and forwarded to all PE routers that are part of the\
    \ MD.  While this\n   limits the number of multicast routing states the provider\
    \ network\n   has to maintain, it also requires PE routers to discard multicast\n\
    \   C-packets if there are no receivers for those packets in the\n   corresponding\
    \ sites.  In some cases, especially when the content\n   involves high bandwidth\
    \ but only a limited set of receivers, it is\n   desirable that certain C-packets\
    \ only travel to PE routers that do\n   have receivers in the VPN to save bandwidth\
    \ in the network and reduce\n   load on the PE routers.\n"
- title: 6.2.  Signaling Data MDTs
  contents:
  - "6.2.  Signaling Data MDTs\n   A simple protocol is proposed to signal additional\
    \ P-group addresses\n   to encapsulate VPN traffic.  These P-group addresses are\
    \ called Data\n   MDT groups.  The ingress PE router advertises a different P-group\n\
    \   address (as opposed to always using the Default MDT group) to\n   encapsulate\
    \ VPN multicast traffic.  Only the PE routers on the path\n   to eventual receivers\
    \ join the P-group, and therefore form an optimal\n   multicast distribution tree\
    \ in the service-provider network for the\n   VPN multicast traffic.  These multicast\
    \ distribution trees are called\n   Data MDTs because they do not carry PIM control\
    \ packets exchanged by\n   PE routers.\n   The following text documents the procedures\
    \ of the initiation and\n   teardown of the Data MDTs.  The definition of the\
    \ constants and\n   timers can be found in Section 7.\n   -  The PE router connected\
    \ to the source of the content initially\n      uses the Default MDT group when\
    \ forwarding the content to the MD.\n   -  When one or more pre-configured conditions\
    \ are met, it starts to\n      periodically announce the MDT Join TLV at the interval\
    \ of\n      [MDT_INTERVAL].  The MDT Join TLV is forwarded to all the PE\n   \
    \   routers in the MD.\n      A commonly used condition is the bandwidth.  When\
    \ the VPN traffic\n      exceeds a certain threshold, it is more desirable to\
    \ deliver the\n      flow to the PE routers connected to receivers in order to\
    \ optimize\n      the performance of PE routers and the resources of the provider\n\
    \      network.  However, other conditions can also be devised, and they\n   \
    \   are purely implementation specific.\n   -  The MDT Join TLV is encapsulated\
    \ in UDP.\n      UDP over IPv4 is used if the multicast stream being assigned\
    \ to a\n      Data MDT is an IPv4 stream.  In this case, the UDP datagram is\n\
    \      addressed to ALL-PIM-ROUTERS (224.0.0.13).\n      UDP over IPv6 is used\
    \ if the multicast stream being assigned to a\n      Data MDT is an IPv6 stream.\
    \  In this case, the UDP datagram is\n      addressed to ALL-PIM-ROUTERS (ff02::d).\n\
    \      The destination UDP port is 3232.\n      The UDP datagram is sent on the\
    \ Default MDT.  This allows all PE\n      routers to receive the information.\
    \  Any MDT Join that is not\n      received over a Default MDT MUST be dropped.\n\
    \   -  Upon receiving an MDT Join TLV, PE routers connected to receivers\n   \
    \   will join the Data MDT group announced by the MDT Join TLV in the\n      global\
    \ table.  When the Data MDT group is in PIM-SM or\n      bidirectional PIM mode,\
    \ the PE routers build a shared tree toward\n      the RP.  When the Data MDT\
    \ group is set up using PIM-SSM, the PE\n      routers build a source tree toward\
    \ the PE router that is\n      advertising the MDT Join TLV.  The IP address of\
    \ that PE router is\n      learned from the IP Source Address field of the UDP\
    \ packet that\n      contains the MDT Join TLV.\n      PE routers that are not\
    \ connected to receivers may wish to cache\n      the states in order to reduce\
    \ the delay when a receiver comes up\n      in the future.\n   -  After [MDT_DATA_DELAY],\
    \ the PE router connected to the source\n      starts encapsulating traffic using\
    \ the Data MDT group.\n   -  When the pre-configured conditions are no longer\
    \ met, e.g., the\n      traffic stops, the PE router connected to the source stops\n\
    \      announcing the MDT Join TLV.\n   -  If the MDT Join TLV is not received\
    \ for an interval longer than\n      [MDT_DATA_TIMEOUT], PE routers connected\
    \ to the receivers just\n      leave the Data MDT group in the global instance.\n"
- title: 6.3.  Use of SSM for Data MDTs
  contents:
  - "6.3.  Use of SSM for Data MDTs\n   The use of Data MDTs requires that a set of\
    \ multicast P-addresses be\n   pre-allocated and dedicated for use as the destination\
    \ addresses for\n   the Data MDTs.\n   If SSM is used to set up the Data MDTs,\
    \ then each MD needs to be\n   assigned a set of these multicast P-addresses.\
    \  Each VRF in the MD\n   needs to be configured with this same set of multicast\
    \ P-addresses.\n   If there are n addresses in this set, then each PE in the MD\
    \ can be\n   the source of n Data MDTs in that MD.\n   If SSM is not used for\
    \ setting up Data MDTs, then each VRF needs to\n   be configured with a unique\
    \ set of multicast P-addresses; two VRFs in\n   the same MD cannot be configured\
    \ with the same set of addresses.\n   This requires the pre-allocation of many\
    \ more multicast P-addresses,\n   and the need to configure a different set for\
    \ each VRF greatly\n   complicates the operations and management.  Therefore,\
    \ the use of SSM\n   for Data MDTs is very strongly recommended.\n"
- title: 7.  Packet Formats and Constants
  contents:
  - '7.  Packet Formats and Constants

    '
- title: 7.1.  MDT TLV
  contents:
  - "7.1.  MDT TLV\n   The MDT TLV has the following format.\n       0           \
    \        1                   2                   3\n       0 1 2 3 4 5 6 7 8 9\
    \ 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      |     Type      |            Length           |     Value       |\n   \
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n      |\
    \                               .                               |\n      |   \
    \                            .                               |\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      Type (8 bits):\n         the type of the MDT TLV.  In this specification,\n\
    \         types 1 and 4 are defined.\n      Length (16 bits):\n         the total\
    \ number of octets in the TLV for this type,\n         including both the Type\
    \ and Length fields.\n      Value (variable length):\n         the content of\
    \ the TLV.\n"
- title: 7.2.  MDT Join TLV for IPv4 Streams
  contents:
  - "7.2.  MDT Join TLV for IPv4 Streams\n   The MDT Join TLV for IPv4 streams has\
    \ the following format.\n       0                   1                   2    \
    \               3\n       0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5\
    \ 6 7 8 9 0 1\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      |     Type      |           Length            |    Reserved     |\n   \
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n      |\
    \                           C-source                            |\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      |                           C-group                             |\n   \
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n      |\
    \                           P-group                             |\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      Type (8 bits):\n         Must be set to 1.\n      Length (16 bits):\n \
    \        Must be set to 16.\n      Reserved (8 bits):\n         for future use.\n\
    \      C-source (32 bits):\n         the IPv4 address of the traffic source in\
    \ the VPN.\n      C-group (32 bits):\n         the IPv4 address of the multicast\
    \ traffic destination address\n         in the VPN.\n      P-group (32 bits):\n\
    \         the IPv4 group address that the PE router is going to use to\n     \
    \    encapsulate the flow (C-source, C-group).\n"
- title: 7.3.  MDT Join TLV for IPv6 Streams
  contents:
  - "7.3.  MDT Join TLV for IPv6 Streams\n   The MDT Join TLV for IPv6 streams has\
    \ the following format.\n       0                   1                   2    \
    \               3\n       0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5\
    \ 6 7 8 9 0 1\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      |     Type      |           Length            |    Reserved     |\n   \
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n      |\
    \                                                               |\n      |   \
    \                        C-source                            |\n      |      \
    \                                                         |\n      |         \
    \                                                      |\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      |                                                               |\n   \
    \   |                           C-group                             |\n      |\
    \                                                               |\n      |   \
    \                                                            |\n      +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \      |                           P-group                             |\n   \
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n      Type\
    \ (8 bits):\n         Must be set to 4.\n      Length (16 bits):\n         Must\
    \ be set to 40.\n      Reserved (8 bits):\n         for future use.\n      C-source\
    \ (128 bits):\n         the IPv6 address of the traffic source in the VPN.\n \
    \     C-group (128 bits):\n         the IPv6 address of the multicast traffic\
    \ destination\n         address in the VPN.\n      P-group (32 bits):\n      \
    \   the IPv4 group address that the PE router is going to use\n         to encapsulate\
    \ the flow (C-source, C-group).\n"
- title: 7.4.  Multiple MDT Join TLVs per Datagram
  contents:
  - "7.4.  Multiple MDT Join TLVs per Datagram\n   A single UDP datagram MAY carry\
    \ multiple MDT Join TLVs, as many as\n   can fit entirely within it.  If there\
    \ are multiple MDT Join TLVs in a\n   UDP datagram, they MUST be of the same type.\
    \  The end of the last MDT\n   Join TLV (as determined by the MDT Join TLV Length\
    \ field) MUST\n   coincide with the end of the UDP datagram, as determined by\
    \ the UDP\n   Length field.  When processing a received UDP datagram that contains\n\
    \   one or more MDT Join TLVs, a router MUST be able to process all the\n   MDT\
    \ Join TLVs that fit into the datagram.\n"
- title: 7.5.  Constants
  contents:
  - "7.5.  Constants\n   [MDT_DATA_DELAY]:\n      the interval before the PE router\
    \ connected to the source will\n      switch to the Data MDT group.  The default\
    \ value is 3 seconds.\n   [MDT_DATA_TIMEOUT]:\n      the interval before which\
    \ the PE router connected to the receivers\n      will time out and leave the\
    \ Data MDT group if no MDT_JOIN_TLV\n      message has been received.  The default\
    \ value is 3 minutes.  This\n      value must be consistent among PE routers.\n\
    \   [MDT_DATA_HOLDDOWN]:\n      the interval before which the PE router will switch\
    \ back to the\n      Default MDT after it started encapsulating packets using\
    \ the Data\n      MDT group.  This is used to avoid oscillation when traffic is\n\
    \      bursty.  The default value is 1 minute.\n   [MDT_INTERVAL]:\n      the\
    \ interval the source PE router uses to periodically send\n      MDT_JOIN_TLV\
    \ messages.  The default value is 60 seconds.\n"
- title: 8.  IANA Considerations
  contents:
  - "8.  IANA Considerations\n   The codepoint for the Connector Attribute is defined\
    \ in IANA's\n   registry of BGP attributes.  The reference has been updated to\
    \ refer\n   to this document.  On the IANA web page, the codepoint is denoted\
    \ as\n   \"deprecated\".  This document does not change that status.  However,\n\
    \   note that there are a large number of deployments using this\n   codepoint,\
    \ and this is likely to be the case for a number of years.\n   The codepoint for\
    \ MDT-SAFI is defined in IANA's registry of BGP SAFI\n   assignments.  The reference\
    \ has been updated to refer to this\n   document.\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   [RFC4364] discusses in general the security considerations\
    \ that\n   pertain to when the RFC 4364 type of VPN is deployed.\n   [PIM-SM]\
    \ discusses the security considerations that pertain to the\n   use of PIM.\n\
    \   The security considerations of [RFC4023] and [RFC4797] apply whenever\n  \
    \ VPN traffic is carried through IP or GRE tunnels.\n"
- title: 10.  Acknowledgments
  contents:
  - "10.  Acknowledgments\n   Major contributions to this work have been made by Dan\
    \ Tappan and\n   Tony Speakman.\n   The authors also wish to thank Arjen Boers,\
    \ Robert Raszuk, Toerless\n   Eckert, and Ted Qian for their help and their ideas.\n"
- title: 11.  References
  contents:
  - '11.  References

    '
- title: 11.1.  Normative References
  contents:
  - "11.1.  Normative References\n   [GRE2784]      Farinacci, D., Li, T., Hanks,\
    \ S., Meyer, D., and P.\n                  Traina, \"Generic Routing Encapsulation\
    \ (GRE)\",\n                  RFC 2784, March 2000.\n   [PIM-SM]       Fenner,\
    \ B., Handley, M., Holbrook, H., and I.\n                  Kouvelas, \"Protocol\
    \ Independent Multicast - Sparse\n                  Mode (PIM-SM): Protocol Specification\
    \ (Revised)\",\n                  RFC 4601, August 2006.\n   [PIM-ATTRIB]   Boers,\
    \ A., Wijnands, I., and E. Rosen, \"The Protocol\n                  Independent\
    \ Multicast (PIM) Join Attribute Format\",\n                  RFC 5384, November\
    \ 2008.\n   [RFC2119]      Bradner, S., \"Key words for use in RFCs to Indicate\n\
    \                  Requirement Levels\", BCP 14, RFC 2119, March 1997.\n   [RFC4364]\
    \      Rosen, E. and Y. Rekhter, \"BGP/MPLS IP Virtual Private\n             \
    \     Networks (VPNs)\", RFC 4364, February 2006.\n"
- title: 11.2.  Informative References
  contents:
  - "11.2.  Informative References\n   [ADMIN-ADDR]   Meyer, D., \"Administratively\
    \ Scoped IP Multicast\",\n                  BCP 23, RFC 2365, July 1998.\n   [BIDIR]\
    \        Handley, M., Kouvelas, I., Speakman, T., and L.\n                  Vicisano,\
    \ \"Bidirectional Protocol Independent\n                  Multicast (BIDIR-PIM)\"\
    , RFC 5015, October 2007.\n   [DIFF2983]     Black, D., \"Differentiated Services\
    \ and Tunnels\",\n                  RFC 2983, October 2000.\n   [GRE1701]    \
    \  Hanks, S., Li, T., Farinacci, D., and P. Traina,\n                  \"Generic\
    \ Routing Encapsulation (GRE)\", RFC 1701,\n                  October 1994.\n\
    \   [GRE2890]      Dommety, G., \"Key and Sequence Number Extensions to\n    \
    \              GRE\", RFC 2890, September 2000.\n   [MVPN]         Rosen, E.,\
    \ Ed., and R. Aggarwal, Ed., \"Multicast in\n                  MPLS/BGP IP VPNs\"\
    , Work in Progress, January 2010.\n   [SSM]          Holbrook, H. and B. Cain,\
    \ \"Source-Specific Multicast\n                  for IP\", RFC 4607, August 2006.\n\
    \   [RFC4023]      Worster, T., Rekhter, Y., and E. Rosen, Ed.,\n            \
    \      \"Encapsulating MPLS in IP or Generic Routing\n                  Encapsulation\
    \ (GRE)\", RFC 4023, March 2005.\n   [RFC4797]      Rekhter, Y., Bonica, R., and\
    \ E. Rosen, \"Use of\n                  Provider Edge to Provider Edge (PE-PE)\
    \ Generic Routing\n                  Encapsulation (GRE) or IP in BGP/MPLS IP\
    \ Virtual\n                  Private Networks\", RFC 4797, January 2007.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Eric C. Rosen (editor)\n   Cisco Systems, Inc.\n   1414\
    \ Massachusetts Avenue\n   Boxborough, MA  01719\n   EMail: erosen@cisco.com\n\
    \   Yiqun Cai (editor)\n   Cisco Systems, Inc.\n   170 Tasman Drive\n   San Jose,\
    \ CA  95134\n   EMail: ycai@cisco.com\n   IJsbrand Wijnands\n   Cisco Systems,\
    \ Inc.\n   170 Tasman Drive\n   San Jose, CA  95134\n   EMail: ice@cisco.com\n"
