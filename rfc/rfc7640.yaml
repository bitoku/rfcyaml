- title: __initial_text__
  contents:
  - '                    Traffic Management Benchmarking

    '
- title: Abstract
  contents:
  - "Abstract\n   This framework describes a practical methodology for benchmarking\
    \ the\n   traffic management capabilities of networking devices (i.e.,\n   policing,\
    \ shaping, etc.).  The goals are to provide a repeatable test\n   method that\
    \ objectively compares performance of the device's traffic\n   management capabilities\
    \ and to specify the means to benchmark traffic\n   management with representative\
    \ application traffic.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7640.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2015 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Traffic Management Overview ................................3\n  \
    \    1.2. Lab Configuration and Testing Overview .....................5\n   2.\
    \ Conventions Used in This Document ...............................6\n   3. Scope\
    \ and Goals .................................................7\n   4. Traffic\
    \ Benchmarking Metrics ...................................10\n      4.1. Metrics\
    \ for Stateless Traffic Tests .......................10\n      4.2. Metrics for\
    \ Stateful Traffic Tests ........................12\n   5. Tester Capabilities\
    \ ............................................13\n      5.1. Stateless Test Traffic\
    \ Generation .........................13\n           5.1.1. Burst Hunt with Stateless\
    \ Traffic ..................14\n      5.2. Stateful Test Pattern Generation ..........................14\n\
    \           5.2.1. TCP Test Pattern Definitions .......................15\n  \
    \ 6. Traffic Benchmarking Methodology ...............................17\n    \
    \  6.1. Policing Tests ............................................17\n      \
    \     6.1.1. Policer Individual Tests ...........................18\n        \
    \   6.1.2. Policer Capacity Tests .............................19\n          \
    \        6.1.2.1. Maximum Policers on Single Physical Port ..20\n            \
    \      6.1.2.2. Single Policer on All Physical Ports ......22\n              \
    \    6.1.2.3. Maximum Policers on All Physical Ports ....22\n      6.2. Queue/Scheduler\
    \ Tests .....................................23\n           6.2.1. Queue/Scheduler\
    \ Individual Tests ...................23\n                  6.2.1.1. Testing Queue/Scheduler\
    \ with\n                           Stateless Traffic .........................23\n\
    \                  6.2.1.2. Testing Queue/Scheduler with\n                   \
    \        Stateful Traffic ..........................25\n           6.2.2. Queue/Scheduler\
    \ Capacity Tests .....................28\n                  6.2.2.1. Multiple\
    \ Queues, Single Port Active .......28\n                           6.2.2.1.1.\
    \ Strict Priority on\n                                      Egress Port ....................28\n\
    \                           6.2.2.1.2. Strict Priority + WFQ on\n            \
    \                          Egress Port ....................29\n              \
    \    6.2.2.2. Single Queue per Port, All Ports Active ...30\n                \
    \  6.2.2.3. Multiple Queues per Port, All\n                           Ports Active\
    \ ..............................31\n      6.3. Shaper Tests ..............................................32\n\
    \           6.3.1. Shaper Individual Tests ............................32\n  \
    \                6.3.1.1. Testing Shaper with Stateless Traffic .....33\n    \
    \              6.3.1.2. Testing Shaper with Stateful Traffic ......34\n      \
    \     6.3.2. Shaper Capacity Tests ..............................36\n        \
    \          6.3.2.1. Single Queue Shaped, All Physical\n                      \
    \     Ports Active ..............................37\n                  6.3.2.2.\
    \ All Queues Shaped, Single Port Active .....37\n                  6.3.2.3. All\
    \ Queues Shaped, All Ports Active .......39\n      6.4. Concurrent Capacity Load\
    \ Tests ............................40\n   7. Security Considerations ........................................40\n\
    \   8. References .....................................................41\n  \
    \    8.1. Normative References ......................................41\n    \
    \  8.2. Informative References ....................................42\n   Appendix\
    \ A. Open Source Tools for Traffic Management Testing ......44\n   Appendix B.\
    \ Stateful TCP Test Patterns ............................45\n   Acknowledgments\
    \ ...................................................51\n   Authors' Addresses\
    \ ................................................51\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Traffic management (i.e., policing, shaping, etc.) is an\
    \ increasingly\n   important component when implementing network Quality of Service\n\
    \   (QoS).\n   There is currently no framework to benchmark these features, although\n\
    \   some standards address specific areas as described in Section 1.1.\n   This\
    \ document provides a framework to conduct repeatable traffic\n   management benchmarks\
    \ for devices and systems in a lab environment.\n   Specifically, this framework\
    \ defines the methods to characterize the\n   capacity of the following traffic\
    \ management features in network\n   devices: classification, policing, queuing/scheduling,\
    \ and traffic\n   shaping.\n   This benchmarking framework can also be used as\
    \ a test procedure to\n   assist in the tuning of traffic management parameters\
    \ before service\n   activation.  In addition to Layer 2/3 (Ethernet/IP) benchmarking,\n\
    \   Layer 4 (TCP) test patterns are proposed by this document in order to\n  \
    \ more realistically benchmark end-user traffic.\n"
- title: 1.1.  Traffic Management Overview
  contents:
  - "1.1.  Traffic Management Overview\n   In general, a device with traffic management\
    \ capabilities performs\n   the following functions:\n   -  Traffic classification:\
    \ identifies traffic according to various\n      configuration rules (for example,\
    \ IEEE 802.1Q Virtual LAN (VLAN),\n      Differentiated Services Code Point (DSCP))\
    \ and marks this traffic\n      internally to the network device.  Multiple external\
    \ priorities\n      (DSCP, 802.1p, etc.) can map to the same priority in the device.\n\
    \   -  Traffic policing: limits the rate of traffic that enters a network\n  \
    \    device according to the traffic classification.  If the traffic\n      exceeds\
    \ the provisioned limits, the traffic is either dropped or\n      remarked and\
    \ forwarded onto the next network device.\n   -  Traffic scheduling: provides\
    \ traffic classification within the\n      network device by directing packets\
    \ to various types of queues and\n      applies a dispatching algorithm to assign\
    \ the forwarding sequence\n      of packets.\n   -  Traffic shaping: controls\
    \ traffic by actively buffering and\n      smoothing the output rate in an attempt\
    \ to adapt bursty traffic to\n      the configured limits.\n   -  Active Queue\
    \ Management (AQM): involves monitoring the status of\n      internal queues and\
    \ proactively dropping (or remarking) packets,\n      which causes hosts using\
    \ congestion-aware protocols to \"back off\"\n      and in turn alleviate queue\
    \ congestion [RFC7567].  On the other\n      hand, classic traffic management\
    \ techniques reactively drop (or\n      remark) packets based on queue-full conditions.\
    \  The benchmarking\n      scenarios for AQM are different and are outside the\
    \ scope of this\n      testing framework.\n   Even though AQM is outside the scope\
    \ of this framework, it should be\n   noted that the TCP metrics and TCP test\
    \ patterns (defined in\n   Sections 4.2 and 5.2, respectively) could be useful\
    \ to test new AQM\n   algorithms (targeted to alleviate \"bufferbloat\").  Examples\
    \ of these\n   algorithms include Controlled Delay [CoDel] and Proportional Integral\n\
    \   controller Enhanced [PIE].\n   The following diagram is a generic model of\
    \ the traffic management\n   capabilities within a network device.  It is not\
    \ intended to\n   represent all variations of manufacturer traffic management\n\
    \   capabilities, but it provides context for this test framework.\n    |----------|\
    \   |----------------|   |--------------|   |----------|\n    |          |   |\
    \                |   |              |   |          |\n    |Interface |   |Ingress\
    \ Actions |   |Egress Actions|   |Interface |\n    |Ingress   |   |(classification,|\
    \   |(scheduling,  |   |Egress    |\n    |Queues    |   | marking,       |   |\
    \ shaping,     |   |Queues    |\n    |          |-->| policing, or   |-->| active\
    \ queue |-->|          |\n    |          |   | shaping)       |   | management,\
    \  |   |          |\n    |          |   |                |   | remarking)   |\
    \   |          |\n    |----------|   |----------------|   |--------------|   |----------|\n\
    \   Figure 1: Generic Traffic Management Capabilities of a Network Device\n  \
    \ Ingress actions such as classification are defined in [RFC4689] and\n   include\
    \ IP addresses, port numbers, and DSCP.  In terms of marking,\n   [RFC2697] and\
    \ [RFC2698] define a Single Rate Three Color Marker and a\n   Two Rate Three Color\
    \ Marker, respectively.\n   The Metro Ethernet Forum (MEF) specifies policing\
    \ and shaping in\n   terms of ingress and egress subscriber/provider conditioning\n\
    \   functions as described in MEF 12.2 [MEF-12.2], as well as ingress and\n  \
    \ bandwidth profile attributes as described in MEF 10.3 [MEF-10.3] and\n   MEF\
    \ 26.1 [MEF-26.1].\n"
- title: 1.2.  Lab Configuration and Testing Overview
  contents:
  - "1.2.  Lab Configuration and Testing Overview\n   The following diagram shows\
    \ the lab setup for the traffic management\n   tests:\n     +--------------+ \
    \    +-------+     +----------+    +-----------+\n     | Transmitting |     |\
    \       |     |          |    | Receiving |\n     | Test Host    |     |     \
    \  |     |          |    | Test Host |\n     |              |-----| Device|---->|\
    \ Network  |--->|           |\n     |              |     | Under |     | Delay\
    \    |    |           |\n     |              |     | Test  |     | Emulator |\
    \    |           |\n     |              |<----|       |<----|          |<---|\
    \           |\n     |              |     |       |     |          |    |     \
    \      |\n     +--------------+     +-------+     +----------+    +-----------+\n\
    \             Figure 2: Lab Setup for Traffic Management Tests\n   As shown in\
    \ the test diagram, the framework supports unidirectional\n   and bidirectional\
    \ traffic management tests (where the transmitting\n   and receiving roles would\
    \ be reversed on the return path).\n   This testing framework describes the tests\
    \ and metrics for each of\n   the following traffic management functions:\n  \
    \ -  Classification\n   -  Policing\n   -  Queuing/scheduling\n   -  Shaping\n\
    \   The tests are divided into individual and rated capacity tests.  The\n   individual\
    \ tests are intended to benchmark the traffic management\n   functions according\
    \ to the metrics defined in Section 4.  The\n   capacity tests verify traffic\
    \ management functions under the load of\n   many simultaneous individual tests\
    \ and their flows.\n   This involves concurrent testing of multiple interfaces\
    \ with the\n   specific traffic management function enabled, and increasing the\
    \ load\n   to the capacity limit of each interface.\n   For example, a device\
    \ is specified to be capable of shaping on all of\n   its egress ports.  The individual\
    \ test would first be conducted to\n   benchmark the specified shaping function\
    \ against the metrics defined\n   in Section 4.  Then, the capacity test would\
    \ be executed to test the\n   shaping function concurrently on all interfaces\
    \ and with maximum\n   traffic load.\n   The Network Delay Emulator (NDE) is required\
    \ for TCP stateful tests\n   in order to allow TCP to utilize a TCP window of\
    \ significant size in\n   its control loop.\n   Note also that the NDE SHOULD\
    \ be passive in nature (e.g., a fiber\n   spool).  This is recommended to eliminate\
    \ the potential effects that\n   an active delay element (i.e., test impairment\
    \ generator) may have on\n   the test flows.  In the case where a fiber spool\
    \ is not practical due\n   to the desired latency, an active NDE MUST be independently\
    \ verified\n   to be capable of adding the configured delay without loss.  In\
    \ other\n   words, the Device Under Test (DUT) would be removed and the NDE\n\
    \   performance benchmarked independently.\n   Note that the NDE SHOULD be used\
    \ only as emulated delay.  Most NDEs\n   allow for per-flow delay actions, emulating\
    \ QoS prioritization.  For\n   this framework, the NDE's sole purpose is simply\
    \ to add delay to all\n   packets (emulate network latency).  So, to benchmark\
    \ the performance\n   of the NDE, the maximum offered load should be tested against\
    \ the\n   following frame sizes: 128, 256, 512, 768, 1024, 1500, and\n   9600\
    \ bytes.  The delay accuracy at each of these packet sizes can\n   then be used\
    \ to calibrate the range of expected Bandwidth-Delay\n   Product (BDP) for the\
    \ TCP stateful tests.\n"
- title: 2.  Conventions Used in This Document
  contents:
  - "2.  Conventions Used in This Document\n   The key words \"MUST\", \"MUST NOT\"\
    , \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\"\
    , \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in [RFC2119].\n   The following acronyms are used:\n      AQM: Active Queue\
    \ Management\n      BB: Bottleneck Bandwidth\n      BDP: Bandwidth-Delay Product\n\
    \      BSA: Burst Size Achieved\n      CBS: Committed Burst Size\n      CIR: Committed\
    \ Information Rate\n      DUT: Device Under Test\n      EBS: Excess Burst Size\n\
    \      EIR: Excess Information Rate\n      NDE: Network Delay Emulator\n     \
    \ QL: Queue Length\n      QoS: Quality of Service\n      RTT: Round-Trip Time\n\
    \      SBB: Shaper Burst Bytes\n      SBI: Shaper Burst Interval\n      SP: Strict\
    \ Priority\n      SR: Shaper Rate\n      SSB: Send Socket Buffer\n      SUT: System\
    \ Under Test\n      Ti: Transmission Interval\n      TTP: TCP Test Pattern\n \
    \     TTPET: TCP Test Pattern Execution Time\n"
- title: 3.  Scope and Goals
  contents:
  - "3.  Scope and Goals\n   The scope of this work is to develop a framework for\
    \ benchmarking and\n   testing the traffic management capabilities of network\
    \ devices in the\n   lab environment.  These network devices may include but are\
    \ not\n   limited to:\n   -  Switches (including Layer 2/3 devices)\n   -  Routers\n\
    \   -  Firewalls\n   -  General Layer 4-7 appliances (Proxies, WAN Accelerators,\
    \ etc.)\n   Essentially, any network device that performs traffic management as\n\
    \   defined in Section 1.1 can be benchmarked or tested with this\n   framework.\n\
    \   The primary goal is to assess the maximum forwarding performance\n   deemed\
    \ to be within the provisioned traffic limits that a network\n   device can sustain\
    \ without dropping or impairing packets, and without\n   compromising the accuracy\
    \ of multiple instances of traffic management\n   functions.  This is the benchmark\
    \ for comparison between devices.\n   Within this framework, the metrics are defined\
    \ for each traffic\n   management test but do not include pass/fail criteria,\
    \ which are not\n   within the charter of the BMWG.  This framework provides the\
    \ test\n   methods and metrics to conduct repeatable testing, which will provide\n\
    \   the means to compare measured performance between DUTs.\n   As mentioned in\
    \ Section 1.2, these methods describe the individual\n   tests and metrics for\
    \ several management functions.  It is also\n   within scope that this framework\
    \ will benchmark each function in\n   terms of overall rated capacity.  This involves\
    \ concurrent testing of\n   multiple interfaces with the specific traffic management\
    \ function\n   enabled, up to the capacity limit of each interface.\n   It is\
    \ not within the scope of this framework to specify the procedure\n   for testing\
    \ multiple configurations of traffic management functions\n   concurrently.  The\
    \ multitudes of possible combinations are almost\n   unbounded, and the ability\
    \ to identify functional \"break points\"\n   would be almost impossible.\n  \
    \ However, Section 6.4 provides suggestions for some profiles of\n   concurrent\
    \ functions that would be useful to benchmark.  The key\n   requirement for any\
    \ concurrent test function is that tests MUST\n   produce reliable and repeatable\
    \ results.\n   Also, it is not within scope to perform conformance testing.  Tests\n\
    \   defined in this framework benchmark the traffic management functions\n   according\
    \ to the metrics defined in Section 4 and do not address any\n   conformance to\
    \ standards related to traffic management.\n   The current specifications don't\
    \ specify exact behavior or\n   implementation, and the specifications that do\
    \ exist (cited in\n   Section 1.1) allow implementations to vary with regard to\
    \ short-term\n   rate accuracy and other factors.  This is a primary driver for\
    \ this\n   framework: to provide an objective means to compare vendor traffic\n\
    \   management functions.\n   Another goal is to devise methods that utilize flows\
    \ with congestion-\n   aware transport (TCP) as part of the traffic load and still\
    \ produce\n   repeatable results in the isolated test environment.  This framework\n\
    \   will derive stateful test patterns (TCP or application layer) that\n   can\
    \ also be used to further benchmark the performance of applicable\n   traffic\
    \ management techniques such as queuing/scheduling and traffic\n   shaping.  In\
    \ cases where the network device is stateful in nature\n   (i.e., firewall, etc.),\
    \ stateful test pattern traffic is important to\n   test, along with stateless\
    \ UDP traffic in specific test scenarios\n   (i.e., applications using TCP transport\
    \ and UDP VoIP, etc.).\n   As mentioned earlier in this document, repeatability\
    \ of test results\n   is critical, especially considering the nature of stateful\
    \ TCP\n   traffic.  To this end, the stateful tests will use TCP test patterns\n\
    \   to emulate applications.  This framework also provides guidelines for\n  \
    \ application modeling and open source tools to achieve the repeatable\n   stimulus.\
    \  Finally, TCP metrics from [RFC6349] MUST be measured for\n   each stateful\
    \ test and provide the means to compare each repeated\n   test.\n   Even though\
    \ this framework targets the testing of TCP applications\n   (i.e., web, email,\
    \ database, etc.), it could also be applied to the\n   Stream Control Transmission\
    \ Protocol (SCTP) in terms of test\n   patterns.  WebRTC, Signaling System 7 (SS7)\
    \ signaling, and 3GPP are\n   SCTP-based applications that could be modeled with\
    \ this framework to\n   benchmark SCTP's effect on traffic management performance.\n\
    \   Note that at the time of this writing, this framework does not\n   address\
    \ tcpcrypt (encrypted TCP) test patterns, although the metrics\n   defined in\
    \ Section 4.2 can still be used because the metrics are\n   based on TCP retransmission\
    \ and RTT measurements (versus any of the\n   payload).  Thus, if tcpcrypt becomes\
    \ popular, it would be natural for\n   benchmarkers to consider encrypted TCP\
    \ patterns and include them in\n   test cases.\n"
- title: 4.  Traffic Benchmarking Metrics
  contents:
  - "4.  Traffic Benchmarking Metrics\n   The metrics to be measured during the benchmarks\
    \ are divided into two\n   (2) sections: packet-layer metrics used for the stateless\
    \ traffic\n   testing and TCP-layer metrics used for the stateful traffic testing.\n"
- title: 4.1.  Metrics for Stateless Traffic Tests
  contents:
  - "4.1.  Metrics for Stateless Traffic Tests\n   Stateless traffic measurements\
    \ require that a sequence number and\n   timestamp be inserted into the payload\
    \ for lost-packet analysis.\n   Delay analysis may be achieved by insertion of\
    \ timestamps directly\n   into the packets or timestamps stored elsewhere (packet\
    \ captures).\n   This framework does not specify the packet format to carry sequence\n\
    \   number or timing information.\n   However, [RFC4737] and [RFC4689] provide\
    \ recommendations for sequence\n   tracking, along with definitions of in-sequence\
    \ and out-of-order\n   packets.\n   The following metrics MUST be measured during\
    \ the stateless traffic\n   benchmarking components of the tests:\n   -  Burst\
    \ Size Achieved (BSA): For the traffic policing and network\n      queue tests,\
    \ the tester will be configured to send bursts to test\n      either the Committed\
    \ Burst Size (CBS) or Excess Burst Size (EBS)\n      of a policer or the queue/buffer\
    \ size configured in the DUT.  The\n      BSA metric is a measure of the actual\
    \ burst size received at the\n      egress port of the DUT with no lost packets.\
    \  For example, the\n      configured CBS of a DUT is 64 KB, and after the burst\
    \ test, only a\n      63 KB burst can be achieved without packet loss.  Then,\
    \ 63 KB is\n      the BSA.  Also, the average Packet Delay Variation (PDV) (see\n\
    \      below) as experienced by the packets sent at the BSA burst size\n     \
    \ should be recorded.  This metric SHALL be reported in units of\n      bytes,\
    \ KB, or MB.\n   -  Lost Packets (LP): For all traffic management tests, the tester\n\
    \      will transmit the test packets into the DUT ingress port, and the\n   \
    \   number of packets received at the egress port will be measured.\n      The\
    \ difference between packets transmitted into the ingress port\n      and received\
    \ at the egress port is the number of lost packets as\n      measured at the egress\
    \ port.  These packets must have unique\n      identifiers such that only the\
    \ test packets are measured.  For\n      cases where multiple flows are transmitted\
    \ from the ingress port\n      to the egress port (e.g., IP conversations), each\
    \ flow must have\n      sequence numbers within the stream of test packets.\n\
    \   [RFC6703] and [RFC2680] describe the need to establish the time\n   threshold\
    \ to wait before a packet is declared as lost.  This\n   threshold MUST be reported,\
    \ with the results reported as an integer\n   number that cannot be negative.\n\
    \   -  Out-of-Sequence (OOS): In addition to the LP metric, the test\n      packets\
    \ must be monitored for sequence.  [RFC4689] defines the\n      general function\
    \ of sequence tracking, as well as definitions for\n      in-sequence and out-of-order\
    \ packets.  Out-of-order packets will\n      be counted per [RFC4737].  This metric\
    \ SHALL be reported as an\n      integer number that cannot be negative.\n   -\
    \  Packet Delay (PD): The PD metric is the difference between the\n      timestamp\
    \ of the received egress port packets and the packets\n      transmitted into\
    \ the ingress port, as specified in [RFC1242].  The\n      transmitting host and\
    \ receiving host time must be in time sync\n      (achieved by using NTP, GPS,\
    \ etc.).  This metric SHALL be reported\n      as a real number of seconds, where\
    \ a negative measurement usually\n      indicates a time synchronization problem\
    \ between test devices.\n   -  Packet Delay Variation (PDV): The PDV metric is\
    \ the variation\n      between the timestamp of the received egress port packets,\
    \ as\n      specified in [RFC5481].  Note that per [RFC5481], this PDV is the\n\
    \      variation of one-way delay across many packets in the traffic\n      flow.\
    \  Per the measurement formula in [RFC5481], select the high\n      percentile\
    \ of 99%, and units of measure will be a real number of\n      seconds (a negative\
    \ value is not possible for the PDV and would\n      indicate a measurement error).\n\
    \   -  Shaper Rate (SR): The SR represents the average DUT output rate\n     \
    \ (bps) over the test interval.  The SR is only applicable to the\n      traffic-shaping\
    \ tests.\n   -  Shaper Burst Bytes (SBB): A traffic shaper will emit packets in\n\
    \      \"trains\" of different sizes; these frames are emitted \"back-to-\n  \
    \    back\" with respect to the mandatory interframe gap.  This metric\n     \
    \ characterizes the method by which the shaper emits traffic.  Some\n      shapers\
    \ transmit larger bursts per interval, and a burst of\n      one packet would\
    \ apply to the less common case of a shaper sending\n      a constant-bitrate\
    \ stream of single packets.  This metric SHALL be\n      reported in units of\
    \ bytes, KB, or MB.  The SBB metric is only\n      applicable to the traffic-shaping\
    \ tests.\n   -  Shaper Burst Interval (SBI): The SBI is the time between bursts\n\
    \      emitted by the shaper and is measured at the DUT egress port.\n      This\
    \ metric SHALL be reported as a real number of seconds.  The\n      SBI is only\
    \ applicable to the traffic-shaping tests.\n"
- title: 4.2.  Metrics for Stateful Traffic Tests
  contents:
  - "4.2.  Metrics for Stateful Traffic Tests\n   The stateful metrics will be based\
    \ on [RFC6349] TCP metrics and MUST\n   include:\n   -  TCP Test Pattern Execution\
    \ Time (TTPET): [RFC6349] defined the TCP\n      Transfer Time for bulk transfers,\
    \ which is simply the measured\n      time to transfer bytes across single or\
    \ concurrent TCP\n      connections.  The TCP test patterns used in traffic management\n\
    \      tests will include bulk transfer and interactive applications.\n      The\
    \ interactive patterns include instances such as HTTP business\n      applications\
    \ and database applications.  The TTPET will be the\n      measure of the time\
    \ for a single execution of a TCP Test Pattern\n      (TTP).  Average, minimum,\
    \ and maximum times will be measured or\n      calculated and expressed as a real\
    \ number of seconds.\n   An example would be an interactive HTTP TTP session that\
    \ should take\n   5 seconds on a GigE network with 0.5-millisecond latency.  During\
    \ ten\n   (10) executions of this TTP, the TTPET results might be an average of\n\
    \   6.5 seconds, a minimum of 5.0 seconds, and a maximum of 7.9 seconds.\n   -\
    \  TCP Efficiency: After the execution of the TTP, TCP Efficiency\n      represents\
    \ the percentage of bytes that were not retransmitted.\n                     \
    \    Transmitted Bytes - Retransmitted Bytes\n     TCP Efficiency % =  ---------------------------------------\
    \  X 100\n                                  Transmitted Bytes\n   \"Transmitted\
    \ Bytes\" is the total number of TCP bytes to be\n   transmitted, including the\
    \ original bytes and the retransmitted\n   bytes.  To avoid any misinterpretation\
    \ that a reordered packet is a\n   retransmitted packet (as may be the case with\
    \ packet decode\n   interpretation), these retransmitted bytes should be recorded\
    \ from\n   the perspective of the sender's TCP/IP stack.\n   -  Buffer Delay:\
    \ Buffer Delay represents the increase in RTT during a\n      TCP test versus\
    \ the baseline DUT RTT (non-congested, inherent\n      latency).  RTT and the\
    \ technique to measure RTT (average versus\n      baseline) are defined in [RFC6349].\
    \  Referencing [RFC6349], the\n      average RTT is derived from the total of\
    \ all measured RTTs during\n      the actual test sampled at every second divided\
    \ by the test\n      duration in seconds.\n                                  \
    \    Total RTTs during transfer\n     Average RTT during transfer =  ------------------------------\n\
    \                                     Transfer duration in seconds\n         \
    \            Average RTT during transfer - Baseline RTT\n   Buffer Delay % = \
    \ ------------------------------------------  X 100\n                        \
    \         Baseline RTT\n   Note that even though this was not explicitly stated\
    \ in [RFC6349],\n   retransmitted packets should not be used in RTT measurements.\n\
    \   Also, the test results should record the average RTT in milliseconds\n   across\
    \ the entire test duration, as well as the number of samples.\n"
- title: 5.  Tester Capabilities
  contents:
  - "5.  Tester Capabilities\n   The testing capabilities of the traffic management\
    \ test environment\n   are divided into two (2) sections: stateless traffic testing\
    \ and\n   stateful traffic testing.\n"
- title: 5.1.  Stateless Test Traffic Generation
  contents:
  - "5.1.  Stateless Test Traffic Generation\n   The test device MUST be capable of\
    \ generating traffic at up to the\n   link speed of the DUT.  The test device\
    \ must be calibrated to verify\n   that it will not drop any packets.  The test\
    \ device's inherent PD and\n   PDV must also be calibrated and subtracted from\
    \ the PD and PDV\n   metrics.  The test device must support the encapsulation\
    \ to be\n   tested, e.g., IEEE 802.1Q VLAN, IEEE 802.1ad Q-in-Q, Multiprotocol\n\
    \   Label Switching (MPLS).  Also, the test device must allow control of\n   the\
    \ classification techniques defined in [RFC4689] (e.g., IP address,\n   DSCP,\
    \ classification of Type of Service).\n   The open source tool \"iperf\" can be\
    \ used to generate stateless UDP\n   traffic and is discussed in Appendix A. \
    \ Since iperf is a software-\n   based tool, there will be performance limitations\
    \ at higher link\n   speeds (e.g., 1 GigE, 10 GigE).  Careful calibration of any\
    \ test\n   environment using iperf is important.  At higher link speeds, using\n\
    \   hardware-based packet test equipment is recommended.\n"
- title: 5.1.1.  Burst Hunt with Stateless Traffic
  contents:
  - "5.1.1.  Burst Hunt with Stateless Traffic\n   A central theme for the traffic\
    \ management tests is to benchmark the\n   specified burst parameter of a traffic\
    \ management function, since\n   burst parameters listed in Service Level Agreements\
    \ (SLAs) are\n   specified in bytes.  For testing efficiency, including a burst\
    \ hunt\n   feature is recommended, as this feature automates the manual process\n\
    \   of determining the maximum burst size that can be supported by a\n   traffic\
    \ management function.\n   The burst hunt algorithm should start at the target\
    \ burst size\n   (maximum burst size supported by the traffic management function)\
    \ and\n   will send single bursts until it can determine the largest burst that\n\
    \   can pass without loss.  If the target burst size passes, then the\n   test\
    \ is complete.  The \"hunt\" aspect occurs when the target burst\n   size is not\
    \ achieved; the algorithm will drop down to a configured\n   minimum burst size\
    \ and incrementally increase the burst until the\n   maximum burst supported by\
    \ the DUT is discovered.  The recommended\n   granularity of the incremental burst\
    \ size increase is 1 KB.\n   For a policer function, if the burst size passes,\
    \ the burst should be\n   increased by increments of 1 KB to verify that the policer\
    \ is truly\n   configured properly (or enabled at all).\n"
- title: 5.2.  Stateful Test Pattern Generation
  contents:
  - "5.2.  Stateful Test Pattern Generation\n   The TCP test host will have many of\
    \ the same attributes as the TCP\n   test host defined in [RFC6349].  The TCP\
    \ test device may be a\n   standard computer or a dedicated communications test\
    \ instrument.  In\n   both cases, it must be capable of emulating both a client\
    \ and a\n   server.\n   For any test using stateful TCP test traffic, the Network\
    \ Delay\n   Emulator (the NDE function as shown in the lab setup diagram in\n\
    \   Section 1.2) must be used in order to provide a meaningful BDP.  As\n   discussed\
    \ in Section 1.2, the target traffic rate and configured RTT\n   MUST be verified\
    \ independently, using just the NDE for all stateful\n   tests (to ensure that\
    \ the NDE can add delay without inducing any\n   packet loss).\n   The TCP test\
    \ host MUST be capable of generating and receiving\n   stateful TCP test traffic\
    \ at the full link speed of the DUT.  As a\n   general rule of thumb, testing\
    \ TCP throughput at rates greater than\n   500 Mbps may require high-performance\
    \ server hardware or dedicated\n   hardware-based test tools.\n   The TCP test\
    \ host MUST allow the adjustment of both Send and Receive\n   Socket Buffer sizes.\
    \  The Socket Buffers must be large enough to fill\n   the BDP for bulk transfer\
    \ of TCP test application traffic.\n   Measuring RTT and retransmissions per connection\
    \ will generally\n   require a dedicated communications test instrument.  In the\
    \ absence\n   of dedicated hardware-based test tools, these measurements may need\n\
    \   to be conducted with packet capture tools; i.e., conduct TCP\n   throughput\
    \ tests, and analyze RTT and retransmissions in packet\n   captures.\n   The TCP\
    \ implementation used by the test host MUST be specified in the\n   test results\
    \ (e.g., TCP New Reno, TCP options supported).\n   Additionally, the test results\
    \ SHALL provide specific congestion\n   control algorithm details, as per [RFC3148].\n\
    \   While [RFC6349] defined the means to conduct throughput tests of TCP\n   bulk\
    \ transfers, the traffic management framework will extend TCP test\n   execution\
    \ into interactive TCP application traffic.  Examples include\n   email, HTTP,\
    \ and business applications.  This interactive traffic is\n   bidirectional and\
    \ can be chatty, meaning many turns in traffic\n   communication during the course\
    \ of a transaction (versus the\n   relatively unidirectional flow of bulk transfer\
    \ applications).\n   The test device must not only support bulk TCP transfer application\n\
    \   traffic but MUST also support chatty traffic.  A valid stress test\n   SHOULD\
    \ include both traffic types.  This is due to the non-uniform,\n   bursty nature\
    \ of chatty applications versus the relatively uniform\n   nature of bulk transfers\
    \ (the bulk transfer smoothly stabilizes to\n   equilibrium state under lossless\
    \ conditions).\n   While iperf is an excellent choice for TCP bulk transfer testing,\
    \ the\n   \"netperf\" open source tool provides the ability to control client\
    \ and\n   server request/response behavior.  The netperf-wrapper tool is a\n \
    \  Python script that runs multiple simultaneous netperf instances and\n   aggregates\
    \ the results.  Appendix A provides an overview of\n   netperf/netperf-wrapper,\
    \ as well as iperf.  As with any software-\n   based tool, the performance must\
    \ be qualified to the link speed to be\n   tested.  Hardware-based test equipment\
    \ should be considered for\n   reliable results at higher link speeds (e.g., 1\
    \ GigE, 10 GigE).\n"
- title: 5.2.1.  TCP Test Pattern Definitions
  contents:
  - "5.2.1.  TCP Test Pattern Definitions\n   As mentioned in the goals of this framework,\
    \ techniques are defined\n   to specify TCP traffic test patterns to benchmark\
    \ traffic management\n   technique(s) and produce repeatable results.  Some network\
    \ devices,\n   such as firewalls, will not process stateless test traffic; this\
    \ is\n   another reason why stateful TCP test traffic must be used.\n   An application\
    \ could be fully emulated up to Layer 7; however, this\n   framework proposes\
    \ that stateful TCP test patterns be used in order\n   to provide granular and\
    \ repeatable control for the benchmarks.  The\n   following diagram illustrates\
    \ a simple web-browsing application\n   (HTTP).\n                            \
    \ GET URL\n             Client      ------------------------->   Web\n       \
    \                                           |\n             Web             200\
    \ OK        100 ms |\n                                                  |\n  \
    \           Browser     <-------------------------   Server\n            Figure\
    \ 3: Simple Flow Diagram for a Web Application\n   In this example, the Client\
    \ Web Browser (client) requests a URL, and\n   then the Web Server delivers the\
    \ web page content to the client\n   (after a server delay of 100 milliseconds).\
    \  This asynchronous\n   \"request/response\" behavior is intrinsic to most TCP-based\n\
    \   applications, such as email (SMTP), file transfers (FTP and Server\n   Message\
    \ Block (SMB)), database (SQL), web applications (SOAP), and\n   Representational\
    \ State Transfer (REST).  The impact on the network\n   elements is due to the\
    \ multitudes of clients and the variety of\n   bursty traffic, which stress traffic\
    \ management functions.  The\n   actual emulation of the specific application\
    \ protocols is not\n   required, and TCP test patterns can be defined to mimic\
    \ the\n   application network traffic flows and produce repeatable results.\n\
    \   Application modeling techniques have been proposed in\n   [3GPP2-C_R1002-A],\
    \ which provides examples to model the behavior of\n   HTTP, FTP, and Wireless\
    \ Application Protocol (WAP) applications at\n   the TCP layer.  The models have\
    \ been defined with various\n   mathematical distributions for the request/response\
    \ bytes and\n   inter-request gap times.  The model definition formats described\
    \ in\n   [3GPP2-C_R1002-A] are the basis for the guidelines provided in\n   Appendix\
    \ B and are also similar to formats used by network modeling\n   tools.  Packet\
    \ captures can also be used to characterize application\n   traffic and specify\
    \ some of the test patterns listed in Appendix B.\n   This framework does not\
    \ specify a fixed set of TCP test patterns but\n   does provide test cases that\
    \ SHOULD be performed; see Appendix B.\n   Some of these examples reflect those\
    \ specified in [CA-Benchmark],\n   which suggests traffic mixes for a variety\
    \ of representative\n   application profiles.  Other examples are simply well-known\n\
    \   application traffic types such as HTTP.\n"
- title: 6.  Traffic Benchmarking Methodology
  contents:
  - "6.  Traffic Benchmarking Methodology\n   The traffic benchmarking methodology\
    \ uses the test setup from\n   Section 1.2 and metrics defined in Section 4.\n\
    \   Each test SHOULD compare the network device's internal statistics\n   (available\
    \ via command line management interface, SNMP, etc.) to the\n   measured metrics\
    \ defined in Section 4.  This evaluates the accuracy\n   of the internal traffic\
    \ management counters under individual test\n   conditions and capacity test conditions\
    \ as defined in Sections 4.1\n   and 4.2.  This comparison is not intended to\
    \ compare real-time\n   statistics, but rather the cumulative statistics reported\
    \ after the\n   test has completed and device counters have updated (it is common\
    \ for\n   device counters to update after an interval of 10 seconds or more).\n\
    \   From a device configuration standpoint, scheduling and shaping\n   functionality\
    \ can be applied to logical ports (e.g., Link Aggregation\n   (LAG)).  This would\
    \ result in the same scheduling and shaping\n   configuration applied to all of\
    \ the member physical ports.  The focus\n   of this document is only on tests\
    \ at a physical-port level.\n   The following sections provide the objective,\
    \ procedure, metrics, and\n   reporting format for each test.  For all test steps,\
    \ the following\n   global parameters must be specified:\n      Test Runs (Tr):\n\
    \         The number of times the test needs to be run to ensure accurate\n  \
    \       and repeatable results.  The recommended value is a minimum\n        \
    \ of 10.\n      Test Duration (Td):\n         The duration of a test iteration,\
    \ expressed in seconds.  The\n         recommended minimum value is 60 seconds.\n\
    \   The variability in the test results MUST be measured between test\n   runs,\
    \ and if the variation is characterized as a significant portion\n   of the measured\
    \ values, the next step may be to revise the methods to\n   achieve better consistency.\n"
- title: 6.1.  Policing Tests
  contents:
  - "6.1.  Policing Tests\n   A policer is defined as the entity performing the policy\
    \ function.\n   The intent of the policing tests is to verify the policer performance\n\
    \   (i.e., CIR/CBS and EIR/EBS parameters).  The tests will verify that\n   the\
    \ network device can handle the CIR with CBS and the EIR with EBS,\n   and will\
    \ use back-to-back packet-testing concepts as described in\n   [RFC2544] (but\
    \ adapted to burst size algorithms and terminology).\n   Also, [MEF-14], [MEF-19],\
    \ and [MEF-37] provide some bases for\n   specific components of this test.  The\
    \ burst hunt algorithm defined\n   in Section 5.1.1 can also be used to automate\
    \ the measurement of the\n   CBS value.\n   The tests are divided into two (2)\
    \ sections: individual policer tests\n   and then full-capacity policing tests.\
    \  It is important to benchmark\n   the basic functionality of the individual\
    \ policer and then proceed\n   into the fully rated capacity of the device.  This\
    \ capacity may\n   include the number of policing policies per device and the\
    \ number of\n   policers simultaneously active across all ports.\n"
- title: 6.1.1.  Policer Individual Tests
  contents:
  - "6.1.1.  Policer Individual Tests\n   Objective:\n      Test a policer as defined\
    \ by [RFC4115] or [MEF-10.3], depending\n      upon the equipment's specification.\
    \  In addition to verifying that\n      the policer allows the specified CBS and\
    \ EBS bursts to pass, the\n      policer test MUST verify that the policer will\
    \ remark or drop\n      excess packets, and pass traffic at the specified CBS/EBS\
    \ values.\n   Test Summary:\n      Policing tests should use stateless traffic.\
    \  Stateful TCP test\n      traffic will generally be adversely affected by a\
    \ policer in the\n      absence of traffic shaping.  So, while TCP traffic could\
    \ be used,\n      it is more accurate to benchmark a policer with stateless traffic.\n\
    \      As an example of a policer as defined by [RFC4115], consider a\n      CBS/EBS\
    \ of 64 KB and CIR/EIR of 100 Mbps on a 1 GigE physical link\n      (in color-blind\
    \ mode).  A stateless traffic burst of 64 KB would\n      be sent into the policer\
    \ at the GigE rate.  This equates to an\n      approximately 0.512-millisecond\
    \ burst time (64 KB at 1 GigE).  The\n      traffic generator must space these\
    \ bursts to ensure that the\n      aggregate throughput does not exceed the CIR.\
    \  The Ti between the\n      bursts would equal CBS * 8 / CIR = 5.12 milliseconds\
    \ in this\n      example.\n   Test Metrics:\n      The metrics defined in Section\
    \ 4.1 (BSA, LP, OOS, PD, and PDV)\n      SHALL be measured at the egress port\
    \ and recorded.\n   Procedure:\n      1. Configure the DUT policing parameters\
    \ for the desired CIR/EIR\n         and CBS/EBS values to be tested.\n      2.\
    \ Configure the tester to generate a stateless traffic burst\n         equal to\
    \ CBS and an interval equal to Ti (CBS in bits/CIR).\n      3. Compliant Traffic\
    \ Test: Generate bursts of CBS + EBS traffic\n         into the policer ingress\
    \ port, and measure the metrics defined\n         in Section 4.1 (BSA, LP, OOS,\
    \ PD, and PDV) at the egress port\n         and across the entire Td (default\
    \ 60-second duration).\n      4. Excess Traffic Test: Generate bursts of greater\
    \ than CBS + EBS\n         bytes into the policer ingress port, and verify that\
    \ the\n         policer only allowed the BSA bytes to exit the egress.  The\n\
    \         excess burst MUST be recorded; the recommended value is\n         1000\
    \ bytes.  Additional tests beyond the simple color-blind\n         example might\
    \ include color-aware mode, configurations where\n         EIR is greater than\
    \ CIR, etc.\n   Reporting Format:\n      The policer individual report MUST contain\
    \ all results for each\n      CIR/EIR/CBS/EBS test run.  A recommended format\
    \ is as follows:\n      ***********************************************************\n\
    \      Test Configuration Summary: Tr, Td\n      DUT Configuration Summary: CIR,\
    \ EIR, CBS, EBS\n      The results table should contain entries for each test\
    \ run,\n      as follows (Test #1 to Test #Tr):\n      -  Compliant Traffic Test:\
    \ BSA, LP, OOS, PD, and PDV\n      -  Excess Traffic Test: BSA\n      ***********************************************************\n"
- title: 6.1.2.  Policer Capacity Tests
  contents:
  - "6.1.2.  Policer Capacity Tests\n   Objective:\n      The intent of the capacity\
    \ tests is to verify the policer\n      performance in a scaled environment with\
    \ multiple ingress customer\n      policers on multiple physical ports.  This\
    \ test will benchmark the\n      maximum number of active policers as specified\
    \ by the device\n      manufacturer.\n   Test Summary:\n      The specified policing\
    \ function capacity is generally expressed in\n      terms of the number of policers\
    \ active on each individual physical\n      port as well as the number of unique\
    \ policer rates that are\n      utilized.  For all of the capacity tests, the\
    \ benchmarking test\n      procedure and reporting format described in Section\
    \ 6.1.1 for a\n      single policer MUST be applied to each of the physical-port\n\
    \      policers.\n      For example, a Layer 2 switching device may specify that\
    \ each of\n      the 32 physical ports can be policed using a pool of policing\n\
    \      service policies.  The device may carry a single customer's\n      traffic\
    \ on each physical port, and a single policer is\n      instantiated per physical\
    \ port.  Another possibility is that a\n      single physical port may carry multiple\
    \ customers, in which case\n      many customer flows would be policed concurrently\
    \ on an individual\n      physical port (separate policers per customer on an\
    \ individual\n      port).\n   Test Metrics:\n      The metrics defined in Section\
    \ 4.1 (BSA, LP, OOS, PD, and PDV)\n      SHALL be measured at the egress port\
    \ and recorded.\n   The following sections provide the specific test scenarios,\n\
    \   procedures, and reporting formats for each policer capacity test.\n"
- title: 6.1.2.1.  Maximum Policers on Single Physical Port
  contents:
  - "6.1.2.1.  Maximum Policers on Single Physical Port\n   Test Summary:\n      The\
    \ first policer capacity test will benchmark a single physical\n      port, with\
    \ maximum policers on that physical port.\n      Assume multiple categories of\
    \ ingress policers at rates\n      r1, r2, ..., rn.  There are multiple customers\
    \ on a single\n      physical port.  Each customer could be represented by a\n\
    \      single-tagged VLAN, a double-tagged VLAN, a Virtual Private LAN\n     \
    \ Service (VPLS) instance, etc.  Each customer is mapped to a\n      different\
    \ policer.  Each of the policers can be of rates\n      r1, r2, ..., rn.\n   \
    \   An example configuration would be\n      -  Y1 customers, policer rate r1\n\
    \      -  Y2 customers, policer rate r2\n      -  Y3 customers, policer rate r3\n\
    \      ...\n      -  Yn customers, policer rate rn\n      Some bandwidth on the\
    \ physical port is dedicated for other traffic\n      (i.e., other than customer\
    \ traffic); this includes network control\n      protocol traffic.  There is a\
    \ separate policer for the other\n      traffic.  Typical deployments have three\
    \ categories of policers;\n      there may be some deployments with more or less\
    \ than three\n      categories of ingress policers.\n   Procedure:\n      1. Configure\
    \ the DUT policing parameters for the desired CIR/EIR\n         and CBS/EBS values\
    \ for each policer rate (r1-rn) to be tested.\n      2. Configure the tester to\
    \ generate a stateless traffic burst\n         equal to CBS and an interval equal\
    \ to Ti (CBS in bits/CIR) for\n         each customer stream (Y1-Yn).  The encapsulation\
    \ for each\n         customer must also be configured according to the service\n\
    \         tested (VLAN, VPLS, IP mapping, etc.).\n      3. Compliant Traffic Test:\
    \ Generate bursts of CBS + EBS traffic\n         into the policer ingress port\
    \ for each customer traffic stream,\n         and measure the metrics defined\
    \ in Section 4.1 (BSA, LP, OOS,\n         PD, and PDV) at the egress port for\
    \ each stream and across the\n         entire Td (default 30-second duration).\n\
    \      4. Excess Traffic Test: Generate bursts of greater than CBS + EBS\n   \
    \      bytes into the policer ingress port for each customer traffic\n       \
    \  stream, and verify that the policer only allowed the BSA bytes\n         to\
    \ exit the egress for each stream.  The excess burst MUST be\n         recorded;\
    \ the recommended value is 1000 bytes.\n   Reporting Format:\n      The policer\
    \ individual report MUST contain all results for each\n      CIR/EIR/CBS/EBS test\
    \ run, per customer traffic stream.  A\n      recommended format is as follows:\n\
    \      *****************************************************************\n   \
    \   Test Configuration Summary: Tr, Td\n      Customer Traffic Stream Encapsulation:\
    \ Map each stream to VLAN,\n      VPLS, IP address\n      DUT Configuration Summary\
    \ per Customer Traffic Stream: CIR, EIR,\n      CBS, EBS\n      The results table\
    \ should contain entries for each test run,\n      as follows (Test #1 to Test\
    \ #Tr):\n      -  Customer Stream Y1-Yn (see note) Compliant Traffic Test:\n \
    \        BSA, LP, OOS, PD, and PDV\n      -  Customer Stream Y1-Yn (see note)\
    \ Excess Traffic Test: BSA\n      *****************************************************************\n\
    \      Note: For each test run, there will be two (2) rows for each\n      customer\
    \ stream: the Compliant Traffic Test result and the Excess\n      Traffic Test\
    \ result.\n"
- title: 6.1.2.2.  Single Policer on All Physical Ports
  contents:
  - "6.1.2.2.  Single Policer on All Physical Ports\n   Test Summary:\n      The second\
    \ policer capacity test involves a single policer\n      function per physical\
    \ port with all physical ports active.  In\n      this test, there is a single\
    \ policer per physical port.  The\n      policer can have one of the rates r1,\
    \ r2, ..., rn.  All of the\n      physical ports in the networking device are\
    \ active.\n   Procedure:\n      The procedure for this test is identical to the\
    \ procedure listed\n      in Section 6.1.1.  The configured parameters must be\
    \ reported\n      per port, and the test report must include results per measured\n\
    \      egress port.\n"
- title: 6.1.2.3.  Maximum Policers on All Physical Ports
  contents:
  - "6.1.2.3.  Maximum Policers on All Physical Ports\n   The third policer capacity\
    \ test is a combination of the first and\n   second capacity tests, i.e., maximum\
    \ policers active per physical\n   port and all physical ports active.\n   Procedure:\n\
    \      The procedure for this test is identical to the procedure listed\n    \
    \  in Section 6.1.2.1.  The configured parameters must be reported\n      per\
    \ port, and the test report must include per-stream results per\n      measured\
    \ egress port.\n"
- title: 6.2.  Queue/Scheduler Tests
  contents:
  - "6.2.  Queue/Scheduler Tests\n   Queues and traffic scheduling are closely related\
    \ in that a queue's\n   priority dictates the manner in which the traffic scheduler\
    \ transmits\n   packets out of the egress port.\n   Since device queues/buffers\
    \ are generally an egress function, this\n   test framework will discuss testing\
    \ at the egress (although the\n   technique can be applied to ingress-side queues).\n\
    \   Similar to the policing tests, these tests are divided into two\n   sections:\
    \ individual queue/scheduler function tests and then\n   full-capacity tests.\n"
- title: 6.2.1.  Queue/Scheduler Individual Tests
  contents:
  - "6.2.1.  Queue/Scheduler Individual Tests\n   The various types of scheduling\
    \ techniques include FIFO, Strict\n   Priority (SP) queuing, and Weighted Fair\
    \ Queuing (WFQ), along with\n   other variations.  This test framework recommends\
    \ testing with a\n   minimum of three techniques, although benchmarking other\n\
    \   device-scheduling algorithms is left to the discretion of the tester.\n"
- title: 6.2.1.1.  Testing Queue/Scheduler with Stateless Traffic
  contents:
  - "6.2.1.1.  Testing Queue/Scheduler with Stateless Traffic\n   Objective:\n   \
    \   Verify that the configured queue and scheduling technique can\n      handle\
    \ stateless traffic bursts up to the queue depth.\n   Test Summary:\n      A network\
    \ device queue is memory based, unlike a policing\n      function, which is token\
    \ or credit based.  However, the same\n      concepts from Section 6.1 can be\
    \ applied to testing network device\n      queues.\n      The device's network\
    \ queue should be configured to the desired\n      size in KB (i.e., Queue Length\
    \ (QL)), and then stateless traffic\n      should be transmitted to test this\
    \ QL.\n      A queue should be able to handle repetitive bursts with the\n   \
    \   transmission gaps proportional to the Bottleneck Bandwidth (BB).\n      The\
    \ transmission gap is referred to here as the transmission\n      interval (Ti).\
    \  The Ti can be defined for the traffic bursts and\n      is based on the QL\
    \ and BB of the egress interface.\n         Ti = QL * 8 / BB\n      Note that\
    \ this equation is similar to the Ti required for\n      transmission into a policer\
    \ (QL = CBS, BB = CIR).  Note also that\n      the burst hunt algorithm defined\
    \ in Section 5.1.1 can also be used\n      to automate the measurement of the\
    \ queue value.\n      The stateless traffic burst SHALL be transmitted at the\
    \ link speed\n      and spaced within the transmission interval (Ti).  The metrics\n\
    \      defined in Section 4.1 SHALL be measured at the egress port and\n     \
    \ recorded; the primary intent is to verify the BSA and verify that\n      no\
    \ packets are dropped.\n      The scheduling function must also be characterized\
    \ to benchmark\n      the device's ability to schedule the queues according to\
    \ the\n      priority.  An example would be two levels of priority that include\n\
    \      SP and FIFO queuing.  Under a flow load greater than the egress\n     \
    \ port speed, the higher-priority packets should be transmitted\n      without\
    \ drops (and also maintain low latency), while the lower-\n      priority (or\
    \ best-effort) queue may be dropped.\n   Test Metrics:\n      The metrics defined\
    \ in Section 4.1 (BSA, LP, OOS, PD, and PDV)\n      SHALL be measured at the egress\
    \ port and recorded.\n   Procedure:\n      1. Configure the DUT QL and scheduling\
    \ technique parameters (FIFO,\n         SP, etc.).\n      2. Configure the tester\
    \ to generate a stateless traffic burst\n         equal to QL and an interval\
    \ equal to Ti (QL in bits/BB).\n      3. Generate bursts of QL traffic into the\
    \ DUT, and measure the\n         metrics defined in Section 4.1 (LP, OOS, PD,\
    \ and PDV) at the\n         egress port and across the entire Td (default 30-second\n\
    \         duration).\n   Reporting Format:\n      The Queue/Scheduler Stateless\
    \ Traffic individual report MUST\n      contain all results for each QL/BB test\
    \ run.  A recommended format\n      is as follows:\n      ****************************************************************\n\
    \      Test Configuration Summary: Tr, Td\n      DUT Configuration Summary: Scheduling\
    \ technique (i.e., FIFO, SP,\n      WFQ, etc.), BB, and QL\n      The results\
    \ table should contain entries for each test run,\n      as follows (Test #1 to\
    \ Test #Tr):\n      -  LP, OOS, PD, and PDV\n      ****************************************************************\n"
- title: 6.2.1.2.  Testing Queue/Scheduler with Stateful Traffic
  contents:
  - "6.2.1.2.  Testing Queue/Scheduler with Stateful Traffic\n   Objective:\n    \
    \  Verify that the configured queue and scheduling technique can\n      handle\
    \ stateful traffic bursts up to the queue depth.\n   Test Background and Summary:\n\
    \      To provide a more realistic benchmark and to test queues in\n      Layer\
    \ 4 devices such as firewalls, stateful traffic testing is\n      recommended\
    \ for the queue tests.  Stateful traffic tests will also\n      utilize the Network\
    \ Delay Emulator (NDE) from the network setup\n      configuration in Section\
    \ 1.2.\n      The BDP of the TCP test traffic must be calibrated to the QL of\n\
    \      the device queue.  Referencing [RFC6349], the BDP is equal to:\n      \
    \   BB * RTT / 8 (in bytes)\n      The NDE must be configured to an RTT value\
    \ that is large enough to\n      allow the BDP to be greater than QL.  An example\
    \ test scenario is\n      defined below:\n      -  Ingress link = GigE\n     \
    \ -  Egress link = 100 Mbps (BB)\n      -  QL = 32 KB\n      RTT(min) = QL * 8\
    \ / BB and would equal 2.56 ms\n         (and the BDP = 32 KB)\n      In this\
    \ example, one (1) TCP connection with window size / SSB of\n      32 KB would\
    \ be required to test the QL of 32 KB.  This Bulk\n      Transfer Test can be\
    \ accomplished using iperf, as described in\n      Appendix A.\n      Two types\
    \ of TCP tests MUST be performed: the Bulk Transfer Test\n      and the Micro\
    \ Burst Test Pattern, as documented in Appendix B.\n      The Bulk Transfer Test\
    \ only bursts during the TCP Slow Start (or\n      Congestion Avoidance) state,\
    \ while the Micro Burst Test Pattern\n      emulates application-layer bursting,\
    \ which may occur any time\n      during the TCP connection.\n      Other types\
    \ of tests SHOULD include the following: simple web\n      sites, complex web\
    \ sites, business applications, email, and\n      SMB/CIFS (Common Internet File\
    \ System) file copy (all of which are\n      also documented in Appendix B).\n\
    \   Test Metrics:\n      The test results will be recorded per the stateful metrics\
    \ defined\n      in Section 4.2 -- primarily the TCP Test Pattern Execution Time\n\
    \      (TTPET), TCP Efficiency, and Buffer Delay.\n   Procedure:\n      1. Configure\
    \ the DUT QL and scheduling technique parameters (FIFO,\n         SP, etc.).\n\
    \      2. Configure the test generator* with a profile of an emulated\n      \
    \   application traffic mixture.\n         -  The application mixture MUST be\
    \ defined in terms of\n            percentage of the total bandwidth to be tested.\n\
    \         -  The rate of transmission for each application within the\n      \
    \      mixture MUST also be configurable.\n         *  To ensure repeatable results,\
    \ the test generator MUST be\n            capable of generating precise TCP test\
    \ patterns for each\n            application specified.\n      3. Generate application\
    \ traffic between the ingress (client side)\n         and egress (server side)\
    \ ports of the DUT, and measure the\n         metrics (TTPET, TCP Efficiency,\
    \ and Buffer Delay) per\n         application stream and at the ingress and egress\
    \ ports (across\n         the entire Td, default 60-second duration).\n      A\
    \ couple of items require clarification concerning application\n      measurements:\
    \ an application session may be comprised of a single\n      TCP connection or\
    \ multiple TCP connections.\n      If an application session utilizes a single\
    \ TCP connection, the\n      application throughput/metrics have a 1-1 relationship\
    \ to the TCP\n      connection measurements.\n      If an application session\
    \ (e.g., an HTTP-based application)\n      utilizes multiple TCP connections,\
    \ then all of the TCP connections\n      are aggregated in the application throughput\
    \ measurement/metrics\n      for that application.\n      Then, there is the case\
    \ of multiple instances of an application\n      session (i.e., multiple FTPs\
    \ emulating multiple clients).  In this\n      situation, the test should measure/record\
    \ each FTP application\n      session independently, tabulating the minimum, maximum,\
    \ and\n      average for all FTP sessions.\n      Finally, application throughput\
    \ measurements are based on Layer 4\n      TCP throughput and do not include bytes\
    \ retransmitted.  The TCP\n      Efficiency metric MUST be measured during the\
    \ test, because it\n      provides a measure of \"goodput\" during each test.\n\
    \   Reporting Format:\n      The Queue/Scheduler Stateful Traffic individual report\
    \ MUST\n      contain all results for each traffic scheduler and QL/BB test run.\n\
    \      A recommended format is as follows:\n      ******************************************************************\n\
    \      Test Configuration Summary: Tr, Td\n      DUT Configuration Summary: Scheduling\
    \ technique (i.e., FIFO, SP,\n      WFQ, etc.), BB, and QL\n      Application\
    \ Mixture and Intensities: These are the percentages\n      configured for each\
    \ application type.\n      The results table should contain entries for each test\
    \ run, with\n      minimum, maximum, and average per application session, as follows\n\
    \      (Test #1 to Test #Tr):\n      -  Throughput (bps) and TTPET for each application\
    \ session\n      -  Bytes In and Bytes Out for each application session\n    \
    \  -  TCP Efficiency and Buffer Delay for each application session\n      ******************************************************************\n"
- title: 6.2.2.  Queue/Scheduler Capacity Tests
  contents:
  - "6.2.2.  Queue/Scheduler Capacity Tests\n   Objective:\n      The intent of these\
    \ capacity tests is to benchmark queue/scheduler\n      performance in a scaled\
    \ environment with multiple\n      queues/schedulers active on multiple egress\
    \ physical ports.  These\n      tests will benchmark the maximum number of queues\
    \ and schedulers\n      as specified by the device manufacturer.  Each priority\
    \ in the\n      system will map to a separate queue.\n   Test Metrics:\n     \
    \ The metrics defined in Section 4.1 (BSA, LP, OOS, PD, and PDV)\n      SHALL\
    \ be measured at the egress port and recorded.\n   The following sections provide\
    \ the specific test scenarios,\n   procedures, and reporting formats for each\
    \ queue/scheduler capacity\n   test.\n"
- title: 6.2.2.1.  Multiple Queues, Single Port Active
  contents:
  - "6.2.2.1.  Multiple Queues, Single Port Active\n   For the first queue/scheduler\
    \ capacity test, multiple queues per port\n   will be tested on a single physical\
    \ port.  In this case, all of the\n   queues (typically eight) are active on a\
    \ single physical port.\n   Traffic from multiple ingress physical ports is directed\
    \ to the same\n   egress physical port.  This will cause oversubscription on the\
    \ egress\n   physical port.\n   There are many types of priority schemes and combinations\
    \ of\n   priorities that are managed by the scheduler.  The following sections\n\
    \   specify the priority schemes that should be tested.\n"
- title: 6.2.2.1.1.  Strict Priority on Egress Port
  contents:
  - "6.2.2.1.1.  Strict Priority on Egress Port\n   Test Summary:\n      For this\
    \ test, SP scheduling on the egress physical port should be\n      tested, and\
    \ the benchmarking methodologies specified in\n      Sections 6.2.1.1 (stateless)\
    \ and 6.2.1.2 (stateful) (procedure,\n      metrics, and reporting format) should\
    \ be applied here.  For a\n      given priority, each ingress physical port should\
    \ get a fair share\n      of the egress physical-port bandwidth.\n      Since\
    \ this is a capacity test, the configuration and report\n      results format\
    \ (see Sections 6.2.1.1 and 6.2.1.2) MUST also\n      include:\n      Configuration:\n\
    \      -  The number of physical ingress ports active during the test\n      -\
    \  The classification marking (DSCP, VLAN, etc.) for each physical\n         ingress\
    \ port\n      -  The traffic rate for stateful traffic and the traffic\n     \
    \    rate/mixture for stateful traffic for each physical\n         ingress port\n\
    \      Report Results:\n      -  For each ingress port traffic stream, the achieved\
    \ throughput\n         rate and metrics at the egress port\n"
- title: 6.2.2.1.2.  Strict Priority + WFQ on Egress Port
  contents:
  - "6.2.2.1.2.  Strict Priority + WFQ on Egress Port\n   Test Summary:\n      For\
    \ this test, SP and WFQ should be enabled simultaneously in the\n      scheduler,\
    \ but on a single egress port.  The benchmarking\n      methodologies specified\
    \ in Sections 6.2.1.1 (stateless) and\n      6.2.1.2 (stateful) (procedure, metrics,\
    \ and reporting format)\n      should be applied here.  Additionally, the egress\
    \ port\n      bandwidth-sharing among weighted queues should be proportional to\n\
    \      the assigned weights.  For a given priority, each ingress physical\n  \
    \    port should get a fair share of the egress physical-port\n      bandwidth.\n\
    \      Since this is a capacity test, the configuration and report\n      results\
    \ format (see Sections 6.2.1.1 and 6.2.1.2) MUST also\n      include:\n      Configuration:\n\
    \      -  The number of physical ingress ports active during the test\n      -\
    \  The classification marking (DSCP, VLAN, etc.) for each physical\n         ingress\
    \ port\n      -  The traffic rate for stateful traffic and the traffic\n     \
    \    rate/mixture for stateful traffic for each physical\n         ingress port\n\
    \      Report Results:\n      -  For each ingress port traffic stream, the achieved\
    \ throughput\n         rate and metrics at each queue of the egress port queue\
    \ (both\n         the SP and WFQ)\n      Example:\n      -  Egress Port SP Queue:\
    \ throughput and metrics for ingress\n         streams 1-n\n      -  Egress Port\
    \ WFQ: throughput and metrics for ingress streams 1-n\n"
- title: 6.2.2.2.  Single Queue per Port, All Ports Active
  contents:
  - "6.2.2.2.  Single Queue per Port, All Ports Active\n   Test Summary:\n      Traffic\
    \ from multiple ingress physical ports is directed to the\n      same egress physical\
    \ port.  This will cause oversubscription on\n      the egress physical port.\
    \  Also, the same amount of traffic is\n      directed to each egress physical\
    \ port.\n      The benchmarking methodologies specified in Sections 6.2.1.1\n\
    \      (stateless) and 6.2.1.2 (stateful) (procedure, metrics, and\n      reporting\
    \ format)  should be applied here.  Each ingress physical\n      port should get\
    \ a fair share of the egress physical-port\n      bandwidth.  Additionally, each\
    \ egress physical port should receive\n      the same amount of traffic.\n   \
    \   Since this is a capacity test, the configuration and report\n      results\
    \ format (see Sections 6.2.1.1 and 6.2.1.2) MUST also\n      include:\n      Configuration:\n\
    \      -  The number of ingress ports active during the test\n      -  The number\
    \ of egress ports active during the test\n      -  The classification marking\
    \ (DSCP, VLAN, etc.) for each physical\n         ingress port\n      -  The traffic\
    \ rate for stateful traffic and the traffic\n         rate/mixture for stateful\
    \ traffic for each physical\n         ingress port\n      Report Results:\n  \
    \    -  For each egress port, the achieved throughput rate and metrics\n     \
    \    at the egress port queue for each ingress port stream\n      Example:\n \
    \     -  Egress Port 1: throughput and metrics for ingress streams 1-n\n     \
    \ -  Egress Port n: throughput and metrics for ingress streams 1-n\n"
- title: 6.2.2.3.  Multiple Queues per Port, All Ports Active
  contents:
  - "6.2.2.3.  Multiple Queues per Port, All Ports Active\n   Test Summary:\n    \
    \  Traffic from multiple ingress physical ports is directed to all\n      queues\
    \ of each egress physical port.  This will cause\n      oversubscription on the\
    \ egress physical ports.  Also, the same\n      amount of traffic is directed\
    \ to each egress physical port.\n      The benchmarking methodologies specified\
    \ in Sections 6.2.1.1\n      (stateless) and 6.2.1.2 (stateful) (procedure, metrics,\
    \ and\n      reporting format) should be applied here.  For a given priority,\n\
    \      each ingress physical port should get a fair share of the egress\n    \
    \  physical-port bandwidth.  Additionally, each egress physical port\n      should\
    \ receive the same amount of traffic.\n      Since this is a capacity test, the\
    \ configuration and report\n      results format (see Sections 6.2.1.1 and 6.2.1.2)\
    \ MUST also\n      include:\n      Configuration:\n      -  The number of physical\
    \ ingress ports active during the test\n      -  The classification marking (DSCP,\
    \ VLAN, etc.) for each physical\n         ingress port\n      -  The traffic rate\
    \ for stateful traffic and the traffic\n         rate/mixture for stateful traffic\
    \ for each physical\n         ingress port\n      Report Results:\n      -  For\
    \ each egress port, the achieved throughput rate and metrics\n         at each\
    \ egress port queue for each ingress port stream\n      Example:\n      -  Egress\
    \ Port 1, SP Queue: throughput and metrics for ingress\n         streams 1-n\n\
    \      -  Egress Port 2, WFQ: throughput and metrics for ingress\n         streams\
    \ 1-n\n      ...\n      -  Egress Port n, SP Queue: throughput and metrics for\
    \ ingress\n         streams 1-n\n      -  Egress Port n, WFQ: throughput and metrics\
    \ for ingress\n         streams 1-n\n"
- title: 6.3.  Shaper Tests
  contents:
  - "6.3.  Shaper Tests\n   Like a queue, a traffic shaper is memory based, but with\
    \ the added\n   intelligence of an active traffic scheduler.  The same concepts\
    \ as\n   those described in Section 6.2 (queue testing) can be applied to\n  \
    \ testing a network device shaper.\n   Again, the tests are divided into two sections:\
    \ individual shaper\n   benchmark tests and then full-capacity shaper benchmark\
    \ tests.\n"
- title: 6.3.1.  Shaper Individual Tests
  contents:
  - "6.3.1.  Shaper Individual Tests\n   A traffic shaper generally has three (3)\
    \ components that can be\n   configured:\n   -  Ingress Queue bytes\n   -  Shaper\
    \ Rate (SR), bps\n   -  Burst Committed (Bc) and Burst Excess (Be), bytes\n  \
    \ The Ingress Queue holds burst traffic, and the shaper then meters\n   traffic\
    \ out of the egress port according to the SR and Bc/Be\n   parameters.  Shapers\
    \ generally transmit into policers, so the idea is\n   for the emitted traffic\
    \ to conform to the policer's limits.\n"
- title: 6.3.1.1.  Testing Shaper with Stateless Traffic
  contents:
  - "6.3.1.1.  Testing Shaper with Stateless Traffic\n   Objective:\n      Test a\
    \ shaper by transmitting stateless traffic bursts into the\n      shaper ingress\
    \ port and verifying that the egress traffic is\n      shaped according to the\
    \ shaper traffic profile.\n   Test Summary:\n      The stateless traffic must\
    \ be burst into the DUT ingress port and\n      not exceed the Ingress Queue.\
    \  The burst can be a single burst or\n      multiple bursts.  If multiple bursts\
    \ are transmitted, then the\n      transmission interval (Ti) must be large enough\
    \ so that the SR is\n      not exceeded.  An example will clarify single-burst\
    \ and multiple-\n      burst test cases.\n      In this example, the shaper's\
    \ ingress and egress ports are both\n      full-duplex Gigabit Ethernet.  The\
    \ Ingress Queue is configured to\n      be 512,000 bytes, the SR = 50 Mbps, and\
    \ both Bc and Be are\n      configured to be 32,000 bytes.  For a single-burst\
    \ test, the\n      transmitting test device would burst 512,000 bytes maximum\
    \ into\n      the ingress port and then stop transmitting.\n      If a multiple-burst\
    \ test is to be conducted, then the burst bytes\n      divided by the transmission\
    \ interval between the 512,000-byte\n      bursts must not exceed the SR.  The\
    \ transmission interval (Ti)\n      must adhere to a formula similar to the formula\
    \ described in\n      Section 6.2.1.1 for queues, namely:\n         Ti = Ingress\
    \ Queue * 8 / SR\n      For the example from the previous paragraph, the Ti between\
    \ bursts\n      must be greater than 82 milliseconds (512,000 bytes * 8 /\n  \
    \    50,000,000 bps).  This yields an average rate of 50 Mbps so that\n      an\
    \ Ingress Queue would not overflow.\n   Test Metrics:\n      The metrics defined\
    \ in Section 4.1 (LP, OOS, PDV, SR, SBB, and\n      SBI) SHALL be measured at\
    \ the egress port and recorded.\n   Procedure:\n      1. Configure the DUT shaper\
    \ ingress QL and shaper egress rate\n         parameters (SR, Bc, Be).\n     \
    \ 2. Configure the tester to generate a stateless traffic burst\n         equal\
    \ to QL and an interval equal to Ti (QL in bits/BB).\n      3. Generate bursts\
    \ of QL traffic into the DUT, and measure the\n         metrics defined in Section\
    \ 4.1 (LP, OOS, PDV, SR, SBB, and SBI)\n         at the egress port and across\
    \ the entire Td (default 30-second\n         duration).\n   Reporting Format:\n\
    \      The Shaper Stateless Traffic individual report MUST contain all\n     \
    \ results for each QL/SR test run.  A recommended format is as\n      follows:\n\
    \      ***********************************************************\n      Test\
    \ Configuration Summary: Tr, Td\n      DUT Configuration Summary: Ingress Burst\
    \ Rate, QL, SR\n      The results table should contain entries for each test run,\n\
    \      as follows (Test #1 to Test #Tr):\n      -  LP, OOS, PDV, SR, SBB, and\
    \ SBI\n      ***********************************************************\n"
- title: 6.3.1.2.  Testing Shaper with Stateful Traffic
  contents:
  - "6.3.1.2.  Testing Shaper with Stateful Traffic\n   Objective:\n      Test a shaper\
    \ by transmitting stateful traffic bursts into the\n      shaper ingress port\
    \ and verifying that the egress traffic is\n      shaped according to the shaper\
    \ traffic profile.\n   Test Summary:\n      To provide a more realistic benchmark\
    \ and to test queues in\n      Layer 4 devices such as firewalls, stateful traffic\
    \ testing is\n      also recommended for the shaper tests.  Stateful traffic tests\n\
    \      will also utilize the Network Delay Emulator (NDE) from the\n      network\
    \ setup configuration in Section 1.2.\n      The BDP of the TCP test traffic must\
    \ be calculated as described in\n      Section 6.2.1.2.  To properly stress network\
    \ buffers and the\n      traffic-shaping function, the TCP window size (which\
    \ is the\n      minimum of the TCP RWND and sender socket) should be greater than\n\
    \      the BDP, which will stress the shaper.  BDP factors of 1.1 to 1.5\n   \
    \   are recommended, but the values are left to the discretion of the\n      tester\
    \ and should be documented.\n      The cumulative TCP window sizes* (RWND at the\
    \ receiving end and\n      CWND at the transmitting end) equates to the TCP window\
    \ size* for\n      each connection, multiplied by the number of connections.\n\
    \      *  As described in Section 3 of [RFC6349], the SSB MUST be large\n    \
    \     enough to fill the BDP.\n      For example, if the BDP is equal to 256 KB\
    \ and a connection size\n      of 64 KB is used for each connection, then it would\
    \ require four\n      (4) connections to fill the BDP and 5-6 connections (oversubscribe\n\
    \      the BDP) to stress-test the traffic-shaping function.\n      Two types\
    \ of TCP tests MUST be performed: the Bulk Transfer Test\n      and the Micro\
    \ Burst Test Pattern, as documented in Appendix B.\n      The Bulk Transfer Test\
    \ only bursts during the TCP Slow Start (or\n      Congestion Avoidance) state,\
    \ while the Micro Burst Test Pattern\n      emulates application-layer bursting,\
    \ which may occur any time\n      during the TCP connection.\n      Other types\
    \ of tests SHOULD include the following: simple web\n      sites, complex web\
    \ sites, business applications, email, and\n      SMB/CIFS file copy (all of which\
    \ are also documented in\n      Appendix B).\n   Test Metrics:\n      The test\
    \ results will be recorded per the stateful metrics defined\n      in Section\
    \ 4.2 -- primarily the TCP Test Pattern Execution Time\n      (TTPET), TCP Efficiency,\
    \ and Buffer Delay.\n   Procedure:\n      1. Configure the DUT shaper ingress\
    \ QL and shaper egress rate\n         parameters (SR, Bc, Be).\n      2. Configure\
    \ the test generator* with a profile of an emulated\n         application traffic\
    \ mixture.\n         -  The application mixture MUST be defined in terms of\n\
    \            percentage of the total bandwidth to be tested.\n         -  The\
    \ rate of transmission for each application within the\n            mixture MUST\
    \ also be configurable.\n         *  To ensure repeatable results, the test generator\
    \ MUST be\n            capable of generating precise TCP test patterns for each\n\
    \            application specified.\n      3. Generate application traffic between\
    \ the ingress (client side)\n         and egress (server side) ports of the DUT,\
    \ and measure the\n         metrics (TTPET, TCP Efficiency, and Buffer Delay)\
    \ per\n         application stream and at the ingress and egress ports (across\n\
    \         the entire Td, default 30-second duration).\n   Reporting Format:\n\
    \      The Shaper Stateful Traffic individual report MUST contain all\n      results\
    \ for each traffic scheduler and QL/SR test run.  A\n      recommended format\
    \ is as follows:\n      ******************************************************************\n\
    \      Test Configuration Summary: Tr, Td\n      DUT Configuration Summary: Ingress\
    \ Burst Rate, QL, SR\n      Application Mixture and Intensities: These are the\
    \ percentages\n      configured for each application type.\n      The results\
    \ table should contain entries for each test run, with\n      minimum, maximum,\
    \ and average per application session, as follows\n      (Test #1 to Test #Tr):\n\
    \      -  Throughput (bps) and TTPET for each application session\n      -  Bytes\
    \ In and Bytes Out for each application session\n      -  TCP Efficiency and Buffer\
    \ Delay for each application session\n      ******************************************************************\n"
- title: 6.3.2.  Shaper Capacity Tests
  contents:
  - "6.3.2.  Shaper Capacity Tests\n   Objective:\n      The intent of these scalability\
    \ tests is to verify shaper\n      performance in a scaled environment with shapers\
    \ active on\n      multiple queues on multiple egress physical ports.  These tests\n\
    \      will benchmark the maximum number of shapers as specified by the\n    \
    \  device manufacturer.\n   The following sections provide the specific test scenarios,\n\
    \   procedures, and reporting formats for each shaper capacity test.\n"
- title: 6.3.2.1.  Single Queue Shaped, All Physical Ports Active
  contents:
  - "6.3.2.1.  Single Queue Shaped, All Physical Ports Active\n   Test Summary:\n\
    \      The first shaper capacity test involves per-port shaping with all\n   \
    \   physical ports active.  Traffic from multiple ingress physical\n      ports\
    \ is directed to the same egress physical port.  This will\n      cause oversubscription\
    \ on the egress physical port.  Also, the\n      same amount of traffic is directed\
    \ to each egress physical port.\n      The benchmarking methodologies specified\
    \ in Sections 6.3.1.1\n      (stateless) and 6.3.1.2 (stateful) (procedure, metrics,\
    \ and\n      reporting format) should be applied here.  Since this is a\n    \
    \  capacity test, the configuration and report results format (see\n      Section\
    \ 6.3.1) MUST also include:\n      Configuration:\n      -  The number of physical\
    \ ingress ports active during the test\n      -  The classification marking (DSCP,\
    \ VLAN, etc.) for each physical\n         ingress port\n      -  The traffic rate\
    \ for stateful traffic and the traffic\n         rate/mixture for stateful traffic\
    \ for each physical\n         ingress port\n      -  The shaped egress port shaper\
    \ parameters (QL, SR, Bc, Be)\n      Report Results:\n      -  For each active\
    \ egress port, the achieved throughput rate and\n         shaper metrics for each\
    \ ingress port traffic stream\n      Example:\n      -  Egress Port 1: throughput\
    \ and metrics for ingress streams 1-n\n      -  Egress Port n: throughput and\
    \ metrics for ingress streams 1-n\n"
- title: 6.3.2.2.  All Queues Shaped, Single Port Active
  contents:
  - "6.3.2.2.  All Queues Shaped, Single Port Active\n   Test Summary:\n      The\
    \ second shaper capacity test is conducted with all queues\n      actively shaping\
    \ on a single physical port.  The benchmarking\n      methodology described in\
    \ the per-port shaping test\n      (Section 6.3.2.1) serves as the foundation\
    \ for this.\n      Additionally, each of the SP queues on the egress physical\
    \ port is\n      configured with a shaper.  For the highest-priority queue, the\n\
    \      maximum amount of bandwidth available is limited by the bandwidth\n   \
    \   of the shaper.  For the lower-priority queues, the maximum amount\n      of\
    \ bandwidth available is limited by the bandwidth of the shaper\n      and traffic\
    \ in higher-priority queues.\n      The benchmarking methodologies specified in\
    \ Sections 6.3.1.1\n      (stateless) and 6.3.1.2 (stateful) (procedure, metrics,\
    \ and\n      reporting format) should be applied here.  Since this is a\n    \
    \  capacity test, the configuration and report results format (see\n      Section\
    \ 6.3.1) MUST also include:\n      Configuration:\n      -  The number of physical\
    \ ingress ports active during the test\n      -  The classification marking (DSCP,\
    \ VLAN, etc.) for each physical\n         ingress port\n      -  The traffic rate\
    \ for stateful traffic and the traffic\n         rate/mixture for stateful traffic\
    \ for each physical\n         ingress port\n      -  For the active egress port,\
    \ each of the following shaper queue\n         parameters: QL, SR, Bc, Be\n  \
    \    Report Results:\n      -  For each queue of the active egress port, the achieved\n\
    \         throughput rate and shaper metrics for each ingress port\n         traffic\
    \ stream\n      Example:\n      -  Egress Port High-Priority Queue: throughput\
    \ and metrics for\n         ingress streams 1-n\n      -  Egress Port Lower-Priority\
    \ Queue: throughput and metrics for\n         ingress streams 1-n\n"
- title: 6.3.2.3.  All Queues Shaped, All Ports Active
  contents:
  - "6.3.2.3.  All Queues Shaped, All Ports Active\n   Test Summary:\n      For the\
    \ third shaper capacity test (which is a combination of the\n      tests listed\
    \ in Sections 6.3.2.1 and 6.3.2.2), all queues will be\n      actively shaping\
    \ and all physical ports active.\n      The benchmarking methodologies specified\
    \ in Sections 6.3.1.1\n      (stateless) and 6.3.1.2 (stateful) (procedure, metrics,\
    \ and\n      reporting format) should be applied here.  Since this is a\n    \
    \  capacity test, the configuration and report results format (see\n      Section\
    \ 6.3.1) MUST also include:\n      Configuration:\n      -  The number of physical\
    \ ingress ports active during the test\n      -  The classification marking (DSCP,\
    \ VLAN, etc.) for each physical\n         ingress port\n      -  The traffic rate\
    \ for stateful traffic and the traffic\n         rate/mixture for stateful traffic\
    \ for each physical\n         ingress port\n      -  For each of the active egress\
    \ ports: shaper port parameters and\n         per-queue parameters (QL, SR, Bc,\
    \ Be)\n      Report Results:\n      -  For each queue of each active egress port,\
    \ the achieved\n         throughput rate and shaper metrics for each ingress port\n\
    \         traffic stream\n      Example:\n      -  Egress Port 1, High-Priority\
    \ Queue: throughput and metrics for\n         ingress streams 1-n\n      -  Egress\
    \ Port 1, Lower-Priority Queue: throughput and metrics for\n         ingress streams\
    \ 1-n\n      ...\n      -  Egress Port n, High-Priority Queue: throughput and\
    \ metrics for\n         ingress streams 1-n\n      -  Egress Port n, Lower-Priority\
    \ Queue: throughput and metrics for\n         ingress streams 1-n\n"
- title: 6.4.  Concurrent Capacity Load Tests
  contents:
  - "6.4.  Concurrent Capacity Load Tests\n   As mentioned in Section 3 of this document,\
    \ it is impossible to\n   specify the various permutations of concurrent traffic\
    \ management\n   functions that should be tested in a device for capacity testing.\n\
    \   However, some profiles are listed below that may be useful for\n   testing\
    \ multiple configurations of traffic management functions:\n   -  Policers on\
    \ ingress and queuing on egress\n   -  Policers on ingress and shapers on egress\
    \ (not intended for a flow\n      to be policed and then shaped; these would be\
    \ two different flows\n      tested at the same time)\n   The test procedures\
    \ and reporting formats from Sections 6.1, 6.2,\n   and 6.3 may be modified to\
    \ accommodate the capacity test profile.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   Documents of this type do not directly affect\
    \ the security of the\n   Internet or of corporate networks as long as benchmarking\
    \ is not\n   performed on devices or systems connected to production networks.\n\
    \   Further, benchmarking is performed on a \"black box\" basis, relying\n   solely\
    \ on measurements observable external to the DUT/SUT.\n   Special capabilities\
    \ SHOULD NOT exist in the DUT/SUT specifically for\n   benchmarking purposes.\
    \  Any implications for network security arising\n   from the DUT/SUT SHOULD be\
    \ identical in the lab and in production\n   networks.\n"
- title: 8.  References
  contents:
  - '8.  References

    '
- title: 8.1.  Normative References
  contents:
  - "8.1.  Normative References\n   [3GPP2-C_R1002-A]\n              3rd Generation\
    \ Partnership Project 2, \"cdma2000 Evaluation\n              Methodology\", Version\
    \ 1.0, Revision A, May 2009,\n              <http://www.3gpp2.org/public_html/specs/\n\
    \              C.R1002-A_v1.0_Evaluation_Methodology.pdf>.\n   [RFC1242]  Bradner,\
    \ S., \"Benchmarking Terminology for Network\n              Interconnection Devices\"\
    , RFC 1242, DOI 10.17487/RFC1242,\n              July 1991, <http://www.rfc-editor.org/info/rfc1242>.\n\
    \   [RFC2119]  Bradner, S., \"Key words for use in RFCs to Indicate\n        \
    \      Requirement Levels\", BCP 14, RFC 2119,\n              DOI 10.17487/RFC2119,\
    \ March 1997,\n              <http://www.rfc-editor.org/info/rfc2119>.\n   [RFC2544]\
    \  Bradner, S. and J. McQuaid, \"Benchmarking Methodology for\n              Network\
    \ Interconnect Devices\", RFC 2544,\n              DOI 10.17487/RFC2544, March\
    \ 1999,\n              <http://www.rfc-editor.org/info/rfc2544>.\n   [RFC2680]\
    \  Almes, G., Kalidindi, S., and M. Zekauskas, \"A One-way\n              Packet\
    \ Loss Metric for IPPM\", RFC 2680,\n              DOI 10.17487/RFC2680, September\
    \ 1999,\n              <http://www.rfc-editor.org/info/rfc2680>.\n   [RFC3148]\
    \  Mathis, M. and M. Allman, \"A Framework for Defining\n              Empirical\
    \ Bulk Transfer Capacity Metrics\", RFC 3148,\n              DOI 10.17487/RFC3148,\
    \ July 2001,\n              <http://www.rfc-editor.org/info/rfc3148>.\n   [RFC4115]\
    \  Aboul-Magd, O. and S. Rabie, \"A Differentiated Service\n              Two-Rate,\
    \ Three-Color Marker with Efficient Handling of\n              in-Profile Traffic\"\
    , RFC 4115, DOI 10.17487/RFC4115,\n              July 2005, <http://www.rfc-editor.org/info/rfc4115>.\n\
    \   [RFC4689]  Poretsky, S., Perser, J., Erramilli, S., and S. Khurana,\n    \
    \          \"Terminology for Benchmarking Network-layer Traffic\n            \
    \  Control Mechanisms\", RFC 4689, DOI 10.17487/RFC4689,\n              October\
    \ 2006, <http://www.rfc-editor.org/info/rfc4689>.\n   [RFC4737]  Morton, A., Ciavattone,\
    \ L., Ramachandran, G., Shalunov,\n              S., and J. Perser, \"Packet Reordering\
    \ Metrics\", RFC 4737,\n              DOI 10.17487/RFC4737, November 2006,\n \
    \             <http://www.rfc-editor.org/info/rfc4737>.\n   [RFC5481]  Morton,\
    \ A. and B. Claise, \"Packet Delay Variation\n              Applicability Statement\"\
    , RFC 5481, DOI 10.17487/RFC5481,\n              March 2009, <http://www.rfc-editor.org/info/rfc5481>.\n\
    \   [RFC6349]  Constantine, B., Forget, G., Geib, R., and R. Schrage,\n      \
    \        \"Framework for TCP Throughput Testing\", RFC 6349,\n              DOI\
    \ 10.17487/RFC6349, August 2011,\n              <http://www.rfc-editor.org/info/rfc6349>.\n\
    \   [RFC6703]  Morton, A., Ramachandran, G., and G. Maguluri, \"Reporting\n  \
    \            IP Network Performance Metrics: Different Points of View\",\n   \
    \           RFC 6703, DOI 10.17487/RFC6703, August 2012,\n              <http://www.rfc-editor.org/info/rfc6703>.\n\
    \   [SPECweb2009]\n              Standard Performance Evaluation Corporation (SPEC),\n\
    \              \"SPECweb2009 Release 1.20 Benchmark Design Document\",\n     \
    \         April 2010, <https://www.spec.org/web2009/docs/design/\n           \
    \   SPECweb2009_Design.html>.\n"
- title: 8.2.  Informative References
  contents:
  - "8.2.  Informative References\n   [CA-Benchmark]\n              Hamilton, M. and\
    \ S. Banks, \"Benchmarking Methodology for\n              Content-Aware Network\
    \ Devices\", Work in Progress,\n              draft-ietf-bmwg-ca-bench-meth-04,\
    \ February 2013.\n   [CoDel]    Nichols, K., Jacobson, V., McGregor, A., and J.\
    \ Iyengar,\n              \"Controlled Delay Active Queue Management\", Work in\n\
    \              Progress, draft-ietf-aqm-codel-01, April 2015.\n   [MEF-10.3] Metro\
    \ Ethernet Forum, \"Ethernet Services Attributes\n              Phase 3\", MEF\
    \ 10.3, October 2013,\n              <https://www.mef.net/Assets/Technical_Specifications/\n\
    \              PDF/MEF_10.3.pdf>.\n   [MEF-12.2] Metro Ethernet Forum, \"Carrier\
    \ Ethernet Network\n              Architecture Framework -- Part 2: Ethernet Services\n\
    \              Layer\", MEF 12.2, May 2014,\n              <https://www.mef.net/Assets/Technical_Specifications/\n\
    \              PDF/MEF_12.2.pdf>.\n   [MEF-14]   Metro Ethernet Forum, \"Abstract\
    \ Test Suite for Traffic\n              Management Phase 1\", MEF 14, November\
    \ 2005,\n              <https://www.mef.net/Assets/\n              Technical_Specifications/PDF/MEF_14.pdf>.\n\
    \   [MEF-19]   Metro Ethernet Forum, \"Abstract Test Suite for UNI\n         \
    \     Type 1\", MEF 19, April 2007, <https://www.mef.net/Assets/\n           \
    \   Technical_Specifications/PDF/MEF_19.pdf>.\n   [MEF-26.1] Metro Ethernet Forum,\
    \ \"External Network Network Interface\n              (ENNI) - Phase 2\", MEF\
    \ 26.1, January 2012,\n              <http://www.mef.net/Assets/Technical_Specifications/\n\
    \              PDF/MEF_26.1.pdf>.\n   [MEF-37]   Metro Ethernet Forum, \"Abstract\
    \ Test Suite for ENNI\",\n              MEF 37, January 2012, <https://www.mef.net/Assets/\n\
    \              Technical_Specifications/PDF/MEF_37.pdf>.\n   [PIE]      Pan, R.,\
    \ Natarajan, P., Baker, F., White, G., VerSteeg,\n              B., Prabhu, M.,\
    \ Piglione, C., and V. Subramanian, \"PIE: A\n              Lightweight Control\
    \ Scheme To Address the Bufferbloat\n              Problem\", Work in Progress,\
    \ draft-ietf-aqm-pie-02,\n              August 2015.\n   [RFC2697]  Heinanen,\
    \ J. and R. Guerin, \"A Single Rate Three Color\n              Marker\", RFC 2697,\
    \ DOI 10.17487/RFC2697, September 1999,\n              <http://www.rfc-editor.org/info/rfc2697>.\n\
    \   [RFC2698]  Heinanen, J. and R. Guerin, \"A Two Rate Three Color\n        \
    \      Marker\", RFC 2698, DOI 10.17487/RFC2698, September 1999,\n           \
    \   <http://www.rfc-editor.org/info/rfc2698>.\n   [RFC7567]  Baker, F., Ed., and\
    \ G. Fairhurst, Ed., \"IETF\n              Recommendations Regarding Active Queue\
    \ Management\",\n              BCP 197, RFC 7567, DOI 10.17487/RFC7567, July 2015,\n\
    \              <http://www.rfc-editor.org/info/rfc7567>.\n"
- title: Appendix A.  Open Source Tools for Traffic Management Testing
  contents:
  - "Appendix A.  Open Source Tools for Traffic Management Testing\n   This framework\
    \ specifies that stateless and stateful behaviors SHOULD\n   both be tested. \
    \ Some open source tools that can be used to\n   accomplish many of the tests\
    \ proposed in this framework are iperf,\n   netperf (with netperf-wrapper), the\
    \ \"uperf\" tool, Tmix,\n   TCP-incast-generator, and D-ITG (Distributed Internet\
    \ Traffic\n   Generator).\n   iperf can generate UDP-based or TCP-based traffic;\
    \ a client and\n   server must both run the iperf software in the same traffic\
    \ mode.\n   The server is set up to listen, and then the test traffic is\n   controlled\
    \ from the client.  Both unidirectional and bidirectional\n   concurrent testing\
    \ are supported.\n   The UDP mode can be used for the stateless traffic testing.\
    \  The\n   target bandwidth, packet size, UDP port, and test duration can be\n\
    \   controlled.  A report of bytes transmitted, packets lost, and delay\n   variation\
    \ is provided by the iperf receiver.\n   iperf (TCP mode), TCP-incast-generator,\
    \ and D-ITG can be used for\n   stateful traffic testing to test bulk transfer\
    \ traffic.  The TCP\n   window size (which is actually the SSB), number of connections,\n\
    \   packet size, TCP port, and test duration can be controlled.  A report\n  \
    \ of bytes transmitted and throughput achieved is provided by the iperf\n   sender,\
    \ while TCP-incast-generator and D-ITG provide even more\n   statistics.\n   netperf\
    \ is a software application that provides network bandwidth\n   testing between\
    \ two hosts on a network.  It supports UNIX domain\n   sockets, TCP, SCTP, and\
    \ UDP via BSD Sockets.  netperf provides a\n   number of predefined tests, e.g.,\
    \ to measure bulk (unidirectional)\n   data transfer or request/response performance\n\
    \   (http://en.wikipedia.org/wiki/Netperf).  netperf-wrapper is a Python\n   script\
    \ that runs multiple simultaneous netperf instances and\n   aggregates the results.\n\
    \   uperf uses a description (or model) of an application mixture.  It\n   generates\
    \ the load according to the model descriptor.  uperf is more\n   flexible than\
    \ netperf in its ability to generate request/response\n   application behavior\
    \ within a single TCP connection.  The application\n   model descriptor can be\
    \ based on empirical data, but at the time of\n   this writing, the import of\
    \ packet captures is not directly\n   supported.\n   Tmix is another application\
    \ traffic emulation tool.  It uses packet\n   captures directly to create the\
    \ traffic profile.  The packet trace is\n   \"reverse compiled\" into a source-level\
    \ characterization, called a\n   \"connection vector\", of each TCP connection\
    \ present in the trace.\n   While most widely used in ns2 simulation environments,\
    \ Tmix also runs\n   on Linux hosts.\n   The traffic generation capabilities of\
    \ these open source tools\n   facilitate the emulation of the TCP test patterns\
    \ discussed in\n   Appendix B.\n"
- title: Appendix B.  Stateful TCP Test Patterns
  contents:
  - "Appendix B.  Stateful TCP Test Patterns\n   This framework recommends at a minimum\
    \ the following TCP test\n   patterns, since they are representative of real-world\
    \ application\n   traffic (Section 5.2.1 describes some methods to derive other\n\
    \   application-based TCP test patterns).\n   -  Bulk Transfer: Generate concurrent\
    \ TCP connections whose aggregate\n      number of in-flight data bytes would\
    \ fill the BDP.  Guidelines\n      from [RFC6349] are used to create this TCP\
    \ traffic pattern.\n   -  Micro Burst: Generate precise burst patterns within\
    \ a single TCP\n      connection or multiple TCP connections.  The idea is for\
    \ TCP to\n      establish equilibrium and then burst application bytes at defined\n\
    \      sizes.  The test tool must allow the burst size and burst time\n      interval\
    \ to be configurable.\n   -  Web Site Patterns: The HTTP traffic model shown in\
    \ Table 4.1.3-1\n      of [3GPP2-C_R1002-A] demonstrates a way to develop these\
    \ TCP test\n      patterns.  In summary, the HTTP traffic model consists of the\n\
    \      following parameters:\n      -  Main object size (Sm)\n      -  Embedded\
    \ object size (Se)\n      -  Number of embedded objects per page (Nd)\n      -\
    \  Client processing time (Tcp)\n      -  Server processing time (Tsp)\n   Web\
    \ site test patterns are illustrated with the following examples:\n   -  Simple\
    \ web site: Mimic the request/response and object download\n      behavior of\
    \ a basic web site (small company).\n   -  Complex web site: Mimic the request/response\
    \ and object download\n      behavior of a complex web site (eCommerce site).\n\
    \   Referencing the HTTP traffic model parameters, the following table\n   was\
    \ derived (by analysis and experimentation) for simple web site and\n   complex\
    \ web site TCP test patterns:\n                             Simple         Complex\n\
    \    Parameter                Web Site       Web Site\n    -----------------------------------------------------\n\
    \    Main object              Ave. = 10KB    Ave. = 300KB\n     size (Sm)    \
    \           Min. = 100B    Min. = 50KB\n                             Max. = 500KB\
    \   Max. = 2MB\n    Embedded object          Ave. = 7KB     Ave. = 10KB\n    \
    \ size (Se)               Min. = 50B     Min. = 100B\n                       \
    \      Max. = 350KB   Max. = 1MB\n    Number of embedded       Ave. = 5      \
    \ Ave. = 25\n     objects per page (Nd)   Min. = 2       Min. = 10\n         \
    \                    Max. = 10      Max. = 50\n    Client processing        Ave.\
    \ = 3s      Ave. = 10s\n     time (Tcp)*             Min. = 1s      Min. = 3s\n\
    \                             Max. = 10s     Max. = 30s\n    Server processing\
    \        Ave. = 5s      Ave. = 8s\n     time (Tsp)*             Min. = 1s    \
    \  Min. = 2s\n                             Max. = 15s     Max. = 30s\n   *  The\
    \ client and server processing time is distributed across the\n      transmission/receipt\
    \ of all of the main and embedded objects.\n   To be clear, the parameters in\
    \ this table are reasonable guidelines\n   for the TCP test pattern traffic generation.\
    \  The test tool can use\n   fixed parameters for simpler tests and mathematical\
    \ distributions for\n   more complex tests.  However, the test pattern must be\
    \ repeatable to\n   ensure that the benchmark results can be reliably compared.\n\
    \   -  Interactive Patterns: While web site patterns are interactive to a\n  \
    \    degree, they mainly emulate the downloading of web sites of\n      varying\
    \ complexity.  Interactive patterns are more chatty in\n      nature, since there\
    \ is a lot of user interaction with the servers.\n      Examples include business\
    \ applications such as PeopleSoft and\n      Oracle, and consumer applications\
    \ such as Facebook and IM.  For\n      the interactive patterns, the packet capture\
    \ technique was used to\n      characterize some business applications and also\
    \ the email\n      application.\n   In summary, an interactive application can\
    \ be described by the\n   following parameters:\n   -  Client message size (Scm)\n\
    \   -  Number of client messages (Nc)\n   -  Server response size (Srs)\n   -\
    \  Number of server messages (Ns)\n   -  Client processing time (Tcp)\n   -  Server\
    \ processing time (Tsp)\n   -  File size upload (Su)*\n   -  File size download\
    \ (Sd)*\n   *  The file size parameters account for attachments uploaded or\n\
    \      downloaded and may not be present in all interactive applications.\n  \
    \ Again using packet capture as a means to characterize, the following\n   table\
    \ reflects the guidelines for simple business applications,\n   complex business\
    \ applications, eCommerce, and email Send/Receive:\n                     Simple\
    \       Complex\n                     Business     Business\n   Parameter    \
    \     Application  Application  eCommerce*   Email\n   --------------------------------------------------------------------\n\
    \   Client message    Ave. = 450B  Ave. = 2KB   Ave. = 1KB   Ave. = 200B\n   \
    \ size (Scm)       Min. = 100B  Min. = 500B  Min. = 100B  Min. = 100B\n      \
    \               Max. = 1.5KB Max. = 100KB Max. = 50KB  Max. = 1KB\n   Number of\
    \ client  Ave. = 10    Ave. = 100   Ave. = 20    Ave. = 10\n    messages (Nc)\
    \    Min. = 5     Min. = 50    Min. = 10    Min. = 5\n                     Max.\
    \ = 25    Max. = 250   Max. = 100   Max. = 25\n   Client processing Ave. = 10s\
    \   Ave. = 30s   Ave. = 15s   Ave. = 5s\n    time (Tcp)**     Min. = 3s    Min.\
    \ = 3s    Min. = 5s    Min. = 3s\n                     Max. = 30s   Max. = 60s\
    \   Max. = 120s  Max. = 45s\n   Server response   Ave. = 2KB   Ave. = 5KB   Ave.\
    \ = 8KB   Ave. = 200B\n    size (Srs)       Min. = 500B  Min. = 1KB   Min. = 100B\
    \  Min. = 150B\n                     Max. = 100KB Max. = 1MB   Max. = 50KB  Max.\
    \ = 750B\n   Number of server  Ave. = 50    Ave. = 200   Ave. = 100   Ave. = 15\n\
    \    messages (Ns)    Min. = 10    Min. = 25    Min. = 15    Min. = 5\n      \
    \               Max. = 200   Max. = 1000  Max. = 500   Max. = 40\n   Server processing\
    \ Ave. = 0.5s  Ave. = 1s    Ave. = 2s    Ave. = 4s\n    time (Tsp)**     Min.\
    \ = 0.1s  Min. = 0.5s  Min. = 1s    Min. = 0.5s\n                     Max. = 5s\
    \    Max. = 20s   Max. = 10s   Max. = 15s\n   File size         Ave. = 50KB  Ave.\
    \ = 100KB Ave. = N/A   Ave. = 100KB\n    upload (Su)      Min. = 2KB   Min. =\
    \ 10KB  Min. = N/A   Min. = 20KB\n                     Max. = 200KB Max. = 2MB\
    \   Max. = N/A   Max. = 10MB\n   File size         Ave. = 50KB  Ave. = 100KB Ave.\
    \ = N/A   Ave. = 100KB\n    download (Sd)    Min. = 2KB   Min. = 10KB  Min. =\
    \ N/A   Min. = 20KB\n                     Max. = 200KB Max. = 2MB   Max. = N/A\
    \   Max. = 10MB\n   *  eCommerce used a combination of packet capture techniques\
    \ and\n      reference traffic flows as described in [SPECweb2009].\n   ** The\
    \ client and server processing time is distributed across the\n      transmission/receipt\
    \ of all of the messages.  The client\n      processing time consists mainly of\
    \ the delay between user\n      interactions (not machine processing).\n   Again,\
    \ the parameters in this table are the guidelines for the TCP\n   test pattern\
    \ traffic generation.  The test tool can use fixed\n   parameters for simpler\
    \ tests and mathematical distributions for more\n   complex tests.  However, the\
    \ test pattern must be repeatable to\n   ensure that the benchmark results can\
    \ be reliably compared.\n   -  SMB/CIFS file copy: Mimic a network file copy,\
    \ both read and\n      write.  As opposed to FTP, which is a bulk transfer and\
    \ is only\n      flow-controlled via TCP, SMB/CIFS divides a file into application\n\
    \      blocks and utilizes application-level handshaking in addition to\n    \
    \  TCP flow control.\n   In summary, an SMB/CIFS file copy can be described by\
    \ the following\n   parameters:\n   -  Client message size (Scm)\n   -  Number\
    \ of client messages (Nc)\n   -  Server response size (Srs)\n   -  Number of server\
    \ messages (Ns)\n   -  Client processing time (Tcp)\n   -  Server processing time\
    \ (Tsp)\n   -  Block size (Sb)\n   The client and server messages are SMB control\
    \ messages.  The block\n   size is the data portion of the file transfer.\n  \
    \ Again using packet capture as a means to characterize, the following\n   table\
    \ reflects the guidelines for SMB/CIFS file copy:\n                          SMB/CIFS\n\
    \      Parameter           File Copy\n      --------------------------------\n\
    \      Client message      Ave. = 450B\n       size (Scm)         Min. = 100B\n\
    \                          Max. = 1.5KB\n      Number of client    Ave. = 10\n\
    \       messages (Nc)      Min. = 5\n                          Max. = 25\n   \
    \   Client processing   Ave. = 1ms\n       time (Tcp)         Min. = 0.5ms\n \
    \                         Max. = 2\n      Server response     Ave. = 2KB\n   \
    \    size (Srs)         Min. = 500B\n                          Max. = 100KB\n\
    \      Number of server    Ave. = 10\n       messages (Ns)      Min. = 10\n  \
    \                        Max. = 200\n      Server processing   Ave. = 1ms\n  \
    \     time (Tsp)         Min. = 0.5ms\n                          Max. = 2ms\n\
    \      Block               Ave. = N/A\n       size (Sb)*         Min. = 16KB\n\
    \                          Max. = 128KB\n      *  Depending upon the tested file\
    \ size, the block size will be\n         transferred \"n\" number of times to\
    \ complete the example.  An\n         example would be a 10 MB file test and 64\
    \ KB block size.  In\n         this case, 160 blocks would be transferred after\
    \ the control\n         channel is opened between the client and server.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   We would like to thank Al Morton for his continuous review\
    \ and\n   invaluable input to this document.  We would also like to thank Scott\n\
    \   Bradner for providing guidance early in this document's conception,\n   in\
    \ the area of the benchmarking scope of traffic management\n   functions.  Additionally,\
    \ we would like to thank Tim Copley for his\n   original input, as well as David\
    \ Taht, Gory Erg, and Toke\n   Hoiland-Jorgensen for their review and input for\
    \ the AQM group.\n   Also, for the formal reviews of this document, we would like\
    \ to thank\n   Gilles Forget, Vijay Gurbani, Reinhard Schrage, and Bhuvaneswaran\n\
    \   Vengainathan.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Barry Constantine\n   JDSU, Test and Measurement Division\n\
    \   Germantown, MD  20876-7100\n   United States\n   Phone: +1-240-404-2227\n\
    \   Email: barry.constantine@jdsu.com\n   Ram (Ramki) Krishnan\n   Dell Inc.\n\
    \   Santa Clara, CA  95054\n   United States\n   Phone: +1-408-406-7890\n   Email:\
    \ ramkri123@gmail.com\n"
