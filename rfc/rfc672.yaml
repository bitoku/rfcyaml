- title: __initial_text__
  contents:
  - "                     A Multi-Site Data Collection Facility\n        Preface:\n\
    \        This RFC reproduces most of a working document\n        prepared during\
    \ the design and implementation of the\n        protocols for the TIP-TENEX integrated\
    \ system for\n        handling TIP accounting. Bernie Cosell (BBN-TIP)\n     \
    \   and Bob Thomas (BBN-TENEX) have contributed to\n        various aspects of\
    \ this work. The system has been\n        partially operational for about a month\
    \ on selected\n        hosts. We feel that the techniques described here\n   \
    \     have wide applicability beyond TIP accounting.\fSection I\nProtocols for\
    \ a Multi-site Data Collection Facility\nIntroduction\n     The development of\
    \ computer networks has provided the\ngroundwork for distributed computation:\
    \ one in which a job or task\nis comprised of components from various computer\
    \ systems. In a\nsingle computer system, the unavailability or malfunction of\
    \ any of\nthe job components (e.g. program, file, device, etc.) usually\nnecessitates\
    \ job termination. With computer networks, it becomes\nfeasible to duplicate certain\
    \ job components which previously had no\nbasis for duplication. (In a single\
    \ system, it does not matter how\nmany times a process that performs a certain\
    \ function is duplicated;\na system crash makes all unavailable). It is such resource\n\
    duplication that enables us to utilize the network to achieve high\nreliability\
    \ and load leveling. In order to realize the potential of\nresource duplication,\
    \ it is necessary to have protocols which\nprovide for the orderly use of these\
    \ resources. In this document,\nwe first discuss in general terms a problem of\
    \ protocol definition\nfor interacting with a multiply defined resource (server).\
    \ The\nproblem deals with providing a highly reliable data collection\nfacility,\
    \ by supporting it at many sites throughout the network. In\nthe second section\
    \ of this document, we describe in detail a\nparticular implementation of the\
    \ protocol which handles the problem\nof utilizing multiple data collector processes\
    \ for collecting\naccounting data generated by the network TIPs. This example\
    \ also\nillustrates the specialization of hosts to perform parts of a\ncomputation\
    \ they are best equipped to handle. The large network\nhosts (TENEX systems) perform\
    \ the accounting function for the small\nnetwork access TiPs.\n     The situation\
    \ to be discussed is the following: a data\ngenerating process needs to use a\
    \ data collection service which is\nduplicately provided by processes on a number\
    \ of network machines.\nA request to a server involves sending the data to be\
    \ collected.\nAn Initial Approach\n     The data generator could proceed by selecting\
    \ a particular\nserver and sending its request to that server. It might also take\n\
    the attitude that if the message reaches the destination host (the\ncommunication\
    \ subsystem will indicate this) the message will be\nproperly processed to completion.\
    \ Failure of the request Message\nwould then lead to selecting another server,\
    \ until the request\nsucceeds or all servers have been tried.\n     Such a simple\
    \ strategy is a poor one. It makes sense to\nrequire that the servicing process\
    \ send a positive acknowledgement\nto the requesting process. If nothing else,\
    \ the reply indicates\nthat the server process itself is still functioning. Waiting\
    \ for\nsuch a reply also implies that there is a strategy for selecting\nanother\
    \ server if the reply is not forthcoming. Herein lies a\nproblem. If the expected\
    \ reply is timed out, and then a new request\nis sent to another server, we run\
    \ the risk of receiving the\n(delayed) original acknowledgement at a later time.\
    \ This could\nresult in having the data entered into the collection system twice\n\
    (data duplication). If the request is re-transmitted to the same\nserver only,\
    \ we face the possibility of not being able to access a\ncollector (data loss).\
    \ In addition, for load leveling purposes, we\nmay wish to send new requests to\
    \ some (or all) servers. We can then\nuse their reply (or lack of reply) as an\
    \ indicator of load on that\nparticular instance of the service. Doing this without\
    \ data\nduplication requires more than a simple request and acknowledgement\n\
    protocol*.\nExtension of the Protocol\n     The general protocol developed to\
    \ handle multiple collection\nservers involves having the data generator send\
    \ the data request to\nsome (or all) data collectors. Those willing to handle\
    \ the request\nreply with an \"I've got it\" message. They then await further\n\
    notification before finalizing the processing of the data. The data\ngenerator\
    \ sends a \"go ahead\" message to one of the replying\ncollectors, and a \"discard\"\
    \ message to all other replying\ncollectors. The \"go ahead\" message is the signal\
    \ to process the\ndata (i.e. collect permanently), while the \"discard\" message\n\
    indicates that the data is being collected elsewhere and should not\nbe retained.\n\
    \     The question now arises as to whether or not the collector\nprocess should\
    \ acknowledge receipt of the \"go ahead\" message with a\nreply of its own, and\
    \ then should the generator process acknowledge\nthis acknowledgement, etc. We\
    \ would like to send as few messages as\npossible to achieve reliable communication.\
    \ Therefore, when a state\n--------------------\n* If the servers are independent\
    \ of each other to the extent that if\ntwo or more servers all act on the same\
    \ request, the end result is\nthe same as having a single server act on the request,\
    \ then a simple\nrequest/acknowledgement protocol is adequate. Such may be the\
    \ case,\nfor example, if we subject the totality of collected data (i.e. all\n\
    data collected by all collectors for a certain period) to a\nduplicate detection\
    \ scan. If we could store enough context in each\nentry to be able to determine\
    \ duplicates, then having two or more\nservers act on the data would be functionally\
    \ equivalent to\nprocessing by a single server.\nis reached for which further\
    \ acknowledgements lead to a previously\nvisited state, or when the cost of further\
    \ acknowledgements outweigh\nthe increase in reliability they bring, further acknowledgements\n\
    become unnecessary.\n     The initial question was should the collector process\n\
    acknowledge the \"go ahead\" message? Assume for the moment that it\nshould not\
    \ send such an acknowledgement. The data generator could\nverify, through the\
    \ communication subsystem, the transmission of the\n\"go ahead\" message to the\
    \ host of the collector. If this message\ndid not arrive correctly, the generator\
    \ has the option of\nre-transmitting it or sending a \"go ahead\" to another collector\n\
    which has acknowledged receipt of the data. Either strategy\ninvolves no risk\
    \ of duplication. If the \"go ahead\" message arrives\ncorrectly, and a collector\
    \ acknowledgement to the \"go ahead\" message\nis not required, then we incur\
    \ a vulnerability to (collector host)\nsystem crash from the time the \"go ahead\"\
    \ message is accepted by the\nhost until the time the data is totally processed.\
    \ Call the data\nprocessing time P. Once the data generator has selected a\nparticular\
    \ collector (on the basis of receiving its \"I've got it\"\nmessage), we also\
    \ incur a vulnerability to malfunction of this\ncollector process. The vulnerable\
    \ period is from the time the\ncollector sends its \"i've got it\" message until\
    \ the time the data is\nprocessed. This amounts to two network transit times (2N)\
    \ plus IMP\nand host overhead for message delivery (0) plus data processing time\n\
    (P). [Total time=2N+P+O]. A malfunction (crash) in this period can\ncause the\
    \ loss of data. There is no potential for duplication.\n     Now, assume that\
    \ the data collector process must acknowledge\nthe \"go ahead\" message. The question\
    \ then arises as to when such an\nacknowledgement should be sent. The reasonable\
    \ choices are either\nimmediately before final processing of the data (i.c. before\
    \ the\ndata is permanently recorded) or immediately after final processing.\n\
    It can be argued that unless another acknowledgement is required (by\nthe generator\
    \ to the collector) to this acknowledgement BEFORE the\nactual data update, then\
    \ the best time for the collector to\nacknowledge the \"go ahead\" is after final\
    \ processing. This is so\nbecause receiving the acknowledgement conveys more information\
    \ if it\nis sent after processing, while not receiving it (timeout), in\neither\
    \ case, leaves us in an unknown state with respect to the data\nupdate. Depending\
    \ on the relative speeds of various network and\nsystem components, the data may\
    \ or may not be permanently entered.\nTherefore if we interpret the timeout as\
    \ a signal to have the data\nprocessed at another site, we run the risk of duplication\
    \ of data.\nTo avoid data duplication, the timeout strategy must only involve\n\
    re-sending the \"go ahead\" message to the same collector. This will\nonly help\
    \ if the lack of reply is due to a lost network message.\nOur vulnerability intervals\
    \ to system and process malfunction remain\nas before.\n     It is our conjecture\
    \ (to be analyzed further) that any further\nacknowledgements to these acknowledgements\
    \ will have virtually no\neffect on reducing the period of vulnerability outlined\
    \ above. As\nsuch, the protocol with the fewest messages required is superior.\n\
    Data Dependent Aspects of the Protocol\n     As discussed above, a main issue\
    \ is which process should be the\nlast to respond (send an acknowledgement). If\
    \ the data generator\nsends the last message (i.e. \"go ahead\"), we can only\
    \ check on its\ncorrect arrival at the destination host. We must \"take on faith\"\
    \nthe ability of the collector to correctly complete the transaction.\nThis strategy\
    \ is geared toward avoiding data duplication. If on the\nother hand, the protocol\
    \ specifies that the collector is to send the\nlast message, with the timeout\
    \ of such a message causing the data\ngenerator to use another collector, then\
    \ the protocol is geared\ntoward the best efforts of recording the data somewhere,\
    \ at the\nexpense of possible duplication.\n     Thus, the nature of the problem\
    \ will dictate which of the\nprotocols is appropriate for a given situation. The\
    \ next section\ndeals in the specifics of an implement;tion of a data collection\n\
    protocol to handle the problem of collecting TIP accounting data by\nusing the\
    \ TENEX systems for running the collection server processes.\nIt is shown how\
    \ the general protocol is optimized for the accounting\ndata collection.\nSection\
    \ II\nProtocol for TIP-TENEX Accounting Server Information Exchange\nOverview\
    \ of the Facility\n     When a user initially requests service from a TIP, the\
    \ TIP will\nperform a broadcast ICP to find an available RSEXEC which maintains\n\
    an authentication data base. The user must then complete s login\nsequence in\
    \ order to authenticate himself. If he is successful the\nRSEXEC will transmit\
    \ his unique ID code to the TIP. Failure will\ncause the RSEXEC to close the connection\
    \ and the TIP to hang up on\nthe user. After the user is authenticated, the TIP\
    \ will accumulate\naccounting data for the user session. The data includes a count\
    \ of\nmessages sent on behalf of the user, and the connect time for the\nuser.\
    \ From time to time the TIP will transmit intermediate\naccounting data to Accounting\
    \ Server (ACTSER) processes scattered\nthroughout the network. These accounting\
    \ servers will maintain\nfiles containing intermediate raw accounting data. The\
    \ raw\naccounting data will periodically be collected and sorted to produce\n\
    an accounting data base. Providing a number of accounting servers\nreduces the\
    \ possibility of being unable to find a repository for the\nintermediate data,\
    \ which otherwise would be lost due to buffering\nlimitations in the TiPs. The\
    \ multitude of accounting servers can\nalso serve to reduce the load on the individual\
    \ hosts providing this\nfacility.\nThe rest of this document details the protocol\
    \ that has been\ndeveloped to ensure delivery of TIP accounting data to one of\
    \ the\navailable accounting servers for storage in the intermediate\naccounting\
    \ files.\nAdapting the Protocol\nThe TIP to Accounting Server data exchange uses\
    \ a protocol that\nallows the TIP to select for data transmission one, some, or\
    \ all\nserver hosts either sequentially or in parallel, yet insures that\nthe\
    \ data that becomes part of the accounting file does not contain\nduplicate information.\
    \ The protocol also minimizes the amount of\ndata buffering that must be done\
    \ by the limited capacity TiPs. The\nprotocol is applicable to a wide class of\
    \ data collection problems\nwhich use a number of data generators and collectors.\
    \ The following\ndescribes how the protocol works for TIP accounting.\nEach TIP\
    \ is responsible for maintaining in its memory the cells\nindicating the connect\
    \ time and the number of messages sent for each\nof its current users. These cells\
    \ are incremented by the TIP for\nevery quantum of connect time and message sent,\
    \ as the case may be.\nThis is the data generation phase. Periodically, the TIP\
    \ will scan\nall its active counters, and along with each user ID code, pack the\n\
    accumulated data into one network message (i.e. less than 8K bits).\nThe TIP then\
    \ transmits this data to a set of Accounting Server\nprocesses residing throughout\
    \ the network. The data transfer is\nover a specially designated host-host link.\
    \ The accounting servers\nutilize the raw network message facility of TENEX 1.32\
    \ in order to\ndirectly access that link. When an ACTSER receives a data message\n\
    from a TIP, it buffers the data and replies by returning the entire\nmessage to\
    \ the originating TIP. The TIP responds with a positive\nacknowledgement (\"go\
    \ ahead\") to the first ACTSER which returns the\ndata, and responds with a negative\
    \ acknowledgement (\"discard\") to\nall subsequent ACTSER data return messages\
    \ for this series of\ntransfers. If the TIP does not receive a reply from any\
    \ ACTSER, it\naccumulates new data (i.e. the TIP has all the while been\nincrementing\
    \ its local counters to reflect the increased connect\ntime and message count;\
    \ the current values will comprise new data\ntransfers) and sends the new data\
    \ to the Accounting Server\nprocesses. When an ACTSER receives a positive acknowledgement\
    \ from\na TIP (i.e. \"go ahead\"), it appends the appropriate parts of the\nbuffered\
    \ data to the locally maintained accounting information file.\nOn receiving a\
    \ negative acknowledgement from the TIP (i.e.\n\"discard\"), the ACTSER discards\
    \ the data buffered for this TIP. In\n-addition, when the TIP responds with a\
    \ \"go ahead\" to the first\nACTSER which has accepted the data (acknowledged\
    \ by returning the\ndata along with the \"I've got it\"), the TIP decrements the\
    \ connect\ntime and message counters for each user by the amount indicated in\n\
    the data returned by the ACTSER. This data will already be\naccounted for in the\
    \ intermediate accounting files.\nAs an aid in determining which ACTSER replies\
    \ are to current\nrequests, and which are tardy replies to old requests, the TIP\n\
    maintains a sequence number indicator, and appends this number to\neach data message\
    \ sent to an ACTSER. On receiving a reply from an\nACTSER, the TIP merely checks\
    \ the returned sequence number to see if\nthis is the first reply to the current\
    \ set of TIP requests. If the\nreturned sequence number is the same as the current\
    \ sequence number,\nthen this is the first reply; a positive acknowledgement is\
    \ sent\noff, the counters are decremented by the returned data, and the\nsequence\
    \ number is incremented. If the returned sequence number is\nnot the same as the\
    \ current one (i.e. not the one we are now\nseeking a reply for) then a negative\
    \ acknowledgement is sent to the\nreplying ACTSER. After a positive acknowledgement\
    \ to an ACTSER (and\nthe implied incrementing of the sequence number), the TIP\
    \ can wait\nfor more information to accumulate, and then start transmitting\n\
    again using the new sequence number.\nFurther Clarification of the Protocol\n\
    There are a number of points concerning the protocol that\nshould be noted.\n\
    1. The data generator (TIP) can send different (i.e. updated\nversions) data to\
    \ different data collectors (accounting servers) as\npart of the same logical\
    \ transmission sequence. This is possible\nbecause the TIP does not account for\
    \ the data sent until it receives\nthe acknowledgement of the data echo. This\
    \ strategy relieves the\nTIP of any buffering in conjunction with re-transmission\
    \ of data\nwhich hasn't been acknowledged.\n2. A new data request to an accounting\
    \ server from a TIP will\nalso serve as a negative acknowledgement concerning\
    \ any data already\nbuffered by the ACTSER for that TIP, but not yet acknowledged.\
    \ The\nold data will be discarded, and the new data will be buffered and\nechoed\
    \ as an acknowledgement. This allows the TIP the option of not\nsending a negative\
    \ acknowledgement when it is not convenient to do\nso, without having to remember\
    \ that it must be sent at a later time.\nThere is one exception to this convention.\
    \ If the new data message\nhas the same sequence number as the old buffered message,\
    \ then the\nnew data must be discarded, and the old data kept and re-echoed.\n\
    This is to prevent a slow acknowledgement to the old data from being\naccepted\
    \ by the TIP, after the TIP has already sent the new data to\nthe slow host. This\
    \ caveat can be avoided if the TIP does not\nresend to a non-responding server\
    \ within the time period that a\nmessage could possibly be stuck in the network,\
    \ but could still be\ndelivered. Ignoring this situation may result in some accounting\n\
    data being counted twice. Because of the rule to keep old data when\nconfronted\
    \ with matching sequence numbers, on restarting after a\ncrash, the TIP should\
    \ send a \"discard\" message to all servers in\norder to clear any data which\
    \ has been buffered for it prior to the\ncrash. An alternative to this would be\
    \ for the TIP to initialize\nits sequence number from a varying source such as\
    \ time of day.\n3. The accounting server similarly need not acknowledge receipt\n\
    of data (by echoing) if it finds itself otherwise occupied. This\nwill mean that\
    \ the ACTSER is not buffering the data, and hence is\nnot a candidate for entering\
    \ the data into the file. However, the\nTIP may try this ACTSER at a later time\
    \ (even with the same data),\nwith no ill effects.\n4. Because of 2 and 3 above,\
    \ the protocol is robust with respect\nto lost or garbled transmissions of TIP\
    \ data requests and accounting\nserver echo replies. That is, in the event of\
    \ loss of such a\nmessage, a re-transmission will occur as the normal procedure.\n\
    5. There is no synchronization problem with respect to the\nsequence number used\
    \ for duplicate detection, since this number is\nmaintained only at the TIP site.\
    \ The accounting server merely\nechoes the sequence number it has received as\
    \ part of the data.\n6. There are, however, some constraints on the size of the\n\
    sequence number field. It must be large enough so that ALL traces\nof the previous\
    \ use of a given sequence number are totally reMoved\nfrom the network before\
    \ the number is re-used by the TIP. The\nsequence number is modulo the size of\
    \ the largest number represented\nby the number of bits allocated, and is cyclic.\
    \ Problems generally\narise when a host proceeds from a service interruption while\
    \ it was\nholding on to a reply. If during the service interruption, we have\n\
    cycled through our sequence numbers exactly N times (where N is any\ninteger),\
    \ this VERY tardy reply could be mistaken for a reply to the\nnew data, which\
    \ has the same sequence number (i.e. N revolutions of\nsequence numbers later).\
    \ By utilizing a sufficiently large sequence\nnumber field (16 bits), and by allowing\
    \ sufficient time between\ninstances of sending new data, we can effectively reduce\
    \ the\nprobability of such an error to zero.\n7. Since the data involved in this\
    \ problem is the source of\naccounting information, care must be taken to avoid\
    \ duplicate\nentries. This must be done at the expense of potentially losing\n\
    data in certain instances. Other than the obvious TIP malfunction,\nthere are\
    \ two known ways of losing data. One is the situation where\nno accounting server\
    \ responds to a TIP for an extended period of\ntime causing the TIP counters to\
    \ overflow (highly unlikely if there\nare sufficient Accounting Servers). In this\
    \ case, the TIP can hold\nthe counters at their maximum value until a server comes\
    \ up, thereby\nkeeping the lost accounting data at its minimum. The other\nsituation\
    \ results from adapting the protocol to our insistence on no\nduplicate data in\
    \ the incremental files. We are vulnerable to data\nloss with no recourse from\
    \ the time the server receives the \"go\nahead\" to update the file with the buffered\
    \ data (i.e. positive\nacknowledgement) until the time the update is completed\
    \ and the file\nis closed. An accounting server crash during this period will\
    \ cause\nthat accounting data to be lost. In our initial implementation, we\n\
    have slightly extended this period of vulnerability in order to save\nthe TIP\
    \ from having to buffer the acknowledged data for a short\nperiod of time. By\
    \ updating TIP counters from the returned data in\nparallel with sending the \"\
    go ahead\" acknowledgement, we relieve the\nTIP of the burden of buffering this\
    \ data until the Request for Next\nMessage (RFNM) from the accounting server IMP\
    \ is received. This\nadds slightly to our period of vulnerability to malfunction,\
    \ moving\nthe beginning of the period from the point when the ACTSER host\nreceives\
    \ the \"go ahead\", back to the point when the TIP sends off\nthe \"go ahead\"\
    \ (i.e. a period of one network transit time plus some\nIMP processing time).\
    \ However, loss of data in this period is\ndetectable through the Host Dead or\
    \ Incomplete Transmission return\nin place of the RFNM. We intend to record such\
    \ occurrences with the\nNetwork Control Center. If this data loss becomes intolerable,\
    \ the\nTIP program will be modified to await the RFNM for the positive\nacknowledgement\
    \ before updating its counters. In such a case, if\nthe RFNM does not come, the\
    \ TIP can discard the buffered data and\nre-transmit new data to other servers.\n\
    8. There is adequate protection against the entry of forged data\ninto the intermediate\
    \ accounting files. This is primarily due to\nthe system enforced limited access\
    \ to Host-Imp messages and\nHost-Host links. In addition, messages received on\
    \ such designated\nlimited access links can be easily verified as coming from\
    \ a TIP.\nThe IMP subnet appends the signature (address) of the sending host\n\
    to all of its messages, so there can be no forging. The Accounting\nServer is\
    \ in a position to check if the source of the message is in\nfact a TIP data generator.\n\
    Current Parameters of the Protocol\nIn the initial implementation, the TIP sends\
    \ its accumulated\naccounting data about once every half hour. If it gets no positive\n\
    acknowledgement, it tries to send with greater frequency (about\nevery 5 minutes)\
    \ until it finally succeeds. It can then return to\nthe normal waiting period.\
    \ (A TIP user logout introduces an\nexception to this behavior. In order to re-use\
    \ the TIP port and its\nassociated counters as soon as possible, a user terminating\
    \ his TIP\nsession causes the accounting data to be sent immediately).\ninitially,\
    \ our implementation calls for each TIP to remember a\n\"favored\" accounting\
    \ server. At the wait period expiration, the TIP\nwill try to deposit the data\
    \ at its \"favored\" site. If successful\nwithin a short timeout period, this\
    \ site remains the favored site,\nand the wait interval is reset. If unsuccessful\
    \ within the short\ntimeout, the data can be sent to all servers*. The one replying\n\
    first will update its file with the data and also become the\n\"favored\" server\
    \ for this TIP. With these parameters, a host would\nhave to undergo a proceedable\
    \ service interruption of more than a\nyear in order for the potential sequence\
    \ number problem outlined in\n(6) above to occur.\nConcluding Remarks\nWhen the\
    \ implementation is complete, we will have a general\ndata accumulation and collection\
    \ system which can be used to gather\na wide variety of information. The protocol\
    \ as outlined is geared\nto gathering data which is either independent of the\
    \ previously\naccumulated data items (e.g. recording names), or data which\nadheres\
    \ to a commutative relationship (e.g. counting). This is a\nconsequence of the\
    \ policy of retransmission of different versions of\nthe data to different potential\
    \ collectors (to relieve TIP buffering\nproblems).\nIn the specified version of\
    \ the protocol, care was taken to\navoid duplicate data entries, at the cost of\
    \ possibly losing some\ndata through collector malfunction. Data collection problems\
    \ which\nrequire avoiding such loss (at the cost of possible duplication of\n\
    some data items) can easily be accommodated with a slight adjustment\nto the protocol.\
    \ Collected data which does not adhere to the\ncommutative relationship indicated\
    \ above, can also be handled by\nutilizing more buffer space at the data generator\
    \ sites.\nThe sequence number can be incremented for this new set of data\nmessages,\
    \ and the new data can also be sent to the slow host. In\nthis way we won't be\
    \ giving the tardy response from the old favored\nhost unfair advantage in determining\
    \ which server can respond most\nquickly. If there is no reply to this series\
    \ of messages, the TIP\ncan continue to resend the new data. However, the sequence\
    \ number\nshould not be incremented, since no reply was received, and since\n\
    indiscriminate incrementing of the sequence number increases the\nchance of recycling\
    \ during the lifetime of a message.\n"
