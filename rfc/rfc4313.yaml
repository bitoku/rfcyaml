- title: __initial_text__
  contents:
  - "                Requirements for Distributed Control of\n                  Automatic\
    \ Speech Recognition (ASR),\n       Speaker Identification/Speaker Verification\
    \ (SI/SV), and\n                     Text-to-Speech (TTS) Resources\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2005).\n"
- title: Abstract
  contents:
  - "Abstract\n   This document outlines the needs and requirements for a protocol\
    \ to\n   control distributed speech processing of audio streams.  By speech\n\
    \   processing, this document specifically means automatic speech\n   recognition\
    \ (ASR), speaker recognition -- which includes both speaker\n   identification\
    \ (SI) and speaker verification (SV) -- and\n   text-to-speech (TTS).  Other IETF\
    \ protocols, such as SIP and Real\n   Time Streaming Protocol (RTSP), address\
    \ rendezvous and control for\n   generalized media streams.  However, speech processing\
    \ presents\n   additional requirements that none of the extant IETF protocols\n\
    \   address.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Document Conventions .......................................3\n  \
    \ 2. SPEECHSC Framework ..............................................4\n    \
    \  2.1. TTS Example ................................................5\n      2.2.\
    \ Automatic Speech Recognition Example .......................6\n      2.3. Speaker\
    \ Identification example .............................6\n   3. General Requirements\
    \ ............................................7\n      3.1. Reuse Existing Protocols\
    \ ...................................7\n      3.2. Maintain Existing Protocol\
    \ Integrity .......................7\n      3.3. Avoid Duplicating Existing Protocols\
    \ .......................7\n      3.4. Efficiency .................................................8\n\
    \      3.5. Invocation of Services .....................................8\n  \
    \    3.6. Location and Load Balancing ................................8\n    \
    \  3.7. Multiple Services ..........................................8\n      3.8.\
    \ Multiple Media Sessions ....................................8\n      3.9. Users\
    \ with Disabilities ....................................9\n      3.10. Identification\
    \ of Process That Produced Media or\n            Control Output ............................................9\n\
    \   4. TTS Requirements ................................................9\n  \
    \    4.1. Requesting Text Playback ...................................9\n    \
    \  4.2. Text Formats ...............................................9\n      \
    \     4.2.1. Plain Text ..........................................9\n        \
    \   4.2.2. SSML ................................................9\n          \
    \ 4.2.3. Text in Control Channel ............................10\n           4.2.4.\
    \ Document Type Indication ...........................10\n      4.3. Control Channel\
    \ ...........................................10\n      4.4. Media Origination/Termination\
    \ by Control Elements .........10\n      4.5. Playback Controls .........................................10\n\
    \      4.6. Session Parameters ........................................11\n  \
    \    4.7. Speech Markers ............................................11\n   5.\
    \ ASR Requirements ...............................................11\n      5.1.\
    \ Requesting Automatic Speech Recognition ...................11\n      5.2. XML\
    \ .......................................................11\n      5.3. Grammar\
    \ Requirements ......................................12\n           5.3.1. Grammar\
    \ Specification ..............................12\n           5.3.2. Explicit Indication\
    \ of Grammar Format ..............12\n           5.3.3. Grammar Sharing ....................................12\n\
    \      5.4. Session Parameters ........................................12\n  \
    \    5.5. Input Capture .............................................12\n   6.\
    \ Speaker Identification and Verification Requirements ...........13\n      6.1.\
    \ Requesting SI/SV ..........................................13\n      6.2. Identifiers\
    \ for SI/SV .....................................13\n      6.3. State for Multiple\
    \ Utterances .............................13\n      6.4. Input Capture .............................................13\n\
    \      6.5. SI/SV Functional Extensibility ............................13\n  \
    \ 7. Duplexing and Parallel Operation Requirements ..................13\n    \
    \  7.1. Full Duplex Operation .....................................14\n      7.2.\
    \ Multiple Services in Parallel .............................14\n      7.3. Combination\
    \ of Services ...................................14\n   8. Additional Considerations\
    \ (Non-Normative) ......................14\n   9. Security Considerations ........................................15\n\
    \      9.1. SPEECHSC Protocol Security ................................15\n  \
    \    9.2. Client and Server Implementation and Deployment ...........16\n    \
    \  9.3. Use of SPEECHSC for Security Functions ....................16\n   10.\
    \ Acknowledgements ..............................................17\n   11. References\
    \ ....................................................18\n      11.1. Normative\
    \ References .....................................18\n      11.2. Informative\
    \ References ...................................18\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   There are multiple IETF protocols for establishment and\
    \ termination\n   of media sessions (SIP [6]), low-level media control (Media\
    \ Gateway\n   Control Protocol (MGCP) [7] and Media Gateway Controller (MEGACO)\n\
    \   [8]), and media record and playback (RTSP [9]).  This document\n   focuses\
    \ on requirements for one or more protocols to support the\n   control of network\
    \ elements that perform Automated Speech Recognition\n   (ASR), speaker identification\
    \ or verification (SI/SV), and rendering\n   text into audio, also known as Text-to-Speech\
    \ (TTS).  Many multimedia\n   applications can benefit from having automatic speech\
    \ recognition\n   (ASR) and text-to-speech (TTS) processing available as a distributed,\n\
    \   network resource.  This requirements document limits its focus to the\n  \
    \ distributed control of ASR, SI/SV, and TTS servers.\n   There is a broad range\
    \ of systems that can benefit from a unified\n   approach to control of TTS, ASR,\
    \ and SI/SV.  These include\n   environments such as Voice over IP (VoIP) gateways\
    \ to the Public\n   Switched Telephone Network (PSTN), IP telephones, media servers,\
    \ and\n   wireless mobile devices that obtain speech services via servers on\n\
    \   the network.\n   To date, there are a number of proprietary ASR and TTS APIs,\
    \ as well\n   as two IETF documents that address this problem [13], [14].  However,\n\
    \   there are serious deficiencies to the existing documents.  In\n   particular,\
    \ they mix the semantics of existing protocols yet are\n   close enough to other\
    \ protocols as to be confusing to the\n   implementer.\n   This document sets\
    \ forth requirements for protocols to support\n   distributed speech processing\
    \ of audio streams.  For simplicity, and\n   to remove confusion with existing\
    \ protocol proposals, this document\n   presents the requirements as being for\
    \ a \"framework\" that addresses\n   the distributed control of speech resources.\
    \  It refers to such a\n   framework as \"SPEECHSC\", for Speech Services Control.\n"
- title: 1.1.  Document Conventions
  contents:
  - "1.1.  Document Conventions\n   In this document, the key words \"MUST\", \"MUST\
    \ NOT\", \"REQUIRED\",\n   \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\"\
    , \"RECOMMENDED\", \"MAY\",\n   and \"OPTIONAL\" are to be interpreted as described\
    \ in RFC 2119 [3].\n"
- title: 2.  SPEECHSC Framework
  contents:
  - "2.  SPEECHSC Framework\n   Figure 1 below shows the SPEECHSC framework for speech\
    \ processing.\n                          +-------------+\n                   \
    \       | Application |\n                          |   Server    |\\\n       \
    \                   +-------------+ \\ SPEECHSC\n            SIP, VoiceXML,  /\
    \              \\\n             etc.          /                \\\n          \
    \ +------------+ /                  \\    +-------------+\n           |   Media\
    \    |/       SPEECHSC     \\---| ASR, SI/SV, |\n           | Processing |-------------------------|\
    \ and/or TTS  |\n       RTP |   Entity   |           RTP           |    Server\
    \   |\n      =====|            |=========================|             |\n   \
    \        +------------+                         +-------------+\n            \
    \           Figure 1: SPEECHSC Framework\n   The \"Media Processing Entity\" is\
    \ a network element that processes\n   media.  It may be a pure media handler,\
    \ or it may also have an\n   associated SIP user agent, VoiceXML browser, or other\
    \ control entity.\n   The \"ASR, SI/SV, and/or TTS Server\" is a network element\
    \ that\n   performs the back-end speech processing.  It may generate an RTP\n\
    \   stream as output based on text input (TTS) or return recognition\n   results\
    \ in response to an RTP stream as input (ASR, SI/SV).  The\n   \"Application Server\"\
    \ is a network element that instructs the Media\n   Processing Entity on what\
    \ transformations to make to the media\n   stream.  Those instructions may be\
    \ established via a session protocol\n   such as SIP, or provided via a client/server\
    \ exchange such as\n   VoiceXML.  The framework allows either the Media Processing\
    \ Entity or\n   the Application Server to control the ASR or TTS Server using\n\
    \   SPEECHSC as a control protocol, which accounts for the SPEECHSC\n   protocol\
    \ appearing twice in the diagram.\n   Physical embodiments of the entities can\
    \ reside in one physical\n   instance per entity, or some combination of entities.\
    \  For example, a\n   VoiceXML [11] gateway may combine the ASR and TTS functions\
    \ on the\n   same platform as the Media Processing Entity.  Note that VoiceXML\n\
    \   gateways themselves are outside the scope of this protocol.\n   Likewise,\
    \ one can combine the Application Server and Media Processing\n   Entity, as would\
    \ be the case in an interactive voice response (IVR)\n   platform.\n   One can\
    \ also decompose the Media Processing Entity into an entity\n   that controls\
    \ media endpoints and entities that process media\n   directly.  Such would be\
    \ the case with a decomposed gateway using\n   MGCP or MEGACO.  However, this\
    \ decomposition is again orthogonal to\n   the scope of SPEECHSC.  The following\
    \ subsections provide a number of\n   example use cases of the SPEECHSC, one each\
    \ for TTS, ASR, and SI/SV.\n   They are intended to be illustrative only, and\
    \ not to imply any\n   restriction on the scope of the framework or to limit the\n\
    \   decomposition or configuration to that shown in the example.\n"
- title: 2.1.  TTS Example
  contents:
  - "2.1.  TTS Example\n   This example illustrates a simple usage of SPEECHSC to\
    \ provide a\n   Text-to-Speech service for playing announcements to a user on\
    \ a phone\n   with no display for textual error messages.  The example scenario\
    \ is\n   shown below in Figure 2.  In the figure, the VoIP gateway acts as\n \
    \  both the Media Processing Entity and the Application Server of the\n   SPEECHSC\
    \ framework in Figure 1.\n                                      +---------+\n\
    \                                     _|   SIP   |\n                         \
    \          _/ |  Server |\n                +-----------+  SIP/   +---------+\n\
    \                |           |  _/\n    +-------+   |   VoIP    |_/\n    | POTS\
    \  |___| Gateway   |   RTP   +---------+\n    | Phone |   | (SIP UA)  |=========|\
    \         |\n    +-------+   |           |\\_       | SPEECHSC|\n            \
    \    +-----------+  \\      |   TTS   |\n                                \\__\
    \   |  Server |\n                             SPEECHSC |         |\n         \
    \                           \\_|         |\n                                 \
    \     +---------+\n               Figure 2: Text-to-Speech Example of SPEECHSC\n\
    \   The Plain Old Telephone Service (POTS) phone on the left attempts to\n   make\
    \ a phone call.  The VoIP gateway, acting as a SIP UA, tries to\n   establish\
    \ a SIP session to complete the call, but gets an error, such\n   as a SIP \"\
    486 Busy Here\" response.  Without SPEECHSC, the gateway\n   would most likely\
    \ just output a busy signal to the POTS phone.\n   However, with SPEECHSC access\
    \ to a TTS server, it can provide a\n   spoken error message.  The VoIP gateway\
    \ therefore constructs a text\n   error string using information from the SIP\
    \ messages, such as \"Your\n   call to 978-555-1212 did not go through because\
    \ the called party was\n   busy\".  It then can use SPEECHSC to establish an association\
    \ with a\n   SPEECHSC server, open an RTP stream between itself and the server,\n\
    \   and issue a TTS request for the error message, which will be played\n   to\
    \ the user on the POTS phone.\n"
- title: 2.2.  Automatic Speech Recognition Example
  contents:
  - "2.2.  Automatic Speech Recognition Example\n   This example illustrates a VXML-enabled\
    \ media processing entity and\n   associated application server using the SPEECHSC\
    \ framework to supply\n   an ASR-based user interface through an Interactive Voice\
    \ Response\n   (IVR) system.  The example scenario is shown below in Figure 3.\
    \  The\n   VXML-client corresponds to the \"media processing entity\", while the\n\
    \   IVR application server corresponds to the \"application server\" of the\n\
    \   SPEECHSC framework of Figure 1.\n                                      +------------+\n\
    \                                      |    IVR     |\n                      \
    \               _|Application |\n                               VXML_/ +------------+\n\
    \                +-----------+  __/\n                |           |_/       +------------+\n\
    \    PSTN Trunk  |   VoIP    | SPEECHSC|            |\n   =============| Gateway\
    \   |---------| SPEECHSC   |\n                |(VXML voice|         |   ASR  \
    \    |\n                | browser)  |=========|  Server    |\n               \
    \ +-----------+   RTP   +------------+\n              Figure 3: Automatic Speech\
    \ Recognition Example\n   In this example, users call into the service in order\
    \ to obtain stock\n   quotes.  The VoIP gateway answers their PSTN call.  An IVR\n\
    \   application feeds VXML scripts to the gateway to drive the user\n   interaction.\
    \  The VXML interpreter on the gateway directs the user's\n   media stream to\
    \ the SPEECHSC ASR server and uses SPEECHSC to control\n   the ASR server.\n \
    \  When, for example, the user speaks the name of a stock in response to\n   an\
    \ IVR prompt, the SPEECHSC ASR server attempts recognition of the\n   name, and\
    \ returns the results to the VXML gateway.  The VXML gateway,\n   following standard\
    \ VXML mechanisms, informs the IVR Application of\n   the recognized result. \
    \ The IVR Application can then do the\n   appropriate information lookup.  The\
    \ answer, of course, can be sent\n   back to the user using text-to-speech.  This\
    \ example does not show\n   this scenario, but it would work analogously to the\
    \ scenario shown in\n   section Section 2.1.\n"
- title: 2.3.  Speaker Identification example
  contents:
  - "2.3.  Speaker Identification example\n   This example illustrates using speaker\
    \ identification to allow\n   voice-actuated login to an IP phone.  The example\
    \ scenario is shown\n   below in Figure 4.  In the figure, the IP Phone acts as\
    \ both the\n   \"Media Processing Entity\" and the \"Application Server\" of the\n\
    \   SPEECHSC framework in Figure 1.\n   +-----------+         +---------+\n  \
    \ |           |   RTP   |         |\n   |   IP      |=========| SPEECHSC|\n  \
    \ |  Phone    |         |   TTS   |\n   |           |_________|  Server |\n  \
    \ |           | SPEECHSC|         |\n   +-----------+         +---------+\n  \
    \               Figure 4: Speaker Identification Example\n   In this example,\
    \ a user speaks into a SIP phone in order to get\n   \"logged in\" to that phone\
    \ to make and receive phone calls using his\n   identity and preferences.  The\
    \ IP phone uses the SPEECHSC framework\n   to set up an RTP stream between the\
    \ phone and the SPEECHSC SI/SV\n   server and to request verification.  The SV\
    \ server verifies the\n   user's identity and returns the result, including the\
    \ necessary login\n   credentials, to the phone via SPEECHSC.  The IP Phone may\
    \ use the\n   identity directly to identify the user in outgoing calls, to fetch\n\
    \   the user's preferences from a configuration server, or to request\n   authorization\
    \ from an Authentication, Authorization, and Accounting\n   (AAA) server, in any\
    \ combination.  Since this example uses SPEECHSC\n   to perform a security-related\
    \ function, be sure to note the\n   associated material in Section 9.\n"
- title: 3.  General Requirements
  contents:
  - '3.  General Requirements

    '
- title: 3.1.  Reuse Existing Protocols
  contents:
  - "3.1.  Reuse Existing Protocols\n   To the extent feasible, the SPEECHSC framework\
    \ SHOULD use existing\n   protocols.\n"
- title: 3.2.  Maintain Existing Protocol Integrity
  contents:
  - "3.2.  Maintain Existing Protocol Integrity\n   In meeting the requirement of\
    \ Section 3.1, the SPEECHSC framework\n   MUST NOT redefine the semantics of an\
    \ existing protocol.  Said\n   differently, we will not break existing protocols\
    \ or cause\n   backward-compatibility problems.\n"
- title: 3.3.  Avoid Duplicating Existing Protocols
  contents:
  - "3.3.  Avoid Duplicating Existing Protocols\n   To the extent feasible, SPEECHSC\
    \ SHOULD NOT duplicate the\n   functionality of existing protocols.  For example,\
    \ network\n   announcements using SIP [12] and RTSP [9] already define how to\n\
    \   request playback of audio.  The focus of SPEECHSC is new\n   functionality\
    \ not addressed by existing protocols or extending\n   existing protocols within\
    \ the strictures of the requirement in\n   Section 3.2.  Where an existing protocol\
    \ can be gracefully extended\n   to support SPEECHSC requirements, such extensions\
    \ are acceptable\n   alternatives for meeting the requirements.\n   As a corollary\
    \ to this, the SPEECHSC should not require a separate\n   protocol to perform\
    \ functions that could be easily added into the\n   SPEECHSC protocol (like redirecting\
    \ media streams, or discovering\n   capabilities), unless it is similarly easy\
    \ to embed that protocol\n   directly into the SPEECHSC framework.\n"
- title: 3.4.  Efficiency
  contents:
  - "3.4.  Efficiency\n   The SPEECHSC framework SHOULD employ protocol elements known\
    \ to\n   result in efficient operation.  Techniques to be considered include:\n\
    \   o  Re-use of transport connections across sessions\n   o  Piggybacking of\
    \ responses on requests in the reverse direction\n   o  Caching of state across\
    \ requests\n"
- title: 3.5.  Invocation of Services
  contents:
  - "3.5.  Invocation of Services\n   The SPEECHSC framework MUST be compliant with\
    \ the IAB Open Pluggable\n   Edge Services (OPES) [4] framework.  The applicability\
    \ of the\n   SPEECHSC protocol will therefore be specified as occurring between\n\
    \   clients and servers at least one of which is operating directly on\n   behalf\
    \ of the user requesting the service.\n"
- title: 3.6.  Location and Load Balancing
  contents:
  - "3.6.  Location and Load Balancing\n   To the extent feasible, the SPEECHSC framework\
    \ SHOULD exploit\n   existing schemes for supporting service location and load\
    \ balancing,\n   such as the Service Location Protocol [13] or DNS SRV records\
    \ [14].\n   Where such facilities are not deemed adequate, the SPEECHSC framework\n\
    \   MAY define additional load balancing techniques.\n"
- title: 3.7.  Multiple Services
  contents:
  - "3.7.  Multiple Services\n   The SPEECHSC framework MUST permit multiple services\
    \ to operate on a\n   single media stream so that either the same or different\
    \ servers may\n   be performing speech recognition, speaker identification or\n\
    \   verification, etc., in parallel.\n"
- title: 3.8.  Multiple Media Sessions
  contents:
  - "3.8.  Multiple Media Sessions\n   The SPEECHSC framework MUST allow a 1:N mapping\
    \ between session and\n   RTP channels.  For example, a single session may include\
    \ an outbound\n   RTP channel for TTS, an inbound for ASR, and a different inbound\
    \ for\n   SI/SV (e.g., if processed by different elements on the Media Resource\n\
    \   Element).  Note: All of these can be described via SDP, so if SDP is\n   utilized\
    \ for media channel description, this requirement is met \"for\n   free\".\n"
- title: 3.9.  Users with Disabilities
  contents:
  - "3.9.  Users with Disabilities\n   The SPEECHSC framework must have sufficient\
    \ capabilities to address\n   the critical needs of people with disabilities.\
    \  In particular, the\n   set of requirements set forth in RFC 3351 [5] MUST be\
    \ taken into\n   account by the framework.  It is also important that implementers\
    \ of\n   SPEECHSC clients and servers be cognizant that some interaction\n   modalities\
    \ of SPEECHSC may be inconvenient or simply inappropriate\n   for disabled users.\
    \  Hearing-impaired individuals may find TTS of\n   limited utility.  Speech-impaired\
    \ users may be unable to make use of\n   ASR or SI/SV capabilities.  Therefore,\
    \ systems employing SPEECHSC\n   MUST provide alternative interaction modes or\
    \ avoid the use of speech\n   processing entirely.\n"
- title: 3.10.  Identification of Process That Produced Media or Control Output
  contents:
  - "3.10.  Identification of Process That Produced Media or Control Output\n   The\
    \ client of a SPEECHSC operation SHOULD be able to ascertain via\n   the SPEECHSC\
    \ framework what speech process produced the output.  For\n   example, an RTP\
    \ stream containing the spoken output of TTS should be\n   identifiable as TTS\
    \ output, and the recognized utterance of ASR\n   should be identifiable as having\
    \ been produced by ASR processing.\n"
- title: 4.  TTS Requirements
  contents:
  - '4.  TTS Requirements

    '
- title: 4.1.  Requesting Text Playback
  contents:
  - "4.1.  Requesting Text Playback\n   The SPEECHSC framework MUST allow a Media\
    \ Processing Entity or\n   Application Server, using a control protocol, to request\
    \ the TTS\n   Server to play back text as voice in an RTP stream.\n"
- title: 4.2.  Text Formats
  contents:
  - '4.2.  Text Formats

    '
- title: 4.2.1.  Plain Text
  contents:
  - "4.2.1.  Plain Text\n   The SPEECHSC framework MAY assume that all TTS servers\
    \ are capable of\n   reading plain text.  For reading plain text, framework MUST\
    \ allow the\n   language and voicing to be indicated via session parameters. \
    \ For\n   finer control over such properties, see [1].\n"
- title: 4.2.2.  SSML
  contents:
  - "4.2.2.  SSML\n   The SPEECHSC framework MUST support Speech Synthesis Markup\
    \ Language\n   (SSML)[1] <speak> basics, and SHOULD support other SSML tags. \
    \ The\n   framework assumes all TTS servers are capable of reading SSML\n   formatted\
    \ text.  Internationalization of TTS in the SPEECHSC\n   framework, including\
    \ multi-lingual output within a single utterance,\n   is accomplished via SSML\
    \ xml:lang tags.\n"
- title: 4.2.3.  Text in Control Channel
  contents:
  - "4.2.3.  Text in Control Channel\n   The SPEECHSC framework assumes all TTS servers\
    \ accept text over the\n   SPEECHSC connection for reading over the RTP connection.\
    \  The\n   framework assumes the server can accept text either \"by value\"\n\
    \   (embedded in the protocol) or \"by reference\" (e.g., by de-referencing\n\
    \   a Uniform Resource Identifier (URI) embedded in the protocol).\n"
- title: 4.2.4.  Document Type Indication
  contents:
  - "4.2.4.  Document Type Indication\n   A document type specifies the syntax in\
    \ which the text to be read is\n   encoded.  The SPEECHSC framework MUST be capable\
    \ of explicitly\n   indicating the document type of the text to be processed,\
    \ as opposed\n   to forcing the server to infer the content by other means.\n"
- title: 4.3.  Control Channel
  contents:
  - "4.3.  Control Channel\n   The SPEECHSC framework MUST be capable of establishing\
    \ the control\n   channel between the client and server on a per-session basis,\
    \ where a\n   session is loosely defined to be associated with a single \"call\"\
    \ or\n   \"dialog\".  The protocol SHOULD be capable of maintaining a long-lived\n\
    \   control channel for multiple sessions serially, and MAY be capable of\n  \
    \ shorter time horizons as well, including as short as for the\n   processing\
    \ of a single utterance.\n"
- title: 4.4.  Media Origination/Termination by Control Elements
  contents:
  - "4.4.  Media Origination/Termination by Control Elements\n   The SPEECHSC framework\
    \ MUST NOT require the controlling element\n   (application server, media processing\
    \ entity) to accept or originate\n   media streams.  Media streams MAY source\
    \ & sink from the controlled\n   element (ASR, TTS, etc.).\n"
- title: 4.5.  Playback Controls
  contents:
  - "4.5.  Playback Controls\n   The SPEECHSC framework MUST support \"VCR controls\"\
    \ for controlling\n   the playout of streaming media output from SPEECHSC processing,\
    \ and\n   MUST allow for servers with varying capabilities to accommodate such\n\
    \   controls.  The protocol SHOULD allow clients to state what controls\n   they\
    \ wish to use, and for servers to report which ones they honor.\n   These capabilities\
    \ include:\n   o  The ability to jump in time to the location of a specific marker.\n\
    \   o  The ability to jump in time, forwards or backwards, by a specified\n  \
    \    amount of time.  Valid time units MUST include seconds, words,\n      paragraphs,\
    \ sentences, and markers.\n   o  The ability to increase and decrease playout\
    \ speed.\n   o  The ability to fast-forward and fast-rewind the audio, where\n\
    \      snippets of audio are played as the server moves forwards or\n      backwards\
    \ in time.\n   o  The ability to pause and resume playout.\n   o  The ability\
    \ to increase and decrease playout volume.\n   These controls SHOULD be made easily\
    \ available to users through the\n   client user interface and through per-user\
    \ customization capabilities\n   of the client.  This is particularly important\
    \ for hearing-impaired\n   users, who will likely desire settings and control\
    \ regimes different\n   from those that would be acceptable for non-impaired users.\n"
- title: 4.6.  Session Parameters
  contents:
  - "4.6.  Session Parameters\n   The SPEECHSC framework MUST support the specification\
    \ of session\n   parameters, such as language, prosody, and voicing.\n"
- title: 4.7.  Speech Markers
  contents:
  - "4.7.  Speech Markers\n   The SPEECHSC framework MUST accommodate speech markers,\
    \ with\n   capability at least as flexible as that provided in SSML [1].  The\n\
    \   framework MUST further provide an efficient mechanism for reporting\n   that\
    \ a marker has been reached during playout.\n"
- title: 5.  ASR Requirements
  contents:
  - '5.  ASR Requirements

    '
- title: 5.1.  Requesting Automatic Speech Recognition
  contents:
  - "5.1.  Requesting Automatic Speech Recognition\n   The SPEECHSC framework MUST\
    \ allow a Media Processing Entity or\n   Application Server to request the ASR\
    \ Server to perform automatic\n   speech recognition on an RTP stream, returning\
    \ the results over\n   SPEECHSC.\n"
- title: 5.2.  XML
  contents:
  - "5.2.  XML\n   The SPEECHSC framework assumes that all ASR servers support the\n\
    \   VoiceXML speech recognition grammar specification (SRGS) for speech\n   recognition\
    \ [2].\n"
- title: 5.3.  Grammar Requirements
  contents:
  - '5.3.  Grammar Requirements

    '
- title: 5.3.1.  Grammar Specification
  contents:
  - "5.3.1.  Grammar Specification\n   The SPEECHSC framework assumes all ASR servers\
    \ are capable of\n   accepting grammar specifications either \"by value\" (embedded\
    \ in the\n   protocol) or \"by reference\" (e.g., by de-referencing a URI embedded\n\
    \   in the protocol).  The latter MUST allow the indication of a grammar\n   already\
    \ known to, or otherwise \"built in\" to, the server.  The\n   framework and protocol\
    \ further SHOULD exploit the ability to store\n   and later retrieve by reference\
    \ large grammars that were originally\n   supplied by the client.\n"
- title: 5.3.2.  Explicit Indication of Grammar Format
  contents:
  - "5.3.2.  Explicit Indication of Grammar Format\n   The SPEECHSC framework protocol\
    \ MUST be able to explicitly convey the\n   grammar format in which the grammar\
    \ is encoded and MUST be extensible\n   to allow for conveying new grammar formats\
    \ as they are defined.\n"
- title: 5.3.3.  Grammar Sharing
  contents:
  - "5.3.3.  Grammar Sharing\n   The SPEECHSC framework SHOULD exploit sharing grammars\
    \ across\n   sessions for servers that are capable of doing so.  This supports\n\
    \   applications with large grammars for which it is unrealistic to\n   dynamically\
    \ load.  An example is a city-country grammar for a weather\n   service.\n"
- title: 5.4.  Session Parameters
  contents:
  - "5.4.  Session Parameters\n   The SPEECHSC framework MUST accommodate at a minimum\
    \ all of the\n   protocol parameters currently defined in Media Resource Control\n\
    \   Protocol (MRCP) [10] In addition, there SHOULD be a capability to\n   reset\
    \ parameters within a session.\n"
- title: 5.5.  Input Capture
  contents:
  - "5.5.  Input Capture\n   The SPEECHSC framework MUST support a method directing\
    \ the ASR Server\n   to capture the input media stream for later analysis and\
    \ tuning of\n   the ASR engine.\n"
- title: 6.  Speaker Identification and Verification Requirements
  contents:
  - '6.  Speaker Identification and Verification Requirements

    '
- title: 6.1.  Requesting SI/SV
  contents:
  - "6.1.  Requesting SI/SV\n   The SPEECHSC framework MUST allow a Media Processing\
    \ Entity to\n   request the SI/SV Server to perform speaker identification or\n\
    \   verification on an RTP stream, returning the results over SPEECHSC.\n"
- title: 6.2.  Identifiers for SI/SV
  contents:
  - "6.2.  Identifiers for SI/SV\n   The SPEECHSC framework MUST accommodate an identifier\
    \ for each\n   verification resource and permit control of that resource by ID,\n\
    \   because voiceprint format and contents are vendor specific.\n"
- title: 6.3.  State for Multiple Utterances
  contents:
  - "6.3.  State for Multiple Utterances\n   The SPEECHSC framework MUST work with\
    \ SI/SV servers that maintain\n   state to handle multi-utterance verification.\n"
- title: 6.4.  Input Capture
  contents:
  - "6.4.  Input Capture\n   The SPEECHSC framework MUST support a method for capturing\
    \ the input\n   media stream for later analysis and tuning of the SI/SV engine.\
    \  The\n   framework may assume all servers are capable of doing so.  In\n   addition,\
    \ the framework assumes that the captured stream contains\n   enough timestamp\
    \ context (e.g., the NTP time range from the RTP\n   Control Protocol (RTCP) packets,\
    \ which corresponds to the RTP\n   timestamps of the captured input) to ascertain\
    \ after the fact exactly\n   when the verification was requested.\n"
- title: 6.5.  SI/SV Functional Extensibility
  contents:
  - "6.5.  SI/SV Functional Extensibility\n   The SPEECHSC framework SHOULD be extensible\
    \ to additional functions\n   associated with SI/SV, such as prompting, utterance\
    \ verification, and\n   retraining.\n"
- title: 7.  Duplexing and Parallel Operation Requirements
  contents:
  - "7.  Duplexing and Parallel Operation Requirements\n   One very important requirement\
    \ for an interactive speech-driven\n   system is that user perception of the quality\
    \ of the interaction\n   depends strongly on the ability of the user to interrupt\
    \ a prompt or\n   rendered TTS with speech.  Interrupting, or barging, the speech\n\
    \   output requires more than energy detection from the user's direction.\n  \
    \ Many advanced systems halt the media towards the user by employing\n   the ASR\
    \ engine to decide if an utterance is likely to be real speech,\n   as opposed\
    \ to a cough, for example.\n"
- title: 7.1.  Full Duplex Operation
  contents:
  - "7.1.  Full Duplex Operation\n   To achieve low latency between utterance detection\
    \ and halting of\n   playback, many implementations combine the speaking and ASR\n\
    \   functions.  The SPEECHSC framework MUST support such full-duplex\n   implementations.\n"
- title: 7.2.  Multiple Services in Parallel
  contents:
  - "7.2.  Multiple Services in Parallel\n   Good spoken user interfaces typically\
    \ depend upon the ease with which\n   the user can accomplish his or her task.\
    \  When making use of speaker\n   identification or verification technologies,\
    \ user interface\n   improvements often come from the combination of the different\n\
    \   technologies: simultaneous identity claim and verification (on the\n   same\
    \ utterance), simultaneous knowledge and voice verification (using\n   ASR and\
    \ verification simultaneously).  Using ASR and verification on\n   the same utterance\
    \ is in fact the only way to support rolling or\n   dynamically-generated challenge\
    \ phrases (e.g., \"say 51723\").  The\n   SPEECHSC framework MUST support such\
    \ parallel service\n   implementations.\n"
- title: 7.3.  Combination of Services
  contents:
  - "7.3.  Combination of Services\n   It is optionally of interest that the SPEECHSC\
    \ framework support more\n   complex remote combination and controls of speech\
    \ engines:\n   o  Combination in series of engines that may then act on the input\
    \ or\n      output of ASR, TTS, or Speaker recognition engines.  The control\n\
    \      MAY then extend beyond such engines to include other audio input\n    \
    \  and output processing and natural language processing.\n   o  Intermediate\
    \ exchanges and coordination between engines.\n   o  Remote specification of flows\
    \ between engines.\n   These capabilities MAY benefit from service discovery mechanisms\n\
    \   (e.g., engines, properties, and states discovery).\n"
- title: 8.  Additional Considerations (Non-Normative)
  contents:
  - "8.  Additional Considerations (Non-Normative)\n   The framework assumes that\
    \ Session Description Protocol (SDP) will be\n   used to describe media sessions\
    \ and streams.  The framework further\n   assumes RTP carriage of media.  However,\
    \ since SDP can be used to\n   describe other media transport schemes (e.g., ATM)\
    \ these could be\n   used if they provide the necessary elements (e.g., explicit\n\
    \   timestamps).\n   The working group will not be defining distributed speech\
    \ recognition\n   (DSR) methods, as exemplified by the European Telecommunications\n\
    \   Standards Institute (ETSI) Aurora project.  The working group will\n   not\
    \ be recreating functionality available in other protocols, such as\n   SIP or\
    \ SDP.\n   TTS looks very much like playing back a file.  Extending RTSP looks\n\
    \   promising for when one requires VCR controls or markers in the text\n   to\
    \ be spoken.  When one does not require VCR controls, SIP in a\n   framework such\
    \ as Network Announcements [12] works directly without\n   modification.\n   ASR\
    \ has an entirely different set of characteristics.  For barge-in\n   support,\
    \ ASR requires real-time return of intermediate results.\n   Barring the discovery\
    \ of a good reuse model for an existing protocol,\n   this will most likely become\
    \ the focus of SPEECHSC.\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   Protocols relating to speech processing must\
    \ take security and\n   privacy into account.  Many applications of speech technology\
    \ deal\n   with sensitive information, such as the use of Text-to-Speech to read\n\
    \   financial information.  Likewise, popular uses for automatic speech\n   recognition\
    \ include executing financial transactions and shopping.\n   There are at least\
    \ three aspects of speech processing security that\n   intersect with the SPEECHSC\
    \ requirements -- securing the SPEECHSC\n   protocol itself, implementing and\
    \ deploying the servers that run the\n   protocol, and ensuring that utilization\
    \ of the technology for\n   providing security functions is appropriate.  Each\
    \ of these aspects\n   in discussed in the following subsections.  While some\
    \ of these\n   considerations are, strictly speaking, out of scope of the protocol\n\
    \   itself, they will be carefully considered and accommodated during\n   protocol\
    \ design, and will be called out as part of the applicability\n   statement accompanying\
    \ the protocol specification(s).  Privacy\n   considerations are discussed as\
    \ well.\n"
- title: 9.1.  SPEECHSC Protocol Security
  contents:
  - "9.1.  SPEECHSC Protocol Security\n   The SPEECHSC protocol MUST in all cases\
    \ support authentication,\n   authorization, and integrity, and SHOULD support\
    \ confidentiality.\n   For privacy-sensitive applications, the protocol MUST support\n\
    \   confidentiality.  We envision that rather than providing\n   protocol-specific\
    \ security mechanisms in SPEECHSC itself, the\n   resulting protocol will employ\
    \ security machinery of either a\n   containing protocol or the transport on which\
    \ it runs.  For example,\n   we will consider solutions such as using Transport\
    \ Layer Security\n   (TLS) for securing the control channel, and Secure Realtime\
    \ Transport\n   Protocol (SRTP) for securing the media channel.  Third-party\n\
    \   dependencies necessitating transitive trust will be minimized or\n   explicitly\
    \ dealt with through the authentication and authorization\n   aspects of the protocol\
    \ design.\n"
- title: 9.2.  Client and Server Implementation and Deployment
  contents:
  - "9.2.  Client and Server Implementation and Deployment\n   Given the possibly\
    \ sensitive nature of the information carried,\n   SPEECHSC clients and servers\
    \ need to take steps to ensure\n   confidentiality and integrity of the data and\
    \ its transformations to\n   and from spoken form.  In addition to these general\
    \ considerations,\n   certain SPEECHSC functions, such as speaker verification\
    \ and\n   identification, employ voiceprints whose privacy, confidentiality,\n\
    \   and integrity must be maintained.  Similarly, the requirement to\n   support\
    \ input capture for analysis and tuning can represent a privacy\n   vulnerability\
    \ because user utterances are recorded and could be\n   either revealed or replayed\
    \ inappropriately.  Implementers must take\n   care to prevent the exploitation\
    \ of any centralized voiceprint\n   database and the recorded material from which\
    \ such voiceprints may be\n   derived.  Specific actions that are recommended\
    \ to minimize these\n   threats include:\n   o  End-to-end authentication, confidentiality,\
    \ and integrity\n      protection (like TLS) of access to the database to minimize\
    \ the\n      exposure to external attack.\n   o  Database protection measures\
    \ such as read/write access control and\n      local login authentication to minimize\
    \ the exposure to insider\n      threats.\n   o  Copies of the database, especially\
    \ ones that are maintained at\n      off-site locations, need the same protection\
    \ as the operational\n      database.\n   Inappropriate disclosure of this data\
    \ does not as of the date of this\n   document represent an exploitable threat,\
    \ but quite possibly might in\n   the future.  Specific vulnerabilities that might\
    \ become feasible are\n   discussed in the next subsection.  It is prudent to\
    \ take measures\n   such as encrypting the voiceprint database and permitting\
    \ access only\n   through programming interfaces enforcing adequate authorization\n\
    \   machinery.\n"
- title: 9.3.  Use of SPEECHSC for Security Functions
  contents:
  - "9.3.  Use of SPEECHSC for Security Functions\n   Either speaker identification\
    \ or verification can be used directly as\n   an authentication technology.  Authorization\
    \ decisions can be coupled\n   with speaker verification in a direct fashion through\n\
    \   challenge-response protocols, or indirectly with speaker\n   identification\
    \ through the use of access control lists or other\n   identity-based authorization\
    \ mechanisms.  When so employed, there are\n   additional security concerns that\
    \ need to be addressed through the\n   use of protocol security mechanisms for\
    \ clients and servers.  For\n   example, the ability to manipulate the media stream\
    \ of a speaker\n   verification request could inappropriately permit or deny access\n\
    \   based on impersonation, or simple garbling via noise injection,\n   making\
    \ it critical to properly secure both the control and data\n   channels, as recommended\
    \ above.  The following issues specific to the\n   use of SI/SV for authentication\
    \ should be carefully considered:\n   1.  Theft of voiceprints or the recorded\
    \ samples used to construct\n       them represents a future threat against the\
    \ use of speaker\n       identification/verification as a biometric authentication\n\
    \       technology.  A plausible attack vector (not feasible today) is to\n  \
    \     use the voiceprint information as parametric input to a\n       text-to-speech\
    \ synthesis system that could mimic the user's voice\n       accurately enough\
    \ to match the voiceprint.  Since it is not very\n       difficult to surreptitiously\
    \ record reasonably large corpuses of\n       voice samples, the ability to construct\
    \ voiceprints for input to\n       this attack would render the security of voice-based\
    \ biometric\n       authentication, even using advanced challenge-response\n \
    \      techniques, highly vulnerable.  Users of speaker verification for\n   \
    \    authentication should monitor technological developments in this\n      \
    \ area closely for such future vulnerabilities (much as users of\n       other\
    \ authentication technologies should monitor advances in\n       factoring as\
    \ a way to break asymmetric keying systems).\n   2.  As with other biometric authentication\
    \ technologies, a downside\n       to the use of speech identification is that\
    \ revocation is not\n       possible.  Once compromised, the biometric information\
    \ can be\n       used in identification and authentication to other independent\n\
    \       systems.\n   3.  Enrollment procedures can be vulnerable to impersonation\
    \ if not\n       protected both by protocol security mechanisms and some\n   \
    \    independent proof of identity.  (Proof of identity may not be\n       needed\
    \ in systems that only need to verify continuity of identity\n       since enrollment,\
    \ as opposed to association with a particular\n       individual.\n   Further\
    \ discussion of the use of SI/SV as an authentication\n   technology, and some\
    \ recommendations concerning advantages and\n   vulnerabilities, can be found\
    \ in Chapter 5 of [15].\n"
- title: 10.  Acknowledgements
  contents:
  - "10.  Acknowledgements\n   Eric Burger wrote the original version of these requirements\
    \ and has\n   continued to contribute actively throughout their development. \
    \ He is\n   a co-author in all but formal authorship, and is instead acknowledged\n\
    \   here as it is preferable that working group co-chairs have\n   non-conflicting\
    \ roles with respect to the progression of documents.\n"
- title: 11.  References
  contents:
  - '11.  References

    '
- title: 11.1.  Normative References
  contents:
  - "11.1.  Normative References\n   [1]  Walker, M., Burnett, D., and A. Hunt, \"\
    Speech Synthesis Markup\n        Language (SSML) Version 1.0\", W3C\n        REC\
    \ REC-speech-synthesis-20040907, September 2004.\n   [2]  McGlashan, S. and A.\
    \ Hunt, \"Speech Recognition Grammar\n        Specification Version 1.0\", W3C\
    \ REC REC-speech-grammar-20040316,\n        March 2004.\n   [3]  Bradner, S.,\
    \ \"Key words for use in RFCs to Indicate Requirement\n        Levels\", BCP 14,\
    \ RFC 2119, March 1997.\n   [4]  Floyd, S. and L. Daigle, \"IAB Architectural\
    \ and Policy\n        Considerations for Open Pluggable Edge Services\", RFC 3238,\n\
    \        January 2002.\n   [5]  Charlton, N., Gasson, M., Gybels, G., Spanner,\
    \ M., and A. van\n        Wijk, \"User Requirements for the Session Initiation\
    \ Protocol\n        (SIP) in Support of Deaf, Hard of Hearing and Speech-impaired\n\
    \        Individuals\", RFC 3351, August 2002.\n"
- title: 11.2.  Informative References
  contents:
  - "11.2.  Informative References\n   [6]   Rosenberg, J., Schulzrinne, H., Camarillo,\
    \ G., Johnston, A.,\n         Peterson, J., Sparks, R., Handley, M., and E. Schooler,\
    \ \"SIP:\n         Session Initiation Protocol\", RFC 3261, June 2002.\n   [7]\
    \   Andreasen, F. and B. Foster, \"Media Gateway Control Protocol\n         (MGCP)\
    \ Version 1.0\", RFC 3435, January 2003.\n   [8]   Groves, C., Pantaleo, M., Ericsson,\
    \ LM., Anderson, T., and T.\n         Taylor, \"Gateway Control Protocol Version\
    \ 1\", RFC 3525,\n         June 2003.\n   [9]   Schulzrinne, H., Rao, A., and\
    \ R. Lanphier, \"Real Time Streaming\n         Protocol (RTSP)\", RFC 2326, April\
    \ 1998.\n   [10]  Shanmugham, S., Monaco, P., and B. Eberman, \"MRCP: Media\n\
    \         Resource Control Protocol\", Work in Progress.\n   [11]  World Wide\
    \ Web Consortium, \"Voice Extensible Markup Language\n         (VoiceXML) Version\
    \ 2.0\", W3C Working Draft , April 2002,\n         <http://www.w3.org/TR/2002/WD-voicexml20-20020424/>.\n\
    \   [12]  Burger, E., Ed., Van Dyke, J., and A. Spitzer, \"Basic Network\n   \
    \      Media Services with SIP\", RFC 4240, December 2005.\n   [13]  Guttman,\
    \ E., Perkins, C., Veizades, J., and M. Day, \"Service\n         Location Protocol,\
    \ Version 2\", RFC 2608, June 1999.\n   [14]  Gulbrandsen, A., Vixie, P., and\
    \ L. Esibov, \"A DNS RR for\n         specifying the location of services (DNS\
    \ SRV)\", RFC 2782,\n         February 2000.\n   [15]  Committee on Authentication\
    \ Technologies and Their Privacy\n         Implications, National Research Council,\
    \ \"Who Goes There?:\n         Authentication Through the Lens of Privacy\", Computer\
    \ Science\n         and Telecommunications Board (CSTB) , 2003,\n         <http://www.nap.edu/catalog/10656.html/\
    \ >.\n"
- title: Author's Address
  contents:
  - "Author's Address\n   David R. Oran\n   Cisco Systems, Inc.\n   7 Ladyslipper\
    \ Lane\n   Acton, MA\n   USA\n   EMail: oran@cisco.com\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The Internet Society (2005).\n   This\
    \ document is subject to the rights, licenses and restrictions\n   contained in\
    \ BCP 78, and except as set forth therein, the authors\n   retain all their rights.\n\
    \   This document and the information contained herein are provided on an\n  \
    \ \"AS IS\" basis and THE CONTRIBUTOR, THE ORGANIZATION HE/SHE REPRESENTS\n  \
    \ OR IS SPONSORED BY (IF ANY), THE INTERNET SOCIETY AND THE INTERNET\n   ENGINEERING\
    \ TASK FORCE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED,\n   INCLUDING BUT NOT\
    \ LIMITED TO ANY WARRANTY THAT THE USE OF THE\n   INFORMATION HEREIN WILL NOT\
    \ INFRINGE ANY RIGHTS OR ANY IMPLIED\n   WARRANTIES OF MERCHANTABILITY OR FITNESS\
    \ FOR A PARTICULAR PURPOSE.\n"
- title: Intellectual Property
  contents:
  - "Intellectual Property\n   The IETF takes no position regarding the validity or\
    \ scope of any\n   Intellectual Property Rights or other rights that might be\
    \ claimed to\n   pertain to the implementation or use of the technology described\
    \ in\n   this document or the extent to which any license under such rights\n\
    \   might or might not be available; nor does it represent that it has\n   made\
    \ any independent effort to identify any such rights.  Information\n   on the\
    \ procedures with respect to rights in RFC documents can be\n   found in BCP 78\
    \ and BCP 79.\n   Copies of IPR disclosures made to the IETF Secretariat and any\n\
    \   assurances of licenses to be made available, or the result of an\n   attempt\
    \ made to obtain a general license or permission for the use of\n   such proprietary\
    \ rights by implementers or users of this\n   specification can be obtained from\
    \ the IETF on-line IPR repository at\n   http://www.ietf.org/ipr.\n   The IETF\
    \ invites any interested party to bring to its attention any\n   copyrights, patents\
    \ or patent applications, or other proprietary\n   rights that may cover technology\
    \ that may be required to implement\n   this standard.  Please address the information\
    \ to the IETF at ietf-\n   ipr@ietf.org.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
