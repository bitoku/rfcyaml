- contents:
  - '           Media Resource Control Protocol Version 2 (MRCPv2)

    '
  title: __initial_text__
- contents:
  - "Abstract\n   The Media Resource Control Protocol Version 2 (MRCPv2) allows client\n
    \  hosts to control media service resources such as speech synthesizers,\n   recognizers,
    verifiers, and identifiers residing in servers on the\n   network.  MRCPv2 is
    not a \"stand-alone\" protocol -- it relies on\n   other protocols, such as the
    Session Initiation Protocol (SIP), to\n   coordinate MRCPv2 clients and servers
    and manage sessions between\n   them, and the Session Description Protocol (SDP)
    to describe,\n   discover, and exchange capabilities.  It also depends on SIP
    and SDP\n   to establish the media sessions and associated parameters between
    the\n   media source or sink and the media server.  Once this is done, the\n   MRCPv2
    exchange operates over the control session established above,\n   allowing the
    client to control the media processing resources on the\n   speech resource server.\n"
  title: Abstract
- contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This
    document is a product of the Internet Engineering Task Force\n   (IETF).  It represents
    the consensus of the IETF community.  It has\n   received public review and has
    been approved for publication by the\n   Internet Engineering Steering Group (IESG).
    \ Further information on\n   Internet Standards is available in Section 2 of RFC
    5741.\n   Information about the current status of this document, any errata,\n
    \  and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc6787.\n"
  title: Status of This Memo
- contents:
  - "Copyright Notice\n   Copyright (c) 2012 IETF Trust and the persons identified
    as the\n   document authors.  All rights reserved.\n   This document is subject
    to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n
    \  (http://trustee.ietf.org/license-info) in effect on the date of\n   publication
    of this document.  Please review these documents\n   carefully, as they describe
    your rights and restrictions with respect\n   to this document.  Code Components
    extracted from this document must\n   include Simplified BSD License text as described
    in Section 4.e of\n   the Trust Legal Provisions and are provided without warranty
    as\n   described in the Simplified BSD License.\n   This document may contain
    material from IETF Documents or IETF\n   Contributions published or made publicly
    available before November\n   10, 2008.  The person(s) controlling the copyright
    in some of this\n   material may not have granted the IETF Trust the right to
    allow\n   modifications of such material outside the IETF Standards Process.\n
    \  Without obtaining an adequate license from the person(s) controlling\n   the
    copyright in such materials, this document may not be modified\n   outside the
    IETF Standards Process, and derivative works of it may\n   not be created outside
    the IETF Standards Process, except to format\n   it for publication as an RFC
    or to translate it into languages other\n   than English.\n"
  title: Copyright Notice
- contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .
    . . . . .   8\n   2.  Document Conventions  . . . . . . . . . . . . . . . . .
    . . .   9\n     2.1.   Definitions  . . . . . . . . . . . . . . . . . . . . .
    .  10\n     2.2.   State-Machine Diagrams . . . . . . . . . . . . . . . . .  10\n
    \    2.3.   URI Schemes  . . . . . . . . . . . . . . . . . . . . . .  11\n   3.
    \ Architecture  . . . . . . . . . . . . . . . . . . . . . . . .  11\n     3.1.
    \  MRCPv2 Media Resource Types  . . . . . . . . . . . . . .  12\n     3.2.   Server
    and Resource Addressing . . . . . . . . . . . . .  14\n   4.  MRCPv2 Basics .
    . . . . . . . . . . . . . . . . . . . . . . .  14\n     4.1.   Connecting to the
    Server . . . . . . . . . . . . . . . .  14\n     4.2.   Managing Resource Control
    Channels . . . . . . . . . . .  14\n     4.3.   SIP Session Example  . . . . .
    . . . . . . . . . . . . .  17\n     4.4.   Media Streams and RTP Ports  . . .
    . . . . . . . . . . .  22\n     4.5.   MRCPv2 Message Transport . . . . . . .
    . . . . . . . . .  24\n     4.6.   MRCPv2 Session Termination . . . . . . . .
    . . . . . . .  24\n   5.  MRCPv2 Specification  . . . . . . . . . . . . . . .
    . . . . .  24\n     5.1.   Common Protocol Elements . . . . . . . . . . . . .
    . . .  25\n     5.2.   Request  . . . . . . . . . . . . . . . . . . . . . . .
    .  28\n     5.3.   Response . . . . . . . . . . . . . . . . . . . . . . . .  29\n
    \    5.4.   Status Codes . . . . . . . . . . . . . . . . . . . . . .  30\n     5.5.
    \  Events . . . . . . . . . . . . . . . . . . . . . . . . .  31\n   6.  MRCPv2
    Generic Methods, Headers, and Result Structure . . . .  32\n     6.1.   Generic
    Methods  . . . . . . . . . . . . . . . . . . . .  32\n       6.1.1.   SET-PARAMS
    . . . . . . . . . . . . . . . . . . . . .  32\n       6.1.2.   GET-PARAMS . .
    . . . . . . . . . . . . . . . . . . .  33\n     6.2.   Generic Message Headers
    \ . . . . . . . . . . . . . . . .  34\n       6.2.1.   Channel-Identifier . .
    . . . . . . . . . . . . . . .  35\n       6.2.2.   Accept . . . . . . . . . .
    . . . . . . . . . . . . .  36\n       6.2.3.   Active-Request-Id-List . . . .
    . . . . . . . . . . .  36\n       6.2.4.   Proxy-Sync-Id  . . . . . . . . . .
    . . . . . . . . .  36\n       6.2.5.   Accept-Charset . . . . . . . . . . . .
    . . . . . . .  37\n       6.2.6.   Content-Type . . . . . . . . . . . . . . .
    . . . . .  37\n       6.2.7.   Content-ID . . . . . . . . . . . . . . . . . .
    . . .  38\n       6.2.8.   Content-Base . . . . . . . . . . . . . . . . . . .
    .  38\n       6.2.9.   Content-Encoding . . . . . . . . . . . . . . . . . .  38\n
    \      6.2.10.  Content-Location . . . . . . . . . . . . . . . . . .  39\n       6.2.11.
    \ Content-Length . . . . . . . . . . . . . . . . . . .  39\n       6.2.12.  Fetch
    Timeout  . . . . . . . . . . . . . . . . . . .  39\n       6.2.13.  Cache-Control
    \ . . . . . . . . . . . . . . . . . . .  40\n       6.2.14.  Logging-Tag  . .
    . . . . . . . . . . . . . . . . . .  41\n       6.2.15.  Set-Cookie . . . . .
    . . . . . . . . . . . . . . . .  42\n       6.2.16.  Vendor-Specific Parameters
    . . . . . . . . . . . . .  44\n     6.3.   Generic Result Structure . . . . .
    . . . . . . . . . . .  44\n       6.3.1.   Natural Language Semantics Markup Language
    . . . . .  45\n   7.  Resource Discovery  . . . . . . . . . . . . . . . . . .
    . . .  46\n   8.  Speech Synthesizer Resource . . . . . . . . . . . . . . . .
    .  47\n     8.1.   Synthesizer State Machine  . . . . . . . . . . . . . . .  48\n
    \    8.2.   Synthesizer Methods  . . . . . . . . . . . . . . . . . .  48\n     8.3.
    \  Synthesizer Events . . . . . . . . . . . . . . . . . . .  49\n     8.4.   Synthesizer
    Header Fields  . . . . . . . . . . . . . . .  49\n       8.4.1.   Jump-Size  .
    . . . . . . . . . . . . . . . . . . . .  49\n       8.4.2.   Kill-On-Barge-In
    . . . . . . . . . . . . . . . . . .  50\n       8.4.3.   Speaker-Profile  . .
    . . . . . . . . . . . . . . . .  51\n       8.4.4.   Completion-Cause . . . .
    . . . . . . . . . . . . . .  51\n       8.4.5.   Completion-Reason  . . . . .
    . . . . . . . . . . . .  52\n       8.4.6.   Voice-Parameter  . . . . . . . .
    . . . . . . . . . .  52\n       8.4.7.   Prosody-Parameters . . . . . . . . .
    . . . . . . . .  53\n       8.4.8.   Speech-Marker  . . . . . . . . . . . . .
    . . . . . .  53\n       8.4.9.   Speech-Language  . . . . . . . . . . . . . .
    . . . .  54\n       8.4.10.  Fetch-Hint . . . . . . . . . . . . . . . . . . .
    . .  54\n       8.4.11.  Audio-Fetch-Hint . . . . . . . . . . . . . . . . . .
    \ 55\n       8.4.12.  Failed-URI . . . . . . . . . . . . . . . . . . . . .  55\n
    \      8.4.13.  Failed-URI-Cause . . . . . . . . . . . . . . . . . .  55\n       8.4.14.
    \ Speak-Restart  . . . . . . . . . . . . . . . . . . .  56\n       8.4.15.  Speak-Length
    . . . . . . . . . . . . . . . . . . . .  56\n       8.4.16.  Load-Lexicon . .
    . . . . . . . . . . . . . . . . . .  57\n       8.4.17.  Lexicon-Search-Order
    . . . . . . . . . . . . . . . .  57\n     8.5.   Synthesizer Message Body . .
    . . . . . . . . . . . . . .  57\n       8.5.1.   Synthesizer Speech Data  . .
    . . . . . . . . . . . .  57\n       8.5.2.   Lexicon Data . . . . . . . . . .
    . . . . . . . . . .  59\n     8.6.   SPEAK Method . . . . . . . . . . . . . .
    . . . . . . . .  60\n     8.7.   STOP . . . . . . . . . . . . . . . . . . . .
    . . . . . .  62\n     8.8.   BARGE-IN-OCCURRED  . . . . . . . . . . . . . . .
    . . . .  63\n     8.9.   PAUSE  . . . . . . . . . . . . . . . . . . . . . . .
    . .  65\n     8.10.  RESUME . . . . . . . . . . . . . . . . . . . . . . . . .
    \ 66\n     8.11.  CONTROL  . . . . . . . . . . . . . . . . . . . . . . . .  67\n
    \    8.12.  SPEAK-COMPLETE . . . . . . . . . . . . . . . . . . . . .  69\n     8.13.
    \ SPEECH-MARKER  . . . . . . . . . . . . . . . . . . . . .  70\n     8.14.  DEFINE-LEXICON
    . . . . . . . . . . . . . . . . . . . . .  71\n   9.  Speech Recognizer Resource
    \ . . . . . . . . . . . . . . . . .  72\n     9.1.   Recognizer State Machine
    . . . . . . . . . . . . . . . .  74\n     9.2.   Recognizer Methods . . . . .
    . . . . . . . . . . . . . .  74\n     9.3.   Recognizer Events  . . . . . . .
    . . . . . . . . . . . .  75\n     9.4.   Recognizer Header Fields . . . . . .
    . . . . . . . . . .  75\n       9.4.1.   Confidence-Threshold . . . . . . . .
    . . . . . . . .  77\n       9.4.2.   Sensitivity-Level  . . . . . . . . . . .
    . . . . . .  77\n       9.4.3.   Speed-Vs-Accuracy  . . . . . . . . . . . . .
    . . . .  77\n       9.4.4.   N-Best-List-Length . . . . . . . . . . . . . . .
    . .  78\n       9.4.5.   Input-Type . . . . . . . . . . . . . . . . . . . . .
    \ 78\n       9.4.6.   No-Input-Timeout . . . . . . . . . . . . . . . . . .  78\n
    \      9.4.7.   Recognition-Timeout  . . . . . . . . . . . . . . . .  79\n       9.4.8.
    \  Waveform-URI . . . . . . . . . . . . . . . . . . . .  79\n       9.4.9.   Media-Type
    . . . . . . . . . . . . . . . . . . . . .  80\n       9.4.10.  Input-Waveform-URI
    . . . . . . . . . . . . . . . . .  80\n       9.4.11.  Completion-Cause . . .
    . . . . . . . . . . . . . . .  80\n       9.4.12.  Completion-Reason  . . . .
    . . . . . . . . . . . . .  83\n       9.4.13.  Recognizer-Context-Block . . .
    . . . . . . . . . . .  83\n       9.4.14.  Start-Input-Timers . . . . . . . .
    . . . . . . . . .  83\n       9.4.15.  Speech-Complete-Timeout  . . . . . . .
    . . . . . . .  84\n       9.4.16.  Speech-Incomplete-Timeout  . . . . . . . .
    . . . . .  84\n       9.4.17.  DTMF-Interdigit-Timeout  . . . . . . . . . . .
    . . .  85\n       9.4.18.  DTMF-Term-Timeout  . . . . . . . . . . . . . . . .
    .  85\n       9.4.19.  DTMF-Term-Char . . . . . . . . . . . . . . . . . . .  85\n
    \      9.4.20.  Failed-URI . . . . . . . . . . . . . . . . . . . . .  86\n       9.4.21.
    \ Failed-URI-Cause . . . . . . . . . . . . . . . . . .  86\n       9.4.22.  Save-Waveform
    \ . . . . . . . . . . . . . . . . . . .  86\n       9.4.23.  New-Audio-Channel
    \ . . . . . . . . . . . . . . . . .  86\n       9.4.24.  Speech-Language  . .
    . . . . . . . . . . . . . . . .  87\n       9.4.25.  Ver-Buffer-Utterance . .
    . . . . . . . . . . . . . .  87\n       9.4.26.  Recognition-Mode . . . . . .
    . . . . . . . . . . . .  87\n       9.4.27.  Cancel-If-Queue  . . . . . . . .
    . . . . . . . . . .  88\n       9.4.28.  Hotword-Max-Duration . . . . . . . .
    . . . . . . . .  88\n       9.4.29.  Hotword-Min-Duration . . . . . . . . . .
    . . . . . .  88\n       9.4.30.  Interpret-Text . . . . . . . . . . . . . . .
    . . . .  89\n       9.4.31.  DTMF-Buffer-Time . . . . . . . . . . . . . . . .
    . .  89\n       9.4.32.  Clear-DTMF-Buffer  . . . . . . . . . . . . . . . . .
    \ 89\n       9.4.33.  Early-No-Match . . . . . . . . . . . . . . . . . . .  90\n
    \      9.4.34.  Num-Min-Consistent-Pronunciations  . . . . . . . . .  90\n       9.4.35.
    \ Consistency-Threshold  . . . . . . . . . . . . . . .  90\n       9.4.36.  Clash-Threshold
    \ . . . . . . . . . . . . . . . . . .  90\n       9.4.37.  Personal-Grammar-URI
    . . . . . . . . . . . . . . . .  91\n       9.4.38.  Enroll-Utterance . . . .
    . . . . . . . . . . . . . .  91\n       9.4.39.  Phrase-Id  . . . . . . . . .
    . . . . . . . . . . . .  91\n       9.4.40.  Phrase-NL  . . . . . . . . . . .
    . . . . . . . . . .  92\n       9.4.41.  Weight . . . . . . . . . . . . . . .
    . . . . . . . .  92\n       9.4.42.  Save-Best-Waveform . . . . . . . . . . .
    . . . . . .  92\n       9.4.43.  New-Phrase-Id  . . . . . . . . . . . . . . .
    . . . .  93\n       9.4.44.  Confusable-Phrases-URI . . . . . . . . . . . . .
    . .  93\n       9.4.45.  Abort-Phrase-Enrollment  . . . . . . . . . . . . . .
    \ 93\n     9.5.   Recognizer Message Body  . . . . . . . . . . . . . . . .  93\n
    \      9.5.1.   Recognizer Grammar Data  . . . . . . . . . . . . . .  93\n       9.5.2.
    \  Recognizer Result Data . . . . . . . . . . . . . . .  97\n       9.5.3.   Enrollment
    Result Data . . . . . . . . . . . . . . .  98\n       9.5.4.   Recognizer Context
    Block . . . . . . . . . . . . . .  98\n     9.6.   Recognizer Results . . . .
    . . . . . . . . . . . . . . .  99\n       9.6.1.   Markup Functions . . . . .
    . . . . . . . . . . . . .  99\n       9.6.2.   Overview of Recognizer Result Elements
    and Their\n                Relationships  . . . . . . . . . . . . . . . . . .
    . 100\n       9.6.3.   Elements and Attributes  . . . . . . . . . . . . . . 101\n
    \    9.7.   Enrollment Results . . . . . . . . . . . . . . . . . . . 106\n       9.7.1.
    \  <num-clashes> Element  . . . . . . . . . . . . . . . 106\n       9.7.2.   <num-good-repetitions>
    Element . . . . . . . . . . . 106\n       9.7.3.   <num-repetitions-still-needed>
    Element . . . . . . . 107\n       9.7.4.   <consistency-status> Element . . .
    . . . . . . . . . 107\n       9.7.5.   <clash-phrase-ids> Element . . . . . .
    . . . . . . . 107\n       9.7.6.   <transcriptions> Element . . . . . . . . .
    . . . . . 107\n       9.7.7.   <confusable-phrases> Element . . . . . . . . .
    . . . 107\n     9.8.   DEFINE-GRAMMAR . . . . . . . . . . . . . . . . . . . .
    . 107\n     9.9.   RECOGNIZE  . . . . . . . . . . . . . . . . . . . . . . . 111\n
    \    9.10.  STOP . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n     9.11.
    \ GET-RESULT . . . . . . . . . . . . . . . . . . . . . . . 119\n     9.12.  START-OF-INPUT
    . . . . . . . . . . . . . . . . . . . . . 120\n     9.13.  START-INPUT-TIMERS
    . . . . . . . . . . . . . . . . . . . 120\n     9.14.  RECOGNITION-COMPLETE .
    . . . . . . . . . . . . . . . . . 120\n     9.15.  START-PHRASE-ENROLLMENT  .
    . . . . . . . . . . . . . . . 123\n     9.16.  ENROLLMENT-ROLLBACK  . . . . .
    . . . . . . . . . . . . . 124\n     9.17.  END-PHRASE-ENROLLMENT  . . . . . .
    . . . . . . . . . . . 124\n     9.18.  MODIFY-PHRASE  . . . . . . . . . . . .
    . . . . . . . . . 125\n     9.19.  DELETE-PHRASE  . . . . . . . . . . . . . .
    . . . . . . . 125\n     9.20.  INTERPRET  . . . . . . . . . . . . . . . . . .
    . . . . . 125\n     9.21.  INTERPRETATION-COMPLETE  . . . . . . . . . . . . .
    . . . 127\n     9.22.  DTMF Detection . . . . . . . . . . . . . . . . . . . .
    . 128\n   10. Recorder Resource . . . . . . . . . . . . . . . . . . . . . . 129\n
    \    10.1.  Recorder State Machine . . . . . . . . . . . . . . . . . 129\n     10.2.
    \ Recorder Methods . . . . . . . . . . . . . . . . . . . . 130\n     10.3.  Recorder
    Events  . . . . . . . . . . . . . . . . . . . . 130\n     10.4.  Recorder Header
    Fields . . . . . . . . . . . . . . . . . 130\n       10.4.1.  Sensitivity-Level
    \ . . . . . . . . . . . . . . . . . 130\n       10.4.2.  No-Input-Timeout . .
    . . . . . . . . . . . . . . . . 131\n       10.4.3.  Completion-Cause . . . .
    . . . . . . . . . . . . . . 131\n       10.4.4.  Completion-Reason  . . . . .
    . . . . . . . . . . . . 132\n       10.4.5.  Failed-URI . . . . . . . . . . .
    . . . . . . . . . . 132\n       10.4.6.  Failed-URI-Cause . . . . . . . . . .
    . . . . . . . . 132\n       10.4.7.  Record-URI . . . . . . . . . . . . . . .
    . . . . . . 132\n       10.4.8.  Media-Type . . . . . . . . . . . . . . . . .
    . . . . 133\n       10.4.9.  Max-Time . . . . . . . . . . . . . . . . . . . .
    . . 133\n       10.4.10. Trim-Length  . . . . . . . . . . . . . . . . . . . .
    134\n       10.4.11. Final-Silence  . . . . . . . . . . . . . . . . . . . 134\n
    \      10.4.12. Capture-On-Speech  . . . . . . . . . . . . . . . . . 134\n       10.4.13.
    Ver-Buffer-Utterance . . . . . . . . . . . . . . . . 134\n       10.4.14. Start-Input-Timers
    . . . . . . . . . . . . . . . . . 135\n       10.4.15. New-Audio-Channel  . .
    . . . . . . . . . . . . . . . 135\n     10.5.  Recorder Message Body  . . . .
    . . . . . . . . . . . . . 135\n     10.6.  RECORD . . . . . . . . . . . . . .
    . . . . . . . . . . . 135\n     10.7.  STOP . . . . . . . . . . . . . . . . .
    . . . . . . . . . 136\n     10.8.  RECORD-COMPLETE  . . . . . . . . . . . . .
    . . . . . . . 137\n     10.9.  START-INPUT-TIMERS . . . . . . . . . . . . . .
    . . . . . 138\n     10.10. START-OF-INPUT . . . . . . . . . . . . . . . . . .
    . . . 138\n   11. Speaker Verification and Identification . . . . . . . . . .
    . 139\n     11.1.  Speaker Verification State Machine . . . . . . . . . . . 140\n
    \    11.2.  Speaker Verification Methods . . . . . . . . . . . . . . 142\n     11.3.
    \ Verification Events  . . . . . . . . . . . . . . . . . . 144\n     11.4.  Verification
    Header Fields . . . . . . . . . . . . . . . 144\n       11.4.1.  Repository-URI
    . . . . . . . . . . . . . . . . . . . 144\n       11.4.2.  Voiceprint-Identifier
    \ . . . . . . . . . . . . . . . 145\n       11.4.3.  Verification-Mode  . . .
    . . . . . . . . . . . . . . 145\n       11.4.4.  Adapt-Model  . . . . . . . .
    . . . . . . . . . . . . 146\n       11.4.5.  Abort-Model  . . . . . . . . . .
    . . . . . . . . . . 146\n       11.4.6.  Min-Verification-Score . . . . . . .
    . . . . . . . . 147\n       11.4.7.  Num-Min-Verification-Phrases . . . . . .
    . . . . . . 147\n       11.4.8.  Num-Max-Verification-Phrases . . . . . . . .
    . . . . 147\n       11.4.9.  No-Input-Timeout . . . . . . . . . . . . . . . .
    . . 148\n       11.4.10. Save-Waveform  . . . . . . . . . . . . . . . . . . .
    148\n       11.4.11. Media-Type . . . . . . . . . . . . . . . . . . . . . 148\n
    \      11.4.12. Waveform-URI . . . . . . . . . . . . . . . . . . . . 148\n       11.4.13.
    Voiceprint-Exists  . . . . . . . . . . . . . . . . . 149\n       11.4.14. Ver-Buffer-Utterance
    . . . . . . . . . . . . . . . . 149\n       11.4.15. Input-Waveform-URI . . .
    . . . . . . . . . . . . . . 149\n       11.4.16. Completion-Cause . . . . . .
    . . . . . . . . . . . . 150\n       11.4.17. Completion-Reason  . . . . . . .
    . . . . . . . . . . 151\n       11.4.18. Speech-Complete-Timeout  . . . . . .
    . . . . . . . . 151\n       11.4.19. New-Audio-Channel  . . . . . . . . . . .
    . . . . . . 152\n       11.4.20. Abort-Verification . . . . . . . . . . . . .
    . . . . 152\n       11.4.21. Start-Input-Timers . . . . . . . . . . . . . . .
    . . 152\n     11.5.  Verification Message Body  . . . . . . . . . . . . . . .
    152\n       11.5.1.  Verification Result Data . . . . . . . . . . . . . . 152\n
    \      11.5.2.  Verification Result Elements . . . . . . . . . . . . 153\n     11.6.
    \ START-SESSION  . . . . . . . . . . . . . . . . . . . . . 157\n     11.7.  END-SESSION
    \ . . . . . . . . . . . . . . . . . . . . . . 158\n     11.8.  QUERY-VOICEPRINT
    . . . . . . . . . . . . . . . . . . . . 159\n     11.9.  DELETE-VOICEPRINT  .
    . . . . . . . . . . . . . . . . . . 160\n     11.10. VERIFY . . . . . . . . .
    . . . . . . . . . . . . . . . . 160\n     11.11. VERIFY-FROM-BUFFER . . . . .
    . . . . . . . . . . . . . . 160\n     11.12. VERIFY-ROLLBACK  . . . . . . . .
    . . . . . . . . . . . . 164\n     11.13. STOP . . . . . . . . . . . . . . . .
    . . . . . . . . . . 164\n     11.14. START-INPUT-TIMERS . . . . . . . . . . .
    . . . . . . . . 165\n     11.15. VERIFICATION-COMPLETE  . . . . . . . . . . .
    . . . . . . 165\n     11.16. START-OF-INPUT . . . . . . . . . . . . . . . . .
    . . . . 166\n     11.17. CLEAR-BUFFER . . . . . . . . . . . . . . . . . . . .
    . . 166\n     11.18. GET-INTERMEDIATE-RESULT  . . . . . . . . . . . . . . . .
    167\n   12. Security Considerations . . . . . . . . . . . . . . . . . . . 168\n
    \    12.1.  Rendezvous and Session Establishment . . . . . . . . . . 168\n     12.2.
    \ Control Channel Protection . . . . . . . . . . . . . . . 168\n     12.3.  Media
    Session Protection . . . . . . . . . . . . . . . . 169\n     12.4.  Indirect Content
    Access  . . . . . . . . . . . . . . . . 169\n     12.5.  Protection of Stored
    Media . . . . . . . . . . . . . . . 170\n     12.6.  DTMF and Recognition Buffers
    . . . . . . . . . . . . . . 171\n     12.7.  Client-Set Server Parameters . .
    . . . . . . . . . . . . 171\n     12.8.  DELETE-VOICEPRINT and Authorization  .
    . . . . . . . . . 171\n   13. IANA Considerations . . . . . . . . . . . . . .
    . . . . . . . 171\n     13.1.  New Registries . . . . . . . . . . . . . . . .
    . . . . . 171\n       13.1.1.  MRCPv2 Resource Types  . . . . . . . . . . . .
    . . . 171\n       13.1.2.  MRCPv2 Methods and Events  . . . . . . . . . . . .
    . 172\n       13.1.3.  MRCPv2 Header Fields . . . . . . . . . . . . . . . . 173\n
    \      13.1.4.  MRCPv2 Status Codes  . . . . . . . . . . . . . . . . 176\n       13.1.5.
    \ Grammar Reference List Parameters  . . . . . . . . . 176\n       13.1.6.  MRCPv2
    Vendor-Specific Parameters  . . . . . . . . . 176\n     13.2.  NLSML-Related Registrations
    \ . . . . . . . . . . . . . . 177\n       13.2.1.  'application/nlsml+xml' Media
    Type Registration  . . 177\n     13.3.  NLSML XML Schema Registration  . . . .
    . . . . . . . . . 178\n     13.4.  MRCPv2 XML Namespace Registration  . . . .
    . . . . . . . 178\n     13.5.  Text Media Type Registrations  . . . . . . . .
    . . . . . 178\n       13.5.1.  text/grammar-ref-list  . . . . . . . . . . . .
    . . . 178\n     13.6.  'session' URI Scheme Registration  . . . . . . . . . .
    . 180\n     13.7.  SDP Parameter Registrations  . . . . . . . . . . . . . . 181\n
    \      13.7.1.  Sub-Registry \"proto\" . . . . . . . . . . . . . . . . 181\n       13.7.2.
    \ Sub-Registry \"att-field (media-level)\" . . . . . . . 182\n   14. Examples
    \ . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n     14.1.  Message
    Flow . . . . . . . . . . . . . . . . . . . . . . 183\n     14.2.  Recognition
    Result Examples  . . . . . . . . . . . . . . 192\n       14.2.1.  Simple ASR Ambiguity
    . . . . . . . . . . . . . . . . 192\n       14.2.2.  Mixed Initiative . . . .
    . . . . . . . . . . . . . . 192\n       14.2.3.  DTMF Input . . . . . . . . .
    . . . . . . . . . . . . 193\n       14.2.4.  Interpreting Meta-Dialog and Meta-Task
    Utterances  . 194\n       14.2.5.  Anaphora and Deixis  . . . . . . . . . . .
    . . . . . 195\n       14.2.6.  Distinguishing Individual Items from Sets with\n
    \               One Member . . . . . . . . . . . . . . . . . . . . . 195\n       14.2.7.
    \ Extensibility  . . . . . . . . . . . . . . . . . . . 196\n   15. ABNF Normative
    Definition . . . . . . . . . . . . . . . . . . 196\n   16. XML Schemas . . . .
    . . . . . . . . . . . . . . . . . . . . . 211\n     16.1.  NLSML Schema Definition
    \ . . . . . . . . . . . . . . . . 211\n     16.2.  Enrollment Results Schema Definition
    . . . . . . . . . . 213\n     16.3.  Verification Results Schema Definition .
    . . . . . . . . 214\n   17. References  . . . . . . . . . . . . . . . . . . .
    . . . . . . 218\n     17.1.  Normative References . . . . . . . . . . . . . .
    . . . . 218\n     17.2.  Informative References . . . . . . . . . . . . . . .
    . . 220\n   Appendix A.  Contributors . . . . . . . . . . . . . . . . . . . .
    223\n   Appendix B.  Acknowledgements . . . . . . . . . . . . . . . . . . 223\n"
  title: Table of Contents
- contents:
  - "1.  Introduction\n   MRCPv2 is designed to allow a client device to control media\n
    \  processing resources on the network.  Some of these media processing\n   resources
    include speech recognition engines, speech synthesis\n   engines, speaker verification,
    and speaker identification engines.\n   MRCPv2 enables the implementation of distributed
    Interactive Voice\n   Response platforms using VoiceXML [W3C.REC-voicexml20-20040316]\n
    \  browsers or other client applications while maintaining separate\n   back-end
    speech processing capabilities on specialized speech\n   processing servers.  MRCPv2
    is based on the earlier Media Resource\n   Control Protocol (MRCP) [RFC4463] developed
    jointly by Cisco Systems,\n   Inc., Nuance Communications, and Speechworks, Inc.
    \ Although some of\n   the method names are similar, the way in which these methods
    are\n   communicated is different.  There are also more resources and more\n   methods
    for each resource.  The first version of MRCP was essentially\n   taken only as
    input to the development of this protocol.  There is no\n   expectation that an
    MRCPv2 client will work with an MRCPv1 server or\n   vice versa.  There is no
    migration plan or gateway definition between\n   the two protocols.\n   The protocol
    requirements of Speech Services Control (SPEECHSC)\n   [RFC4313] include that
    the solution be capable of reaching a media\n   processing server, setting up
    communication channels to the media\n   resources, and sending and receiving control
    messages and media\n   streams to/from the server.  The Session Initiation Protocol
    (SIP)\n   [RFC3261] meets these requirements.\n   The proprietary version of MRCP
    ran over the Real Time Streaming\n   Protocol (RTSP) [RFC2326].  At the time work
    on MRCPv2 was begun, the\n   consensus was that this use of RTSP would break the
    RTSP protocol or\n   cause backward-compatibility problems, something forbidden
    by Section\n   3.2 of [RFC4313].  This is the reason why MRCPv2 does not run over\n
    \  RTSP.\n   MRCPv2 leverages these capabilities by building upon SIP and the\n
    \  Session Description Protocol (SDP) [RFC4566].  MRCPv2 uses SIP to set\n   up
    and tear down media and control sessions with the server.  In\n   addition, the
    client can use a SIP re-INVITE method (an INVITE dialog\n   sent within an existing
    SIP session) to change the characteristics of\n   these media and control session
    while maintaining the SIP dialog\n   between the client and server.  SDP is used
    to describe the\n   parameters of the media sessions associated with that dialog.
    \ It is\n   mandatory to support SIP as the session establishment protocol to\n
    \  ensure interoperability.  Other protocols can be used for session\n   establishment
    by prior agreement.  This document only describes the\n   use of SIP and SDP.\n
    \  MRCPv2 uses SIP and SDP to create the speech client/server dialog and\n   set
    up the media channels to the server.  It also uses SIP and SDP to\n   establish
    MRCPv2 control sessions between the client and the server\n   for each media processing
    resource required for that dialog.  The\n   MRCPv2 protocol exchange between the
    client and the media resource is\n   carried on that control session.  MRCPv2
    exchanges do not change the\n   state of the SIP dialog, the media sessions, or
    other parameters of\n   the dialog initiated via SIP.  It controls and affects
    the state of\n   the media processing resource associated with the MRCPv2 session(s).\n
    \  MRCPv2 defines the messages to control the different media processing\n   resources
    and the state machines required to guide their operation.\n   It also describes
    how these messages are carried over a transport-\n   layer protocol such as the
    Transmission Control Protocol (TCP)\n   [RFC0793] or the Transport Layer Security
    (TLS) Protocol [RFC5246].\n   (Note: the Stream Control Transmission Protocol
    (SCTP) [RFC4960] is a\n   viable transport for MRCPv2 as well, but the mapping
    onto SCTP is not\n   described in this specification.)\n"
  title: 1.  Introduction
- contents:
  - "2.  Document Conventions\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\",
    \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\",
    and \"OPTIONAL\" in this\n   document are to be interpreted as described in RFC
    2119 [RFC2119].\n   Since many of the definitions and syntax are identical to
    those for\n   the Hypertext Transfer Protocol -- HTTP/1.1 [RFC2616], this\n   specification
    refers to the section where they are defined rather\n   than copying it.  For
    brevity, [HX.Y] is to be taken to refer to\n   Section X.Y of RFC 2616.\n   All
    the mechanisms specified in this document are described in both\n   prose and
    an augmented Backus-Naur form (ABNF [RFC5234]).\n   The complete message format
    in ABNF form is provided in Section 15\n   and is the normative format definition.
    \ Note that productions may be\n   duplicated within the main body of the document
    for reading\n   convenience.  If a production in the body of the text conflicts
    with\n   one in the normative definition, the latter rules.\n"
  - contents:
    - "2.1.  Definitions\n   Media Resource\n                  An entity on the speech
      processing server that can be\n                  controlled through MRCPv2.\n
      \  MRCP Server\n                  Aggregate of one or more \"Media Resource\"
      entities on\n                  a server, exposed through MRCPv2.  Often, 'server'
      in\n                  this document refers to an MRCP server.\n   MRCP Client\n
      \                 An entity controlling one or more Media Resources\n                  through
      MRCPv2 (\"Client\" for short).\n   DTMF\n                  Dual-Tone Multi-Frequency;
      a method of transmitting\n                  key presses in-band, either as actual
      tones (Q.23\n                  [Q.23]) or as named tone events (RFC 4733 [RFC4733]).\n
      \  Endpointing\n                  The process of automatically detecting the
      beginning\n                  and end of speech in an audio stream.  This is\n
      \                 critical both for speech recognition and for automated\n                  recording
      as one would find in voice mail systems.\n   Hotword Mode\n                  A
      mode of speech recognition where a stream of\n                  utterances is
      evaluated for match against a small set\n                  of command words.
      \ This is generally employed either\n                  to trigger some action
      or to control the subsequent\n                  grammar to be used for further
      recognition.\n"
    title: 2.1.  Definitions
  - contents:
    - "2.2.  State-Machine Diagrams\n   The state-machine diagrams in this document
      do not show every\n   possible method call.  Rather, they reflect the state
      of the resource\n   based on the methods that have moved to IN-PROGRESS or COMPLETE\n
      \  states (see Section 5.3).  Note that since PENDING requests\n   essentially
      have not affected the resource yet and are in the queue\n   to be processed,
      they are not reflected in the state-machine\n   diagrams.\n"
    title: 2.2.  State-Machine Diagrams
  - contents:
    - "2.3.  URI Schemes\n   This document defines many protocol headers that contain
      URIs\n   (Uniform Resource Identifiers [RFC3986]) or lists of URIs for\n   referencing
      media.  The entire document, including the Security\n   Considerations section
      (Section 12), assumes that HTTP or HTTP over\n   TLS (HTTPS) [RFC2818] will
      be used as the URI addressing scheme\n   unless otherwise stated.  However,
      implementations MAY support other\n   schemes (such as 'file'), provided they
      have addressed any security\n   considerations described in this document and
      any others particular\n   to the specific scheme.  For example, implementations
      where the\n   client and server both reside on the same physical hardware and
      the\n   file system is secured by traditional user-level file access controls\n
      \  could be reasonable candidates for supporting the 'file' scheme.\n"
    title: 2.3.  URI Schemes
  title: 2.  Document Conventions
- contents:
  - "3.  Architecture\n   A system using MRCPv2 consists of a client that requires
    the\n   generation and/or consumption of media streams and a media resource\n
    \  server that has the resources or \"engines\" to process these streams\n   as
    input or generate these streams as output.  The client uses SIP\n   and SDP to
    establish an MRCPv2 control channel with the server to use\n   its media processing
    resources.  MRCPv2 servers are addressed using\n   SIP URIs.\n   SIP uses SDP
    with the offer/answer model described in RFC 3264\n   [RFC3264] to set up the
    MRCPv2 control channels and describe their\n   characteristics.  A separate MRCPv2
    session is needed to control each\n   of the media processing resources associated
    with the SIP dialog\n   between the client and server.  Within a SIP dialog, the
    individual\n   resource control channels for the different resources are added
    or\n   removed through SDP offer/answer carried in a SIP re-INVITE\n   transaction.\n
    \  The server, through the SDP exchange, provides the client with a\n   difficult-to-guess,
    unambiguous channel identifier and a TCP port\n   number (see Section 4.2).  The
    client MAY then open a new TCP\n   connection with the server on this port number.
    \ Multiple MRCPv2\n   channels can share a TCP connection between the client and
    the\n   server.  All MRCPv2 messages exchanged between the client and the\n   server
    carry the specified channel identifier that the server MUST\n   ensure is unambiguous
    among all MRCPv2 control channels that are\n   active on that server.  The client
    uses this channel identifier to\n   indicate the media processing resource associated
    with that channel.\n   For information on message framing, see Section 5.\n   SIP
    also establishes the media sessions between the client (or other\n   source/sink
    of media) and the MRCPv2 server using SDP \"m=\" lines.\n   One or more media
    processing resources may share a media session\n   under a SIP session, or each
    media processing resource may have its\n   own media session.\n   The following
    diagram shows the general architecture of a system that\n   uses MRCPv2.  To simplify
    the diagram, only a few resources are\n   shown.\n     MRCPv2 client                   MRCPv2
    Media Resource Server\n"
  - '|--------------------|            |------------------------------------|

    '
  - '||------------------||            ||----------------------------------||

    '
  - '|| Application Layer||            ||Synthesis|Recognition|Verification||

    '
  - '||------------------||            || Engine  |  Engine   |   Engine   ||

    '
  - '||Media Resource API||            ||    ||   |    ||     |    ||      ||

    '
  - '||------------------||            ||Synthesis|Recognizer |  Verifier  ||

    '
  - '|| SIP  |  MRCPv2   ||            ||Resource | Resource  |  Resource  ||

    '
  - '||Stack |           ||            ||     Media Resource Management    ||

    '
  - '||      |           ||            ||----------------------------------||

    '
  - '||------------------||            ||   SIP  |        MRCPv2           ||

    '
  - '||   TCP/IP Stack   ||---MRCPv2---||  Stack |                         ||

    '
  - '||                  ||            ||----------------------------------||

    '
  - '||------------------||----SIP-----||           TCP/IP Stack           ||

    '
  - "|--------------------|            ||                                  ||\n         |
    \                       ||----------------------------------||\n        SIP                       |------------------------------------|\n
    \        |                          /\n"
  - '|-------------------|             RTP

    '
  - '|                   |             /

    '
  - '| Media Source/Sink |------------/

    '
  - '|                   |

    '
  - "|-------------------|\n                      Figure 1: Architectural Diagram\n"
  - contents:
    - "3.1.  MRCPv2 Media Resource Types\n   An MRCPv2 server may offer one or more
      of the following media\n   processing resources to its clients.\n   Basic Synthesizer\n
      \                 A speech synthesizer resource that has very limited\n                  capabilities
      and can generate its media stream\n                  exclusively from concatenated
      audio clips.  The speech\n                  data is described using a limited
      subset of the Speech\n                  Synthesis Markup Language (SSML)\n                  [W3C.REC-speech-synthesis-20040907]
      elements.  A basic\n                  synthesizer MUST support the SSML tags
      <speak>,\n                  <audio>, <say-as>, and <mark>.\n   Speech Synthesizer\n
      \                 A full-capability speech synthesis resource that can\n                  render
      speech from text.  Such a synthesizer MUST have\n                  full SSML
      [W3C.REC-speech-synthesis-20040907] support.\n   Recorder\n                  A
      resource capable of recording audio and providing a\n                  URI pointer
      to the recording.  A recorder MUST provide\n                  endpointing capabilities
      for suppressing silence at\n                  the beginning and end of a recording,
      and MAY also\n                  suppress silence in the middle of a recording.
      \ If\n                  such suppression is done, the recorder MUST maintain\n
      \                 timing metadata to indicate the actual timestamps of\n                  the
      recorded media.\n   DTMF Recognizer\n                  A recognizer resource
      capable of extracting and\n                  interpreting Dual-Tone Multi-Frequency
      (DTMF) [Q.23]\n                  digits in a media stream and matching them
      against a\n                  supplied digit grammar.  It could also do a semantic\n
      \                 interpretation based on semantic tags in the grammar.\n   Speech
      Recognizer\n                  A full speech recognition resource that is capable
      of\n                  receiving a media stream containing audio and\n                  interpreting
      it to recognition results.  It also has a\n                  natural language
      semantic interpreter to post-process\n                  the recognized data
      according to the semantic data in\n                  the grammar and provide
      semantic results along with\n                  the recognized input.  The recognizer
      MAY also support\n                  enrolled grammars, where the client can
      enroll and\n                  create new personal grammars for use in future\n
      \                 recognition operations.\n   Speaker Verifier\n                  A
      resource capable of verifying the authenticity of a\n                  claimed
      identity by matching a media stream containing\n                  spoken input
      to a pre-existing voiceprint.  This may\n                  also involve matching
      the caller's voice against more\n                  than one voiceprint, also
      called multi-verification or\n                  speaker identification.\n"
    title: 3.1.  MRCPv2 Media Resource Types
  - contents:
    - "3.2.  Server and Resource Addressing\n   The MRCPv2 server is a generic SIP
      server, and is thus addressed by a\n   SIP URI (RFC 3261 [RFC3261]).\n   For
      example:\n        sip:mrcpv2@example.net   or\n        sips:mrcpv2@example.net\n"
    title: 3.2.  Server and Resource Addressing
  title: 3.  Architecture
- contents:
  - "4.  MRCPv2 Basics\n   MRCPv2 requires a connection-oriented transport-layer protocol
    such\n   as TCP to guarantee reliable sequencing and delivery of MRCPv2\n   control
    messages between the client and the server.  In order to meet\n   the requirements
    for security enumerated in SPEECHSC requirements\n   [RFC4313], clients and servers
    MUST implement TLS as well.  One or\n   more connections between the client and
    the server can be shared\n   among different MRCPv2 channels to the server.  The
    individual\n   messages carry the channel identifier to differentiate messages
    on\n   different channels.  MRCPv2 encoding is text based with mechanisms to\n
    \  carry embedded binary data.  This allows arbitrary data like\n   recognition
    grammars, recognition results, synthesizer speech markup,\n   etc., to be carried
    in MRCPv2 messages.  For information on message\n   framing, see Section 5.\n"
  - contents:
    - "4.1.  Connecting to the Server\n   MRCPv2 employs SIP, in conjunction with
      SDP, as the session\n   establishment and management protocol.  The client reaches
      an MRCPv2\n   server using conventional INVITE and other SIP requests for\n
      \  establishing, maintaining, and terminating SIP dialogs.  The SDP\n   offer/answer
      exchange model over SIP is used to establish a resource\n   control channel
      for each resource.  The SDP offer/answer exchange is\n   also used to establish
      media sessions between the server and the\n   source or sink of audio.\n"
    title: 4.1.  Connecting to the Server
  - contents:
    - "4.2.  Managing Resource Control Channels\n   The client needs a separate MRCPv2
      resource control channel to\n   control each media processing resource under
      the SIP dialog.  A\n   unique channel identifier string identifies these resource
      control\n   channels.  The channel identifier is a difficult-to-guess,\n   unambiguous
      string followed by an \"@\", then by a string token\n   specifying the type
      of resource.  The server generates the channel\n   identifier and MUST make
      sure it does not clash with the identifier\n   of any other MRCP channel currently
      allocated by that server.  MRCPv2\n   defines the following IANA-registered
      types of media processing\n   resources.  Additional resource types and their
      associated methods/\n   events and state machines may be added as described
      below in\n   Section 13.\n          +---------------+----------------------+--------------+\n
      \         | Resource Type | Resource Description | Described in |\n          +---------------+----------------------+--------------+\n
      \         | speechrecog   | Speech Recognizer    | Section 9    |\n          |
      dtmfrecog     | DTMF Recognizer      | Section 9    |\n          | speechsynth
      \  | Speech Synthesizer   | Section 8    |\n          | basicsynth    | Basic
      Synthesizer    | Section 8    |\n          | speakverify   | Speaker Verification
      | Section 11   |\n          | recorder      | Speech Recorder      | Section
      10   |\n          +---------------+----------------------+--------------+\n
      \                         Table 1: Resource Types\n   The SIP INVITE or re-INVITE
      transaction and the SDP offer/answer\n   exchange it carries contain \"m=\"
      lines describing the resource\n   control channel to be allocated.  There MUST
      be one SDP \"m=\" line for\n   each MRCPv2 resource to be used in the session.
      \ This \"m=\" line MUST\n   have a media type field of \"application\" and a
      transport type field\n   of either \"TCP/MRCPv2\" or \"TCP/TLS/MRCPv2\".  The
      port number field of\n   the \"m=\" line MUST contain the \"discard\" port of
      the transport\n   protocol (port 9 for TCP) in the SDP offer from the client
      and MUST\n   contain the TCP listen port on the server in the SDP answer.  The\n
      \  client may then either set up a TCP or TLS connection to that server\n   port
      or share an already established connection to that port.  Since\n   MRCPv2 allows
      multiple sessions to share the same TCP connection,\n   multiple \"m=\" lines
      in a single SDP document MAY share the same port\n   field value; MRCPv2 servers
      MUST NOT assume any relationship between\n   resources using the same port other
      than the sharing of the\n   communication channel.\n   MRCPv2 resources do not
      use the port or format field of the \"m=\" line\n   to distinguish themselves
      from other resources using the same\n   channel.  The client MUST specify the
      resource type identifier in the\n   resource attribute associated with the control
      \"m=\" line of the SDP\n   offer.  The server MUST respond with the full Channel-Identifier\n
      \  (which includes the resource type identifier and a difficult-to-\n   guess,
      unambiguous string) in the \"channel\" attribute associated with\n   the control
      \"m=\" line of the SDP answer.  To remain backwards\n   compatible with conventional
      SDP usage, the format field of the \"m=\"\n   line MUST have the arbitrarily
      selected value of \"1\".\n   When the client wants to add a media processing
      resource to the\n   session, it issues a new SDP offer, according to the procedures
      of\n   RFC 3264 [RFC3264], in a SIP re-INVITE request.  The SDP offer/answer\n
      \  exchange carried by this SIP transaction contains one or more\n   additional
      control \"m=\" lines for the new resources to be allocated\n   to the session.
      \ The server, on seeing the new \"m=\" line, allocates\n   the resources (if
      they are available) and responds with a\n   corresponding control \"m=\" line
      in the SDP answer carried in the SIP\n   response.  If the new resources are
      not available, the re-INVITE\n   receives an error message, and existing media
      processing going on\n   before the re-INVITE will continue as it was before.
      \ It is not\n   possible to allocate more than one resource of each type.  If
      a\n   client requests more than one resource of any type, the server MUST\n
      \  behave as if the resources of that type (beyond the first one) are\n   not
      available.\n   MRCPv2 clients and servers using TCP as a transport protocol
      MUST use\n   the procedures specified in RFC 4145 [RFC4145] for setting up the
      TCP\n   connection, with the considerations described hereby.  Similarly,\n
      \  MRCPv2 clients and servers using TCP/TLS as a transport protocol MUST\n   use
      the procedures specified in RFC 4572 [RFC4572] for setting up the\n   TLS connection,
      with the considerations described hereby.  The\n   a=setup attribute, as described
      in RFC 4145 [RFC4145], MUST be\n   \"active\" for the offer from the client
      and MUST be \"passive\" for the\n   answer from the MRCPv2 server.  The a=connection
      attribute MUST have\n   a value of \"new\" on the very first control \"m=\"
      line offer from the\n   client to an MRCPv2 server.  Subsequent control \"m=\"
      line offers from\n   the client to the MRCP server MAY contain \"new\" or \"existing\",\n
      \  depending on whether the client wants to set up a new connection or\n   share
      an existing connection, respectively.  If the client specifies\n   a value of
      \"new\", the server MUST respond with a value of \"new\".  If\n   the client
      specifies a value of \"existing\", the server MUST respond.\n   The legal values
      in the response are \"existing\" if the server prefers\n   to share an existing
      connection or \"new\" if not.  In the latter case,\n   the client MUST initiate
      a new transport connection.\n   When the client wants to deallocate the resource
      from this session,\n   it issues a new SDP offer, according to RFC 3264 [RFC3264],
      where the\n   control \"m=\" line port MUST be set to 0.  This SDP offer is
      sent in a\n   SIP re-INVITE request.  This deallocates the associated MRCPv2\n
      \  identifier and resource.  The server MUST NOT close the TCP or TLS\n   connection
      if it is currently being shared among multiple MRCP\n   channels.  When all
      MRCP channels that may be sharing the connection\n   are released and/or the
      associated SIP dialog is terminated, the\n   client or server terminates the
      connection.\n   When the client wants to tear down the whole session and all
      its\n   resources, it MUST issue a SIP BYE request to close the SIP session.\n
      \  This will deallocate all the control channels and resources allocated\n   under
      the session.\n   All servers MUST support TLS.  Servers MAY use TCP without
      TLS in\n   controlled environments (e.g., not in the public Internet) where
      both\n   nodes are inside a protected perimeter, for example, preventing\n   access
      to the MRCP server from remote nodes outside the controlled\n   perimeter.  It
      is up to the client, through the SDP offer, to choose\n   which transport it
      wants to use for an MRCPv2 session.  Aside from\n   the exceptions given above,
      when using TCP, the \"m=\" lines MUST\n   conform to RFC4145 [RFC4145], which
      describes the usage of SDP for\n   connection-oriented transport.  When using
      TLS, the SDP \"m=\" line for\n   the control stream MUST conform to Connection-Oriented
      Media\n   (COMEDIA) over TLS [RFC4572], which specifies the usage of SDP for\n
      \  establishing a secure connection-oriented transport over TLS.\n"
    title: 4.2.  Managing Resource Control Channels
  - contents:
    - "4.3.  SIP Session Example\n   This first example shows the power of using SIP
      to route to the\n   appropriate resource.  In the example, note the use of a
      request to a\n   domain's speech server service in the INVITE to\n   mresources@example.com.
      \ The SIP routing machinery in the domain\n   locates the actual server, mresources@server.example.com,
      which gets\n   returned in the 200 OK.  Note that \"cmid\" is defined in Section
      4.4.\n   This example exchange adds a resource control channel for a\n   synthesizer.
      \ Since a synthesizer also generates an audio stream,\n   this interaction also
      creates a receive-only Real-Time Protocol (RTP)\n   [RFC3550] media session
      for the server to send audio to.  The SIP\n   dialog with the media source/sink
      is independent of MRCP and is not\n   shown.\n   C->S:  INVITE sip:mresources@example.com
      SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bf1\n
      \         Max-Forwards:6\n          To:MediaServer <sip:mresources@example.com>\n
      \         From:sarvi <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n
      \         CSeq:314161 INVITE\n          Contact:<sip:sarvi@client.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=sarvi 2890844526 2890844526 IN IP4 192.0.2.12\n          s=-\n          c=IN
      IP4 192.0.2.12\n          t=0 0\n          m=application 9 TCP/MRCPv2 1\n          a=setup:active\n
      \         a=connection:new\n          a=resource:speechsynth\n          a=cmid:1\n
      \         m=audio 49170 RTP/AVP 0\n          a=rtpmap:0 pcmu/8000\n          a=recvonly\n
      \         a=mid:1\n   S->C:  SIP/2.0 200 OK\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n
      \          branch=z9hG4bK74bf1;received=192.0.32.10\n          To:MediaServer
      <sip:mresources@example.com>;tag=62784\n          From:sarvi <sip:sarvi@example.com>;tag=1928301774\n
      \         Call-ID:a84b4c76e66710\n          CSeq:314161 INVITE\n          Contact:<sip:mresources@server.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=- 2890842808 2890842808 IN IP4 192.0.2.11\n          s=-\n          c=IN
      IP4 192.0.2.11\n          t=0 0\n          m=application 32416 TCP/MRCPv2 1\n
      \         a=setup:passive\n          a=connection:new\n          a=channel:32AECB234338@speechsynth\n
      \         a=cmid:1\n          m=audio 48260 RTP/AVP 0\n          a=rtpmap:0
      pcmu/8000\n          a=sendonly\n          a=mid:1\n   C->S:  ACK sip:mresources@server.example.com
      SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bf2\n
      \         Max-Forwards:6\n          To:MediaServer <sip:mresources@example.com>;tag=62784\n
      \         From:Sarvi <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n
      \         CSeq:314161 ACK\n          Content-Length:0\n                 Example:
      Add Synthesizer Control Channel\n   This example exchange continues from the
      previous figure and\n   allocates an additional resource control channel for
      a recognizer.\n   Since a recognizer would need to receive an audio stream for\n
      \  recognition, this interaction also updates the audio stream to\n   sendrecv,
      making it a two-way RTP media session.\n   C->S:  INVITE sip:mresources@server.example.com
      SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bf3\n
      \         Max-Forwards:6\n          To:MediaServer <sip:mresources@example.com>;tag=62784\n
      \         From:sarvi <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n
      \         CSeq:314162 INVITE\n          Contact:<sip:sarvi@client.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=sarvi 2890844526 2890844527 IN IP4 192.0.2.12\n          s=-\n          c=IN
      IP4 192.0.2.12\n          t=0 0\n          m=application 9 TCP/MRCPv2 1\n          a=setup:active\n
      \         a=connection:existing\n          a=resource:speechsynth\n          a=cmid:1\n
      \         m=audio 49170 RTP/AVP 0 96\n          a=rtpmap:0 pcmu/8000\n          a=rtpmap:96
      telephone-event/8000\n          a=fmtp:96 0-15\n          a=sendrecv\n          a=mid:1\n
      \         m=application 9 TCP/MRCPv2 1\n          a=setup:active\n          a=connection:existing\n
      \         a=resource:speechrecog\n          a=cmid:1\n   S->C:  SIP/2.0 200
      OK\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bf3;received=192.0.32.10\n
      \         To:MediaServer <sip:mresources@example.com>;tag=62784\n          From:sarvi
      <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n          CSeq:314162
      INVITE\n          Contact:<sip:mresources@server.example.com>\n          Content-Type:application/sdp\n
      \         Content-Length:...\n          v=0\n          o=- 2890842808 2890842809
      IN IP4 192.0.2.11\n          s=-\n          c=IN IP4 192.0.2.11\n          t=0
      0\n          m=application 32416 TCP/MRCPv2 1\n          a=setup:passive\n          a=connection:existing\n
      \         a=channel:32AECB234338@speechsynth\n          a=cmid:1\n          m=audio
      48260 RTP/AVP 0 96\n          a=rtpmap:0 pcmu/8000\n          a=rtpmap:96 telephone-event/8000\n
      \         a=fmtp:96 0-15\n          a=sendrecv\n          a=mid:1\n          m=application
      32416 TCP/MRCPv2 1\n          a=setup:passive\n          a=connection:existing\n
      \         a=channel:32AECB234338@speechrecog\n          a=cmid:1\n   C->S:  ACK
      sip:mresources@server.example.com SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n
      \          branch=z9hG4bK74bf4\n          Max-Forwards:6\n          To:MediaServer
      <sip:mresources@example.com>;tag=62784\n          From:Sarvi <sip:sarvi@example.com>;tag=1928301774\n
      \         Call-ID:a84b4c76e66710\n          CSeq:314162 ACK\n          Content-Length:0\n
      \                         Example: Add Recognizer\n   This example exchange
      continues from the previous figure and\n   deallocates the recognizer channel.
      \ Since a recognizer no longer\n   needs to receive an audio stream, this interaction
      also updates the\n   RTP media session to recvonly.\n   C->S:  INVITE sip:mresources@server.example.com
      SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bf5\n
      \         Max-Forwards:6\n          To:MediaServer <sip:mresources@example.com>;tag=62784\n
      \         From:sarvi <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n
      \         CSeq:314163 INVITE\n          Contact:<sip:sarvi@client.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=sarvi 2890844526 2890844528 IN IP4 192.0.2.12\n          s=-\n          c=IN
      IP4 192.0.2.12\n          t=0 0\n          m=application 9 TCP/MRCPv2 1\n          a=resource:speechsynth\n
      \         a=cmid:1\n          m=audio 49170 RTP/AVP 0\n          a=rtpmap:0
      pcmu/8000\n          a=recvonly\n          a=mid:1\n          m=application
      0 TCP/MRCPv2 1\n          a=resource:speechrecog\n          a=cmid:1\n   S->C:
      \ SIP/2.0 200 OK\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n
      \          branch=z9hG4bK74bf5;received=192.0.32.10\n          To:MediaServer
      <sip:mresources@example.com>;tag=62784\n          From:sarvi <sip:sarvi@example.com>;tag=1928301774\n
      \         Call-ID:a84b4c76e66710\n          CSeq:314163 INVITE\n          Contact:<sip:mresources@server.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=- 2890842808 2890842810 IN IP4 192.0.2.11\n          s=-\n          c=IN
      IP4 192.0.2.11\n          t=0 0\n          m=application 32416 TCP/MRCPv2 1\n
      \         a=channel:32AECB234338@speechsynth\n          a=cmid:1\n          m=audio
      48260 RTP/AVP 0\n          a=rtpmap:0 pcmu/8000\n          a=sendonly\n          a=mid:1\n
      \         m=application 0 TCP/MRCPv2 1\n          a=channel:32AECB234338@speechrecog\n
      \         a=cmid:1\n   C->S:  ACK sip:mresources@server.example.com SIP/2.0\n
      \         Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bf6\n
      \         Max-Forwards:6\n          To:MediaServer <sip:mresources@example.com>;tag=62784\n
      \         From:Sarvi <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n
      \         CSeq:314163 ACK\n          Content-Length:0\n                      Example:
      Deallocate Recognizer\n"
    title: 4.3.  SIP Session Example
  - contents:
    - "4.4.  Media Streams and RTP Ports\n   Since MRCPv2 resources either generate
      or consume media streams, the\n   client or the server needs to associate media
      sessions with their\n   corresponding resource or resources.  More than one
      resource could be\n   associated with a single media session or each resource
      could be\n   assigned a separate media session.  Also, note that more than one\n
      \  media session can be associated with a single resource if need be,\n   but
      this scenario is not useful for the current set of resources.\n   For example,
      a synthesizer and a recognizer could be associated to\n   the same media session
      (m=audio line), if it is opened in \"sendrecv\"\n   mode.  Alternatively, the
      recognizer could have its own \"sendonly\"\n   audio session, and the synthesizer
      could have its own \"recvonly\"\n   audio session.\n   The association between
      control channels and their corresponding\n   media sessions is established using
      a new \"resource channel media\n   identifier\" media-level attribute (\"cmid\").
      \ Valid values of this\n   attribute are the values of the \"mid\" attribute
      defined in RFC 5888\n   [RFC5888].  If there is more than one audio \"m=\" line,
      then each\n   audio \"m=\" line MUST have a \"mid\" attribute.  Each control
      \"m=\" line\n   MAY have one or more \"cmid\" attributes that match the resource\n
      \  control channel to the \"mid\" attributes of the audio \"m=\" lines it is\n
      \  associated with.  Note that if a control \"m=\" line does not have a\n   \"cmid\"
      attribute it will not be associated with any media.  The\n   operations on such
      a resource will hence be limited.  For example, if\n   it was a recognizer resource,
      the RECOGNIZE method requires an\n   associated media to process while the INTERPRET
      method does not.  The\n   formatting of the \"cmid\" attribute is described
      by the following\n   ABNF:\n   cmid-attribute     = \"a=cmid:\" identification-tag\n
      \  identification-tag = token\n   To allow this flexible mapping of media sessions
      to MRCPv2 control\n   channels, a single audio \"m=\" line can be associated
      with multiple\n   resources, or each resource can have its own audio \"m=\"
      line.  For\n   example, if the client wants to allocate a recognizer and a\n
      \  synthesizer and associate them with a single two-way audio stream,\n   the
      SDP offer would contain two control \"m=\" lines and a single audio\n   \"m=\"
      line with an attribute of \"sendrecv\".  Each of the control \"m=\"\n   lines
      would have a \"cmid\" attribute whose value matches the \"mid\" of\n   the audio
      \"m=\" line.  If, on the other hand, the client wants to\n   allocate a recognizer
      and a synthesizer each with its own separate\n   audio stream, the SDP offer
      would carry two control \"m=\" lines (one\n   for the recognizer and another
      for the synthesizer) and two audio\n   \"m=\" lines (one with the attribute
      \"sendonly\" and another with\n   attribute \"recvonly\").  The \"cmid\" attribute
      of the recognizer\n   control \"m=\" line would match the \"mid\" value of the
      \"sendonly\" audio\n   \"m=\" line, and the \"cmid\" attribute of the synthesizer
      control \"m=\"\n   line would match the \"mid\" attribute of the \"recvonly\"
      \"m=\" line.\n   When a server receives media (e.g., audio) on a media session
      that is\n   associated with more than one media processing resource, it is the\n
      \  responsibility of the server to receive and fork the media to the\n   resources
      that need to consume it.  If multiple resources in an\n   MRCPv2 session are
      generating audio (or other media) to be sent on a\n   single associated media
      session, it is the responsibility of the\n   server either to multiplex the
      multiple streams onto the single RTP\n   session or to contain an embedded RTP
      mixer (see RFC 3550 [RFC3550])\n   to combine the multiple streams into one.
      \ In the former case, the\n   media stream will contain RTP packets generated
      by different sources,\n   and hence the packets will have different Synchronization
      Source\n   Identifiers (SSRCs).  In the latter case, the RTP packets will\n
      \  contain multiple Contributing Source Identifiers (CSRCs)\n   corresponding
      to the original streams before being combined by the\n   mixer.  If an MRCPv2
      server implementation neither multiplexes nor\n   mixes, it MUST disallow the
      client from associating multiple such\n   resources to a single audio stream
      by rejecting the SDP offer with a\n   SIP 488 \"Not Acceptable\" error.  Note
      that there is a large installed\n   base that will return a SIP 501 \"Not Implemented\"
      error in this case.\n   To facilitate interoperability with this installed base,
      new\n   implementations SHOULD treat a 501 in this context as a 488 when it\n
      \  is received from an element known to be a legacy implementation.\n"
    title: 4.4.  Media Streams and RTP Ports
  - contents:
    - "4.5.  MRCPv2 Message Transport\n   The MRCPv2 messages defined in this document
      are transported over a\n   TCP or TLS connection between the client and the
      server.  The method\n   for setting up this transport connection and the resource
      control\n   channel is discussed in Sections 4.1 and 4.2.  Multiple resource\n
      \  control channels between a client and a server that belong to\n   different
      SIP dialogs can share one or more TLS or TCP connections\n   between them; the
      server and client MUST support this mode of\n   operation.  Clients and servers
      MUST use the MRCPv2 channel\n   identifier, carried in the Channel-Identifier
      header field in\n   individual MRCPv2 messages, to differentiate MRCPv2 messages
      from\n   different resource channels (see Section 6.2.1 for details).  All\n
      \  MRCPv2 servers MUST support TLS.  Servers MAY use TCP without TLS in\n   controlled
      environments (e.g., not in the public Internet) where both\n   nodes are inside
      a protected perimeter, for example, preventing\n   access to the MRCP server
      from remote nodes outside the controlled\n   perimeter.  It is up to the client
      to choose which mode of transport\n   it wants to use for an MRCPv2 session.\n
      \  Most examples from here on show only the MRCPv2 messages and do not\n   show
      the SIP messages that may have been used to establish the MRCPv2\n   control
      channel.\n"
    title: 4.5.  MRCPv2 Message Transport
  - contents:
    - "4.6.  MRCPv2 Session Termination\n   If an MRCP client notices that the underlying
      connection has been\n   closed for one of its MRCP channels, and it has not
      previously\n   initiated a re-INVITE to close that channel, it MUST send a BYE
      to\n   close down the SIP dialog and all other MRCP channels.  If an MRCP\n
      \  server notices that the underlying connection has been closed for one\n   of
      its MRCP channels, and it has not previously received and accepted\n   a re-INVITE
      closing that channel, then it MUST send a BYE to close\n   down the SIP dialog
      and all other MRCP channels.\n"
    title: 4.6.  MRCPv2 Session Termination
  title: 4.  MRCPv2 Basics
- contents:
  - "5.  MRCPv2 Specification\n   Except as otherwise indicated, MRCPv2 messages are
    Unicode encoded in\n   UTF-8 (RFC 3629 [RFC3629]) to allow many different languages
    to be\n   represented.  DEFINE-GRAMMAR (Section 9.8), for example, is one such\n
    \  exception, since its body can contain arbitrary XML in arbitrary (but\n   specified
    via XML) encodings.  MRCPv2 also allows message bodies to\n   be represented in
    other character sets (for example, ISO 8859-1\n   [ISO.8859-1.1987]) because,
    in some locales, other character sets are\n   already in widespread use.  The
    MRCPv2 headers (the first line of an\n   MRCP message) and header field names
    use only the US-ASCII subset of\n   UTF-8.\n   Lines are terminated by CRLF (carriage
    return, then line feed).\n   Also, some parameters in the message may contain
    binary data or a\n   record spanning multiple lines.  Such fields have a length
    value\n   associated with the parameter, which indicates the number of octets\n
    \  immediately following the parameter.\n"
  - contents:
    - "5.1.  Common Protocol Elements\n   The MRCPv2 message set consists of requests
      from the client to the\n   server, responses from the server to the client,
      and asynchronous\n   events from the server to the client.  All these messages
      consist of\n   a start-line, one or more header fields, an empty line (i.e.,
      a line\n   with nothing preceding the CRLF) indicating the end of the header\n
      \  fields, and an optional message body.\n"
    - "generic-message  =    start-line\n                      message-header\n                      CRLF\n
      \                     [ message-body ]\n"
    - 'message-body     =    *OCTET

      '
    - 'start-line       =    request-line / response-line / event-line

      '
    - 'message-header   =  1*(generic-header / resource-header / generic-field)

      '
    - "resource-header  =    synthesizer-header\n                 /    recognizer-header\n
      \                /    recorder-header\n                 /    verifier-header\n
      \  The message-body contains resource-specific and message-specific\n   data.
      \ The actual media types used to carry the data are specified in\n   the sections
      defining the individual messages.  Generic header fields\n   are described in
      Section 6.2.\n   If a message contains a message body, the message MUST contain\n
      \  content-headers indicating the media type and encoding of the data in\n   the
      message body.\n   Request, response and event messages (described in following\n
      \  sections) include the version of MRCP that the message conforms to.\n   Version
      compatibility rules follow [H3.1] regarding version ordering,\n   compliance
      requirements, and upgrading of version numbers.  The\n   version information
      is indicated by \"MRCP\" (as opposed to \"HTTP\" in\n   [H3.1]) or \"MRCP/2.0\"
      (as opposed to \"HTTP/1.1\" in [H3.1]).  To be\n   compliant with this specification,
      clients and servers sending MRCPv2\n   messages MUST indicate an mrcp-version
      of \"MRCP/2.0\".  ABNF\n   productions using mrcp-version can be found in Sections
      5.2, 5.3, and\n   5.5.\n   mrcp-version   =    \"MRCP\" \"/\" 1*2DIGIT \".\"
      1*2DIGIT\n   The message-length field specifies the length of the message in\n
      \  octets, including the start-line, and MUST be the second token from\n   the
      beginning of the message.  This is to make the framing and\n   parsing of the
      message simpler to do.  This field specifies the\n   length of the message including
      data that may be encoded into the\n   body of the message.  Note that this value
      MAY be given as a fixed-\n   length integer that is zero-padded (with leading
      zeros) in order to\n   eliminate or reduce inefficiency in cases where the message-length\n
      \  value would change as a result of the length of the message-length\n   token
      itself.  This value, as with all lengths in MRCP, is to be\n   interpreted as
      a base-10 number.  In particular, leading zeros do not\n   indicate that the
      value is to be interpreted as a base-8 number.\n   message-length =    1*19DIGIT\n
      \  The following sample MRCP exchange demonstrates proper message-length\n   values.
      \ The values for message-length have been removed from all\n   other examples
      in the specification and replaced by '...' to reduce\n   confusion in the case
      of minor message-length computation errors in\n   those examples.\n   C->S:
      \  MRCP/2.0 877 INTERPRET 543266\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Interpret-Text:may I speak to Andre Roy\n           Content-Type:application/srgs+xml\n
      \          Content-ID:<request1@form-level.store>\n           Content-Length:661\n
      \          <?xml version=\"1.0\"?>\n           <!-- the default grammar language
      is US English -->\n           <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \                   xml:lang=\"en-US\" version=\"1.0\" root=\"request\">\n           <!--
      single language attachment to tokens -->\n               <rule id=\"yes\">\n
      \                  <one-of>\n                       <item xml:lang=\"fr-CA\">oui</item>\n
      \                      <item xml:lang=\"en-US\">yes</item>\n                   </one-of>\n
      \              </rule>\n           <!-- single language attachment to a rule
      expansion -->\n               <rule id=\"request\">\n                   may
      I speak to\n                   <one-of xml:lang=\"fr-CA\">\n                       <item>Michel
      Tremblay</item>\n                       <item>Andre Roy</item>\n                   </one-of>\n
      \              </rule>\n           </grammar>\n   S->C:   MRCP/2.0 82 543266
      200 IN-PROGRESS\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:   MRCP/2.0 634 INTERPRETATION-COMPLETE 543266 200 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Completion-Cause:000 success\n           Content-Type:application/nlsml+xml\n
      \          Content-Length:441\n           <?xml version=\"1.0\"?>\n           <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                   xmlns:ex=\"http://www.example.com/example\"\n
      \                  grammar=\"session:request1@form-level.store\">\n               <interpretation>\n
      \                  <instance name=\"Person\">\n                       <ex:Person>\n
      \                          <ex:Name> Andre Roy </ex:Name>\n                       </ex:Person>\n
      \                  </instance>\n                   <input>   may I speak to
      Andre Roy </input>\n               </interpretation>\n           </result>\n
      \  All MRCPv2 messages, responses and events MUST carry the Channel-\n   Identifier
      header field so the server or client can differentiate\n   messages from different
      control channels that may share the same\n   transport connection.\n   In the
      resource-specific header field descriptions in Sections 8-11,\n   a header field
      is disallowed on a method (request, response, or\n   event) for that resource
      unless specifically listed as being allowed.\n   Also, the phrasing \"This header
      field MAY occur on method X\"\n   indicates that the header field is allowed
      on that method but is not\n   required to be used in every instance of that
      method.\n"
    title: 5.1.  Common Protocol Elements
  - contents:
    - "5.2.  Request\n   An MRCPv2 request consists of a Request line followed by
      the message\n   header section and an optional message body containing data
      specific\n   to the request message.\n   The Request message from a client to
      the server includes within the\n   first line the method to be applied, a method
      tag for that request\n   and the version of the protocol in use.\n   request-line
      \  =    mrcp-version SP message-length SP method-name\n                       SP
      request-id CRLF\n   The mrcp-version field is the MRCP protocol version that
      is being\n   used by the client.\n   The message-length field specifies the
      length of the message,\n   including the start-line.\n   Details about the mrcp-version
      and message-length fields are given in\n   Section 5.1.\n   The method-name
      field identifies the specific request that the client\n   is making to the server.
      \ Each resource supports a subset of the\n   MRCPv2 methods.  The subset for
      each resource is defined in the\n   section of the specification for the corresponding
      resource.\n   method-name    =    generic-method\n                  /    synthesizer-method\n
      \                 /    recognizer-method\n                  /    recorder-method\n
      \                 /    verifier-method\n   The request-id field is a unique
      identifier representable as an\n   unsigned 32-bit integer created by the client
      and sent to the server.\n   Clients MUST utilize monotonically increasing request-ids
      for\n   consecutive requests within an MRCP session.  The request-id space is\n
      \  linear (i.e., not mod(32)), so the space does not wrap, and validity\n   can
      be checked with a simple unsigned comparison operation.  The\n   client may
      choose any initial value for its first request, but a\n   small integer is RECOMMENDED
      to avoid exhausting the space in long\n   sessions.  If the server receives
      duplicate or out-of-order requests,\n   the server MUST reject the request with
      a response code of 410.\n   Since request-ids are scoped to the MRCP session,
      they are unique\n   across all TCP connections and all resource channels in
      the session.\n   The server resource MUST use the client-assigned identifier
      in its\n   response to the request.  If the request does not complete\n   synchronously,
      future asynchronous events associated with this\n   request MUST carry the client-assigned
      request-id.\n   request-id     =    1*10DIGIT\n"
    title: 5.2.  Request
  - contents:
    - "5.3.  Response\n   After receiving and interpreting the request message for
      a method,\n   the server resource responds with an MRCPv2 response message.
      \ The\n   response consists of a response line followed by the message header\n
      \  section and an optional message body containing data specific to the\n   method.\n
      \  response-line  =    mrcp-version SP message-length SP request-id\n                       SP
      status-code SP request-state CRLF\n   The mrcp-version field MUST contain the
      version of the request if\n   supported; otherwise, it MUST contain the highest
      version of MRCP\n   supported by the server.\n   The message-length field specifies
      the length of the message,\n   including the start-line.\n   Details about the
      mrcp-version and message-length fields are given in\n   Section 5.1.\n   The
      request-id used in the response MUST match the one sent in the\n   corresponding
      request message.\n   The status-code field is a 3-digit code representing the
      success or\n   failure or other status of the request.\n   status-code     =
      \   3DIGIT\n   The request-state field indicates if the action initiated by
      the\n   Request is PENDING, IN-PROGRESS, or COMPLETE.  The COMPLETE status\n
      \  means that the request was processed to completion and that there\n   will
      be no more events or other messages from that resource to the\n   client with
      that request-id.  The PENDING status means that the\n   request has been placed
      in a queue and will be processed in first-in-\n   first-out order.  The IN-PROGRESS
      status means that the request is\n   being processed and is not yet complete.
      \ A PENDING or IN-PROGRESS\n   status indicates that further Event messages
      may be delivered with\n   that request-id.\n   request-state    =  \"COMPLETE\"\n
      \                   /  \"IN-PROGRESS\"\n                    /  \"PENDING\"\n"
    title: 5.3.  Response
  - contents:
    - "5.4.  Status Codes\n   The status codes are classified under the Success (2xx),
      Client\n   Failure (4xx), and Server Failure (5xx) codes.\n     +------------+--------------------------------------------------+\n
      \    | Code       | Meaning                                          |\n     +------------+--------------------------------------------------+\n
      \    | 200        | Success                                          |\n     |
      201        | Success with some optional header fields ignored |\n     +------------+--------------------------------------------------+\n
      \                              Success (2xx)\n   +--------+----------------------------------------------------------+\n
      \  | Code   | Meaning                                                  |\n   +--------+----------------------------------------------------------+\n
      \  | 401    | Method not allowed                                       |\n   |
      402    | Method not valid in this state                           |\n   | 403
      \   | Unsupported header field                                 |\n   | 404    |
      Illegal value for header field. This is the error for a  |\n   |        | syntax
      violation.                                        |\n   | 405    | Resource
      not allocated for this session or does not      |\n   |        | exist                                                    |\n
      \  | 406    | Mandatory Header Field Missing                           |\n   |
      407    | Method or Operation Failed (e.g., Grammar compilation    |\n   |        |
      failed in the recognizer. Detailed cause codes might be  |\n   |        | available
      through a resource-specific header.)           |\n   | 408    | Unrecognized
      or unsupported message entity               |\n   | 409    | Unsupported Header
      Field Value. This is a value that is  |\n   |        | syntactically legal but
      exceeds the implementation's     |\n   |        | capabilities or expectations.
      \                           |\n   | 410    | Non-Monotonic or Out-of-order sequence
      number in request.|\n   | 411-420| Reserved for future assignment                           |\n
      \  +--------+----------------------------------------------------------+\n                           Client
      Failure (4xx)\n              +------------+--------------------------------+\n
      \             | Code       | Meaning                        |\n              +------------+--------------------------------+\n
      \             | 501        | Server Internal Error          |\n              |
      502        | Protocol Version not supported |\n              | 503        |
      Reserved for future assignment |\n              | 504        | Message too large
      \             |\n              +------------+--------------------------------+\n
      \                          Server Failure (5xx)\n"
    title: 5.4.  Status Codes
  - contents:
    - "5.5.  Events\n   The server resource may need to communicate a change in state
      or the\n   occurrence of a certain event to the client.  These messages are
      used\n   when a request does not complete immediately and the response returns\n
      \  a status of PENDING or IN-PROGRESS.  The intermediate results and\n   events
      of the request are indicated to the client through the event\n   message from
      the server.  The event message consists of an event\n   header line followed
      by the message header section and an optional\n   message body containing data
      specific to the event message.  The\n   header line has the request-id of the
      corresponding request and\n   status value.  The request-state value is COMPLETE
      if the request is\n   done and this was the last event, else it is IN-PROGRESS.\n
      \  event-line       =  mrcp-version SP message-length SP event-name\n                       SP
      request-id SP request-state CRLF\n   The mrcp-version used here is identical
      to the one used in the\n   Request/Response line and indicates the highest version
      of MRCP\n   running on the server.\n   The message-length field specifies the
      length of the message,\n   including the start-line.\n   Details about the mrcp-version
      and message-length fields are given in\n   Section 5.1.\n   The event-name identifies
      the nature of the event generated by the\n   media resource.  The set of valid
      event names depends on the resource\n   generating it.  See the corresponding
      resource-specific section of\n   the document.\n   event-name       =  synthesizer-event\n
      \                   /  recognizer-event\n                    /  recorder-event\n
      \                   /  verifier-event\n   The request-id used in the event MUST
      match the one sent in the\n   request that caused this event.\n   The request-state
      indicates whether the Request/Command causing this\n   event is complete or
      still in progress and whether it is the same as\n   the one mentioned in Section
      5.3.  The final event for a request has\n   a COMPLETE status indicating the
      completion of the request.\n"
    title: 5.5.  Events
  title: 5.  MRCPv2 Specification
- contents:
  - "6.  MRCPv2 Generic Methods, Headers, and Result Structure\n   MRCPv2 supports
    a set of methods and header fields that are common to\n   all resources.  These
    are discussed here; resource-specific methods\n   and header fields are discussed
    in the corresponding resource-\n   specific section of the document.\n"
  - contents:
    - "6.1.  Generic Methods\n   MRCPv2 supports two generic methods for reading and
      writing the state\n   associated with a resource.\n   generic-method      =
      \   \"SET-PARAMS\"\n                       /    \"GET-PARAMS\"\n   These are
      described in the following subsections.\n"
    - contents:
      - "6.1.1.  SET-PARAMS\n   The SET-PARAMS method, from the client to the server,
        tells the\n   MRCPv2 resource to define parameters for the session, such as
        voice\n   characteristics and prosody on synthesizers, recognition timers
        on\n   recognizers, etc.  If the server accepts and sets all parameters, it\n
        \  MUST return a response status-code of 200.  If it chooses to ignore\n   some
        optional header fields that can be safely ignored without\n   affecting operation
        of the server, it MUST return 201.\n   If one or more of the header fields
        being sent is incorrect, error\n   403, 404, or 409 MUST be returned as follows:\n
        \  o  If one or more of the header fields being set has an illegal\n      value,
        the server MUST reject the request with a 404 Illegal Value\n      for Header
        Field.\n   o  If one or more of the header fields being set is unsupported
        for\n      the resource, the server MUST reject the request with a 403\n      Unsupported
        Header Field, except as described in the next\n      paragraph.\n   o  If
        one or more of the header fields being set has an unsupported\n      value,
        the server MUST reject the request with a 409 Unsupported\n      Header Field
        Value, except as described in the next paragraph.\n   If both error 404 and
        another error have occurred, only error 404\n   MUST be returned.  If both
        errors 403 and 409 have occurred, but not\n   error 404, only error 403 MUST
        be returned.\n   If error 403, 404, or 409 is returned, the response MUST
        include the\n   bad or unsupported header fields and their values exactly
        as they\n   were sent from the client.  Session parameters modified using\n
        \  SET-PARAMS do not override parameters explicitly specified on\n   individual
        requests or requests that are IN-PROGRESS.\n   C->S:  MRCP/2.0 ... SET-PARAMS
        543256\n          Channel-Identifier:32AECB23433802@speechsynth\n          Voice-gender:female\n
        \         Voice-variant:3\n   S->C:  MRCP/2.0 ... 543256 200 COMPLETE\n          Channel-Identifier:32AECB23433802@speechsynth\n"
      title: 6.1.1.  SET-PARAMS
    - contents:
      - "6.1.2.  GET-PARAMS\n   The GET-PARAMS method, from the client to the server,
        asks the MRCPv2\n   resource for its current session parameters, such as voice\n
        \  characteristics and prosody on synthesizers, recognition timers on\n   recognizers,
        etc.  For every header field the client sends in the\n   request without a
        value, the server MUST include the header field and\n   its corresponding
        value in the response.  If no parameter header\n   fields are specified by
        the client, then the server MUST return all\n   the settable parameters and
        their values in the corresponding header\n   section of the response, including
        vendor-specific parameters.  Such\n   wildcard parameter requests can be very
        processing-intensive, since\n   the number of settable parameters can be large
        depending on the\n   implementation.  Hence, it is RECOMMENDED that the client
        not use the\n   wildcard GET-PARAMS operation very often.  Note that GET-PARAMS\n
        \  returns header field values that apply to the whole session and not\n   values
        that have a request-level scope.  For example, Input-Waveform-\n   URI is
        a request-level header field and thus would not be returned by\n   GET-PARAMS.\n
        \  If all of the header fields requested are supported, the server MUST\n
        \  return a response status-code of 200.  If some of the header fields\n   being
        retrieved are unsupported for the resource, the server MUST\n   reject the
        request with a 403 Unsupported Header Field.  Such a\n   response MUST include
        the unsupported header fields exactly as they\n   were sent from the client,
        without values.\n   C->S:   MRCP/2.0 ... GET-PARAMS 543256\n           Channel-Identifier:32AECB23433802@speechsynth\n
        \          Voice-gender:\n           Voice-variant:\n           Vendor-Specific-Parameters:com.example.param1;\n
        \                        com.example.param2\n   S->C:   MRCP/2.0 ... 543256
        200 COMPLETE\n           Channel-Identifier:32AECB23433802@speechsynth\n           Voice-gender:female\n
        \          Voice-variant:3\n           Vendor-Specific-Parameters:com.example.param1=\"Company
        Name\";\n                         com.example.param2=\"124324234@example.com\"\n"
      title: 6.1.2.  GET-PARAMS
    title: 6.1.  Generic Methods
  - contents:
    - "6.2.  Generic Message Headers\n   All MRCPv2 header fields, which include both
      the generic-headers\n   defined in the following subsections and the resource-specific
      header\n   fields defined later, follow the same generic format as that given
      in\n   Section 3.1 of RFC 5322 [RFC5322].  Each header field consists of a\n
      \  name followed by a colon (\":\") and the value.  Header field names are\n
      \  case-insensitive.  The value MAY be preceded by any amount of LWS\n   (linear
      white space), though a single SP (space) is preferred.\n   Header fields may
      extend over multiple lines by preceding each extra\n   line with at least one
      SP or HT (horizontal tab).\n   generic-field  = field-name \":\" [ field-value
      ]\n   field-name     = token\n   field-value    = *LWS field-content *( CRLF
      1*LWS field-content)\n   field-content  = <the OCTETs making up the field-value\n
      \                   and consisting of either *TEXT or combinations\n                    of
      token, separators, and quoted-string>\n   The field-content does not include
      any leading or trailing LWS (i.e.,\n   linear white space occurring before the
      first non-whitespace\n   character of the field-value or after the last non-whitespace\n
      \  character of the field-value).  Such leading or trailing LWS MAY be\n   removed
      without changing the semantics of the field value.  Any LWS\n   that occurs
      between field-content MAY be replaced with a single SP\n   before interpreting
      the field value or forwarding the message\n   downstream.\n   MRCPv2 servers
      and clients MUST NOT depend on header field order.  It\n   is RECOMMENDED to
      send general-header fields first, followed by\n   request-header or response-header
      fields, and ending with the entity-\n   header fields.  However, MRCPv2 servers
      and clients MUST be prepared\n   to process the header fields in any order.
      \ The only exception to\n   this rule is when there are multiple header fields
      with the same name\n   in a message.\n   Multiple header fields with the same
      name MAY be present in a message\n   if and only if the entire value for that
      header field is defined as a\n   comma-separated list [i.e., #(values)].\n   Since
      vendor-specific parameters may be order-dependent, it MUST be\n   possible to
      combine multiple header fields of the same name into one\n   \"name:value\"
      pair without changing the semantics of the message, by\n   appending each subsequent
      value to the first, each separated by a\n   comma.  The order in which header
      fields with the same name are\n   received is therefore significant to the interpretation
      of the\n   combined header field value, and thus an intermediary MUST NOT change\n
      \  the order of these values when a message is forwarded.\n   generic-header
      \     =    channel-identifier\n                       /    accept\n                       /
      \   active-request-id-list\n                       /    proxy-sync-id\n                       /
      \   accept-charset\n                       /    content-type\n                       /
      \   content-id\n                       /    content-base\n                       /
      \   content-encoding\n                       /    content-location\n                       /
      \   content-length\n                       /    fetch-timeout\n                       /
      \   cache-control\n                       /    logging-tag\n                       /
      \   set-cookie\n                       /    vendor-specific\n"
    - contents:
      - "6.2.1.  Channel-Identifier\n   All MRCPv2 requests, responses, and events
        MUST contain the Channel-\n   Identifier header field.  The value is allocated
        by the server when a\n   control channel is added to the session and communicated
        to the\n   client by the \"a=channel\" attribute in the SDP answer from the\n
        \  server.  The header field value consists of 2 parts separated by the\n
        \  '@' symbol.  The first part is an unambiguous string identifying the\n
        \  MRCPv2 session.  The second part is a string token that specifies one\n
        \  of the media processing resource types listed in Section 3.1.  The\n   unambiguous
        string (first part) MUST be difficult to guess, unique\n   among the resource
        instances managed by the server, and common to all\n   resource channels with
        that server established through a single SIP\n   dialog.\n   channel-identifier
        \ = \"Channel-Identifier\" \":\" channel-id CRLF\n   channel-id          =
        1*alphanum \"@\" 1*alphanum\n"
      title: 6.2.1.  Channel-Identifier
    - contents:
      - "6.2.2.  Accept\n   The Accept header field follows the syntax defined in
        [H14.1].  The\n   semantics are also identical, with the exception that if
        no Accept\n   header field is present, the server MUST assume a default value
        that\n   is specific to the resource type that is being controlled.  This\n
        \  default value can be changed for a resource on a session by sending\n   this
        header field in a SET-PARAMS method.  The current default value\n   of this
        header field for a resource in a session can be found through\n   a GET-PARAMS
        method.  This header field MAY occur on any request.\n"
      title: 6.2.2.  Accept
    - contents:
      - "6.2.3.  Active-Request-Id-List\n   In a request, this header field indicates
        the list of request-ids to\n   which the request applies.  This is useful
        when there are multiple\n   requests that are PENDING or IN-PROGRESS and the
        client wants this\n   request to apply to one or more of these specifically.\n
        \  In a response, this header field returns the list of request-ids that\n
        \  the method modified or affected.  There could be one or more requests\n
        \  in a request-state of PENDING or IN-PROGRESS.  When a method\n   affecting
        one or more PENDING or IN-PROGRESS requests is sent from\n   the client to
        the server, the response MUST contain the list of\n   request-ids that were
        affected or modified by this command in its\n   header section.\n   The Active-Request-Id-List
        is only used in requests and responses,\n   not in events.\n   For example,
        if a STOP request with no Active-Request-Id-List is sent\n   to a synthesizer
        resource that has one or more SPEAK requests in the\n   PENDING or IN-PROGRESS
        state, all SPEAK requests MUST be cancelled,\n   including the one IN-PROGRESS.
        \ The response to the STOP request\n   contains in the Active-Request-Id-List
        value the request-ids of all\n   the SPEAK requests that were terminated.
        \ After sending the STOP\n   response, the server MUST NOT send any SPEAK-COMPLETE
        or RECOGNITION-\n   COMPLETE events for the terminated requests.\n   active-request-id-list
        \ =  \"Active-Request-Id-List\" \":\"\n                              request-id
        *(\",\" request-id) CRLF\n"
      title: 6.2.3.  Active-Request-Id-List
    - contents:
      - "6.2.4.  Proxy-Sync-Id\n   When any server resource generates a \"barge-in-able\"
        event, it also\n   generates a unique tag.  The tag is sent as this header
        field's value\n   in an event to the client.  The client then acts as an intermediary\n
        \  among the server resources and sends a BARGE-IN-OCCURRED method to\n   the
        synthesizer server resource with the Proxy-Sync-Id it received\n   from the
        server resource.  When the recognizer and synthesizer\n   resources are part
        of the same session, they may choose to work\n   together to achieve quicker
        interaction and response.  Here, the\n   Proxy-Sync-Id helps the resource
        receiving the event, intermediated\n   by the client, to decide if this event
        has been processed through a\n   direct interaction of the resources.  This
        header field MAY occur\n   only on events and the BARGE-IN-OCCURRED method.
        \ The name of this\n   header field contains the word 'proxy' only for historical
        reasons\n   and does not imply that a proxy server is involved.\n   proxy-sync-id
        \   =  \"Proxy-Sync-Id\" \":\" 1*VCHAR CRLF\n"
      title: 6.2.4.  Proxy-Sync-Id
    - contents:
      - "6.2.5.  Accept-Charset\n   See [H14.2].  This specifies the acceptable character
        sets for\n   entities returned in the response or events associated with this\n
        \  request.  This is useful in specifying the character set to use in\n   the
        Natural Language Semantic Markup Language (NLSML) results of a\n   RECOGNITION-COMPLETE
        event.  This header field is only used on\n   requests.\n"
      title: 6.2.5.  Accept-Charset
    - contents:
      - "6.2.6.  Content-Type\n   See [H14.17].  MRCPv2 supports a restricted set
        of registered media\n   types for content, including speech markup, grammar,
        and recognition\n   results.  The content types applicable to each MRCPv2
        resource-type\n   are specified in the corresponding section of the document
        and are\n   registered in the MIME Media Types registry maintained by IANA.
        \ The\n   multipart content type \"multipart/mixed\" is supported to communicate\n
        \  multiple of the above mentioned contents, in which case the body\n   parts
        MUST NOT contain any MRCPv2-specific header fields.  This\n   header field
        MAY occur on all messages.\n   content-type     =    \"Content-Type\" \":\"
        media-type-value CRLF\n   media-type-value =    type \"/\" subtype *( \";\"
        parameter )\n   type             =    token\n   subtype          =    token\n
        \  parameter        =    attribute \"=\" value\n   attribute        =    token\n
        \  value            =    token / quoted-string\n"
      title: 6.2.6.  Content-Type
    - contents:
      - "6.2.7.  Content-ID\n   This header field contains an ID or name for the content
        by which it\n   can be referenced.  This header field operates according to
        the\n   specification in RFC 2392 [RFC2392] and is required for content\n
        \  disambiguation in multipart messages.  In MRCPv2, whenever the\n   associated
        content is stored by either the client or the server, it\n   MUST be retrievable
        using this ID.  Such content can be referenced\n   later in a session by addressing
        it with the 'session' URI scheme\n   described in Section 13.6.  This header
        field MAY occur on all\n   messages.\n"
      title: 6.2.7.  Content-ID
    - contents:
      - "6.2.8.  Content-Base\n   The Content-Base entity-header MAY be used to specify
        the base URI\n   for resolving relative URIs within the entity.\n   content-base
        \     = \"Content-Base\" \":\" absoluteURI CRLF\n   Note, however, that the
        base URI of the contents within the entity-\n   body may be redefined within
        that entity-body.  An example of this\n   would be multipart media, which
        in turn can have multiple entities\n   within it.  This header field MAY occur
        on all messages.\n"
      title: 6.2.8.  Content-Base
    - contents:
      - "6.2.9.  Content-Encoding\n   The Content-Encoding entity-header is used as
        a modifier to the\n   Content-Type.  When present, its value indicates what
        additional\n   content encoding has been applied to the entity-body, and thus
        what\n   decoding mechanisms must be applied in order to obtain the Media
        Type\n   referenced by the Content-Type header field.  Content-Encoding is\n
        \  primarily used to allow a document to be compressed without losing\n   the
        identity of its underlying media type.  Note that the SIP session\n   can
        be used to determine accepted encodings (see Section 7).  This\n   header
        field MAY occur on all messages.\n   content-encoding  = \"Content-Encoding\"
        \":\"\n                       *WSP content-coding\n                       *(*WSP
        \",\" *WSP content-coding *WSP )\n                       CRLF\n   Content
        codings are defined in [H3.5].  An example of its use is\n   Content-Encoding:gzip\n
        \  If multiple encodings have been applied to an entity, the content\n   encodings
        MUST be listed in the order in which they were applied.\n"
      title: 6.2.9.  Content-Encoding
    - contents:
      - "6.2.10.  Content-Location\n   The Content-Location entity-header MAY be used
        to supply the resource\n   location for the entity enclosed in the message
        when that entity is\n   accessible from a location separate from the requested
        resource's\n   URI.  Refer to [H14.14].\n   content-location  =  \"Content-Location\"
        \":\"\n                        ( absoluteURI / relativeURI ) CRLF\n   The
        Content-Location value is a statement of the location of the\n   resource
        corresponding to this particular entity at the time of the\n   request.  This
        header field is provided for optimization purposes\n   only.  The receiver
        of this header field MAY assume that the entity\n   being sent is identical
        to what would have been retrieved or might\n   already have been retrieved
        from the Content-Location URI.\n   For example, if the client provided a grammar
        markup inline, and it\n   had previously retrieved it from a certain URI,
        that URI can be\n   provided as part of the entity, using the Content-Location
        header\n   field.  This allows a resource like the recognizer to look into
        its\n   cache to see if this grammar was previously retrieved, compiled, and\n
        \  cached.  In this case, it might optimize by using the previously\n   compiled
        grammar object.\n   If the Content-Location is a relative URI, the relative
        URI is\n   interpreted relative to the Content-Base URI.  This header field
        MAY\n   occur on all messages.\n"
      title: 6.2.10.  Content-Location
    - contents:
      - "6.2.11.  Content-Length\n   This header field contains the length of the
        content of the message\n   body (i.e., after the double CRLF following the
        last header field).\n   Unlike in HTTP, it MUST be included in all messages
        that carry\n   content beyond the header section.  If it is missing, a default
        value\n   of zero is assumed.  Otherwise, it is interpreted according to\n
        \  [H14.13].  When a message having no use for a message body contains\n   one,
        i.e., the Content-Length is non-zero, the receiver MUST ignore\n   the content
        of the message body.  This header field MAY occur on all\n   messages.\n   content-length
        \ =  \"Content-Length\" \":\" 1*19DIGIT CRLF\n"
      title: 6.2.11.  Content-Length
    - contents:
      - "6.2.12.  Fetch Timeout\n   When the recognizer or synthesizer needs to fetch
        documents or other\n   resources, this header field controls the corresponding
        URI access\n   properties.  This defines the timeout for content that the
        server may\n   need to fetch over the network.  The value is interpreted to
        be in\n   milliseconds and ranges from 0 to an implementation-specific maximum\n
        \  value.  It is RECOMMENDED that servers be cautious about accepting\n   long
        timeout values.  The default value for this header field is\n   implementation
        specific.  This header field MAY occur in DEFINE-\n   GRAMMAR, RECOGNIZE,
        SPEAK, SET-PARAMS, or GET-PARAMS.\n   fetch-timeout       =   \"Fetch-Timeout\"
        \":\" 1*19DIGIT CRLF\n"
      title: 6.2.12.  Fetch Timeout
    - contents:
      - "6.2.13.  Cache-Control\n   If the server implements content caching, it MUST
        adhere to the cache\n   correctness rules of HTTP 1.1 [RFC2616] when accessing
        and caching\n   stored content.  In particular, the \"expires\" and \"cache-control\"\n
        \  header fields of the cached URI or document MUST be honored and take\n
        \  precedence over the Cache-Control defaults set by this header field.\n
        \  The Cache-Control directives are used to define the default caching\n   algorithms
        on the server for the session or request.  The scope of\n   the directive
        is based on the method it is sent on.  If the directive\n   is sent on a SET-PARAMS
        method, it applies for all requests for\n   external documents the server
        makes during that session, unless it is\n   overridden by a Cache-Control
        header field on an individual request.\n   If the directives are sent on any
        other requests, they apply only to\n   external document requests the server
        makes for that request.  An\n   empty Cache-Control header field on the GET-PARAMS
        method is a\n   request for the server to return the current Cache-Control
        directives\n   setting on the server.  This header field MAY occur only on
        requests.\n   cache-control    =    \"Cache-Control\" \":\"\n                         [*WSP
        cache-directive\n                         *( *WSP \",\" *WSP cache-directive
        *WSP )]\n                         CRLF\n   cache-directive     = \"max-age\"
        \"=\" delta-seconds\n                       / \"max-stale\" [ \"=\" delta-seconds
        ]\n                       / \"min-fresh\" \"=\" delta-seconds\n   delta-seconds
        \      = 1*19DIGIT\n   Here, delta-seconds is a decimal time value specifying
        the number of\n   seconds since the instant the message response or data was
        received\n   by the server.\n   The different cache-directive options allow
        the client to ask the\n   server to override the default cache expiration
        mechanisms:\n   max-age        Indicates that the client can tolerate the
        server\n                  using content whose age is no greater than the\n
        \                 specified time in seconds.  Unless a \"max-stale\"\n                  directive
        is also included, the client is not willing\n                  to accept a
        response based on stale data.\n   min-fresh      Indicates that the client
        is willing to accept a\n                  server response with cached data
        whose expiration is\n                  no less than its current age plus the
        specified time\n                  in seconds.  If the server's cache time-to-live\n
        \                 exceeds the client-supplied min-fresh value, the\n                  server
        MUST NOT utilize cached content.\n   max-stale      Indicates that the client
        is willing to allow a server\n                  to utilize cached data that
        has exceeded its\n                  expiration time.  If \"max-stale\" is
        assigned a value,\n                  then the client is willing to allow the
        server to use\n                  cached data that has exceeded its expiration
        time by\n                  no more than the specified number of seconds.  If
        no\n                  value is assigned to \"max-stale\", then the client
        is\n                  willing to allow the server to use stale data of any\n
        \                 age.\n   If the server cache is requested to use stale response/data
        without\n   validation, it MAY do so only if this does not conflict with any\n
        \  \"MUST\"-level requirements concerning cache validation (e.g., a \"must-\n
        \  revalidate\" Cache-Control directive in the HTTP 1.1 specification\n   pertaining
        to the corresponding URI).\n   If both the MRCPv2 Cache-Control directive
        and the cached entry on\n   the server include \"max-age\" directives, then
        the lesser of the two\n   values is used for determining the freshness of
        the cached entry for\n   that request.\n"
      title: 6.2.13.  Cache-Control
    - contents:
      - "6.2.14.  Logging-Tag\n   This header field MAY be sent as part of a SET-PARAMS/GET-PARAMS\n
        \  method to set or retrieve the logging tag for logs generated by the\n   server.
        \ Once set, the value persists until a new value is set or the\n   session
        ends.  The MRCPv2 server MAY provide a mechanism to create\n   subsets of
        its output logs so that system administrators can examine\n   or extract only
        the log file portion during which the logging tag was\n   set to a certain
        value.\n   It is RECOMMENDED that clients include in the logging tag information\n
        \  to identify the MRCPv2 client User Agent, so that one can determine\n   which
        MRCPv2 client request generated a given log message at the\n   server.  It
        is also RECOMMENDED that MRCPv2 clients not log\n   personally identifiable
        information such as credit card numbers and\n   national identification numbers.\n
        \  logging-tag    = \"Logging-Tag\" \":\" 1*UTFCHAR CRLF\n"
      title: 6.2.14.  Logging-Tag
    - contents:
      - "6.2.15.  Set-Cookie\n   Since the associated HTTP client on an MRCPv2 server
        fetches\n   documents for processing on behalf of the MRCPv2 client, the cookie\n
        \  store in the HTTP client of the MRCPv2 server is treated as an\n   extension
        of the cookie store in the HTTP client of the MRCPv2\n   client.  This requires
        that the MRCPv2 client and server be able to\n   synchronize their common
        cookie store as needed.  To enable the\n   MRCPv2 client to push its stored
        cookies to the MRCPv2 server and get\n   new cookies from the MRCPv2 server
        stored back to the MRCPv2 client,\n   the Set-Cookie entity-header field MAY
        be included in MRCPv2 requests\n   to update the cookie store on a server
        and be returned in final\n   MRCPv2 responses or events to subsequently update
        the client's own\n   cookie store.  The stored cookies on the server persist
        for the\n   duration of the MRCPv2 session and MUST be destroyed at the end
        of\n   the session.  To ensure support for cookies, MRCPv2 clients and\n   servers
        MUST support the Set-Cookie entity-header field.\n   Note that it is the MRCPv2
        client that determines which, if any,\n   cookies are sent to the server.
        \ There is no requirement that all\n   cookies be shared.  Rather, it is RECOMMENDED
        that MRCPv2 clients\n   communicate only cookies needed by the MRCPv2 server
        to process its\n   requests.\n set-cookie      =       \"Set-Cookie:\" cookies
        CRLF\n cookies         =       cookie *(\",\" *LWS cookie)\n cookie          =
        \      attribute \"=\" value *(\";\" cookie-av)\n cookie-av       =       \"Comment\"
        \"=\" value\n                 /       \"Domain\" \"=\" value\n                 /
        \      \"Max-Age\" \"=\" value\n                 /       \"Path\" \"=\" value\n
        \                /       \"Secure\"\n                 /       \"Version\"
        \"=\" 1*19DIGIT\n                 /       \"Age\" \"=\" delta-seconds\n set-cookie
        \       = \"Set-Cookie:\" SP set-cookie-string\n set-cookie-string = cookie-pair
        *( \";\" SP cookie-av )\n cookie-pair       = cookie-name \"=\" cookie-value\n
        cookie-name       = token\n cookie-value      = *cookie-octet / ( DQUOTE *cookie-octet
        DQUOTE )\n cookie-octet      = %x21 / %x23-2B / %x2D-3A / %x3C-5B / %x5D-7E\n
        token             = <token, defined in [RFC2616], Section 2.2>\n cookie-av
        \        = expires-av / max-age-av / domain-av /\n                      path-av
        / secure-av / httponly-av /\n                      extension-av / age-av\n
        expires-av        = \"Expires=\" sane-cookie-date\n sane-cookie-date  = <rfc1123-date,
        defined in [RFC2616], Section 3.3.1>\n max-age-av        = \"Max-Age=\" non-zero-digit
        *DIGIT\n non-zero-digit    = %x31-39\n domain-av         = \"Domain=\" domain-value\n
        domain-value      = <subdomain>\n path-av           = \"Path=\" path-value\n
        path-value        = <any CHAR except CTLs or \";\">\n secure-av         =
        \"Secure\"\n httponly-av       = \"HttpOnly\"\n extension-av      = <any CHAR
        except CTLs or \";\">\n age-av            = \"Age=\" delta-seconds\n   The
        Set-Cookie header field is specified in RFC 6265 [RFC6265].  The\n   \"Age\"
        attribute is introduced in this specification to indicate the\n   age of the
        cookie and is OPTIONAL.  An MRCPv2 client or server MUST\n   calculate the
        age of the cookie according to the age calculation\n   rules in the HTTP/1.1
        specification [RFC2616] and append the \"Age\"\n   attribute accordingly.
        \ This attribute is provided because time may\n   have passed since the client
        received the cookie from an HTTP server.\n   Rather than having the client
        reduce Max-Age by the actual age, it\n   passes Max-Age verbatim and appends
        the \"Age\" attribute, thus\n   maintaining the cookie as received while still
        accounting for the\n   fact that time has passed.\n   The MRCPv2 client or
        server MUST supply defaults for the \"Domain\" and\n   \"Path\" attributes,
        as specified in RFC 6265, if they are omitted by\n   the HTTP origin server.
        \ Note that there is no leading dot present in\n   the \"Domain\" attribute
        value in this case.  Although an explicitly\n   specified \"Domain\" value
        received via the HTTP protocol may be\n   modified to include a leading dot,
        an MRCPv2 client or server MUST\n   NOT modify the \"Domain\" value when received
        via the MRCPv2 protocol.\n   An MRCPv2 client or server MAY combine multiple
        cookie header fields\n   of the same type into a single \"field-name:field-value\"
        pair as\n   described in Section 6.2.\n   The Set-Cookie header field MAY
        be specified in any request that\n   subsequently results in the server performing
        an HTTP access.  When a\n   server receives new cookie information from an
        HTTP origin server,\n   and assuming the cookie store is modified according
        to RFC 6265, the\n   server MUST return the new cookie information in the
        MRCPv2 COMPLETE\n   response or event, as appropriate, to allow the client
        to update its\n   own cookie store.\n   The SET-PARAMS request MAY specify
        the Set-Cookie header field to\n   update the cookie store on a server.  The
        GET-PARAMS request MAY be\n   used to return the entire cookie store of \"Set-Cookie\"
        type cookies\n   to the client.\n"
      title: 6.2.15.  Set-Cookie
    - contents:
      - "6.2.16.  Vendor-Specific Parameters\n   This set of header fields allows
        for the client to set or retrieve\n   vendor-specific parameters.\n   vendor-specific
        \         =    \"Vendor-Specific-Parameters\" \":\"\n                                 [vendor-specific-av-pair\n
        \                                *(\";\" vendor-specific-av-pair)] CRLF\n
        \  vendor-specific-av-pair  = vendor-av-pair-name \"=\"\n                              value\n
        \  vendor-av-pair-name     = 1*UTFCHAR\n   Header fields of this form MAY
        be sent in any method (request) and\n   are used to manage implementation-specific
        parameters on the server\n   side.  The vendor-av-pair-name follows the reverse
        Internet Domain\n   Name convention (see Section 13.1.6 for syntax and registration\n
        \  information).  The value of the vendor attribute is specified after\n   the
        \"=\" symbol and MAY be quoted.  For example:\n   com.example.companyA.paramxyz=256\n
        \  com.example.companyA.paramabc=High\n   com.example.companyB.paramxyz=Low\n
        \  When used in GET-PARAMS to get the current value of these parameters\n
        \  from the server, this header field value MAY contain a semicolon-\n   separated
        list of implementation-specific attribute names.\n"
      title: 6.2.16.  Vendor-Specific Parameters
    title: 6.2.  Generic Message Headers
  - contents:
    - "6.3.  Generic Result Structure\n   Result data from the server for the Recognizer
      and Verifier resources\n   is carried as a typed media entity in the MRCPv2
      message body of\n   various events.  The Natural Language Semantics Markup Language\n
      \  (NLSML), an XML markup based on an early draft from the W3C, is the\n   default
      standard for returning results back to the client.  Hence,\n   all servers implementing
      these resource types MUST support the media\n   type 'application/nlsml+xml'.
      \ The Extensible MultiModal Annotation\n   (EMMA) [W3C.REC-emma-20090210] format
      can be used to return results\n   as well.  This can be done by negotiating
      the format at session\n   establishment time with SDP (a=resultformat:application/emma+xml)
      or\n   with SIP (Allow/Accept).  With SIP, for example, if a client wants\n
      \  results in EMMA, an MRCPv2 server can route the request to another\n   server
      that supports EMMA by inspecting the SIP header fields, rather\n   than having
      to inspect the SDP.\n   MRCPv2 uses this representation to convey content among
      the clients\n   and servers that generate and make use of the markup.  MRCPv2
      uses\n   NSLML specifically to convey recognition, enrollment, and\n   verification
      results between the corresponding resource on the MRCPv2\n   server and the
      MRCPv2 client.  Details of this result format are\n   fully described in Section
      6.3.1.\n   Content-Type:application/nlsml+xml\n   Content-Length:...\n   <?xml
      version=\"1.0\"?>\n   <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n           xmlns:ex=\"http://www.example.com/example\"\n
      \          grammar=\"http://theYesNoGrammar\">\n       <interpretation>\n           <instance>\n
      \                  <ex:response>yes</ex:response>\n           </instance>\n
      \          <input>OK</input>\n       </interpretation>\n   </result>\n                              Result
      Example\n"
    - contents:
      - "6.3.1.  Natural Language Semantics Markup Language\n   The Natural Language
        Semantics Markup Language (NLSML) is an XML data\n   structure with elements
        and attributes designed to carry result\n   information from recognizer (including
        enrollment) and verifier\n   resources.  The normative definition of NLSML
        is the RelaxNG schema\n   in Section 16.1.  Note that the elements and attributes
        of this\n   format are defined in the MRCPv2 namespace.  In the result structure,\n
        \  they must either be prefixed by a namespace prefix declared within\n   the
        result or must be children of an element identified as belonging\n   to the
        respective namespace.  For details on how to use XML\n   Namespaces, see [W3C.REC-xml-names11-20040204].
        \ Section 2 of\n   [W3C.REC-xml-names11-20040204] provides details on how
        to declare\n   namespaces and namespace prefixes.\n   The root element of
        NLSML is <result>.  Optional child elements are\n   <interpretation>, <enrollment-result>,
        and <verification-result>, at\n   least one of which must be present.  A single
        <result> MAY contain\n   any or all of the optional child elements.  Details
        of the <result>\n   and <interpretation> elements and their subelements and
        attributes\n   can be found in Section 9.6.  Details of the <enrollment-result>\n
        \  element and its subelements can be found in Section 9.7.  Details of\n
        \  the <verification-result> element and its subelements can be found in\n
        \  Section 11.5.2.\n"
      title: 6.3.1.  Natural Language Semantics Markup Language
    title: 6.3.  Generic Result Structure
  title: 6.  MRCPv2 Generic Methods, Headers, and Result Structure
- contents:
  - "7.  Resource Discovery\n   Server resources may be discovered and their capabilities
    learned by\n   clients through standard SIP machinery.  The client MAY issue a
    SIP\n   OPTIONS transaction to a server, which has the effect of requesting\n
    \  the capabilities of the server.  The server MUST respond to such a\n   request
    with an SDP-encoded description of its capabilities according\n   to RFC 3264
    [RFC3264].  The MRCPv2 capabilities are described by a\n   single \"m=\" line
    containing the media type \"application\" and\n   transport type \"TCP/TLS/MRCPv2\"
    or \"TCP/MRCPv2\".  There MUST be one\n   \"resource\" attribute for each media
    resource that the server\n   supports, and it has the resource type identifier
    as its value.\n   The SDP description MUST also contain \"m=\" lines describing
    the audio\n   capabilities and the coders the server supports.\n   In this example,
    the client uses the SIP OPTIONS method to query the\n   capabilities of the MRCPv2
    server.\n   C->S:\n        OPTIONS sip:mrcp@server.example.com SIP/2.0\n        Via:SIP/2.0/TCP
    client.atlanta.example.com:5060;\n         branch=z9hG4bK74bf7\n        Max-Forwards:6\n
    \       To:<sip:mrcp@example.com>\n        From:Sarvi <sip:sarvi@example.com>;tag=1928301774\n
    \       Call-ID:a84b4c76e66710\n        CSeq:63104 OPTIONS\n        Contact:<sip:sarvi@client.example.com>\n
    \       Accept:application/sdp\n        Content-Length:0\n   S->C:\n        SIP/2.0
    200 OK\n        Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n         branch=z9hG4bK74bf7;received=192.0.32.10\n
    \       To:<sip:mrcp@example.com>;tag=62784\n        From:Sarvi <sip:sarvi@example.com>;tag=1928301774\n
    \       Call-ID:a84b4c76e66710\n        CSeq:63104 OPTIONS\n        Contact:<sip:mrcp@server.example.com>\n
    \       Allow:INVITE, ACK, CANCEL, OPTIONS, BYE\n        Accept:application/sdp\n
    \       Accept-Encoding:gzip\n        Accept-Language:en\n        Supported:foo\n
    \       Content-Type:application/sdp\n        Content-Length:...\n        v=0\n
    \       o=sarvi 2890844536 2890842811 IN IP4 192.0.2.12\n        s=-\n        i=MRCPv2
    server capabilities\n        c=IN IP4 192.0.2.12/127\n        t=0 0\n        m=application
    0 TCP/TLS/MRCPv2 1\n        a=resource:speechsynth\n        a=resource:speechrecog\n
    \       a=resource:speakverify\n        m=audio 0 RTP/AVP 0 3\n        a=rtpmap:0
    PCMU/8000\n        a=rtpmap:3 GSM/8000\n         Using SIP OPTIONS for MRCPv2
    Server Capability Discovery\n"
  title: 7.  Resource Discovery
- contents:
  - "8.  Speech Synthesizer Resource\n   This resource processes text markup provided
    by the client and\n   generates a stream of synthesized speech in real time.  Depending\n
    \  upon the server implementation and capability of this resource, the\n   client
    can also dictate parameters of the synthesized speech such as\n   voice characteristics,
    speaker speed, etc.\n   The synthesizer resource is controlled by MRCPv2 requests
    from the\n   client.  Similarly, the resource can respond to these requests or\n
    \  generate asynchronous events to the client to indicate conditions of\n   interest
    to the client during the generation of the synthesized\n   speech stream.\n   This
    section applies for the following resource types:\n   o  speechsynth\n   o  basicsynth\n
    \  The capabilities of these resources are defined in Section 3.1.\n"
  - contents:
    - "8.1.  Synthesizer State Machine\n   The synthesizer maintains a state machine
      to process MRCPv2 requests\n   from the client.  The state transitions shown
      below describe the\n   states of the synthesizer and reflect the state of the
      request at the\n   head of the synthesizer resource queue.  A SPEAK request
      in the\n   PENDING state can be deleted or stopped by a STOP request without\n
      \  affecting the state of the resource.\n   Idle                    Speaking
      \                 Paused\n   State                   State                     State\n
      \    |                        |                          |\n     |----------SPEAK-------->|
      \                |--------|\n     |<------STOP-------------|             CONTROL
      \     |\n     |<----SPEAK-COMPLETE-----|                 |------->|\n     |<----BARGE-IN-OCCURRED--|
      \                         |\n     |              |---------|                          |\n
      \    |          CONTROL       |-----------PAUSE--------->|\n     |              |-------->|<----------RESUME---------|\n
      \    |                        |               |----------|\n     |----------|
      \            |              PAUSE       |\n     |    BARGE-IN-OCCURRED   |               |--------->|\n
      \    |<---------|             |----------|               |\n     |                        |
      \     SPEECH-MARKER       |\n     |                        |<---------|               |\n
      \    |----------|             |----------|               |\n     |         STOP
      \          |       RESUME             |\n     |          |             |<---------|
      \              |\n     |<---------|             |                          |\n
      \    |<---------------------STOP-------------------------|\n     |----------|
      \            |                          |\n     |     DEFINE-LEXICON     |                          |\n
      \    |          |             |                          |\n     |<---------|
      \            |                          |\n     |<---------------BARGE-IN-OCCURRED------------------|\n
      \                        Synthesizer State Machine\n"
    title: 8.1.  Synthesizer State Machine
  - contents:
    - "8.2.  Synthesizer Methods\n   The synthesizer supports the following methods.\n
      \  synthesizer-method   =  \"SPEAK\"\n                        /  \"STOP\"\n
      \                       /  \"PAUSE\"\n                        /  \"RESUME\"\n
      \                       /  \"BARGE-IN-OCCURRED\"\n                        /
      \ \"CONTROL\"\n                        /  \"DEFINE-LEXICON\"\n"
    title: 8.2.  Synthesizer Methods
  - contents:
    - "8.3.  Synthesizer Events\n   The synthesizer can generate the following events.\n
      \  synthesizer-event    =  \"SPEECH-MARKER\"\n                        /  \"SPEAK-COMPLETE\"\n"
    title: 8.3.  Synthesizer Events
  - contents:
    - "8.4.  Synthesizer Header Fields\n   A synthesizer method can contain header
      fields containing request\n   options and information to augment the Request,
      Response, or Event it\n   is associated with.\n   synthesizer-header  =  jump-size\n
      \                      /  kill-on-barge-in\n                       /  speaker-profile\n
      \                      /  completion-cause\n                       /  completion-reason\n
      \                      /  voice-parameter\n                       /  prosody-parameter\n
      \                      /  speech-marker\n                       /  speech-language\n
      \                      /  fetch-hint\n                       /  audio-fetch-hint\n
      \                      /  failed-uri\n                       /  failed-uri-cause\n
      \                      /  speak-restart\n                       /  speak-length\n
      \                      /  load-lexicon\n                       /  lexicon-search-order\n"
    - contents:
      - "8.4.1.  Jump-Size\n   This header field MAY be specified in a CONTROL method
        and controls\n   the amount to jump forward or backward in an active SPEAK
        request.  A\n   '+' or '-' indicates a relative value to what is being currently\n
        \  played.  This header field MAY also be specified in a SPEAK request\n   as
        a desired offset into the synthesized speech.  In this case, the\n   synthesizer
        MUST begin speaking from this amount of time into the\n   speech markup.  Note
        that an offset that extends beyond the end of\n   the produced speech will
        result in audio of length zero.  The\n   different speech length units supported
        are dependent on the\n   synthesizer implementation.  If the synthesizer resource
        does not\n   support a unit for the operation, the resource MUST respond with
        a\n   status-code of 409 \"Unsupported Header Field Value\".\n   jump-size
        \            =   \"Jump-Size\" \":\" speech-length-value CRLF\n   speech-length-value
        \  =   numeric-speech-length\n                         /   text-speech-length\n
        \  text-speech-length    =   1*UTFCHAR SP \"Tag\"\n   numeric-speech-length
        =    (\"+\" / \"-\") positive-speech-length\n   positive-speech-length =   1*19DIGIT
        SP numeric-speech-unit\n   numeric-speech-unit   =   \"Second\"\n                         /
        \  \"Word\"\n                         /   \"Sentence\"\n                         /
        \  \"Paragraph\"\n"
      title: 8.4.1.  Jump-Size
    - contents:
      - "8.4.2.  Kill-On-Barge-In\n   This header field MAY be sent as part of the
        SPEAK method to enable\n   \"kill-on-barge-in\" support.  If enabled, the
        SPEAK method is\n   interrupted by DTMF input detected by a signal detector
        resource or\n   by the start of speech sensed or recognized by the speech
        recognizer\n   resource.\n   kill-on-barge-in      =   \"Kill-On-Barge-In\"
        \":\" BOOLEAN CRLF\n   The client MUST send a BARGE-IN-OCCURRED method to
        the synthesizer\n   resource when it receives a barge-in-able event from any
        source.\n   This source could be a synthesizer resource or signal detector\n
        \  resource and MAY be either local or distributed.  If this header\n   field
        is not specified in a SPEAK request or explicitly set by a\n   SET-PARAMS,
        the default value for this header field is \"true\".\n   If the recognizer
        or signal detector resource is on the same server\n   as the synthesizer and
        both are part of the same session, the server\n   MAY work with both to provide
        internal notification to the\n   synthesizer so that audio may be stopped
        without having to wait for\n   the client's BARGE-IN-OCCURRED event.\n   It
        is generally RECOMMENDED when playing a prompt to the user with\n   Kill-On-Barge-In
        and asking for input, that the client issue the\n   RECOGNIZE request ahead
        of the SPEAK request for optimum performance\n   and user experience.  This
        way, it is guaranteed that the recognizer\n   is online before the prompt
        starts playing and the user's speech will\n   not be truncated at the beginning
        (especially for power users).\n"
      title: 8.4.2.  Kill-On-Barge-In
    - contents:
      - "8.4.3.  Speaker-Profile\n   This header field MAY be part of the SET-PARAMS/GET-PARAMS
        or SPEAK\n   request from the client to the server and specifies a URI that\n
        \  references the profile of the speaker.  Speaker profiles are\n   collections
        of voice parameters like gender, accent, etc.\n   speaker-profile       =
        \  \"Speaker-Profile\" \":\" uri CRLF\n"
      title: 8.4.3.  Speaker-Profile
    - contents:
      - "8.4.4.  Completion-Cause\n   This header field MUST be specified in a SPEAK-COMPLETE
        event coming\n   from the synthesizer resource to the client.  This indicates
        the\n   reason the SPEAK request completed.\n   completion-cause      =   \"Completion-Cause\"
        \":\" 3DIGIT SP\n                             1*VCHAR CRLF\n   +------------+-----------------------+------------------------------+\n
        \  | Cause-Code | Cause-Name            | Description                  |\n
        \  +------------+-----------------------+------------------------------+\n
        \  | 000        | normal                | SPEAK completed normally.    |\n
        \  | 001        | barge-in              | SPEAK request was terminated |\n
        \  |            |                       | because of barge-in.         |\n
        \  | 002        | parse-failure         | SPEAK request terminated     |\n
        \  |            |                       | because of a failure to      |\n
        \  |            |                       | parse the speech markup      |\n
        \  |            |                       | text.                        |\n
        \  | 003        | uri-failure           | SPEAK request terminated     |\n
        \  |            |                       | because access to one of the |\n
        \  |            |                       | URIs failed.                 |\n
        \  | 004        | error                 | SPEAK request terminated     |\n
        \  |            |                       | prematurely due to           |\n
        \  |            |                       | synthesizer error.           |\n
        \  | 005        | language-unsupported  | Language not supported.      |\n
        \  | 006        | lexicon-load-failure  | Lexicon loading failed.      |\n
        \  | 007        | cancelled             | A prior SPEAK request failed |\n
        \  |            |                       | while this one was still in  |\n
        \  |            |                       | the queue.                   |\n
        \  +------------+-----------------------+------------------------------+\n
        \               Synthesizer Resource Completion Cause Codes\n"
      title: 8.4.4.  Completion-Cause
    - contents:
      - "8.4.5.  Completion-Reason\n   This header field MAY be specified in a SPEAK-COMPLETE
        event coming\n   from the synthesizer resource to the client.  This contains
        the\n   reason text behind the SPEAK request completion.  This header field\n
        \  communicates text describing the reason for the failure, such as an\n   error
        in parsing the speech markup text.\n   completion-reason   =   \"Completion-Reason\"
        \":\"\n                           quoted-string CRLF\n   The completion reason
        text is provided for client use in logs and for\n   debugging and instrumentation
        purposes.  Clients MUST NOT interpret\n   the completion reason text.\n"
      title: 8.4.5.  Completion-Reason
    - contents:
      - "8.4.6.  Voice-Parameter\n   This set of header fields defines the voice of
        the speaker.\n   voice-parameter    =   voice-gender\n                       /
        \  voice-age\n                       /   voice-variant\n                       /
        \  voice-name\n   voice-gender        =   \"Voice-Gender:\" voice-gender-value
        CRLF\n   voice-gender-value  =   \"male\"\n                       /   \"female\"\n
        \                      /   \"neutral\"\n   voice-age           =   \"Voice-Age:\"
        1*3DIGIT CRLF\n   voice-variant       =   \"Voice-Variant:\" 1*19DIGIT CRLF\n
        \  voice-name          =   \"Voice-Name:\"\n                           1*UTFCHAR
        *(1*WSP 1*UTFCHAR) CRLF\n   The \"Voice-\" parameters are derived from the
        similarly named\n   attributes of the voice element specified in W3C's Speech
        Synthesis\n   Markup Language Specification (SSML)\n   [W3C.REC-speech-synthesis-20040907].
        \ Legal values for these\n   parameters are as defined in that specification.\n
        \  These header fields MAY be sent in SET-PARAMS or GET-PARAMS requests\n
        \  to define or get default values for the entire session or MAY be sent\n
        \  in the SPEAK request to define default values for that SPEAK request.\n
        \  Note that SSML content can itself set these values internal to the\n   SSML
        document, of course.\n   Voice parameter header fields MAY also be sent in
        a CONTROL method to\n   affect a SPEAK request in progress and change its
        behavior on the\n   fly.  If the synthesizer resource does not support this
        operation, it\n   MUST reject the request with a status-code of 403 \"Unsupported
        Header\n   Field\".\n"
      title: 8.4.6.  Voice-Parameter
    - contents:
      - "8.4.7.  Prosody-Parameters\n   This set of header fields defines the prosody
        of the speech.\n   prosody-parameter   =   \"Prosody-\" prosody-param-name
        \":\"\n                           prosody-param-value CRLF\n   prosody-param-name
        \   =    1*VCHAR\n   prosody-param-value   =    1*VCHAR\n   prosody-param-name
        is any one of the attribute names under the\n   prosody element specified
        in W3C's Speech Synthesis Markup Language\n   Specification [W3C.REC-speech-synthesis-20040907].
        \ The prosody-\n   param-value is any one of the value choices of the corresponding\n
        \  prosody element attribute from that specification.\n   These header fields
        MAY be sent in SET-PARAMS or GET-PARAMS requests\n   to define or get default
        values for the entire session or MAY be sent\n   in the SPEAK request to define
        default values for that SPEAK request.\n   Furthermore, these attributes can
        be part of the speech text marked\n   up in SSML.\n   The prosody parameter
        header fields in the SET-PARAMS or SPEAK\n   request only apply if the speech
        data is of type 'text/plain' and\n   does not use a speech markup format.\n
        \  These prosody parameter header fields MAY also be sent in a CONTROL\n   method
        to affect a SPEAK request in progress and change its behavior\n   on the fly.
        \ If the synthesizer resource does not support this\n   operation, it MUST
        respond back to the client with a status-code of\n   403 \"Unsupported Header
        Field\".\n"
      title: 8.4.7.  Prosody-Parameters
    - contents:
      - "8.4.8.  Speech-Marker\n   This header field contains timestamp information
        in a \"timestamp\"\n   field.  This is a Network Time Protocol (NTP) [RFC5905]
        timestamp, a\n   64-bit number in decimal form.  It MUST be synced with the
        Real-Time\n   Protocol (RTP) [RFC3550] timestamp of the media stream through
        the\n   Real-Time Control Protocol (RTCP) [RFC3550].\n   Markers are bookmarks
        that are defined within the markup.  Most\n   speech markup formats provide
        mechanisms to embed marker fields\n   within speech texts.  The synthesizer
        generates SPEECH-MARKER events\n   when it reaches these marker fields.  This
        header field MUST be part\n   of the SPEECH-MARKER event and contain the marker
        tag value after the\n   timestamp, separated by a semicolon.  In these events,
        the timestamp\n   marks the time the text corresponding to the marker was
        emitted as\n   speech by the synthesizer.\n   This header field MUST also
        be returned in responses to STOP,\n   CONTROL, and BARGE-IN-OCCURRED methods,
        in the SPEAK-COMPLETE event,\n   and in an IN-PROGRESS SPEAK response.  In
        these messages, if any\n   markers have been encountered for the current SPEAK,
        the marker tag\n   value MUST be the last embedded marker encountered.  If
        no markers\n   have yet been encountered for the current SPEAK, only the timestamp\n
        \  is REQUIRED.  Note that in these events, the purpose of this header\n   field
        is to provide timestamp information associated with important\n   events within
        the lifecycle of a request (start of SPEAK processing,\n   end of SPEAK processing,
        receipt of CONTROL/STOP/BARGE-IN-OCCURRED).\n   timestamp           =   \"timestamp\"
        \"=\" time-stamp-value\n   time-stamp-value    =   1*20DIGIT\n   speech-marker
        \      =   \"Speech-Marker\" \":\"\n                           timestamp\n
        \                          [\";\" 1*(UTFCHAR / %x20)] CRLF\n"
      title: 8.4.8.  Speech-Marker
    - contents:
      - "8.4.9.  Speech-Language\n   This header field specifies the default language
        of the speech data\n   if the language is not specified in the markup.  The
        value of this\n   header field MUST follow RFC 5646 [RFC5646] for its values.
        \ The\n   header field MAY occur in SPEAK, SET-PARAMS, or GET-PARAMS requests.\n
        \  speech-language     =   \"Speech-Language\" \":\" 1*VCHAR CRLF\n"
      title: 8.4.9.  Speech-Language
    - contents:
      - "8.4.10.  Fetch-Hint\n   When the synthesizer needs to fetch documents or
        other resources like\n   speech markup or audio files, this header field controls
        the\n   corresponding URI access properties.  This provides client policy
        on\n   when the synthesizer should retrieve content from the server.  A\n
        \  value of \"prefetch\" indicates the content MAY be downloaded when the\n
        \  request is received, whereas \"safe\" indicates that content MUST NOT\n
        \  be downloaded until actually referenced.  The default value is\n   \"prefetch\".
        \ This header field MAY occur in SPEAK, SET-PARAMS, or\n   GET-PARAMS requests.\n
        \  fetch-hint          =   \"Fetch-Hint\" \":\" (\"prefetch\" / \"safe\")
        CRLF\n"
      title: 8.4.10.  Fetch-Hint
    - contents:
      - "8.4.11.  Audio-Fetch-Hint\n   When the synthesizer needs to fetch documents
        or other resources like\n   speech audio files, this header field controls
        the corresponding URI\n   access properties.  This provides client policy
        whether or not the\n   synthesizer is permitted to attempt to optimize speech
        by pre-\n   fetching audio.  The value is either \"safe\" to say that audio
        is only\n   fetched when it is referenced, never before; \"prefetch\" to permit,\n
        \  but not require the implementation to pre-fetch the audio; or\n   \"stream\"
        to allow it to stream the audio fetches.  The default value\n   is \"prefetch\".
        \ This header field MAY occur in SPEAK, SET-PARAMS, or\n   GET-PARAMS requests.\n
        \  audio-fetch-hint    =   \"Audio-Fetch-Hint\" \":\"\n                           (\"prefetch\"
        / \"safe\" / \"stream\") CRLF\n"
      title: 8.4.11.  Audio-Fetch-Hint
    - contents:
      - "8.4.12.  Failed-URI\n   When a synthesizer method needs a synthesizer to
        fetch or access a\n   URI and the access fails, the server SHOULD provide
        the failed URI in\n   this header field in the method response, unless there
        are multiple\n   URI failures, in which case the server MUST provide one of
        the failed\n   URIs in this header field in the method response.\n   failed-uri
        \         =   \"Failed-URI\" \":\" absoluteURI CRLF\n"
      title: 8.4.12.  Failed-URI
    - contents:
      - "8.4.13.  Failed-URI-Cause\n   When a synthesizer method needs a synthesizer
        to fetch or access a\n   URI and the access fails, the server MUST provide
        the URI-specific or\n   protocol-specific response code for the URI in the
        Failed-URI header\n   field in the method response through this header field.
        \ The value\n   encoding is UTF-8 (RFC 3629 [RFC3629]) to accommodate any
        access\n   protocol -- some access protocols might have a response string\n
        \  instead of a numeric response code.\n   failed-uri-cause    =   \"Failed-URI-Cause\"
        \":\" 1*UTFCHAR CRLF\n"
      title: 8.4.13.  Failed-URI-Cause
    - contents:
      - "8.4.14.  Speak-Restart\n   When a client issues a CONTROL request to a currently
        speaking\n   synthesizer resource to jump backward, and the target jump point
        is\n   before the start of the current SPEAK request, the current SPEAK\n
        \  request MUST restart from the beginning of its speech data and the\n   server's
        response to the CONTROL request MUST contain this header\n   field with a
        value of \"true\" indicating a restart.\n   speak-restart       =   \"Speak-Restart\"
        \":\" BOOLEAN CRLF\n"
      title: 8.4.14.  Speak-Restart
    - contents:
      - "8.4.15.  Speak-Length\n   This header field MAY be specified in a CONTROL
        method to control the\n   maximum length of speech to speak, relative to the
        current speaking\n   point in the currently active SPEAK request.  If numeric,
        the value\n   MUST be a positive integer.  If a header field with a Tag unit
        is\n   specified, then the speech output continues until the tag is reached\n
        \  or the SPEAK request is completed, whichever comes first.  This\n   header
        field MAY be specified in a SPEAK request to indicate the\n   length to speak
        from the speech data and is relative to the point in\n   speech that the SPEAK
        request starts.  The different speech length\n   units supported are synthesizer
        implementation dependent.  If a\n   server does not support the specified
        unit, the server MUST respond\n   with a status-code of 409 \"Unsupported
        Header Field Value\".\n   speak-length          =   \"Speak-Length\" \":\"
        positive-length-value\n                             CRLF\n   positive-length-value
        =   positive-speech-length\n                         /   text-speech-length\n
        \  text-speech-length    =   1*UTFCHAR SP \"Tag\"\n   positive-speech-length
        =  1*19DIGIT SP numeric-speech-unit\n   numeric-speech-unit   =   \"Second\"\n
        \                        /   \"Word\"\n                         /   \"Sentence\"\n
        \                        /   \"Paragraph\"\n"
      title: 8.4.15.  Speak-Length
    - contents:
      - "8.4.16.  Load-Lexicon\n   This header field is used to indicate whether a
        lexicon has to be\n   loaded or unloaded.  The value \"true\" means to load
        the lexicon if\n   not already loaded, and the value \"false\" means to unload
        the lexicon\n   if it is loaded.  The default value for this header field
        is \"true\".\n   This header field MAY be specified in a DEFINE-LEXICON method.\n
        \  load-lexicon       =   \"Load-Lexicon\" \":\" BOOLEAN CRLF\n"
      title: 8.4.16.  Load-Lexicon
    - contents:
      - "8.4.17.  Lexicon-Search-Order\n   This header field is used to specify a
        list of active pronunciation\n   lexicon URIs and the search order among the
        active lexicons.\n   Lexicons specified within the SSML document take precedence
        over the\n   lexicons specified in this header field.  This header field MAY
        be\n   specified in the SPEAK, SET-PARAMS, and GET-PARAMS methods.\n   lexicon-search-order
        =   \"Lexicon-Search-Order\" \":\"\n             \"<\" absoluteURI \">\" *(\"
        \" \"<\" absoluteURI \">\") CRLF\n"
      title: 8.4.17.  Lexicon-Search-Order
    title: 8.4.  Synthesizer Header Fields
  - contents:
    - "8.5.  Synthesizer Message Body\n   A synthesizer message can contain additional
      information associated\n   with the Request, Response, or Event in its message
      body.\n"
    - contents:
      - "8.5.1.  Synthesizer Speech Data\n   Marked-up text for the synthesizer to
        speak is specified as a typed\n   media entity in the message body.  The speech
        data to be spoken by\n   the synthesizer can be specified inline by embedding
        the data in the\n   message body or by reference by providing a URI for accessing
        the\n   data.  In either case, the data and the format used to markup the\n
        \  speech needs to be of a content type supported by the server.\n   All MRCPv2
        servers containing synthesizer resources MUST support both\n   plain text
        speech data and W3C's Speech Synthesis Markup Language\n   [W3C.REC-speech-synthesis-20040907]
        and hence MUST support the media\n   types 'text/plain' and 'application/ssml+xml'.
        \ Other formats MAY be\n   supported.\n   If the speech data is to be fetched
        by URI reference, the media type\n   'text/uri-list' (see RFC 2483 [RFC2483])
        is used to indicate one or\n   more URIs that, when dereferenced, will contain
        the content to be\n   spoken.  If a list of speech URIs is specified, the
        resource MUST\n   speak the speech data provided by each URI in the order
        in which the\n   URIs are specified in the content.\n   MRCPv2 clients and
        servers MUST support the 'multipart/mixed' media\n   type.  This is the appropriate
        media type to use when providing a mix\n   of URI and inline speech data.
        \ Embedded within the multipart content\n   block, there MAY be content for
        the 'text/uri-list', 'application/\n   ssml+xml', and/or 'text/plain' media
        types.  The character set and\n   encoding used in the speech data is specified
        according to standard\n   media type definitions.  The multipart content MAY
        also contain\n   actual audio data.  Clients may have recorded audio clips
        stored in\n   memory or on a local device and wish to play it as part of the
        SPEAK\n   request.  The audio portions MAY be sent by the client as part of
        the\n   multipart content block.  This audio is referenced in the speech\n
        \  markup data that is another part in the multipart content block\n   according
        to the 'multipart/mixed' media type specification.\n   Content-Type:text/uri-list\n
        \  Content-Length:...\n   http://www.example.com/ASR-Introduction.ssml\n   http://www.example.com/ASR-Document-Part1.ssml\n
        \  http://www.example.com/ASR-Document-Part2.ssml\n   http://www.example.com/ASR-Conclusion.ssml\n
        \                            URI List Example\n   Content-Type:application/ssml+xml\n
        \  Content-Length:...\n   <?xml version=\"1.0\"?>\n        <speak version=\"1.0\"\n
        \              xmlns=\"http://www.w3.org/2001/10/synthesis\"\n               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n
        \              xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
        \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n
        \              xml:lang=\"en-US\">\n          <p>\n            <s>You have
        4 new messages.</s>\n            <s>The first is from Aldine Turnbet\n            and
        arrived at <break/>\n            <say-as interpret-as=\"vxml:time\">0345p</say-as>.</s>\n
        \           <s>The subject is <prosody\n            rate=\"-20%\">ski trip</prosody></s>\n
        \        </p>\n        </speak>\n                               SSML Example\n
        \  Content-Type:multipart/mixed; boundary=\"break\"\n   --break\n   Content-Type:text/uri-list\n
        \  Content-Length:...\n   http://www.example.com/ASR-Introduction.ssml\n   http://www.example.com/ASR-Document-Part1.ssml\n
        \  http://www.example.com/ASR-Document-Part2.ssml\n   http://www.example.com/ASR-Conclusion.ssml\n
        \  --break\n   Content-Type:application/ssml+xml\n   Content-Length:...\n
        \  <?xml version=\"1.0\"?>\n       <speak version=\"1.0\"\n              xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
        \             xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n              xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
        \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n
        \             xml:lang=\"en-US\">\n          <p>\n            <s>You have
        4 new messages.</s>\n            <s>The first is from Stephanie Williams\n
        \           and arrived at <break/>\n            <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n
        \           <s>The subject is <prosody\n            rate=\"-20%\">ski trip</prosody></s>\n
        \         </p>\n       </speak>\n   --break--\n                             Multipart
        Example\n"
      title: 8.5.1.  Synthesizer Speech Data
    - contents:
      - "8.5.2.  Lexicon Data\n   Synthesizer lexicon data from the client to the
        server can be\n   provided inline or by reference.  Either way, they are carried
        as\n   typed media in the message body of the MRCPv2 request message (see\n
        \  Section 8.14).\n   When a lexicon is specified inline in the message, the
        client MUST\n   provide a Content-ID for that lexicon as part of the content
        header\n   fields.  The server MUST store the lexicon associated with that\n
        \  Content-ID for the duration of the session.  A stored lexicon can be\n
        \  overwritten by defining a new lexicon with the same Content-ID.\n   Lexicons
        that have been associated with a Content-ID can be\n   referenced through
        the 'session' URI scheme (see Section 13.6).\n   If lexicon data is specified
        by external URI reference, the media\n   type 'text/uri-list' (see RFC 2483
        [RFC2483] ) is used to list the\n   one or more URIs that may be dereferenced
        to obtain the lexicon data.\n   All MRCPv2 servers MUST support the \"http\"
        and \"https\" URI access\n   mechanisms, and MAY support other mechanisms.\n
        \  If the data in the message body consists of a mix of URI and inline\n   lexicon
        data, the 'multipart/mixed' media type is used.  The\n   character set and
        encoding used in the lexicon data may be specified\n   according to standard
        media type definitions.\n"
      title: 8.5.2.  Lexicon Data
    title: 8.5.  Synthesizer Message Body
  - contents:
    - "8.6.  SPEAK Method\n   The SPEAK request provides the synthesizer resource
      with the speech\n   text and initiates speech synthesis and streaming.  The
      SPEAK method\n   MAY carry voice and prosody header fields that alter the behavior
      of\n   the voice being synthesized, as well as a typed media message body\n
      \  containing the actual marked-up text to be spoken.\n   The SPEAK method implementation
      MUST do a fetch of all external URIs\n   that are part of that operation.  If
      caching is implemented, this URI\n   fetching MUST conform to the cache-control
      hints and parameter header\n   fields associated with the method in deciding
      whether it is to be\n   fetched from cache or from the external server.  If
      these hints/\n   parameters are not specified in the method, the values set
      for the\n   session using SET-PARAMS/GET-PARAMS apply.  If it was not set for
      the\n   session, their default values apply.\n   When applying voice parameters,
      there are three levels of precedence.\n   The highest precedence are those specified
      within the speech markup\n   text, followed by those specified in the header
      fields of the SPEAK\n   request and hence that apply for that SPEAK request
      only, followed by\n   the session default values that can be set using the SET-PARAMS\n
      \  request and apply for subsequent methods invoked during the session.\n   If
      the resource was idle at the time the SPEAK request arrived at the\n   server
      and the SPEAK method is being actively processed, the resource\n   responds
      immediately with a success status code and a request-state\n   of IN-PROGRESS.\n
      \  If the resource is in the speaking or paused state when the SPEAK\n   method
      arrives at the server, i.e., it is in the middle of processing\n   a previous
      SPEAK request, the status returns success with a request-\n   state of PENDING.
      \ The server places the SPEAK request in the\n   synthesizer resource request
      queue.  The request queue operates\n   strictly FIFO: requests are processed
      serially in order of receipt.\n   If the current SPEAK fails, all SPEAK methods
      in the pending queue\n   are cancelled and each generates a SPEAK-COMPLETE event
      with a\n   Completion-Cause of \"cancelled\".\n   For the synthesizer resource,
      SPEAK is the only method that can\n   return a request-state of IN-PROGRESS
      or PENDING.  When the text has\n   been synthesized and played into the media
      stream, the resource\n   issues a SPEAK-COMPLETE event with the request-id of
      the SPEAK\n   request and a request-state of COMPLETE.\n   C->S: MRCP/2.0 ...
      SPEAK 543257\n         Channel-Identifier:32AECB23433802@speechsynth\n         Voice-gender:neutral\n
      \        Voice-Age:25\n         Prosody-volume:medium\n         Content-Type:application/ssml+xml\n
      \        Content-Length:...\n         <?xml version=\"1.0\"?>\n            <speak
      version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams and arrived at\n                <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.\n                </s>\n
      \            <s>The subject is\n                    <prosody rate=\"-20%\">ski
      trip</prosody>\n             </s>\n            </p>\n           </speak>\n   S->C:
      MRCP/2.0 ... 543257 200 IN-PROGRESS\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Speech-Marker:timestamp=857206027059\n   S->C: MRCP/2.0 ... SPEAK-COMPLETE
      543257 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n         Completion-Cause:000
      normal\n         Speech-Marker:timestamp=857206027059\n                               SPEAK
      Example\n"
    title: 8.6.  SPEAK Method
  - contents:
    - "8.7.  STOP\n   The STOP method from the client to the server tells the synthesizer\n
      \  resource to stop speaking if it is speaking something.\n   The STOP request
      can be sent with an Active-Request-Id-List header\n   field to stop the zero
      or more specific SPEAK requests that may be in\n   queue and return a response
      status-code of 200 \"Success\".  If no\n   Active-Request-Id-List header field
      is sent in the STOP request, the\n   server terminates all outstanding SPEAK
      requests.\n   If a STOP request successfully terminated one or more PENDING
      or\n   IN-PROGRESS SPEAK requests, then the response MUST contain an Active-\n
      \  Request-Id-List header field enumerating the SPEAK request-ids that\n   were
      terminated.  Otherwise, there is no Active-Request-Id-List\n   header field
      in the response.  No SPEAK-COMPLETE events are sent for\n   such terminated
      requests.\n   If a SPEAK request that was IN-PROGRESS and speaking was stopped,
      the\n   next pending SPEAK request, if any, becomes IN-PROGRESS at the\n   resource
      and enters the speaking state.\n   If a SPEAK request that was IN-PROGRESS and
      paused was stopped, the\n   next pending SPEAK request, if any, becomes IN-PROGRESS
      and enters\n   the paused state.\n   C->S: MRCP/2.0 ... SPEAK 543258\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Content-Type:application/ssml+xml\n         Content-Length:...\n         <?xml
      version=\"1.0\"?>\n           <speak version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams and arrived at\n                <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n             <s>The
      subject is\n                 <prosody rate=\"-20%\">ski trip</prosody></s>\n
      \           </p>\n           </speak>\n   S->C: MRCP/2.0 ... 543258 200 IN-PROGRESS\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n         Speech-Marker:timestamp=857206027059\n
      \  C->S: MRCP/2.0 ... STOP 543259\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \  S->C: MRCP/2.0 ... 543259 200 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Active-Request-Id-List:543258\n         Speech-Marker:timestamp=857206039059\n
      \                              STOP Example\n"
    title: 8.7.  STOP
  - contents:
    - "8.8.  BARGE-IN-OCCURRED\n   The BARGE-IN-OCCURRED method, when used with the
      synthesizer\n   resource, provides a client that has detected a barge-in-able
      event a\n   means to communicate the occurrence of the event to the synthesizer\n
      \  resource.\n   This method is useful in two scenarios:\n   1.  The client
      has detected DTMF digits in the input media or some\n       other barge-in-able
      event and wants to communicate that to the\n       synthesizer resource.\n   2.
      \ The recognizer resource and the synthesizer resource are in\n       different
      servers.  In this case, the client acts as an\n       intermediary for the two
      servers.  It receives an event from the\n       recognition resource and sends
      a BARGE-IN-OCCURRED request to the\n       synthesizer.  In such cases, the
      BARGE-IN-OCCURRED method would\n       also have a Proxy-Sync-Id header field
      received from the resource\n       generating the original event.\n   If a SPEAK
      request is active with kill-on-barge-in enabled (see\n   Section 8.4.2), and
      the BARGE-IN-OCCURRED event is received, the\n   synthesizer MUST immediately
      stop streaming out audio.  It MUST also\n   terminate any speech requests queued
      behind the current active one,\n   irrespective of whether or not they have
      barge-in enabled.  If a\n   barge-in-able SPEAK request was playing and it was
      terminated, the\n   response MUST contain an Active-Request-Id-List header field
      listing\n   the request-ids of all SPEAK requests that were terminated.  The\n
      \  server generates no SPEAK-COMPLETE events for these requests.\n   If there
      were no SPEAK requests terminated by the synthesizer\n   resource as a result
      of the BARGE-IN-OCCURRED method, the server MUST\n   respond to the BARGE-IN-OCCURRED
      with a status-code of 200 \"Success\",\n   and the response MUST NOT contain
      an Active-Request-Id-List header\n   field.\n   If the synthesizer and recognizer
      resources are part of the same\n   MRCPv2 session, they can be optimized for
      a quicker kill-on-barge-in\n   response if the recognizer and synthesizer interact
      directly.  In\n   these cases, the client MUST still react to a START-OF-INPUT
      event\n   from the recognizer by invoking the BARGE-IN-OCCURRED method to the\n
      \  synthesizer.  The client MUST invoke the BARGE-IN-OCCURRED if it has\n   any
      outstanding requests to the synthesizer resource in either the\n   PENDING or
      IN-PROGRESS state.\n   C->S: MRCP/2.0 ... SPEAK 543258\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Voice-gender:neutral\n         Voice-Age:25\n         Prosody-volume:medium\n
      \        Content-Type:application/ssml+xml\n         Content-Length:...\n         <?xml
      version=\"1.0\"?>\n           <speak version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams and arrived at\n                <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n             <s>The
      subject is\n                <prosody rate=\"-20%\">ski trip</prosody></s>\n
      \           </p>\n           </speak>\n   S->C: MRCP/2.0 ... 543258 200 IN-PROGRESS\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n         Speech-Marker:timestamp=857206027059\n
      \  C->S: MRCP/2.0 ... BARGE-IN-OCCURRED 543259\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Proxy-Sync-Id:987654321\n   S->C:MRCP/2.0 ... 543259 200 COMPLETE\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n         Active-Request-Id-List:543258\n
      \        Speech-Marker:timestamp=857206039059\n                         BARGE-IN-OCCURRED
      Example\n"
    title: 8.8.  BARGE-IN-OCCURRED
  - contents:
    - "8.9.  PAUSE\n   The PAUSE method from the client to the server tells the synthesizer\n
      \  resource to pause speech output if it is speaking something.  If a\n   PAUSE
      method is issued on a session when a SPEAK is not active, the\n   server MUST
      respond with a status-code of 402 \"Method not valid in\n   this state\".  If
      a PAUSE method is issued on a session when a SPEAK\n   is active and paused,
      the server MUST respond with a status-code of\n   200 \"Success\".  If a SPEAK
      request was active, the server MUST return\n   an Active-Request-Id-List header
      field whose value contains the\n   request-id of the SPEAK request that was
      paused.\n   C->S: MRCP/2.0 ... SPEAK 543258\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Voice-gender:neutral\n         Voice-Age:25\n         Prosody-volume:medium\n
      \        Content-Type:application/ssml+xml\n         Content-Length:...\n         <?xml
      version=\"1.0\"?>\n           <speak version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams and arrived at\n                <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n             <s>The
      subject is\n                <prosody rate=\"-20%\">ski trip</prosody></s>\n
      \           </p>\n           </speak>\n   S->C: MRCP/2.0 ... 543258 200 IN-PROGRESS\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n         Speech-Marker:timestamp=857206027059\n
      \  C->S: MRCP/2.0 ... PAUSE 543259\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \  S->C: MRCP/2.0 ... 543259 200 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Active-Request-Id-List:543258\n                               PAUSE
      Example\n"
    title: 8.9.  PAUSE
  - contents:
    - "8.10.  RESUME\n   The RESUME method from the client to the server tells a paused\n
      \  synthesizer resource to resume speaking.  If a RESUME request is\n   issued
      on a session with no active SPEAK request, the server MUST\n   respond with
      a status-code of 402 \"Method not valid in this state\".\n   If a RESUME request
      is issued on a session with an active SPEAK\n   request that is speaking (i.e.,
      not paused), the server MUST respond\n   with a status-code of 200 \"Success\".
      \ If a SPEAK request was paused,\n   the server MUST return an Active-Request-Id-List
      header field whose\n   value contains the request-id of the SPEAK request that
      was resumed.\n   C->S: MRCP/2.0 ... SPEAK 543258\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Voice-gender:neutral\n         Voice-age:25\n         Prosody-volume:medium\n
      \        Content-Type:application/ssml+xml\n         Content-Length:...\n         <?xml
      version=\"1.0\"?>\n           <speak version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams and arrived at\n                <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n             <s>The
      subject is\n                <prosody rate=\"-20%\">ski trip</prosody></s>\n
      \           </p>\n           </speak>\n   S->C: MRCP/2.0 ... 543258 200 IN-PROGRESS@speechsynth\n
      \        Channel-Identifier:32AECB23433802\n         Speech-Marker:timestamp=857206027059\n
      \  C->S: MRCP/2.0 ... PAUSE 543259\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \  S->C: MRCP/2.0 ... 543259 200 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Active-Request-Id-List:543258\n   C->S: MRCP/2.0 ... RESUME 543260\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n   S->C: MRCP/2.0 ...
      543260 200 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Active-Request-Id-List:543258\n                              RESUME
      Example\n"
    title: 8.10.  RESUME
  - contents:
    - "8.11.  CONTROL\n   The CONTROL method from the client to the server tells a
      synthesizer\n   that is speaking to modify what it is speaking on the fly.  This\n
      \  method is used to request the synthesizer to jump forward or backward\n   in
      what it is speaking, change speaker rate, speaker parameters, etc.\n   It affects
      only the currently IN-PROGRESS SPEAK request.  Depending\n   on the implementation
      and capability of the synthesizer resource, it\n   may or may not support the
      various modifications indicated by header\n   fields in the CONTROL request.\n
      \  When a client invokes a CONTROL method to jump forward and the\n   operation
      goes beyond the end of the active SPEAK method's text, the\n   CONTROL request
      still succeeds.  The active SPEAK request completes\n   and returns a SPEAK-COMPLETE
      event following the response to the\n   CONTROL method.  If there are more SPEAK
      requests in the queue, the\n   synthesizer resource starts at the beginning
      of the next SPEAK\n   request in the queue.\n   When a client invokes a CONTROL
      method to jump backward and the\n   operation jumps to the beginning or beyond
      the beginning of the\n   speech data of the active SPEAK method, the CONTROL
      request still\n   succeeds.  The response to the CONTROL request contains the
      speak-\n   restart header field, and the active SPEAK request restarts from
      the\n   beginning of its speech data.\n   These two behaviors can be used to
      rewind or fast-forward across\n   multiple speech requests, if the client wants
      to break up a speech\n   markup text into multiple SPEAK requests.\n   If a
      SPEAK request was active when the CONTROL method was received,\n   the server
      MUST return an Active-Request-Id-List header field\n   containing the request-id
      of the SPEAK request that was active.\n   C->S: MRCP/2.0 ... SPEAK 543258\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n         Voice-gender:neutral\n
      \        Voice-age:25\n         Prosody-volume:medium\n         Content-Type:application/ssml+xml\n
      \        Content-Length:...\n         <?xml version=\"1.0\"?>\n           <speak
      version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams\n                and arrived at <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n             <s>The
      subject is <prosody\n                rate=\"-20%\">ski trip</prosody></s>\n
      \           </p>\n           </speak>\n   S->C: MRCP/2.0 ... 543258 200 IN-PROGRESS\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n         Speech-Marker:timestamp=857205016059\n
      \  C->S: MRCP/2.0 ... CONTROL 543259\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Prosody-rate:fast\n   S->C: MRCP/2.0 ... 543259 200 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Active-Request-Id-List:543258\n         Speech-Marker:timestamp=857206027059\n
      \  C->S: MRCP/2.0 ... CONTROL 543260\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Jump-Size:-15 Words\n   S->C: MRCP/2.0 ... 543260 200 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Active-Request-Id-List:543258\n         Speech-Marker:timestamp=857206039059\n
      \                             CONTROL Example\n"
    title: 8.11.  CONTROL
  - contents:
    - "8.12.  SPEAK-COMPLETE\n   This is an Event message from the synthesizer resource
      to the client\n   that indicates the corresponding SPEAK request was completed.
      \ The\n   request-id field matches the request-id of the SPEAK request that\n
      \  initiated the speech that just completed.  The request-state field is\n   set
      to COMPLETE by the server, indicating that this is the last event\n   with the
      corresponding request-id.  The Completion-Cause header field\n   specifies the
      cause code pertaining to the status and reason of\n   request completion, such
      as the SPEAK completed normally or because\n   of an error, kill-on-barge-in,
      etc.\n   C->S: MRCP/2.0 ... SPEAK 543260\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Voice-gender:neutral\n         Voice-age:25\n         Prosody-volume:medium\n
      \        Content-Type:application/ssml+xml\n         Content-Length:...\n         <?xml
      version=\"1.0\"?>\n           <speak version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams\n                and arrived at <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n             <s>The
      subject is\n                <prosody rate=\"-20%\">ski trip</prosody></s>\n
      \           </p>\n           </speak>\n   S->C: MRCP/2.0 ... 543260 200 IN-PROGRESS\n
      \        Channel-Identifier:32AECB23433802@speechsynth\n         Speech-Marker:timestamp=857206027059\n
      \  S->C: MRCP/2.0 ... SPEAK-COMPLETE 543260 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Completion-Cause:000 normal\n         Speech-Marker:timestamp=857206039059\n
      \                         SPEAK-COMPLETE Example\n"
    title: 8.12.  SPEAK-COMPLETE
  - contents:
    - "8.13.  SPEECH-MARKER\n   This is an event generated by the synthesizer resource
      to the client\n   when the synthesizer encounters a marker tag in the speech
      markup it\n   is currently processing.  The value of the request-id field MUST\n
      \  match that of the corresponding SPEAK request.  The request-state\n   field
      MUST have the value \"IN-PROGRESS\" as the speech is still not\n   complete.
      \ The value of the speech marker tag hit, describing where\n   the synthesizer
      is in the speech markup, MUST be returned in the\n   Speech-Marker header field,
      along with an NTP timestamp indicating\n   the instant in the output speech
      stream that the marker was\n   encountered.  The SPEECH-MARKER event MUST also
      be generated with a\n   null marker value and output NTP timestamp when a SPEAK
      request in\n   Pending-State (i.e., in the queue) changes state to IN-PROGRESS
      and\n   starts speaking.  The NTP timestamp MUST be synchronized with the RTP\n
      \  timestamp used to generate the speech stream through standard RTCP\n   machinery.\n
      \  C->S: MRCP/2.0 ... SPEAK 543261\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Voice-gender:neutral\n         Voice-age:25\n         Prosody-volume:medium\n
      \        Content-Type:application/ssml+xml\n         Content-Length:...\n         <?xml
      version=\"1.0\"?>\n           <speak version=\"1.0\"\n                xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \               xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                  http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                xml:lang=\"en-US\">\n
      \           <p>\n             <s>You have 4 new messages.</s>\n             <s>The
      first is from Stephanie Williams\n                and arrived at <break/>\n
      \               <say-as interpret-as=\"vxml:time\">0342p</say-as>.</s>\n                <mark
      name=\"here\"/>\n             <s>The subject is\n                <prosody rate=\"-20%\">ski
      trip</prosody>\n             </s>\n             <mark name=\"ANSWER\"/>\n            </p>\n
      \          </speak>\n   S->C: MRCP/2.0 ... 543261 200 IN-PROGRESS\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Speech-Marker:timestamp=857205015059\n   S->C: MRCP/2.0 ... SPEECH-MARKER
      543261 IN-PROGRESS\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Speech-Marker:timestamp=857206027059;here\n   S->C: MRCP/2.0 ... SPEECH-MARKER
      543261 IN-PROGRESS\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Speech-Marker:timestamp=857206039059;ANSWER\n   S->C: MRCP/2.0 ...
      SPEAK-COMPLETE 543261 COMPLETE\n         Channel-Identifier:32AECB23433802@speechsynth\n
      \        Completion-Cause:000 normal\n         Speech-Marker:timestamp=857207689259;ANSWER\n
      \                          SPEECH-MARKER Example\n"
    title: 8.13.  SPEECH-MARKER
  - contents:
    - "8.14.  DEFINE-LEXICON\n   The DEFINE-LEXICON method, from the client to the
      server, provides a\n   lexicon and tells the server to load or unload the lexicon
      (see\n   Section 8.4.16).  The media type of the lexicon is provided in the\n
      \  Content-Type header (see Section 8.5.2).  One such media type is\n   \"application/pls+xml\"
      for the Pronunciation Lexicon Specification\n   (PLS) [W3C.REC-pronunciation-lexicon-20081014]
      [RFC4267].\n   If the server resource is in the speaking or paused state, the
      server\n   MUST respond with a failure status-code of 402 \"Method not valid
      in\n   this state\".\n   If the resource is in the idle state and is able to
      successfully\n   load/unload the lexicon, the status MUST return a 200 \"Success\"\n
      \  status-code and the request-state MUST be COMPLETE.\n   If the synthesizer
      could not define the lexicon for some reason, for\n   example, because the download
      failed or the lexicon was in an\n   unsupported form, the server MUST respond
      with a failure status-code\n   of 407 and a Completion-Cause header field describing
      the failure\n   reason.\n"
    title: 8.14.  DEFINE-LEXICON
  title: 8.  Speech Synthesizer Resource
- contents:
  - "9.  Speech Recognizer Resource\n   The speech recognizer resource receives an
    incoming voice stream and\n   provides the client with an interpretation of what
    was spoken in\n   textual form.\n   The recognizer resource is controlled by MRCPv2
    requests from the\n   client.  The recognizer resource can both respond to these
    requests\n   and generate asynchronous events to the client to indicate conditions\n
    \  of interest during the processing of the method.\n   This section applies to
    the following resource types.\n   1.  speechrecog\n   2.  dtmfrecog\n   The difference
    between the above two resources is in their level of\n   support for recognition
    grammars.  The \"dtmfrecog\" resource type is\n   capable of recognizing only
    DTMF digits and hence accepts only DTMF\n   grammars.  It only generates barge-in
    for DTMF inputs and ignores\n   speech.  The \"speechrecog\" resource type can
    recognize regular speech\n   as well as DTMF digits and hence MUST support grammars
    describing\n   either speech or DTMF.  This resource generates barge-in events
    for\n   speech and/or DTMF.  By analyzing the grammars that are activated by\n
    \  the RECOGNIZE method, it determines if a barge-in should occur for\n   speech
    and/or DTMF.  When the recognizer decides it needs to generate\n   a barge-in,
    it also generates a START-OF-INPUT event to the client.\n   The recognizer resource
    MAY support recognition in the normal or\n   hotword modes or both (although note
    that a single \"speechrecog\"\n   resource does not perform normal and hotword
    mode recognition\n   simultaneously).  For implementations where a single recognizer\n
    \  resource does not support both modes, or simultaneous normal and\n   hotword
    recognition is desired, the two modes can be invoked through\n   separate resources
    allocated to the same SIP dialog (with different\n   MRCP session identifiers)
    and share the RTP audio feed.\n   The capabilities of the recognizer resource
    are enumerated below:\n   Normal Mode Recognition  Normal mode recognition tries
    to match all\n      of the speech or DTMF against the grammar and returns a no-match\n
    \     status if the input fails to match or the method times out.\n   Hotword
    Mode Recognition  Hotword mode is where the recognizer looks\n      for a match
    against specific speech grammar or DTMF sequence and\n      ignores speech or
    DTMF that does not match.  The recognition\n      completes only if there is a
    successful match of grammar, if the\n      client cancels the request, or if there
    is a non-input or\n      recognition timeout.\n   Voice Enrolled Grammars  A recognizer
    resource MAY optionally support\n      Voice Enrolled Grammars.  With this functionality,
    enrollment is\n      performed using a person's voice.  For example, a list of
    contacts\n      can be created and maintained by recording the person's names\n
    \     using the caller's voice.  This technique is sometimes also called\n      speaker-dependent
    recognition.\n   Interpretation  A recognizer resource MAY be employed strictly
    for\n      its natural language interpretation capabilities by supplying it\n
    \     with a text string as input instead of speech.  In this mode, the\n      resource
    takes text as input and produces an \"interpretation\" of\n      the input according
    to the supplied grammar.\n   Voice enrollment has the concept of an enrollment
    session.  A session\n   to add a new phrase to a personal grammar involves the
    initial\n   enrollment followed by a repeat of enough utterances before\n   committing
    the new phrase to the personal grammar.  Each time an\n   utterance is recorded,
    it is compared for similarity with the other\n   samples and a clash test is performed
    against other entries in the\n   personal grammar to ensure there are no similar
    and confusable\n   entries.\n   Enrollment is done using a recognizer resource.
    \ Controlling which\n   utterances are to be considered for enrollment of a new
    phrase is\n   done by setting a header field (see Section 9.4.39) in the Recognize\n
    \  request.\n   Interpretation is accomplished through the INTERPRET method\n
    \  (Section 9.20) and the Interpret-Text header field (Section 9.4.30).\n"
  - contents:
    - "9.1.  Recognizer State Machine\n   The recognizer resource maintains a state
      machine to process MRCPv2\n   requests from the client.\n   Idle                   Recognizing
      \              Recognized\n   State                  State                     State\n
      \   |                       |                          |\n    |---------RECOGNIZE---->|---RECOGNITION-COMPLETE-->|\n
      \   |<------STOP------------|<-----RECOGNIZE-----------|\n    |                       |
      \                         |\n    |              |--------|              |-----------|\n
      \   |       START-OF-INPUT  |       GET-RESULT         |\n    |              |------->|
      \             |---------->|\n    |------------|          |                          |\n
      \   |      DEFINE-GRAMMAR   |----------|               |\n    |<-----------|
      \         | START-INPUT-TIMERS       |\n    |                       |<---------|
      \              |\n    |------|                |                          |\n
      \   |  INTERPRET            |                          |\n    |<-----|                |------|
      \                  |\n    |                       |   RECOGNIZE              |\n
      \   |-------|               |<-----|                   |\n    |      STOP                                        |\n
      \   |<------|                                          |\n    |<-------------------STOP--------------------------|\n
      \   |<-------------------DEFINE-GRAMMAR----------------|\n                         Recognizer
      State Machine\n   If a recognizer resource supports voice enrolled grammars,
      starting\n   an enrollment session does not change the state of the recognizer\n
      \  resource.  Once an enrollment session is started, then utterances are\n   enrolled
      by calling the RECOGNIZE method repeatedly.  The state of\n   the speech recognizer
      resource goes from IDLE to RECOGNIZING state\n   each time RECOGNIZE is called.\n"
    title: 9.1.  Recognizer State Machine
  - contents:
    - "9.2.  Recognizer Methods\n   The recognizer supports the following methods.\n
      \  recognizer-method    =  recog-only-method\n                        /  enrollment-method\n
      \  recog-only-method    =  \"DEFINE-GRAMMAR\"\n                        /  \"RECOGNIZE\"\n
      \                       /  \"INTERPRET\"\n                        /  \"GET-RESULT\"\n
      \                       /  \"START-INPUT-TIMERS\"\n                        /
      \ \"STOP\"\n   It is OPTIONAL for a recognizer resource to support voice enrolled\n
      \  grammars.  If the recognizer resource does support voice enrolled\n   grammars,
      it MUST support the following methods.\n   enrollment-method    =  \"START-PHRASE-ENROLLMENT\"\n
      \                       /  \"ENROLLMENT-ROLLBACK\"\n                        /
      \ \"END-PHRASE-ENROLLMENT\"\n                        /  \"MODIFY-PHRASE\"\n
      \                       /  \"DELETE-PHRASE\"\n"
    title: 9.2.  Recognizer Methods
  - contents:
    - "9.3.  Recognizer Events\n   The recognizer can generate the following events.\n
      \  recognizer-event     =  \"START-OF-INPUT\"\n                        /  \"RECOGNITION-COMPLETE\"\n
      \                       /  \"INTERPRETATION-COMPLETE\"\n"
    title: 9.3.  Recognizer Events
  - contents:
    - "9.4.  Recognizer Header Fields\n   A recognizer message can contain header
      fields containing request\n   options and information to augment the Method,
      Response, or Event\n   message it is associated with.\n   recognizer-header
      \   =  recog-only-header\n                        /  enrollment-header\n   recog-only-header
      \   =  confidence-threshold\n                        /  sensitivity-level\n
      \                       /  speed-vs-accuracy\n                        /  n-best-list-length\n
      \                       /  no-input-timeout\n                        /  input-type\n
      \                       /  recognition-timeout\n                        /  waveform-uri\n
      \                       /  input-waveform-uri\n                        /  completion-cause\n
      \                       /  completion-reason\n                        /  recognizer-context-block\n
      \                       /  start-input-timers\n                        /  speech-complete-timeout\n
      \                       /  speech-incomplete-timeout\n                        /
      \ dtmf-interdigit-timeout\n                        /  dtmf-term-timeout\n                        /
      \ dtmf-term-char\n                        /  failed-uri\n                        /
      \ failed-uri-cause\n                        /  save-waveform\n                        /
      \ media-type\n                        /  new-audio-channel\n                        /
      \ speech-language\n                        /  ver-buffer-utterance\n                        /
      \ recognition-mode\n                        /  cancel-if-queue\n                        /
      \ hotword-max-duration\n                        /  hotword-min-duration\n                        /
      \ interpret-text\n                        /  dtmf-buffer-time\n                        /
      \ clear-dtmf-buffer\n                        /  early-no-match\n   If a recognizer
      resource supports voice enrolled grammars, the\n   following header fields are
      also used.\n   enrollment-header    =  num-min-consistent-pronunciations\n                        /
      \ consistency-threshold\n                        /  clash-threshold\n                        /
      \ personal-grammar-uri\n                        /  enroll-utterance\n                        /
      \ phrase-id\n                        /  phrase-nl\n                        /
      \ weight\n                        /  save-best-waveform\n                        /
      \ new-phrase-id\n                        /  confusable-phrases-uri\n                        /
      \ abort-phrase-enrollment\n   For enrollment-specific header fields that can
      appear as part of\n   SET-PARAMS or GET-PARAMS methods, the following general
      rule applies:\n   the START-PHRASE-ENROLLMENT method MUST be invoked before
      these\n   header fields may be set through the SET-PARAMS method or retrieved\n
      \  through the GET-PARAMS method.\n   Note that the Waveform-URI header field
      of the Recognizer resource\n   can also appear in the response to the END-PHRASE-ENROLLMENT
      method.\n"
    - contents:
      - "9.4.1.  Confidence-Threshold\n   When a recognizer resource recognizes or
        matches a spoken phrase with\n   some portion of the grammar, it associates
        a confidence level with\n   that match.  The Confidence-Threshold header field
        tells the\n   recognizer resource what confidence level the client considers
        a\n   successful match.  This is a float value between 0.0-1.0 indicating\n
        \  the recognizer's confidence in the recognition.  If the recognizer\n   determines
        that there is no candidate match with a confidence that is\n   greater than
        the confidence threshold, then it MUST return no-match\n   as the recognition
        result.  This header field MAY occur in RECOGNIZE,\n   SET-PARAMS, or GET-PARAMS.
        \ The default value for this header field\n   is implementation specific,
        as is the interpretation of any specific\n   value for this header field.
        \ Although values for servers from\n   different vendors are not comparable,
        it is expected that clients\n   will tune this value over time for a given
        server.\n   confidence-threshold     =  \"Confidence-Threshold\" \":\" FLOAT
        CRLF\n"
      title: 9.4.1.  Confidence-Threshold
    - contents:
      - "9.4.2.  Sensitivity-Level\n   To filter out background noise and not mistake
        it for speech, the\n   recognizer resource supports a variable level of sound
        sensitivity.\n   The Sensitivity-Level header field is a float value between
        0.0 and\n   1.0 and allows the client to set the sensitivity level for the\n
        \  recognizer.  This header field MAY occur in RECOGNIZE, SET-PARAMS, or\n
        \  GET-PARAMS.  A higher value for this header field means higher\n   sensitivity.
        \ The default value for this header field is\n   implementation specific,
        as is the interpretation of any specific\n   value for this header field.
        \ Although values for servers from\n   different vendors are not comparable,
        it is expected that clients\n   will tune this value over time for a given
        server.\n   sensitivity-level        =  \"Sensitivity-Level\" \":\" FLOAT
        CRLF\n"
      title: 9.4.2.  Sensitivity-Level
    - contents:
      - "9.4.3.  Speed-Vs-Accuracy\n   Depending on the implementation and capability
        of the recognizer\n   resource it may be tunable towards Performance or Accuracy.
        \ Higher\n   accuracy may mean more processing and higher CPU utilization,
        meaning\n   fewer active sessions per server and vice versa.  The value is
        a\n   float between 0.0 and 1.0.  A value of 0.0 means fastest recognition.\n
        \  A value of 1.0 means best accuracy.  This header field MAY occur in\n   RECOGNIZE,
        SET-PARAMS, or GET-PARAMS.  The default value for this\n   header field is
        implementation specific.  Although values for servers\n   from different vendors
        are not comparable, it is expected that\n   clients will tune this value over
        time for a given server.\n   speed-vs-accuracy        =  \"Speed-Vs-Accuracy\"
        \":\" FLOAT CRLF\n"
      title: 9.4.3.  Speed-Vs-Accuracy
    - contents:
      - "9.4.4.  N-Best-List-Length\n   When the recognizer matches an incoming stream
        with the grammar, it\n   may come up with more than one alternative match
        because of\n   confidence levels in certain words or conversation paths.  If
        this\n   header field is not specified, by default, the recognizer resource\n
        \  returns only the best match above the confidence threshold.  The\n   client,
        by setting this header field, can ask the recognition\n   resource to send
        it more than one alternative.  All alternatives must\n   still be above the
        Confidence-Threshold.  A value greater than one\n   does not guarantee that
        the recognizer will provide the requested\n   number of alternatives.  This
        header field MAY occur in RECOGNIZE,\n   SET-PARAMS, or GET-PARAMS.  The minimum
        value for this header field\n   is 1.  The default value for this header field
        is 1.\n   n-best-list-length       =  \"N-Best-List-Length\" \":\" 1*19DIGIT
        CRLF\n"
      title: 9.4.4.  N-Best-List-Length
    - contents:
      - "9.4.5.  Input-Type\n   When the recognizer detects barge-in-able input and
        generates a\n   START-OF-INPUT event, that event MUST carry this header field
        to\n   specify whether the input that caused the barge-in was DTMF or\n   speech.\n
        \  input-type         =  \"Input-Type\" \":\"  inputs CRLF\n   inputs             =
        \ \"speech\" / \"dtmf\"\n"
      title: 9.4.5.  Input-Type
    - contents:
      - "9.4.6.  No-Input-Timeout\n   When recognition is started and there is no
        speech detected for a\n   certain period of time, the recognizer can send
        a RECOGNITION-\n   COMPLETE event to the client with a Completion-Cause of
        \"no-input-\n   timeout\" and terminate the recognition operation.  The client
        can use\n   the No-Input-Timeout header field to set this timeout.  The value
        is\n   in milliseconds and can range from 0 to an implementation-specific\n
        \  maximum value.  This header field MAY occur in RECOGNIZE, SET-PARAMS,\n
        \  or GET-PARAMS.  The default value is implementation specific.\n   no-input-timeout
        \        =  \"No-Input-Timeout\" \":\" 1*19DIGIT CRLF\n"
      title: 9.4.6.  No-Input-Timeout
    - contents:
      - "9.4.7.  Recognition-Timeout\n   When recognition is started and there is
        no match for a certain\n   period of time, the recognizer can send a RECOGNITION-COMPLETE
        event\n   to the client and terminate the recognition operation.  The\n   Recognition-Timeout
        header field allows the client to set this\n   timeout value.  The value is
        in milliseconds.  The value for this\n   header field ranges from 0 to an
        implementation-specific maximum\n   value.  The default value is 10 seconds.
        \ This header field MAY occur\n   in RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n
        \  recognition-timeout      =  \"Recognition-Timeout\" \":\" 1*19DIGIT CRLF\n"
      title: 9.4.7.  Recognition-Timeout
    - contents:
      - "9.4.8.  Waveform-URI\n   If the Save-Waveform header field is set to \"true\",
        the recognizer\n   MUST record the incoming audio stream of the recognition
        into a\n   stored form and provide a URI for the client to access it.  This\n
        \  header field MUST be present in the RECOGNITION-COMPLETE event if the\n
        \  Save-Waveform header field was set to \"true\".  The value of the\n   header
        field MUST be empty if there was some error condition\n   preventing the server
        from recording.  Otherwise, the URI generated\n   by the server MUST be unambiguous
        across the server and all its\n   recognition sessions.  The content associated
        with the URI MUST be\n   available to the client until the MRCPv2 session
        terminates.\n   Similarly, if the Save-Best-Waveform header field is set to
        \"true\",\n   the recognizer MUST save the audio stream for the best repetition
        of\n   the phrase that was used during the enrollment session.  The\n   recognizer
        MUST then record the recognized audio and make it\n   available to the client
        by returning a URI in the Waveform-URI header\n   field in the response to
        the END-PHRASE-ENROLLMENT method.  The value\n   of the header field MUST
        be empty if there was some error condition\n   preventing the server from
        recording.  Otherwise, the URI generated\n   by the server MUST be unambiguous
        across the server and all its\n   recognition sessions.  The content associated
        with the URI MUST be\n   available to the client until the MRCPv2 session
        terminates.  See the\n   discussion on the sensitivity of saved waveforms
        in Section 12.\n   The server MUST also return the size in octets and the
        duration in\n   milliseconds of the recorded audio waveform as parameters
        associated\n   with the header field.\n   waveform-uri             =  \"Waveform-URI\"
        \":\" [\"<\" uri \">\"\n                               \";\" \"size\" \"=\"
        1*19DIGIT\n                               \";\" \"duration\" \"=\" 1*19DIGIT]
        CRLF\n"
      title: 9.4.8.  Waveform-URI
    - contents:
      - "9.4.9.  Media-Type\n   This header field MAY be specified in the SET-PARAMS,
        GET-PARAMS, or\n   the RECOGNIZE methods and tells the server resource the
        media type in\n   which to store captured audio or video, such as the one
        captured and\n   returned by the Waveform-URI header field.\n   media-type
        \              =  \"Media-Type\" \":\" media-type-value\n                               CRLF\n"
      title: 9.4.9.  Media-Type
    - contents:
      - "9.4.10.  Input-Waveform-URI\n   This optional header field specifies a URI
        pointing to audio content\n   to be processed by the RECOGNIZE operation.
        \ This enables the client\n   to request recognition from a specified buffer
        or audio file.\n   input-waveform-uri       =  \"Input-Waveform-URI\" \":\"
        uri CRLF\n"
      title: 9.4.10.  Input-Waveform-URI
    - contents:
      - "9.4.11.  Completion-Cause\n   This header field MUST be part of a RECOGNITION-COMPLETE
        event coming\n   from the recognizer resource to the client.  It indicates
        the reason\n   behind the RECOGNIZE method completion.  This header field
        MUST be\n   sent in the DEFINE-GRAMMAR and RECOGNIZE responses, if they return\n
        \  with a failure status and a COMPLETE state.  In the ABNF below, the\n   cause-code
        contains a numerical value selected from the Cause-Code\n   column of the
        following table.  The cause-name contains the\n   corresponding token selected
        from the Cause-Name column.\n   completion-cause         =  \"Completion-Cause\"
        \":\" cause-code SP\n                               cause-name CRLF\n   cause-code
        \              =  3DIGIT\n   cause-name               =  *VCHAR\n   +------------+-----------------------+------------------------------+\n
        \  | Cause-Code | Cause-Name            | Description                  |\n
        \  +------------+-----------------------+------------------------------+\n
        \  | 000        | success               | RECOGNIZE completed with a   |\n
        \  |            |                       | match or DEFINE-GRAMMAR      |\n
        \  |            |                       | succeeded in downloading and |\n
        \  |            |                       | compiling the grammar.       |\n
        \  |            |                       |                              |\n
        \  | 001        | no-match              | RECOGNIZE completed, but no  |\n
        \  |            |                       | match was found.             |\n
        \  |            |                       |                              |\n
        \  | 002        | no-input-timeout      | RECOGNIZE completed without  |\n
        \  |            |                       | a match due to a             |\n
        \  |            |                       | no-input-timeout.            |\n
        \  |            |                       |                              |\n
        \  | 003        | hotword-maxtime       | RECOGNIZE in hotword mode    |\n
        \  |            |                       | completed without a match    |\n
        \  |            |                       | due to a                     |\n
        \  |            |                       | recognition-timeout.         |\n
        \  |            |                       |                              |\n
        \  | 004        | grammar-load-failure  | RECOGNIZE failed due to      |\n
        \  |            |                       | grammar load failure.        |\n
        \  |            |                       |                              |\n
        \  | 005        | grammar-compilation-  | RECOGNIZE failed due to      |\n
        \  |            | failure               | grammar compilation failure. |\n
        \  |            |                       |                              |\n
        \  | 006        | recognizer-error      | RECOGNIZE request terminated |\n
        \  |            |                       | prematurely due to a         |\n
        \  |            |                       | recognizer error.            |\n
        \  |            |                       |                              |\n
        \  | 007        | speech-too-early      | RECOGNIZE request terminated |\n
        \  |            |                       | because speech was too       |\n
        \  |            |                       | early. This happens when the |\n
        \  |            |                       | audio stream is already      |\n
        \  |            |                       | \"in-speech\" when the         |\n
        \  |            |                       | RECOGNIZE request was        |\n
        \  |            |                       | received.                    |\n
        \  |            |                       |                              |\n
        \  | 008        | success-maxtime       | RECOGNIZE request terminated |\n
        \  |            |                       | because speech was too long  |\n
        \  |            |                       | but whatever was spoken till |\n
        \  |            |                       | that point was a full match. |\n
        \  |            |                       |                              |\n
        \  | 009        | uri-failure           | Failure accessing a URI.     |\n
        \  |            |                       |                              |\n
        \  | 010        | language-unsupported  | Language not supported.      |\n
        \  |            |                       |                              |\n
        \  | 011        | cancelled             | A new RECOGNIZE cancelled    |\n
        \  |            |                       | this one, or a prior         |\n
        \  |            |                       | RECOGNIZE failed while this  |\n
        \  |            |                       | one was still in the queue.  |\n
        \  |            |                       |                              |\n
        \  | 012        | semantics-failure     | Recognition succeeded, but   |\n
        \  |            |                       | semantic interpretation of   |\n
        \  |            |                       | the recognized input failed. |\n
        \  |            |                       | The RECOGNITION-COMPLETE     |\n
        \  |            |                       | event MUST contain the       |\n
        \  |            |                       | Recognition result with only |\n
        \  |            |                       | input text and no            |\n
        \  |            |                       | interpretation.              |\n
        \  |            |                       |                              |\n
        \  | 013        | partial-match         | Speech Incomplete Timeout    |\n
        \  |            |                       | expired before there was a   |\n
        \  |            |                       | full match. But whatever was |\n
        \  |            |                       | spoken till that point was a |\n
        \  |            |                       | partial match to one or more |\n
        \  |            |                       | grammars.                    |\n
        \  |            |                       |                              |\n
        \  | 014        | partial-match-maxtime | The Recognition-Timeout      |\n
        \  |            |                       | expired before full match    |\n
        \  |            |                       | was achieved. But whatever   |\n
        \  |            |                       | was spoken till that point   |\n
        \  |            |                       | was a partial match to one   |\n
        \  |            |                       | or more grammars.            |\n
        \  |            |                       |                              |\n
        \  | 015        | no-match-maxtime      | The Recognition-Timeout      |\n
        \  |            |                       | expired. Whatever was spoken |\n
        \  |            |                       | till that point did not      |\n
        \  |            |                       | match any of the grammars.   |\n
        \  |            |                       | This cause could also be     |\n
        \  |            |                       | returned if the recognizer   |\n
        \  |            |                       | does not support detecting   |\n
        \  |            |                       | partial grammar matches.     |\n
        \  |            |                       |                              |\n
        \  | 016        | grammar-definition-   | Any DEFINE-GRAMMAR error     |\n
        \  |            | failure               | other than                   |\n
        \  |            |                       | grammar-load-failure and     |\n
        \  |            |                       | grammar-compilation-failure. |\n
        \  +------------+-----------------------+------------------------------+\n"
      title: 9.4.11.  Completion-Cause
    - contents:
      - "9.4.12.  Completion-Reason\n   This header field MAY be specified in a RECOGNITION-COMPLETE
        event\n   coming from the recognizer resource to the client.  This contains
        the\n   reason text behind the RECOGNIZE request completion.  The server uses\n
        \  this header field to communicate text describing the reason for the\n   failure,
        such as the specific error encountered in parsing a grammar\n   markup.\n
        \  The completion reason text is provided for client use in logs and for\n
        \  debugging and instrumentation purposes.  Clients MUST NOT interpret\n   the
        completion reason text.\n   completion-reason        =  \"Completion-Reason\"
        \":\"\n                               quoted-string CRLF\n"
      title: 9.4.12.  Completion-Reason
    - contents:
      - "9.4.13.  Recognizer-Context-Block\n   This header field MAY be sent as part
        of the SET-PARAMS or GET-PARAMS\n   request.  If the GET-PARAMS method contains
        this header field with no\n   value, then it is a request to the recognizer
        to return the\n   recognizer context block.  The response to such a message
        MAY contain\n   a recognizer context block as a typed media message body.
        \ If the\n   server returns a recognizer context block, the response MUST
        contain\n   this header field and its value MUST match the Content-ID of the\n
        \  corresponding media block.\n   If the SET-PARAMS method contains this header
        field, it MUST also\n   contain a message body containing the recognizer context
        data and a\n   Content-ID matching this header field value.  This Content-ID
        MUST\n   match the Content-ID that came with the context data during the\n
        \  GET-PARAMS operation.\n   An implementation choosing to use this mechanism
        to hand off\n   recognizer context data between servers MUST distinguish its\n
        \  implementation-specific block of data by using an IANA-registered\n   content
        type in the IANA Media Type vendor tree.\n   recognizer-context-block  =  \"Recognizer-Context-Block\"
        \":\"\n                                [1*VCHAR] CRLF\n"
      title: 9.4.13.  Recognizer-Context-Block
    - contents:
      - "9.4.14.  Start-Input-Timers\n   This header field MAY be sent as part of
        the RECOGNIZE request.  A\n   value of false tells the recognizer to start
        recognition but not to\n   start the no-input timer yet.  The recognizer MUST
        NOT start the\n   timers until the client sends a START-INPUT-TIMERS request
        to the\n   recognizer.  This is useful in the scenario when the recognizer
        and\n   synthesizer engines are not part of the same session.  In such\n   configurations,
        when a kill-on-barge-in prompt is being played (see\n   Section 8.4.2), the
        client wants the RECOGNIZE request to be\n   simultaneously active so that
        it can detect and implement kill-on-\n   barge-in.  However, the recognizer
        SHOULD NOT start the no-input\n   timers until the prompt is finished.  The
        default value is \"true\".\n   start-input-timers  =  \"Start-Input-Timers\"
        \":\" BOOLEAN CRLF\n"
      title: 9.4.14.  Start-Input-Timers
    - contents:
      - "9.4.15.  Speech-Complete-Timeout\n   This header field specifies the length
        of silence required following\n   user speech before the speech recognizer
        finalizes a result (either\n   accepting it or generating a no-match result).
        \ The Speech-Complete-\n   Timeout value applies when the recognizer currently
        has a complete\n   match against an active grammar, and specifies how long
        the\n   recognizer MUST wait for more input before declaring a match.  By\n
        \  contrast, the Speech-Incomplete-Timeout is used when the speech is an\n
        \  incomplete match to an active grammar.  The value is in milliseconds.\n
        \ speech-complete-timeout = \"Speech-Complete-Timeout\" \":\" 1*19DIGIT CRLF\n
        \  A long Speech-Complete-Timeout value delays the result to the client\n
        \  and therefore makes the application's response to a user slow.  A\n   short
        Speech-Complete-Timeout may lead to an utterance being broken\n   up inappropriately.
        \ Reasonable speech complete timeout values are\n   typically in the range
        of 0.3 seconds to 1.0 seconds.  The value for\n   this header field ranges
        from 0 to an implementation-specific maximum\n   value.  The default value
        for this header field is implementation\n   specific.  This header field MAY
        occur in RECOGNIZE, SET-PARAMS, or\n   GET-PARAMS.\n"
      title: 9.4.15.  Speech-Complete-Timeout
    - contents:
      - "9.4.16.  Speech-Incomplete-Timeout\n   This header field specifies the required
        length of silence following\n   user speech after which a recognizer finalizes
        a result.  The\n   incomplete timeout applies when the speech prior to the
        silence is an\n   incomplete match of all active grammars.  In this case,
        once the\n   timeout is triggered, the partial result is rejected (with a\n
        \  Completion-Cause of \"partial-match\").  The value is in milliseconds.\n
        \  The value for this header field ranges from 0 to an implementation-\n   specific
        maximum value.  The default value for this header field is\n   implementation
        specific.\n   speech-incomplete-timeout = \"Speech-Incomplete-Timeout\" \":\"
        1*19DIGIT\n                                CRLF\n   The Speech-Incomplete-Timeout
        also applies when the speech prior to\n   the silence is a complete match
        of an active grammar, but where it is\n   possible to speak further and still
        match the grammar.  By contrast,\n   the Speech-Complete-Timeout is used when
        the speech is a complete\n   match to an active grammar and no further spoken
        words can continue\n   to represent a match.\n   A long Speech-Incomplete-Timeout
        value delays the result to the\n   client and therefore makes the application's
        response to a user slow.\n   A short Speech-Incomplete-Timeout may lead to
        an utterance being\n   broken up inappropriately.\n   The Speech-Incomplete-Timeout
        is usually longer than the Speech-\n   Complete-Timeout to allow users to
        pause mid-utterance (for example,\n   to breathe).  This header field MAY
        occur in RECOGNIZE, SET-PARAMS,\n   or GET-PARAMS.\n"
      title: 9.4.16.  Speech-Incomplete-Timeout
    - contents:
      - "9.4.17.  DTMF-Interdigit-Timeout\n   This header field specifies the inter-digit
        timeout value to use when\n   recognizing DTMF input.  The value is in milliseconds.
        \ The value for\n   this header field ranges from 0 to an implementation-specific
        maximum\n   value.  The default value is 5 seconds.  This header field MAY
        occur\n   in RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n  dtmf-interdigit-timeout
        = \"DTMF-Interdigit-Timeout\" \":\" 1*19DIGIT CRLF\n"
      title: 9.4.17.  DTMF-Interdigit-Timeout
    - contents:
      - "9.4.18.  DTMF-Term-Timeout\n   This header field specifies the terminating
        timeout to use when\n   recognizing DTMF input.  The DTMF-Term-Timeout applies
        only when no\n   additional input is allowed by the grammar; otherwise, the\n
        \  DTMF-Interdigit-Timeout applies.  The value is in milliseconds.  The\n
        \  value for this header field ranges from 0 to an implementation-\n   specific
        maximum value.  The default value is 10 seconds.  This\n   header field MAY
        occur in RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n   dtmf-term-timeout        =
        \ \"DTMF-Term-Timeout\" \":\" 1*19DIGIT CRLF\n"
      title: 9.4.18.  DTMF-Term-Timeout
    - contents:
      - "9.4.19.  DTMF-Term-Char\n   This header field specifies the terminating DTMF
        character for DTMF\n   input recognition.  The default value is NULL, which
        is indicated by\n   an empty header field value.  This header field MAY occur
        in\n   RECOGNIZE, SET-PARAMS, or GET-PARAMS.\n   dtmf-term-char           =
        \ \"DTMF-Term-Char\" \":\" VCHAR CRLF\n"
      title: 9.4.19.  DTMF-Term-Char
    - contents:
      - "9.4.20.  Failed-URI\n   When a recognizer needs to fetch or access a URI
        and the access\n   fails, the server SHOULD provide the failed URI in this
        header field\n   in the method response, unless there are multiple URI failures,
        in\n   which case one of the failed URIs MUST be provided in this header\n
        \  field in the method response.\n   failed-uri               =  \"Failed-URI\"
        \":\" absoluteURI CRLF\n"
      title: 9.4.20.  Failed-URI
    - contents:
      - "9.4.21.  Failed-URI-Cause\n   When a recognizer method needs a recognizer
        to fetch or access a URI\n   and the access fails, the server MUST provide
        the URI-specific or\n   protocol-specific response code for the URI in the
        Failed-URI header\n   field through this header field in the method response.
        \ The value\n   encoding is UTF-8 (RFC 3629 [RFC3629]) to accommodate any
        access\n   protocol, some of which might have a response string instead of
        a\n   numeric response code.\n   failed-uri-cause         =  \"Failed-URI-Cause\"
        \":\" 1*UTFCHAR CRLF\n"
      title: 9.4.21.  Failed-URI-Cause
    - contents:
      - "9.4.22.  Save-Waveform\n   This header field allows the client to request
        the recognizer\n   resource to save the audio input to the recognizer.  The
        recognizer\n   resource MUST then attempt to record the recognized audio,
        without\n   endpointing, and make it available to the client in the form of
        a URI\n   returned in the Waveform-URI header field in the RECOGNITION-COMPLETE\n
        \  event.  If there was an error in recording the stream or the audio\n   content
        is otherwise not available, the recognizer MUST return an\n   empty Waveform-URI
        header field.  The default value for this field is\n   \"false\".  This header
        field MAY occur in RECOGNIZE, SET-PARAMS, or\n   GET-PARAMS.  See the discussion
        on the sensitivity of saved waveforms\n   in Section 12.\n   save-waveform
        \           =  \"Save-Waveform\" \":\" BOOLEAN CRLF\n"
      title: 9.4.22.  Save-Waveform
    - contents:
      - "9.4.23.  New-Audio-Channel\n   This header field MAY be specified in a RECOGNIZE
        request and allows\n   the client to tell the server that, from this point
        on, further input\n   audio comes from a different audio source, channel,
        or speaker.  If\n   the recognizer resource had collected any input statistics
        or\n   adaptation state, the recognizer resource MUST do what is appropriate\n
        \  for the specific recognition technology, which includes but is not\n   limited
        to discarding any collected input statistics or adaptation\n   state before
        starting the RECOGNIZE request.  Note that if there are\n   multiple resources
        that are sharing a media stream and are collecting\n   or using this data,
        and the client issues this header field to one of\n   the resources, the reset
        operation applies to all resources that use\n   the shared media stream.  This
        helps in a number of use cases,\n   including where the client wishes to reuse
        an open recognition\n   session with an existing media session for multiple
        telephone calls.\n   new-audio-channel        =  \"New-Audio-Channel\" \":\"
        BOOLEAN\n                               CRLF\n"
      title: 9.4.23.  New-Audio-Channel
    - contents:
      - "9.4.24.  Speech-Language\n   This header field specifies the language of
        recognition grammar data\n   within a session or request, if it is not specified
        within the data.\n   The value of this header field MUST follow RFC 5646 [RFC5646]
        for its\n   values.  This MAY occur in DEFINE-GRAMMAR, RECOGNIZE, SET-PARAMS,
        or\n   GET-PARAMS requests.\n   speech-language          =  \"Speech-Language\"
        \":\" 1*VCHAR CRLF\n"
      title: 9.4.24.  Speech-Language
    - contents:
      - "9.4.25.  Ver-Buffer-Utterance\n   This header field lets the client request
        the server to buffer the\n   utterance associated with this recognition request
        into a buffer\n   available to a co-resident verifier resource.  The buffer
        is shared\n   across resources within a session and is allocated when a verifier\n
        \  resource is added to this session.  The client MUST NOT send this\n   header
        field unless a verifier resource is instantiated for the\n   session.  The
        buffer is released when the verifier resource is\n   released from the session.\n"
      title: 9.4.25.  Ver-Buffer-Utterance
    - contents:
      - "9.4.26.  Recognition-Mode\n   This header field specifies what mode the RECOGNIZE
        method will\n   operate in.  The value choices are \"normal\" or \"hotword\".
        \ If the\n   value is \"normal\", the RECOGNIZE starts matching speech and
        DTMF to\n   the grammars specified in the RECOGNIZE request.  If any portion
        of\n   the speech does not match the grammar, the RECOGNIZE command\n   completes
        with a no-match status.  Timers may be active to detect\n   speech in the
        audio (see Section 9.4.14), so the RECOGNIZE method may\n   complete because
        of a timeout waiting for speech.  If the value of\n   this header field is
        \"hotword\", the RECOGNIZE method operates in\n   hotword mode, where it only
        looks for the particular keywords or DTMF\n   sequences specified in the grammar
        and ignores silence or other\n   speech in the audio stream.  The default
        value for this header field\n   is \"normal\".  This header field MAY occur
        on the RECOGNIZE method.\n   recognition-mode         =  \"Recognition-Mode\"
        \":\"\n                               \"normal\" / \"hotword\" CRLF\n"
      title: 9.4.26.  Recognition-Mode
    - contents:
      - "9.4.27.  Cancel-If-Queue\n   This header field specifies what will happen
        if the client attempts\n   to invoke another RECOGNIZE method when this RECOGNIZE
        request is\n   already in progress for the resource.  The value for this header\n
        \  field is a Boolean.  A value of \"true\" means the server MUST\n   terminate
        this RECOGNIZE request, with a Completion-Cause of\n   \"cancelled\", if the
        client issues another RECOGNIZE request for the\n   same resource.  A value
        of \"false\" for this header field indicates to\n   the server that this RECOGNIZE
        request will continue to completion,\n   and if the client issues more RECOGNIZE
        requests to the same\n   resource, they are queued.  When the currently active
        RECOGNIZE\n   request is stopped or completes with a successful match, the
        first\n   RECOGNIZE method in the queue becomes active.  If the current\n
        \  RECOGNIZE fails, all RECOGNIZE methods in the pending queue are\n   cancelled,
        and each generates a RECOGNITION-COMPLETE event with a\n   Completion-Cause
        of \"cancelled\".  This header field MUST be present\n   in every RECOGNIZE
        request.  There is no default value.\n   cancel-if-queue          =  \"Cancel-If-Queue\"
        \":\" BOOLEAN CRLF\n"
      title: 9.4.27.  Cancel-If-Queue
    - contents:
      - "9.4.28.  Hotword-Max-Duration\n   This header field MAY be sent in a hotword
        mode RECOGNIZE request.\n   It specifies the maximum length of an utterance
        (in seconds) that\n   will be considered for hotword recognition.  This header
        field, along\n   with Hotword-Min-Duration, can be used to tune performance
        by\n   preventing the recognizer from evaluating utterances that are too\n
        \  short or too long to be one of the hotwords in the grammar(s).  The\n   value
        is in milliseconds.  The default is implementation dependent.\n   If present
        in a RECOGNIZE request specifying a mode other than\n   \"hotword\", the header
        field is ignored.\n   hotword-max-duration     =  \"Hotword-Max-Duration\"
        \":\" 1*19DIGIT\n                               CRLF\n"
      title: 9.4.28.  Hotword-Max-Duration
    - contents:
      - "9.4.29.  Hotword-Min-Duration\n   This header field MAY be sent in a hotword
        mode RECOGNIZE request.\n   It specifies the minimum length of an utterance
        (in seconds) that\n   will be considered for hotword recognition.  This header
        field, along\n   with Hotword-Max-Duration, can be used to tune performance
        by\n   preventing the recognizer from evaluating utterances that are too\n
        \  short or too long to be one of the hotwords in the grammar(s).  The\n   value
        is in milliseconds.  The default value is implementation\n   dependent.  If
        present in a RECOGNIZE request specifying a mode other\n   than \"hotword\",
        the header field is ignored.\n   hotword-min-duration     =  \"Hotword-Min-Duration\"
        \":\" 1*19DIGIT CRLF\n"
      title: 9.4.29.  Hotword-Min-Duration
    - contents:
      - "9.4.30.  Interpret-Text\n   The value of this header field is used to provide
        a pointer to the\n   text for which a natural language interpretation is desired.
        \ The\n   value is either a URI or text.  If the value is a URI, it MUST be
        a\n   Content-ID that refers to an entity of type 'text/plain' in the body\n
        \  of the message.  Otherwise, the server MUST treat the value as the\n   text
        to be interpreted.  This header field MUST be used when invoking\n   the INTERPRET
        method.\n   interpret-text           =  \"Interpret-Text\" \":\" 1*VCHAR CRLF\n"
      title: 9.4.30.  Interpret-Text
    - contents:
      - "9.4.31.  DTMF-Buffer-Time\n   This header field MAY be specified in a GET-PARAMS
        or SET-PARAMS\n   method and is used to specify the amount of time, in milliseconds,
        of\n   the type-ahead buffer for the recognizer.  This is the buffer that\n
        \  collects DTMF digits as they are pressed even when there is no\n   RECOGNIZE
        command active.  When a subsequent RECOGNIZE method is\n   received, it MUST
        look to this buffer to match the RECOGNIZE request.\n   If the digits in the
        buffer are not sufficient, then it can continue\n   to listen to more digits
        to match the grammar.  The default size of\n   this DTMF buffer is platform
        specific.\n   dtmf-buffer-time  =  \"DTMF-Buffer-Time\" \":\" 1*19DIGIT CRLF\n"
      title: 9.4.31.  DTMF-Buffer-Time
    - contents:
      - "9.4.32.  Clear-DTMF-Buffer\n   This header field MAY be specified in a RECOGNIZE
        method and is used\n   to tell the recognizer to clear the DTMF type-ahead
        buffer before\n   starting the RECOGNIZE.  The default value of this header
        field is\n   \"false\", which does not clear the type-ahead buffer before
        starting\n   the RECOGNIZE method.  If this header field is specified to be\n
        \  \"true\", then the RECOGNIZE will clear the DTMF buffer before starting\n
        \  recognition.  This means digits pressed by the caller before the\n   RECOGNIZE
        command was issued are discarded.\n   clear-dtmf-buffer  = \"Clear-DTMF-Buffer\"
        \":\" BOOLEAN CRLF\n"
      title: 9.4.32.  Clear-DTMF-Buffer
    - contents:
      - "9.4.33.  Early-No-Match\n   This header field MAY be specified in a RECOGNIZE
        method and is used\n   to tell the recognizer that it MUST NOT wait for the
        end of speech\n   before processing the collected speech to match active grammars.
        \ A\n   value of \"true\" indicates the recognizer MUST do early matching.
        \ The\n   default value for this header field if not specified is \"false\".
        \ If\n   the recognizer does not support the processing of the collected audio\n
        \  before the end of speech, this header field can be safely ignored.\n   early-no-match
        \ = \"Early-No-Match\" \":\" BOOLEAN CRLF\n"
      title: 9.4.33.  Early-No-Match
    - contents:
      - "9.4.34.  Num-Min-Consistent-Pronunciations\n   This header field MAY be specified
        in a START-PHRASE-ENROLLMENT,\n   SET-PARAMS, or GET-PARAMS method and is
        used to specify the minimum\n   number of consistent pronunciations that must
        be obtained to voice\n   enroll a new phrase.  The minimum value is 1.  The
        default value is\n   implementation specific and MAY be greater than 1.\n
        \  num-min-consistent-pronunciations  =\n                 \"Num-Min-Consistent-Pronunciations\"
        \":\" 1*19DIGIT CRLF\n"
      title: 9.4.34.  Num-Min-Consistent-Pronunciations
    - contents:
      - "9.4.35.  Consistency-Threshold\n   This header field MAY be sent as part
        of the START-PHRASE-ENROLLMENT,\n   SET-PARAMS, or GET-PARAMS method.  Used
        during voice enrollment, this\n   header field specifies how similar to a
        previously enrolled\n   pronunciation of the same phrase an utterance needs
        to be in order to\n   be considered \"consistent\".  The higher the threshold,
        the closer the\n   match between an utterance and previous pronunciations
        must be for\n   the pronunciation to be considered consistent.  The range
        for this\n   threshold is a float value between 0.0 and 1.0.  The default
        value\n   for this header field is implementation specific.\n   consistency-threshold
        \   =  \"Consistency-Threshold\" \":\" FLOAT CRLF\n"
      title: 9.4.35.  Consistency-Threshold
    - contents:
      - "9.4.36.  Clash-Threshold\n   This header field MAY be sent as part of the
        START-PHRASE-ENROLLMENT,\n   SET-PARAMS, or GET-PARAMS method.  Used during
        voice enrollment, this\n   header field specifies how similar the pronunciations
        of two\n   different phrases can be before they are considered to be clashing.\n
        \  For example, pronunciations of phrases such as \"John Smith\" and \"Jon\n
        \  Smits\" may be so similar that they are difficult to distinguish\n   correctly.
        \ A smaller threshold reduces the number of clashes\n   detected.  The range
        for this threshold is a float value between 0.0\n   and 1.0.  The default
        value for this header field is implementation\n   specific.  Clash testing
        can be turned off completely by setting the\n   Clash-Threshold header field
        value to 0.\n   clash-threshold          =  \"Clash-Threshold\" \":\" FLOAT
        CRLF\n"
      title: 9.4.36.  Clash-Threshold
    - contents:
      - "9.4.37.  Personal-Grammar-URI\n   This header field specifies the speaker-trained
        grammar to be used or\n   referenced during enrollment operations.  Phrases
        are added to this\n   grammar during enrollment.  For example, a contact list
        for user\n   \"Jeff\" could be stored at the Personal-Grammar-URI\n   \"http://myserver.example.com/myenrollmentdb/jeff-list\".
        \ The\n   generated grammar syntax MAY be implementation specific.  There
        is no\n   default value for this header field.  This header field MAY be sent\n
        \  as part of the START-PHRASE-ENROLLMENT, SET-PARAMS, or GET-PARAMS\n   method.\n
        \  personal-grammar-uri     =  \"Personal-Grammar-URI\" \":\" uri CRLF\n"
      title: 9.4.37.  Personal-Grammar-URI
    - contents:
      - "9.4.38.  Enroll-Utterance\n   This header field MAY be specified in the RECOGNIZE
        method.  If this\n   header field is set to \"true\" and an Enrollment is
        active, the\n   RECOGNIZE command MUST add the collected utterance to the
        personal\n   grammar that is being enrolled.  The way in which this occurs
        is\n   engine specific and may be an area of future standardization.  The\n
        \  default value for this header field is \"false\".\n   enroll-utterance
        \    =  \"Enroll-Utterance\" \":\" BOOLEAN CRLF\n"
      title: 9.4.38.  Enroll-Utterance
    - contents:
      - "9.4.39.  Phrase-Id\n   This header field in a request identifies a phrase
        in an existing\n   personal grammar for which enrollment is desired.  It is
        also\n   returned to the client in the RECOGNIZE complete event.  This header\n
        \  field MAY occur in START-PHRASE-ENROLLMENT, MODIFY-PHRASE, or DELETE-\n
        \  PHRASE requests.  There is no default value for this header field.\n   phrase-id
        \               =  \"Phrase-ID\" \":\" 1*VCHAR CRLF\n"
      title: 9.4.39.  Phrase-Id
    - contents:
      - "9.4.40.  Phrase-NL\n   This string specifies the interpreted text to be returned
        when the\n   phrase is recognized.  This header field MAY occur in START-PHRASE-\n
        \  ENROLLMENT and MODIFY-PHRASE requests.  There is no default value for\n
        \  this header field.\n   phrase-nl                =  \"Phrase-NL\" \":\"
        1*UTFCHAR CRLF\n"
      title: 9.4.40.  Phrase-NL
    - contents:
      - "9.4.41.  Weight\n   The value of this header field represents the occurrence
        likelihood\n   of a phrase in an enrolled grammar.  When using grammar enrollment,\n
        \  the system is essentially constructing a grammar segment consisting\n   of
        a list of possible match phrases.  This can be thought of to be\n   similar
        to the dynamic construction of a <one-of> tag in the W3C\n   grammar specification.
        \ Each enrolled-phrase becomes an item in the\n   list that can be matched
        against spoken input similar to the <item>\n   within a <one-of> list.  This
        header field allows you to assign a\n   weight to the phrase (i.e., <item>
        entry) in the <one-of> list that\n   is enrolled.  Grammar weights are normalized
        to a sum of one at\n   grammar compilation time, so a weight value of 1 for
        each phrase in\n   an enrolled grammar list indicates all items in that list
        have the\n   same weight.  This header field MAY occur in START-PHRASE-ENROLLMENT\n
        \  and MODIFY-PHRASE requests.  The default value for this header field\n
        \  is implementation specific.\n   weight                   =  \"Weight\"
        \":\" FLOAT CRLF\n"
      title: 9.4.41.  Weight
    - contents:
      - "9.4.42.  Save-Best-Waveform\n   This header field allows the client to request
        the recognizer\n   resource to save the audio stream for the best repetition
        of the\n   phrase that was used during the enrollment session.  The recognizer\n
        \  MUST attempt to record the recognized audio and make it available to\n
        \  the client in the form of a URI returned in the Waveform-URI header\n   field
        in the response to the END-PHRASE-ENROLLMENT method.  If there\n   was an
        error in recording the stream or the audio data is otherwise\n   not available,
        the recognizer MUST return an empty Waveform-URI\n   header field.  This header
        field MAY occur in the START-PHRASE-\n   ENROLLMENT, SET-PARAMS, and GET-PARAMS
        methods.\n   save-best-waveform  =  \"Save-Best-Waveform\" \":\" BOOLEAN CRLF\n"
      title: 9.4.42.  Save-Best-Waveform
    - contents:
      - "9.4.43.  New-Phrase-Id\n   This header field replaces the ID used to identify
        the phrase in a\n   personal grammar.  The recognizer returns the new ID when
        using an\n   enrollment grammar.  This header field MAY occur in MODIFY-PHRASE\n
        \  requests.\n   new-phrase-id            =  \"New-Phrase-ID\" \":\" 1*VCHAR
        CRLF\n"
      title: 9.4.43.  New-Phrase-Id
    - contents:
      - "9.4.44.  Confusable-Phrases-URI\n   This header field specifies a grammar
        that defines invalid phrases\n   for enrollment.  For example, typical applications
        do not allow an\n   enrolled phrase that is also a command word.  This header
        field MAY\n   occur in RECOGNIZE requests that are part of an enrollment session.\n
        \  confusable-phrases-uri   =  \"Confusable-Phrases-URI\" \":\" uri CRLF\n"
      title: 9.4.44.  Confusable-Phrases-URI
    - contents:
      - "9.4.45.  Abort-Phrase-Enrollment\n   This header field MAY be specified in
        the END-PHRASE-ENROLLMENT\n   method to abort the phrase enrollment, rather
        than committing the\n   phrase to the personal grammar.\n   abort-phrase-enrollment
        \ =  \"Abort-Phrase-Enrollment\" \":\"\n                               BOOLEAN
        CRLF\n"
      title: 9.4.45.  Abort-Phrase-Enrollment
    title: 9.4.  Recognizer Header Fields
  - contents:
    - "9.5.  Recognizer Message Body\n   A recognizer message can carry additional
      data associated with the\n   request, response, or event.  The client MAY provide
      the grammar to\n   be recognized in DEFINE-GRAMMAR or RECOGNIZE requests.  When
      one or\n   more grammars are specified using the DEFINE-GRAMMAR method, the\n
      \  server MUST attempt to fetch, compile, and optimize the grammar\n   before
      returning a response to the DEFINE-GRAMMAR method.  A\n   RECOGNIZE request
      MUST completely specify the grammars to be active\n   during the recognition
      operation, except when the RECOGNIZE method is\n   being used to enroll a grammar.
      \ During grammar enrollment, such\n   grammars are OPTIONAL.  The server resource
      sends the recognition\n   results in the RECOGNITION-COMPLETE event and the
      GET-RESULT\n   response.  Grammars and recognition results are carried in the\n
      \  message body of the corresponding MRCPv2 messages.\n"
    - contents:
      - "9.5.1.  Recognizer Grammar Data\n   Recognizer grammar data from the client
        to the server can be provided\n   inline or by reference.  Either way, grammar
        data is carried as typed\n   media entities in the message body of the RECOGNIZE
        or DEFINE-GRAMMAR\n   request.  All MRCPv2 servers MUST accept grammars in
        the XML form\n   (media type 'application/srgs+xml') of the W3C's XML-based
        Speech\n   Grammar Markup Format (SRGS) [W3C.REC-speech-grammar-20040316]
        and\n   MAY accept grammars in other formats.  Examples include but are not\n
        \  limited to:\n   o  the ABNF form (media type 'application/srgs') of SRGS\n
        \  o  Sun's Java Speech Grammar Format (JSGF)\n      [refs.javaSpeechGrammarFormat]\n
        \  Additionally, MRCPv2 servers MAY support the Semantic Interpretation\n
        \  for Speech Recognition (SISR)\n   [W3C.REC-semantic-interpretation-20070405]
        specification.\n   When a grammar is specified inline in the request, the
        client MUST\n   provide a Content-ID for that grammar as part of the content
        header\n   fields.  If there is no space on the server to store the inline\n
        \  grammar, the request MUST return with a Completion-Cause code of 016\n
        \  \"grammar-definition-failure\".  Otherwise, the server MUST associate\n
        \  the inline grammar block with that Content-ID and MUST store it on\n   the
        server for the duration of the session.  However, if the\n   Content-ID is
        redefined later in the session through a subsequent\n   DEFINE-GRAMMAR, the
        inline grammar previously associated with the\n   Content-ID MUST be freed.
        \ If the Content-ID is redefined through a\n   subsequent DEFINE-GRAMMAR with
        an empty message body (i.e., no\n   grammar definition), then in addition
        to freeing any grammar\n   previously associated with the Content-ID, the
        server MUST clear all\n   bindings and associations to the Content-ID.  Unless
        and until\n   subsequently redefined, this URI MUST be interpreted by the
        server as\n   one that has never been set.\n   Grammars that have been associated
        with a Content-ID can be\n   referenced through the 'session' URI scheme (see
        Section 13.6).  For\n   example:\n   session:help@root-level.store\n   Grammar
        data MAY be specified using external URI references.  To do\n   so, the client
        uses a body of media type 'text/uri-list' (see RFC\n   2483 [RFC2483] ) to
        list the one or more URIs that point to the\n   grammar data.  The client
        can use a body of media type 'text/\n   grammar-ref-list' (see Section 13.5.1)
        if it wants to assign weights\n   to the list of grammar URI.  All MRCPv2
        servers MUST support grammar\n   access using the 'http' and 'https' URI schemes.\n
        \  If the grammar data the client wishes to be used on a request\n   consists
        of a mix of URI and inline grammar data, the client uses the\n   'multipart/mixed'
        media type to enclose the 'text/uri-list',\n   'application/srgs', or 'application/srgs+xml'
        content entities.  The\n   character set and encoding used in the grammar
        data are specified\n   using to standard media type definitions.\n   When
        more than one grammar URI or inline grammar block is specified\n   in a message
        body of the RECOGNIZE request, the server interprets\n   this as a list of
        grammar alternatives to match against.\n   Content-Type:application/srgs+xml\n
        \  Content-ID:<request1@form-level.store>\n   Content-Length:...\n   <?xml
        version=\"1.0\"?>\n   <!-- the default grammar language is US English -->\n
        \  <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n            xml:lang=\"en-US\"
        version=\"1.0\" root=\"request\">\n   <!-- single language attachment to tokens
        -->\n         <rule id=\"yes\">\n               <one-of>\n                     <item
        xml:lang=\"fr-CA\">oui</item>\n                     <item xml:lang=\"en-US\">yes</item>\n
        \              </one-of>\n         </rule>\n   <!-- single language attachment
        to a rule expansion -->\n         <rule id=\"request\">\n               may
        I speak to\n               <one-of xml:lang=\"fr-CA\">\n                     <item>Michel
        Tremblay</item>\n                     <item>Andre Roy</item>\n               </one-of>\n
        \        </rule>\n         <!-- multiple language attachment to a token -->\n
        \        <rule id=\"people1\">\n               <token lexicon=\"en-US,fr-CA\">
        Robert </token>\n         </rule>\n         <!-- the equivalent single-language
        attachment expansion -->\n         <rule id=\"people2\">\n               <one-of>\n
        \                    <item xml:lang=\"en-US\">Robert</item>\n                     <item
        xml:lang=\"fr-CA\">Robert</item>\n               </one-of>\n         </rule>\n
        \        </grammar>\n                           SRGS Grammar Example\n   Content-Type:text/uri-list\n
        \  Content-Length:...\n   session:help@root-level.store\n   http://www.example.com/Directory-Name-List.grxml\n
        \  http://www.example.com/Department-List.grxml\n   http://www.example.com/TAC-Contact-List.grxml\n
        \  session:menu1@menu-level.store\n                         Grammar Reference
        Example\n   Content-Type:multipart/mixed; boundary=\"break\"\n   --break\n
        \  Content-Type:text/uri-list\n   Content-Length:...\n   http://www.example.com/Directory-Name-List.grxml\n
        \  http://www.example.com/Department-List.grxml\n   http://www.example.com/TAC-Contact-List.grxml\n
        \  --break\n   Content-Type:application/srgs+xml\n   Content-ID:<request1@form-level.store>\n
        \  Content-Length:...\n   <?xml version=\"1.0\"?>\n   <!-- the default grammar
        language is US English -->\n   <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
        \           xml:lang=\"en-US\" version=\"1.0\">\n   <!-- single language attachment
        to tokens -->\n         <rule id=\"yes\">\n               <one-of>\n                     <item
        xml:lang=\"fr-CA\">oui</item>\n                     <item xml:lang=\"en-US\">yes</item>\n
        \              </one-of>\n         </rule>\n   <!-- single language attachment
        to a rule expansion -->\n         <rule id=\"request\">\n               may
        I speak to\n               <one-of xml:lang=\"fr-CA\">\n                     <item>Michel
        Tremblay</item>\n                     <item>Andre Roy</item>\n               </one-of>\n
        \        </rule>\n         <!-- multiple language attachment to a token -->\n
        \        <rule id=\"people1\">\n               <token lexicon=\"en-US,fr-CA\">
        Robert </token>\n         </rule>\n         <!-- the equivalent single-language
        attachment expansion -->\n         <rule id=\"people2\">\n               <one-of>\n
        \                    <item xml:lang=\"en-US\">Robert</item>\n                     <item
        xml:lang=\"fr-CA\">Robert</item>\n               </one-of>\n         </rule>\n
        \        </grammar>\n   --break--\n                      Mixed Grammar Reference
        Example\n"
      title: 9.5.1.  Recognizer Grammar Data
    - contents:
      - "9.5.2.  Recognizer Result Data\n   Recognition results are returned to the
        client in the message body of\n   the RECOGNITION-COMPLETE event or the GET-RESULT
        response message as\n   described in Section 6.3.  Element and attribute descriptions
        for the\n   recognition portion of the NLSML format are provided in Section
        9.6\n   with a normative definition of the schema in Section 16.1.\n   Content-Type:application/nlsml+xml\n
        \  Content-Length:...\n   <?xml version=\"1.0\"?>\n   <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n
        \          xmlns:ex=\"http://www.example.com/example\"\n           grammar=\"http://www.example.com/theYesNoGrammar\">\n
        \      <interpretation>\n           <instance>\n                   <ex:response>yes</ex:response>\n
        \          </instance>\n           <input>OK</input>\n       </interpretation>\n
        \  </result>\n                              Result Example\n"
      title: 9.5.2.  Recognizer Result Data
    - contents:
      - "9.5.3.  Enrollment Result Data\n   Enrollment results are returned to the
        client in the message body of\n   the RECOGNITION-COMPLETE event as described
        in Section 6.3.  Element\n   and attribute descriptions for the enrollment
        portion of the NLSML\n   format are provided in Section 9.7 with a normative
        definition of the\n   schema in Section 16.2.\n"
      title: 9.5.3.  Enrollment Result Data
    - contents:
      - "9.5.4.  Recognizer Context Block\n   When a client changes servers while
        operating on the behalf of the\n   same incoming communication session, this
        header field allows the\n   client to collect a block of opaque data from
        one server and provide\n   it to another server.  This capability is desirable
        if the client\n   needs different language support or because the server issued
        a\n   redirect.  Here, the first recognizer resource may have collected\n
        \  acoustic and other data during its execution of recognition methods.\n
        \  After a server switch, communicating this data may allow the\n   recognizer
        resource on the new server to provide better recognition.\n   This block of
        data is implementation specific and MUST be carried as\n   media type 'application/octets'
        in the body of the message.\n   This block of data is communicated in the
        SET-PARAMS and GET-PARAMS\n   method/response messages.  In the GET-PARAMS
        method, if an empty\n   Recognizer-Context-Block header field is present,
        then the recognizer\n   SHOULD return its vendor-specific context block, if
        any, in the\n   message body as an entity of media type 'application/octets'
        with a\n   specific Content-ID.  The Content-ID value MUST also be specified
        in\n   the Recognizer-Context-Block header field in the GET-PARAMS response.\n
        \  The SET-PARAMS request wishing to provide this vendor-specific data\n   MUST
        send it in the message body as a typed entity with the same\n   Content-ID
        that it received from the GET-PARAMS.  The Content-ID MUST\n   also be sent
        in the Recognizer-Context-Block header field of the\n   SET-PARAMS message.\n
        \  Each speech recognition implementation choosing to use this mechanism\n
        \  to hand off recognizer context data among servers MUST distinguish\n   its
        implementation-specific block of data from other implementations\n   by choosing
        a Content-ID that is recognizable among the participating\n   servers and
        unlikely to collide with values chosen by another\n   implementation.\n"
      title: 9.5.4.  Recognizer Context Block
    title: 9.5.  Recognizer Message Body
  - contents:
    - "9.6.  Recognizer Results\n   The recognizer portion of NLSML (see Section 6.3.1)
      represents\n   information automatically extracted from a user's utterances
      by a\n   semantic interpretation component, where \"utterance\" is to be taken\n
      \  in the general sense of a meaningful user input in any modality\n   supported
      by the MRCPv2 implementation.\n"
    - contents:
      - "9.6.1.  Markup Functions\n   MRCPv2 recognizer resources employ the Natural
        Language Semantics\n   Markup Language (NLSML) to interpret natural language
        speech input\n   and to format the interpretation for consumption by an MRCPv2
        client.\n   The elements of the markup fall into the following general functional\n
        \  categories: interpretation, side information, and multi-modal\n   integration.\n"
      - contents:
        - "9.6.1.1.  Interpretation\n   Elements and attributes represent the semantics
          of a user's\n   utterance, including the <result>, <interpretation>, and
          <instance>\n   elements.  The <result> element contains the full result
          of\n   processing one utterance.  It MAY contain multiple <interpretation>\n
          \  elements if the interpretation of the utterance results in multiple\n
          \  alternative meanings due to uncertainty in speech recognition or\n   natural
          language understanding.  There are at least two reasons for\n   providing
          multiple interpretations:\n   1.  The client application might have additional
          information, for\n       example, information from a database, that would
          allow it to\n       select a preferred interpretation from among the possible\n
          \      interpretations returned from the semantic interpreter.\n   2.  A
          client-based dialog manager (e.g., VoiceXML\n       [W3C.REC-voicexml20-20040316])
          that was unable to select between\n       several competing interpretations
          could use this information to\n       go back to the user and find out what
          was intended.  For example,\n       it could issue a SPEAK request to a
          synthesizer resource to emit\n       \"Did you say 'Boston' or 'Austin'?\"\n"
        title: 9.6.1.1.  Interpretation
      - contents:
        - "9.6.1.2.  Side Information\n   These are elements and attributes representing
          additional information\n   about the interpretation, over and above the
          interpretation itself.\n   Side information includes:\n   1.  Whether an
          interpretation was achieved (the <nomatch> element)\n       and the system's
          confidence in an interpretation (the\n       \"confidence\" attribute of
          <interpretation>).\n   2.  Alternative interpretations (<interpretation>)\n
          \  3.  Input formats and Automatic Speech Recognition (ASR) information:\n
          \      the <input> element, representing the input to the semantic\n       interpreter.\n"
        title: 9.6.1.2.  Side Information
      - contents:
        - "9.6.1.3.  Multi-Modal Integration\n   When more than one modality is available
          for input, the\n   interpretation of the inputs needs to be coordinated.
          \ The \"mode\"\n   attribute of <input> supports this by indicating whether
          the\n   utterance was input by speech, DTMF, pointing, etc.  The \"timestamp-\n
          \  start\" and \"timestamp-end\" attributes of <input> also provide for\n
          \  temporal coordination by indicating when inputs occurred.\n"
        title: 9.6.1.3.  Multi-Modal Integration
      title: 9.6.1.  Markup Functions
    - contents:
      - "9.6.2.  Overview of Recognizer Result Elements and Their Relationships\n
        \  The recognizer elements in NLSML fall into two categories:\n   1.  description
        of the input that was processed, and\n   2.  description of the meaning which
        was extracted from the input.\n   Next to each element are its attributes.
        \ In addition, some elements\n   can contain multiple instances of other elements.
        \ For example, a\n   <result> can contain multiple <interpretation> elements,
        each of\n   which is taken to be an alternative.  Similarly, <input> can contain\n
        \  multiple child <input> elements, which are taken to be cumulative.\n   To
        illustrate the basic usage of these elements, as a simple example,\n   consider
        the utterance \"OK\" (interpreted as \"yes\").  The example\n   illustrates
        how that utterance and its interpretation would be\n   represented in the
        NLSML markup.\n   <?xml version=\"1.0\"?>\n   <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n
        \          xmlns:ex=\"http://www.example.com/example\"\n           grammar=\"http://www.example.com/theYesNoGrammar\">\n
        \    <interpretation>\n        <instance>\n           <ex:response>yes</ex:response>\n
        \        </instance>\n       <input>OK</input>\n     </interpretation>\n   </result>\n
        \  This example includes only the minimum required information.  There\n   is
        an overall <result> element, which includes one interpretation and\n   an
        input element.  The interpretation contains the application-\n   specific
        element \"<response>\", which is the semantically interpreted\n   result.\n"
      title: 9.6.2.  Overview of Recognizer Result Elements and Their Relationships
    - contents:
      - '9.6.3.  Elements and Attributes

        '
      - contents:
        - "9.6.3.1.  <result> Root Element\n   The root element of the markup is <result>.
          \ The <result> element\n   includes one or more <interpretation> elements.
          \ Multiple\n   interpretations can result from ambiguities in the input
          or in the\n   semantic interpretation.  If the \"grammar\" attribute does
          not apply\n   to all of the interpretations in the result, it can be overridden
          for\n   individual interpretations at the <interpretation> level.\n   Attributes:\n
          \  1.  grammar: The grammar or recognition rule matched by this result.\n
          \      The format of the grammar attribute will match the rule reference\n
          \      semantics defined in the grammar specification.  Specifically,\n
          \      the rule reference is in the external XML form for grammar rule\n
          \      references.  The markup interpreter needs to know the grammar\n       rule
          that is matched by the utterance because multiple rules may\n       be simultaneously
          active.  The value is the grammar URI used by\n       the markup interpreter
          to specify the grammar.  The grammar can\n       be overridden by a grammar
          attribute in the <interpretation>\n       element if the input was ambiguous
          as to which grammar it\n       matched.  If all interpretation elements
          within the result\n       element contain their own grammar attributes,
          the attribute can\n       be dropped from the result element.\n   <?xml
          version=\"1.0\"?>\n   <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n
          \          grammar=\"http://www.example.com/grammar\">\n     <interpretation>\n
          \     ....\n     </interpretation>\n   </result>\n"
        title: 9.6.3.1.  <result> Root Element
      - contents:
        - "9.6.3.2.  <interpretation> Element\n   An <interpretation> element contains
          a single semantic\n   interpretation.\n   Attributes:\n   1.  confidence:
          A float value from 0.0-1.0 indicating the semantic\n       analyzer's confidence
          in this interpretation.  A value of 1.0\n       indicates maximum confidence.
          \ The values are implementation\n       dependent but are intended to align
          with the value interpretation\n       for the confidence MRCPv2 header field
          defined in Section 9.4.1.\n       This attribute is OPTIONAL.\n   2.  grammar:
          The grammar or recognition rule matched by this\n       interpretation (if
          needed to override the grammar specification\n       at the <interpretation>
          level.)  This attribute is only needed\n       under <interpretation> if
          it is necessary to override a grammar\n       that was defined at the <result>
          level.  Note that the grammar\n       attribute for the interpretation element
          is optional if and only\n       if the grammar attribute is specified in
          the <result> element.\n   Interpretations MUST be sorted best-first by some
          measure of\n   \"goodness\".  The goodness measure is \"confidence\" if
          present;\n   otherwise, it is some implementation-specific indication of
          quality.\n   The grammar is expected to be specified most frequently at
          the\n   <result> level.  However, it can be overridden at the\n   <interpretation>
          level because it is possible that different\n   interpretations may match
          different grammar rules.\n   The <interpretation> element includes an optional
          <input> element\n   containing the input being analyzed, and at least one
          <instance>\n   element containing the interpretation of the utterance.\n
          \  <interpretation confidence=\"0.75\"\n                   grammar=\"http://www.example.com/grammar\">\n
          \      ...\n   </interpretation>\n"
        title: 9.6.3.2.  <interpretation> Element
      - contents:
        - "9.6.3.3.  <instance> Element\n   The <instance> element contains the interpretation
          of the utterance.\n   When the Semantic Interpretation for Speech Recognition
          format is\n   used, the <instance> element contains the XML serialization
          of the\n   result using the approach defined in that specification.  When
          there\n   is semantic markup in the grammar that does not create semantic\n
          \  objects, but instead only does a semantic translation of a portion of\n
          \  the input, such as translating \"coke\" to \"coca-cola\", the instance\n
          \  contains the whole input but with the translation applied.  The NLSML\n
          \  looks like the markup in Figure 2 below.  If there are no semantic\n
          \  objects created, nor any semantic translation, the instance value is\n
          \  the same as the input value.\n   Attributes:\n   1.  confidence: Each
          element of the instance MAY have a confidence\n       attribute, defined
          in the NLSML namespace.  The confidence\n       attribute contains a float
          value in the range from 0.0-1.0\n       reflecting the system's confidence
          in the analysis of that slot.\n       A value of 1.0 indicates maximum confidence.
          \ The values are\n       implementation dependent, but are intended to align
          with the\n       value interpretation for the MRCPv2 header field Confidence-\n
          \      Threshold defined in Section 9.4.1.  This attribute is OPTIONAL.\n
          \  <instance>\n     <nameAddress>\n         <street confidence=\"0.75\">123
          Maple Street</street>\n         <city>Mill Valley</city>\n         <state>CA</state>\n
          \        <zip>90952</zip>\n     </nameAddress>\n   </instance>\n   <input>\n
          \    My address is 123 Maple Street,\n     Mill Valley, California, 90952\n
          \  </input>\n   <instance>\n       I would like to buy a coca-cola\n   </instance>\n
          \  <input>\n     I would like to buy a coke\n   </input>\n                          Figure
          2: NSLML Example\n"
        title: 9.6.3.3.  <instance> Element
      - contents:
        - "9.6.3.4.  <input> Element\n   The <input> element is the text representation
          of a user's input.  It\n   includes an optional \"confidence\" attribute,
          which indicates the\n   recognizer's confidence in the recognition result
          (as opposed to the\n   confidence in the interpretation, which is indicated
          by the\n   \"confidence\" attribute of <interpretation>).  Optional \"timestamp-\n
          \  start\" and \"timestamp-end\" attributes indicate the start and end\n
          \  times of a spoken utterance, in ISO 8601 format [ISO.8601.1988].\n   Attributes:\n
          \  1.  timestamp-start: The time at which the input began. (optional)\n
          \  2.  timestamp-end: The time at which the input ended. (optional)\n   3.
          \ mode: The modality of the input, for example, speech, DTMF, etc.\n       (optional)\n
          \  4.  confidence: The confidence of the recognizer in the correctness\n
          \      of the input in the range 0.0 to 1.0. (optional)\n   Note that it
          may not make sense for temporally overlapping inputs to\n   have the same
          mode; however, this constraint is not expected to be\n   enforced by implementations.\n
          \  When there is no time zone designator, ISO 8601 time representations\n
          \  default to local time.\n   There are three possible formats for the <input>
          element.\n   1.  The <input> element can contain simple text:\n       <input>onions</input>\n
          \      A future possibility is for <input> to contain not only text but\n
          \      additional markup that represents prosodic information that was\n
          \      contained in the original utterance and extracted by the speech\n
          \      recognizer.  This depends on the availability of ASRs that are\n
          \      capable of producing prosodic information.  MRCPv2 clients MUST\n
          \      be prepared to receive such markup and MAY make use of it.\n   2.
          \ An <input> tag can also contain additional <input> tags.  Having\n       additional
          input elements allows the representation to support\n       future multi-modal
          inputs as well as finer-grained speech\n       information, such as timestamps
          for individual words and word-\n       level confidences.\n       <input>\n
          \           <input mode=\"speech\" confidence=\"0.5\"\n                timestamp-start=\"2000-04-03T0:00:00\"\n
          \               timestamp-end=\"2000-04-03T0:00:00.2\">fried</input>\n            <input
          mode=\"speech\" confidence=\"1.0\"\n                timestamp-start=\"2000-04-03T0:00:00.25\"\n
          \               timestamp-end=\"2000-04-03T0:00:00.6\">onions</input>\n
          \      </input>\n   3.  Finally, the <input> element can contain <nomatch>
          and <noinput>\n       elements, which describe situations in which the speech\n
          \      recognizer received input that it was unable to process or did\n
          \      not receive any input at all, respectively.\n"
        title: 9.6.3.4.  <input> Element
      - contents:
        - "9.6.3.5.  <nomatch> Element\n   The <nomatch> element under <input> is
          used to indicate that the\n   semantic interpreter was unable to successfully
          match any input with\n   confidence above the threshold.  It can optionally
          contain the text\n   of the best of the (rejected) matches.\n   <interpretation>\n
          \     <instance/>\n         <input confidence=\"0.1\">\n            <nomatch/>\n
          \        </input>\n   </interpretation>\n   <interpretation>\n      <instance/>\n
          \     <input mode=\"speech\" confidence=\"0.1\">\n        <nomatch>I want
          to go to New York</nomatch>\n      </input>\n   </interpretation>\n"
        title: 9.6.3.5.  <nomatch> Element
      - contents:
        - "9.6.3.6.  <noinput> Element\n   <noinput> indicates that there was no input
          -- a timeout occurred in\n   the speech recognizer due to silence.\n   <interpretation>\n
          \     <instance/>\n      <input>\n         <noinput/>\n      </input>\n
          \  </interpretation>\n   If there are multiple levels of inputs, the most
          natural place for\n   <nomatch> and <noinput> elements to appear is under
          the highest level\n   of <input> for <noinput>, and under the appropriate
          level of\n   <interpretation> for <nomatch>.  So, <noinput> means \"no input
          at\n   all\" and <nomatch> means \"no match in speech modality\" or \"no
          match\n   in DTMF modality\".  For example, to represent garbled speech
          combined\n   with DTMF \"1 2 3 4\", the markup would be:\n   <input>\n      <input
          mode=\"speech\"><nomatch/></input>\n      <input mode=\"dtmf\">1 2 3 4</input>\n
          \  </input>\n   Note: while <noinput> could be represented as an attribute
          of input,\n   <nomatch> cannot, since it could potentially include PCDATA
          content\n   with the best match.  For parallelism, <noinput> is also an
          element.\n"
        title: 9.6.3.6.  <noinput> Element
      title: 9.6.3.  Elements and Attributes
    title: 9.6.  Recognizer Results
  - contents:
    - "9.7.  Enrollment Results\n   All enrollment elements are contained within a
      single\n   <enrollment-result> element under <result>.  The elements are\n   described
      below and have the schema defined in Section 16.2.  The\n   following elements
      are defined:\n   1.  num-clashes\n   2.  num-good-repetitions\n   3.  num-repetitions-still-needed\n
      \  4.  consistency-status\n   5.  clash-phrase-ids\n   6.  transcriptions\n
      \  7.  confusable-phrases\n"
    - contents:
      - "9.7.1.  <num-clashes> Element\n   The <num-clashes> element contains the
        number of clashes that this\n   pronunciation has with other pronunciations
        in an active enrollment\n   session.  The associated Clash-Threshold header
        field determines the\n   sensitivity of the clash measurement.  Note that
        clash testing can be\n   turned off completely by setting the Clash-Threshold
        header field\n   value to 0.\n"
      title: 9.7.1.  <num-clashes> Element
    - contents:
      - "9.7.2.  <num-good-repetitions> Element\n   The <num-good-repetitions> element
        contains the number of consistent\n   pronunciations obtained so far in an
        active enrollment session.\n"
      title: 9.7.2.  <num-good-repetitions> Element
    - contents:
      - "9.7.3.  <num-repetitions-still-needed> Element\n   The <num-repetitions-still-needed>
        element contains the number of\n   consistent pronunciations that must still
        be obtained before the new\n   phrase can be added to the enrollment grammar.
        \ The number of\n   consistent pronunciations required is specified by the
        client in the\n   request header field Num-Min-Consistent-Pronunciations.
        \ The returned\n   value must be 0 before the client can successfully commit
        a phrase to\n   the grammar by ending the enrollment session.\n"
      title: 9.7.3.  <num-repetitions-still-needed> Element
    - contents:
      - "9.7.4.  <consistency-status> Element\n   The <consistency-status> element
        is used to indicate how consistent\n   the repetitions are when learning a
        new phrase.  It can have the\n   values of consistent, inconsistent, and undecided.\n"
      title: 9.7.4.  <consistency-status> Element
    - contents:
      - "9.7.5.  <clash-phrase-ids> Element\n   The <clash-phrase-ids> element contains
        the phrase IDs of clashing\n   pronunciation(s), if any.  This element is
        absent if there are no\n   clashes.\n"
      title: 9.7.5.  <clash-phrase-ids> Element
    - contents:
      - "9.7.6.  <transcriptions> Element\n   The <transcriptions> element contains
        the transcriptions returned in\n   the last repetition of the phrase being
        enrolled.\n"
      title: 9.7.6.  <transcriptions> Element
    - contents:
      - "9.7.7.  <confusable-phrases> Element\n   The <confusable-phrases> element
        contains a list of phrases from a\n   command grammar that are confusable
        with the phrase being added to\n   the personal grammar.  This element MAY
        be absent if there are no\n   confusable phrases.\n"
      title: 9.7.7.  <confusable-phrases> Element
    title: 9.7.  Enrollment Results
  - contents:
    - "9.8.  DEFINE-GRAMMAR\n   The DEFINE-GRAMMAR method, from the client to the
      server, provides\n   one or more grammars and requests the server to access,
      fetch, and\n   compile the grammars as needed.  The DEFINE-GRAMMAR method\n
      \  implementation MUST do a fetch of all external URIs that are part of\n   that
      operation.  If caching is implemented, this URI fetching MUST\n   conform to
      the cache control hints and parameter header fields\n   associated with the
      method in deciding whether the URIs should be\n   fetched from cache or from
      the external server.  If these hints/\n   parameters are not specified in the
      method, the values set for the\n   session using SET-PARAMS/GET-PARAMS apply.
      \ If it was not set for the\n   session, their default values apply.\n   If
      the server resource is in the recognition state, the DEFINE-\n   GRAMMAR request
      MUST respond with a failure status.\n   If the resource is in the idle state
      and is able to successfully\n   process the supplied grammars, the server MUST
      return a success code\n   status and the request-state MUST be COMPLETE.\n   If
      the recognizer resource could not define the grammar for some\n   reason (for
      example, if the download failed, the grammar failed to\n   compile, or the grammar
      was in an unsupported form), the MRCPv2\n   response for the DEFINE-GRAMMAR
      method MUST contain a failure status-\n   code of 407 and contain a Completion-Cause
      header field describing\n   the failure reason.\n   C->S:MRCP/2.0 ... DEFINE-GRAMMAR
      543257\n   Channel-Identifier:32AECB23433801@speechrecog\n   Content-Type:application/srgs+xml\n
      \  Content-ID:<request1@form-level.store>\n   Content-Length:...\n   <?xml version=\"1.0\"?>\n
      \  <!-- the default grammar language is US English -->\n   <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \           xml:lang=\"en-US\" version=\"1.0\">\n   <!-- single language attachment
      to tokens -->\n   <rule id=\"yes\">\n               <one-of>\n                     <item
      xml:lang=\"fr-CA\">oui</item>\n                     <item xml:lang=\"en-US\">yes</item>\n
      \              </one-of>\n         </rule>\n   <!-- single language attachment
      to a rule expansion -->\n         <rule id=\"request\">\n               may
      I speak to\n               <one-of xml:lang=\"fr-CA\">\n                     <item>Michel
      Tremblay</item>\n                     <item>Andre Roy</item>\n               </one-of>\n
      \        </rule>\n         </grammar>\n   S->C:MRCP/2.0 ... 543257 200 COMPLETE\n
      \  Channel-Identifier:32AECB23433801@speechrecog\n           Completion-Cause:000
      success\n   C->S:MRCP/2.0 ... DEFINE-GRAMMAR 543258\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  Content-Type:application/srgs+xml\n   Content-ID:<helpgrammar@root-level.store>\n
      \  Content-Length:...\n   <?xml version=\"1.0\"?>\n   <!-- the default grammar
      language is US English -->\n   <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \           xml:lang=\"en-US\" version=\"1.0\">\n         <rule id=\"request\">\n
      \              I need help\n         </rule>\n   S->C:MRCP/2.0 ... 543258 200
      COMPLETE\n   Channel-Identifier:32AECB23433801@speechrecog\n           Completion-Cause:000
      success\n   C->S:MRCP/2.0 ... DEFINE-GRAMMAR 543259\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  Content-Type:application/srgs+xml\n   Content-ID:<request2@field-level.store>\n
      \  Content-Length:...\n   <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n   <!DOCTYPE
      grammar PUBLIC \"-//W3C//DTD GRAMMAR 1.0//EN\"\n                     \"http://www.w3.org/TR/speech-grammar/grammar.dtd\">\n
      \  <grammar xmlns=\"http://www.w3.org/2001/06/grammar\" xml:lang=\"en\"\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n
      \         xsi:schemaLocation=\"http://www.w3.org/2001/06/grammar\n              http://www.w3.org/TR/speech-grammar/grammar.xsd\"\n
      \             version=\"1.0\" mode=\"voice\" root=\"basicCmd\">\n   <meta name=\"author\"
      content=\"Stephanie Williams\"/>\n   <rule id=\"basicCmd\" scope=\"public\">\n
      \    <example> please move the window </example>\n     <example> open a file
      </example>\n     <ruleref\n       uri=\"http://grammar.example.com/politeness.grxml#startPolite\"/>\n
      \    <ruleref uri=\"#command\"/>\n     <ruleref\n       uri=\"http://grammar.example.com/politeness.grxml#endPolite\"/>\n
      \  </rule>\n   <rule id=\"command\">\n     <ruleref uri=\"#action\"/> <ruleref
      uri=\"#object\"/>\n   </rule>\n   <rule id=\"action\">\n      <one-of>\n         <item
      weight=\"10\"> open   <tag>open</tag>   </item>\n         <item weight=\"2\">
      \ close  <tag>close</tag>  </item>\n         <item weight=\"1\">  delete <tag>delete</tag>
      </item>\n         <item weight=\"1\">  move   <tag>move</tag>   </item>\n      </one-of>\n
      \  </rule>\n   <rule id=\"object\">\n     <item repeat=\"0-1\">\n       <one-of>\n
      \        <item> the </item>\n         <item> a </item>\n       </one-of>\n     </item>\n
      \    <one-of>\n         <item> window </item>\n         <item> file </item>\n
      \        <item> menu </item>\n     </one-of>\n   </rule>\n   </grammar>\n   S->C:MRCP/2.0
      ... 543259 200 COMPLETE\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \          Completion-Cause:000 success\n   C->S:MRCP/2.0 ... RECOGNIZE 543260\n
      \  Channel-Identifier:32AECB23433801@speechrecog\n           N-Best-List-Length:2\n
      \  Content-Type:text/uri-list\n   Content-Length:...\n   session:request1@form-level.store\n
      \  session:request2@field-level.store\n   session:helpgramar@root-level.store\n
      \  S->C:MRCP/2.0 ... 543260 200 IN-PROGRESS\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:MRCP/2.0 ... START-OF-INPUT 543260 IN-PROGRESS\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:MRCP/2.0 ... RECOGNITION-COMPLETE 543260 COMPLETE\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  Completion-Cause:000 success\n   Waveform-URI:<http://web.media.com/session123/audio.wav>;\n
      \               size=124535;duration=2340\n   Content-Type:application/x-nlsml\n
      \  Content-Length:...\n   <?xml version=\"1.0\"?>\n   <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n
      \          xmlns:ex=\"http://www.example.com/example\"\n           grammar=\"session:request1@form-level.store\">\n
      \          <interpretation>\n               <instance name=\"Person\">\n               <ex:Person>\n
      \                  <ex:Name> Andre Roy </ex:Name>\n               </ex:Person>\n
      \           </instance>\n            <input>   may I speak to Andre Roy </input>\n
      \      </interpretation>\n   </result>\n                          Define Grammar
      Example\n"
    title: 9.8.  DEFINE-GRAMMAR
  - contents:
    - "9.9.  RECOGNIZE\n   The RECOGNIZE method from the client to the server requests
      the\n   recognizer to start recognition and provides it with one or more\n   grammar
      references for grammars to match against the input media.\n   The RECOGNIZE
      method can carry header fields to control the\n   sensitivity, confidence level,
      and the level of detail in results\n   provided by the recognizer.  These header
      field values override the\n   current values set by a previous SET-PARAMS method.\n
      \  The RECOGNIZE method can request the recognizer resource to operate\n   in
      normal or hotword mode as specified by the Recognition-Mode header\n   field.
      \ The default value is \"normal\".  If the resource could not\n   start a recognition,
      the server MUST respond with a failure status-\n   code of 407 and a Completion-Cause
      header field in the response\n   describing the cause of failure.\n   The RECOGNIZE
      request uses the message body to specify the grammars\n   applicable to the
      request.  The active grammar(s) for the request can\n   be specified in one
      of three ways.  If the client needs to explicitly\n   control grammar weights
      for the recognition operation, it MUST employ\n   method 3 below.  The order
      of these grammars specifies the precedence\n   of the grammars that is used
      when more than one grammar in the list\n   matches the speech; in this case,
      the grammar with the higher\n   precedence is returned as a match.  This precedence
      capability is\n   useful in applications like VoiceXML browsers to order grammars\n
      \  specified at the dialog, document, and root level of a VoiceXML\n   application.\n
      \  1.  The grammar MAY be placed directly in the message body as typed\n       content.
      \ If more than one grammar is included in the body, the\n       order of inclusion
      controls the corresponding precedence for the\n       grammars during recognition,
      with earlier grammars in the body\n       having a higher precedence than later
      ones.\n   2.  The body MAY contain a list of grammar URIs specified in content\n
      \      of media type 'text/uri-list' [RFC2483].  The order of the URIs\n       determines
      the corresponding precedence for the grammars during\n       recognition, with
      highest precedence first and decreasing for\n       each URI thereafter.\n   3.
      \ The body MAY contain a list of grammar URIs specified in content\n       of
      media type 'text/grammar-ref-list'.  This type defines a list\n       of grammar
      URIs and allows each grammar URI to be assigned a\n       weight in the list.
      \ This weight has the same meaning as the\n       weights described in Section
      2.4.1 of the Speech Grammar Markup\n       Format (SRGS) [W3C.REC-speech-grammar-20040316].\n
      \  In addition to performing recognition on the input, the recognizer\n   MUST
      also enroll the collected utterance in a personal grammar if the\n   Enroll-Utterance
      header field is set to true and an Enrollment is\n   active (via an earlier
      execution of the START-PHRASE-ENROLLMENT\n   method).  If so, and if the RECOGNIZE
      request contains a Content-ID\n   header field, then the resulting grammar (which
      includes the personal\n   grammar as a sub-grammar) can be referenced through
      the 'session' URI\n   scheme (see Section 13.6).\n   If the resource was able
      to successfully start the recognition, the\n   server MUST return a success
      status-code and a request-state of\n   IN-PROGRESS.  This means that the recognizer
      is active and that the\n   client MUST be prepared to receive further events
      with this\n   request-id.\n   If the resource was able to queue the request,
      the server MUST return\n   a success code and request-state of PENDING.  This
      means that the\n   recognizer is currently active with another request and that
      this\n   request has been queued for processing.\n   If the resource could not
      start a recognition, the server MUST\n   respond with a failure status-code
      of 407 and a Completion-Cause\n   header field in the response describing the
      cause of failure.\n   For the recognizer resource, RECOGNIZE and INTERPRET are
      the only\n   requests that return a request-state of IN-PROGRESS, meaning that\n
      \  recognition is in progress.  When the recognition completes by\n   matching
      one of the grammar alternatives or by a timeout without a\n   match or for some
      other reason, the recognizer resource MUST send the\n   client a RECOGNITION-COMPLETE
      event (or INTERPRETATION-COMPLETE, if\n   INTERPRET was the request) with the
      result of the recognition and a\n   request-state of COMPLETE.\n   Large grammars
      can take a long time for the server to compile.  For\n   grammars that are used
      repeatedly, the client can improve server\n   performance by issuing a DEFINE-GRAMMAR
      request with the grammar\n   ahead of time.  In such a case, the client can
      issue the RECOGNIZE\n   request and reference the grammar through the 'session'
      URI scheme\n   (see Section 13.6).  This also applies in general if the client
      wants\n   to repeat recognition with a previous inline grammar.\n   The RECOGNIZE
      method implementation MUST do a fetch of all external\n   URIs that are part
      of that operation.  If caching is implemented,\n   this URI fetching MUST conform
      to the cache control hints and\n   parameter header fields associated with the
      method in deciding\n   whether it should be fetched from cache or from the external
      server.\n   If these hints/parameters are not specified in the method, the values\n
      \  set for the session using SET-PARAMS/GET-PARAMS apply.  If it was not\n   set
      for the session, their default values apply.\n   Note that since the audio and
      the messages are carried over separate\n   communication paths there may be
      a race condition between the start\n   of the flow of audio and the receipt
      of the RECOGNIZE method.  For\n   example, if an audio flow is started by the
      client at the same time\n   as the RECOGNIZE method is sent, either the audio
      or the RECOGNIZE\n   can arrive at the recognizer first.  As another example,
      the client\n   may choose to continuously send audio to the server and signal
      the\n   server to recognize using the RECOGNIZE method.  Mechanisms to\n   resolve
      this condition are outside the scope of this specification.\n   The recognizer
      can expect the media to start flowing when it receives\n   the RECOGNIZE request,
      but it MUST NOT buffer anything it receives\n   beforehand in order to preserve
      the semantics that application\n   authors expect with respect to the input
      timers.\n   When a RECOGNIZE method has been received, the recognition is\n
      \  initiated on the stream.  The No-Input-Timer MUST be started at this\n   time
      if the Start-Input-Timers header field is specified as \"true\".\n   If this
      header field is set to \"false\", the No-Input-Timer MUST be\n   started when
      it receives the START-INPUT-TIMERS method from the\n   client.  The Recognition-Timeout
      MUST be started when the recognition\n   resource detects speech or a DTMF digit
      in the media stream.\n   For recognition when not in hotword mode:\n   When
      the recognizer resource detects speech or a DTMF digit in the\n   media stream,
      it MUST send the START-OF-INPUT event.  When enough\n   speech has been collected
      for the server to process, the recognizer\n   can try to match the collected
      speech with the active grammars.  If\n   the speech collected at this point
      fully matches with any of the\n   active grammars, the Speech-Complete-Timer
      is started.  If it matches\n   partially with one or more of the active grammars,
      with more speech\n   needed before a full match is achieved, then the Speech-Incomplete-\n
      \  Timer is started.\n   1.  When the No-Input-Timer expires, the recognizer
      MUST complete\n       with a Completion-Cause code of \"no-input-timeout\".\n
      \  2.  The recognizer MUST support detecting a no-match condition upon\n       detecting
      end of speech.  The recognizer MAY support detecting a\n       no-match condition
      before waiting for end-of-speech.  If this is\n       supported, this capability
      is enabled by setting the Early-No-\n       Match header field to \"true\".
      \ Upon detecting a no-match\n       condition, the RECOGNIZE MUST return with
      \"no-match\".\n   3.  When the Speech-Incomplete-Timer expires, the recognizer
      SHOULD\n       complete with a Completion-Cause code of \"partial-match\", unless\n
      \      the recognizer cannot differentiate a partial-match, in which\n       case
      it MUST return a Completion-Cause code of \"no-match\".  The\n       recognizer
      MAY return results for the partially matched grammar.\n   4.  When the Speech-Complete-Timer
      expires, the recognizer MUST\n       complete with a Completion-Cause code of
      \"success\".\n   5.  When the Recognition-Timeout expires, one of the following
      MUST\n       happen:\n       5.1.  If there was a partial-match, the recognizer
      SHOULD\n             complete with a Completion-Cause code of \"partial-match-\n
      \            maxtime\", unless the recognizer cannot differentiate a\n             partial-match,
      in which case it MUST complete with a\n             Completion-Cause code of
      \"no-match-maxtime\".  The\n             recognizer MAY return results for the
      partially matched\n             grammar.\n       5.2.  If there was a full-match,
      the recognizer MUST complete\n             with a Completion-Cause code of \"success-maxtime\".\n
      \      5.3.  If there was a no match, the recognizer MUST complete with\n             a
      Completion-Cause code of \"no-match-maxtime\".\n   For recognition in hotword
      mode:\n   Note that for recognition in hotword mode the START-OF-INPUT event
      is\n   not generated when speech or a DTMF digit is detected.\n   1.  When the
      No-Input-Timer expires, the recognizer MUST complete\n       with a Completion-Cause
      code of \"no-input-timeout\".\n   2.  If at any point a match occurs, the RECOGNIZE
      MUST complete with\n       a Completion-Cause code of \"success\".\n   3.  When
      the Recognition-Timeout expires and there is not a match,\n       the RECOGNIZE
      MUST complete with a Completion-Cause code of\n       \"hotword-maxtime\".\n
      \  4.  When the Recognition-Timeout expires and there is a match, the\n       RECOGNIZE
      MUST complete with a Completion-Cause code of \"success-\n       maxtime\".\n
      \  5.  When the Recognition-Timeout is running but the detected speech/\n       DTMF
      has not resulted in a match, the Recognition-Timeout MUST be\n       stopped
      and reset.  It MUST then be restarted when speech/DTMF is\n       again detected.\n
      \  Below is a complete example of using RECOGNIZE.  It shows the call to\n   RECOGNIZE,
      the IN-PROGRESS and START-OF-INPUT status messages, and\n   the final RECOGNITION-COMPLETE
      message containing the result.\n   C->S:MRCP/2.0 ... RECOGNIZE 543257\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \          Confidence-Threshold:0.9\n   Content-Type:application/srgs+xml\n
      \  Content-ID:<request1@form-level.store>\n   Content-Length:...\n   <?xml version=\"1.0\"?>\n
      \  <!-- the default grammar language is US English -->\n   <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \           xml:lang=\"en-US\" version=\"1.0\" root=\"request\">\n   <!-- single
      language attachment to tokens -->\n       <rule id=\"yes\">\n               <one-of>\n
      \                    <item xml:lang=\"fr-CA\">oui</item>\n                     <item
      xml:lang=\"en-US\">yes</item>\n               </one-of>\n         </rule>\n
      \  <!-- single language attachment to a rule expansion -->\n         <rule id=\"request\">\n
      \              may I speak to\n               <one-of xml:lang=\"fr-CA\">\n
      \                    <item>Michel Tremblay</item>\n                     <item>Andre
      Roy</item>\n               </one-of>\n         </rule>\n     </grammar>\n   S->C:
      MRCP/2.0 ... 543257 200 IN-PROGRESS\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:MRCP/2.0 ... START-OF-INPUT 543257 IN-PROGRESS\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:MRCP/2.0 ... RECOGNITION-COMPLETE 543257 COMPLETE\n   Channel-Identifier:32AECB23433801@speechrecog\n
      \  Completion-Cause:000 success\n   Waveform-URI:<http://web.media.com/session123/audio.wav>;\n
      \                size=424252;duration=2543\n   Content-Type:application/nlsml+xml\n
      \  Content-Length:...\n   <?xml version=\"1.0\"?>\n   <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n
      \          xmlns:ex=\"http://www.example.com/example\"\n           grammar=\"session:request1@form-level.store\">\n
      \      <interpretation>\n           <instance name=\"Person\">\n               <ex:Person>\n
      \                  <ex:Name> Andre Roy </ex:Name>\n               </ex:Person>\n
      \          </instance>\n               <input>   may I speak to Andre Roy </input>\n
      \      </interpretation>\n   </result>\n   Below is an example of calling RECOGNIZE
      with a different grammar.\n   No status or completion messages are shown in
      this example, although\n   they would of course occur in normal usage.\n   C->S:
      \  MRCP/2.0 ... RECOGNIZE 543257\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Confidence-Threshold:0.9\n           Fetch-Timeout:20\n           Content-Type:application/srgs+xml\n
      \          Content-Length:...\n           <?xml version=\"1.0\"? Version=\"1.0\"
      mode=\"voice\"\n                 root=\"Basic md\">\n            <rule id=\"rule_list\"
      scope=\"public\">\n                <one-of>\n                    <item weight=10>\n
      \                       <ruleref uri=\n               \"http://grammar.example.com/world-cities.grxml#canada\"/>\n
      \                  </item>\n                   <item weight=1.5>\n                       <ruleref
      uri=\n               \"http://grammar.example.com/world-cities.grxml#america\"/>\n
      \                  </item>\n                  <item weight=0.5>\n                       <ruleref
      uri=\n               \"http://grammar.example.com/world-cities.grxml#india\"/>\n
      \                 </item>\n              </one-of>\n           </rule>\n"
    title: 9.9.  RECOGNIZE
  - contents:
    - "9.10.  STOP\n   The STOP method from the client to the server tells the resource
      to\n   stop recognition if a request is active.  If a RECOGNIZE request is\n
      \  active and the STOP request successfully terminated it, then the\n   response
      header section contains an Active-Request-Id-List header\n   field containing
      the request-id of the RECOGNIZE request that was\n   terminated.  In this case,
      no RECOGNITION-COMPLETE event is sent for\n   the terminated request.  If there
      was no recognition active, then the\n   response MUST NOT contain an Active-Request-Id-List
      header field.\n   Either way, the response MUST contain a status-code of 200
      \"Success\".\n   C->S:   MRCP/2.0 ... RECOGNIZE 543257\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Confidence-Threshold:0.9\n           Content-Type:application/srgs+xml\n
      \          Content-ID:<request1@form-level.store>\n           Content-Length:...\n
      \          <?xml version=\"1.0\"?>\n           <!-- the default grammar language
      is US English -->\n           <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \                   xml:lang=\"en-US\" version=\"1.0\" root=\"request\">\n           <!--
      single language attachment to tokens -->\n               <rule id=\"yes\">\n
      \                  <one-of>\n                         <item xml:lang=\"fr-CA\">oui</item>\n
      \                        <item xml:lang=\"en-US\">yes</item>\n                   </one-of>\n
      \              </rule>\n           <!-- single language attachment to a rule
      expansion -->\n               <rule id=\"request\">\n               may I speak
      to\n                   <one-of xml:lang=\"fr-CA\">\n                         <item>Michel
      Tremblay</item>\n                         <item>Andre Roy</item>\n                   </one-of>\n
      \              </rule>\n           </grammar>\n   S->C:   MRCP/2.0 ... 543257
      200 IN-PROGRESS\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \  C->S:   MRCP/2.0 ... STOP 543258 200\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:   MRCP/2.0 ... 543258 200 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Active-Request-Id-List:543257\n"
    title: 9.10.  STOP
  - contents:
    - "9.11.  GET-RESULT\n   The GET-RESULT method from the client to the server MAY
      be issued\n   when the recognizer resource is in the recognized state.  This\n
      \  request allows the client to retrieve results for a completed\n   recognition.
      \ This is useful if the client decides it wants more\n   alternatives or more
      information.  When the server receives this\n   request, it re-computes and
      returns the results according to the\n   recognition constraints provided in
      the GET-RESULT request.\n   The GET-RESULT request can specify constraints such
      as a different\n   confidence-threshold or n-best-list-length.  This capability
      is\n   OPTIONAL for MRCPv2 servers and the automatic speech recognition\n   engine
      in the server MUST return a status of unsupported feature if\n   not supported.\n
      \  C->S:   MRCP/2.0 ... GET-RESULT 543257\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Confidence-Threshold:0.9\n   S->C:   MRCP/2.0 ... 543257 200 COMPLETE\n
      \          Channel-Identifier:32AECB23433801@speechrecog\n           Content-Type:application/nlsml+xml\n
      \          Content-Length:...\n           <?xml version=\"1.0\"?>\n           <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                   xmlns:ex=\"http://www.example.com/example\"\n
      \                  grammar=\"session:request1@form-level.store\">\n               <interpretation>\n
      \                  <instance name=\"Person\">\n                       <ex:Person>\n
      \                          <ex:Name> Andre Roy </ex:Name>\n                       </ex:Person>\n
      \                  </instance>\n                   <input>   may I speak to
      Andre Roy </input>\n               </interpretation>\n           </result>\n"
    title: 9.11.  GET-RESULT
  - contents:
    - "9.12.  START-OF-INPUT\n   This is an event from the server to the client indicating
      that the\n   recognizer resource has detected speech or a DTMF digit in the
      media\n   stream.  This event is useful in implementing kill-on-barge-in\n   scenarios
      when a synthesizer resource is in a different session from\n   the recognizer
      resource and hence is not aware of an incoming audio\n   source (see Section
      8.4.2).  In these cases, it is up to the client\n   to act as an intermediary
      and respond to this event by issuing a\n   BARGE-IN-OCCURRED event to the synthesizer
      resource.  The recognizer\n   resource also MUST send a Proxy-Sync-Id header
      field with a unique\n   value for this event.\n   This event MUST be generated
      by the server, irrespective of whether\n   or not the synthesizer and recognizer
      are on the same server.\n"
    title: 9.12.  START-OF-INPUT
  - contents:
    - "9.13.  START-INPUT-TIMERS\n   This request is sent from the client to the recognizer
      resource when\n   it knows that a kill-on-barge-in prompt has finished playing
      (see\n   Section 8.4.2).  This is useful in the scenario when the recognition\n
      \  and synthesizer engines are not in the same session.  When a kill-on-\n   barge-in
      prompt is being played, the client may want a RECOGNIZE\n   request to be simultaneously
      active so that it can detect and\n   implement kill-on-barge-in.  But at the
      same time the client doesn't\n   want the recognizer to start the no-input timers
      until the prompt is\n   finished.  The Start-Input-Timers header field in the
      RECOGNIZE\n   request allows the client to say whether or not the timers should
      be\n   started immediately.  If not, the recognizer resource MUST NOT start\n
      \  the timers until the client sends a START-INPUT-TIMERS method to the\n   recognizer.\n"
    title: 9.13.  START-INPUT-TIMERS
  - contents:
    - "9.14.  RECOGNITION-COMPLETE\n   This is an event from the recognizer resource
      to the client\n   indicating that the recognition completed.  The recognition
      result is\n   sent in the body of the MRCPv2 message.  The request-state field
      MUST\n   be COMPLETE indicating that this is the last event with that\n   request-id
      and that the request with that request-id is now complete.\n   The server MUST
      maintain the recognizer context containing the\n   results and the audio waveform
      input of that recognition until the\n   next RECOGNIZE request is issued for
      that resource or the session\n   terminates.  If the server returns a URI to
      the audio waveform, it\n   MUST do so in a Waveform-URI header field in the
      RECOGNITION-COMPLETE\n   event.  The client can use this URI to retrieve or
      playback the\n   audio.\n   Note, if an enrollment session was active, the RECOGNITION-COMPLETE\n
      \  event can contain either recognition or enrollment results depending\n   on
      what was spoken.  The following example shows a complete exchange\n   with a
      recognition result.\n   C->S:   MRCP/2.0 ... RECOGNIZE 543257\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Confidence-Threshold:0.9\n           Content-Type:application/srgs+xml\n
      \          Content-ID:<request1@form-level.store>\n           Content-Length:...\n
      \          <?xml version=\"1.0\"?>\n           <!-- the default grammar language
      is US English -->\n           <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \                   xml:lang=\"en-US\" version=\"1.0\" root=\"request\">\n           <!--
      single language attachment to tokens -->\n               <rule id=\"yes\">\n
      \                     <one-of>\n                          <item xml:lang=\"fr-CA\">oui</item>\n
      \                         <item xml:lang=\"en-US\">yes</item>\n                      </one-of>\n
      \                </rule>\n           <!-- single language attachment to a rule
      expansion -->\n                 <rule id=\"request\">\n                     may
      I speak to\n                      <one-of xml:lang=\"fr-CA\">\n                             <item>Michel
      Tremblay</item>\n                             <item>Andre Roy</item>\n                      </one-of>\n
      \                </rule>\n           </grammar>\n   S->C:   MRCP/2.0 ... 543257
      200 IN-PROGRESS\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:   MRCP/2.0 ... START-OF-INPUT 543257 IN-PROGRESS\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:   MRCP/2.0 ... RECOGNITION-COMPLETE 543257 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Completion-Cause:000 success\n           Waveform-URI:<http://web.media.com/session123/audio.wav>;\n
      \                       size=342456;duration=25435\n           Content-Type:application/nlsml+xml\n
      \          Content-Length:...\n           <?xml version=\"1.0\"?>\n           <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                   xmlns:ex=\"http://www.example.com/example\"\n
      \                  grammar=\"session:request1@form-level.store\">\n               <interpretation>\n
      \                  <instance name=\"Person\">\n                       <ex:Person>\n
      \                          <ex:Name> Andre Roy </ex:Name>\n                       </ex:Person>\n
      \                  </instance>\n                   <input>   may I speak to
      Andre Roy </input>\n               </interpretation>\n           </result>\n
      \  If the result were instead an enrollment result, the final message\n   from
      the server above could have been:\n   S->C:   MRCP/2.0 ... RECOGNITION-COMPLETE
      543257 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Completion-Cause:000 success\n           Content-Type:application/nlsml+xml\n
      \          Content-Length:...\n           <?xml version= \"1.0\"?>\n           <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                   grammar=\"Personal-Grammar-URI\">\n
      \              <enrollment-result>\n                   <num-clashes> 2 </num-clashes>\n
      \                  <num-good-repetitions> 1 </num-good-repetitions>\n                   <num-repetitions-still-needed>\n
      \                     1\n                   </num-repetitions-still-needed>\n
      \                  <consistency-status> consistent </consistency-status>\n                   <clash-phrase-ids>\n
      \                      <item> Jeff </item> <item> Andre </item>\n                   </clash-phrase-ids>\n
      \                  <transcriptions>\n                        <item> m ay b r
      ow k er </item>\n                        <item> m ax r aa k ah </item>\n                   </transcriptions>\n
      \                  <confusable-phrases>\n                        <item>\n                             <phrase>
      call </phrase>\n                             <confusion-level> 10 </confusion-level>\n
      \                       </item>\n                   </confusable-phrases>\n
      \              </enrollment-result>\n           </result>\n"
    title: 9.14.  RECOGNITION-COMPLETE
  - contents:
    - "9.15.  START-PHRASE-ENROLLMENT\n   The START-PHRASE-ENROLLMENT method from
      the client to the server\n   starts a new phrase enrollment session during which
      the client can\n   call RECOGNIZE multiple times to enroll a new utterance in
      a grammar.\n   An enrollment session consists of a set of calls to RECOGNIZE
      in\n   which the caller speaks a phrase several times so the system can\n   \"learn\"
      it.  The phrase is then added to a personal grammar (speaker-\n   trained grammar),
      so that the system can recognize it later.\n   Only one phrase enrollment session
      can be active at a time for a\n   resource.  The Personal-Grammar-URI identifies
      the grammar that is\n   used during enrollment to store the personal list of
      phrases.  Once\n   RECOGNIZE is called, the result is returned in a RECOGNITION-COMPLETE\n
      \  event and will contain either an enrollment result OR a recognition\n   result
      for a regular recognition.\n   Calling END-PHRASE-ENROLLMENT ends the ongoing
      phrase enrollment\n   session, which is typically done after a sequence of successful
      calls\n   to RECOGNIZE.  This method can be called to commit the new phrase
      to\n   the personal grammar or to abort the phrase enrollment session.\n   The
      grammar to contain the new enrolled phrase, specified by\n   Personal-Grammar-URI,
      is created if it does not exist.  Also, the\n   personal grammar MUST ONLY contain
      phrases added via a phrase\n   enrollment session.\n   The Phrase-ID passed
      to this method is used to identify this phrase\n   in the grammar and will be
      returned as the speech input when doing a\n   RECOGNIZE on the grammar.  The
      Phrase-NL similarly is returned in a\n   RECOGNITION-COMPLETE event in the same
      manner as other Natural\n   Language (NL) in a grammar.  The tag-format of this
      NL is\n   implementation specific.\n   If the client has specified Save-Best-Waveform
      as true, then the\n   response after ending the phrase enrollment session MUST
      contain the\n   location/URI of a recording of the best repetition of the learned\n
      \  phrase.\n   C->S:   MRCP/2.0 ... START-PHRASE-ENROLLMENT 543258\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Num-Min-Consistent-Pronunciations:2\n           Consistency-Threshold:30\n
      \          Clash-Threshold:12\n           Personal-Grammar-URI:<personal grammar
      uri>\n           Phrase-Id:<phrase id>\n           Phrase-NL:<NL phrase>\n           Weight:1\n
      \          Save-Best-Waveform:true\n   S->C:   MRCP/2.0 ... 543258 200 COMPLETE\n
      \          Channel-Identifier:32AECB23433801@speechrecog\n"
    title: 9.15.  START-PHRASE-ENROLLMENT
  - contents:
    - "9.16.  ENROLLMENT-ROLLBACK\n   The ENROLLMENT-ROLLBACK method discards the
      last live utterance from\n   the RECOGNIZE operation.  The client can invoke
      this method when the\n   caller provides undesirable input such as non-speech
      noises, side-\n   speech, commands, utterance from the RECOGNIZE grammar, etc.
      \ Note\n   that this method does not provide a stack of rollback states.\n   Executing
      ENROLLMENT-ROLLBACK twice in succession without an\n   intervening recognition
      operation has no effect the second time.\n   C->S:   MRCP/2.0 ... ENROLLMENT-ROLLBACK
      543261\n           Channel-Identifier:32AECB23433801@speechrecog\n   S->C:   MRCP/2.0
      ... 543261 200 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n"
    title: 9.16.  ENROLLMENT-ROLLBACK
  - contents:
    - "9.17.  END-PHRASE-ENROLLMENT\n   The client MAY call the END-PHRASE-ENROLLMENT
      method ONLY during an\n   active phrase enrollment session.  It MUST NOT be
      called during an\n   ongoing RECOGNIZE operation.  To commit the new phrase
      in the\n   grammar, the client MAY call this method once successive calls to\n
      \  RECOGNIZE have succeeded and Num-Repetitions-Still-Needed has been\n   returned
      as 0 in the RECOGNITION-COMPLETE event.  Alternatively, the\n   client MAY abort
      the phrase enrollment session by calling this method\n   with the Abort-Phrase-Enrollment
      header field.\n   If the client has specified Save-Best-Waveform as \"true\"
      in the\n   START-PHRASE-ENROLLMENT request, then the response MUST contain a\n
      \  Waveform-URI header whose value is the location/URI of a recording of\n   the
      best repetition of the learned phrase.\n  C->S:   MRCP/2.0 ... END-PHRASE-ENROLLMENT
      543262\n          Channel-Identifier:32AECB23433801@speechrecog\n  S->C:   MRCP/2.0
      ... 543262 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speechrecog\n
      \         Waveform-URI:<http://mediaserver.com/recordings/file1324.wav>;\n                       size=242453;duration=25432\n"
    title: 9.17.  END-PHRASE-ENROLLMENT
  - contents:
    - "9.18.  MODIFY-PHRASE\n   The MODIFY-PHRASE method sent from the client to the
      server is used\n   to change the phrase ID, NL phrase, and/or weight for a given
      phrase\n   in a personal grammar.\n   If no fields are supplied, then calling
      this method has no effect.\n   C->S:   MRCP/2.0 ... MODIFY-PHRASE 543265\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Personal-Grammar-URI:<personal grammar uri>\n           Phrase-Id:<phrase
      id>\n           New-Phrase-Id:<new phrase id>\n           Phrase-NL:<NL phrase>\n
      \          Weight:1\n   S->C:   MRCP/2.0 ... 543265 200 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n"
    title: 9.18.  MODIFY-PHRASE
  - contents:
    - "9.19.  DELETE-PHRASE\n   The DELETE-PHRASE method sent from the client to the
      server is used\n   to delete a phase that is in a personal grammar and was added
      through\n   voice enrollment or text enrollment.  If the specified phrase does\n
      \  not exist, this method has no effect.\n   C->S:   MRCP/2.0 ... DELETE-PHRASE
      543266\n           Channel-Identifier:32AECB23433801@speechrecog\n           Personal-Grammar-URI:<personal
      grammar uri>\n           Phrase-Id:<phrase id>\n   S->C:   MRCP/2.0 ... 543266
      200 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n"
    title: 9.19.  DELETE-PHRASE
  - contents:
    - "9.20.  INTERPRET\n   The INTERPRET method from the client to the server takes
      as input an\n   Interpret-Text header field containing the text for which the\n
      \  semantic interpretation is desired, and returns, via the\n   INTERPRETATION-COMPLETE
      event, an interpretation result that is very\n   similar to the one returned
      from a RECOGNIZE method invocation.  Only\n   portions of the result relevant
      to acoustic matching are excluded\n   from the result.  The Interpret-Text header
      field MUST be included in\n   the INTERPRET request.\n   Recognizer grammar
      data is treated in the same way as it is when\n   issuing a RECOGNIZE method
      call.\n   If a RECOGNIZE, RECORD, or another INTERPRET operation is already
      in\n   progress for the resource, the server MUST reject the request with a\n
      \  response having a status-code of 402 \"Method not valid in this\n   state\",
      and a COMPLETE request state.\n   C->S:   MRCP/2.0 ... INTERPRET 543266\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Interpret-Text:may I speak to Andre Roy\n           Content-Type:application/srgs+xml\n
      \          Content-ID:<request1@form-level.store>\n           Content-Length:...\n
      \          <?xml version=\"1.0\"?>\n           <!-- the default grammar language
      is US English -->\n           <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \                   xml:lang=\"en-US\" version=\"1.0\" root=\"request\">\n           <!--
      single language attachment to tokens -->\n               <rule id=\"yes\">\n
      \                  <one-of>\n                       <item xml:lang=\"fr-CA\">oui</item>\n
      \                      <item xml:lang=\"en-US\">yes</item>\n                   </one-of>\n
      \              </rule>\n           <!-- single language attachment to a rule
      expansion -->\n               <rule id=\"request\">\n                   may
      I speak to\n                   <one-of xml:lang=\"fr-CA\">\n                       <item>Michel
      Tremblay</item>\n                       <item>Andre Roy</item>\n                   </one-of>\n
      \              </rule>\n           </grammar>\n   S->C:   MRCP/2.0 ... 543266
      200 IN-PROGRESS\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:   MRCP/2.0 ... INTERPRETATION-COMPLETE 543266 200 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Completion-Cause:000 success\n           Content-Type:application/nlsml+xml\n
      \          Content-Length:...\n           <?xml version=\"1.0\"?>\n           <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                   xmlns:ex=\"http://www.example.com/example\"\n
      \                  grammar=\"session:request1@form-level.store\">\n               <interpretation>\n
      \                  <instance name=\"Person\">\n                       <ex:Person>\n
      \                          <ex:Name> Andre Roy </ex:Name>\n                       </ex:Person>\n
      \                  </instance>\n                   <input>   may I speak to
      Andre Roy </input>\n               </interpretation>\n           </result>\n"
    title: 9.20.  INTERPRET
  - contents:
    - "9.21.  INTERPRETATION-COMPLETE\n   This event from the recognizer resource
      to the client indicates that\n   the INTERPRET operation is complete.  The interpretation
      result is\n   sent in the body of the MRCP message.  The request state MUST
      be set\n   to COMPLETE.\n   The Completion-Cause header field MUST be included
      in this event and\n   MUST be set to an appropriate value from the list of cause
      codes.\n   C->S:    MRCP/2.0 ... INTERPRET 543266\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Interpret-Text:may I speak to Andre Roy\n           Content-Type:application/srgs+xml\n
      \          Content-ID:<request1@form-level.store>\n           Content-Length:...\n
      \          <?xml version=\"1.0\"?>\n           <!-- the default grammar language
      is US English -->\n           <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \                   xml:lang=\"en-US\" version=\"1.0\" root=\"request\">\n           <!--
      single language attachment to tokens -->\n               <rule id=\"yes\">\n
      \                  <one-of>\n                       <item xml:lang=\"fr-CA\">oui</item>\n
      \                      <item xml:lang=\"en-US\">yes</item>\n                   </one-of>\n
      \              </rule>\n           <!-- single language attachment to a rule
      expansion -->\n               <rule id=\"request\">\n                   may
      I speak to\n                   <one-of xml:lang=\"fr-CA\">\n                       <item>Michel
      Tremblay</item>\n                       <item>Andre Roy</item>\n                   </one-of>\n
      \              </rule>\n           </grammar>\n   S->C:    MRCP/2.0 ... 543266
      200 IN-PROGRESS\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \  S->C:    MRCP/2.0 ... INTERPRETATION-COMPLETE 543266 200 COMPLETE\n           Channel-Identifier:32AECB23433801@speechrecog\n
      \          Completion-Cause:000 success\n           Content-Type:application/nlsml+xml\n
      \          Content-Length:...\n           <?xml version=\"1.0\"?>\n           <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                   xmlns:ex=\"http://www.example.com/example\"\n
      \                  grammar=\"session:request1@form-level.store\">\n               <interpretation>\n
      \                  <instance name=\"Person\">\n                       <ex:Person>\n
      \                          <ex:Name> Andre Roy </ex:Name>\n                       </ex:Person>\n
      \                  </instance>\n                   <input>   may I speak to
      Andre Roy </input>\n               </interpretation>\n           </result>\n"
    title: 9.21.  INTERPRETATION-COMPLETE
  - contents:
    - "9.22.  DTMF Detection\n   Digits received as DTMF tones are delivered to the
      recognition\n   resource in the MRCPv2 server in the RTP stream according to
      RFC 4733\n   [RFC4733].  The Automatic Speech Recognizer (ASR) MUST support
      RFC\n   4733 to recognize digits, and it MAY support recognizing DTMF tones\n
      \  [Q.23] in the audio.\n"
    title: 9.22.  DTMF Detection
  title: 9.  Speech Recognizer Resource
- contents:
  - "10.  Recorder Resource\n   This resource captures received audio and video and
    stores it as\n   content pointed to by a URI.  The main usages of recorders are\n
    \  1.  to capture speech audio that may be submitted for recognition at\n       a
    later time, and\n   2.  recording voice or video mails.\n   Both these applications
    require functionality above and beyond those\n   specified by protocols such as
    RTSP [RFC2326].  This includes audio\n   endpointing (i.e., detecting speech or
    silence).  The support for\n   video is OPTIONAL and is mainly capturing video
    mails that may\n   require the speech or audio processing mentioned above.\n   A
    recorder MUST provide endpointing capabilities for suppressing\n   silence at
    the beginning and end of a recording, and it MAY also\n   suppress silence in
    the middle of a recording.  If such suppression\n   is done, the recorder MUST
    maintain timing metadata to indicate the\n   actual time stamps of the recorded
    media.\n   See the discussion on the sensitivity of saved waveforms in\n   Section
    12.\n"
  - contents:
    - "10.1.  Recorder State Machine\n   Idle                   Recording\n   State
      \                 State\n    |                       |\n    |---------RECORD------->|\n
      \   |                       |\n    |<------STOP------------|\n    |                       |\n
      \   |<--RECORD-COMPLETE-----|\n    |                       |\n    |              |--------|\n
      \   |       START-OF-INPUT  |\n    |              |------->|\n    |                       |\n
      \   |              |--------|\n    |    START-INPUT-TIMERS |\n    |              |------->|\n
      \   |                       |\n                          Recorder State Machine\n"
    title: 10.1.  Recorder State Machine
  - contents:
    - "10.2.  Recorder Methods\n   The recorder resource supports the following methods.\n
      \  recorder-method      =  \"RECORD\"\n                        /  \"STOP\"\n
      \                       /  \"START-INPUT-TIMERS\"\n"
    title: 10.2.  Recorder Methods
  - contents:
    - "10.3.  Recorder Events\n   The recorder resource can generate the following
      events.\n   recorder-event       =  \"START-OF-INPUT\"\n                        /
      \ \"RECORD-COMPLETE\"\n"
    title: 10.3.  Recorder Events
  - contents:
    - "10.4.  Recorder Header Fields\n   Method invocations for the recorder resource
      can contain resource-\n   specific header fields containing request options
      and information to\n   augment the Method, Response, or Event message it is
      associated with.\n   recorder-header      =  sensitivity-level\n                        /
      \ no-input-timeout\n                        /  completion-cause\n                        /
      \ completion-reason\n                        /  failed-uri\n                        /
      \ failed-uri-cause\n                        /  record-uri\n                        /
      \ media-type\n                        /  max-time\n                        /
      \ trim-length\n                        /  final-silence\n                        /
      \ capture-on-speech\n                        /  ver-buffer-utterance\n                        /
      \ start-input-timers\n                        /  new-audio-channel\n"
    - contents:
      - "10.4.1.  Sensitivity-Level\n   To filter out background noise and not mistake
        it for speech, the\n   recorder can support a variable level of sound sensitivity.
        \ The\n   Sensitivity-Level header field is a float value between 0.0 and
        1.0\n   and allows the client to set the sensitivity level for the recorder.\n
        \  This header field MAY occur in RECORD, SET-PARAMS, or GET-PARAMS.  A\n
        \  higher value for this header field means higher sensitivity.  The\n   default
        value for this header field is implementation specific.\n   sensitivity-level
        \   =     \"Sensitivity-Level\" \":\" FLOAT CRLF\n"
      title: 10.4.1.  Sensitivity-Level
    - contents:
      - "10.4.2.  No-Input-Timeout\n   When recording is started and there is no speech
        detected for a\n   certain period of time, the recorder can send a RECORD-COMPLETE
        event\n   to the client and terminate the record operation.  The No-Input-\n
        \  Timeout header field can set this timeout value.  The value is in\n   milliseconds.
        \ This header field MAY occur in RECORD, SET-PARAMS, or\n   GET-PARAMS.  The
        value for this header field ranges from 0 to an\n   implementation-specific
        maximum value.  The default value for this\n   header field is implementation
        specific.\n   no-input-timeout    =     \"No-Input-Timeout\" \":\" 1*19DIGIT
        CRLF\n"
      title: 10.4.2.  No-Input-Timeout
    - contents:
      - "10.4.3.  Completion-Cause\n   This header field MUST be part of a RECORD-COMPLETE
        event from the\n   recorder resource to the client.  This indicates the reason
        behind\n   the RECORD method completion.  This header field MUST be sent in
        the\n   RECORD responses if they return with a failure status and a COMPLETE\n
        \  state.  In the ABNF below, the 'cause-code' contains a numerical\n   value
        selected from the Cause-Code column of the following table.\n   The 'cause-name'
        contains the corresponding token selected from the\n   Cause-Name column.\n
        \  completion-cause         =  \"Completion-Cause\" \":\" cause-code SP\n
        \                              cause-name CRLF\n   cause-code               =
        \ 3DIGIT\n   cause-name               =  *VCHAR\n   +------------+-----------------------+------------------------------+\n
        \  | Cause-Code | Cause-Name            | Description                  |\n
        \  +------------+-----------------------+------------------------------+\n
        \  | 000        | success-silence       | RECORD completed with a      |\n
        \  |            |                       | silence at the end.          |\n
        \  | 001        | success-maxtime       | RECORD completed after       |\n
        \  |            |                       | reaching maximum recording   |\n
        \  |            |                       | time specified in record     |\n
        \  |            |                       | method.                      |\n
        \  | 002        | no-input-timeout      | RECORD failed due to no      |\n
        \  |            |                       | input.                       |\n
        \  | 003        | uri-failure           | Failure accessing the record |\n
        \  |            |                       | URI.                         |\n
        \  | 004        | error                 | RECORD request terminated    |\n
        \  |            |                       | prematurely due to a         |\n
        \  |            |                       | recorder error.              |\n
        \  +------------+-----------------------+------------------------------+\n"
      title: 10.4.3.  Completion-Cause
    - contents:
      - "10.4.4.  Completion-Reason\n   This header field MAY be present in a RECORD-COMPLETE
        event coming\n   from the recorder resource to the client.  It contains the
        reason\n   text behind the RECORD request completion.  This header field\n
        \  communicates text describing the reason for the failure.\n   The completion
        reason text is provided for client use in logs and for\n   debugging and instrumentation
        purposes.  Clients MUST NOT interpret\n   the completion reason text.\n   completion-reason
        \       =  \"Completion-Reason\" \":\"\n                               quoted-string
        CRLF\n"
      title: 10.4.4.  Completion-Reason
    - contents:
      - "10.4.5.  Failed-URI\n   When a recorder method needs to post the audio to
        a URI and access to\n   the URI fails, the server MUST provide the failed
        URI in this header\n   field in the method response.\n   failed-uri               =
        \ \"Failed-URI\" \":\" absoluteURI CRLF\n"
      title: 10.4.5.  Failed-URI
    - contents:
      - "10.4.6.  Failed-URI-Cause\n   When a recorder method needs to post the audio
        to a URI and access to\n   the URI fails, the server MAY provide the URI-specific
        or protocol-\n   specific response code through this header field in the method\n
        \  response.  The value encoding is UTF-8 (RFC 3629 [RFC3629]) to\n   accommodate
        any access protocol -- some access protocols might have a\n   response string
        instead of a numeric response code.\n   failed-uri-cause         =  \"Failed-URI-Cause\"
        \":\" 1*UTFCHAR\n                               CRLF\n"
      title: 10.4.6.  Failed-URI-Cause
    - contents:
      - "10.4.7.  Record-URI\n   When a recorder method contains this header field,
        the server MUST\n   capture the audio and store it.  If the header field is
        present but\n   specified with no value, the server MUST store the content
        locally\n   and generate a URI that points to it.  This URI is then returned
        in\n   either the STOP response or the RECORD-COMPLETE event.  If the header\n
        \  field in the RECORD method specifies a URI, the server MUST attempt\n   to
        capture and store the audio at that location.  If this header\n   field is
        not specified in the RECORD request, the server MUST capture\n   the audio,
        MUST encode it, and MUST send it in the STOP response or\n   the RECORD-COMPLETE
        event as a message body.  In this case, the\n   response carrying the audio
        content MUST include a Content ID (cid)\n   [RFC2392] value in this header
        pointing to the Content-ID in the\n   message body.\n   The server MUST also
        return the size in octets and the duration in\n   milliseconds of the recorded
        audio waveform as parameters associated\n   with the header field.\n   Implementations
        MUST support 'http' [RFC2616], 'https' [RFC2818],\n   'file' [RFC3986], and
        'cid' [RFC2392] schemes in the URI.  Note that\n   implementations already
        exist that support other schemes.\n   record-uri               =  \"Record-URI\"
        \":\" [\"<\" uri \">\"\n                               \";\" \"size\" \"=\"
        1*19DIGIT\n                               \";\" \"duration\" \"=\" 1*19DIGIT]
        CRLF\n"
      title: 10.4.7.  Record-URI
    - contents:
      - "10.4.8.  Media-Type\n   A RECORD method MUST contain this header field, which
        specifies to\n   the server the media type of the captured audio or video.\n
        \  media-type               =  \"Media-Type\" \":\" media-type-value\n                               CRLF\n"
      title: 10.4.8.  Media-Type
    - contents:
      - "10.4.9.  Max-Time\n   When recording is started, this specifies the maximum
        length of the\n   recording in milliseconds, calculated from the time the
        actual\n   capture and store begins and is not necessarily the time the RECORD\n
        \  method is received.  It specifies the duration before silence\n   suppression,
        if any, has been applied by the recorder resource.\n   After this time, the
        recording stops and the server MUST return a\n   RECORD-COMPLETE event to
        the client having a request-state of\n   COMPLETE.  This header field MAY
        occur in RECORD, SET-PARAMS, or GET-\n   PARAMS.  The value for this header
        field ranges from 0 to an\n   implementation-specific maximum value.  A value
        of 0 means infinity,\n   and hence the recording continues until one or more
        of the other stop\n   conditions are met.  The default value for this header
        field is 0.\n   max-time                 =  \"Max-Time\" \":\" 1*19DIGIT CRLF\n"
      title: 10.4.9.  Max-Time
    - contents:
      - "10.4.10.  Trim-Length\n   This header field MAY be sent on a STOP method
        and specifies the\n   length of audio to be trimmed from the end of the recording
        after the\n   stop.  The length is interpreted to be in milliseconds.  The
        default\n   value for this header field is 0.\n   trim-length                 =
        \ \"Trim-Length\" \":\" 1*19DIGIT CRLF\n"
      title: 10.4.10.  Trim-Length
    - contents:
      - "10.4.11.  Final-Silence\n   When the recorder is started and the actual capture
        begins, this\n   header field specifies the length of silence in the audio
        that is to\n   be interpreted as the end of the recording.  This header field
        MAY\n   occur in RECORD, SET-PARAMS, or GET-PARAMS.  The value for this\n
        \  header field ranges from 0 to an implementation-specific maximum\n   value
        and is interpreted to be in milliseconds.  A value of 0 means\n   infinity,
        and hence the recording will continue until one of the\n   other stop conditions
        are met.  The default value for this header\n   field is implementation specific.\n
        \  final-silence            =  \"Final-Silence\" \":\" 1*19DIGIT CRLF\n"
      title: 10.4.11.  Final-Silence
    - contents:
      - "10.4.12.  Capture-On-Speech\n   If \"false\", the recorder MUST start capturing
        immediately when\n   started.  If \"true\", the recorder MUST wait for the
        endpointing\n   functionality to detect speech before it starts capturing.
        \ This\n   header field MAY occur in the RECORD, SET-PARAMS, or GET-PARAMS.
        \ The\n   value for this header field is a Boolean.  The default value for
        this\n   header field is \"false\".\n   capture-on-speech        =  \"Capture-On-Speech
        \" \":\" BOOLEAN CRLF\n"
      title: 10.4.12.  Capture-On-Speech
    - contents:
      - "10.4.13.  Ver-Buffer-Utterance\n   This header field is the same as the one
        described for the verifier\n   resource (see Section 11.4.14).  This tells
        the server to buffer the\n   utterance associated with this recording request
        into the\n   verification buffer.  Sending this header field is permitted
        only if\n   the verification buffer is for the session.  This buffer is shared\n
        \  across resources within a session.  It gets instantiated when a\n   verifier
        resource is added to this session and is released when the\n   verifier resource
        is released from the session.\n"
      title: 10.4.13.  Ver-Buffer-Utterance
    - contents:
      - "10.4.14.  Start-Input-Timers\n   This header field MAY be sent as part of
        the RECORD request.  A value\n   of \"false\" tells the recorder resource
        to start the operation, but\n   not to start the no-input timer until the
        client sends a START-INPUT-\n   TIMERS request to the recorder resource.  This
        is useful in the\n   scenario when the recorder and synthesizer resources
        are not part of\n   the same session.  When a kill-on-barge-in prompt is being
        played,\n   the client may want the RECORD request to be simultaneously active
        so\n   that it can detect and implement kill-on-barge-in (see\n   Section
        8.4.2).  But at the same time, the client doesn't want the\n   recorder resource
        to start the no-input timers until the prompt is\n   finished.  The default
        value is \"true\".\n   start-input-timers       =  \"Start-Input-Timers\"
        \":\"\n                               BOOLEAN CRLF\n"
      title: 10.4.14.  Start-Input-Timers
    - contents:
      - "10.4.15.  New-Audio-Channel\n   This header field is the same as the one
        described for the recognizer\n   resource (see Section 9.4.23).\n"
      title: 10.4.15.  New-Audio-Channel
    title: 10.4.  Recorder Header Fields
  - contents:
    - "10.5.  Recorder Message Body\n   If the RECORD request did not have a Record-URI
      header field, the\n   STOP response or the RECORD-COMPLETE event MUST contain
      a message\n   body carrying the captured audio.  In this case, the message carrying\n
      \  the audio content has a Record-URI header field with a Content ID\n   value
      pointing to the message body entity that contains the recorded\n   audio.  See
      Section 10.4.7 for details.\n"
    title: 10.5.  Recorder Message Body
  - contents:
    - "10.6.  RECORD\n   The RECORD request places the recorder resource in the recording\n
      \  state.  Depending on the header fields specified in the RECORD\n   method,
      the resource may start recording the audio immediately or\n   wait for the endpointing
      functionality to detect speech in the audio.\n   The audio is then made available
      to the client either in the message\n   body or as specified by Record-URI.\n
      \  The server MUST support the 'https' URI scheme and MAY support other\n   schemes.
      \ Note that, due to the sensitive nature of voice recordings,\n   any protocols
      used for dereferencing SHOULD employ integrity and\n   confidentiality, unless
      other means, such as use of a controlled\n   environment (see Section 4.2),
      are employed.\n   If a RECORD operation is already in progress, invoking this
      method\n   causes the server to issue a response having a status-code of 402\n
      \  \"Method not valid in this state\" and a request-state of COMPLETE.\n   If
      the Record-URI is not valid, a status-code of 404 \"Illegal Value\n   for Header
      Field\" is returned in the response.  If it is impossible\n   for the server
      to create the requested stored content, a status-code\n   of 407 \"Method or
      Operation Failed\" is returned.\n   If the type specified in the Media-Type
      header field is not\n   supported, the server MUST respond with a status-code
      of 409\n   \"Unsupported Header Field Value\" with the Media-Type header field
      in\n   its response.\n   When the recording operation is initiated, the response
      indicates an\n   IN-PROGRESS request state.  The server MAY generate a subsequent\n
      \  START-OF-INPUT event when speech is detected.  Upon completion of the\n   recording
      operation, the server generates a RECORD-COMPLETE event.\n   C->S:  MRCP/2.0
      ... RECORD 543257\n          Channel-Identifier:32AECB23433802@recorder\n          Record-URI:<file://mediaserver/recordings/myfile.wav>\n
      \         Media-Type:audio/wav\n          Capture-On-Speech:true\n          Final-Silence:300\n
      \         Max-Time:6000\n   S->C:  MRCP/2.0 ... 543257 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433802@recorder\n
      \  S->C:  MRCP/2.0 ... START-OF-INPUT 543257 IN-PROGRESS\n          Channel-Identifier:32AECB23433802@recorder\n
      \  S->C:  MRCP/2.0 ... RECORD-COMPLETE 543257 COMPLETE\n          Channel-Identifier:32AECB23433802@recorder\n
      \         Completion-Cause:000 success-silence\n          Record-URI:<file://mediaserver/recordings/myfile.wav>;\n
      \                    size=242552;duration=25645\n                              RECORD
      Example\n"
    title: 10.6.  RECORD
  - contents:
    - "10.7.  STOP\n   The STOP method moves the recorder from the recording state
      back to\n   the idle state.  If a RECORD request is active and the STOP request\n
      \  successfully terminates it, then the STOP response MUST contain an\n   Active-Request-Id-List
      header field containing the RECORD request-id\n   that was terminated.  In this
      case, no RECORD-COMPLETE event is sent\n   for the terminated request.  If there
      was no recording active, then\n   the response MUST NOT contain an Active-Request-Id-List
      header field.\n   If the recording was a success, the STOP response MUST contain
      a\n   Record-URI header field pointing to the recorded audio content or to\n
      \  a typed entity in the body of the STOP response containing the\n   recorded
      audio.  The STOP method MAY have a Trim-Length header field,\n   in which case
      the specified length of audio is trimmed from the end\n   of the recording after
      the stop.  In any case, the response MUST\n   contain a status-code of 200 \"Success\".\n
      \  C->S:  MRCP/2.0 ... RECORD 543257\n          Channel-Identifier:32AECB23433802@recorder\n
      \         Record-URI:<file://mediaserver/recordings/myfile.wav>\n          Capture-On-Speech:true\n
      \         Final-Silence:300\n          Max-Time:6000\n   S->C:  MRCP/2.0 ...
      543257 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433802@recorder\n
      \  S->C:  MRCP/2.0 ... START-OF-INPUT 543257 IN-PROGRESS\n          Channel-Identifier:32AECB23433802@recorder\n
      \  C->S:  MRCP/2.0 ... STOP 543257\n          Channel-Identifier:32AECB23433802@recorder\n
      \         Trim-Length:200\n   S->C:  MRCP/2.0 ... 543257 200 COMPLETE\n          Channel-Identifier:32AECB23433802@recorder\n
      \         Record-URI:<file://mediaserver/recordings/myfile.wav>;\n                     size=324253;duration=24561\n
      \         Active-Request-Id-List:543257\n                               STOP
      Example\n"
    title: 10.7.  STOP
  - contents:
    - "10.8.  RECORD-COMPLETE\n   If the recording completes due to no input, silence
      after speech, or\n   reaching the max-time, the server MUST generate the RECORD-COMPLETE\n
      \  event to the client with a request-state of COMPLETE.  If the\n   recording
      was a success, the RECORD-COMPLETE event contains a Record-\n   URI header field
      pointing to the recorded audio file on the server or\n   to a typed entity in
      the message body containing the recorded audio.\n   C->S:  MRCP/2.0 ... RECORD
      543257\n          Channel-Identifier:32AECB23433802@recorder\n          Record-URI:<file://mediaserver/recordings/myfile.wav>\n
      \         Capture-On-Speech:true\n          Final-Silence:300\n          Max-Time:6000\n
      \  S->C:  MRCP/2.0 ... 543257 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433802@recorder\n
      \  S->C:  MRCP/2.0 ... START-OF-INPUT 543257 IN-PROGRESS\n          Channel-Identifier:32AECB23433802@recorder\n
      \  S->C:  MRCP/2.0 ... RECORD-COMPLETE 543257 COMPLETE\n          Channel-Identifier:32AECB23433802@recorder\n
      \         Completion-Cause:000 success\n          Record-URI:<file://mediaserver/recordings/myfile.wav>;\n
      \                    size=325325;duration=24652\n                          RECORD-COMPLETE
      Example\n"
    title: 10.8.  RECORD-COMPLETE
  - contents:
    - "10.9.  START-INPUT-TIMERS\n   This request is sent from the client to the recorder
      resource when it\n   discovers that a kill-on-barge-in prompt has finished playing
      (see\n   Section 8.4.2).  This is useful in the scenario when the recorder and\n
      \  synthesizer resources are not in the same MRCPv2 session.  When a\n   kill-on-barge-in
      prompt is being played, the client wants the RECORD\n   request to be simultaneously
      active so that it can detect and\n   implement kill-on-barge-in.  But at the
      same time, the client doesn't\n   want the recorder resource to start the no-input
      timers until the\n   prompt is finished.  The Start-Input-Timers header field
      in the\n   RECORD request allows the client to say if the timers should be\n
      \  started or not.  In the above case, the recorder resource does not\n   start
      the timers until the client sends a START-INPUT-TIMERS method\n   to the recorder.\n"
    title: 10.9.  START-INPUT-TIMERS
  - contents:
    - "10.10.  START-OF-INPUT\n   The START-OF-INPUT event is returned from the server
      to the client\n   once the server has detected speech.  This event is always
      returned\n   by the recorder resource when speech has been detected.  The recorder\n
      \  resource also MUST send a Proxy-Sync-Id header field with a unique\n   value
      for this event.\n   S->C:  MRCP/2.0 ... START-OF-INPUT 543259 IN-PROGRESS\n
      \         Channel-Identifier:32AECB23433801@recorder\n          Proxy-Sync-Id:987654321\n"
    title: 10.10.  START-OF-INPUT
  title: 10.  Recorder Resource
- contents:
  - "11.  Speaker Verification and Identification\n   This section describes the methods,
    responses and events employed by\n   MRCPv2 for doing speaker verification/identification.\n
    \  Speaker verification is a voice authentication methodology that can\n   be
    used to identify the speaker in order to grant the user access to\n   sensitive
    information and transactions.  Because speech is a\n   biometric, a number of
    essential security considerations related to\n   biometric authentication technologies
    apply to its implementation and\n   usage.  Implementers should carefully read
    Section 12 in this\n   document and the corresponding section of the SPEECHSC
    requirements\n   [RFC4313].  Implementers and deployers of this technology are\n
    \  strongly encouraged to check the state of the art for any new risks\n   and
    solutions that might have been developed.\n   In speaker verification, a recorded
    utterance is compared to a\n   previously stored voiceprint, which is in turn
    associated with a\n   claimed identity for that user.  Verification typically
    consists of\n   two phases: a designation phase to establish the claimed identity
    of\n   the caller and an execution phase in which a voiceprint is either\n   created
    (training) or used to authenticate the claimed identity\n   (verification).\n
    \  Speaker identification is the process of associating an unknown\n   speaker
    with a member in a population.  It does not employ a claim of\n   identity.  When
    an individual claims to belong to a group (e.g., one\n   of the owners of a joint
    bank account) a group authentication is\n   performed.  This is generally implemented
    as a kind of verification\n   involving comparison with more than one voice model.
    \ It is sometimes\n   called 'multi-verification'.  If the individual speaker
    can be\n   identified from the group, this may be useful for applications where\n
    \  multiple users share the same access privileges to some data or\n   application.
    \ Speaker identification and group authentication are\n   also done in two phases,
    a designation phase and an execution phase.\n   Note that, from a functionality
    standpoint, identification can be\n   thought of as a special case of group authentication
    (if the\n   individual is identified) where the group is the entire population,\n
    \  although the implementation of speaker identification may be\n   different
    from the way group authentication is performed.  To\n   accommodate single-voiceprint
    verification, verification against\n   multiple voiceprints, group authentication,
    and identification, this\n   specification provides a single set of methods that
    can take a list\n   of identifiers, called \"voiceprint identifiers\", and return
    a list of\n   identifiers, with a score for each that represents how well the
    input\n   speech matched each identifier.  The input and output lists of\n   identifiers
    do not have to match, allowing a vendor-specific group\n   identifier to be used
    as input to indicate that identification is to\n   be performed.  In this specification,
    the terms \"identification\" and\n   \"multi-verification\" are used to indicate
    that the input represents a\n   group (potentially the entire population) and
    that results for\n   multiple voiceprints may be returned.\n   It is possible
    for a verifier resource to share the same session with\n   a recognizer resource
    or to operate independently.  In order to share\n   the same session, the verifier
    and recognizer resources MUST be\n   allocated from within the same SIP dialog.
    \ Otherwise, an independent\n   verifier resource, running on the same physical
    server or a separate\n   one, will be set up.  Note that, in addition to allowing
    both\n   resources to be allocated in the same INVITE, it is possible to\n   allocate
    one initially and the other later via a re-INVITE.\n   Some of the speaker verification
    methods, described below, apply only\n   to a specific mode of operation.\n   The
    verifier resource has a verification buffer associated with it\n   (see Section
    11.4.14).  This allows the storage of speech utterances\n   for the purposes of
    verification, identification, or training from\n   the buffered speech.  This
    buffer is owned by the verifier resource,\n   but other input resources (such
    as the recognizer resource or\n   recorder resource) may write to it.  This allows
    the speech received\n   as part of a recognition or recording operation to be
    later used for\n   verification, identification, or training.  Access to the buffer
    is\n   limited to one operation at time.  Hence, when the resource is doing\n
    \  read, write, or delete operations, such as a RECOGNIZE with\n   ver-buffer-utterance
    turned on, another operation involving the\n   buffer fails with a status-code
    of 402.  The verification buffer can\n   be cleared by a CLEAR-BUFFER request
    from the client and is freed\n   when the verifier resource is deallocated or
    the session with the\n   server terminates.\n   The verification buffer is different
    from collecting waveforms and\n   processing them using either the real-time audio
    stream or stored\n   audio, because this buffering mechanism does not simply accumulate\n
    \  speech to a buffer.  The verification buffer MAY contain additional\n   information
    gathered by the recognizer resource that serves to\n   improve verification performance.\n"
  - contents:
    - "11.1.  Speaker Verification State Machine\n   Speaker verification may operate
      in a training or a verification\n   session.  Starting one of these sessions
      does not change the state of\n   the verifier resource, i.e., it remains idle.
      \ Once a verification or\n   training session is started, then utterances are
      trained or verified\n   by calling the VERIFY or VERIFY-FROM-BUFFER method.
      \ The state of the\n   verifier resources goes from IDLE to VERIFYING state
      each time VERIFY\n   or VERIFY-FROM-BUFFER is called.\n     Idle              Session
      Opened       Verifying/Training\n     State             State                State\n
      \     |                   |                         |\n      |--START-SESSION--->|
      \                        |\n      |                   |                         |\n
      \     |                   |----------|              |\n      |                   |
      \    START-SESSION       |\n      |                   |<---------|              |\n
      \     |                   |                         |\n      |<--END-SESSION-----|
      \                        |\n      |                   |                         |\n
      \     |                   |---------VERIFY--------->|\n      |                   |
      \                        |\n      |                   |---VERIFY-FROM-BUFFER--->|\n
      \     |                   |                         |\n      |                   |----------|
      \             |\n      |                   |  VERIFY-ROLLBACK        |\n      |
      \                  |<---------|              |\n      |                   |
      \                        |\n      |                   |                |--------|\n
      \     |                   | GET-INTERMEDIATE-RESULT |\n      |                   |
      \               |------->|\n      |                   |                         |\n
      \     |                   |                |--------|\n      |                   |
      \    START-INPUT-TIMERS  |\n      |                   |                |------->|\n
      \     |                   |                         |\n      |                   |
      \               |--------|\n      |                   |         START-OF-INPUT
      \ |\n      |                   |                |------->|\n      |                   |
      \                        |\n      |                   |<-VERIFICATION-COMPLETE--|\n
      \     |                   |                         |\n      |                   |<--------STOP------------|\n
      \     |                   |                         |\n      |                   |----------|
      \             |\n      |                   |         STOP            |\n      |
      \                  |<---------|              |\n      |                   |
      \                        |\n      |----------|        |                         |\n
      \     |         STOP      |                         |\n      |<---------|        |
      \                        |\n      |                   |----------|              |\n
      \     |                   |    CLEAR-BUFFER         |\n      |                   |<---------|
      \             |\n      |                   |                         |\n      |----------|
      \       |                         |\n      |   CLEAR-BUFFER    |                         |\n
      \     |<---------|        |                         |\n      |                   |
      \                        |\n      |                   |----------|              |\n
      \     |                   |   QUERY-VOICEPRINT      |\n      |                   |<---------|
      \             |\n      |                   |                         |\n      |----------|
      \       |                         |\n      | QUERY-VOICEPRINT  |                         |\n
      \     |<---------|        |                         |\n      |                   |
      \                        |\n      |                   |----------|              |\n
      \     |                   |  DELETE-VOICEPRINT      |\n      |                   |<---------|
      \             |\n      |                   |                         |\n      |----------|
      \       |                         |\n      | DELETE-VOICEPRINT |                         |\n
      \     |<---------|        |                         |\n                      Verifier
      Resource State Machine\n"
    title: 11.1.  Speaker Verification State Machine
  - contents:
    - "11.2.  Speaker Verification Methods\n   The verifier resource supports the
      following methods.\n   verifier-method          =  \"START-SESSION\"\n                            /
      \"END-SESSION\"\n                            / \"QUERY-VOICEPRINT\"\n                            /
      \"DELETE-VOICEPRINT\"\n                            / \"VERIFY\"\n                            /
      \"VERIFY-FROM-BUFFER\"\n                            / \"VERIFY-ROLLBACK\"\n
      \                           / \"STOP\"\n                            / \"CLEAR-BUFFER\"\n
      \                           / \"START-INPUT-TIMERS\"\n                            /
      \"GET-INTERMEDIATE-RESULT\"\n   These methods allow the client to control the
      mode and target of\n   verification or identification operations within the
      context of a\n   session.  All the verification input operations that occur
      within a\n   session can be used to create, update, or validate against the\n
      \  voiceprint specified during the session.  At the beginning of each\n   session,
      the verifier resource is reset to the state it had prior to\n   any previous
      verification session.\n   Verification/identification operations can be executed
      against live\n   or buffered audio.  The verifier resource provides methods
      for\n   collecting and evaluating live audio data, and methods for\n   controlling
      the verifier resource and adjusting its configured\n   behavior.\n   There are
      no dedicated methods for collecting buffered audio data.\n   This is accomplished
      by calling VERIFY, RECOGNIZE, or RECORD as\n   appropriate for the resource,
      with the header field\n   Ver-Buffer-Utterance.  Then, when the following method
      is called,\n   verification is performed using the set of buffered audio.\n
      \  1.  VERIFY-FROM-BUFFER\n   The following methods are used for verification
      of live audio\n   utterances:\n   1.  VERIFY\n   2.  START-INPUT-TIMERS\n   The
      following methods are used for configuring the verifier resource\n   and for
      establishing resource states:\n   1.  START-SESSION\n   2.  END-SESSION\n   3.
      \ QUERY-VOICEPRINT\n   4.  DELETE-VOICEPRINT\n   5.  VERIFY-ROLLBACK\n   6.
      \ STOP\n   7.  CLEAR-BUFFER\n   The following method allows the polling of a
      verification in progress\n   for intermediate results.\n   1.  GET-INTERMEDIATE-RESULT\n"
    title: 11.2.  Speaker Verification Methods
  - contents:
    - "11.3.  Verification Events\n   The verifier resource generates the following
      events.\n   verifier-event       =  \"VERIFICATION-COMPLETE\"\n                        /
      \ \"START-OF-INPUT\"\n"
    title: 11.3.  Verification Events
  - contents:
    - "11.4.  Verification Header Fields\n   A verifier resource message can contain
      header fields containing\n   request options and information to augment the
      Request, Response, or\n   Event message it is associated with.\n   verification-header
      \     =  repository-uri\n                            /  voiceprint-identifier\n
      \                           /  verification-mode\n                            /
      \ adapt-model\n                            /  abort-model\n                            /
      \ min-verification-score\n                            /  num-min-verification-phrases\n
      \                           /  num-max-verification-phrases\n                            /
      \ no-input-timeout\n                            /  save-waveform\n                            /
      \ media-type\n                            /  waveform-uri\n                            /
      \ voiceprint-exists\n                            /  ver-buffer-utterance\n                            /
      \ input-waveform-uri\n                            /  completion-cause\n                            /
      \ completion-reason\n                            /  speech-complete-timeout\n
      \                           /  new-audio-channel\n                            /
      \ abort-verification\n                            /  start-input-timers\n"
    - contents:
      - "11.4.1.  Repository-URI\n   This header field specifies the voiceprint repository
        to be used or\n   referenced during speaker verification or identification
        operations.\n   This header field is required in the START-SESSION, QUERY-VOICEPRINT,\n
        \  and DELETE-VOICEPRINT methods.\n   repository-uri           =  \"Repository-URI\"
        \":\" uri CRLF\n"
      title: 11.4.1.  Repository-URI
    - contents:
      - "11.4.2.  Voiceprint-Identifier\n   This header field specifies the claimed
        identity for verification\n   applications.  The claimed identity MAY be used
        to specify an\n   existing voiceprint or to establish a new voiceprint.  This
        header\n   field MUST be present in the QUERY-VOICEPRINT and DELETE-VOICEPRINT\n
        \  methods.  The Voiceprint-Identifier MUST be present in the START-\n   SESSION
        method for verification operations.  For identification or\n   multi-verification
        operations, this header field MAY contain a list\n   of voiceprint identifiers
        separated by semicolons.  For\n   identification operations, the client MAY
        also specify a voiceprint\n   group identifier instead of a list of voiceprint
        identifiers.\n   voiceprint-identifier        =  \"Voiceprint-Identifier\"
        \":\"\n                                   vid *[\";\" vid] CRLF\n   vid                          =
        \ 1*VCHAR [\".\" 1*VCHAR]\n"
      title: 11.4.2.  Voiceprint-Identifier
    - contents:
      - "11.4.3.  Verification-Mode\n   This header field specifies the mode of the
        verifier resource and is\n   set by the START-SESSION method.  Acceptable
        values indicate whether\n   the verification session will train a voiceprint
        (\"train\") or verify/\n   identify using an existing voiceprint (\"verify\").\n
        \  Training and verification sessions both require the voiceprint\n   Repository-URI
        to be specified in the START-SESSION.  In many usage\n   scenarios, however,
        the system does not know the speaker's claimed\n   identity until a recognition
        operation has, for example, recognized\n   an account number to which the
        user desires access.  In order to\n   allow the first few utterances of a
        dialog to be both recognized and\n   verified, the verifier resource on the
        MRCPv2 server retains a\n   buffer.  In this buffer, the MRCPv2 server accumulates
        recognized\n   utterances.  The client can later execute a verification method
        and\n   apply the buffered utterances to the current verification session.\n
        \  Some voice user interfaces may require additional user input that\n   should
        not be subject to verification.  For example, the user's input\n   may have
        been recognized with low confidence and thus require a\n   confirmation cycle.
        \ In such cases, the client SHOULD NOT execute the\n   VERIFY or VERIFY-FROM-BUFFER
        methods to collect and analyze the\n   caller's input.  A separate recognizer
        resource can analyze the\n   caller's response without any participation by
        the verifier resource.\n   Once the following conditions have been met:\n
        \  1.  the voiceprint identity has been successfully established through\n
        \      the Voiceprint-Identifier header fields of the START-SESSION\n       method,
        and\n   2.  the verification mode has been set to one of \"train\" or \"verify\",\n
        \  the verifier resource can begin providing verification information\n   during
        verification operations.  If the verifier resource does not\n   reach one
        of the two major states (\"train\" or \"verify\") , it MUST\n   report an
        error condition in the MRCPv2 status code to indicate why\n   the verifier
        resource is not ready for the corresponding usage.\n   The value of verification-mode
        is persistent within a verification\n   session.  If the client attempts to
        change the mode during a\n   verification session, the verifier resource reports
        an error and the\n   mode retains its current value.\n   verification-mode
        \           =  \"Verification-Mode\" \":\"\n                                   verification-mode-string\n
        \  verification-mode-string     =  \"train\"\n                                /
        \ \"verify\"\n"
      title: 11.4.3.  Verification-Mode
    - contents:
      - "11.4.4.  Adapt-Model\n   This header field indicates the desired behavior
        of the verifier\n   resource after a successful verification operation.  If
        the value of\n   this header field is \"true\", the server SHOULD use audio
        collected\n   during the verification session to update the voiceprint to
        account\n   for ongoing changes in a speaker's incoming speech characteristics,\n
        \  unless local policy prohibits updating the voiceprint.  If the value\n
        \  is \"false\" (the default), the server MUST NOT update the voiceprint.\n
        \  This header field MAY occur in the START-SESSION method.\n   adapt-model
        \             = \"Adapt-Model\" \":\" BOOLEAN CRLF\n"
      title: 11.4.4.  Adapt-Model
    - contents:
      - "11.4.5.  Abort-Model\n   The Abort-Model header field indicates the desired
        behavior of the\n   verifier resource upon session termination.  If the value
        of this\n   header field is \"true\", the server MUST discard any pending
        changes\n   to a voiceprint due to verification training or verification\n
        \  adaptation.  If the value is \"false\" (the default), the server MUST\n
        \  commit any pending changes for a training session or a successful\n   verification
        session to the voiceprint repository.  A value of \"true\"\n   for Abort-Model
        overrides a value of \"true\" for the Adapt-Model\n   header field.  This
        header field MAY occur in the END-SESSION method.\n   abort-model             =
        \"Abort-Model\" \":\" BOOLEAN CRLF\n"
      title: 11.4.5.  Abort-Model
    - contents:
      - "11.4.6.  Min-Verification-Score\n   The Min-Verification-Score header field,
        when used with a verifier\n   resource through a SET-PARAMS, GET-PARAMS, or
        START-SESSION method,\n   determines the minimum verification score for which
        a verification\n   decision of \"accepted\" may be declared by the server.
        \ This is a\n   float value between -1.0 and 1.0.  The default value for this
        header\n   field is implementation specific.\n   min-verification-score  =
        \"Min-Verification-Score\" \":\"\n                             [ %x2D ] FLOAT
        CRLF\n"
      title: 11.4.6.  Min-Verification-Score
    - contents:
      - "11.4.7.  Num-Min-Verification-Phrases\n   The Num-Min-Verification-Phrases
        header field is used to specify the\n   minimum number of valid utterances
        before a positive decision is\n   given for verification.  The value for this
        header field is an\n   integer and the default value is 1.  The verifier resource
        MUST NOT\n   declare a verification 'accepted' unless Num-Min-Verification-Phrases\n
        \  valid utterances have been received.  The minimum value is 1.  This\n   header
        field MAY occur in START-SESSION, SET-PARAMS, or GET-PARAMS.\n   num-min-verification-phrases
        =  \"Num-Min-Verification-Phrases\" \":\"\n                                   1*19DIGIT
        CRLF\n"
      title: 11.4.7.  Num-Min-Verification-Phrases
    - contents:
      - "11.4.8.  Num-Max-Verification-Phrases\n   The Num-Max-Verification-Phrases
        header field is used to specify the\n   number of valid utterances required
        before a decision is forced for\n   verification.  The verifier resource MUST
        NOT return a decision of\n   'undecided' once Num-Max-Verification-Phrases
        have been collected and\n   used to determine a verification score.  The value
        for this header\n   field is an integer and the minimum value is 1.  The default
        value is\n   implementation specific.  This header field MAY occur in START-\n
        \  SESSION, SET-PARAMS, or GET-PARAMS.\n   num-max-verification-phrases =
        \ \"Num-Max-Verification-Phrases\" \":\"\n                                    1*19DIGIT
        CRLF\n"
      title: 11.4.8.  Num-Max-Verification-Phrases
    - contents:
      - "11.4.9.  No-Input-Timeout\n   The No-Input-Timeout header field sets the
        length of time from the\n   start of the verification timers (see START-INPUT-TIMERS)
        until the\n   VERIFICATION-COMPLETE server event message declares that no
        input has\n   been received (i.e., has a Completion-Cause of no-input-timeout).\n
        \  The value is in milliseconds.  This header field MAY occur in VERIFY,\n
        \  SET-PARAMS, or GET-PARAMS.  The value for this header field ranges\n   from
        0 to an implementation-specific maximum value.  The default\n   value for
        this header field is implementation specific.\n   no-input-timeout         =
        \"No-Input-Timeout\" \":\" 1*19DIGIT CRLF\n"
      title: 11.4.9.  No-Input-Timeout
    - contents:
      - "11.4.10.  Save-Waveform\n   This header field allows the client to request
        that the verifier\n   resource save the audio stream that was used for verification/\n
        \  identification.  The verifier resource MUST attempt to record the\n   audio
        and make it available to the client in the form of a URI\n   returned in the
        Waveform-URI header field in the VERIFICATION-\n   COMPLETE event.  If there
        was an error in recording the stream, or\n   the audio content is otherwise
        not available, the verifier resource\n   MUST return an empty Waveform-URI
        header field.  The default value\n   for this header field is \"false\".  This
        header field MAY appear in\n   the VERIFY method.  Note that this header field
        does not appear in\n   the VERIFY-FROM-BUFFER method since it only controls
        whether or not\n   to save the waveform for live verification/identification
        operations.\n   save-waveform            =  \"Save-Waveform\" \":\" BOOLEAN
        CRLF\n"
      title: 11.4.10.  Save-Waveform
    - contents:
      - "11.4.11.  Media-Type\n   This header field MAY be specified in the SET-PARAMS,
        GET-PARAMS, or\n   the VERIFY methods and tells the server resource the media
        type of\n   the captured audio or video such as the one captured and returned
        by\n   the Waveform-URI header field.\n   media-type               =  \"Media-Type\"
        \":\" media-type-value\n                               CRLF\n"
      title: 11.4.11.  Media-Type
    - contents:
      - "11.4.12.  Waveform-URI\n   If the Save-Waveform header field is set to \"true\",
        the verifier\n   resource MUST attempt to record the incoming audio stream
        of the\n   verification into a file and provide a URI for the client to access\n
        \  it.  This header field MUST be present in the VERIFICATION-COMPLETE\n   event
        if the Save-Waveform header field was set to true by the\n   client.  The
        value of the header field MUST be empty if there was\n   some error condition
        preventing the server from recording.\n   Otherwise, the URI generated by
        the server MUST be globally unique\n   across the server and all its verification
        sessions.  The content\n   MUST be available via the URI until the verification
        session ends.\n   Since the Save-Waveform header field applies only to live\n
        \  verification/identification operations, the server can return the\n   Waveform-URI
        only in the VERIFICATION-COMPLETE event for live\n   verification/identification
        operations.\n   The server MUST also return the size in octets and the duration
        in\n   milliseconds of the recorded audio waveform as parameters associated\n
        \  with the header field.\n   waveform-uri             =  \"Waveform-URI\"
        \":\" [\"<\" uri \">\"\n                               \";\" \"size\" \"=\"
        1*19DIGIT\n                               \";\" \"duration\" \"=\" 1*19DIGIT]
        CRLF\n"
      title: 11.4.12.  Waveform-URI
    - contents:
      - "11.4.13.  Voiceprint-Exists\n   This header field MUST be returned in QUERY-VOICEPRINT
        and DELETE-\n   VOICEPRINT responses.  This is the status of the voiceprint
        specified\n   in the QUERY-VOICEPRINT method.  For the DELETE-VOICEPRINT method,\n
        \  this header field indicates the status of the voiceprint at the\n   moment
        the method execution started.\n   voiceprint-exists    =  \"Voiceprint-Exists\"
        \":\" BOOLEAN CRLF\n"
      title: 11.4.13.  Voiceprint-Exists
    - contents:
      - "11.4.14.  Ver-Buffer-Utterance\n   This header field is used to indicate
        that this utterance could be\n   later considered for speaker verification.
        \ This way, a client can\n   request the server to buffer utterances while
        doing regular\n   recognition or verification activities, and speaker verification
        can\n   later be requested on the buffered utterances.  This header field
        is\n   optional in the RECOGNIZE, VERIFY, and RECORD methods.  The default\n
        \  value for this header field is \"false\".\n   ver-buffer-utterance     =
        \"Ver-Buffer-Utterance\" \":\" BOOLEAN\n                              CRLF\n"
      title: 11.4.14.  Ver-Buffer-Utterance
    - contents:
      - "11.4.15.  Input-Waveform-URI\n   This header field specifies stored audio
        content that the client\n   requests the server to fetch and process according
        to the current\n   verification mode, either to train the voiceprint or verify
        a claimed\n   identity.  This header field enables the client to implement
        the\n   buffering use case where the recognizer and verifier resources are
        in\n   different sessions and the verification buffer technique cannot be\n
        \  used.  It MAY be specified on the VERIFY request.\n   input-waveform-uri
        \          =  \"Input-Waveform-URI\" \":\" uri CRLF\n"
      title: 11.4.15.  Input-Waveform-URI
    - contents:
      - "11.4.16.  Completion-Cause\n   This header field MUST be part of a VERIFICATION-COMPLETE
        event from\n   the verifier resource to the client.  This indicates the cause
        of\n   VERIFY or VERIFY-FROM-BUFFER method completion.  This header field\n
        \  MUST be sent in the VERIFY, VERIFY-FROM-BUFFER, and QUERY-VOICEPRINT\n
        \  responses, if they return with a failure status and a COMPLETE state.\n
        \  In the ABNF below, the 'cause-code' contains a numerical value\n   selected
        from the Cause-Code column of the following table.  The\n   'cause-name' contains
        the corresponding token selected from the\n   Cause-Name column.\n   completion-cause
        \        =  \"Completion-Cause\" \":\" cause-code SP\n                               cause-name
        CRLF\n   cause-code               =  3DIGIT\n   cause-name               =
        \ *VCHAR\n   +------------+--------------------------+---------------------------+\n
        \  | Cause-Code | Cause-Name               | Description               |\n
        \  +------------+--------------------------+---------------------------+\n
        \  | 000        | success                  | VERIFY or                 |\n
        \  |            |                          | VERIFY-FROM-BUFFER        |\n
        \  |            |                          | request completed         |\n
        \  |            |                          | successfully. The verify  |\n
        \  |            |                          | decision can be           |\n
        \  |            |                          | \"accepted\", \"rejected\",   |\n
        \  |            |                          | or \"undecided\".           |\n
        \  | 001        | error                    | VERIFY or                 |\n
        \  |            |                          | VERIFY-FROM-BUFFER        |\n
        \  |            |                          | request terminated        |\n
        \  |            |                          | prematurely due to a      |\n
        \  |            |                          | verifier resource or      |\n
        \  |            |                          | system error.             |\n
        \  | 002        | no-input-timeout         | VERIFY request completed  |\n
        \  |            |                          | with no result due to a   |\n
        \  |            |                          | no-input-timeout.         |\n
        \  | 003        | too-much-speech-timeout  | VERIFY request completed  |\n
        \  |            |                          | with no result due to too |\n
        \  |            |                          | much speech.              |\n
        \  | 004        | speech-too-early         | VERIFY request completed  |\n
        \  |            |                          | with no result due to     |\n
        \  |            |                          | speech too soon.          |\n
        \  | 005        | buffer-empty             | VERIFY-FROM-BUFFER        |\n
        \  |            |                          | request completed with no |\n
        \  |            |                          | result due to empty       |\n
        \  |            |                          | buffer.                   |\n
        \  | 006        | out-of-sequence          | Verification operation    |\n
        \  |            |                          | failed due to             |\n
        \  |            |                          | out-of-sequence method    |\n
        \  |            |                          | invocations, for example, |\n
        \  |            |                          | calling VERIFY before     |\n
        \  |            |                          | QUERY-VOICEPRINT.         |\n
        \  | 007        | repository-uri-failure   | Failure accessing         |\n
        \  |            |                          | Repository URI.           |\n
        \  | 008        | repository-uri-missing   | Repository-URI is not     |\n
        \  |            |                          | specified.                |\n
        \  | 009        | voiceprint-id-missing    | Voiceprint-Identifier is  |\n
        \  |            |                          | not specified.            |\n
        \  | 010        | voiceprint-id-not-exist  | Voiceprint-Identifier     |\n
        \  |            |                          | does not exist in the     |\n
        \  |            |                          | voiceprint repository.    |\n
        \  | 011        | speech-not-usable        | VERIFY request completed  |\n
        \  |            |                          | with no result because    |\n
        \  |            |                          | the speech was not usable |\n
        \  |            |                          | (too noisy, too short,    |\n
        \  |            |                          | etc.)                     |\n
        \  +------------+--------------------------+---------------------------+\n"
      title: 11.4.16.  Completion-Cause
    - contents:
      - "11.4.17.  Completion-Reason\n   This header field MAY be specified in a VERIFICATION-COMPLETE
        event\n   coming from the verifier resource to the client.  It contains the\n
        \  reason text behind the VERIFY request completion.  This header field\n
        \  communicates text describing the reason for the failure.\n   The completion
        reason text is provided for client use in logs and for\n   debugging and instrumentation
        purposes.  Clients MUST NOT interpret\n   the completion reason text.\n   completion-reason
        \       =  \"Completion-Reason\" \":\"\n                               quoted-string
        CRLF\n"
      title: 11.4.17.  Completion-Reason
    - contents:
      - "11.4.18.  Speech-Complete-Timeout\n   This header field is the same as the
        one described for the Recognizer\n   resource.  See Section 9.4.15.  This
        header field MAY occur in\n   VERIFY, SET-PARAMS, or GET-PARAMS.\n"
      title: 11.4.18.  Speech-Complete-Timeout
    - contents:
      - "11.4.19.  New-Audio-Channel\n   This header field is the same as the one
        described for the Recognizer\n   resource.  See Section 9.4.23.  This header
        field MAY be specified in\n   a VERIFY request.\n"
      title: 11.4.19.  New-Audio-Channel
    - contents:
      - "11.4.20.  Abort-Verification\n   This header field MUST be sent in a STOP
        request to indicate whether\n   or not to abort a VERIFY method in progress.
        \ A value of \"true\"\n   requests the server to discard the results.  A value
        of \"false\"\n   requests the server to return in the STOP response the verification\n
        \  results obtained up to the point it received the STOP request.\n   abort-verification
        \  =  \"Abort-Verification \" \":\" BOOLEAN CRLF\n"
      title: 11.4.20.  Abort-Verification
    - contents:
      - "11.4.21.  Start-Input-Timers\n   This header field MAY be sent as part of
        a VERIFY request.  A value\n   of \"false\" tells the verifier resource to
        start the VERIFY operation\n   but not to start the no-input timer yet.  The
        verifier resource MUST\n   NOT start the timers until the client sends a START-INPUT-TIMERS\n
        \  request to the resource.  This is useful in the scenario when the\n   verifier
        and synthesizer resources are not part of the same session.\n   In this scenario,
        when a kill-on-barge-in prompt is being played, the\n   client may want the
        VERIFY request to be simultaneously active so\n   that it can detect and implement
        kill-on-barge-in (see\n   Section 8.4.2).  But at the same time, the client
        doesn't want the\n   verifier resource to start the no-input timers until
        the prompt is\n   finished.  The default value is \"true\".\n   start-input-timers
        \      =  \"Start-Input-Timers\" \":\"\n                               BOOLEAN
        CRLF\n"
      title: 11.4.21.  Start-Input-Timers
    title: 11.4.  Verification Header Fields
  - contents:
    - "11.5.  Verification Message Body\n   A verification response or event message
      can carry additional data as\n   described in the following subsection.\n"
    - contents:
      - "11.5.1.  Verification Result Data\n   Verification results are returned to
        the client in the message body\n   of the VERIFICATION-COMPLETE event or the
        GET-INTERMEDIATE-RESULT\n   response message as described in Section 6.3.
        \ Element and attribute\n   descriptions for the verification portion of the
        NLSML format are\n   provided in Section 11.5.2 with a normative definition
        of the schema\n   in Section 16.3.\n"
      title: 11.5.1.  Verification Result Data
    - contents:
      - "11.5.2.  Verification Result Elements\n   All verification elements are contained
        within a single\n   <verification-result> element under <result>.  The elements
        are\n   described below and have the schema defined in Section 16.2.  The\n
        \  following elements are defined:\n   1.   <voiceprint>\n   2.   <incremental>\n
        \  3.   <cumulative>\n   4.   <decision>\n   5.   <utterance-length>\n   6.
        \  <device>\n   7.   <gender>\n   8.   <adapted>\n   9.   <verification-score>\n
        \  10.  <vendor-specific-results>\n"
      - contents:
        - "11.5.2.1.  <voiceprint> Element\n   This element in the verification results
          provides information on how\n   the speech data matched a single voiceprint.
          \ The result data\n   returned MAY have more than one such entity in the
          case of\n   identification or multi-verification.  Each <voiceprint> element
          and\n   the XML data within the element describe verification result\n   information
          for how well the speech data matched that particular\n   voiceprint.  The
          list of <voiceprint> element data are ordered\n   according to their cumulative
          verification match scores, with the\n   highest score first.\n"
        title: 11.5.2.1.  <voiceprint> Element
      - contents:
        - "11.5.2.2.  <cumulative> Element\n   Within each <voiceprint> element there
          MUST be a <cumulative> element\n   with the cumulative scores of how well
          multiple utterances matched\n   the voiceprint.\n"
        title: 11.5.2.2.  <cumulative> Element
      - contents:
        - "11.5.2.3.  <incremental> Element\n   The first <voiceprint> element MAY
          contain an <incremental> element\n   with the incremental scores of how
          well the last utterance matched\n   the voiceprint.\n"
        title: 11.5.2.3.  <incremental> Element
      - contents:
        - "11.5.2.4.  <Decision> Element\n   This element is found within the <incremental>
          or <cumulative>\n   element within the verification results.  Its value
          indicates the\n   verification decision.  It can have the values of \"accepted\",\n
          \  \"rejected\", or \"undecided\".\n"
        title: 11.5.2.4.  <Decision> Element
      - contents:
        - "11.5.2.5.  <utterance-length> Element\n   This element MAY occur within
          either the <incremental> or\n   <cumulative> elements within the first <voiceprint>
          element.  Its\n   value indicates the size in milliseconds, respectively,
          of the last\n   utterance or the cumulated set of utterances.\n"
        title: 11.5.2.5.  <utterance-length> Element
      - contents:
        - "11.5.2.6.  <device> Element\n   This element is found within the <incremental>
          or <cumulative>\n   element within the verification results.  Its value
          indicates the\n   apparent type of device used by the caller as determined
          by the\n   verifier resource.  It can have the values of \"cellular-phone\",\n
          \  \"electret-phone\", \"carbon-button-phone\", or \"unknown\".\n"
        title: 11.5.2.6.  <device> Element
      - contents:
        - "11.5.2.7.  <gender> Element\n   This element is found within the <incremental>
          or <cumulative>\n   element within the verification results.  Its value
          indicates the\n   apparent gender of the speaker as determined by the verifier\n
          \  resource.  It can have the values of \"male\", \"female\", or \"unknown\".\n"
        title: 11.5.2.7.  <gender> Element
      - contents:
        - "11.5.2.8.  <adapted> Element\n   This element is found within the first
          <voiceprint> element within\n   the verification results.  When verification
          is trying to confirm the\n   voiceprint, this indicates if the voiceprint
          has been adapted as a\n   consequence of analyzing the source utterances.
          \ It is not returned\n   during verification training.  The value can be
          \"true\" or \"false\".\n"
        title: 11.5.2.8.  <adapted> Element
      - contents:
        - "11.5.2.9.  <verification-score> Element\n   This element is found within
          the <incremental> or <cumulative>\n   element within the verification results.
          \ Its value indicates the\n   score of the last utterance as determined
          by verification.\n   During verification, the higher the score, the more
          likely it is that\n   the speaker is the same one as the one who spoke the
          voiceprint\n   utterances.  During training, the higher the score, the more
          likely\n   the speaker is to have spoken all of the analyzed utterances.
          \ The\n   value is a floating point between -1.0 and 1.0.  If there are
          no such\n   utterances, the score is -1.  Note that the verification score
          is not\n   a probability value.\n"
        title: 11.5.2.9.  <verification-score> Element
      - contents:
        - "11.5.2.10.  <vendor-specific-results> Element\n   MRCPv2 servers MAY send
          verification results that contain\n   implementation-specific data that
          augment the information provided by\n   the MRCPv2-defined elements.  Such
          data might be useful to clients\n   who have private knowledge of how to
          interpret these schema\n   extensions.  Implementation-specific additions
          to the verification\n   results schema MUST belong to the vendor's own namespace.
          \ In the\n   result structure, either they MUST be indicated by a namespace
          prefix\n   declared within the result, or they MUST be children of an element\n
          \  identified as belonging to the respective namespace.\n   The following
          example shows the results of three voiceprints.  Note\n   that the first
          one has crossed the verification score threshold, and\n   the speaker has
          been accepted.  The voiceprint was also adapted with\n   the most recent
          utterance.\n   <?xml version=\"1.0\"?>\n   <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n
          \          grammar=\"What-Grammar-URI\">\n     <verification-result>\n       <voiceprint
          id=\"johnsmith\">\n         <adapted> true </adapted>\n         <incremental>\n
          \          <utterance-length> 500 </utterance-length>\n           <device>
          cellular-phone </device>\n           <gender> male </gender>\n           <decision>
          accepted </decision>\n           <verification-score> 0.98514 </verification-score>\n
          \        </incremental>\n         <cumulative>\n           <utterance-length>
          10000 </utterance-length>\n           <device> cellular-phone </device>\n
          \          <gender> male </gender>\n           <decision> accepted </decision>\n
          \          <verification-score> 0.96725</verification-score>\n         </cumulative>\n
          \      </voiceprint>\n       <voiceprint id=\"marysmith\">\n         <cumulative>\n
          \          <verification-score> 0.93410 </verification-score>\n         </cumulative>\n
          \      </voiceprint>\n       <voiceprint uri=\"juniorsmith\">\n         <cumulative>\n
          \          <verification-score> 0.74209 </verification-score>\n         </cumulative>\n
          \      </voiceprint>\n     </verification-result>\n   </result>\n                      Verification
          Results Example 1\n   In this next example, the verifier has enough information
          to decide\n   to reject the speaker.\n   <?xml version=\"1.0\"?>\n   <result
          xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n           xmlns:xmpl=\"http://www.example.org/2003/12/mrcpv2\"\n
          \          grammar=\"What-Grammar-URI\">\n     <verification-result>\n       <voiceprint
          id=\"johnsmith\">\n         <incremental>\n           <utterance-length>
          500 </utterance-length>\n           <device> cellular-phone </device>\n
          \          <gender> male </gender>\n           <verification-score> 0.88514
          </verification-score>\n           <xmpl:raspiness> high </xmpl:raspiness>\n
          \          <xmpl:emotion> sadness </xmpl:emotion>\n         </incremental>\n
          \        <cumulative>\n           <utterance-length> 10000 </utterance-length>\n
          \          <device> cellular-phone </device>\n           <gender> male </gender>\n
          \          <decision> rejected </decision>\n           <verification-score>
          0.9345 </verification-score>\n         </cumulative>\n       </voiceprint>\n
          \    </verification-result>\n   </result>\n                      Verification
          Results Example 2\n"
        title: 11.5.2.10.  <vendor-specific-results> Element
      title: 11.5.2.  Verification Result Elements
    title: 11.5.  Verification Message Body
  - contents:
    - "11.6.  START-SESSION\n   The START-SESSION method starts a speaker verification
      or speaker\n   identification session.  Execution of this method places the
      verifier\n   resource into its initial state.  If this method is called during
      an\n   ongoing verification session, the previous session is implicitly\n   aborted.
      \ If this method is invoked when VERIFY or VERIFY-FROM-BUFFER\n   is active,
      the method fails and the server returns a status-code of\n   402.\n   Upon completion
      of the START-SESSION method, the verifier resource\n   MUST have terminated
      any ongoing verification session and cleared any\n   voiceprint designation.\n
      \  A verification session is associated with the voiceprint repository\n   to
      be used during the session.  This is specified through the\n   Repository-URI
      header field (see Section 11.4.1).\n   The START-SESSION method also establishes,
      through the Voiceprint-\n   Identifier header field, which voiceprints are to
      be matched or\n   trained during the verification session.  If this is an\n
      \  Identification session or if the client wants to do Multi-\n   Verification,
      the Voiceprint-Identifier header field contains a list\n   of semicolon-separated
      voiceprint identifiers.\n   The Adapt-Model header field MAY also be present
      in the START-SESSION\n   request to indicate whether or not to adapt a voiceprint
      based on\n   data collected during the session (if the voiceprint verification\n
      \  phase succeeds).  By default, the voiceprint model MUST NOT be\n   adapted
      with data from a verification session.\n   The START-SESSION also determines
      whether the session is for a train\n   or verify of a voiceprint.  Hence, the
      Verification-Mode header field\n   MUST be sent in every START-SESSION request.
      \ The value of the\n   Verification-Mode header field MUST be one of either
      \"train\" or\n   \"verify\".\n   Before a verification/identification session
      is started, the client\n   may only request that VERIFY-ROLLBACK and generic
      SET-PARAMS and\n   GET-PARAMS operations be performed on the verifier resource.
      \ The\n   server MUST return status-code 402 \"Method not valid in this state\"\n
      \  for all other verification operations.\n   A verifier resource MUST NOT have
      more than a single session active\n   at one time.\n   C->S:  MRCP/2.0 ... START-SESSION
      314161\n          Channel-Identifier:32AECB23433801@speakverify\n          Repository-URI:http://www.example.com/voiceprintdbase/\n
      \         Voiceprint-Mode:verify\n          Voiceprint-Identifier:johnsmith.voiceprint\n
      \         Adapt-Model:true\n   S->C:  MRCP/2.0 ... 314161 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n"
    title: 11.6.  START-SESSION
  - contents:
    - "11.7.  END-SESSION\n   The END-SESSION method terminates an ongoing verification
      session and\n   releases the verification voiceprint resources.  The session
      may\n   terminate in one of three ways:\n   1.  abort - the voiceprint adaptation
      or creation may be aborted so\n       that the voiceprint remains unchanged
      (or is not created).\n   2.  commit - when terminating a voiceprint training
      session, the new\n       voiceprint is committed to the repository.\n   3.  adapt
      - an existing voiceprint is modified using a successful\n       verification.\n
      \  The Abort-Model header field MAY be included in the END-SESSION to\n   control
      whether or not to abort any pending changes to the\n   voiceprint.  The default
      behavior is to commit (not abort) any\n   pending changes to the designated
      voiceprint.\n   The END-SESSION method may be safely executed multiple times
      without\n   first executing the START-SESSION method.  Any additional executions\n
      \  of this method without an intervening use of the START-SESSION method\n   have
      no effect on the verifier resource.\n   The following example assumes there
      is either a training session or a\n   verification session in progress.\n   C->S:
      \ MRCP/2.0 ... END-SESSION 314174\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Abort-Model:true\n   S->C:  MRCP/2.0 ... 314174 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n"
    title: 11.7.  END-SESSION
  - contents:
    - "11.8.  QUERY-VOICEPRINT\n   The QUERY-VOICEPRINT method is used to get status
      information on a\n   particular voiceprint and can be used by the client to
      ascertain if a\n   voiceprint or repository exists and if it contains trained\n
      \  voiceprints.\n   The response to the QUERY-VOICEPRINT request contains an
      indication\n   of the status of the designated voiceprint in the Voiceprint-Exists\n
      \  header field, allowing the client to determine whether to use the\n   current
      voiceprint for verification, train a new voiceprint, or\n   choose a different
      voiceprint.\n   A voiceprint is completely specified by providing a repository\n
      \  location and a voiceprint identifier.  The particular voiceprint or\n   identity
      within the repository is specified by a string identifier\n   that is unique
      within the repository.  The Voiceprint-Identifier\n   header field carries this
      unique voiceprint identifier within a given\n   repository.\n   The following
      example assumes a verification session is in progress\n   and the voiceprint
      exists in the voiceprint repository.\n   C->S:  MRCP/2.0 ... QUERY-VOICEPRINT
      314168\n          Channel-Identifier:32AECB23433801@speakverify\n          Repository-URI:http://www.example.com/voiceprints/\n
      \         Voiceprint-Identifier:johnsmith.voiceprint\n   S->C:  MRCP/2.0 ...
      314168 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Repository-URI:http://www.example.com/voiceprints/\n          Voiceprint-Identifier:johnsmith.voiceprint\n
      \         Voiceprint-Exists:true\n   The following example assumes that the
      URI provided in the\n   Repository-URI header field is a bad URI.\n   C->S:
      \ MRCP/2.0 ... QUERY-VOICEPRINT 314168\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Repository-URI:http://www.example.com/bad-uri/\n          Voiceprint-Identifier:johnsmith.voiceprint\n
      \  S->C:  MRCP/2.0 ... 314168 405 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Repository-URI:http://www.example.com/bad-uri/\n          Voiceprint-Identifier:johnsmith.voiceprint\n
      \         Completion-Cause:007 repository-uri-failure\n"
    title: 11.8.  QUERY-VOICEPRINT
  - contents:
    - "11.9.  DELETE-VOICEPRINT\n   The DELETE-VOICEPRINT method removes a voiceprint
      from a repository.\n   This method MUST carry the Repository-URI and Voiceprint-Identifier\n
      \  header fields.\n   An MRCPv2 server MUST reject a DELETE-VOICEPRINT request
      with a 401\n   status code unless the MRCPv2 client has been authenticated and\n
      \  authorized.  Note that MRCPv2 does not have a standard mechanism for\n   this.
      \ See Section 12.8.\n   If the corresponding voiceprint does not exist, the
      DELETE-VOICEPRINT\n   method MUST return a 200 status code.\n   The following
      example demonstrates a DELETE-VOICEPRINT operation to\n   remove a specific
      voiceprint.\n   C->S:  MRCP/2.0 ... DELETE-VOICEPRINT 314168\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Repository-URI:http://www.example.com/bad-uri/\n          Voiceprint-Identifier:johnsmith.voiceprint\n
      \  S->C:  MRCP/2.0 ... 314168 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n"
    title: 11.9.  DELETE-VOICEPRINT
  - contents:
    - "11.10.  VERIFY\n   The VERIFY method is used to request that the verifier resource\n
      \  either train/adapt the voiceprint or verify/identify a claimed\n   identity.
      \ If the voiceprint is new or was deleted by a previous\n   DELETE-VOICEPRINT
      method, the VERIFY method trains the voiceprint.\n   If the voiceprint already
      exists, it is adapted and not retrained by\n   the VERIFY command.\n   C->S:
      \ MRCP/2.0 ... VERIFY 543260\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... 543260 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  When the VERIFY request completes, the MRCPv2 server MUST send a\n   VERIFICATION-COMPLETE
      event to the client.\n"
    title: 11.10.  VERIFY
  - contents:
    - "11.11.  VERIFY-FROM-BUFFER\n   The VERIFY-FROM-BUFFER method directs the verifier
      resource to verify\n   buffered audio against a voiceprint.  Only one VERIFY
      or VERIFY-FROM-\n   BUFFER method may be active for a verifier resource at a
      time.\n   The buffered audio is not consumed by this method and thus VERIFY-\n
      \  FROM-BUFFER may be invoked multiple times by the client to attempt\n   verification
      against different voiceprints.\n   For the VERIFY-FROM-BUFFER method, the server
      MAY optionally return\n   an IN-PROGRESS response before the VERIFICATION-COMPLETE
      event.\n   When the VERIFY-FROM-BUFFER method is invoked and the verification\n
      \  buffer is in use by another resource sharing it, the server MUST\n   return
      an IN-PROGRESS response and wait until the buffer is available\n   to it.  The
      verification buffer is owned by the verifier resource but\n   is shared with
      write access from other input resources on the same\n   session.  Hence, it
      is considered to be in use if there is a read or\n   write operation such as
      a RECORD or RECOGNIZE with the\n   Ver-Buffer-Utterance header field set to
      \"true\" on a resource that\n   shares this buffer.  Note that if a RECORD or
      RECOGNIZE method\n   returns with a failure cause code, the VERIFY-FROM-BUFFER
      request\n   waiting to process that buffer MUST also fail with a Completion-Cause\n
      \  of 005 (buffer-empty).\n   The following example illustrates the usage of
      some buffering\n   methods.  In this scenario, the client first performed a
      live\n   verification, but the utterance had been rejected.  In the meantime,\n
      \  the utterance is also saved to the audio buffer.  Then, another\n   voiceprint
      is used to do verification against the audio buffer and\n   the utterance is
      accepted.  For the example, we assume both\n   Num-Min-Verification-Phrases
      and Num-Max-Verification-Phrases are 1.\n   C->S:  MRCP/2.0 ... START-SESSION
      314161\n          Channel-Identifier:32AECB23433801@speakverify\n          Verification-Mode:verify\n
      \         Adapt-Model:true\n          Repository-URI:http://www.example.com/voiceprints\n
      \         Voiceprint-Identifier:johnsmith.voiceprint\n   S->C:  MRCP/2.0 ...
      314161 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  C->S:  MRCP/2.0 ... VERIFY 314162\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Ver-buffer-utterance:true\n   S->C:  MRCP/2.0 ... 314162 200 IN-PROGRESS\n
      \         Channel-Identifier:32AECB23433801@speakverify\n   S->C:  MRCP/2.0
      ... VERIFICATION-COMPLETE 314162 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Completion-Cause:000 success\n          Content-Type:application/nlsml+xml\n
      \         Content-Length:...\n          <?xml version=\"1.0\"?>\n          <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                  grammar=\"What-Grammar-URI\">\n
      \           <verification-result>\n              <voiceprint id=\"johnsmith\">\n
      \               <incremental>\n                  <utterance-length> 500 </utterance-length>\n
      \                 <device> cellular-phone </device>\n                  <gender>
      female </gender>\n                  <decision> rejected </decision>\n                  <verification-score>
      0.05465 </verification-score>\n                </incremental>\n                <cumulative>\n
      \                 <utterance-length> 500 </utterance-length>\n                  <device>
      cellular-phone </device>\n                  <gender> female </gender>\n                  <decision>
      rejected </decision>\n                  <verification-score> 0.05465 </verification-score>\n
      \               </cumulative>\n              </voiceprint>\n            </verification-result>\n
      \         </result>\n   C->S:  MRCP/2.0 ... QUERY-VOICEPRINT 314163\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Repository-URI:http://www.example.com/voiceprints/\n          Voiceprint-Identifier:johnsmith\n
      \  S->C:  MRCP/2.0 ... 314163 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Repository-URI:http://www.example.com/voiceprints/\n          Voiceprint-Identifier:johnsmith.voiceprint\n
      \         Voiceprint-Exists:true\n   C->S:  MRCP/2.0 ... START-SESSION 314164\n
      \         Channel-Identifier:32AECB23433801@speakverify\n          Verification-Mode:verify\n
      \         Adapt-Model:true\n          Repository-URI:http://www.example.com/voiceprints\n
      \         Voiceprint-Identifier:marysmith.voiceprint\n   S->C:  MRCP/2.0 ...
      314164 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  C->S:  MRCP/2.0 ... VERIFY-FROM-BUFFER 314165\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... 314165 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... VERIFICATION-COMPLETE 314165 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Completion-Cause:000 success\n          Content-Type:application/nlsml+xml\n
      \         Content-Length:...\n          <?xml version=\"1.0\"?>\n          <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                  grammar=\"What-Grammar-URI\">\n
      \           <verification-result>\n              <voiceprint id=\"marysmith\">\n
      \               <incremental>\n                  <utterance-length> 1000 </utterance-length>\n
      \                 <device> cellular-phone </device>\n                  <gender>
      female </gender>\n                  <decision> accepted </decision>\n                  <verification-score>
      0.98 </verification-score>\n                </incremental>\n                <cumulative>\n
      \                 <utterance-length> 1000 </utterance-length>\n                  <device>
      cellular-phone </device>\n                  <gender> female </gender>\n                  <decision>
      accepted </decision>\n                  <verification-score> 0.98 </verification-score>\n
      \               </cumulative>\n              </voiceprint>\n            </verification-result>\n
      \         </result>\n   C->S:  MRCP/2.0 ... END-SESSION 314166\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... 314166 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \                       VERIFY-FROM-BUFFER Example\n"
    title: 11.11.  VERIFY-FROM-BUFFER
  - contents:
    - "11.12.  VERIFY-ROLLBACK\n   The VERIFY-ROLLBACK method discards the last buffered
      utterance or\n   discards the last live utterances (when the mode is \"train\"
      or\n   \"verify\").  The client will likely want to invoke this method when\n
      \  the user provides undesirable input such as non-speech noises, side-\n   speech,
      out-of-grammar utterances, commands, etc.  Note that this\n   method does not
      provide a stack of rollback states.  Executing\n   VERIFY-ROLLBACK twice in
      succession without an intervening\n   recognition operation has no effect on
      the second attempt.\n   C->S:  MRCP/2.0 ... VERIFY-ROLLBACK 314165\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... 314165 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \                         VERIFY-ROLLBACK Example\n"
    title: 11.12.  VERIFY-ROLLBACK
  - contents:
    - "11.13.  STOP\n   The STOP method from the client to the server tells the verifier\n
      \  resource to stop the VERIFY or VERIFY-FROM-BUFFER request if one is\n   active.
      \ If such a request is active and the STOP request\n   successfully terminated
      it, then the response header section contains\n   an Active-Request-Id-List
      header field containing the request-id of\n   the VERIFY or VERIFY-FROM-BUFFER
      request that was terminated.  In\n   this case, no VERIFICATION-COMPLETE event
      is sent for the terminated\n   request.  If there was no verify request active,
      then the response\n   MUST NOT contain an Active-Request-Id-List header field.
      \ Either way,\n   the response MUST contain a status-code of 200 \"Success\".\n
      \  The STOP method can carry an Abort-Verification header field, which\n   specifies
      if the verification result until that point should be\n   discarded or returned.
      \ If this header field is not present or if the\n   value is \"true\", the verification
      result is discarded and the STOP\n   response does not contain any result data.
      \ If the header field is\n   present and its value is \"false\", the STOP response
      MUST contain a\n   Completion-Cause header field and carry the Verification
      result data\n   in its body.\n   An aborted VERIFY request does an automatic
      rollback and hence does\n   not affect the cumulative score.  A VERIFY request
      that was stopped\n   with no Abort-Verification header field or with the Abort-\n
      \  Verification header field set to \"false\" does affect cumulative\n   scores
      and would need to be explicitly rolled back if the client does\n   not want
      the verification result considered in the cumulative scores.\n   The following
      example assumes a voiceprint identity has already been\n   established.\n   C->S:
      \ MRCP/2.0 ... VERIFY 314177\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... 314177 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  C->S:  MRCP/2.0 ... STOP 314178\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... 314178 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Active-Request-Id-List:314177\n                         STOP Verification
      Example\n"
    title: 11.13.  STOP
  - contents:
    - "11.14.  START-INPUT-TIMERS\n   This request is sent from the client to the
      verifier resource to\n   start the no-input timer, usually once the client has
      ascertained\n   that any audio prompts to the user have played to completion.\n
      \  C->S:  MRCP/2.0 ... START-INPUT-TIMERS 543260\n          Channel-Identifier:32AECB23433801@speakverify\n
      \  S->C:  MRCP/2.0 ... 543260 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n"
    title: 11.14.  START-INPUT-TIMERS
  - contents:
    - "11.15.  VERIFICATION-COMPLETE\n   The VERIFICATION-COMPLETE event follows a
      call to VERIFY or VERIFY-\n   FROM-BUFFER and is used to communicate the verification
      results to\n   the client.  The event message body contains only verification\n
      \  results.\n   S->C:  MRCP/2.0 ... VERIFICATION-COMPLETE 543259 COMPLETE\n
      \         Completion-Cause:000 success\n          Content-Type:application/nlsml+xml\n
      \         Content-Length:...\n          <?xml version=\"1.0\"?>\n          <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                  grammar=\"What-Grammar-URI\">\n
      \           <verification-result>\n              <voiceprint id=\"johnsmith\">\n
      \               <incremental>\n                  <utterance-length> 500 </utterance-length>\n
      \                 <device> cellular-phone </device>\n                  <gender>
      male </gender>\n                  <decision> accepted </decision>\n                  <verification-score>
      0.85 </verification-score>\n                </incremental>\n                <cumulative>\n
      \                 <utterance-length> 1500 </utterance-length>\n                  <device>
      cellular-phone </device>\n                  <gender> male </gender>\n                  <decision>
      accepted </decision>\n                  <verification-score> 0.75 </verification-score>\n
      \               </cumulative>\n              </voiceprint>\n            </verification-result>\n
      \         </result>\n"
    title: 11.15.  VERIFICATION-COMPLETE
  - contents:
    - "11.16.  START-OF-INPUT\n   The START-OF-INPUT event is returned from the server
      to the client\n   once the server has detected speech.  This event is always
      returned\n   by the verifier resource when speech has been detected, irrespective\n
      \  of whether or not the recognizer and verifier resources share the\n   same
      session.\n   S->C:  MRCP/2.0 ... START-OF-INPUT 543259 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speakverify\n"
    title: 11.16.  START-OF-INPUT
  - contents:
    - "11.17.  CLEAR-BUFFER\n   The CLEAR-BUFFER method can be used to clear the verification
      buffer.\n   This buffer is used to buffer speech during recognition, record,
      or\n   verification operations that may later be used by VERIFY-FROM-BUFFER.\n
      \  As noted before, the buffer associated with the verifier resource is\n   shared
      by other input resources like recognizers and recorders.\n   Hence, a CLEAR-BUFFER
      request fails if the verification buffer is in\n   use.  This can happen when
      any one of the input resources that share\n   this buffer has an active read
      or write operation such as RECORD,\n   RECOGNIZE, or VERIFY with the Ver-Buffer-Utterance
      header field set\n   to \"true\".\n   C->S:  MRCP/2.0 ... CLEAR-BUFFER 543260\n
      \         Channel-Identifier:32AECB23433801@speakverify\n   S->C:  MRCP/2.0
      ... 543260 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n"
    title: 11.17.  CLEAR-BUFFER
  - contents:
    - "11.18.  GET-INTERMEDIATE-RESULT\n   A client can use the GET-INTERMEDIATE-RESULT
      method to poll for\n   intermediate results of a verification request that is
      in progress.\n   Invoking this method does not change the state of the resource.
      \ The\n   verifier resource collects the accumulated verification results and\n
      \  returns the information in the method response.  The message body in\n   the
      response to a GET-INTERMEDIATE-RESULT REQUEST contains only\n   verification
      results.  The method response MUST NOT contain a\n   Completion-Cause header
      field as the request is not yet complete.  If\n   the resource does not have
      a verification in progress, the response\n   has a 402 failure status-code and
      no result in the body.\n   C->S:  MRCP/2.0 ... GET-INTERMEDIATE-RESULT 543260\n
      \         Channel-Identifier:32AECB23433801@speakverify\n   S->C:  MRCP/2.0
      ... 543260 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speakverify\n
      \         Content-Type:application/nlsml+xml\n          Content-Length:...\n
      \         <?xml version=\"1.0\"?>\n          <result xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n
      \                 grammar=\"What-Grammar-URI\">\n            <verification-result>\n
      \             <voiceprint id=\"marysmith\">\n                <incremental>\n
      \                 <utterance-length> 50 </utterance-length>\n                  <device>
      cellular-phone </device>\n                  <gender> female </gender>\n                  <decision>
      undecided </decision>\n                  <verification-score> 0.85 </verification-score>\n
      \               </incremental>\n                <cumulative>\n                  <utterance-length>
      150 </utterance-length>\n                  <device> cellular-phone </device>\n
      \                 <gender> female </gender>\n                  <decision> undecided
      </decision>\n                  <verification-score> 0.65 </verification-score>\n
      \               </cumulative>\n              </voiceprint>\n            </verification-result>\n
      \         </result>\n"
    title: 11.18.  GET-INTERMEDIATE-RESULT
  title: 11.  Speaker Verification and Identification
- contents:
  - "12.  Security Considerations\n   MRCPv2 is designed to comply with the security-related
    requirements\n   documented in the SPEECHSC requirements [RFC4313].  Implementers
    and\n   users of MRCPv2 are strongly encouraged to read the Security\n   Considerations
    section of [RFC4313], because that document contains\n   discussion of a number
    of important security issues associated with\n   the utilization of speech as
    biometric authentication technology, and\n   on the threats against systems which
    store recorded speech, contain\n   large corpora of voiceprints, and send and
    receive sensitive\n   information based on voice input to a recognizer or speech
    output\n   from a synthesizer.  Specific security measures employed by MRCPv2\n
    \  are summarized in the following subsections.  See the corresponding\n   sections
    of this specification for how the security-related machinery\n   is invoked by
    individual protocol operations.\n"
  - contents:
    - "12.1.  Rendezvous and Session Establishment\n   MRCPv2 control sessions are
      established as media sessions described\n   by SDP within the context of a SIP
      dialog.  In order to ensure secure\n   rendezvous between MRCPv2 clients and
      servers, the following are\n   required:\n   1.  The SIP implementation in MRCPv2
      clients and servers MUST support\n       SIP digest authentication [RFC3261]
      and SHOULD employ it.\n   2.  The SIP implementation in MRCPv2 clients and servers
      MUST support\n       'sips' URIs and SHOULD employ 'sips' URIs; this includes
      that\n       clients and servers SHOULD set up TLS [RFC5246] connections.\n
      \  3.  If media stream cryptographic keying is done through SDP (e.g.\n       using
      [RFC4568]), the MRCPv2 clients and servers MUST employ the\n       'sips' URI.\n
      \  4.  When TLS is used for SIP, the client MUST verify the identity of\n       the
      server to which it connects, following the rules and\n       guidelines defined
      in [RFC5922].\n"
    title: 12.1.  Rendezvous and Session Establishment
  - contents:
    - "12.2.  Control Channel Protection\n   Sensitive data is carried over the MRCPv2
      control channel.  This\n   includes things like the output of speech recognition
      operations,\n   speaker verification results, input to text-to-speech conversion,\n
      \  personally identifying grammars, etc.  For this reason, MRCPv2\n   servers
      must be properly authenticated, and the control channel must\n   permit the
      use of both confidentiality and integrity for the data.\n   To ensure control
      channel protection, MRCPv2 clients and servers MUST\n   support TLS and SHOULD
      utilize it by default unless alternative\n   control channel protection is used.
      \ When TLS is used, the client\n   MUST verify the identity of the server to
      which it connects,\n   following the rules and guidelines defined in [RFC4572].
      \ If there\n   are multiple TLS-protected channels between the client and the\n
      \  server, the server MUST NOT send a response to the client over a\n   channel
      for which the TLS identities of the server or client differ\n   from the channel
      over which the server received the corresponding\n   request.  Alternative control-channel
      protection MAY be used if\n   desired (e.g., Security Architecture for the Internet
      Protocol\n   (IPsec) [RFC4301]).\n"
    title: 12.2.  Control Channel Protection
  - contents:
    - "12.3.  Media Session Protection\n   Sensitive data is also carried on media
      sessions terminating on\n   MRCPv2 servers (the other end of a media channel
      may or may not be on\n   the MRCPv2 client).  This data includes the user's
      spoken utterances\n   and the output of text-to-speech operations.  MRCPv2 servers
      MUST\n   support a security mechanism for protection of audio media sessions.\n
      \  MRCPv2 clients that originate or consume audio similarly MUST support\n   a
      security mechanism for protection of the audio.  One such mechanism\n   is the
      Secure Real-time Transport Protocol (SRTP) [RFC3711].\n"
    title: 12.3.  Media Session Protection
  - contents:
    - "12.4.  Indirect Content Access\n   MCRPv2 employs content indirection extensively.
      \ Content may be\n   fetched and/or stored based on URI addressing on systems
      other than\n   the MRCPv2 client or server.  Not all of the stored content is\n
      \  necessarily sensitive (e.g., XML schemas), but the majority generally\n   needs
      protection, and some indirect content, such as voice recordings\n   and voiceprints,
      is extremely sensitive and must always be protected.\n   MRCPv2 clients and
      servers MUST implement HTTPS for indirect content\n   access and SHOULD employ
      secure access for all sensitive indirect\n   content.  Other secure URI schemes
      such as Secure FTP (FTPS)\n   [RFC4217] MAY also be used.  See Section 6.2.15
      for the header fields\n   used to transfer cookie information between the MRCPv2
      client and\n   server if needed for authentication.\n   Access to URIs provided
      by servers introduces risks that need to be\n   considered.  Although RFC 6454
      [RFC6454] discusses and focuses on a\n   same-origin policy, which MRCPv2 does
      not restrict URIs to, it still\n   provides an excellent description of the
      pitfalls of blindly\n   following server-provided URIs in Section 3 of the RFC.
      \ Servers also\n   need to be aware that clients could provide URIs to sites
      designed to\n   tie up the server in long or otherwise problematic document
      fetches.\n   MRCPv2 servers, and the services they access, MUST always be prepared\n
      \  for the possibility of such a denial-of-service attack.\n   MRCPv2 makes
      no inherent assumptions about the lifetime and access\n   controls associated
      with a URI.  For example, if neither\n   authentication nor scheme-specific
      access controls are used, a leak\n   of the URI is equivalent to a leak of the
      content.  Moreover, MRCPv2\n   makes no specific demands on the lifetime of
      a URI.  If a server\n   offers a URI and the client takes a long, long time
      to access that\n   URI, the server may have removed the resource in the interim
      time\n   period.  MRCPv2 deals with this case by using the URI access scheme's\n
      \  'resource not found' error, such as 404 for HTTPS.  How long a server\n   should
      keep a dynamic resource available is highly application and\n   context dependent.
      \ However, the server SHOULD keep the resource\n   available for a reasonable
      amount of time to make it likely the\n   client will have the resource available
      when the client needs the\n   resource.  Conversely, to mitigate state exhaustion
      attacks, MRCPv2\n   servers are not obligated to keep resources and resource
      state in\n   perpetuity.  The server SHOULD delete dynamically generated resources\n
      \  associated with an MRCPv2 session when the session ends.\n   One method to
      avoid resource leakage is for the server to use\n   difficult-to-guess, one-time
      resource URIs.  In this instance, there\n   can be only a single access to the
      underlying resource using the\n   given URI.  A downside to this approach is
      if an attacker uses the\n   URI before the client uses the URI, then the client
      is denied the\n   resource.  Other methods would be to adopt a mechanism similar
      to the\n   URLAUTH IMAP extension [RFC4467], where the server sets cryptographic\n
      \  checks on URI usage, as well as capabilities for expiration,\n   revocation,
      and so on.  Specifying such a mechanism is beyond the\n   scope of this document.\n"
    title: 12.4.  Indirect Content Access
  - contents:
    - "12.5.  Protection of Stored Media\n   MRCPv2 applications often require the
      use of stored media.  Voice\n   recordings are both stored (e.g., for diagnosis
      and system tuning),\n   and fetched (for replaying utterances into multiple
      MRCPv2\n   resources).  Voiceprints are fundamental to the speaker\n   identification
      and verification functions.  This data can be\n   extremely sensitive and can
      present substantial privacy and\n   impersonation risks if stolen.  Systems
      employing MRCPv2 SHOULD be\n   deployed in ways that minimize these risks.  The
      SPEECHSC\n   requirements RFC [RFC4313] contains a more extensive discussion
      of\n   these risks and ways they may be mitigated.\n"
    title: 12.5.  Protection of Stored Media
  - contents:
    - "12.6.  DTMF and Recognition Buffers\n   DTMF buffers and recognition buffers
      may grow large enough to exceed\n   the capabilities of a server, and the server
      MUST be prepared to\n   gracefully handle resource consumption.  A server MAY
      respond with\n   the appropriate recognition incomplete if the server is in
      danger of\n   running out of resources.\n"
    title: 12.6.  DTMF and Recognition Buffers
  - contents:
    - "12.7.  Client-Set Server Parameters\n   In MRCPv2, there are some tasks, such
      as URI resource fetches, that\n   the server does on behalf of the client.  To
      control this behavior,\n   MRCPv2 has a number of server parameters that a client
      can configure.\n   With one such parameter, Fetch-Timeout (Section 6.2.12),
      a malicious\n   client could set a very large value and then request the server
      to\n   fetch a non-existent document.  It is RECOMMENDED that servers be\n   cautious
      about accepting long timeout values or abnormally large\n   values for other
      client-set parameters.\n"
    title: 12.7.  Client-Set Server Parameters
  - contents:
    - "12.8.  DELETE-VOICEPRINT and Authorization\n   Since this specification does
      not mandate a specific mechanism for\n   authentication and authorization when
      requesting DELETE-VOICEPRINT\n   (Section 11.9), there is a risk that an MRCPv2
      server may not do such\n   a check for authentication and authorization.  In
      practice, each\n   provider of voice biometric solutions does insist on its
      own\n   authentication and authorization mechanism, outside of this\n   specification,
      so this is not likely to be a major problem.  If in\n   the future voice biometric
      providers standardize on such a mechanism,\n   then a future version of MRCP
      can mandate it.\n"
    title: 12.8.  DELETE-VOICEPRINT and Authorization
  title: 12.  Security Considerations
- contents:
  - '13.  IANA Considerations

    '
  - contents:
    - "13.1.  New Registries\n   This section describes the name spaces (registries)
      for MRCPv2 that\n   IANA has created and now maintains.  Assignment/registration
      policies\n   are described in RFC 5226 [RFC5226].\n"
    - contents:
      - "13.1.1.  MRCPv2 Resource Types\n   IANA has created a new name space of \"MRCPv2
        Resource Types\".  All\n   maintenance within and additions to the contents
        of this name space\n   MUST be according to the \"Standards Action\" registration
        policy.  The\n   initial contents of the registry, defined in Section 4.2,
        are given\n   below:\n   Resource type  Resource description  Reference\n
        \  -------------  --------------------  ---------\n   speechrecog    Speech
        Recognizer     [RFC6787]\n   dtmfrecog      DTMF Recognizer       [RFC6787]\n
        \  speechsynth    Speech Synthesizer    [RFC6787]\n   basicsynth     Basic
        Synthesizer     [RFC6787]\n   speakverify    Speaker Verifier      [RFC6787]\n
        \  recorder       Speech Recorder       [RFC6787]\n"
      title: 13.1.1.  MRCPv2 Resource Types
    - contents:
      - "13.1.2.  MRCPv2 Methods and Events\n   IANA has created a new name space
        of \"MRCPv2 Methods and Events\".\n   All maintenance within and additions
        to the contents of this name\n   space MUST be according to the \"Standards
        Action\" registration\n   policy.  The initial contents of the registry, defined
        by the\n   \"method-name\" and \"event-name\" BNF in Section 15 and explained
        in\n   Sections 5.2 and 5.5, are given below.\n   Name                     Resource
        type  Method/Event  Reference\n   ----                     -------------  ------------
        \ ---------\n   SET-PARAMS               Generic        Method        [RFC6787]\n
        \  GET-PARAMS               Generic        Method        [RFC6787]\n   SPEAK
        \                   Synthesizer    Method        [RFC6787]\n   STOP                     Synthesizer
        \   Method        [RFC6787]\n   PAUSE                    Synthesizer    Method
        \       [RFC6787]\n   RESUME                   Synthesizer    Method        [RFC6787]\n
        \  BARGE-IN-OCCURRED        Synthesizer    Method        [RFC6787]\n   CONTROL
        \                 Synthesizer    Method        [RFC6787]\n   DEFINE-LEXICON
        \          Synthesizer    Method        [RFC6787]\n   DEFINE-GRAMMAR           Recognizer
        \    Method        [RFC6787]\n   RECOGNIZE                Recognizer     Method
        \       [RFC6787]\n   INTERPRET                Recognizer     Method        [RFC6787]\n
        \  GET-RESULT               Recognizer     Method        [RFC6787]\n   START-INPUT-TIMERS
        \      Recognizer     Method        [RFC6787]\n   STOP                     Recognizer
        \    Method        [RFC6787]\n   START-PHRASE-ENROLLMENT  Recognizer     Method
        \       [RFC6787]\n   ENROLLMENT-ROLLBACK      Recognizer     Method        [RFC6787]\n
        \  END-PHRASE-ENROLLMENT    Recognizer     Method        [RFC6787]\n   MODIFY-PHRASE
        \           Recognizer     Method        [RFC6787]\n   DELETE-PHRASE            Recognizer
        \    Method        [RFC6787]\n   RECORD                   Recorder       Method
        \       [RFC6787]\n   STOP                     Recorder       Method        [RFC6787]\n
        \  START-INPUT-TIMERS       Recorder       Method        [RFC6787]\n   START-SESSION
        \           Verifier       Method        [RFC6787]\n   END-SESSION              Verifier
        \      Method        [RFC6787]\n   QUERY-VOICEPRINT         Verifier       Method
        \       [RFC6787]\n   DELETE-VOICEPRINT        Verifier       Method        [RFC6787]\n
        \  VERIFY                   Verifier       Method        [RFC6787]\n   VERIFY-FROM-BUFFER
        \      Verifier       Method        [RFC6787]\n   VERIFY-ROLLBACK          Verifier
        \      Method        [RFC6787]\n   STOP                     Verifier       Method
        \       [RFC6787]\n   START-INPUT-TIMERS       Verifier       Method        [RFC6787]\n
        \  GET-INTERMEDIATE-RESULT  Verifier       Method        [RFC6787]\n   SPEECH-MARKER
        \           Synthesizer    Event         [RFC6787]\n   SPEAK-COMPLETE           Synthesizer
        \   Event         [RFC6787]\n   START-OF-INPUT           Recognizer     Event
        \        [RFC6787]\n   RECOGNITION-COMPLETE     Recognizer     Event         [RFC6787]\n
        \  INTERPRETATION-COMPLETE  Recognizer     Event         [RFC6787]\n   START-OF-INPUT
        \          Recorder       Event         [RFC6787]\n   RECORD-COMPLETE          Recorder
        \      Event         [RFC6787]\n   VERIFICATION-COMPLETE    Verifier       Event
        \        [RFC6787]\n   START-OF-INPUT           Verifier       Event         [RFC6787]\n"
      title: 13.1.2.  MRCPv2 Methods and Events
    - contents:
      - "13.1.3.  MRCPv2 Header Fields\n   IANA has created a new name space of \"MRCPv2
        Header Fields\".  All\n   maintenance within and additions to the contents
        of this name space\n   MUST be according to the \"Standards Action\" registration
        policy.  The\n   initial contents of the registry, defined by the \"message-header\"
        BNF\n   in Section 15 and explained in Section 5.1, are given below.  Note\n
        \  that the values permitted for the \"Vendor-Specific-Parameters\"\n   parameter
        are managed according to a different policy.  See\n   Section 13.1.6.\n   Name
        \                              Resource type    Reference\n   ----                               -------------
        \   ---------\n   Channel-Identifier                 Generic          [RFC6787]\n
        \  Accept                             Generic          [RFC2616]\n   Active-Request-Id-List
        \            Generic          [RFC6787]\n   Proxy-Sync-Id                      Generic
        \         [RFC6787]\n   Accept-Charset                     Generic          [RFC2616]\n
        \  Content-Type                       Generic          [RFC6787]\n   Content-ID
        \                        Generic\n                             [RFC2392],
        [RFC2046], and [RFC5322]\n   Content-Base                       Generic          [RFC6787]\n
        \  Content-Encoding                   Generic          [RFC6787]\n   Content-Location
        \                  Generic          [RFC6787]\n   Content-Length                     Generic
        \         [RFC6787]\n   Fetch-Timeout                      Generic          [RFC6787]\n
        \  Cache-Control                      Generic          [RFC6787]\n   Logging-Tag
        \                       Generic          [RFC6787]\n   Set-Cookie                         Generic
        \         [RFC6787]\n   Vendor-Specific                    Generic          [RFC6787]\n
        \  Jump-Size                          Synthesizer      [RFC6787]\n   Kill-On-Barge-In
        \                  Synthesizer      [RFC6787]\n   Speaker-Profile                    Synthesizer
        \     [RFC6787]\n   Completion-Cause                   Synthesizer      [RFC6787]\n
        \  Completion-Reason                  Synthesizer      [RFC6787]\n   Voice-Parameter
        \                   Synthesizer      [RFC6787]\n   Prosody-Parameter                  Synthesizer
        \     [RFC6787]\n   Speech-Marker                      Synthesizer      [RFC6787]\n
        \  Speech-Language                    Synthesizer      [RFC6787]\n   Fetch-Hint
        \                        Synthesizer      [RFC6787]\n   Audio-Fetch-Hint                   Synthesizer
        \     [RFC6787]\n   Failed-URI                         Synthesizer      [RFC6787]\n
        \  Failed-URI-Cause                   Synthesizer      [RFC6787]\n   Speak-Restart
        \                     Synthesizer      [RFC6787]\n   Speak-Length                       Synthesizer
        \     [RFC6787]\n   Load-Lexicon                       Synthesizer      [RFC6787]\n
        \  Lexicon-Search-Order               Synthesizer      [RFC6787]\n   Confidence-Threshold
        \              Recognizer       [RFC6787]\n   Sensitivity-Level                  Recognizer
        \      [RFC6787]\n   Speed-Vs-Accuracy                  Recognizer       [RFC6787]\n
        \  N-Best-List-Length                 Recognizer       [RFC6787]\n   Input-Type
        \                        Recognizer       [RFC6787]\n   No-Input-Timeout                   Recognizer
        \      [RFC6787]\n   Recognition-Timeout                Recognizer       [RFC6787]\n
        \  Waveform-URI                       Recognizer       [RFC6787]\n   Input-Waveform-URI
        \                Recognizer       [RFC6787]\n   Completion-Cause                   Recognizer
        \      [RFC6787]\n   Completion-Reason                  Recognizer       [RFC6787]\n
        \  Recognizer-Context-Block           Recognizer       [RFC6787]\n   Start-Input-Timers
        \                Recognizer       [RFC6787]\n   Speech-Complete-Timeout            Recognizer
        \      [RFC6787]\n   Speech-Incomplete-Timeout          Recognizer       [RFC6787]\n
        \  Dtmf-Interdigit-Timeout            Recognizer       [RFC6787]\n   Dtmf-Term-Timeout
        \                 Recognizer       [RFC6787]\n   Dtmf-Term-Char                     Recognizer
        \      [RFC6787]\n   Failed-URI                         Recognizer       [RFC6787]\n
        \  Failed-URI-Cause                   Recognizer       [RFC6787]\n   Save-Waveform
        \                     Recognizer       [RFC6787]\n   Media-Type                         Recognizer
        \      [RFC6787]\n   New-Audio-Channel                  Recognizer       [RFC6787]\n
        \  Speech-Language                    Recognizer       [RFC6787]\n   Ver-Buffer-Utterance
        \              Recognizer       [RFC6787]\n   Recognition-Mode                   Recognizer
        \      [RFC6787]\n   Cancel-If-Queue                    Recognizer       [RFC6787]\n
        \  Hotword-Max-Duration               Recognizer       [RFC6787]\n   Hotword-Min-Duration
        \              Recognizer       [RFC6787]\n   Interpret-Text                     Recognizer
        \      [RFC6787]\n   Dtmf-Buffer-Time                   Recognizer       [RFC6787]\n
        \  Clear-Dtmf-Buffer                  Recognizer       [RFC6787]\n   Early-No-Match
        \                    Recognizer       [RFC6787]\n   Num-Min-Consistent-Pronunciations
        \ Recognizer       [RFC6787]\n   Consistency-Threshold              Recognizer
        \      [RFC6787]\n   Clash-Threshold                    Recognizer       [RFC6787]\n
        \  Personal-Grammar-URI               Recognizer       [RFC6787]\n   Enroll-Utterance
        \                  Recognizer       [RFC6787]\n   Phrase-ID                          Recognizer
        \      [RFC6787]\n   Phrase-NL                          Recognizer       [RFC6787]\n
        \  Weight                             Recognizer       [RFC6787]\n   Save-Best-Waveform
        \                Recognizer       [RFC6787]\n   New-Phrase-ID                      Recognizer
        \      [RFC6787]\n   Confusable-Phrases-URI             Recognizer       [RFC6787]\n
        \  Abort-Phrase-Enrollment            Recognizer       [RFC6787]\n   Sensitivity-Level
        \                 Recorder         [RFC6787]\n   No-Input-Timeout                   Recorder
        \        [RFC6787]\n   Completion-Cause                   Recorder         [RFC6787]\n
        \  Completion-Reason                  Recorder         [RFC6787]\n   Failed-URI
        \                        Recorder         [RFC6787]\n   Failed-URI-Cause                   Recorder
        \        [RFC6787]\n   Record-URI                         Recorder         [RFC6787]\n
        \  Media-Type                         Recorder         [RFC6787]\n   Max-Time
        \                          Recorder         [RFC6787]\n   Trim-Length                        Recorder
        \        [RFC6787]\n   Final-Silence                      Recorder         [RFC6787]\n
        \  Capture-On-Speech                  Recorder         [RFC6787]\n   Ver-Buffer-Utterance
        \              Recorder         [RFC6787]\n   Start-Input-Timers                 Recorder
        \        [RFC6787]\n   New-Audio-Channel                  Recorder         [RFC6787]\n
        \  Repository-URI                     Verifier         [RFC6787]\n   Voiceprint-Identifier
        \             Verifier         [RFC6787]\n   Verification-Mode                  Verifier
        \        [RFC6787]\n   Adapt-Model                        Verifier         [RFC6787]\n
        \  Abort-Model                        Verifier         [RFC6787]\n   Min-Verification-Score
        \            Verifier         [RFC6787]\n   Num-Min-Verification-Phrases       Verifier
        \        [RFC6787]\n   Num-Max-Verification-Phrases       Verifier         [RFC6787]\n
        \  No-Input-Timeout                   Verifier         [RFC6787]\n   Save-Waveform
        \                     Verifier         [RFC6787]\n   Media-Type                         Verifier
        \        [RFC6787]\n   Waveform-URI                       Verifier         [RFC6787]\n
        \  Voiceprint-Exists                  Verifier         [RFC6787]\n   Ver-Buffer-Utterance
        \              Verifier         [RFC6787]\n   Input-Waveform-URI                 Verifier
        \        [RFC6787]\n   Completion-Cause                   Verifier         [RFC6787]\n
        \  Completion-Reason                  Verifier         [RFC6787]\n   Speech-Complete-Timeout
        \           Verifier         [RFC6787]\n   New-Audio-Channel                  Verifier
        \        [RFC6787]\n   Abort-Verification                 Verifier         [RFC6787]\n
        \  Start-Input-Timers                 Verifier         [RFC6787]\n   Input-Type
        \                        Verifier         [RFC6787]\n"
      title: 13.1.3.  MRCPv2 Header Fields
    - contents:
      - "13.1.4.  MRCPv2 Status Codes\n   IANA has created a new name space of \"MRCPv2
        Status Codes\" with the\n   initial values that are defined in Section 5.4.
        \ All maintenance\n   within and additions to the contents of this name space
        MUST be\n   according to the \"Specification Required with Expert Review\"\n
        \  registration policy.\n"
      title: 13.1.4.  MRCPv2 Status Codes
    - contents:
      - "13.1.5.  Grammar Reference List Parameters\n   IANA has created a new name
        space of \"Grammar Reference List\n   Parameters\".  All maintenance within
        and additions to the contents of\n   this name space MUST be according to
        the \"Specification Required with\n   Expert Review\" registration policy.
        \ There is only one initial\n   parameter as shown below.\n   Name                       Reference\n
        \  ----                       -------------\n   weight                     [RFC6787]\n"
      title: 13.1.5.  Grammar Reference List Parameters
    - contents:
      - "13.1.6.  MRCPv2 Vendor-Specific Parameters\n   IANA has created a new name
        space of \"MRCPv2 Vendor-Specific\n   Parameters\".  All maintenance within
        and additions to the contents of\n   this name space MUST be according to
        the \"Hierarchical Allocation\"\n   registration policy as follows.  Each
        name (corresponding to the\n   \"vendor-av-pair-name\" ABNF production) MUST
        satisfy the syntax\n   requirements of Internet Domain Names as described
        in Section 2.3.1\n   of RFC 1035 [RFC1035] (and as updated or obsoleted by
        successive\n   RFCs), with one exception, the order of the domain names is
        reversed.\n   For example, a vendor-specific parameter \"foo\" by example.com
        would\n   have the form \"com.example.foo\".  The first, or top-level domain,
        is\n   restricted to exactly the set of Top-Level Internet Domains defined\n
        \  by IANA and will be updated by IANA when and only when that set\n   changes.
        \ The second-level and all subdomains within the parameter\n   name MUST be
        allocated according to the \"First Come First Served\"\n   policy.  It is
        RECOMMENDED that assignment requests adhere to the\n   existing allocations
        of Internet domain names to organizations,\n   institutions, corporations,
        etc.\n   The registry contains a list of vendor-registered parameters, where\n
        \  each defined parameter is associated with a contact person and\n   includes
        an optional reference to the definition of the parameter,\n   preferably an
        RFC.  The registry is initially empty.\n"
      title: 13.1.6.  MRCPv2 Vendor-Specific Parameters
    title: 13.1.  New Registries
  - contents:
    - '13.2.  NLSML-Related Registrations

      '
    - contents:
      - "13.2.1.  'application/nlsml+xml' Media Type Registration\n   IANA has registered
        the following media type according to the process\n   defined in RFC 4288
        [RFC4288].\n   To:  ietf-types@iana.org\n   Subject:  Registration of media
        type application/nlsml+xml\n   MIME media type name:  application\n   MIME
        subtype name:  nlsml+xml\n   Required parameters:  none\n   Optional parameters:\n
        \     charset:  All of the considerations described in RFC 3023\n         [RFC3023]
        also apply to the application/nlsml+xml media type.\n   Encoding considerations:
        \ All of the considerations described in RFC\n      3023 also apply to the
        'application/nlsml+xml' media type.\n   Security considerations:  As with
        HTML, NLSML documents contain links\n      to other data stores (grammars,
        verifier resources, etc.).  Unlike\n      HTML, however, the data stores are
        not treated as media to be\n      rendered.  Nevertheless, linked files may
        themselves have security\n      considerations, which would be those of the
        individual registered\n      types.  Additionally, this media type has all
        of the security\n      considerations described in RFC 3023.\n   Interoperability
        considerations:  Although an NLSML document is\n      itself a complete XML
        document, for a fuller interpretation of the\n      content a receiver of
        an NLSML document may wish to access\n      resources linked to by the document.
        \ The inability of an NLSML\n      processor to access or process such linked
        resources could result\n      in different behavior by the ultimate consumer
        of the data.\n   Published specification:  RFC 6787\n   Applications that
        use this media type:  MRCPv2 clients and servers\n   Additional information:
        \ none\n   Magic number(s):  There is no single initial octet sequence that
        is\n      always present for NLSML files.\n   Person & email address to contact
        for further information:\n      Sarvi Shanmugham, sarvi@cisco.com\n   Intended
        usage:  This media type is expected to be used only in\n      conjunction
        with MRCPv2.\n"
      title: 13.2.1.  'application/nlsml+xml' Media Type Registration
    title: 13.2.  NLSML-Related Registrations
  - contents:
    - "13.3.  NLSML XML Schema Registration\n   IANA has registered and now maintains
      the following XML Schema.\n   Information provided follows the template in RFC
      3688 [RFC3688].\n   XML element type:  schema\n   URI:  urn:ietf:params:xml:schema:nlsml\n
      \  Registrant Contact:  IESG\n   XML:  See Section 16.1.\n"
    title: 13.3.  NLSML XML Schema Registration
  - contents:
    - "13.4.  MRCPv2 XML Namespace Registration\n   IANA has registered and now maintains
      the following XML Name space.\n   Information provided follows the template
      in RFC 3688 [RFC3688].\n   XML element type:  ns\n   URI:  urn:ietf:params:xml:ns:mrcpv2\n
      \  Registrant Contact:  IESG\n   XML:  RFC 6787\n"
    title: 13.4.  MRCPv2 XML Namespace Registration
  - contents:
    - "13.5.  Text Media Type Registrations\n   IANA has registered the following
      text media type according to the\n   process defined in RFC 4288 [RFC4288].\n"
    - contents:
      - "13.5.1.  text/grammar-ref-list\n   To:  ietf-types@iana.org\n   Subject:
        \ Registration of media type text/grammar-ref-list\n   MIME media type name:
        \ text\n   MIME subtype name:  text/grammar-ref-list\n   Required parameters:
        \ none\n   Optional parameters:  none\n   Encoding considerations:  Depending
        on the transfer protocol, a\n      transfer encoding may be necessary to deal
        with very long lines.\n   Security considerations:  This media type contains
        URIs that may\n      represent references to external resources.  As these
        resources\n      are assumed to be speech recognition grammars, similar\n
        \     considerations as for the media types 'application/srgs' and\n      'application/srgs+xml'
        apply.\n   Interoperability considerations:  '>' must be percent encoded in
        URIs\n      according to RFC 3986 [RFC3986].\n   Published specification:
        \ The RECOGNIZE method of the MRCP protocol\n      performs a recognition
        operation that matches input against a set\n      of grammars.  When matching
        against more than one grammar, it is\n      sometimes necessary to use different
        weights for the individual\n      grammars.  These weights are not a property
        of the grammar\n      resource itself but qualify the reference to that grammar
        for the\n      particular recognition operation initiated by the RECOGNIZE\n
        \     method.  The format of the proposed 'text/grammar-ref-list' media\n
        \     type is as follows:\n      body       = *reference\n      reference
        \ = \"<\" uri \">\" [parameters] CRLF\n      parameters = \";\" parameter
        *(\";\" parameter)\n      parameter  = attribute \"=\" value\n      This specification
        currently only defines a 'weight' parameter,\n      but new parameters MAY
        be added through the \"Grammar Reference\n      List Parameters\" IANA registry
        established through this\n      specification.  Example:\n            <http://example.com/grammars/field1.gram>\n
        \           <http://example.com/grammars/field2.gram>;weight=\"0.85\"\n            <session:field3@form-level.store>;weight=\"0.9\"\n
        \           <http://example.com/grammars/universals.gram>;weight=\"0.75\"\n
        \  Applications that use this media type:  MRCPv2 clients and servers\n   Additional
        information:  none\n   Magic number(s):  none\n   Person & email address to
        contact for further information:\n      Sarvi Shanmugham, sarvi@cisco.com\n
        \  Intended usage:  This media type is expected to be used only in\n      conjunction
        with MRCPv2.\n"
      title: 13.5.1.  text/grammar-ref-list
    title: 13.5.  Text Media Type Registrations
  - contents:
    - "13.6.  'session' URI Scheme Registration\n   IANA has registered the following
      new URI scheme.  The information\n   below follows the template given in RFC
      4395 [RFC4395].\n   URI scheme name:  session\n   Status:  Permanent\n   URI
      scheme syntax:  The syntax of this scheme is identical to that\n      defined
      for the \"cid\" scheme in Section 2 of RFC 2392 [RFC2392].\n   URI scheme semantics:
      \ The URI is intended to identify a data\n      resource previously given to
      the network computing resource.  The\n      purpose of this scheme is to permit
      access to the specific\n      resource for the lifetime of the session with
      the entity storing\n      the resource.  The media type of the resource CAN
      vary.  There is\n      no explicit mechanism for communication of the media
      type.  This\n      scheme is currently widely used internally by existing\n
      \     implementations, and the registration is intended to provide\n      information
      in the rare (and unfortunate) case that the scheme is\n      used elsewhere.
      \ The scheme SHOULD NOT be used for open Internet\n      protocols.\n   Encoding
      considerations:  There are no other encoding considerations\n      for the 'session'
      URIs not described in RFC 3986 [RFC3986]\n   Applications/protocols that use
      this URI scheme name:  This scheme\n      name is used by MRCPv2 clients and
      servers.\n   Interoperability considerations:  Note that none of the resources
      are\n      accessible after the MCRPv2 session ends, hence the name of the\n
      \     scheme.  For clients who establish one MRCPv2 session only for the\n      entire
      speech application being implemented, this is sufficient,\n      but clients
      who create, terminate, and recreate MRCP sessions for\n      performance or
      scalability reasons will lose access to resources\n      established in the
      earlier session(s).\n   Security considerations:  Generic security considerations
      for URIs\n      described in RFC 3986 [RFC3986] apply to this scheme as well.
      \ The\n      URIs defined here provide an identification mechanism only.  Given\n
      \     that the communication channel between client and server is\n      secure,
      that the server correctly accesses the resource associated\n      with the URI,
      and that the server ensures session-only lifetime\n      and access for each
      URI, the only additional security issues are\n      those of the types of media
      referred to by the URI.\n   Contact:  Sarvi Shanmugham, sarvi@cisco.com\n   Author/Change
      controller:  IESG, iesg@ietf.org\n   References:  This specification, particularly
      Sections 6.2.7, 8.5.2,\n      9.5.1, and 9.9.\n"
    title: 13.6.  'session' URI Scheme Registration
  - contents:
    - "13.7.  SDP Parameter Registrations\n   IANA has registered the following SDP
      parameter values.  The\n   information for each follows the template given in
      RFC 4566\n   [RFC4566], Appendix B.\n"
    - contents:
      - "13.7.1.  Sub-Registry \"proto\"\n   \"TCP/MRCPv2\" value of the \"proto\"
        parameter\n   Contact name, email address, and telephone number:  Sarvi Shanmugham,\n
        \     sarvi@cisco.com, +1.408.902.3875\n   Name being registered (as it will
        appear in SDP):  TCP/MRCPv2\n   Long-form name in English:  MCRPv2 over TCP\n
        \  Type of name:  proto\n   Explanation of name:  This name represents the
        MCRPv2 protocol\n      carried over TCP.\n   Reference to specification of
        name:  RFC 6787\n   \"TCP/TLS/MRCPv2\" value of the \"proto\" parameter\n
        \  Contact name, email address, and telephone number:  Sarvi Shanmugham,\n
        \     sarvi@cisco.com, +1.408.902.3875\n   Name being registered (as it will
        appear in SDP):  TCP/TLS/MRCPv2\n   Long-form name in English:  MCRPv2 over
        TLS over TCP\n   Type of name:  proto\n   Explanation of name:  This name
        represents the MCRPv2 protocol\n      carried over TLS over TCP.\n   Reference
        to specification of name:  RFC 6787\n"
      title: 13.7.1.  Sub-Registry "proto"
    - contents:
      - "13.7.2.  Sub-Registry \"att-field (media-level)\"\n   \"resource\" value
        of the \"att-field\" parameter\n   Contact name, email address, and telephone
        number:  Sarvi Shanmugham,\n      sarvi@cisco.com, +1.408.902.3875\n   Attribute
        name (as it will appear in SDP):  resource\n   Long-form attribute name in
        English:  MRCPv2 resource type\n   Type of attribute:  media-level\n   Subject
        to charset attribute?  no\n   Explanation of attribute:  See Section 4.2 of
        RFC 6787 for\n      description and examples.\n   Specification of appropriate
        attribute values:  See section\n      Section 13.1.1 of RFC 6787.\n   \"channel\"
        value of the \"att-field\" parameter\n   Contact name, email address, and
        telephone number:  Sarvi Shanmugham,\n      sarvi@cisco.com, +1.408.902.3875\n
        \  Attribute name (as it will appear in SDP):  channel\n   Long-form attribute
        name in English:  MRCPv2 resource channel\n      identifier\n   Type of attribute:
        \ media-level\n   Subject to charset attribute?  no\n   Explanation of attribute:
        \ See Section 4.2 of RFC 6787 for\n      description and examples.\n   Specification
        of appropriate attribute values:  See Section 4.2 and\n      the \"channel-id\"
        ABNF production rules of RFC 6787.\n   \"cmid\" value of the \"att-field\"
        parameter\n   Contact name, email address, and telephone number:  Sarvi Shanmugham,\n
        \     sarvi@cisco.com, +1.408.902.3875\n   Attribute name (as it will appear
        in SDP):  cmid\n   Long-form attribute name in English:  MRCPv2 resource channel
        media\n      identifier\n   Type of attribute:  media-level\n   Subject to
        charset attribute?  no\n   Explanation of attribute:  See Section 4.4 of RFC
        6787 for\n      description and examples.\n   Specification of appropriate
        attribute values:  See Section 4.4 and\n      the \"cmid-attribute\" ABNF
        production rules of RFC 6787.\n"
      title: 13.7.2.  Sub-Registry "att-field (media-level)"
    title: 13.7.  SDP Parameter Registrations
  title: 13.  IANA Considerations
- contents:
  - '14.  Examples

    '
  - contents:
    - "14.1.  Message Flow\n   The following is an example of a typical MRCPv2 session
      of speech\n   synthesis and recognition between a client and a server.  Although\n
      \  the SDP \"s=\" attribute in these examples has a text description value\n
      \  to assist in understanding the examples, please keep in mind that RFC\n   3264
      [RFC3264] recommends that messages actually put on the wire use\n   a space
      or a dash.\n   The figure below illustrates opening a session to the MRCPv2
      server.\n   This exchange does not allocate a resource or setup media.  It simply\n
      \  establishes a SIP session with the MRCPv2 server.\n   C->S:\n          INVITE
      sip:mresources@example.com SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n
      \          branch=z9hG4bK74bg1\n          Max-Forwards:6\n          To:MediaServer
      <sip:mresources@example.com>\n          From:sarvi <sip:sarvi@example.com>;tag=1928301774\n
      \         Call-ID:a84b4c76e66710\n          CSeq:323123 INVITE\n          Contact:<sip:sarvi@client.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=sarvi 2614933546 2614933546 IN IP4 192.0.2.12\n          s=Set up
      MRCPv2 control and audio\n          i=Initial contact\n          c=IN IP4 192.0.2.12\n
      \  S->C:\n          SIP/2.0 200 OK\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n
      \          branch=z9hG4bK74bg1;received=192.0.32.10\n          To:MediaServer
      <sip:mresources@example.com>;tag=62784\n          From:sarvi <sip:sarvi@example.com>;tag=1928301774\n
      \         Call-ID:a84b4c76e66710\n          CSeq:323123 INVITE\n          Contact:<sip:mresources@server.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=- 3000000001 3000000001 IN IP4 192.0.2.11\n          s=Set up MRCPv2
      control and audio\n          i=Initial contact\n          c=IN IP4 192.0.2.11\n
      \  C->S:\n          ACK sip:mresources@server.example.com SIP/2.0\n          Via:SIP/2.0/TCP
      client.atlanta.example.com:5060;\n           branch=z9hG4bK74bg2\n          Max-Forwards:6\n
      \         To:MediaServer <sip:mresources@example.com>;tag=62784\n          From:Sarvi
      <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n          CSeq:323123
      ACK\n          Content-Length:0\n   The client requests the server to create
      a synthesizer resource\n   control channel to do speech synthesis.  This also
      adds a media\n   stream to send the generated speech.  Note that, in this example,
      the\n   client requests a new MRCPv2 TCP stream between the client and the\n
      \  server.  In the following requests, the client will ask to use the\n   existing
      connection.\n   C->S:\n          INVITE sip:mresources@server.example.com SIP/2.0\n
      \         Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bg3\n
      \         Max-Forwards:6\n          To:MediaServer <sip:mresources@example.com>;tag=62784\n
      \         From:sarvi <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n
      \         CSeq:323124 INVITE\n          Contact:<sip:sarvi@client.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=sarvi 2614933546 2614933547 IN IP4 192.0.2.12\n          s=Set up
      MRCPv2 control and audio\n          i=Add TCP channel, synthesizer and one-way
      audio\n          c=IN IP4 192.0.2.12\n          t=0 0\n          m=application
      9  TCP/MRCPv2 1\n          a=setup:active\n          a=connection:new\n          a=resource:speechsynth\n
      \         a=cmid:1\n          m=audio 49170 RTP/AVP 0 96\n          a=rtpmap:0
      pcmu/8000\n          a=rtpmap:96 telephone-event/8000\n          a=fmtp:96 0-15\n
      \         a=recvonly\n          a=mid:1\n   S->C:\n          SIP/2.0 200 OK\n
      \         Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bg3;received=192.0.32.10\n
      \         To:MediaServer <sip:mresources@example.com>;tag=62784\n          From:sarvi
      <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n          CSeq:323124
      INVITE\n          Contact:<sip:mresources@server.example.com>\n          Content-Type:application/sdp\n
      \         Content-Length:...\n          v=0\n          o=- 3000000001 3000000002
      IN IP4 192.0.2.11\n          s=Set up MRCPv2 control and audio\n          i=Add
      TCP channel, synthesizer and one-way audio\n          c=IN IP4 192.0.2.11\n
      \         t=0 0\n          m=application 32416  TCP/MRCPv2 1\n          a=setup:passive\n
      \         a=connection:new\n          a=channel:32AECB23433801@speechsynth\n
      \         a=cmid:1\n          m=audio 48260 RTP/AVP 0\n          a=rtpmap:0
      pcmu/8000\n          a=sendonly\n          a=mid:1\n   C->S:\n          ACK
      sip:mresources@server.example.com SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n
      \          branch=z9hG4bK74bg4\n          Max-Forwards:6\n          To:MediaServer
      <sip:mresources@example.com>;tag=62784\n          From:Sarvi <sip:sarvi@example.com>;tag=1928301774\n
      \         Call-ID:a84b4c76e66710\n          CSeq:323124 ACK\n          Content-Length:0\n
      \  This exchange allocates an additional resource control channel for a\n   recognizer.
      \ Since a recognizer would need to receive an audio stream\n   for recognition,
      this interaction also updates the audio stream to\n   sendrecv, making it a
      two-way audio stream.\n   C->S:\n          INVITE sip:mresources@server.example.com
      SIP/2.0\n          Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bg5\n
      \         Max-Forwards:6\n          To:MediaServer <sip:mresources@example.com>;tag=62784\n
      \         From:sarvi <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n
      \         CSeq:323125 INVITE\n          Contact:<sip:sarvi@client.example.com>\n
      \         Content-Type:application/sdp\n          Content-Length:...\n          v=0\n
      \         o=sarvi 2614933546 2614933548 IN IP4 192.0.2.12\n          s=Set up
      MRCPv2 control and audio\n          i=Add recognizer and duplex the audio\n
      \         c=IN IP4 192.0.2.12\n          t=0 0\n          m=application 9  TCP/MRCPv2
      1\n          a=setup:active\n          a=connection:existing\n          a=resource:speechsynth\n
      \         a=cmid:1\n          m=audio 49170 RTP/AVP 0 96\n          a=rtpmap:0
      pcmu/8000\n          a=rtpmap:96 telephone-event/8000\n          a=fmtp:96 0-15\n
      \         a=recvonly\n          a=mid:1\n          m=application 9  TCP/MRCPv2
      1\n          a=setup:active\n          a=connection:existing\n          a=resource:speechrecog\n
      \         a=cmid:2\n          m=audio 49180 RTP/AVP 0 96\n          a=rtpmap:0
      pcmu/8000\n          a=rtpmap:96 telephone-event/8000\n          a=fmtp:96 0-15\n
      \         a=sendonly\n          a=mid:2\n   S->C:\n          SIP/2.0 200 OK\n
      \         Via:SIP/2.0/TCP client.atlanta.example.com:5060;\n           branch=z9hG4bK74bg5;received=192.0.32.10\n
      \         To:MediaServer <sip:mresources@example.com>;tag=62784\n          From:sarvi
      <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n          CSeq:323125
      INVITE\n          Contact:<sip:mresources@server.example.com>\n          Content-Type:application/sdp\n
      \         Content-Length:...\n          v=0\n          o=- 3000000001 3000000003
      IN IP4 192.0.2.11\n          s=Set up MRCPv2 control and audio\n          i=Add
      recognizer and duplex the audio\n          c=IN IP4 192.0.2.11\n          t=0
      0\n          m=application 32416  TCP/MRCPv2 1\n          a=channel:32AECB23433801@speechsynth\n
      \         a=cmid:1\n          m=audio 48260 RTP/AVP 0\n          a=rtpmap:0
      pcmu/8000\n          a=sendonly\n          a=mid:1\n          m=application
      32416  TCP/MRCPv2 1\n          a=channel:32AECB23433801@speechrecog\n          a=cmid:2\n
      \         m=audio 48260 RTP/AVP 0\n          a=rtpmap:0 pcmu/8000\n          a=rtpmap:96
      telephone-event/8000\n          a=fmtp:96 0-15\n          a=recvonly\n          a=mid:2\n
      \  C->S:\n          ACK sip:mresources@server.example.com SIP/2.0\n          Via:SIP/2.0/TCP
      client.atlanta.example.com:5060;\n           branch=z9hG4bK74bg6\n          Max-Forwards:6\n
      \         To:MediaServer <sip:mresources@example.com>;tag=62784\n          From:Sarvi
      <sip:sarvi@example.com>;tag=1928301774\n          Call-ID:a84b4c76e66710\n          CSeq:323125
      ACK\n          Content-Length:0\n   A MRCPv2 SPEAK request initiates speech.\n
      \  C->S:\n          MRCP/2.0 ... SPEAK 543257\n          Channel-Identifier:32AECB23433801@speechsynth\n
      \         Kill-On-Barge-In:false\n          Voice-gender:neutral\n          Voice-age:25\n
      \         Prosody-volume:medium\n          Content-Type:application/ssml+xml\n
      \         Content-Length:...\n          <?xml version=\"1.0\"?>\n          <speak
      version=\"1.0\"\n                 xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \                xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                 xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                 xml:lang=\"en-US\">\n
      \           <p>\n              <s>You have 4 new messages.</s>\n              <s>The
      first is from Stephanie Williams\n                <mark name=\"Stephanie\"/>\n
      \               and arrived at <break/>\n                <say-as interpret-as=\"vxml:time\">0345p</say-as>.</s>\n
      \             <s>The subject is <prosody\n                 rate=\"-20%\">ski
      trip</prosody></s>\n            </p>\n          </speak>\n   S->C:\n          MRCP/2.0
      ... 543257 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speechsynth\n
      \         Speech-Marker:timestamp=857205015059\n   The synthesizer hits the
      special marker in the message to be spoken\n   and faithfully informs the client
      of the event.\n   S->C:  MRCP/2.0 ... SPEECH-MARKER 543257 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speechsynth\n
      \         Speech-Marker:timestamp=857206027059;Stephanie\n   The synthesizer
      finishes with the SPEAK request.\n   S->C:  MRCP/2.0 ... SPEAK-COMPLETE 543257
      COMPLETE\n          Channel-Identifier:32AECB23433801@speechsynth\n          Speech-Marker:timestamp=857207685213;Stephanie\n
      \  The recognizer is issued a request to listen for the customer\n   choices.\n
      \  C->S:  MRCP/2.0 ... RECOGNIZE 543258\n          Channel-Identifier:32AECB23433801@speechrecog\n
      \         Content-Type:application/srgs+xml\n          Content-Length:...\n
      \         <?xml version=\"1.0\"?>\n          <!-- the default grammar language
      is US English -->\n          <grammar xmlns=\"http://www.w3.org/2001/06/grammar\"\n
      \                  xml:lang=\"en-US\" version=\"1.0\" root=\"request\">\n          <!--
      single language attachment to a rule expansion -->\n            <rule id=\"request\">\n
      \             Can I speak to\n              <one-of xml:lang=\"fr-CA\">\n                <item>Michel
      Tremblay</item>\n                <item>Andre Roy</item>\n              </one-of>\n
      \           </rule>\n          </grammar>\n   S->C:  MRCP/2.0 ... 543258 200
      IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speechrecog\n   The
      client issues the next MRCPv2 SPEAK method.\n   C->S:  MRCP/2.0 ... SPEAK 543259\n
      \         Channel-Identifier:32AECB23433801@speechsynth\n          Kill-On-Barge-In:true\n
      \         Content-Type:application/ssml+xml\n          Content-Length:...\n
      \         <?xml version=\"1.0\"?>\n          <speak version=\"1.0\"\n                 xmlns=\"http://www.w3.org/2001/10/synthesis\"\n
      \                xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n                 xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis\n
      \                http://www.w3.org/TR/speech-synthesis/synthesis.xsd\"\n                 xml:lang=\"en-US\">\n
      \           <p>\n              <s>Welcome to ABC corporation.</s>\n              <s>Who
      would you like to talk to?</s>\n            </p>\n          </speak>\n   S->C:
      \ MRCP/2.0 ... 543259 200 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speechsynth\n
      \         Speech-Marker:timestamp=857207696314\n   This next section of this
      ongoing example demonstrates how kill-on-\n   barge-in support works.  Since
      this last SPEAK request had Kill-On-\n   Barge-In set to \"true\", when the
      recognizer (the server) generated\n   the START-OF-INPUT event while a SPEAK
      was active, the client\n   immediately issued a BARGE-IN-OCCURRED method to
      the synthesizer\n   resource.  The speech synthesizer then terminated playback
      and\n   notified the client.  The completion-cause code provided the\n   indication
      that this was a kill-on-barge-in interruption rather than\n   a normal completion.\n
      \  Note that, since the recognition and synthesizer resources are in the\n   same
      session on the same server, to obtain a faster response the\n   server might
      have internally relayed the start-of-input condition to\n   the synthesizer
      directly, before receiving the expected BARGE-IN-\n   OCCURRED event.  However,
      any such communication is outside the scope\n   of MRCPv2.\n   S->C:  MRCP/2.0
      ... START-OF-INPUT 543258 IN-PROGRESS\n          Channel-Identifier:32AECB23433801@speechrecog\n
      \         Proxy-Sync-Id:987654321\n   C->S:  MRCP/2.0 ... BARGE-IN-OCCURRED
      543259\n          Channel-Identifier:32AECB23433801@speechsynth\n          Proxy-Sync-Id:987654321\n
      \  S->C:  MRCP/2.0 ... 543259 200 COMPLETE\n          Channel-Identifier:32AECB23433801@speechsynth\n
      \         Active-Request-Id-List:543258\n          Speech-Marker:timestamp=857206096314\n
      \  S->C:  MRCP/2.0 ... SPEAK-COMPLETE 543259 COMPLETE\n          Channel-Identifier:32AECB23433801@speechsynth\n
      \         Completion-Cause:001 barge-in\n          Speech-Marker:timestamp=857207685213\n
      \  The recognizer resource matched the spoken stream to a grammar and\n   generated
      results.  The result of the recognition is returned by the\n   server as part
      of the RECOGNITION-COMPLETE event.\n   S->C:  MRCP/2.0 ... RECOGNITION-COMPLETE
      543258 COMPLETE\n          Channel-Identifier:32AECB23433801@speechrecog\n          Completion-Cause:000
      success\n          Waveform-URI:<http://web.media.com/session123/audio.wav>;\n
      \                      size=423523;duration=25432\n          Content-Type:application/nlsml+xml\n
      \         Content-Length:...\n          <?xml version=\"1.0\"?>\n          <result
      xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n                  xmlns:ex=\"http://www.example.com/example\"\n
      \                 grammar=\"session:request1@form-level.store\">\n              <interpretation>\n
      \                 <instance name=\"Person\">\n                      <ex:Person>\n
      \                         <ex:Name> Andre Roy </ex:Name>\n                      </ex:Person>\n
      \                 </instance>\n                  <input>   may I speak to Andre
      Roy </input>\n              </interpretation>\n          </result>\n   Since
      the client was now finished with the session, including all\n   resources, it
      issued a SIP BYE request to close the SIP session.\n   This caused all control
      channels and resources allocated under the\n   session to be deallocated.\n
      \  C->S:  BYE sip:mresources@server.example.com SIP/2.0\n          Via:SIP/2.0/TCP
      client.atlanta.example.com:5060;\n           branch=z9hG4bK74bg7\n          Max-Forwards:6\n
      \         From:Sarvi <sip:sarvi@example.com>;tag=1928301774\n          To:MediaServer
      <sip:mresources@example.com>;tag=62784\n          Call-ID:a84b4c76e66710\n          CSeq:323126
      BYE\n          Content-Length:0\n"
    title: 14.1.  Message Flow
  - contents:
    - '14.2.  Recognition Result Examples

      '
    - contents:
      - "14.2.1.  Simple ASR Ambiguity\n   System: To which city will you be traveling?\n
        \  User:   I want to go to Pittsburgh.\n   <?xml version=\"1.0\"?>\n   <result
        xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n           xmlns:ex=\"http://www.example.com/example\"\n
        \          grammar=\"http://www.example.com/flight\">\n     <interpretation
        confidence=\"0.6\">\n        <instance>\n           <ex:airline>\n              <ex:to_city>Pittsburgh</ex:to_city>\n
        \          <ex:airline>\n        <instance>\n        <input mode=\"speech\">\n
        \          I want to go to Pittsburgh\n        </input>\n     </interpretation>\n
        \    <interpretation confidence=\"0.4\"\n        <instance>\n           <ex:airline>\n
        \             <ex:to_city>Stockholm</ex:to_city>\n           </ex:airline>\n
        \       </instance>\n        <input>I want to go to Stockholm</input>\n     </interpretation>\n
        \  </result>\n"
      title: 14.2.1.  Simple ASR Ambiguity
    - contents:
      - "14.2.2.  Mixed Initiative\n   System: What would you like?\n   User:   I
        would like 2 pizzas, one with pepperoni and cheese,\n           one with sausage
        and a bottle of coke, to go.\n   This example includes an order object which
        in turn contains objects\n   named \"food_item\", \"drink_item\", and \"delivery_method\".
        \ The\n   representation assumes there are no ambiguities in the speech or\n
        \  natural language processing.  Note that this representation also\n   assumes
        some level of intra-sentential anaphora resolution, i.e., to\n   resolve the
        two \"one\"s as \"pizza\".\n   <?xml version=\"1.0\"?>\n   <nl:result xmlns:nl=\"urn:ietf:params:xml:ns:mrcpv2\"\n
        \             xmlns=\"http://www.example.com/example\"\n              grammar=\"http://www.example.com/foodorder\">\n
        \    <nl:interpretation confidence=\"1.0\" >\n        <nl:instance>\n         <order>\n
        \          <food_item confidence=\"1.0\">\n             <pizza>\n               <ingredients
        confidence=\"1.0\">\n                 pepperoni\n               </ingredients>\n
        \              <ingredients confidence=\"1.0\">\n                 cheese\n
        \              </ingredients>\n             </pizza>\n             <pizza>\n
        \              <ingredients>sausage</ingredients>\n             </pizza>\n
        \          </food_item>\n           <drink_item confidence=\"1.0\">\n             <size>2-liter</size>\n
        \          </drink_item>\n           <delivery_method>to go</delivery_method>\n
        \        </order>\n       </nl:instance>\n       <nl:input mode=\"speech\">I
        would like 2 pizzas,\n            one with pepperoni and cheese, one with
        sausage\n            and a bottle of coke, to go.\n       </nl:input>\n     </nl:interpretation>\n
        \  </nl:result>\n"
      title: 14.2.2.  Mixed Initiative
    - contents:
      - "14.2.3.  DTMF Input\n   A combination of DTMF input and speech is represented
        using nested\n   input elements.  For example:\n   User: My pin is (dtmf 1
        2 3 4)\n   <input>\n     <input mode=\"speech\" confidence =\"1.0\"\n        timestamp-start=\"2000-04-03T0:00:00\"\n
        \       timestamp-end=\"2000-04-03T0:00:01.5\">My pin is\n     </input>\n
        \    <input mode=\"dtmf\" confidence =\"1.0\"\n        timestamp-start=\"2000-04-03T0:00:01.5\"\n
        \       timestamp-end=\"2000-04-03T0:00:02.0\">1 2 3 4\n     </input>\n   </input>\n
        \  Note that grammars that recognize mixtures of speech and DTMF are not\n
        \  currently possible in SRGS; however, this representation might be\n   needed
        for other applications of NLSML, and this mixture capability\n   might be
        introduced in future versions of SRGS.\n"
      title: 14.2.3.  DTMF Input
    - contents:
      - "14.2.4.  Interpreting Meta-Dialog and Meta-Task Utterances\n   Natural language
        communication makes use of meta-dialog and meta-task\n   utterances.  This
        specification is flexible enough so that meta-\n   utterances can be represented
        on an application-specific basis\n   without requiring other standard markup.\n
        \  Here are two examples of how meta-task and meta-dialog utterances\n   might
        be represented.\n"
      - 'System: What toppings do you want on your pizza?

        '
      - 'User:   What toppings do you have?

        '
      - "<interpretation grammar=\"http://www.example.com/toppings\">\n   <instance>\n
        \     <question>\n         <questioned_item>toppings<questioned_item>\n         <questioned_property>\n
        \         availability\n         </questioned_property>\n      </question>\n
        \  </instance>\n   <input mode=\"speech\">\n     what toppings do you have?\n
        \  </input>\n"
      - '</interpretation>

        '
      - 'User:   slow down.

        '
      - "<interpretation grammar=\"http://www.example.com/generalCommandsGrammar\">\n
        \  <instance>\n    <command>\n       <action>reduce speech rate</action>\n
        \      <doer>system</doer>\n    </command>\n   </instance>\n  <input mode=\"speech\">slow
        down</input>\n"
      - '</interpretation>

        '
      title: 14.2.4.  Interpreting Meta-Dialog and Meta-Task Utterances
    - contents:
      - "14.2.5.  Anaphora and Deixis\n   This specification can be used on an application-specific
        basis to\n   represent utterances that contain unresolved anaphoric and deictic\n
        \  references.  Anaphoric references, which include pronouns and\n   definite
        noun phrases that refer to something that was mentioned in\n   the preceding
        linguistic context, and deictic references, which refer\n   to something that
        is present in the non-linguistic context, present\n   similar problems in
        that there may not be sufficient unambiguous\n   linguistic context to determine
        what their exact role in the\n   interpretation should be.  In order to represent
        unresolved anaphora\n   and deixis using this specification, one strategy
        would be for the\n   developer to define a more surface-oriented representation
        that\n   leaves the specific details of the interpretation of the reference\n
        \  open.  (This assumes that a later component is responsible for\n   actually
        resolving the reference).\n   Example: (ignoring the issue of representing
        the input from the\n             pointing gesture.)\n   System: What do you
        want to drink?\n   User:   I want this. (clicks on picture of large root beer.)\n
        \  <?xml version=\"1.0\"?>\n   <nl:result xmlns:nl=\"urn:ietf:params:xml:ns:mrcpv2\"\n
        \          xmlns=\"http://www.example.com/example\"\n           grammar=\"http://www.example.com/beverages.grxml\">\n
        \     <nl:interpretation>\n         <nl:instance>\n          <doer>I</doer>\n
        \         <action>want</action>\n          <object>this</object>\n         </nl:instance>\n
        \        <nl:input mode=\"speech\">I want this</nl:input>\n      </nl:interpretation>\n
        \  </nl:result>\n"
      title: 14.2.5.  Anaphora and Deixis
    - contents:
      - "14.2.6.  Distinguishing Individual Items from Sets with One Member\n   For
        programming convenience, it is useful to be able to distinguish\n   between
        individual items and sets containing one item in the XML\n   representation
        of semantic results.  For example, a pizza order might\n   consist of exactly
        one pizza, but a pizza might contain zero or more\n   toppings.  Since there
        is no standard way of marking this distinction\n   directly in XML, in the
        current framework, the developer is free to\n   adopt any conventions that
        would convey this information in the XML\n   markup.  One strategy would be
        for the developer to wrap the set of\n   items in a grouping element, as in
        the following example.\n   <order>\n      <pizza>\n         <topping-group>\n
        \           <topping>mushrooms</topping>\n         </topping-group>\n      </pizza>\n
        \     <drink>coke</drink>\n   </order>\n   In this example, the programmer
        can assume that there is supposed to\n   be exactly one pizza and one drink
        in the order, but the fact that\n   there is only one topping is an accident
        of this particular pizza\n   order.\n   Note that the client controls both
        the grammar and the semantics to\n   be returned upon grammar matches, so
        the user of MRCPv2 is fully\n   empowered to cause results to be returned
        in NLSML in such a way that\n   the interpretation is clear to that user.\n"
      title: 14.2.6.  Distinguishing Individual Items from Sets with One Member
    - contents:
      - "14.2.7.  Extensibility\n   Extensibility in NLSML is provided via result
        content flexibility, as\n   described in the discussions of meta-utterances
        and anaphora.  NLSML\n   can easily be used in sophisticated systems to convey
        application-\n   specific information that more basic systems would not make
        use of,\n   for example, defining speech acts.\n"
      title: 14.2.7.  Extensibility
    title: 14.2.  Recognition Result Examples
  title: 14.  Examples
- contents:
  - "15.  ABNF Normative Definition\n   The following productions make use of the
    core rules defined in\n   Section B.1 of RFC 5234 [RFC5234].\n"
  - 'LWS    =    [*WSP CRLF] 1*WSP ; linear whitespace

    '
  - 'SWS    =    [LWS] ; sep whitespace

    '
  - "UTF8-NONASCII    =    %xC0-DF 1UTF8-CONT\n                 /    %xE0-EF 2UTF8-CONT\n
    \                /    %xF0-F7 3UTF8-CONT\n                 /    %xF8-FB 4UTF8-CONT\n
    \                /    %xFC-FD 5UTF8-CONT\n"
  - 'UTF8-CONT        =    %x80-BF

    '
  - "UTFCHAR          =    %x21-7E\n                 /    UTF8-NONASCII\n"
  - 'param            =    *pchar

    '
  - "quoted-string    =    SWS DQUOTE *(qdtext / quoted-pair )\n                      DQUOTE\n"
  - "qdtext           =    LWS / %x21 / %x23-5B / %x5D-7E\n                 /    UTF8-NONASCII\n"
  - 'quoted-pair      =    "\" (%x00-09 / %x0B-0C / %x0E-7F)

    '
  - "token            =    1*(alphanum / \"-\" / \".\" / \"!\" / \"%\" / \"*\"\n                      /
    \"_\" / \"+\" / \"`\" / \"'\" / \"~\" )\n"
  - "reserved         =    \";\" / \"/\" / \"?\" / \":\" / \"@\" / \"&\" / \"=\"\n
    \                     / \"+\" / \"$\" / \",\"\n"
  - "mark             =    \"-\" / \"_\" / \".\" / \"!\" / \"~\" / \"*\" / \"'\"\n
    \                /    \"(\" / \")\"\n"
  - 'unreserved       =    alphanum / mark

    '
  - "pchar            =    unreserved / escaped\n                 /    \":\" / \"@\"
    / \"&\" / \"=\" / \"+\" / \"$\" / \",\"\n"
  - 'alphanum         =    ALPHA / DIGIT

    '
  - 'BOOLEAN          =    "true" / "false"

    '
  - 'FLOAT            =    *DIGIT ["." *DIGIT]

    '
  - 'escaped          =    "%" HEXDIG HEXDIG

    '
  - 'fragment         =    *uric

    '
  - "uri              =    [ absoluteURI / relativeURI ]\n                      [
    \"#\" fragment ]\n"
  - 'absoluteURI      =    scheme ":" ( hier-part / opaque-part )

    '
  - "relativeURI      =    ( net-path / abs-path / rel-path )\n                      [
    \"?\" query ]\n"
  - 'hier-part        =    ( net-path / abs-path ) [ "?" query ]

    '
  - 'net-path         =    "//" authority [ abs-path ]

    '
  - 'abs-path         =    "/" path-segments

    '
  - 'rel-path         =    rel-segment [ abs-path ]

    '
  - "rel-segment      =    1*( unreserved / escaped / \";\" / \"@\"\n                 /
    \   \"&\" / \"=\" / \"+\" / \"$\" / \",\" )\n"
  - 'opaque-part      =    uric-no-slash *uric

    '
  - 'uric             =    reserved / unreserved / escaped

    '
  - "uric-no-slash    =    unreserved / escaped / \";\" / \"?\" / \":\"\n                      /
    \"@\" / \"&\" / \"=\" / \"+\" / \"$\" / \",\"\n"
  - 'path-segments    =    segment *( "/" segment )

    '
  - 'segment          =    *pchar *( ";" param )

    '
  - 'scheme           =    ALPHA *( ALPHA / DIGIT / "+" / "-" / "." )

    '
  - 'authority        =    srvr / reg-name

    '
  - 'srvr             =    [ [ userinfo "@" ] hostport ]

    '
  - "reg-name         =    1*( unreserved / escaped / \"$\" / \",\"\n                 /
    \    \";\" / \":\" / \"@\" / \"&\" / \"=\" / \"+\" )\n"
  - 'query            =    *uric

    '
  - 'userinfo         =    ( user ) [ ":" password ] "@"

    '
  - "user             =    1*( unreserved / escaped\n                 /    user-unreserved
    )\n"
  - "user-unreserved  =    \"&\" / \"=\" / \"+\" / \"$\" / \",\" / \";\"\n                 /
    \   \"?\" / \"/\"\n"
  - "password         =    *( unreserved / escaped\n                 /    \"&\" /
    \"=\" / \"+\" / \"$\" / \",\" )\n"
  - 'hostport         =    host [ ":" port ]

    '
  - 'host             =    hostname / IPv4address / IPv6reference

    '
  - 'hostname         =    *( domainlabel "." ) toplabel [ "." ]

    '
  - "domainlabel      =    alphanum / alphanum *( alphanum / \"-\" )\n                      alphanum\n"
  - "toplabel         =    ALPHA / ALPHA *( alphanum / \"-\" )\n                      alphanum\n"
  - "IPv4address      =    1*3DIGIT \".\" 1*3DIGIT \".\" 1*3DIGIT \".\"\n                      1*3DIGIT\n"
  - 'IPv6reference    =    "[" IPv6address "]"

    '
  - 'IPv6address      =    hexpart [ ":" IPv4address ]

    '
  - "hexpart          =    hexseq / hexseq \"::\" [ hexseq ] / \"::\"\n                      [
    hexseq ]\n"
  - 'hexseq           =    hex4 *( ":" hex4)

    '
  - 'hex4             =    1*4HEXDIG

    '
  - 'port             =    1*19DIGIT

    '
  - '; generic-message is the top-level rule

    '
  - "generic-message  =    start-line message-header CRLF\n                      [
    message-body ]\n"
  - 'message-body     =    *OCTET

    '
  - 'start-line       =    request-line / response-line / event-line

    '
  - "request-line     =    mrcp-version SP message-length SP method-name\n                      SP
    request-id CRLF\n"
  - "response-line    =    mrcp-version SP message-length SP request-id\n                      SP
    status-code SP request-state CRLF\n"
  - "event-line       =    mrcp-version SP message-length SP event-name\n                      SP
    request-id SP request-state CRLF\n"
  - "method-name      =    generic-method\n                 /    synthesizer-method\n
    \                /    recognizer-method\n                 /    recorder-method\n
    \                /    verifier-method\n"
  - "generic-method   =    \"SET-PARAMS\"\n                 /    \"GET-PARAMS\"\n"
  - "request-state    =    \"COMPLETE\"\n                 /    \"IN-PROGRESS\"\n                 /
    \   \"PENDING\"\n"
  - "event-name       =    synthesizer-event\n                 /    recognizer-event\n
    \                /    recorder-event\n                 /    verifier-event\n"
  - 'message-header   =  1*(generic-header / resource-header / generic-field)

    '
  - 'generic-field    =    field-name ":" [ field-value ]

    '
  - 'field-name       =    token

    '
  - 'field-value      =    *LWS field-content *( CRLF 1*LWS field-content)

    '
  - "field-content    =    <the OCTETs making up the field-value\n                      and
    consisting of either *TEXT or combinations\n                      of token, separators,
    and quoted-string>\n"
  - "resource-header  =    synthesizer-header\n                 /    recognizer-header\n
    \                /    recorder-header\n                 /    verifier-header\n"
  - "generic-header   =    channel-identifier\n                 /    accept\n                 /
    \   active-request-id-list\n                 /    proxy-sync-id\n                 /
    \   accept-charset\n                 /    content-type\n                 /    content-id\n
    \                /    content-base\n                 /    content-encoding\n                 /
    \   content-location\n                 /    content-length\n                 /
    \   fetch-timeout\n                 /    cache-control\n                 /    logging-tag\n
    \                /    set-cookie\n                 /    vendor-specific\n"
  - '; -- content-id is as defined in RFC 2392, RFC 2046 and RFC 5322

    '
  - '; -- accept and accept-charset are as defined in RFC 2616

    '
  - 'mrcp-version     =    "MRCP" "/" 1*2DIGIT "." 1*2DIGIT

    '
  - 'message-length   =    1*19DIGIT

    '
  - 'request-id       =    1*10DIGIT

    '
  - 'status-code      =    3DIGIT

    '
  - "channel-identifier =  \"Channel-Identifier\" \":\"\n                      channel-id
    CRLF\n"
  - 'channel-id       =    1*alphanum "@" 1*alphanum

    '
  - "active-request-id-list = \"Active-Request-Id-List\" \":\"\n                         request-id
    *(\",\" request-id) CRLF\n"
  - 'proxy-sync-id    =    "Proxy-Sync-Id" ":" 1*VCHAR CRLF

    '
  - 'content-base     =    "Content-Base" ":" absoluteURI CRLF

    '
  - 'content-length   =    "Content-Length" ":" 1*19DIGIT CRLF

    '
  - 'content-type     =    "Content-Type" ":" media-type-value CRLF

    '
  - 'media-type-value =    type "/" subtype *( ";" parameter )

    '
  - 'type             =    token

    '
  - 'subtype          =    token

    '
  - 'parameter        =    attribute "=" value

    '
  - 'attribute        =    token

    '
  - 'value            =    token / quoted-string

    '
  - "content-encoding =    \"Content-Encoding\" \":\"\n                      *WSP
    content-coding\n                      *(*WSP \",\" *WSP content-coding *WSP )\n
    \                     CRLF\n"
  - 'content-coding   =    token

    '
  - "content-location =    \"Content-Location\" \":\"\n                      ( absoluteURI
    / relativeURI )  CRLF\n"
  - "cache-control    =    \"Cache-Control\" \":\"\n                      [*WSP cache-directive\n
    \                     *( *WSP \",\" *WSP cache-directive *WSP )]\n                      CRLF\n"
  - 'fetch-timeout    =    "Fetch-Timeout" ":" 1*19DIGIT CRLF

    '
  - "cache-directive  =    \"max-age\" \"=\" delta-seconds\n                 /    \"max-stale\"
    [\"=\" delta-seconds ]\n                 /    \"min-fresh\" \"=\" delta-seconds\n"
  - 'delta-seconds    =    1*19DIGIT

    '
  - 'logging-tag      =    "Logging-Tag" ":" 1*UTFCHAR CRLF

    '
  - "vendor-specific  =    \"Vendor-Specific-Parameters\" \":\"\n                      [vendor-specific-av-pair\n
    \                     *(\";\" vendor-specific-av-pair)] CRLF\n"
  - "vendor-specific-av-pair = vendor-av-pair-name \"=\"\n                          value\n"
  - 'vendor-av-pair-name     = 1*UTFCHAR

    '
  - 'set-cookie        = "Set-Cookie:" SP set-cookie-string

    '
  - 'set-cookie-string = cookie-pair *( ";" SP cookie-av )

    '
  - 'cookie-pair       = cookie-name "=" cookie-value

    '
  - 'cookie-name       = token

    '
  - 'cookie-value      = *cookie-octet / ( DQUOTE *cookie-octet DQUOTE )

    '
  - 'cookie-octet      = %x21 / %x23-2B / %x2D-3A / %x3C-5B / %x5D-7E

    '
  - 'token             = <token, defined in [RFC2616], Section 2.2>

    '
  - "cookie-av         = expires-av / max-age-av / domain-av /\n                     path-av
    / secure-av / httponly-av /\n                     extension-av / age-av\n"
  - 'expires-av        = "Expires=" sane-cookie-date

    '
  - 'sane-cookie-date  = <rfc1123-date, defined in [RFC2616], Section 3.3.1>

    '
  - 'max-age-av        = "Max-Age=" non-zero-digit *DIGIT

    '
  - 'non-zero-digit    = %x31-39

    '
  - 'domain-av         = "Domain=" domain-value

    '
  - 'domain-value      = <subdomain>

    '
  - 'path-av           = "Path=" path-value

    '
  - 'path-value        = <any CHAR except CTLs or ";">

    '
  - 'secure-av         = "Secure"

    '
  - 'httponly-av       = "HttpOnly"

    '
  - 'extension-av      = <any CHAR except CTLs or ";">

    '
  - 'age-av            = "Age=" delta-seconds

    '
  - '; Synthesizer ABNF

    '
  - "synthesizer-method    =    \"SPEAK\"\n                      /    \"STOP\"\n                      /
    \   \"PAUSE\"\n                      /    \"RESUME\"\n                      /
    \   \"BARGE-IN-OCCURRED\"\n                      /    \"CONTROL\"\n                      /
    \   \"DEFINE-LEXICON\"\n"
  - "synthesizer-event     =    \"SPEECH-MARKER\"\n                      /    \"SPEAK-COMPLETE\"\n"
  - "synthesizer-header    =    jump-size\n                      /    kill-on-barge-in\n
    \                     /    speaker-profile\n                      /    completion-cause\n
    \                     /    completion-reason\n                      /    voice-parameter\n
    \                     /    prosody-parameter\n                      /    speech-marker\n
    \                     /    speech-language\n                      /    fetch-hint\n
    \                     /    audio-fetch-hint\n                      /    failed-uri\n
    \                     /    failed-uri-cause\n                      /    speak-restart\n
    \                     /    speak-length\n                      /    load-lexicon\n
    \                     /    lexicon-search-order\n"
  - 'jump-size             =    "Jump-Size" ":" speech-length-value CRLF

    '
  - "speech-length-value   =    numeric-speech-length\n                      /    text-speech-length\n"
  - 'text-speech-length    =    1*UTFCHAR SP "Tag"

    '
  - 'numeric-speech-length =    ("+" / "-") positive-speech-length

    '
  - 'positive-speech-length =   1*19DIGIT SP numeric-speech-unit

    '
  - "numeric-speech-unit   =    \"Second\"\n                      /    \"Word\"\n
    \                     /    \"Sentence\"\n                      /    \"Paragraph\"\n"
  - "kill-on-barge-in      =    \"Kill-On-Barge-In\" \":\" BOOLEAN\n                           CRLF\n"
  - 'speaker-profile       =    "Speaker-Profile" ":" uri CRLF

    '
  - "completion-cause         =  \"Completion-Cause\" \":\" cause-code SP\n                            cause-name
    CRLF\n"
  - 'cause-code               =  3DIGIT

    '
  - 'cause-name               =  *VCHAR

    '
  - "completion-reason     =    \"Completion-Reason\" \":\"\n                           quoted-string
    CRLF\n"
  - "voice-parameter       =    voice-gender\n                      /    voice-age\n
    \                     /    voice-variant\n                      /    voice-name\n"
  - 'voice-gender          =    "Voice-Gender:" voice-gender-value CRLF

    '
  - "voice-gender-value    =    \"male\"\n                      /    \"female\"\n
    \                     /    \"neutral\"\n"
  - 'voice-age             =    "Voice-Age:" 1*3DIGIT CRLF

    '
  - 'voice-variant         =    "Voice-Variant:" 1*19DIGIT CRLF

    '
  - "voice-name            =    \"Voice-Name:\"\n                           1*UTFCHAR
    *(1*WSP 1*UTFCHAR) CRLF\n"
  - "prosody-parameter     =    \"Prosody-\" prosody-param-name \":\"\n                           prosody-param-value
    CRLF\n"
  - 'prosody-param-name    =    1*VCHAR

    '
  - 'prosody-param-value   =    1*VCHAR

    '
  - 'timestamp             =    "timestamp" "=" time-stamp-value

    '
  - 'time-stamp-value      =    1*20DIGIT

    '
  - "speech-marker         =    \"Speech-Marker\" \":\"\n                           timestamp\n
    \                          [\";\" 1*(UTFCHAR / %x20)] CRLF\n"
  - 'speech-language       =    "Speech-Language" ":" 1*VCHAR CRLF

    '
  - 'fetch-hint            =    "Fetch-Hint" ":" ("prefetch" / "safe") CRLF

    '
  - "audio-fetch-hint      =    \"Audio-Fetch-Hint\" \":\"\n                          (\"prefetch\"
    / \"safe\" / \"stream\") CRLF\n"
  - 'failed-uri            =    "Failed-URI" ":" absoluteURI CRLF

    '
  - 'failed-uri-cause      =    "Failed-URI-Cause" ":" 1*UTFCHAR CRLF

    '
  - 'speak-restart         =    "Speak-Restart" ":" BOOLEAN CRLF

    '
  - "speak-length          =    \"Speak-Length\" \":\" positive-length-value\n                           CRLF\n"
  - "positive-length-value   =  positive-speech-length\n                        /
    \ text-speech-length\n"
  - 'load-lexicon          =    "Load-Lexicon" ":" BOOLEAN CRLF

    '
  - "lexicon-search-order  =    \"Lexicon-Search-Order\" \":\"\n          \"<\" absoluteURI
    \">\" *(\" \" \"<\" absoluteURI \">\") CRLF\n"
  - '; Recognizer ABNF

    '
  - "recognizer-method     =    recog-only-method\n                      /    enrollment-method\n"
  - "recog-only-method     =    \"DEFINE-GRAMMAR\"\n                      /    \"RECOGNIZE\"\n
    \                     /    \"INTERPRET\"\n                      /    \"GET-RESULT\"\n
    \                     /    \"START-INPUT-TIMERS\"\n                      /    \"STOP\"\n"
  - "enrollment-method     =    \"START-PHRASE-ENROLLMENT\"\n                      /
    \   \"ENROLLMENT-ROLLBACK\"\n                      /    \"END-PHRASE-ENROLLMENT\"\n
    \                     /    \"MODIFY-PHRASE\"\n                      /    \"DELETE-PHRASE\"\n"
  - "recognizer-event      =    \"START-OF-INPUT\"\n                      /    \"RECOGNITION-COMPLETE\"\n
    \                     /    \"INTERPRETATION-COMPLETE\"\n"
  - "recognizer-header     =    recog-only-header\n                      /    enrollment-header\n"
  - "recog-only-header     =    confidence-threshold\n                      /    sensitivity-level\n
    \                     /    speed-vs-accuracy\n                      /    n-best-list-length\n
    \                     /    input-type\n                      /    no-input-timeout\n
    \                     /    recognition-timeout\n                      /    waveform-uri\n
    \                     /    input-waveform-uri\n                      /    completion-cause\n
    \                     /    completion-reason\n                      /    recognizer-context-block\n
    \                     /    start-input-timers\n                      /    speech-complete-timeout\n
    \                     /    speech-incomplete-timeout\n                      /
    \   dtmf-interdigit-timeout\n                      /    dtmf-term-timeout\n                      /
    \   dtmf-term-char\n                      /    failed-uri\n                      /
    \   failed-uri-cause\n                      /    save-waveform\n                      /
    \   media-type\n                      /    new-audio-channel\n                      /
    \   speech-language\n                      /    ver-buffer-utterance\n                      /
    \   recognition-mode\n                      /    cancel-if-queue\n                      /
    \   hotword-max-duration\n                      /    hotword-min-duration\n                      /
    \   interpret-text\n                      /    dtmf-buffer-time\n                      /
    \   clear-dtmf-buffer\n                      /    early-no-match\n"
  - "enrollment-header     =    num-min-consistent-pronunciations\n                      /
    \   consistency-threshold\n                      /    clash-threshold\n                      /
    \   personal-grammar-uri\n                      /    enroll-utterance\n                      /
    \   phrase-id\n                      /    phrase-nl\n                      /    weight\n
    \                     /    save-best-waveform\n                      /    new-phrase-id\n
    \                     /    confusable-phrases-uri\n                      /    abort-phrase-enrollment\n"
  - "confidence-threshold  =    \"Confidence-Threshold\" \":\"\n                           FLOAT
    CRLF\n"
  - "sensitivity-level     =    \"Sensitivity-Level\" \":\" FLOAT\n                           CRLF\n"
  - "speed-vs-accuracy     =    \"Speed-Vs-Accuracy\" \":\" FLOAT\n                           CRLF\n"
  - "n-best-list-length    =    \"N-Best-List-Length\" \":\" 1*19DIGIT\n                           CRLF\n"
  - 'input-type            =    "Input-Type" ":"  inputs CRLF

    '
  - 'inputs                =    "speech" / "dtmf"

    '
  - "no-input-timeout      =    \"No-Input-Timeout\" \":\" 1*19DIGIT\n                           CRLF\n"
  - "recognition-timeout   =    \"Recognition-Timeout\" \":\" 1*19DIGIT\n                           CRLF\n"
  - "waveform-uri          =    \"Waveform-URI\" \":\" [\"<\" uri \">\"\n                           \";\"
    \"size\" \"=\" 1*19DIGIT\n                           \";\" \"duration\" \"=\"
    1*19DIGIT] CRLF\n"
  - "recognizer-context-block = \"Recognizer-Context-Block\" \":\"\n                           [1*VCHAR]
    CRLF\n"
  - "start-input-timers    =    \"Start-Input-Timers\" \":\"\n                           BOOLEAN
    CRLF\n"
  - "speech-complete-timeout =  \"Speech-Complete-Timeout\" \":\"\n                           1*19DIGIT
    CRLF\n"
  - "speech-incomplete-timeout = \"Speech-Incomplete-Timeout\" \":\"\n                            1*19DIGIT
    CRLF\n"
  - "dtmf-interdigit-timeout = \"DTMF-Interdigit-Timeout\" \":\"\n                          1*19DIGIT
    CRLF\n"
  - "dtmf-term-timeout     =    \"DTMF-Term-Timeout\" \":\" 1*19DIGIT\n                           CRLF\n"
  - 'dtmf-term-char        =    "DTMF-Term-Char" ":" VCHAR CRLF

    '
  - 'save-waveform         =    "Save-Waveform" ":" BOOLEAN CRLF

    '
  - "new-audio-channel     =    \"New-Audio-Channel\" \":\"\n                           BOOLEAN
    CRLF\n"
  - "recognition-mode      =    \"Recognition-Mode\" \":\"\n                           \"normal\"
    / \"hotword\" CRLF\n"
  - 'cancel-if-queue       =    "Cancel-If-Queue" ":" BOOLEAN CRLF

    '
  - "hotword-max-duration  =    \"Hotword-Max-Duration\" \":\"\n                           1*19DIGIT
    CRLF\n"
  - "hotword-min-duration  =    \"Hotword-Min-Duration\" \":\"\n                           1*19DIGIT
    CRLF\n"
  - 'interpret-text        =    "Interpret-Text" ":" 1*VCHAR CRLF

    '
  - 'dtmf-buffer-time      =    "DTMF-Buffer-Time" ":" 1*19DIGIT CRLF

    '
  - 'clear-dtmf-buffer     =    "Clear-DTMF-Buffer" ":" BOOLEAN CRLF

    '
  - 'early-no-match        =    "Early-No-Match" ":" BOOLEAN CRLF

    '
  - "num-min-consistent-pronunciations    =\n    \"Num-Min-Consistent-Pronunciations\"
    \":\" 1*19DIGIT CRLF\n"
  - "consistency-threshold =    \"Consistency-Threshold\" \":\" FLOAT\n                           CRLF\n"
  - 'clash-threshold       =    "Clash-Threshold" ":" FLOAT CRLF

    '
  - 'personal-grammar-uri  =    "Personal-Grammar-URI" ":" uri CRLF

    '
  - 'enroll-utterance      =    "Enroll-Utterance" ":" BOOLEAN CRLF

    '
  - 'phrase-id             =    "Phrase-ID" ":" 1*VCHAR CRLF

    '
  - 'phrase-nl             =    "Phrase-NL" ":" 1*UTFCHAR CRLF

    '
  - 'weight                =    "Weight" ":" FLOAT CRLF

    '
  - "save-best-waveform    =    \"Save-Best-Waveform\" \":\"\n                           BOOLEAN
    CRLF\n"
  - 'new-phrase-id         =    "New-Phrase-ID" ":" 1*VCHAR CRLF

    '
  - "confusable-phrases-uri =   \"Confusable-Phrases-URI\" \":\"\n                           uri
    CRLF\n"
  - "abort-phrase-enrollment =  \"Abort-Phrase-Enrollment\" \":\"\n                           BOOLEAN
    CRLF\n"
  - '; Recorder ABNF

    '
  - "recorder-method       =    \"RECORD\"\n                      /    \"STOP\"\n
    \                     /    \"START-INPUT-TIMERS\"\n"
  - "recorder-event        =    \"START-OF-INPUT\"\n                      /    \"RECORD-COMPLETE\"\n"
  - "recorder-header       =    sensitivity-level\n                      /    no-input-timeout\n
    \                     /    completion-cause\n                      /    completion-reason\n
    \                     /    failed-uri\n                      /    failed-uri-cause\n
    \                     /    record-uri\n                      /    media-type\n
    \                     /    max-time\n                      /    trim-length\n
    \                     /    final-silence\n                      /    capture-on-speech\n
    \                     /    ver-buffer-utterance\n                      /    start-input-timers\n
    \                     /    new-audio-channel\n"
  - "record-uri            =    \"Record-URI\" \":\" [ \"<\" uri \">\"\n                           \";\"
    \"size\" \"=\" 1*19DIGIT\n                           \";\" \"duration\" \"=\"
    1*19DIGIT] CRLF\n"
  - 'media-type            =    "Media-Type" ":" media-type-value CRLF

    '
  - 'max-time              =    "Max-Time" ":" 1*19DIGIT CRLF

    '
  - 'trim-length           =    "Trim-Length" ":" 1*19DIGIT CRLF

    '
  - 'final-silence         =    "Final-Silence" ":" 1*19DIGIT CRLF

    '
  - "capture-on-speech     =    \"Capture-On-Speech \" \":\"\n                           BOOLEAN
    CRLF\n"
  - '; Verifier ABNF

    '
  - "verifier-method       =    \"START-SESSION\"\n                      /    \"END-SESSION\"\n
    \                     /    \"QUERY-VOICEPRINT\"\n                      /    \"DELETE-VOICEPRINT\"\n
    \                     /    \"VERIFY\"\n                      /    \"VERIFY-FROM-BUFFER\"\n
    \                     /    \"VERIFY-ROLLBACK\"\n                      /    \"STOP\"\n
    \                     /    \"CLEAR-BUFFER\"\n                      /    \"START-INPUT-TIMERS\"\n
    \                     /    \"GET-INTERMEDIATE-RESULT\"\n"
  - "verifier-event        =    \"VERIFICATION-COMPLETE\"\n                      /
    \   \"START-OF-INPUT\"\n"
  - "verifier-header       =    repository-uri\n                      /    voiceprint-identifier\n
    \                     /    verification-mode\n                      /    adapt-model\n
    \                     /    abort-model\n                      /    min-verification-score\n
    \                     /    num-min-verification-phrases\n                      /
    \   num-max-verification-phrases\n                      /    no-input-timeout\n
    \                     /    save-waveform\n                      /    media-type\n
    \                     /    waveform-uri\n                      /    voiceprint-exists\n
    \                     /    ver-buffer-utterance\n                      /    input-waveform-uri\n
    \                     /    completion-cause\n                      /    completion-reason\n
    \                     /    speech-complete-timeout\n                      /    new-audio-channel\n
    \                     /    abort-verification\n                      /    start-input-timers\n
    \                     /    input-type\n"
  - 'repository-uri        =    "Repository-URI" ":" uri CRLF

    '
  - "voiceprint-identifier        =  \"Voiceprint-Identifier\" \":\"\n                                vid
    *[\";\" vid] CRLF\n"
  - 'vid                          =  1*VCHAR ["." 1*VCHAR]

    '
  - "verification-mode     =    \"Verification-Mode\" \":\"\n                           verification-mode-string\n"
  - 'verification-mode-string = "train" / "verify"

    '
  - 'adapt-model           =    "Adapt-Model" ":" BOOLEAN CRLF

    '
  - 'abort-model           =    "Abort-Model" ":" BOOLEAN CRLF

    '
  - "min-verification-score  =  \"Min-Verification-Score\" \":\"\n                           [
    %x2D ] FLOAT CRLF\n"
  - "num-min-verification-phrases = \"Num-Min-Verification-Phrases\"\n                               \":\"
    1*19DIGIT CRLF\n"
  - "num-max-verification-phrases = \"Num-Max-Verification-Phrases\"\n                               \":\"
    1*19DIGIT CRLF\n"
  - "voiceprint-exists     =    \"Voiceprint-Exists\" \":\"\n                           BOOLEAN
    CRLF\n"
  - "ver-buffer-utterance  =    \"Ver-Buffer-Utterance\" \":\"\n                           BOOLEAN
    CRLF\n"
  - 'input-waveform-uri    =    "Input-Waveform-URI" ":" uri CRLF

    '
  - "abort-verification    =    \"Abort-Verification \" \":\"\n                           BOOLEAN
    CRLF\n   The following productions add a new SDP session-level attribute.  See\n
    \  Paragraph 5.\n   cmid-attribute     =    \"a=cmid:\" identification-tag\n   identification-tag
    =    token\n"
  title: 15.  ABNF Normative Definition
- contents:
  - '16.  XML Schemas

    '
  - contents:
    - "16.1.  NLSML Schema Definition\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n
      <xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n             targetNamespace=\"urn:ietf:params:xml:ns:mrcpv2\"\n
      \            xmlns=\"urn:ietf:params:xml:ns:mrcpv2\"\n             elementFormDefault=\"qualified\"\n
      \            attributeFormDefault=\"unqualified\" >\n   <xs:annotation>\n     <xs:documentation>
      Natural Language Semantic Markup Schema\n     </xs:documentation>\n   </xs:annotation>\n
      \  <xs:include schemaLocation=\"enrollment-schema.rng\"/>\n   <xs:include schemaLocation=\"verification-schema.rng\"/>\n
      \  <xs:element name=\"result\">\n     <xs:complexType>\n       <xs:sequence>\n
      \        <xs:element name=\"interpretation\" maxOccurs=\"unbounded\">\n           <xs:complexType>\n
      \            <xs:sequence>\n               <xs:element name=\"instance\">\n
      \                <xs:complexType mixed=\"true\">\n                   <xs:sequence
      minOccurs=\"0\">\n                     <xs:any namespace=\"##other\" processContents=\"lax\"/>\n
      \                  </xs:sequence>\n                 </xs:complexType>\n               </xs:element>\n
      \              <xs:element name=\"input\" minOccurs=\"0\">\n                 <xs:complexType
      mixed=\"true\">\n                   <xs:choice>\n                     <xs:element
      name=\"noinput\" minOccurs=\"0\"/>\n                     <xs:element name=\"nomatch\"
      minOccurs=\"0\"/>\n                     <xs:element name=\"input\" minOccurs=\"0\"/>\n
      \                  </xs:choice>\n                   <xs:attribute name=\"mode\"\n
      \                                type=\"xs:string\"\n                                 default=\"speech\"/>\n
      \                  <xs:attribute name=\"confidence\"\n                                 type=\"confidenceinfo\"\n
      \                                default=\"1.0\"/>\n                   <xs:attribute
      name=\"timestamp-start\"\n                                 type=\"xs:string\"/>\n
      \                  <xs:attribute name=\"timestamp-end\"\n                                 type=\"xs:string\"/>\n
      \                </xs:complexType>\n               </xs:element>\n             </xs:sequence>\n
      \            <xs:attribute name=\"confidence\" type=\"confidenceinfo\"\n                           default=\"1.0\"/>\n
      \            <xs:attribute name=\"grammar\" type=\"xs:anyURI\"\n                           use=\"optional\"/>\n
      \          </xs:complexType>\n         </xs:element>\n         <xs:element name=\"enrollment-result\"\n
      \                    type=\"enrollment-contents\"/>\n         <xs:element name=\"verification-result\"\n
      \                    type=\"verification-contents\"/>\n       </xs:sequence>\n
      \      <xs:attribute name=\"grammar\" type=\"xs:anyURI\"\n                     use=\"optional\"/>\n
      \    </xs:complexType>\n   </xs:element>\n   <xs:simpleType name=\"confidenceinfo\">\n
      \    <xs:restriction base=\"xs:float\">\n        <xs:minInclusive value=\"0.0\"/>\n
      \       <xs:maxInclusive value=\"1.0\"/>\n     </xs:restriction>\n   </xs:simpleType>\n
      </xs:schema>\n"
    title: 16.1.  NLSML Schema Definition
  - contents:
    - "16.2.  Enrollment Results Schema Definition\n   <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n
      \  <!-- MRCP Enrollment Schema\n   (See http://www.oasis-open.org/committees/relax-ng/spec.html)\n
      \  -->\n   <grammar datatypeLibrary=\"http://www.w3.org/2001/XMLSchema-datatypes\"\n
      \           ns=\"urn:ietf:params:xml:ns:mrcpv2\"\n            xmlns=\"http://relaxng.org/ns/structure/1.0\">\n
      \    <start>\n       <element name=\"enrollment-result\">\n         <ref name=\"enrollment-content\"/>\n
      \      </element>\n     </start>\n     <define name=\"enrollment-content\">\n
      \      <interleave>\n         <element name=\"num-clashes\">\n           <data
      type=\"nonNegativeInteger\"/>\n         </element>\n         <element name=\"num-good-repetitions\">\n
      \          <data type=\"nonNegativeInteger\"/>\n         </element>\n         <element
      name=\"num-repetitions-still-needed\">\n           <data type=\"nonNegativeInteger\"/>\n
      \        </element>\n         <element name=\"consistency-status\">\n           <choice>\n
      \            <value>consistent</value>\n             <value>inconsistent</value>\n
      \            <value>undecided</value>\n           </choice>\n         </element>\n
      \        <optional>\n           <element name=\"clash-phrase-ids\">\n             <oneOrMore>\n
      \              <element name=\"item\">\n                 <data type=\"token\"/>\n
      \              </element>\n             </oneOrMore>\n           </element>\n
      \        </optional>\n         <optional>\n           <element name=\"transcriptions\">\n
      \            <oneOrMore>\n               <element name=\"item\">\n                 <text/>\n
      \              </element>\n             </oneOrMore>\n           </element>\n
      \        </optional>\n         <optional>\n           <element name=\"confusable-phrases\">\n
      \            <oneOrMore>\n               <element name=\"item\">\n                 <text/>\n
      \              </element>\n             </oneOrMore>\n           </element>\n
      \        </optional>\n       </interleave>\n     </define>\n   </grammar>\n"
    title: 16.2.  Enrollment Results Schema Definition
  - contents:
    - "16.3.  Verification Results Schema Definition\n   <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n
      \  <!--    MRCP Verification Results Schema\n           (See http://www.oasis-open.org/committees/relax-ng/spec.html)\n
      \     -->\n   <grammar datatypeLibrary=\"http://www.w3.org/2001/XMLSchema-datatypes\"\n
      \           ns=\"urn:ietf:params:xml:ns:mrcpv2\"\n            xmlns=\"http://relaxng.org/ns/structure/1.0\">\n
      \    <start>\n       <element name=\"verification-result\">\n         <ref name=\"verification-contents\"/>\n
      \      </element>\n     </start>\n     <define name=\"verification-contents\">\n
      \      <element name=\"voiceprint\">\n         <ref name=\"firstVoiceprintContent\"/>\n
      \      </element>\n       <zeroOrMore>\n         <element name=\"voiceprint\">\n
      \          <ref name=\"restVoiceprintContent\"/>\n         </element>\n       </zeroOrMore>\n
      \    </define>\n     <define name=\"firstVoiceprintContent\">\n       <attribute
      name=\"id\">\n         <data type=\"string\"/>\n       </attribute>\n       <interleave>\n
      \        <optional>\n           <element name=\"adapted\">\n             <data
      type=\"boolean\"/>\n           </element>\n         </optional>\n         <optional>\n
      \          <element name=\"needmoredata\">\n             <ref name=\"needmoredataContent\"/>\n
      \          </element>\n         </optional>\n         <optional>\n           <element
      name=\"incremental\">\n             <ref name=\"firstCommonContent\"/>\n           </element>\n
      \        </optional>\n         <element name=\"cumulative\">\n           <ref
      name=\"firstCommonContent\"/>\n         </element>\n       </interleave>\n     </define>\n
      \    <define name=\"restVoiceprintContent\">\n       <attribute name=\"id\">\n
      \        <data type=\"string\"/>\n       </attribute>\n       <element name=\"cumulative\">\n
      \        <ref name=\"restCommonContent\"/>\n       </element>\n     </define>\n
      \    <define name=\"firstCommonContent\">\n       <interleave>\n         <element
      name=\"decision\">\n           <ref name=\"decisionContent\"/>\n         </element>\n
      \        <optional>\n           <element name=\"utterance-length\">\n             <ref
      name=\"utterance-lengthContent\"/>\n           </element>\n         </optional>\n
      \        <optional>\n           <element name=\"device\">\n             <ref
      name=\"deviceContent\"/>\n           </element>\n         </optional>\n         <optional>\n
      \          <element name=\"gender\">\n             <ref name=\"genderContent\"/>\n
      \          </element>\n         </optional>\n         <zeroOrMore>\n           <element
      name=\"verification-score\">\n             <ref name=\"verification-scoreContent\"/>\n
      \          </element>\n         </zeroOrMore>\n       </interleave>\n     </define>\n
      \    <define name=\"restCommonContent\">\n       <interleave>\n         <optional>\n
      \          <element name=\"decision\">\n             <ref name=\"decisionContent\"/>\n
      \          </element>\n         </optional>\n         <optional>\n           <element
      name=\"device\">\n             <ref name=\"deviceContent\"/>\n           </element>\n
      \        </optional>\n         <optional>\n           <element name=\"gender\">\n
      \            <ref name=\"genderContent\"/>\n           </element>\n         </optional>\n
      \       <zeroOrMore>\n           <element name=\"verification-score\">\n             <ref
      name=\"verification-scoreContent\"/>\n           </element>\n        </zeroOrMore>\n
      \       </interleave>\n     </define>\n     <define name=\"decisionContent\">\n
      \      <choice>\n         <value>accepted</value>\n         <value>rejected</value>\n
      \        <value>undecided</value>\n       </choice>\n     </define>\n     <define
      name=\"needmoredataContent\">\n       <data type=\"boolean\"/>\n     </define>\n
      \    <define name=\"utterance-lengthContent\">\n       <data type=\"nonNegativeInteger\"/>\n
      \    </define>\n     <define name=\"deviceContent\">\n       <choice>\n         <value>cellular-phone</value>\n
      \        <value>electret-phone</value>\n         <value>carbon-button-phone</value>\n
      \        <value>unknown</value>\n       </choice>\n     </define>\n     <define
      name=\"genderContent\">\n       <choice>\n         <value>male</value>\n         <value>female</value>\n
      \        <value>unknown</value>\n       </choice>\n     </define>\n     <define
      name=\"verification-scoreContent\">\n       <data type=\"float\">\n         <param
      name=\"minInclusive\">-1</param>\n         <param name=\"maxInclusive\">1</param>\n
      \      </data>\n     </define>\n   </grammar>\n"
    title: 16.3.  Verification Results Schema Definition
  title: 16.  XML Schemas
- contents:
  - '17.  References

    '
  - contents:
    - "17.1.  Normative References\n   [ISO.8859-1.1987]\n              International
      Organization for Standardization,\n              \"Information technology -
      8-bit single byte coded graphic\n              - character sets - Part 1: Latin
      alphabet No. 1, JTC1/\n              SC2\", ISO Standard 8859-1, 1987.\n   [RFC0793]
      \ Postel, J., \"Transmission Control Protocol\", STD 7,\n              RFC 793,
      September 1981.\n   [RFC1035]  Mockapetris, P., \"Domain names - implementation
      and\n              specification\", STD 13, RFC 1035, November 1987.\n   [RFC2119]
      \ Bradner, S., \"Key words for use in RFCs to Indicate\n              Requirement
      Levels\", BCP 14, RFC 2119, March 1997.\n   [RFC2326]  Schulzrinne, H., Rao,
      A., and R. Lanphier, \"Real Time\n              Streaming Protocol (RTSP)\",
      RFC 2326, April 1998.\n   [RFC2392]  Levinson, E., \"Content-ID and Message-ID
      Uniform Resource\n              Locators\", RFC 2392, August 1998.\n   [RFC2483]
      \ Mealling, M. and R. Daniel, \"URI Resolution Services\n              Necessary
      for URN Resolution\", RFC 2483, January 1999.\n   [RFC2616]  Fielding, R., Gettys,
      J., Mogul, J., Frystyk, H.,\n              Masinter, L., Leach, P., and T. Berners-Lee,
      \"Hypertext\n              Transfer Protocol -- HTTP/1.1\", RFC 2616, June 1999.\n
      \  [RFC3023]  Murata, M., St. Laurent, S., and D. Kohn, \"XML Media\n              Types\",
      RFC 3023, January 2001.\n   [RFC3261]  Rosenberg, J., Schulzrinne, H., Camarillo,
      G., Johnston,\n              A., Peterson, J., Sparks, R., Handley, M., and
      E.\n              Schooler, \"SIP: Session Initiation Protocol\", RFC 3261,\n
      \             June 2002.\n   [RFC3264]  Rosenberg, J. and H. Schulzrinne, \"An
      Offer/Answer Model\n              with Session Description Protocol (SDP)\",
      RFC 3264,\n              June 2002.\n   [RFC3550]  Schulzrinne, H., Casner,
      S., Frederick, R., and V.\n              Jacobson, \"RTP: A Transport Protocol
      for Real-Time\n              Applications\", STD 64, RFC 3550, July 2003.\n
      \  [RFC3629]  Yergeau, F., \"UTF-8, a transformation format of ISO\n              10646\",
      STD 63, RFC 3629, November 2003.\n   [RFC3688]  Mealling, M., \"The IETF XML
      Registry\", BCP 81, RFC 3688,\n              January 2004.\n   [RFC3711]  Baugher,
      M., McGrew, D., Naslund, M., Carrara, E., and K.\n              Norrman, \"The
      Secure Real-time Transport Protocol (SRTP)\",\n              RFC 3711, March
      2004.\n   [RFC3986]  Berners-Lee, T., Fielding, R., and L. Masinter, \"Uniform\n
      \             Resource Identifier (URI): Generic Syntax\", STD 66,\n              RFC
      3986, January 2005.\n   [RFC4145]  Yon, D. and G. Camarillo, \"TCP-Based Media
      Transport in\n              the Session Description Protocol (SDP)\", RFC 4145,\n
      \             September 2005.\n   [RFC4288]  Freed, N. and J. Klensin, \"Media
      Type Specifications and\n              Registration Procedures\", BCP 13, RFC
      4288, December 2005.\n   [RFC4566]  Handley, M., Jacobson, V., and C. Perkins,
      \"SDP: Session\n              Description Protocol\", RFC 4566, July 2006.\n
      \  [RFC4568]  Andreasen, F., Baugher, M., and D. Wing, \"Session\n              Description
      Protocol (SDP) Security Descriptions for Media\n              Streams\", RFC
      4568, July 2006.\n   [RFC4572]  Lennox, J., \"Connection-Oriented Media Transport
      over the\n              Transport Layer Security (TLS) Protocol in the Session\n
      \             Description Protocol (SDP)\", RFC 4572, July 2006.\n   [RFC5226]
      \ Narten, T. and H. Alvestrand, \"Guidelines for Writing an\n              IANA
      Considerations Section in RFCs\", BCP 26, RFC 5226,\n              May 2008.\n
      \  [RFC5234]  Crocker, D. and P. Overell, \"Augmented BNF for Syntax\n              Specifications:
      ABNF\", STD 68, RFC 5234, January 2008.\n   [RFC5246]  Dierks, T. and E. Rescorla,
      \"The Transport Layer Security\n              (TLS) Protocol Version 1.2\",
      RFC 5246, August 2008.\n   [RFC5322]  Resnick, P., Ed., \"Internet Message Format\",
      RFC 5322,\n              October 2008.\n   [RFC5646]  Phillips, A. and M. Davis,
      \"Tags for Identifying\n              Languages\", BCP 47, RFC 5646, September
      2009.\n   [RFC5888]  Camarillo, G. and H. Schulzrinne, \"The Session Description\n
      \             Protocol (SDP) Grouping Framework\", RFC 5888, June 2010.\n   [RFC5905]
      \ Mills, D., Martin, J., Burbank, J., and W. Kasch, \"Network\n              Time
      Protocol Version 4: Protocol and Algorithms\n              Specification\",
      RFC 5905, June 2010.\n   [RFC5922]  Gurbani, V., Lawrence, S., and A. Jeffrey,
      \"Domain\n              Certificates in the Session Initiation Protocol (SIP)\",\n
      \             RFC 5922, June 2010.\n   [RFC6265]  Barth, A., \"HTTP State Management
      Mechanism\", RFC 6265,\n              April 2011.\n   [W3C.REC-semantic-interpretation-20070405]\n
      \             Tichelen, L. and D. Burke, \"Semantic Interpretation for\n              Speech
      Recognition (SISR) Version 1.0\", World Wide Web\n              Consortium Recommendation
      REC-semantic-\n              interpretation-20070405, April 2007,\n              <http://www.w3.org/TR/2007/\n
      \             REC-semantic-interpretation-20070405>.\n   [W3C.REC-speech-grammar-20040316]\n
      \             McGlashan, S. and A. Hunt, \"Speech Recognition Grammar\n              Specification
      Version 1.0\", World Wide Web Consortium\n              Recommendation REC-speech-grammar-20040316,
      March 2004,\n              <http://www.w3.org/TR/2004/REC-speech-grammar-20040316>.\n
      \  [W3C.REC-speech-synthesis-20040907]\n              Walker, M., Burnett, D.,
      and A. Hunt, \"Speech Synthesis\n              Markup Language (SSML) Version
      1.0\", World Wide Web\n              Consortium Recommendation REC-speech-synthesis-20040907,\n
      \             September 2004,\n              <http://www.w3.org/TR/2004/REC-speech-synthesis-20040907>.\n
      \  [W3C.REC-xml-names11-20040204]\n              Layman, A., Bray, T., Hollander,
      D., and R. Tobin,\n              \"Namespaces in XML 1.1\", World Wide Web Consortium
      First\n              Edition REC-xml-names11-20040204, February 2004,\n              <http://www.w3.org/TR/2004/REC-xml-names11-20040204>.\n"
    title: 17.1.  Normative References
  - contents:
    - "17.2.  Informative References\n   [ISO.8601.1988]\n              International
      Organization for Standardization, \"Data\n              elements and interchange
      formats - Information interchange\n              - Representation of dates and
      times\", ISO Standard 8601,\n              June 1988.\n   [Q.23]     International
      Telecommunications Union, \"Technical\n              Features of Push-Button
      Telephone Sets\", ITU-T Q.23, 1993.\n   [RFC2046]  Freed, N. and N. Borenstein,
      \"Multipurpose Internet Mail\n              Extensions (MIME) Part Two: Media
      Types\", RFC 2046,\n              November 1996.\n   [RFC2818]  Rescorla, E.,
      \"HTTP Over TLS\", RFC 2818, May 2000.\n   [RFC4217]  Ford-Hutchinson, P., \"Securing
      FTP with TLS\", RFC 4217,\n              October 2005.\n   [RFC4267]  Froumentin,
      M., \"The W3C Speech Interface Framework Media\n              Types: application/voicexml+xml,
      application/ssml+xml,\n              application/srgs, application/srgs+xml,
      application/\n              ccxml+xml, and application/pls+xml\", RFC 4267,\n
      \             November 2005.\n   [RFC4301]  Kent, S. and K. Seo, \"Security
      Architecture for the\n              Internet Protocol\", RFC 4301, December
      2005.\n   [RFC4313]  Oran, D., \"Requirements for Distributed Control of\n              Automatic
      Speech Recognition (ASR), Speaker\n              Identification/Speaker Verification
      (SI/SV), and Text-to-\n              Speech (TTS) Resources\", RFC 4313, December
      2005.\n   [RFC4395]  Hansen, T., Hardie, T., and L. Masinter, \"Guidelines and\n
      \             Registration Procedures for New URI Schemes\", BCP 35,\n              RFC
      4395, February 2006.\n   [RFC4463]  Shanmugham, S., Monaco, P., and B. Eberman,
      \"A Media\n              Resource Control Protocol (MRCP) Developed by Cisco,\n
      \             Nuance, and Speechworks\", RFC 4463, April 2006.\n   [RFC4467]
      \ Crispin, M., \"Internet Message Access Protocol (IMAP) -\n              URLAUTH
      Extension\", RFC 4467, May 2006.\n   [RFC4733]  Schulzrinne, H. and T. Taylor,
      \"RTP Payload for DTMF\n              Digits, Telephony Tones, and Telephony
      Signals\", RFC 4733,\n              December 2006.\n   [RFC4960]  Stewart, R.,
      \"Stream Control Transmission Protocol\",\n              RFC 4960, September
      2007.\n   [RFC6454]  Barth, A., \"The Web Origin Concept\", RFC 6454,\n              December
      2011.\n   [W3C.REC-emma-20090210]\n              Johnston, M., Baggia, P., Burnett,
      D., Carter, J., Dahl,\n              D., McCobb, G., and D. Raggett, \"EMMA:
      Extensible\n              MultiModal Annotation markup language\", World Wide
      Web\n              Consortium Recommendation REC-emma-20090210,\n              February
      2009,\n              <http://www.w3.org/TR/2009/REC-emma-20090210>.\n   [W3C.REC-pronunciation-lexicon-20081014]\n
      \             Baggia, P., Bagshaw, P., Burnett, D., Carter, J., and F.\n              Scahill,
      \"Pronunciation Lexicon Specification (PLS)\",\n              World Wide Web
      Consortium Recommendation\n              REC-pronunciation-lexicon-20081014,
      October 2008,\n              <http://www.w3.org/TR/2008/\n              REC-pronunciation-lexicon-20081014>.\n
      \  [W3C.REC-voicexml20-20040316]\n              Danielsen, P., Porter, B., Hunt,
      A., Rehor, K., Lucas, B.,\n              Burnett, D., Ferrans, J., Tryphonas,
      S., McGlashan, S.,\n              and J. Carter, \"Voice Extensible Markup Language\n
      \             (VoiceXML) Version 2.0\", World Wide Web Consortium\n              Recommendation
      REC-voicexml20-20040316, March 2004,\n              <http://www.w3.org/TR/2004/REC-voicexml20-20040316>.\n
      \  [refs.javaSpeechGrammarFormat]\n              Sun Microsystems, \"Java Speech
      Grammar Format Version\n              1.0\", October 1998.\n"
    title: 17.2.  Informative References
  title: 17.  References
- contents:
  - "Appendix A.  Contributors\n   Pierre Forgues\n   Nuance Communications Ltd.\n
    \  1500 University Street\n   Suite 935\n   Montreal, Quebec\n   Canada H3A 3S7\n
    \  EMail:  forgues@nuance.com\n   Charles Galles\n   Intervoice, Inc.\n   17811
    Waterview Parkway\n   Dallas, Texas 75252\n   USA\n   EMail:  charles.galles@intervoice.com\n
    \  Klaus Reifenrath\n   Scansoft, Inc\n   Guldensporenpark 32\n   Building D\n
    \  9820 Merelbeke\n   Belgium\n   EMail: klaus.reifenrath@scansoft.com\n"
  title: Appendix A.  Contributors
- contents:
  - "Appendix B.  Acknowledgements\n   Andre Gillet (Nuance Communications)\n   Andrew
    Hunt (ScanSoft)\n   Andrew Wahbe (Genesys)\n   Aaron Kneiss (ScanSoft)\n   Brian
    Eberman (ScanSoft)\n   Corey Stohs (Cisco Systems, Inc.)\n   Dave Burke (VoxPilot)\n
    \  Jeff Kusnitz (IBM Corp)\n   Ganesh N. Ramaswamy (IBM Corp)\n   Klaus Reifenrath
    (ScanSoft)\n   Kristian Finlator (ScanSoft)\n   Magnus Westerlund (Ericsson)\n
    \  Martin Dragomirecky (Cisco Systems, Inc.)\n   Paolo Baggia (Loquendo)\n   Peter
    Monaco (Nuance Communications)\n   Pierre Forgues (Nuance Communications)\n   Ran
    Zilca (IBM Corp)\n   Suresh Kaliannan (Cisco Systems, Inc.)\n   Skip Cave (Intervoice,
    Inc.)\n   Thomas Gal (LumenVox)\n   The chairs of the SPEECHSC work group are
    Eric Burger (Georgetown\n   University) and Dave Oran (Cisco Systems, Inc.).\n
    \  Many thanks go in particular to Robert Sparks, Alex Agranovsky, and\n   Henry
    Phan, who were there at the end to dot all the i's and cross\n   all the t's.\n"
  title: Appendix B.  Acknowledgements
- contents:
  - "Authors' Addresses\n   Daniel C. Burnett\n   Voxeo\n   189 South Orange Avenue
    #1000\n   Orlando, FL  32801\n   USA\n   EMail: dburnett@voxeo.com\n   Saravanan
    Shanmugham\n   Cisco Systems, Inc.\n   170 W. Tasman Dr.\n   San Jose, CA  95134\n
    \  USA\n   EMail: sarvi@cisco.com\n"
  title: Authors' Addresses
