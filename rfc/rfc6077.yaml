- title: __initial_text__
  contents:
  - ''
- title: Internet Research Task Force (IRTF)                D. Papadimitriou, Ed.
  contents:
  - "Internet Research Task Force (IRTF)                D. Papadimitriou, Ed.\n  \
    \        Open Research Issues in Internet Congestion Control\n"
- title: Abstract
  contents:
  - "Abstract\n   This document describes some of the open problems in Internet\n\
    \   congestion control that are known today.  This includes several new\n   challenges\
    \ that are becoming important as the network grows, as well\n   as some issues\
    \ that have been known for many years.  These challenges\n   are generally considered\
    \ to be open research topics that may require\n   more study or application of\
    \ innovative techniques before Internet-\n   scale solutions can be confidently\
    \ engineered and deployed.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Research Task Force\n   (IRTF).  The IRTF publishes the results\
    \ of Internet-related research\n   and development activities.  These results\
    \ might not be suitable for\n   deployment.  This RFC represents the consensus\
    \ of the Internet\n   Congestion Control Research Group (ICCRG) of the Internet\
    \ Research\n   Task Force (IRTF).  Documents approved for publication by the IRSG\n\
    \   are not a candidate for any level of Internet Standard; see Section 2\n  \
    \ of RFC 5741.\n   Information about the current status of this document, any\
    \ errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc6077.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2011 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \   2. Global Challenges ...............................................5\n  \
    \    2.1. Heterogeneity ..............................................5\n    \
    \  2.2. Stability ..................................................7\n      2.3.\
    \ Fairness ...................................................8\n   3. Detailed\
    \ Challenges ............................................10\n      3.1. Challenge\
    \ 1: Network Support ..............................10\n           3.1.1. Performance\
    \ and Robustness .........................14\n           3.1.2. Granularity of\
    \ Network Component Functions .........15\n           3.1.3. Information Acquisition\
    \ ............................16\n           3.1.4. Feedback Signaling .................................17\n\
    \      3.2. Challenge 2: Corruption Loss ..............................17\n  \
    \    3.3. Challenge 3: Packet Size ..................................19\n    \
    \  3.4. Challenge 4: Flow Startup .................................24\n      3.5.\
    \ Challenge 5: Multi-Domain Congestion Control ..............26\n           3.5.1.\
    \ Multi-Domain Transport of Explicit\n                  Congestion Notification\
    \ ............................26\n           3.5.2. Multi-Domain Exchange of Topology\
    \ or\n                  Explicit Rate Information ..........................27\n\
    \           3.5.3. Multi-Domain Pseudowires ...........................28\n  \
    \    3.6. Challenge 6: Precedence for Elastic Traffic ...............30\n    \
    \  3.7. Challenge 7: Misbehaving Senders and Receivers ............31\n      3.8.\
    \ Other Challenges ..........................................33\n           3.8.1.\
    \ RTT Estimation .....................................33\n           3.8.2. Malfunctioning\
    \ Devices .............................35\n           3.8.3. Dependence on RTT\
    \ ..................................36\n           3.8.4. Congestion Control in\
    \ Multi-Layered Networks .......36\n           3.8.5. Multipath End-to-End Congestion\
    \ Control and\n                  Traffic Engineering ................................37\n\
    \           3.8.6. ALGs and Middleboxes ...............................37\n  \
    \ 4. Security Considerations ........................................38\n   5.\
    \ References .....................................................39\n      5.1.\
    \ Informative References ....................................39\n   6. Acknowledgments\
    \ ................................................50\n   7. Contributors ...................................................50\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document, the result of the Internet Congestion Control\
    \ Research\n   Group (ICCRG), describes some of the open research topics in the\n\
    \   domain of Internet congestion control that are known today.  We begin\n  \
    \ by reviewing some proposed definitions of congestion and congestion\n   control\
    \ based on current understandings.\n   Congestion can be defined as a state or\
    \ condition that occurs when\n   network resources are overloaded, resulting in\
    \ impairments for\n   network users as objectively measured by the probability\
    \ of loss\n   and/or delay.  The overload results in the reduction of utility\
    \ in\n   networks that support both spatial and temporal multiplexing, but no\n\
    \   reservation [Keshav07].  Congestion control is a (typically\n   distributed)\
    \ algorithm to share network resources among competing\n   traffic sources.\n\
    \   Two components of distributed congestion control have been defined in\n  \
    \ the context of primal-dual modeling [Kelly98].  Primal congestion\n   control\
    \ refers to the algorithm executed by the traffic sources for\n   controlling\
    \ their sending rates or window sizes.  This is normally a\n   closed-loop control,\
    \ where this operation depends on feedback.  TCP\n   algorithms fall in this category.\
    \  Dual congestion control is\n   implemented by the routers through gathering\
    \ information about the\n   traffic traversing them.  A dual congestion control\
    \ algorithm\n   updates, implicitly or explicitly, a congestion measure or congestion\n\
    \   rate and sends it back, implicitly or explicitly, to the traffic\n   sources\
    \ that use that link.  Queue management algorithms such as\n   Random Early Detection\
    \ (RED) [Floyd93] or Random Exponential Marking\n   (REM) [Ath01] fall into the\
    \ \"dual\" category.\n   Congestion control provides for a fundamental set of\
    \ mechanisms for\n   maintaining the stability and efficiency of the Internet.\
    \  Congestion\n   control has been associated with TCP since Van Jacobson's work\
    \ in\n   1988, but there is also congestion control outside of TCP (e.g., for\n\
    \   real-time multimedia applications, multicast, and router-based\n   mechanisms)\
    \ [RFC5783].  The Van Jacobson end-to-end congestion\n   control algorithms [Jacobson88]\
    \ [RFC2581] [RFC5681] are used by the\n   Internet transport protocol TCP [RFC4614].\
    \  They have been proven to\n   be highly successful over many years but have\
    \ begun to reach their\n   limits, as the heterogeneity of the data link and physical\
    \ layer on\n   the one hand, and of applications on the other, are pulling TCP\n\
    \   congestion control beyond its natural operating regime.  This is\n   because\
    \ it performs poorly as the bandwidth or delay increases.  A\n   side effect of\
    \ these deficiencies is that an increasing share of\n   hosts use non-standardized\
    \ congestion control enhancements (for\n   instance, many Linux distributions\
    \ have been shipped with \"CUBIC\"\n   [Ha08] as the default TCP congestion control\
    \ mechanism).\n   While the original Van Jacobson algorithm requires no congestion-\n\
    \   related state in routers, more recent modifications have departed\n   from\
    \ the strict application of the end-to-end principle [Saltzer84]\n   in order\
    \ to avoid congestion collapse.  Active Queue Management (AQM)\n   in routers,\
    \ e.g., RED and some of its variants such as Adaptive RED\n   (ARED), improves\
    \ performance by keeping queues small (implicit\n   feedback via dropped packets),\
    \ while Explicit Congestion Notification\n   (ECN) [Floyd94] [RFC3168] passes\
    \ one bit of congestion information\n   back to senders when an AQM would normally\
    \ drop a packet.  It is to\n   be noted that other variants of RED built on AQM,\
    \ such as Weighted\n   RED (WRED) and RED with In/Out (RIO) [Clark98] are for\
    \ quality\n   enforcement, whereas Stabilized RED (SRED), and CHOKe [Pan00] and\
    \ its\n   extensions such as XCHOKe [Chhabra02], are flow policers.  In\n   [Bonald00],\
    \ authors analytically evaluated RED performance.\n   These measures do improve\
    \ performance, but there is a limit to how\n   much can be accomplished without\
    \ more information from routers.  The\n   requirement of extreme scalability together\
    \ with robustness has been\n   a difficult hurdle for acceleration of this information\
    \ flow.\n   Primal-dual TCP/AQM distributed algorithm stability and equilibrium\n\
    \   properties have been extensively studied (cf. [Low02], [Low03.1],\n   [Low03.2],\
    \ [Kelly98], and [Kelly05]).\n   Congestion control includes many new challenges\
    \ that are becoming\n   important as the network grows, in addition to the issues\
    \ that have\n   been known for many years.  These are generally considered to\
    \ be open\n   research topics that may require more study or application of\n\
    \   innovative techniques before Internet-scale solutions can be\n   confidently\
    \ engineered and deployed.  In what follows, an overview of\n   some of these\
    \ challenges is given.\n"
- title: 2.  Global Challenges
  contents:
  - "2.  Global Challenges\n   This section describes the global challenges to be\
    \ addressed in the\n   domain of Internet congestion control.\n"
- title: 2.1.  Heterogeneity
  contents:
  - "2.1.  Heterogeneity\n   The Internet encompasses a large variety of heterogeneous\
    \ IP networks\n   that are realized by a multitude of technologies, which result\
    \ in a\n   tremendous variety of link and path characteristics: capacity can be\n\
    \   either scarce in very-slow-speed radio links (several kbps), or there\n  \
    \ may be an abundant supply in high-speed optical links (several\n   gigabit per\
    \ second).  Concerning latency, scenarios range from local\n   interconnects (much\
    \ less than a millisecond) to certain wireless and\n   satellite links with very\
    \ large latencies up to or over a second).\n   Even higher latencies can occur\
    \ in space communication.  As a\n   consequence, both the available bandwidth\
    \ and the end-to-end delay in\n   the Internet may vary over many orders of magnitude,\
    \ and it is likely\n   that the range of parameters will further increase in the\
    \ future.\n   Additionally, neither the available bandwidth nor the end-to-end\n\
    \   delay is constant.  At the IP layer, competing cross-traffic, traffic\n  \
    \ management in routers, and dynamic routing can result in sudden\n   changes\
    \ in the characteristics of an end-to-end path.  Additional\n   dynamics can be\
    \ caused by link layer mechanisms, such as shared-media\n   access (e.g., in wireless\
    \ networks), changes to new links due to\n   mobility (horizontal/vertical handovers),\
    \ topology modifications\n   (e.g., in ad hoc or meshed networks), link layer\
    \ error correction,\n   and dynamic bandwidth provisioning schemes.  From this,\
    \ it follows\n   that path characteristics can be subject to substantial changes\n\
    \   within short time frames.\n   Congestion control algorithms have to deal with\
    \ this variety in an\n   efficient and stable way.  The congestion control principles\n\
    \   introduced by Van Jacobson assume a rather static scenario and\n   implicitly\
    \ target configurations where the bandwidth-delay product is\n   of the order\
    \ of some dozens of packets at most.  While these\n   principles have proved to\
    \ work in the Internet for almost two\n   decades, much larger bandwidth-delay\
    \ products and increased dynamics\n   challenge them more and more.  There are\
    \ many situations where\n   today's congestion control algorithms react in a suboptimal\
    \ way,\n   resulting, among other things, in low resource utilization.\n   This\
    \ has resulted in a multitude of new proposals for congestion\n   control algorithms.\
    \  For instance, since the Additive Increase\n   Multiplicative Decrease (AIMD)\
    \ behavior of TCP is too conservative in\n   practical environments when the congestion\
    \ window is large, several\n   high-speed congestion control extensions have been\
    \ developed.\n   However, these new algorithms may be less robust or starve legacy\n\
    \   flows in certain situations for which they have not been designed.\n   At\
    \ the time of writing, there is no common agreement in the IETF on\n   which algorithm(s)\
    \ and protocol(s) to choose.\n   It is always possible to tune congestion control\
    \ parameters based on\n   some knowledge of the environment and the application\
    \ scenario.\n   However, the interaction between multiple congestion control\n\
    \   techniques is not yet well understood.  The fundamental challenge is\n   whether\
    \ it is possible to define one congestion control mechanism\n   that operates\
    \ reasonably well in a whole range of scenarios that\n   exist in the Internet.\
    \  Hence, important research questions are how\n   new Internet congestion control\
    \ mechanisms would have to be designed,\n   which maximum degree of dynamics they\
    \ can efficiently handle, and\n   whether they can keep the generality of the\
    \ existing end-to-end\n   solutions.\n   Some improvements to congestion control\
    \ could be realized by simple\n   changes to single functions in end-systems or\
    \ optimizations of\n   network components.  However, new mechanism(s) might also\
    \ require a\n   fundamental redesign of the overall network architecture, and\
    \ they\n   may even affect the design of Internet applications.  This can imply\n\
    \   significant interoperability and backward compatibility challenges\n   and/or\
    \ create network accessibility obstacles.  In particular,\n   networks and/or\
    \ applications that do not use or support a new\n   congestion control mechanism\
    \ could be penalized by a significantly\n   worse performance compared to what\
    \ they would get if everybody used\n   the existing mechanisms (cf. the discussion\
    \ on fairness in\n   Section 2.3).  [RFC5033] defines several criteria to evaluate\
    \ the\n   appropriateness of a new congestion control mechanism.  However, a\n\
    \   key issue is how much performance deterioration is acceptable for\n   \"legacy\"\
    \ applications.  This tradeoff between performance and cost\n   has to be very\
    \ carefully examined for all new congestion control\n   schemes.\n"
- title: 2.2.  Stability
  contents:
  - "2.2.  Stability\n   Control theory is a mathematical tool for describing dynamic\
    \ systems.\n   It lends itself to modeling congestion control -- TCP is a perfect\n\
    \   example of a typical \"closed loop\" system that can be described in\n   control\
    \ theoretic terms.  However, control theory has had to be\n   extended to model\
    \ the interactions between multiple control loops in\n   a network [Vinnic02].\
    \  In control theory, there is a mathematically\n   defined notion of system stability.\
    \  In a stable system, for any\n   bounded input over any amount of time, the\
    \ output will also be\n   bounded.  For congestion control, what is actually meant\
    \ by global\n   stability is typically asymptotic stability: a mechanism should\n\
    \   converge to a certain state irrespective of the initial state of the\n   network.\
    \  Local stability means that if the system is perturbed from\n   its stable state\
    \ it will quickly return toward the locally stable\n   state.\n   Some fundamental\
    \ facts known from control theory are useful as\n   guidelines when designing\
    \ a congestion control mechanism.  For\n   instance, a controller should only\
    \ be fed a system state that\n   reflects its output.  A (low-pass) filter function\
    \ should be used in\n   order to pass to the controller only states that are expected\
    \ to last\n   long enough for its action to be meaningful [Jain88].  Action should\n\
    \   be carried out whenever such feedback arrives, as it is a fundamental\n  \
    \ principle of control that the control frequency should ideally be\n   equal\
    \ to the feedback frequency.  Reacting faster leads to\n   oscillations and instability,\
    \ while reacting more slowly makes the\n   system tardy [Jain90].\n   Control\
    \ theoretic modeling of a realistic network can be quite\n   difficult, especially\
    \ when taking distinct packet sizes and\n   heterogeneous round-trip times (RTTs)\
    \ into account.  It has therefore\n   become common practice to model simpler\
    \ cases and to leave the more\n   complicated (realistic) situations for simulations.\
    \  Clearly, if a\n   mechanism is not stable in a simple scenario, it is generally\n\
    \   useless; this method therefore helps to eliminate faulty congestion\n   control\
    \ candidates at an early stage.  However, a mechanism that is\n   found to be\
    \ stable in simulations can still not be safely deployed in\n   real networks,\
    \ since simulation scenarios make simplifying\n   assumptions.\n   TCP stability\
    \ can be attributed to two key aspects that were\n   introduced in [Jacobson88]:\
    \ the AIMD control law during congestion\n   avoidance, which is based on a simple,\
    \ vector-based analysis of two\n   controllers sharing one resource with synchronous\
    \ RTTs [Chiu89]; and\n   the \"conservation of packets principle\", which, once\
    \ the control has\n   reached \"steady state\", tries to maintain an equal amount\
    \ of packets\n   in flight at any time by only sending a packet into the network\
    \ when\n   a packet has left the network (as indicated by an ACK arriving at the\n\
    \   sender).  The latter aspect has guided many decisions regarding\n   changes\
    \ that were made to TCP over the years.\n   The reasoning in [Jacobson88] assumes\
    \ all senders to be acting at the\n   same time.  The stability of TCP under more\
    \ realistic network\n   conditions has been investigated in a large number of\
    \ ensuing works,\n   leading to no clear conclusion that TCP would also be asymptotically\n\
    \   stable under arbitrary network conditions.  On the other hand,\n   research\
    \ has concluded that stability can be assured with constraints\n   on dynamics\
    \ that are less stringent than the \"conservation of packets\n   principle\".\
    \  From control theory, only rate increase (not the target\n   rate) needs to\
    \ be inversely proportional to RTT (whereas window-based\n   control converges\
    \ on a target rate inversely proportional to RTT).  A\n   congestion control mechanism\
    \ can therefore converge on a rate that is\n   independent of RTT as long as its\
    \ dynamics depend on RTT (e.g., FAST\n   TCP [Jin04]).\n   In the stability analysis\
    \ of TCP and of these more modern controls,\n   the impact of slow-start on stability\
    \ (which can be significant as\n   short-lived HTTP flows often never leave this\
    \ phase) is not entirely\n   clear.\n"
- title: 2.3.  Fairness
  contents:
  - "2.3.  Fairness\n   Recently, the way the Internet community reasons about fairness\
    \ has\n   been called deeply into question [Bri07].  Much of the community has\n\
    \   taken fairness to mean approximate equality between the rates of\n   flows\
    \ (flow rate fairness) that experience equivalent path congestion\n   as with\
    \ TCP [RFC2581] [RFC5681] and TCP-Friendly Rate Control (TFRC)\n   [RFC5348].\
    \  [RFC3714] depicts the resulting situation as \"The\n   Amorphous Problem of\
    \ Fairness\".\n   A parallel tradition has been built on [Kelly98] where, as long\
    \ as\n   each user is accountable for the cost their rate causes to others\n \
    \  [MacK95], the set of rates that everyone chooses is deemed fair (cost\n   fairness)\
    \ -- because with any other set of choices people would lose\n   more value than\
    \ they gained overall.\n   In comparison, the debate between max-min, proportional,\
    \ and TCP\n   fairness is about mere details.  These three all share the assumption\n\
    \   that equal flow rates are desirable; they merely differ in the\n   second-order\
    \ issue of how to share out excess capacity in a network\n   of many bottlenecks.\
    \  In contrast, cost fairness should lead to\n   extremely unequal flow rates\
    \ by design.  Equivalently, equal flow\n   rates would typically be considered\
    \ extremely unfair.\n   The two traditional approaches are not protocol options\
    \ that can each\n   be followed in different parts of an internetwork.  They lead\
    \ to\n   research agendas that are different in their respective objectives,\n\
    \   resulting in a different set of open issues.\n   If we assume TCP-friendliness\
    \ as a goal with flow rate as the metric,\n   open issues would be:\n   -  Should\
    \ flow fairness depend on the packet rate or the bit rate?\n   -  Should the target\
    \ flow rate depend on RTT (as in TCP) or should\n      only flow dynamics depend\
    \ on RTT (e.g., as in FAST TCP [Jin04])?\n   -  How should we estimate whether\
    \ a particular flow start strategy is\n      fair, or whether a particular fast\
    \ recovery strategy after a\n      reduction in rate due to congestion is fair?\n\
    \   -  Should we judge what is reasonably fair if an application needs,\n    \
    \  for example, even smoother flows than TFRC, or it needs to burst\n      occasionally,\
    \ or with any other application behavior?\n   -  During brief congestion bursts\
    \ (e.g., due to new flow arrivals),\n      how should we judge at what point it\
    \ becomes unfair for some flows\n      to continue at a smooth rate while others\
    \ reduce their rate?\n   -  Which mechanism(s) could be used to enforce approximate\
    \ flow rate\n      fairness?\n   -  Should we introduce some degree of fairness\
    \ that takes into\n      account different users' flow activity over time?\n \
    \  -  How should we judge the fairness of applications using a large\n      number\
    \ of flows over separate paths (e.g., via an overlay)?\n   If we assume cost fairness\
    \ as a goal with congestion-volume as the\n   metric, open issues would be:\n\
    \   -  Can one application's sensitivity to instantaneous congestion\n      really\
    \ be protected by longer-term accountability of competing\n      applications?\n\
    \   -  Which protocol mechanism(s) are needed to give accountability for\n   \
    \   causing congestion?\n   -  How might we design one or two weighted transport\
    \ protocols (such\n      as TCP, UDP, etc.) with the addition of application policy\
    \ control\n      over the weight?\n   -  Which policy enforcement might be used\
    \ by networks, and what are\n      the interactions between application policy\
    \ and network policy\n      enforcement?\n   -  How should we design a new policy\
    \ enforcement framework that will\n      appropriately compete with existing flows\
    \ aiming for rate equality\n      (e.g., TCP)?\n   The question of how to reason\
    \ about fairness is a prerequisite to\n   agreeing on the research agenda.  If\
    \ the relevant metric is flow\n   rate, it places constraints at protocol design\
    \ time, whereas if the\n   metric is congestion-volume, the constraints move to\
    \ run-time while\n   design-time constraints can be relaxed [Bri08].  However,\
    \ that\n   question does not require more research in itself; it is merely a\n\
    \   debate that needs to be resolved by studying existing research and by\n  \
    \ assessing how bad fairness problems could become if they are not\n   addressed\
    \ rigorously, and whether we can rely on trust to maintain\n   approximate fairness\
    \ without requiring policing complexity [RFC5290].\n   The latter points may themselves\
    \ lead to additional research.\n   However, it is also accepted that more research\
    \ will not necessarily\n   lead to convincing either side to change their opinions.\
    \  More debate\n   would be needed.  It seems also that if the architecture is\
    \ built to\n   support cost fairness, then equal instantaneous cost rates for\
    \ flows\n   sharing a bottleneck result in flow-rate fairness; that is, flow-rate\n\
    \   fairness can be seen as a special case of cost fairness.  One can be\n   used\
    \ to build the other, but not vice-versa.\n"
- title: 3.  Detailed Challenges
  contents:
  - '3.  Detailed Challenges

    '
- title: '3.1.  Challenge 1: Network Support'
  contents:
  - "3.1.  Challenge 1: Network Support\n   This challenge is perhaps the most critical\
    \ to get right.  Changes to\n   the balance of functions between the endpoints\
    \ and network equipment\n   could require a change to the per-datagram data plane\
    \ interface\n   between the transport and network layers.  Network equipment vendors\n\
    \   need to be assured that any new interface is stable enough (on decade\n  \
    \ timescales) to build into firmware and hardware, and operating-system\n   vendors\
    \ will not use a new interface unless it is likely to be widely\n   deployed.\n\
    \   Network components can be involved in congestion control in two ways:\n  \
    \ first, they can implicitly optimize their functions, such as queue\n   management\
    \ and scheduling strategies, in order to support the\n   operation of end-to-end\
    \ congestion control.  Second, network\n   components can participate in congestion\
    \ control via explicit\n   signaling mechanisms.  Explicit signaling mechanisms,\
    \ whether in-band\n   or out-of-band, require a communication between network\
    \ components\n   and end-systems.  Signals realized within or over the IP layer\
    \ are\n   only meaningful to network components that process IP packets.  This\n\
    \   always includes routers and potentially also middleboxes, but not\n   pure\
    \ link layer devices.  The following section distinguishes clearly\n   between\
    \ the term \"network component\" and the term \"router\"; the term\n   \"router\"\
    \ is used whenever the processing of IP packets is explicitly\n   required.  One\
    \ fundamental challenge of network-supported congestion\n   control is that typically\
    \ not all network components along a path are\n   routers (cf. Section 3.1.3).\n\
    \   The first (optimizing) category of implicit mechanisms can be\n   implemented\
    \ in any network component that processes and stores\n   packets.  Various approaches\
    \ have been proposed and also deployed,\n   such as different AQM techniques.\
    \  Even though these implicit\n   techniques are known to improve network performance\
    \ during congestion\n   phases, they are still only partly deployed in the Internet.\
    \  This\n   may be due to the fact that finding optimal and robust\n   parameterizations\
    \ for these mechanisms is a non-trivial problem.\n   Indeed, the problem with\
    \ various AQM schemes is the difficulty in\n   identifying correct values of the\
    \ parameters that affect the\n   performance of the queuing scheme (due to variation\
    \ in the number of\n   sources, the capacity, and the feedback delay) [Firoiu00]\
    \ [Hollot01]\n   [Zhang03].  Many AQM schemes (RED, REM, BLUE, and PI-Controller,\
    \ but\n   also Adaptive Virtual Queue (AVQ)) do not define a systematic rule\n\
    \   for setting their parameters.\n   The second class of approaches uses explicit\
    \ signaling.  By using\n   explicit feedback from the network, connection endpoints\
    \ can obtain\n   more accurate information about the current network characteristics\n\
    \   on the path.  This allows endpoints to make more precise decisions\n   that\
    \ can better control congestion.\n   Explicit feedback techniques fall into three\
    \ broad categories:\n   -  Explicit congestion feedback: one-bit Explicit Congestion\n\
    \      Notification (ECN) [RFC3168] or proposals for more than one bit\n     \
    \ [Xia05];\n   -  Explicit per-datagram rate feedback: the eXplicit Control Protocol\n\
    \      (XCP) [Katabi02] [Falk07], or the Rate Control Protocol (RCP)\n      [Dukki05];\n\
    \   -  Explicit rate feedback: by means of in-band signaling, such as by\n   \
    \   Quick-Start [RFC4782], or by means of out-of-band signaling, e.g.,\n     \
    \ Congestion Avoidance with Distributed Proportional\n      Control/Performance\
    \ Transparency Protocol (CADPC/PTP) [Welzl03].\n   Explicit router feedback can\
    \ address some of the inherent\n   shortcomings of TCP.  For instance, XCP was\
    \ developed to overcome the\n   inefficiency and instability that TCP suffers\
    \ from when the per-flow\n   bandwidth-delay product increases.  By decoupling\
    \ resource\n   utilization/congestion control from fairness control, XCP achieves\n\
    \   equal bandwidth allocation, high utilization, a small standing queue\n   size,\
    \ and near-zero packet drops, with both steady and highly varying\n   traffic.\
    \  Importantly, XCP does not maintain any per-flow state in\n   routers and requires\
    \ few CPU cycles per packet, hence making it\n   potentially applicable in high-speed\
    \ routers.  However, XCP is still\n   subject to research: as [Andrew05] has pointed\
    \ out, XCP is locally\n   stable but globally unstable when the maximum RTT of\
    \ a flow is much\n   larger than the mean RTT.  This instability can be removed\
    \ by\n   changing the update strategy for the estimation interval, but this\n\
    \   makes the system vulnerable to erroneous RTT advertisements.  The\n   authors\
    \ of [Pap02] have shown that when flows with different RTTs are\n   applied, XCP\
    \ sometimes discriminates among heterogeneous traffic\n   flows, even if XCP generally\
    \ equalizes rates among different flows.\n   [Low05] provides for a complete characterization\
    \ of the XCP\n   equilibrium properties.\n   Several other explicit router feedback\
    \ schemes have been developed\n   with different design objectives.  For instance,\
    \ RCP uses per-packet\n   feedback similar to XCP.  But unlike XCP, RCP focuses\
    \ on the\n   reduction of flow completion times [Dukki06], taking an optimistic\n\
    \   approach to flows likely to arrive in the next RTT and tolerating\n   larger\
    \ instantaneous queue sizes [Dukki05].  XCP, on the other hand,\n   gives very\
    \ poor flow completion times for short flows.\n   Both implicit and explicit router\
    \ support should be considered in the\n   context of the end-to-end argument [Saltzer84],\
    \ which is one of the\n   key design principles of the Internet.  It suggests\
    \ that functions\n   that can be realized both in the end-systems and in the network\n\
    \   should be implemented in the end-systems.  This principle ensures\n   that\
    \ the network provides a general service and that it remains as\n   simple as\
    \ possible (any additional complexity is placed above the IP\n   layer, i.e.,\
    \ at the edges) so as to ensure evolvability, reliability,\n   and robustness.\
    \  Furthermore, the fate-sharing principle ([Clark88],\n   \"Design Philosophy\
    \ of the DARPA Internet Protocols\") mandates that an\n   end-to-end Internet\
    \ protocol design should not rely on the\n   maintenance of any per-flow state\
    \ (i.e., information about the state\n   of the end-to-end communication) inside\
    \ the network and that the\n   network state (e.g., routing state) maintained\
    \ by the Internet shall\n   minimize its interaction with the states maintained\
    \ at the\n   endpoints/hosts [RFC1958].\n   However, as discussed in [Moors02]\
    \ for instance, congestion control\n   cannot be realized as a pure end-to-end\
    \ function only.  Congestion is\n   an inherent network phenomenon and can only\
    \ be resolved efficiently\n   by some cooperation of end-systems and the network.\
    \  Congestion\n   control in today's Internet protocols follows the end-to-end\
    \ design\n   principle insofar as only minimal feedback from the network is used,\n\
    \   e.g., packet loss and delay.  The end-systems only decide how to\n   react\
    \ and how to avoid congestion.  The crux is that on the one hand,\n   there would\
    \ be substantial benefit by further assistance from the\n   network, but, on the\
    \ other hand, such network support could lead to\n   duplication of functions,\
    \ which might even harmfully interact with\n   end-to-end protocol mechanisms.\
    \  The different requirements of\n   applications (cf. the fairness discussion\
    \ in Section 2.3) call for a\n   variety of different congestion control approaches,\
    \ but putting such\n   per-flow behavior inside the network should be avoided,\
    \ as such a\n   design would clearly be at odds with the end-to-end and fate-sharing\n\
    \   design principles.\n   The end-to-end and fate-sharing principles are generally\
    \ regarded as\n   the key ingredients for ensuring a scalable and survivable network\n\
    \   design.  In order to ensure that new congestion control mechanisms\n   are\
    \ scalable, violating these principles must therefore be avoided.\n   For instance,\
    \ protocols like XCP and RCP seem not to require flow\n   state in the network,\
    \ but this is only the case if the network trusts\n   i) the receiver not to lie\
    \ when feeding back the network's delta to\n   the requested rate; ii) the source\
    \ not to lie when declaring its\n   rate; and iii) the source not to cheat when\
    \ setting its rate in\n   response to the feedback [Katabi04].\n   Solving these\
    \ problems for non-cooperative environments like the\n   public Internet requires\
    \ flow state, at least on a sampled basis.\n   However, because flows can create\
    \ new identifiers whenever they want,\n   sampling does not provide a deterrent\
    \ -- a flow can simply cheat\n   until it is discovered and then switch to a whitewashed\
    \ identifier\n   [Feldman04], and continue cheating until it is discovered again\n\
    \   ([Bri09], S7.3).\n   However, holding flow state in the network only seems\
    \ to solve these\n   policing problems in single autonomous system settings. \
    \ A\n   multi-domain system would seem to require a completely different\n   protocol\
    \ structure, as the information required for policing is only\n   seen as packets\
    \ leave the internetwork, but the networks where\n   packets enter will also want\
    \ to police compliance.\n   Even if a new protocol structure were found, it seems\
    \ unlikely that\n   network flow state could be avoided given the network's per-packet\n\
    \   flow rate instructions would need to be compared against variations\n   in\
    \ the actual flow rate, which is inherently not a per-packet metric.\n   These\
    \ issues have been outstanding ever since integrated services\n   (IntServ) was\
    \ identified as unscalable in 1997 [RFC2208].  All\n   subsequent attempts to\
    \ involve network elements in limiting flow\n   rates (XCP, RCP, etc.) will run\
    \ up against the same open issue if\n   anyone attempts to standardize them for\
    \ use on the public Internet.\n   In general, network support of congestion control\
    \ raises many issues\n   that have not been completely solved yet.\n"
- title: 3.1.1.  Performance and Robustness
  contents:
  - "3.1.1.  Performance and Robustness\n   Congestion control is subject to some\
    \ tradeoffs: on the one hand, it\n   must allow high link utilizations and fair\
    \ resource sharing, but on\n   the other hand, the algorithms must also be robust.\n\
    \   Router support can help to improve performance, but it can also\n   result\
    \ in additional complexity and more control loops.  This\n   requires a careful\
    \ design of the algorithms in order to ensure\n   stability and avoid, e.g., oscillations.\
    \  A further challenge is the\n   fact that feedback information may be imprecise.\
    \  For instance,\n   severe congestion can delay feedback signals.  Also, in-network\n\
    \   measurement of parameters such as RTTs or data rates may contain\n   estimation\
    \ errors.  Even though there has been significant progress\n   in providing fundamental\
    \ theoretical models for such effects,\n   research has not completely explored\
    \ the whole problem space yet.\n   Open questions are:\n   -  How much can network\
    \ elements theoretically improve performance in\n      the complete range of communication\
    \ scenarios that exist in the\n      Internet without damaging or impacting end-to-end\
    \ mechanisms\n      already in place?\n   -  Is it possible to design robust congestion\
    \ control mechanisms that\n      offer significant benefits with minimum additional\
    \ risks, even if\n      Internet traffic patterns will change in the future?\n\
    \   -  What is the minimum support that is needed from the network in\n      order\
    \ to achieve significantly better performance than with end-\n      to-end mechanisms\
    \ and the current IP header limitations that\n      provide at most unary ECN\
    \ signals?\n"
- title: 3.1.2.  Granularity of Network Component Functions
  contents:
  - "3.1.2.  Granularity of Network Component Functions\n   There are several degrees\
    \ of freedom concerning the involvement of\n   network entities, ranging from\
    \ some few additional functions in\n   network management procedures on the one\
    \ end to additional per-packet\n   processing on the other end of the solution\
    \ space.  Furthermore,\n   different amounts of state can be kept in routers (no\
    \ per-flow state,\n   partial per-flow state, soft state, or hard state).  The\
    \ additional\n   router processing is a challenge for Internet scalability and\
    \ could\n   also increase end-to-end latencies.\n   Although there are many research\
    \ proposals that do not require\n   per-flow state and thus do not cause a large\
    \ processing overhead,\n   there are no known full solutions (i.e., including\
    \ anti-cheating)\n   that do not require per-flow processing.  Also, scalability\
    \ issues\n   could be caused, for instance, by synchronization mechanisms for\n\
    \   state information among parallel processing entities, which are,\n   e.g.,\
    \ used in high-speed router hardware designs.\n   Open questions are:\n   -  What\
    \ granularity of router processing can be realized without\n      affecting Internet\
    \ scalability?\n   -  How can additional processing efforts be kept to a minimum?\n"
- title: 3.1.3.  Information Acquisition
  contents:
  - "3.1.3.  Information Acquisition\n   In order to support congestion control, network\
    \ components have to\n   obtain at least a subset of the following information.\
    \  Obtaining\n   that information may result in complex tasks.\n   1. Capacity\
    \ of (outgoing) links\n      Link characteristics depend on the realization of\
    \ lower protocol\n      layers.  Routers operating at the IP layer do not necessarily\
    \ know\n      the link layer network topology and link capacities, and these are\n\
    \      not always constant (e.g., on shared wireless links or bandwidth-\n   \
    \   on-demand links).  Depending on the network technology, there can\n      be\
    \ queues or bottlenecks that are not directly visible at the IP\n      networking\
    \ layer.\n      Difficulties also arise when using IP-in-IP tunnels [RFC2003],\n\
    \      IPsec tunnels [RFC4301], IP encapsulated in the Layer Two\n      Tunneling\
    \ Protocol (L2TP) [RFC2661], Generic Routing Encapsulation\n      (GRE) [RFC1701]\
    \ [RFC2784], the Point-to-Point Tunneling Protocol\n      (PPTP) [RFC2637], or\
    \ Multiprotocol Label Switching (MPLS)\n      [RFC3031] [RFC3032].  In these cases,\
    \ link information could be\n      determined by cross-layer information exchange,\
    \ but this requires\n      interfaces capable of processing link layer technology\
    \ specific\n      information.  An alternative could be online measurements, but\n\
    \      this can cause significant additional network overhead.  It is an\n   \
    \   open research question as to how much, if any, online traffic\n      measurement\
    \ would be acceptable (at run-time).  Encapsulation and\n      decapsulation of\
    \ explicit congestion information have been\n      specified for IP-in-IP tunnelling\
    \ [RFC6040] and for MPLS-in-MPLS\n      or MPLS-in-IP [RFC5129].\n   2. Traffic\
    \ carried over (outgoing) links\n      Accurate online measurement of data rates\
    \ is challenging when\n      traffic is bursty.  For instance, measuring a \"\
    current link load\"\n      requires defining the right measurement interval /\
    \ sampling\n      interval.  This is a challenge for proposals that require\n\
    \      knowledge, e.g., about the current link utilization.\n   3. Internal buffer\
    \ statistics\n      Some proposals use buffer statistics such as a virtual queue\n\
    \      length to trigger feedback.  However, network components can\n      include\
    \ multiple distributed buffer stages that make it difficult\n      to obtain such\
    \ metrics.\n   Open questions are:\n   -  Can and should this information be made\
    \ available, e.g., by\n      additional interfaces or protocols?\n   -  Which\
    \ information is so important to higher-layer controllers that\n      machine\
    \ architecture research should focus on designing to\n      provide it?\n"
- title: 3.1.4.  Feedback Signaling
  contents:
  - "3.1.4.  Feedback Signaling\n   Explicit notification mechanisms can be realized\
    \ either by in-band\n   signaling (notifications piggybacked along with the data\
    \ traffic) or\n   by out-of-band signaling [Sarola07].  The latter case requires\n\
    \   additional protocols and a secure binding between the signals and the\n  \
    \ packets they refer to.  Out-of-band signaling can be further\n   subdivided\
    \ into path-coupled and path-decoupled approaches.\n   Open questions concerning\
    \ feedback signaling include:\n   -  At which protocol layer should the feedback\
    \ signaling occur\n      (IP/network layer assisted, transport layer assisted,\
    \ hybrid\n      solutions, shim layer, intermediate sub-layer, etc.)?  Should\
    \ the\n      feedback signaling be path-coupled or path-decoupled?\n   -  What\
    \ is the optimal frequency of feedback (only in case of\n      congestion events,\
    \ per RTT, per packet, etc.)?\n   -  What direction should feedback take (from\
    \ network resource via\n      receiver to sender, or directly back to sender)?\n"
- title: '3.2.  Challenge 2: Corruption Loss'
  contents:
  - "3.2.  Challenge 2: Corruption Loss\n   It is common for congestion control mechanisms\
    \ to interpret packet\n   loss as a sign of congestion.  This is appropriate when\
    \ packets are\n   dropped in routers because of a queue that overflows, but there\
    \ are\n   other possible reasons for packet drops.  In particular, in wireless\n\
    \   networks, packets can be dropped because of corruption loss,\n   rendering\
    \ the typical reaction of a congestion control mechanism\n   inappropriate.  As\
    \ a result, non-congestive loss may be more\n   prevalent in these networks due\
    \ to corruption loss (when the wireless\n   link cannot be conditioned to properly\
    \ control its error rate or due\n   to transient wireless link interruption in\
    \ areas of poor coverage).\n   TCP over wireless and satellite is a topic that\
    \ has been investigated\n   for a long time [Krishnan04].  There are some proposals\
    \ where the\n   congestion control mechanism would react as if a packet had not\
    \ been\n   dropped in the presence of corruption (cf. TCP HACK [Balan01]), but\n\
    \   discussions in the IETF have shown (see, for instance, the discussion\n  \
    \ that occurred in April 2003 on the Datagram Congestion Control\n   Protocol\
    \ (DCCP) working group list\n   http://www.ietf.org/mail-archive/web/dccp/current/mail6.html)\
    \ that\n   there is no agreement that this type of reaction is appropriate.  For\n\
    \   instance, it has been said that congestion can manifest itself as\n   corruption\
    \ on shared wireless links, and it is questionable whether a\n   source that sends\
    \ packets that are continuously impaired by link\n   noise should keep sending\
    \ at a high rate because it has lost the\n   integrity of the feedback loop.\n\
    \   Generally, two questions must be addressed when designing a\n   congestion\
    \ control mechanism that takes corruption loss into account:\n   1. How is corruption\
    \ detected?\n   2. What should be the reaction?\n   In addition to question 1\
    \ above, it may be useful to consider\n   detecting the reason for corruption,\
    \ but this has not yet been done\n   to the best of our knowledge.\n   Corruption\
    \ detection can be done using an in-band or out-of-band\n   signaling mechanism,\
    \ much in the same way as described for\n   Challenge 1.  Additionally, implicit\
    \ detection can be considered:\n   link layers sometimes retransmit erroneous\
    \ frames, which can cause\n   the end-to-end delay to increase -- but, from the\
    \ perspective of a\n   sender at the transport layer, there are many other possible\
    \ reasons\n   for such an effect.\n   Header checksums provide another implicit\
    \ detection possibility: if a\n   checksum only covers all the necessary header\
    \ fields and this\n   checksum does not show an error, it is possible for errors\
    \ to be\n   found in the payload using a second checksum.  Such error detection\n\
    \   is possible with UDP-Lite and DCCP; it was found to work well over a\n   General\
    \ Packet Radio Service (GPRS) network in a study [Chester04]\n   and poorly over\
    \ a WiFi network in another study [Rossi06] [Welzl08].\n   Note that while UDP-Lite\
    \ and DCCP enable the detection of corruption,\n   the specifications of these\
    \ protocols do not foresee any specific\n   reaction to it for the time being.\n\
    \   The idea of having a transport endpoint detecting and accordingly\n   reacting\
    \ (or not) to corruption poses a number of interesting\n   questions regarding\
    \ cross-layer interactions.  As IP is designed to\n   operate over arbitrary link\
    \ layers, it is therefore difficult to\n   design a congestion control mechanism\
    \ on top of it that appropriately\n   reacts to corruption -- especially as the\
    \ specific data link layers\n   that are in use along an end-to-end path are typically\
    \ unknown to\n   entities at the transport layer.\n   While the IETF has not yet\
    \ specified how a congestion control\n   mechanism should react to corruption,\
    \ proposals exist in the\n   literature, e.g., [Tickoo05].  For instance, TCP\
    \ Westwood [Mascolo01]\n   sets the congestion window equal to the measured bandwidth\
    \ at the\n   time of congestion in response to three DupACKs or a timeout.  This\n\
    \   measurement is obtained by counting and filtering the ACK rate.  This\n  \
    \ setting provides a significant goodput improvement in noisy channels\n   because\
    \ the \"blind\" by half window reduction of standard TCP is\n   avoided, i.e.,\
    \ the window is not reduced by too much.\n   Open questions concerning corruption\
    \ loss include:\n   -  How should corruption loss be detected?\n   -  How should\
    \ a source react when it is known that corruption has\n      occurred?\n   - \
    \ Can an ECN-capable flow infer that loss must be due to corruption\n      just\
    \ from lack of explicit congestion notifications around a loss\n      episode\
    \ [Tickoo05]?  Or could this inference be dangerous, given\n      the transport\
    \ does not know whether all queues on the path are\n      ECN-capable or not?\n"
- title: '3.3.  Challenge 3: Packet Size'
  contents:
  - "3.3.  Challenge 3: Packet Size\n   TCP does not take packet size into account\
    \ when responding to losses\n   or ECN.  Over past years, the performance of TCP\
    \ congestion avoidance\n   algorithms has been extensively studied.  The well-known\
    \ \"square root\n   formula\" provides an estimation of the performance of the\
    \ TCP\n   congestion avoidance algorithm for TCP Reno [RFC2581].  [Padhye98]\n\
    \   enhances the model to account for timeouts, receiver window, and\n   delayed\
    \ ACKs.\n   For the sake of the present discussion, we will assume that the TCP\n\
    \   throughput is expressed using the simplified formula.  Using this\n   formula,\
    \ the TCP throughput B is proportional to the segment size and\n   inversely proportional\
    \ to the RTT and the square root of the drop\n   probability:\n              \
    \  S     1\n         B ~ C --- -------\n               RTT sqrt(p)\n    where\n\
    \         C     is a constant\n         S     is the TCP segment size (in bytes)\n\
    \         RTT   is the end-to-end round-trip time of the TCP\n               connection\
    \ (in seconds)\n         p     is the packet drop probability\n   Neglecting the\
    \ fact that the TCP rate linearly depends on it,\n   choosing the ideal packet\
    \ size is a tradeoff between high throughput\n   (the larger a packet, the smaller\
    \ the relative header overhead) and\n   low packet latency (the smaller a packet,\
    \ the shorter the time that\n   is needed until it is filled with data).  Observing\
    \ that TCP is not\n   optimal for applications with streaming media (since reliable\n\
    \   in-order delivery and congestion control can cause arbitrarily long\n   delays),\
    \ this tradeoff has not usually been considered for TCP\n   applications.  Therefore,\
    \ the influence of the packet size on the\n   sending rate has not typically been\
    \ seen as a significant issue,\n   given there are still few paths through the\
    \ Internet that support\n   packets larger than the 1500 bytes common with Ethernet.\n\
    \   The situation is already different for the Datagram Congestion\n   Control\
    \ Protocol (DCCP) [RFC4340], which has been designed to enable\n   unreliable\
    \ but congestion-controlled datagram transmission, avoiding\n   the arbitrary\
    \ delays associated with TCP.  DCCP is intended for\n   applications such as streaming\
    \ media that can benefit from control\n   over the tradeoffs between delay and\
    \ reliable in-order delivery.\n   DCCP provides for a choice of modular congestion\
    \ control mechanisms.\n   DCCP uses Congestion Control Identifiers (CCIDs) to\
    \ specify the\n   congestion control mechanism.  Three profiles are currently\n\
    \   specified:\n   -  DCCP Congestion Control ID 2 (CCID 2) [RFC4341]:  TCP-like\n\
    \      Congestion Control.  CCID 2 sends data using a close approximation\n  \
    \    of TCP's congestion control as well as incorporating a variant of\n     \
    \ Selective Acknowledgment (SACK) [RFC2018] [RFC3517].  CCID 2 is\n      suitable\
    \ for senders that can adapt to the abrupt changes in the\n      congestion window\
    \ typical of TCP's AIMD congestion control, and\n      particularly useful for\
    \ senders that would like to take advantage\n      of the available bandwidth\
    \ in an environment with rapidly changing\n      conditions.\n   -  DCCP Congestion\
    \ Control ID 3 (CCID 3) [RFC4342]: TCP-Friendly Rate\n      Control (TFRC) [RFC5348]\
    \ is a congestion control mechanism\n      designed for unicast flows operating\
    \ in a best-effort Internet\n      environment.  When competing for bandwidth,\
    \ its window is similar\n      to TCP flows but has a much lower variation of\
    \ throughput over\n      time than TCP, making it more suitable for applications\
    \ such as\n      streaming media where a relatively smooth sending rate is of\n\
    \      importance.  CCID 3 is appropriate for flows that would prefer to\n   \
    \   minimize abrupt changes in the sending rate, including streaming\n      media\
    \ applications with small or moderate receiver buffering\n      before playback.\n\
    \   -  DCCP Congestion Control ID 4 (CCID 4) [RFC5622]: TFRC Small\n      Packets\
    \ (TFRC-SP) [RFC4828], a variant of the TFRC mechanism, has\n      been designed\
    \ for applications that exchange small packets.  The\n      objective of TFRC-SP\
    \ is to achieve the same bandwidth in bits per\n      second as a TCP flow using\
    \ packets of up to 1500 bytes.  TFRC-SP\n      enforces a minimum interval of\
    \ 10 ms between data packets to\n      prevent a single flow from sending small\
    \ packets arbitrarily\n      frequently.  CCID 4 has been designed to be used\
    \ either by\n      applications that use a small fixed segment size, or by\n \
    \     applications that change their sending rate by varying the segment\n   \
    \   size.  Because CCID 4 is intended for applications that use a\n      fixed\
    \ small segment size, or that vary their segment size in\n      response to congestion,\
    \ the transmit rate derived from the TCP\n      throughput equation is reduced\
    \ by a factor that accounts for the\n      packet header size, as specified in\
    \ [RFC4828].\n   The resulting open questions are:\n   -  How does TFRC-SP operate\
    \ under various network conditions?\n   -  How can congestion control be designed\
    \ so as to scale with packet\n      size (dependency of congestion algorithm on\
    \ packet size)?\n   Today, many network resources are designed so that packet\
    \ processing\n   cannot be overloaded even for incoming loads at the maximum bit\
    \ rate\n   of the line.  If packet processing can handle sustained load r\n  \
    \ [packet per second] and the minimum packet size is h [bit] (i.e.,\n   frame,\
    \ packet, and transport headers with no payload), then a line\n   rate of x [bit\
    \ per second] will never be able to overload packet\n   processing as long as\
    \ x =< r*h.\n   However, realistic equipment is often designed to only cope with\
    \ a\n   near-worst-case workload with a few larger packets in the mix, rather\n\
    \   than the worst-case scenario of all minimum-size packets.  In this\n   case,\
    \ x = r*(h + e) for some small value of e.  Therefore, packet\n   congestion is\
    \ not impossible for runs of small packets (e.g., TCP\n   ACKs or denial-of-service\
    \ (DoS) attacks with TCP SYNs or small UDP\n   datagrams).  But absent such anomalous\
    \ workloads, equipment vendors\n   at the 2008 ICCRG meeting believed that equipment\
    \ could still be\n   designed so that any congestion should be due to bit overload\
    \ and not\n   packet overload.\n   This observation raises additional open issues:\n\
    \   -  Can bit congestion remain prevalent?\n      Being able to assume that congestion\
    \ is generally due to excess\n      bits and not excess packets is a useful simplifying\
    \ assumption in\n      the design of congestion control protocols.  Can we rely\
    \ on this\n      assumption for the future?  An alternative view is that in-network\n\
    \      processing will become commonplace, so that per-packet processing\n   \
    \   will as likely be the bottleneck as per-bit transmission [Shin08].\n     \
    \ Over the last three decades, performance gains have mainly been\n      achieved\
    \ through increased packet rates and not bigger packets.\n      But if bigger\
    \ maximum segment sizes do become more prevalent, tiny\n      segments (e.g.,\
    \ ACKs) will not stop being widely used -- leading\n      to a widening range\
    \ of packet sizes.\n      The open question is thus whether or not packet processing\
    \ rates\n      (r) will keep up with growth in transmission rates (x).  A\n  \
    \    superficial look at Moore's Law-type trends would suggest that\n      processing\
    \ (r) will continue to outstrip growth in transmission\n      (x).  But predictions\
    \ based on actual knowledge of technology\n      futures would be useful.  Another\
    \ open question is whether there\n      are likely to be more small packets in\
    \ the average packet mix.  If\n      the answers to either of these questions\
    \ predict that packet\n      congestion could become prevalent, congestion control\
    \ protocols\n      will have to be more complicated.\n   -  Confusable causes\
    \ of loss\n      There is a considerable body of research on how to distinguish\n\
    \      whether packet drops are due to transmission corruption or to\n      congestion.\
    \  But the full list of confusable causes of loss is\n      longer and includes\
    \ transmission corruption loss, congestion loss\n      (bit congestion and packet\
    \ congestion), and policing loss.\n      If congestion is due to excess bits,\
    \ the bit rate should be\n      reduced.  If congestion is due to excess packets,\
    \ the packet rate\n      can be reduced without reducing the bit rate -- by using\
    \ larger\n      packets.  However, if the transport cannot tell which of these\n\
    \      causes led to a specific packet drop, its only safe response is to\n  \
    \    reduce the bit rate.  This is why the Internet would be more\n      complicated\
    \ if packet congestion were prevalent, as reducing the\n      bit rate normally\
    \ also reduces the packet rate, while reducing the\n      packet rate does not\
    \ necessarily reduce the bit rate.\n      Given distinguishing between corruption\
    \ loss and congestion is\n      already an open issue (Section 3.2), if that problem\
    \ is ever\n      solved, a further open issue would be whether to standardize\
    \ a\n      solution that distinguishes all the above causes of loss, and not\n\
    \      just two of them.\n      Nonetheless, even if we find a way for network\
    \ equipment to\n      explicitly distinguish which sort of loss has occurred,\
    \ we will\n      never be able to assume that such a smart AQM solution is deployed\n\
    \      at every congestible resource throughout the Internet -- at every\n   \
    \   higher-layer device like firewalls, proxies, and servers; and at\n      every\
    \ lower-layer device like low-end hubs, DSLAMs, Wireless LAN\n      (WLAN) cards,\
    \ cellular base-stations, and so on.  Thus, transport\n      protocols will always\
    \ have to cope with packet drops due to\n      unpredictable causes, so we should\
    \ always treat AQM as an\n      optimization, given it will never be ubiquitous\
    \ throughout the\n      public Internet.\n   -  What does a congestion notification\
    \ on a packet of a certain size\n      mean?\n      The open issue here is whether\
    \ a loss or explicit congestion mark\n      should be interpreted as a single\
    \ congestion event irrespective of\n      the size of the packet lost or marked,\
    \ or whether the strength of\n      the congestion notification is weighted by\
    \ the size of the packet.\n      This issue is discussed at length in [Bri10],\
    \ along with other\n      aspects of packet size and congestion control.\n   \
    \   [Bri10] makes the strong recommendation that network equipment\n      should\
    \ drop or mark packets with a probability independent of each\n      specific\
    \ packet's size, while congestion controls should respond\n      to dropped or\
    \ marked packets in proportion to the packet's size.\n   -  Packet size and congestion\
    \ control protocol design\n      If the above recommendation is correct -- that\
    \ the packet size of\n      a congestion notification should be taken into account\
    \ when the\n      transport reads, and not when the network writes, the notification\n\
    \      -- it opens up a significant problem of protocol engineering and\n    \
    \  re-engineering.  Indeed, TCP does not take packet size into\n      account\
    \ when responding to losses or ECN.  At present, this is not\n      a pressing\
    \ problem because use of 1500 byte data segments is very\n      prevalent for\
    \ TCP, and the incidence of alternative maximum\n      segment sizes is not large.\
    \  However, we should design the\n      Internet's protocols so they will scale\
    \ with packet size.  So, an\n      open issue is whether we should evolve TCP\
    \ to be sensitive to\n      packet size, or expect new protocols to take over.\n\
    \      As we continue to standardize new congestion control protocols, we\n  \
    \    must then face the issue of how they should account for packet\n      size.\
    \  It is still an open research issue to establish whether TCP\n      was correct\
    \ in not taking packet size into account.  If it is\n      determined that TCP\
    \ was wrong in this respect, we should\n      discourage future protocol designs\
    \ from following TCP's example.\n      For example, as explained above, the small-packet\
    \ variant of TCP-\n      friendly rate control (TFRC-SP [RFC4828]) is an experimental\n\
    \      protocol that aims to take packet size into account.  Whatever\n      packet\
    \ size it uses, it ensures that its rate approximately equals\n      that of a\
    \ TCP using 1500 byte segments.  This raises the further\n      question of whether\
    \ TCP with 1500 byte segments will be a suitable\n      long-term gold standard,\
    \ or whether we need a more thorough review\n      of what it means for a congestion\
    \ control mechanism to scale with\n      packet size.\n"
- title: '3.4.  Challenge 4: Flow Startup'
  contents:
  - "3.4.  Challenge 4: Flow Startup\n   The beginning of data transmissions imposes\
    \ some further, unique\n   challenges: when a connection to a new destination\
    \ is established,\n   the end-systems have hardly any information about the characteristics\n\
    \   of the path in between and the available bandwidth.  In this flow\n   startup\
    \ situation, there is no obvious choice as to how to start to\n   send.  A similar\
    \ problem also occurs after relatively long idle\n   times, since the congestion\
    \ control state then no longer reflects\n   current information about the state\
    \ of the network (flow restart\n   problem).\n   Van Jacobson [Jacobson88] suggested\
    \ using the slow-start mechanism\n   both for the flow startup and the flow restart,\
    \ and this is today's\n   standard solution [RFC2581] [RFC5681].  Per [RFC5681],\
    \ the slow-start\n   algorithm is used when the congestion window (cwnd) < slow-start\n\
    \   threshold (ssthresh), whose initial value is set arbitrarily high\n   (e.g.,\
    \ to the size of the largest possible advertised window) and\n   reduced in response\
    \ to congestion.  During slow-start, TCP increments\n   the cwnd by at most Sender\
    \ Maximum Segment Size (MSS) bytes for each\n   ACK received that cumulatively\
    \ acknowledges new data.  Slow-start\n   ends when cwnd exceeds ssthresh or when\
    \ congestion is observed.\n   However, the slow-start is not optimal in many situations.\
    \  First, it\n   can take quite a long time until a sender can fully utilize the\n\
    \   available bandwidth on a path.  Second, the exponential increase may\n   be\
    \ too aggressive and cause multiple packet loss if large congestion\n   windows\
    \ are reached (slow-start overshooting).  Finally, the slow-\n   start does not\
    \ ensure that new flows converge quickly to a reasonable\n   share of resources,\
    \ particularly when the new flows compete with\n   long-lived flows and come out\
    \ of slow-start early (slow-start vs\n   overshoot tradeoff).  This convergence\
    \ problem may even worsen if\n   more aggressive congestion control variants are\
    \ widely used.\n   The slow-start and its interaction with the congestion avoidance\n\
    \   phase was largely designed by intuition [Jacobson88].  So far, little\n  \
    \ theory has been developed to understand the flow startup problem and\n   its\
    \ implication on congestion control stability and fairness.  There\n   is also\
    \ no established methodology to evaluate whether new flow\n   startup mechanisms\
    \ are appropriate or not.\n   As a consequence, it is a non-trivial task to address\
    \ the\n   shortcomings of the slow-start algorithm.  Several experimental\n  \
    \ enhancements have been proposed, such as congestion window validation\n   [RFC2861]\
    \ and limited slow-start [RFC3742].  There are also ongoing\n   research activities,\
    \ focusing, e.g., on bandwidth estimation\n   techniques, delay-based congestion\
    \ control, or rate-pacing\n   mechanisms.  However, any alternative end-to-end\
    \ flow startup\n   approach has to cope with the inherent problem that there is\
    \ no or\n   only little information about the path at the beginning of a data\n\
    \   transfer.  This uncertainty could be reduced by more expressive\n   feedback\
    \ signaling (cf. Section 3.1).  For instance, a source could\n   learn the path\
    \ characteristics faster with the Quick-Start mechanism\n   [RFC4782].  But even\
    \ if the source knew exactly what rate it should\n   aim for, it would still not\
    \ necessarily be safe to jump straight to\n   that rate.  The end-system still\
    \ does not know how a change in its\n   own rate will affect the path, which also\
    \ might become congested in\n   less than one RTT.  Further research would be\
    \ useful to understand\n   the effect of decreasing the uncertainty by explicit\
    \ feedback\n   separately from control theoretic stability questions.  Furthermore,\n\
    \   flow startup also raises fairness questions.  For instance, it is\n   unclear\
    \ whether it could be reasonable to use a faster startup when\n   an end-system\
    \ detects that a path is currently not congested.\n   In summary, there are several\
    \ topics for further research concerning\n   flow startup:\n   -  Better theoretical\
    \ understanding of the design and evaluation of\n      flow startup mechanisms,\
    \ concerning their impact on congestion\n      risk, stability, and fairness.\n\
    \   -  Evaluating whether it may be appropriate to allow alternative\n      starting\
    \ schemes, e.g., to allow higher initial rates under\n      certain constraints\
    \ [Chu10]; this also requires refining the\n      definition of fairness for startup\
    \ situations.\n   -  Better theoretical models for the effects of decreasing\n\
    \      uncertainty by additional network feedback, particularly if the\n     \
    \ path characteristics are very dynamic.\n"
- title: '3.5.  Challenge 5: Multi-Domain Congestion Control'
  contents:
  - "3.5.  Challenge 5: Multi-Domain Congestion Control\n   Transport protocols such\
    \ as TCP operate over the Internet, which is\n   divided into autonomous systems.\
    \  These systems are characterized by\n   their heterogeneity as IP networks are\
    \ realized by a multitude of\n   technologies.\n"
- title: 3.5.1.  Multi-Domain Transport of Explicit Congestion Notification
  contents:
  - "3.5.1.  Multi-Domain Transport of Explicit Congestion Notification\n   Different\
    \ conditions and their variations lead to correlation effects\n   between policers\
    \ that regulate traffic against certain conformance\n   criteria.\n   With the\
    \ advent of techniques allowing for early detection of\n   congestion, packet\
    \ loss is no longer the sole metric of congestion.\n   ECN (Explicit Congestion\
    \ Notification) marks packets -- set by active\n   queue management techniques\
    \ -- to convey congestion information,\n   trying to prevent packet losses (packet\
    \ loss and the number of\n   packets marked gives an indication of the level of\
    \ congestion).\n   Using TCP ACKs to feed back that information allows the hosts\
    \ to\n   realign their transmission rate and thus encourages them to\n   efficiently\
    \ use the network.  In IP, ECN uses the two least\n   significant bits of the\
    \ (former) IPv4 Type of Service (TOS) octet or\n   the (former) IPv6 Traffic Class\
    \ octet [RFC2474] [RFC3260].  Further,\n   ECN in TCP uses two bits in the TCP\
    \ header that were previously\n   defined as reserved [RFC793].\n   ECN [RFC3168]\
    \ is an example of a congestion feedback mechanism from\n   the network toward\
    \ hosts.  The congestion-based feedback scheme,\n   however, has limitations when\
    \ applied on an inter-domain basis.\n   Indeed, Sections 8 and 19 of [RFC3168]\
    \ detail the implications of two\n   possible attacks:\n   1. non-compliance:\
    \ a network erasing a Congestion Experienced (CE)\n      codepoint introduced\
    \ earlier on the path, and\n   2. subversion: a network changing Not ECN-Capable\
    \ Transport (Not-ECT)\n      to ECT.\n   Both of these problems could allow an\
    \ attacking network to cause\n   excess congestion in an upstream network, even\
    \ if the transports were\n   behaving correctly.  There are to date two possible\
    \ solutions to the\n   non-compliance problem (number 1 above): the ECN-nonce\
    \ [RFC3540] and\n   the [CONEX] work item inspired by the re-ECN incentive system\n\
    \   [Bri09].  Nevertheless, accidental rather than malicious erasure of\n   ECN\
    \ is an issue for IPv6 where the absence of an IPv6 header checksum\n   implies\
    \ that corruption of ECN could be more impacting than in the\n   IPv4 case.\n\
    \   Fragmentation is another issue: the ECN-nonce cannot protect against\n   misbehaving\
    \ receivers that conceal marked fragments; thus, some\n   protection is lost in\
    \ situations where path MTU discovery is\n   disabled.  Note also that ECN-nonce\
    \ wouldn't protect against the\n   subversion issue (number 2 above) because,\
    \ by definition, a Not-ECT\n   packet comes from a source without ECN enabled,\
    \ and therefore without\n   the ECN-nonce enabled.  So, there is still room for\
    \ improvement on\n   the ECN mechanism when operating in multi-domain networks.\n\
    \   Operational/deployment experience is nevertheless required to\n   determine\
    \ the extent of these problems.  The second problem is mainly\n   related to deployment\
    \ and usage practices and does not seem to result\n   in any specific research\
    \ challenge.\n   Another controversial solution in a multi-domain environment\
    \ may be\n   the TCP rate controller (TRC), a traffic conditioner that regulates\n\
    \   the TCP flow at the ingress node in each domain by controlling packet\n  \
    \ drops and delays of the packets in a flow.  The outgoing traffic from\n   a\
    \ TRC-controlled domain is shaped in such a way that no packets are\n   dropped\
    \ at the policer.  However, the TRC interferes with the end-to-\n   end TCP model,\
    \ and thus it would interfere with past and future\n   diversity of TCP implementations\
    \ (violating the end-to-end\n   principle).  In particular, the TRC embeds the\
    \ flow rate equality\n   view of fairness in the network, and would prevent evolution\
    \ to forms\n   of fairness based on congestion-volume (Section 2.3).\n"
- title: 3.5.2.  Multi-Domain Exchange of Topology or Explicit Rate Information
  contents:
  - "3.5.2.  Multi-Domain Exchange of Topology or Explicit Rate Information\n   Security\
    \ is a challenge for multi-domain exchange of explicit rate\n   signals, whether\
    \ in-band or out-of-band.  At domain boundaries,\n   authentication and authorization\
    \ issues can arise whenever congestion\n   control information is exchanged. \
    \ From this perspective, the\n   Internet does not so far have any security architecture\
    \ for this\n   problem.\n   The future evolution of Internet inter-domain operation\
    \ has to show\n   whether more multi-domain information exchange can be effectively\n\
    \   realized.  This is of particular importance for congestion control\n   schemes\
    \ that make use of explicit per-datagram rate feedback (e.g.,\n   RCP or XCP)\
    \ or explicit rate feedback that uses in-band congestion\n   signaling (e.g.,\
    \ Quick-Start) or out-of-band signaling (e.g.,\n   CADPC/PTP).  Explicit signaling\
    \ exchanges at the inter-domain level\n   that result in local domain triggers\
    \ are currently absent from the\n   Internet.  From this perspective, security\
    \ issues resulting from\n   limited trust between different administrative units\
    \ result in policy\n   enforcement that exacerbates the difficulty encountered\
    \ when explicit\n   feedback congestion control information is exchanged between\
    \ domains.\n   Note that even though authentication mechanisms could be extended\
    \ for\n   this purpose (by recognizing that explicit rate schemes such as RCP\n\
    \   or XCP have the same inter-domain security requirements and structure\n  \
    \ as IntServ), they suffer from the same scalability problems as\n   identified\
    \ in [RFC2208].  Indeed, in-band rate signaling or out-of-\n   band per-flow traffic\
    \ specification signaling (like in the Resource\n   Reservation Protocol (RSVP))\
    \ results in similar scalability issues\n   (see Section 3.1).\n   Also, many\
    \ autonomous systems only exchange some limited amount of\n   information about\
    \ their internal state (topology hiding principle),\n   even though having more\
    \ precise information could be highly\n   beneficial for congestion control. \
    \ Indeed, revealing the internal\n   network structure is highly sensitive in\
    \ multi-domain network\n   operations and thus also a concern when it comes to\
    \ the deployability\n   of congestion control schemes.  For instance, a network-assisted\n\
    \   congestion control scheme with explicit signaling could reveal more\n   information\
    \ about the internal network dimensioning than TCP does\n   today.\n"
- title: 3.5.3.  Multi-Domain Pseudowires
  contents:
  - "3.5.3.  Multi-Domain Pseudowires\n   Extending pseudowires across multiple domains\
    \ poses specific issues.\n   Pseudowires (PWs) [RFC3985] may carry non-TCP data\
    \ flows (e.g., Time-\n   Division Multiplexing (TDM) traffic or Constant Bit Rate\
    \ (CBR) ATM\n   traffic) over a multi-domain IP network.  Structure-Agnostic TDM\
    \ over\n   Packet (SAToP) [RFC4553], Circuit Emulation Service over Packet\n \
    \  Switched Network (CESoPSN) [RFC5086], and TDM over IP (TDMoIP)\n   [RFC5087]\
    \ are not responsive to congestion control as discussed in\n   [RFC2914] (see\
    \ also [RFC5033]).  The same observation applies to ATM\n   circuit emulating\
    \ services (CESs) interconnecting CBR equipment\n   (e.g., Private Branch Exchanges\
    \ (PBX)) across a Packet Switched\n   Network (PSN).\n   Moreover, it is not possible\
    \ to simply reduce the flow rate of a TDM\n   PW or an ATM PW when facing packet\
    \ loss.  Providers can rate-control\n   corresponding incoming traffic, but they\
    \ may not be able to detect\n   that PWs carry TDM or CBR ATM traffic (mechanisms\
    \ for characterizing\n   the traffic's temporal properties may not necessarily\
    \ be supported).\n   This can be illustrated with the following example.\n   \
    \             ...........       ............\n               .           .   \
    \  .\n        S1 --- E1 ---      .     .\n               .     |     .     .\n\
    \               .      === E5 === E7 ---\n               .     |     .     . \
    \    |\n        S2 --- E2 ---      .     .     |\n               .           .\
    \     .     |      |\n                ...........      .     |      v\n   .  \
    \                                  ----- R --->\n                ........... \
    \     .     |      ^\n               .           .     .     |      |\n      \
    \  S3 --- E3 ---      .     .     |\n               .     |     .     .     |\n\
    \               .      === E6 === E8 ---\n               .     |     .     .\n\
    \        S4 --- E4 ---      .     .\n               .           .     .\n    \
    \            ...........       ............\n               \\---- P1 ---/   \
    \  \\---------- P2 -----\n   Sources S1, S2, S3, and S4 are originating TDM over\
    \ IP traffic.  P1\n   provider edges E1, E2, E3, and E4 are rate-limiting such\
    \ traffic.\n   The Service Level Agreement (SLA) of provider P1 with transit\n\
    \   provider P2 is such that the latter assumes a BE traffic pattern and\n   that\
    \ the distribution shows the typical properties of common BE\n   traffic (elastic,\
    \ non-real time, non-interactive).\n   The problem arises for transit provider\
    \ P2 because it is not able to\n   detect that IP packets are carrying constant-bit-rate\
    \ service traffic\n   for which the only useful congestion control mechanism would\
    \ rely on\n   implicit or explicit admission control, meaning self-blocking or\n\
    \   enforced blocking, respectively.\n   Assuming P1 providers are rate-limiting\
    \ BE traffic, a transit P2\n   provider router R may be subject to serious congestion\
    \ as all TDM PWs\n   cross the same router.  TCP-friendly traffic (e.g., each\
    \ flow within\n   another PW) would follow TCP's AIMD algorithm of reducing the\
    \ sending\n   rate by half, in response to each packet drop.  Nevertheless, the\
    \ PWs\n   carrying TDM traffic could take all the available capacity while\n \
    \  other more TCP-friendly or generally congestion-responsive traffic\n   reduced\
    \ itself to nothing.  Note here that the situation may simply\n   occur because\
    \ S4 suddenly turns on additional TDM channels.\n   It is neither possible nor\
    \ desirable to assume that edge routers will\n   soon have the ability to detect\
    \ the responsiveness of the carried\n   traffic, but it is still important for\
    \ transit providers to be able\n   to police a fair, robust, responsive, and efficient\
    \ congestion\n   control technique in order to avoid impacting congestion-responsive\n\
    \   Internet traffic.  However, we must not require only certain specific\n  \
    \ responses to congestion to be embedded within the network, which\n   would harm\
    \ evolvability.  So designing the corresponding mechanisms\n   in the data and\
    \ control planes still requires further investigation.\n"
- title: '3.6.  Challenge 6: Precedence for Elastic Traffic'
  contents:
  - "3.6.  Challenge 6: Precedence for Elastic Traffic\n   Traffic initiated by so-called\
    \ elastic applications adapts to the\n   available bandwidth using feedback about\
    \ the state of the network.\n   For elastic applications, the transport dynamically\
    \ adjusts the data\n   traffic sending rate to different network conditions. \
    \ Examples\n   encompass short-lived elastic traffic including HTTP and instant-\n\
    \   messaging traffic, as well as long file transfers with FTP and\n   applications\
    \ targeted by [LEDBAT].  In brief, elastic data\n   applications can show extremely\
    \ different requirements and traffic\n   characteristics.\n   The idea to distinguish\
    \ several classes of best-effort traffic types\n   is rather old, since it would\
    \ be beneficial to address the relative\n   delay sensitivities of different elastic\
    \ applications.  The notion of\n   traffic precedence was already introduced in\
    \ [RFC791], and it was\n   broadly defined as \"An independent measure of the\
    \ importance of this\n   datagram\".  For instance, low-precedence traffic should\
    \ experience\n   lower average throughput than higher-precedence traffic.  Several\n\
    \   questions arise here: What is the meaning of \"relative\"?  What is the\n\
    \   role of the transport layer?\n   The preferential treatment of higher-precedence\
    \ traffic combined with\n   appropriate congestion control mechanisms is still\
    \ an open issue that\n   may, depending on the proposed solution, impact both\
    \ the host and the\n   network precedence awareness, and thereby congestion control.\n\
    \   [RFC2990] points out that the interactions between congestion control\n  \
    \ and DiffServ [RFC2475] remained unaddressed until recently.\n   Recently, a\
    \ study and a potential solution have been proposed that\n   introduce Guaranteed\
    \ TFRC (gTFRC) [Lochin06].  gTFRC is an adaptation\n   of TCP-Friendly Rate Control\
    \ providing throughput guarantees for\n   unicast flows over the DiffServ/Assured\
    \ Forwarding (AF) class.  The\n   purpose of gTFRC is to distinguish the guaranteed\
    \ part from the best-\n   effort part of the traffic resulting from AF conditioning.\
    \  The\n   proposed congestion control has been specified and tested inside\n\
    \   DCCP/CCID 3 for DiffServ/AF networks [Lochin07] [Jourjon08].\n   Nevertheless,\
    \ there is still work to be performed regarding lower-\n   precedence traffic\
    \ -- data transfers that are useful, yet not\n   important enough to warrant significantly\
    \ impairing other traffic.\n   Examples of applications that could make use of\
    \ such traffic are web\n   caches and web browsers (e.g., for pre-fetching) as\
    \ well as peer-to-\n   peer applications.  There are proposals for achieving low\
    \ precedence\n   on a pure end-to-end basis (e.g., TCP Low Priority (TCP-LP)\n\
    \   [Kuzmanovic03]), and there is a specification for achieving it via\n   router\
    \ mechanisms [RFC3662].  It seems, however, that network-based\n   lower-precedence\
    \ mechanisms are not yet a common service on the\n   Internet.  Since early 2010,\
    \ end-to-end mechanisms for lower\n   precedence, e.g., [Shal10], have become\
    \ common -- at least when\n   competing with other traffic as part of its own\
    \ queues (e.g., in a\n   home router).  But it is less clear whether users will\
    \ be willing to\n   make their background traffic yield to other people's foreground\n\
    \   traffic, unless the appropriate incentives are created.\n   There is an issue\
    \ over how to reconcile two divergent views of the\n   relation between traffic\
    \ class precedence and congestion control.\n   One view considers that congestion\
    \ signals (losses or explicit\n   notifications) in one traffic class are independent\
    \ of those in\n   another.  The other relates marking of the classes together\
    \ within\n   the active queue management (AQM) mechanism [Gibbens02].  In the\n\
    \   independent case, using a higher-precedence class of traffic gives a\n   higher\
    \ scheduling precedence and generally lower congestion level.\n   In the linked\
    \ case, using a higher-precedence class of traffic still\n   gives higher scheduling\
    \ precedence, but results in a higher level of\n   congestion.  This higher congestion\
    \ level reflects the extra\n   congestion higher-precedence traffic causes to\
    \ both classes combined.\n   The linked case separates scheduling precedence from\
    \ rate control.\n   The end-to-end congestion control algorithm can separately\
    \ choose to\n   take a higher rate by responding less to the higher level of\n\
    \   congestion.  This second approach could become prevalent if weighted\n   congestion\
    \ controls were common.  However, it is an open issue how\n   the two approaches\
    \ might co-exist or how one might evolve into the\n   other.\n"
- title: '3.7.  Challenge 7: Misbehaving Senders and Receivers'
  contents:
  - "3.7.  Challenge 7: Misbehaving Senders and Receivers\n   In the current Internet\
    \ architecture, congestion control depends on\n   parties acting against their\
    \ own interests.  It is not in a\n   receiver's interest to honestly return feedback\
    \ about congestion on\n   the path, effectively requesting a slower transfer.\
    \  It is not in the\n   sender's interest to reduce its rate in response to congestion\
    \ if it\n   can rely on others to do so.  Additionally, networks may have\n  \
    \ strategic reasons to make other networks appear congested.\n   Numerous strategies\
    \ to improve congestion control have already been\n   identified.  The IETF has\
    \ particularly focused on misbehaving TCP\n   receivers that could confuse a compliant\
    \ sender into assigning\n   excessive network and/or server resources to that\
    \ receiver (e.g.,\n   [Savage99], [RFC3540]).  But, although such strategies are\
    \ worryingly\n   powerful, they do not yet seem common (however, evidence of attack\n\
    \   prevalence is itself a research requirement).\n   A growing proportion of\
    \ Internet traffic comes from applications\n   designed not to use congestion\
    \ control at all, or worse, applications\n   that add more forward error correction\
    \ as they experience more\n   losses.  Some believe the Internet was designed\
    \ to allow such\n   freedom, so it can hardly be called misbehavior.  But others\
    \ consider\n   it misbehavior to abuse this freedom [RFC3714], given one person's\n\
    \   freedom can constrain the freedom of others (congestion represents\n   this\
    \ conflict of interests).  Indeed, leaving freedom unchecked might\n   result\
    \ in congestion collapse in parts of the Internet.\n   Proportionately, large\
    \ volumes of unresponsive voice traffic could\n   represent such a threat, particularly\
    \ for countries with less\n   generous provisioning [RFC3714].  Also, Internet\
    \ video on demand\n   services that transfer much greater data rates without congestion\n\
    \   control are becoming popular.  In general, it is recommended that\n   such\
    \ UDP applications use some form of congestion control [RFC5405].\n   Note that\
    \ the problem is not just misbehavior driven by a self-\n   interested desire\
    \ for more bandwidth.  Indeed, congestion control may\n   be attacked by someone\
    \ who makes no gain for themselves, other than\n   the satisfaction of harming\
    \ others (see Security Considerations in\n   Section 4).\n   Open research questions\
    \ resulting from these considerations are:\n   -  By design, new congestion control\
    \ protocols need to enable one end\n      to check the other for protocol compliance.\
    \  How would such\n      mechanisms be designed?\n   -  Which congestion control\
    \ primitives could safely satisfy more\n      demanding applications (smoother\
    \ than TFRC, faster than high-speed\n      TCPs), so that application developers\
    \ and users do not turn off\n      congestion control to get the rate they expect\
    \ and need?\n   Note also that self-restraint could disappear from the Internet.\
    \  So,\n   it may no longer be sufficient to rely on developers/users\n   voluntarily\
    \ submitting themselves to congestion control.  As a\n   consequence, mechanisms\
    \ to enforce fairness (see Sections 2.3, 3.4,\n   and 3.5) need to have more emphasis\
    \ within the research agenda.\n"
- title: 3.8.  Other Challenges
  contents:
  - "3.8.  Other Challenges\n   This section provides additional challenges and open\
    \ research issues\n   that are not (at this point in time) deemed so significant,\
    \ or they\n   are of a different nature compared to the main challenges depicted\n\
    \   so far.\n"
- title: 3.8.1.  RTT Estimation
  contents:
  - "3.8.1.  RTT Estimation\n   Several congestion control schemes have to precisely\
    \ know the round-\n   trip time (RTT) of a path.  The RTT is a measure of the\
    \ current delay\n   on a network.  It is defined as the delay between the sending\
    \ of a\n   packet and the reception of a corresponding response, if echoed back\n\
    \   immediately by the receiver upon receipt of the packet.  This\n   corresponds\
    \ to the sum of the one-way delay of the packet and the\n   (potentially different)\
    \ one-way delay of the response.  Furthermore,\n   any RTT measurement also includes\
    \ some additional delay due to the\n   packet processing in both end-systems.\n\
    \   There are various techniques to measure the RTT: active measurements\n   inject\
    \ special probe packets into the network and then measure the\n   response time,\
    \ using, e.g., ICMP.  In contrast, passive measurements\n   determine the RTT\
    \ from ongoing communication processes, without\n   sending additional packets.\n\
    \   The connection endpoints of transport protocols such as TCP, the\n   Stream\
    \ Control Transmission Protocol (SCTP), and DCCP, as well as\n   several application\
    \ protocols, keep track of the RTT in order to\n   dynamically adjust protocol\
    \ parameters such as the retransmission\n   timeout (RTO) or the rate-control\
    \ equation.  They can implicitly\n   measure the RTT on the sender side by observing\
    \ the time difference\n   between the sending of data and the arrival of the corresponding\n\
    \   acknowledgments.  For TCP, this is the default RTT measurement\n   procedure;\
    \ it is used in combination with Karn's algorithm, which\n   prohibits RTT measurements\
    \ from retransmitted segments [RFC2988].\n   Traditionally, TCP implementations\
    \ take one RTT measurement at a time\n   (i.e., about once per RTT).  As an alternative,\
    \ the TCP timestamp\n   option [RFC1323] allows more frequent explicit measurements,\
    \ since a\n   sender can safely obtain an RTT sample from every received\n   acknowledgment.\
    \  In principle, similar measurement mechanisms are\n   used by protocols other\
    \ than TCP.\n   Sometimes it would be beneficial to know the RTT not only at the\n\
    \   sender, but also at the receiver, e.g., to find the one-way variation\n  \
    \ in delay due to one-way congestion.  A passive receiver can deduce\n   some\
    \ information about the RTT by analyzing the sequence numbers of\n   received\
    \ segments.  But this method is error-prone and only works if\n   the sender permanently\
    \ sends data.  Other network entities on the\n   path can apply similar heuristics\
    \ in order to approximate the RTT of\n   a connection, but this mechanism is protocol-specific\
    \ and requires\n   per-connection state.  In the current Internet, there is no\
    \ simple\n   and safe solution to determine the RTT of a connection in network\n\
    \   entities other than the sender.  The more fundamental question is to\n   determine\
    \ whether it is necessary or not for network elements to\n   measure or know the\
    \ RTT.\n   As outlined earlier in this document, the round-trip time is\n   typically\
    \ not a constant value.  For a given path, there is a\n   theoretical minimum\
    \ value, which is given by the minimum\n   transmission, processing, and propagation\
    \ delay on that path.\n   However, additional variable delays might be caused\
    \ by congestion,\n   cross-traffic, shared-media access control schemes, recovery\n\
    \   procedures, or other sub-IP layer mechanisms.  Furthermore, a change\n   of\
    \ the path (e.g., route flapping, hand-over in mobile networks) can\n   result\
    \ in completely different delay characteristics.\n   Due to this variability,\
    \ one single measured RTT value is hardly\n   sufficient to characterize a path.\
    \  This is why many protocols use\n   RTT estimators that derive an averaged value\
    \ and keep track of a\n   certain history of previous samples.  For instance,\
    \ TCP endpoints\n   derive a smoothed round-trip time (SRTT) from an exponential\
    \ weighted\n   moving average [RFC2988].  Such a low-pass filter ensures that\n\
    \   measurement noise and single outliers do not significantly affect the\n  \
    \ estimated RTT.  Still, a fundamental drawback of low-pass filters is\n   that\
    \ the averaged value reacts more slowly to sudden changes in the\n   measured\
    \ RTT.  There are various solutions to overcome this effect:\n   For instance,\
    \ the standard TCP retransmission timeout calculation\n   considers not only the\
    \ SRTT, but also a measure for the variability\n   of the RTT measurements [RFC2988].\
    \  Since this algorithm is not well\n   suited for frequent RTT measurements with\
    \ timestamps, certain\n   implementations modify the weight factors (e.g., [Sarola02]).\
    \  There\n   are also proposals for more sophisticated estimators, such as Kalman\n\
    \   filters or estimators that utilize mainly peak values.\n   However, open questions\
    \ related to RTT estimation in the Internet\n   remain:\n   -  Optimal measurement\
    \ frequency: Currently, there is no theory or\n      common understanding of the\
    \ right time scale of RTT measurement.\n      In particular, the necessity for\
    \ rather frequent measurements\n      (e.g., per packet) is not well understood.\
    \  There is some\n      empirical evidence that such frequent sampling may not\
    \ have a\n      significant benefit [Allman99].\n   -  Filter design: A closely\
    \ related question is how to design good\n      filters for the measured samples.\
    \  The existing algorithms are\n      known to be robust, but they are far from\
    \ being perfect.  The\n      fundamental problem is that there is no single set\
    \ of RTT values\n      that could characterize the Internet as a whole, i.e.,\
    \ it is hard\n      to define a design target.\n   -  Default values: RTT estimators\
    \ can fail in certain scenarios,\n      e.g., when any feedback is missing.  In\
    \ this case, default values\n      have to be used.  Today, most default values\
    \ are set to\n      conservative values that may not be optimal for most Internet\n\
    \      communication.  Still, the impact of more aggressive settings is\n    \
    \  not well understood.\n   -  Clock granularities: RTT estimation depends on\
    \ the clock\n      granularities of the protocol stacks.  Even though there is\
    \ a\n      trend toward higher-precision timers, limited granularity\n      (particularly\
    \ on low-cost devices) may still prevent highly\n      accurate RTT estimations.\n"
- title: 3.8.2.  Malfunctioning Devices
  contents:
  - "3.8.2.  Malfunctioning Devices\n   There is a long history of malfunctioning\
    \ devices harming the\n   deployment of new and potentially beneficial functionality\
    \ in the\n   Internet.  Sometimes, such devices drop packets or even crash\n \
    \  completely when a certain mechanism is used, causing users to opt for\n   reliability\
    \ instead of performance and disable the mechanism, or\n   operating-system vendors\
    \ to disable it by default.  One well-known\n   example is ECN, whose deployment\
    \ was long hindered by malfunctioning\n   firewalls and is still hindered by malfunctioning\
    \ home-hubs, but\n   there are many other examples (e.g., the Window Scaling option\
    \ of\n   TCP) [Thaler07].\n   As new congestion control mechanisms are developed\
    \ with the intention\n   of eventually seeing them deployed in the Internet, it\
    \ would be\n   useful to collect information about failures caused by devices\
    \ of\n   this sort, analyze the reasons for these failures, and determine\n  \
    \ whether there are ways for such devices to do what they intend to do\n   without\
    \ causing unintended failures.  Recommendations for vendors of\n   these devices\
    \ could be derived from such an analysis.  It would also\n   be useful to see\
    \ whether there are ways for failures caused by such\n   devices to become more\
    \ visible to endpoints, or to the maintainers of\n   such devices.\n   A possible\
    \ way to reduce such problems in the future would be\n   guidelines for standards\
    \ authors to ensure that \"forward\n   compatibility\" is considered in all IETF\
    \ work.  That is, the default\n   behavior of a device should be precisely defined\
    \ for all possible\n   values and combinations of protocol fields, and not just\
    \ the minimum\n   necessary for the protocol being defined.  Then, when previously\n\
    \   unused or reserved fields start to be used by newer devices to comply\n  \
    \ with a new standard, older devices encountering unusual fields should\n   at\
    \ least behave predictably.\n"
- title: 3.8.3.  Dependence on RTT
  contents:
  - "3.8.3.  Dependence on RTT\n   AIMD window algorithms that have the goal of packet\
    \ conservation end\n   up converging on a rate that is inversely proportional\
    \ to RTT.\n   However, control theoretic approaches to stability have shown that\n\
    \   only the increase in rate (acceleration), and not the target rate,\n   needs\
    \ to be inversely proportional to RTT [Jin04].\n   It is possible to have more\
    \ aggressive behaviors for some demanding\n   applications as long as they are\
    \ part of a mix with less aggressive\n   transports [Key04].  This beneficial\
    \ effect of transport type mixing\n   is probably how the Internet currently manages\
    \ to remain stable even\n   in the presence of TCP slow-start, which is more aggressive\
    \ than the\n   theory allows for stability.  Research giving deeper insight into\n\
    \   these aspects would be very useful.\n"
- title: 3.8.4.  Congestion Control in Multi-Layered Networks
  contents:
  - "3.8.4.  Congestion Control in Multi-Layered Networks\n   A network of IP nodes\
    \ is just as vulnerable to congestion in the\n   lower layers between IP-capable\
    \ nodes as it is to congestion on the\n   IP-capable nodes themselves.  If network\
    \ elements take a greater part\n   in congestion control (ECN, XCP, RCP, etc.\
    \ -- see Section 3.1), these\n   techniques will either need to be deployed at\
    \ lower layers as well,\n   or they will need to interwork with lower-layer mechanisms.\n\
    \   [RFC5129] shows how to propagate ECN from lower layers upwards for\n   the\
    \ specific case of MPLS, but to the authors' knowledge the layering\n   problem\
    \ has not been addressed for explicit rate protocol proposals\n   such as XCP\
    \ and RCP.  Some issues are straightforward matters of\n   interoperability (e.g.,\
    \ how exactly to copy fields up the layers)\n   while others are less obvious\
    \ (e.g., re-framing issues: if RCP were\n   deployed in a lower layer, how might\
    \ multiple small RCP frames, all\n   with different rates in their headers, be\
    \ assembled into a larger IP\n   layer datagram?).\n   Multi-layer considerations\
    \ also confound many mechanisms that aim to\n   discover whether every node on\
    \ the path supports a new congestion\n   control protocol.  For instance, some\
    \ proposals maintain a secondary\n   Time to Live (TTL) field parallel to that\
    \ in the IP header.  Any\n   nodes that support the new behavior update both TTL\
    \ fields, whereas\n   legacy IP nodes will only update the IP TTL field.  This\
    \ allows the\n   endpoints to check whether all IP nodes on the path support the\
    \ new\n   behavior, in which case both TTLs will be equal at the receiver.  But\n\
    \   mechanisms like these overlook nodes at lower layers that might not\n   support\
    \ the new behavior.\n   A further related issue is congestion control across overlay\
    \ networks\n   of relays [Hilt08] [Noel07] [Shen08].\n   Section 3.5.3 deals with\
    \ inelastic multi-domain pseudowires (PWs),\n   where the identity of the pseudowire\
    \ itself implies the\n   characteristics of the traffic crossing the multi-domain\
    \ PSN\n   (independently of the actual characteristics of the traffic carried\n\
    \   in the PW).  A more complex situation arises when inelastic traffic\n   is\
    \ carried as part of a pseudowire (e.g., inelastic traffic over\n   Ethernet PW\
    \ over PSN) whose edges do not have the means to\n   characterize the properties\
    \ of the traffic encapsulated in the\n   Ethernet frames.  In this case, the problem\
    \ explained in\n   Section 3.5.3 is not limited to multi-domain pseudowires but\
    \ more\n   generally arises from a \"pseudowire carrying inelastic traffic\"\n\
    \   (whether over a single- or multi-domain PSN).\n   The problem becomes even\
    \ more intricate when the Ethernet PW carries\n   both inelastic and elastic traffic.\
    \  Addressing this issue further\n   supports our observation that a general framework\
    \ to efficiently deal\n   with congestion control problems in multi-layer networks\
    \ without\n   harming evolvability is absolutely necessary.\n"
- title: 3.8.5.  Multipath End-to-End Congestion Control and Traffic Engineering
  contents:
  - "3.8.5.  Multipath End-to-End Congestion Control and Traffic Engineering\n   Recent\
    \ work has shown that multipath endpoint congestion control\n   [Kelly05] offers\
    \ considerable benefits in terms of resilience and\n   resource usage efficiency.\
    \  The IETF has since initiated a work item\n   on multipath TCP [MPTCP].  By\
    \ pooling the resources on all paths,\n   even nodes not using multiple paths\
    \ benefit from those that are.\n   There is considerable further research to do\
    \ in this area,\n   particularly to understand interactions with network-operator-\n\
    \   controlled route provisioning and traffic engineering, and indeed\n   whether\
    \ multipath congestion control can perform better traffic\n   engineering than\
    \ the network itself, given the right incentives\n   [Arkko09].\n"
- title: 3.8.6.  ALGs and Middleboxes
  contents:
  - "3.8.6.  ALGs and Middleboxes\n   An increasing number of application layer gateways\
    \ (ALGs),\n   middleboxes, and proxies (see Section 3.6 of [RFC2775]) are deployed\n\
    \   at domain boundaries to verify conformance but also filter traffic\n   and\
    \ control flows.  One motivation is to prevent information beyond\n   routing\
    \ data leaking between autonomous systems.  These systems split\n   up end-to-end\
    \ TCP connections and disrupt end-to-end congestion\n   control.  Furthermore,\
    \ transport over encrypted tunnels may not allow\n   other network entities to\
    \ participate in congestion control.\n   Basically, such systems disrupt the primal\
    \ and dual congestion\n   control components.  In particular, end-to-end congestion\
    \ control may\n   be replaced by flow-control backpressure mechanisms on the split\n\
    \   connections.  A large variety of ALGs and middleboxes use such\n   mechanisms\
    \ to improve the performance of applications (Performance\n   Enhancing Proxies,\
    \ Application Accelerators, etc.).  However, the\n   implications of such mechanisms,\
    \ which are often proprietary and not\n   documented, have not been studied systematically\
    \ so far.\n   There are two levels of interference:\n   -  The \"transparent\"\
    \ case, i.e., the endpoint address from the sender\n      perspective is still\
    \ visible to the receiver (the destination IP\n      address).  Relay systems\
    \ that intercept payloads but do not relay\n      congestion control information\
    \ provide an example.  Such\n      middleboxes can prevent the operation of end-to-end\
    \ congestion\n      control.\n   -  The \"non-transparent\" case, which causes\
    \ fewer problems for\n      congestion control.  Although these devices interfere\
    \ with end-to-\n      end network transparency, they correctly terminate network,\n\
    \      transport, and application layer protocols on both sides, which\n     \
    \ individually can be congestion controlled.\n"
- title: 4.  Security Considerations
  contents:
  - "4.  Security Considerations\n   Misbehavior may be driven by pure malice, or\
    \ malice may in turn be\n   driven by wider selfish interests, e.g., using distributed\
    \ denial-of-\n   service (DDoS) attacks to gain rewards by extortion [RFC4948].\
    \  DDoS\n   attacks are possible both because of vulnerabilities in operating\n\
    \   systems and because the Internet delivers packets without requiring\n   congestion\
    \ control.\n   To date, compliance with congestion control rules and being fair\n\
    \   require endpoints to cooperate.  The possibility of uncooperative\n   behavior\
    \ can be regarded as a security issue; its implications are\n   discussed throughout\
    \ these documents in a scattered fashion.\n   Currently the focus of the research\
    \ agenda against denial of service\n   is about identifying attack-packets that\
    \ attack machines and the\n   networks hosting them, with a particular focus on\
    \ mitigating source\n   address spoofing.  But if mechanisms to enforce congestion\
    \ control\n   fairness were robust to both selfishness and malice [Bri06], they\n\
    \   would also naturally mitigate denial of service against the network,\n   which\
    \ can be considered (from the perspective of a well-behaved\n   Internet user)\
    \ as a congestion control enforcement problem.  Even\n   some denial-of-service\
    \ attacks on hosts (rather than the network)\n   could be considered as a congestion\
    \ control enforcement issue at the\n   higher layer.  But clearly there are also\
    \ denial-of-service attacks\n   that would not be solved by enforcing congestion\
    \ control.\n   Sections 3.5 and 3.7 on multi-domain issues and misbehaving senders\n\
    \   and receivers also discuss some information security issues suffered\n   by\
    \ various congestion control approaches.\n"
- title: 5.  References
  contents:
  - '5.  References

    '
- title: 5.1.  Informative References
  contents:
  - "5.1.  Informative References\n   [Allman99]  Allman, M. and V. Paxson, \"On Estimating\
    \ End-to-End\n               Network Path Properties\", Proceedings of ACM SIGCOMM'99,\n\
    \               September 1999.\n   [Andrew05]  Andrew, L., Wydrowski, B., and\
    \ S. Low, \"An Example of\n               Instability in XCP\", Manuscript available\
    \ at\n               <http://netlab.caltech.edu/maxnet/XCP_instability.pdf>.\n\
    \   [Arkko09]   Arkko, J., Briscoe, B., Eggert, L., Feldmann, A., and M.\n   \
    \            Handley, \"Dagstuhl Perspectives Workshop on End-to-End\n       \
    \        Protocols for the Future Internet,\" ACM SIGCOMM Computer\n         \
    \      Communication Review, Vol. 39, No. 2, pp. 42-47, April\n              \
    \ 2009.\n   [Ath01]     Athuraliya, S., Low, S., Li, V., and Q. Yin, \"REM: Active\n\
    \               Queue Management\", IEEE Network Magazine, Vol. 15, No. 3,\n \
    \              pp. 48-53, May 2001.\n   [Balan01]   Balan, R.K., Lee, B.P., Kumar,\
    \ K.R.R., Jacob, L., Seah,\n               W.K.G., and A.L. Ananda, \"TCP HACK:\
    \ TCP Header Checksum\n               Option to Improve Performance over Lossy\
    \ Links\",\n               Proceedings of IEEE INFOCOM'01, Anchorage (Alaska),\
    \ USA,\n               April 2001.\n   [Bonald00]  Bonald, T., May, M., and J.-C.\
    \ Bolot, \"Analytic\n               Evaluation of RED Performance\", Proceedings\
    \ of IEEE\n               INFOCOM'00, Tel Aviv, Israel, March 2000.\n   [Bri06]\
    \     Briscoe, B., \"Using Self-interest to Prevent Malice;\n               Fixing\
    \ the Denial of Service Flaw of the Internet\",\n               Workshop on the\
    \ Economics of Securing the Information\n               Infrastructure, October\
    \ 2006,\n               <http://wesii.econinfosec.org/draft.php?paper_id=19>.\n\
    \   [Bri07]     Briscoe, B., \"Flow Rate Fairness: Dismantling a\n           \
    \    Religion\", ACM SIGCOMM Computer Communication Review,\n               Vol.\
    \ 37, No. 2, pp. 63-74, April 2007.\n   [Bri08]     Briscoe, B., Moncaster, T.\
    \ and L. Burness, \"Problem\n               Statement: Transport Protocols Don't\
    \ Have To Do\n               Fairness\", Work in Progress, July 2008.\n   [Bri09]\
    \     Briscoe, B., \"Re-feedback: Freedom with Accountability\n              \
    \ for Causing Congestion in a Connectionless Internetwork\",\n               UCL\
    \ PhD Thesis (2009).\n   [Bri10]     Briscoe, B. and J. Manner, \"Byte and Packet\
    \ Congestion\n               Notification,\" Work in Progress, October 2010.\n\
    \   [Chester04] Chesterfield, J., Chakravorty, R., Banerjee, S.,\n           \
    \    Rodriguez, P., Pratt, I., and J. Crowcroft, \"Transport\n               level\
    \ optimisations for streaming media over wide-area\n               wireless networks\"\
    , WIOPT'04, March 2004.\n   [Chhabra02] Chhabra, P., Chuig, S., Goel, A., John,\
    \ A., Kumar, A.,\n               Saran, H., and R. Shorey, \"XCHOKe: Malicious\
    \ Source\n               Control for Congestion Avoidance at Internet Gateways,\"\
    \n               Proceedings of IEEE International Conference on Network\n   \
    \            Protocols (ICNP'02), Paris, France, November 2002.\n   [Chiu89] \
    \   Chiu, D.M. and R. Jain, \"Analysis of the increase and\n               decrease\
    \ algorithms for congestion avoidance in computer\n               networks\",\
    \ Computer Networks and ISDN Systems, Vol. 17,\n               pp. 1-14, 1989.\n\
    \   [Clark88]   Clark, D., \"The design philosophy of the DARPA internet\n   \
    \            protocols\", ACM SIGCOMM Computer Communication Review,\n       \
    \        Vol. 18, No. 4, pp. 106-114, August 1988.\n   [Clark98]   Clark, D. and\
    \ W. Fang, \"Explicit Allocation of Best-\n               Effort Packet Delivery\
    \ Service\", IEEE/ACM Transactions on\n               Networking, Vol. 6, No.\
    \ 4, pp. 362-373, August 1998.\n   [Chu10]     Chu, J., Dukkipati, N., Cheng,\
    \ Y., and M. Mathis,\n               \"Increasing TCP's Initial Window\", Work\
    \ in Progress,\n               October 2010.\n   [CONEX]     IETF WG Action: Congestion\
    \ Exposure (conex).\n   [Dukki05]   Dukkipati, N., Kobayashi, M., Zhang-Shen,\
    \ R., and N.\n               McKeown, \"Processor Sharing Flows in the Internet\"\
    ,\n               Proceedings of International Workshop on Quality of\n      \
    \         Service (IWQoS'05), Passau, Germany, June 2005.\n   [Dukki06]   Dukkipati,\
    \ N. and N. McKeown, \"Why Flow-Completion Time\n               is the Right Metric\
    \ for Congestion Control\", ACM SIGCOMM\n               Computer Communication\
    \ Review, Vol. 36, No. 1, January\n               2006.\n   [ECODE]     \"ECODE\
    \ Project\", European Commission Seventh Framework\n               Program, Grant\
    \ No. 223936, <http://www.ecode-project.eu>.\n   [Falk07]    Falk, A., Pryadkin,\
    \ Y., and D. Katabi, \"Specification for\n               the Explicit Control\
    \ Protocol (XCP)\", Work in Progress,\n               January 2007.\n   [Feldman04]\n\
    \               Feldman, M., Papadimitriou, C., Chuang, J., and I.\n         \
    \      Stoica, \"Free-Riding and Whitewashing in Peer-to-Peer\n              \
    \ Systems\" Proceedings of ACM SIGCOMM Workshop on Practice\n               and\
    \ Theory of Incentives in Networked Systems (PINS'04)\n               2004.\n\
    \   [Firoiu00]  Firoiu, V. and M. Borden, \"A Study of Active Queue\n        \
    \       Management for Congestion Control\", Proceedings of IEEE\n           \
    \    INFOCOM'00, Tel Aviv, Israel, March 2000.\n   [Floyd93]   Floyd, S. and V.\
    \ Jacobson, \"Random early detection\n               gateways for congestion avoidance\"\
    , IEEE/ACM Transactions\n               on Networking, Vol. 1, No. 4, pp. 397-413,\
    \ August 1993.\n   [Floyd94]   Floyd, S., \"TCP and Explicit Congestion Notification\"\
    ,\n               ACM Computer Communication Review, Vol. 24, No. 5,\n       \
    \        pp. 10-23, October 1994.\n   [Gibbens02] Gibbens, R. and Kelly, F., \"\
    On Packet Marking at Priority\n               Queues\", IEEE Transactions on Automatic\
    \ Control, Vol. 47,\n               No. 6, pp. 1016-1020, 2002.\n   [Ha08]   \
    \   Ha, S., Rhee, I., and L. Xu, \"CUBIC: A new TCP-friendly\n               high-speed\
    \ TCP variant\", ACM SIGOPS Operating System\n               Review, Vol. 42,\
    \ No. 5, pp. 64-74, 2008.\n   [Hilt08]    Hilt, V. and I. Widjaja, \"Controlling\
    \ Overload in\n               Networks of SIP Servers\", Proceedings of IEEE\n\
    \               International Conference on Network Protocols (ICNP'08),\n   \
    \            Orlando (Florida), USA, October 2008.\n   [Hollot01]  Hollot, C.,\
    \ Misra, V., Towsley, D., and W.-B. Gong, \"A\n               Control Theoretic\
    \ Analysis of RED\", Proceedings of IEEE\n               INFOCOM'01, Anchorage\
    \ (Alaska), USA, April 2001.\n   [Jacobson88]\n               Jacobson, V., \"\
    Congestion Avoidance and Control\",\n               Proceedings of ACM SIGCOMM'88\
    \ Symposium, August 1988.\n   [Jain88]    Jain, R. and K. Ramakrishnan, \"Congestion\
    \ Avoidance in\n               Computer Networks with a Connectionless Network\
    \ Layer:\n               Concepts, Goals, and Methodology\", Proceedings of IEEE\n\
    \               Computer Networking Symposium, Washington DC, USA, April\n   \
    \            1988.\n   [Jain90]    Jain, R., \"Congestion Control in Computer\
    \ Networks:\n               Trends and Issues\", IEEE Network, pp. 24-30, May\
    \ 1990.\n   [Jin04]     Jin, Ch., Wei, D.X., and S. Low, \"FAST TCP: Motivation,\n\
    \               Architecture, Algorithms, Performance\", Proceedings of\n    \
    \           IEEE INFOCOM'04, Hong-Kong, China, March 2004.\n   [Jourjon08] Jourjon,\
    \ G., Lochin, E., and P. Senac, \"Design,\n               Implementation and Evaluation\
    \ of a QoS-aware Transport\n               Protocol\", Elsevier Computer Communications,\
    \ Vol. 31,\n               No. 9, pp. 1713-1722, June 2008.\n   [Katabi02]  Katabi,\
    \ D., M. Handley, and C. Rohrs, \"Internet\n               Congestion Control\
    \ for Future High Bandwidth-Delay\n               Product Environments\", Proceedings\
    \ of ACM SIGCOMM'02\n               Symposium, August 2002.\n   [Katabi04]  Katabi,\
    \ D., \"XCP Performance in the Presence of Malicious\n               Flows\",\
    \ Proceedings of PFLDnet'04 Workshop, Argonne\n               (Illinois), USA,\
    \ February 2004.\n   [Kelly05]   Kelly, F. and Th. Voice, \"Stability of end-to-end\n\
    \               algorithms for joint routing and rate control\", ACM\n       \
    \        SIGCOMM Computer Communication Review, Vol. 35, No. 2,\n            \
    \   pp. 5-12, April 2005.\n   [Kelly98]   Kelly, F., Maulloo, A., and D. Tan,\
    \ \"Rate control in\n               communication networks: shadow prices, proportional\n\
    \               fairness, and stability\", Journal of the Operational\n      \
    \         Research Society, Vol. 49, pp. 237-252, 1998.\n   [Keshav07]  Keshav,\
    \ S., \"What is congestion and what is congestion\n               control\", Presentation\
    \ at IRTF ICCRG Workshop, PFLDnet\n               2007, Los Angeles (California),\
    \ USA, February 2007.\n   [Key04]     Key, P., Massoulie, L., Bain, A., and F.\
    \ Kelly, \"Fair\n               Internet Traffic Integration: Network Flow Models\
    \ and\n               Analysis\", Annales des Telecommunications, Vol. 59,\n \
    \              No. 11-12, pp. 1338-1352, November-December 2004.\n   [Krishnan04]\n\
    \               Krishnan, R., Sterbenz, J., Eddy, W., Partridge, C., and\n   \
    \            M. Allman, \"Explicit Transport Error Notification (ETEN)\n     \
    \          for Error-Prone Wireless and Satellite Networks\",\n              \
    \ Computer Networks, Vol. 46, No. 3, October 2004.\n   [Kuzmanovic03]\n      \
    \         Kuzmanovic, A. and E.W. Knightly, \"TCP-LP: A Distributed\n        \
    \       Algorithm for Low Priority Data Transfer\", Proceedings of\n         \
    \      IEEE INFOCOM'03, San Francisco (California), USA, April\n             \
    \  2003.\n   [LEDBAT]    IETF WG Action: Low Extra Delay Background Transport\n\
    \               (ledbat).\n   [Lochin06]  Lochin, E., Dairaine, L., and G. Jourjon,\
    \ \"Guaranteed TCP\n               Friendly Rate Control (gTFRC) for DiffServ/AF\
    \ Network\",\n               Work in Progress, August 2006.\n   [Lochin07]  Lochin,\
    \ E., Jourjon, G., and L. Dairaine, \"Study and\n               enhancement of\
    \ DCCP over DiffServ Assured Forwarding\n               class\", 4th Conference\
    \ on Universal Multiservice Networks\n               (ECUMN 2007), Toulouse, France,\
    \ February 2007.\n   [Low02]     Low, S., Paganini, F., Wang, J., Adlakha, S.,\
    \ and J.C.\n               Doyle, \"Dynamics of TCP/RED and a Scalable Control\"\
    ,\n               Proceedings of IEEE INFOCOM'02, New York (New Jersey),\n   \
    \            2002.\n   [Low03.1]   Low, S., \"A duality model of TCP and queue\
    \ management\n               algorithms\", IEEE/ACM Transactions on Networking,\n\
    \               Vol. 11, No. 4, pp. 525-536, August 2003.\n   [Low03.2]   Low,\
    \ S., Paganini, F., Wang, J., and J. Doyle, \"Linear\n               stability\
    \ of TCP/RED and a scalable control\", Computer\n               Networks Journal,\
    \ Vol. 43, No. 5, pp. 633-647, December\n               2003.\n   [Low05]    \
    \ Low, S., Andrew, L., and B. Wydrowski, \"Understanding\n               XCP:\
    \ equilibrium and fairness\", Proceedings of IEEE\n               INFOCOM'05,\
    \ Miami (Florida), USA, March 2005.\n   [MacK95]    MacKie-Mason, J. and H. Varian,\
    \ \"Pricing Congestible\n               Network Resources\", IEEE Journal on Selected\
    \ Areas in\n               Communications, Advances in the Fundamentals of\n \
    \              Networking, Vol. 13, No. 7, pp. 1141-1149, 1995.\n   [Mascolo01]\
    \ Mascolo, S., Casetti, Cl., Gerla M., Sanadidi, M.Y., and\n               R.\
    \ Wang, \"TCP Westwood: Bandwidth estimation for enhanced\n               transport\
    \ over wireless links\", Proceedings of MOBICOM\n               2001, Rome, Italy,\
    \ July 2001.\n   [Moors02]   Moors, T., \"A critical review of \"End-to-end arguments\
    \ in\n               system design\"\", Proceedings of IEEE International\n  \
    \             Conference on Communications (ICC) 2002, New York City\n       \
    \        (New Jersey), USA, April/May 2002.\n   [MPTCP]     IETF WG Action: Multipath\
    \ TCP (mptcp).\n   [Noel07]    Noel, E. and C. Johnson, \"Initial Simulation Results\
    \ That\n               Analyze SIP Based VoIP Networks Under Overload\",\n   \
    \            International Teletraffic Congress (ITC'07), Ottawa,\n          \
    \     Canada, June 2007.\n   [Padhye98]  Padhye, J., Firoiu, V., Towsley, D.,\
    \ and J. Kurose,\n               \"Modeling TCP Throughput: A Simple Model and\
    \ Its\n               Empirical Validation\", University of Massachusetts\n  \
    \             (UMass), CMPSCI Tech. Report TR98-008, February 1998.\n   [Pan00]\
    \     Pan, R., Prabhakar, B., and K. Psounis, \"CHOKe: a\n               stateless\
    \ AQM scheme for approximating fair bandwidth\n               allocation\", Proceedings\
    \ of IEEE INFOCOM'00, Tel Aviv,\n               Israel, March 2000.\n   [Pap02]\
    \     Papadimitriou, I. and G. Mavromatis, \"Stability of\n               Congestion\
    \ Control Algorithms using Control Theory with\n               an application\
    \ to XCP\", Technical Report, 2002.\n               <http://www.stanford.edu/class/ee384y/projects/\n\
    \               reports/ionnis.pdf>.\n   [RFC791]    Postel, J., \"Internet Protocol\"\
    , STD 5, RFC 791,\n               September 1981.\n   [RFC793]    Postel, J.,\
    \ \"Transmission Control Protocol\", STD 7,\n               RFC 793, September\
    \ 1981.\n   [RFC1323]   Jacobson, V., Braden, R., and D. Borman, \"TCP Extensions\n\
    \               for High Performance\", RFC 1323, May 1992.\n   [RFC1701]   Hanks,\
    \ S., Li, T., Farinacci, D., and P. Traina, \"Generic\n               Routing\
    \ Encapsulation (GRE)\", RFC 1701, October 1994.\n   [RFC1958]   Carpenter, B.,\
    \ Ed., \"Architectural Principles of the\n               Internet\", RFC 1958,\
    \ June 1996.\n   [RFC2003]   Perkins, C., \"IP Encapsulation within IP\", RFC\
    \ 2003,\n               October 1996.\n   [RFC2018]   Mathis, M., Mahdavi, J.,\
    \ Floyd, S., and A. Romanow, \"TCP\n               Selective Acknowledgment Options\"\
    , RFC 2018, October\n               1996.\n   [RFC2208]   Mankin, A., Ed., Baker,\
    \ F., Braden, B., Bradner, S.,\n               O'Dell, M., Romanow, A., Weinrib,\
    \ A., and L. Zhang,\n               \"Resource ReSerVation Protocol (RSVP) --\
    \ Version 1\n               Applicability Statement Some Guidelines on Deployment\"\
    ,\n               RFC 2208, September 1997.\n   [RFC2474]   Nichols, K., Blake,\
    \ S., Baker, F., and D. Black,\n               \"Definition of the Differentiated\
    \ Services Field (DS\n               Field) in the IPv4 and IPv6 Headers\", RFC\
    \ 2474, December\n               1998.\n   [RFC2475]   Blake, S., Black, D., Carlson,\
    \ M., Davies, E., Wang, Z.,\n               and W. Weiss, \"An Architecture for\
    \ Differentiated\n               Service\", RFC 2475, December 1998.\n   [RFC2581]\
    \   Allman, M., Paxson, V., and W. Stevens, \"TCP Congestion\n               Control\"\
    , RFC 2581, April 1999.\n   [RFC2637]   Hamzeh, K., Pall, G., Verthein, W., Taarud,\
    \ J., Little,\n               W., and G. Zorn, \"Point-to-Point Tunneling Protocol\n\
    \               (PPTP)\", RFC 2637, July 1999.\n   [RFC2661]   Townsley, W., Valencia,\
    \ A., Rubens, A., Pall, G., Zorn,\n               G., and B. Palter, \"Layer Two\
    \ Tunneling Protocol \"L2TP\"\",\n               RFC 2661, August 1999.\n   [RFC2775]\
    \   Carpenter, B., \"Internet Transparency\", RFC 2775,\n               February\
    \ 2000.\n   [RFC2784]   Farinacci, D., Li, T., Hanks, S., Meyer, D., and P.\n\
    \               Traina, \"Generic Routing Encapsulation (GRE)\", RFC 2784,\n \
    \              March 2000.\n   [RFC2861]   Handley, M., Padhye, J., and S. Floyd,\
    \ \"TCP Congestion\n               Window Validation\", RFC 2861, June 2000.\n\
    \   [RFC2914]   Floyd, S., \"Congestion Control Principles\", BCP 41,\n      \
    \         RFC 2914, September 2000.\n   [RFC2988]   Paxson, V. and M. Allman,\
    \ \"Computing TCP's Retransmission\n               Timer\", RFC 2988, November\
    \ 2000.\n   [RFC2990]   Huston, G., \"Next Steps for the IP QoS Architecture\"\
    ,\n               RFC 2990, November 2000.\n   [RFC3031]   Rosen, E., Viswanathan,\
    \ A., and R. Callon, \"Multiprotocol\n               Label Switching Architecture\"\
    , RFC 3031, January 2001.\n   [RFC3032]   Rosen, E., Tappan, D., Fedorkow, G.,\
    \ Rekhter, Y.,\n               Farinacci, D., Li, T., and A. Conta, \"MPLS Label\
    \ Stack\n               Encoding\", RFC 3032, January 2001.\n   [RFC3168]   Ramakrishnan,\
    \ K., Floyd, S., and D. Black, \"The Addition\n               of Explicit Congestion\
    \ Notification (ECN) to IP\",\n               RFC 3168, September 2001.\n   [RFC3260]\
    \   Grossman, D., \"New Terminology and Clarifications for\n               Diffserv\"\
    , RFC 3260, April 2002.\n   [RFC3517]   Blanton, E., Allman, M., Fall, K., and\
    \ L. Wang, \"A\n               Conservative Selective Acknowledgment (SACK)-based\
    \ Loss\n               Recovery Algorithm for TCP\", RFC 3517, April 2003.\n \
    \  [RFC3540]   Spring, N., Wetherall, D., and D. Ely, \"Robust Explicit\n    \
    \           Congestion Notification (ECN) Signaling with Nonces\",\n         \
    \      RFC 3540, June 2003.\n   [RFC3662]   Bless, R., Nichols, K., and K. Wehrle,\
    \ \"A Lower Effort\n               Per-Domain Behavior (PDB) for Differentiated\
    \ Services\",\n               RFC 3662, December 2003.\n   [RFC3714]   Floyd,\
    \ S., Ed., and J. Kempf, Ed., \"IAB Concerns\n               Regarding Congestion\
    \ Control for Voice Traffic in the\n               Internet\", RFC 3714, March\
    \ 2004.\n   [RFC3742]   Floyd, S., \"Limited Slow-Start for TCP with Large\n \
    \              Congestion Windows\", RFC 3742, March 2004.\n   [RFC3985]   Bryant,\
    \ S., Ed., and P. Pate, Ed., \"Pseudo Wire Emulation\n               Edge-to-Edge\
    \ (PWE3) Architecture\", RFC 3985, March 2005.\n   [RFC4301]   Kent, S. and K.\
    \ Seo, \"Security Architecture for the\n               Internet Protocol\", RFC\
    \ 4301, December 2005.\n   [RFC4340]   Kohler, E., Handley, M., and S. Floyd,\
    \ \"Datagram\n               Congestion Control Protocol (DCCP)\", RFC 4340, March\n\
    \               2006.\n   [RFC4341]   Floyd, S. and E. Kohler, \"Profile for Datagram\
    \ Congestion\n               Control Protocol (DCCP) Congestion Control ID 2:\
    \ TCP-like\n               Congestion Control\", RFC 4341, March 2006.\n   [RFC4342]\
    \   Floyd, S., Kohler, E., and J. Padhye, \"Profile for\n               Datagram\
    \ Congestion Control Protocol (DCCP) Congestion\n               Control ID 3:\
    \ TCP-Friendly Rate Control (TFRC)\",\n               RFC 4342, March 2006.\n\
    \   [RFC4553]   Vainshtein, A., Ed., and YJ. Stein, Ed., \"Structure-\n      \
    \         Agnostic Time Division Multiplexing (TDM) over Packet\n            \
    \   (SAToP)\", RFC 4553, June 2006.\n   [RFC4614]   Duke, M., Braden, R., Eddy,\
    \ W., and E. Blanton, \"A\n               Roadmap for Transmission Control Protocol\
    \ (TCP)\n               Specification Documents\", RFC 4614, September 2006.\n\
    \   [RFC4782]   Floyd, S., Allman, M., Jain, A., and P. Sarolahti,\n         \
    \      \"Quick-Start for TCP and IP\", RFC 4782, January 2007.\n   [RFC4828] \
    \  Floyd, S. and E. Kohler, \"TCP Friendly Rate Control\n               (TFRC):\
    \ The Small-Packet (SP) Variant\", RFC 4828, April\n               2007.\n   [RFC4948]\
    \   Andersson, L., Davies, E., and L. Zhang, \"Report from the\n             \
    \  IAB workshop on Unwanted Traffic March 9-10, 2006\",\n               RFC 4948,\
    \ August 2007.\n   [RFC5033]   Floyd, S. and M. Allman, \"Specifying New Congestion\n\
    \               Control Algorithms\", BCP 133, RFC 5033, August 2007.\n   [RFC5086]\
    \   Vainshtein, A., Ed., Sasson, I., Metz, E., Frost, T., and\n              \
    \ P. Pate, \"Structure-Aware Time Division Multiplexed (TDM)\n               Circuit\
    \ Emulation Service over Packet Switched Network\n               (CESoPSN)\",\
    \ RFC 5086, December 2007.\n   [RFC5087]   Stein, Y(J)., Shashoua, R., Insler,\
    \ R., and M. Anavi,\n               \"Time Division Multiplexing over IP (TDMoIP)\"\
    , RFC 5087,\n               December 2007.\n   [RFC5129]   Davie, B., Briscoe,\
    \ B., and J. Tay, \"Explicit Congestion\n               Marking in MPLS\", RFC\
    \ 5129, January 2008.\n   [RFC5290]   Floyd, S. and M. Allman, \"Comments on the\
    \ Usefulness of\n               Simple Best-Effort Traffic\", RFC 5290, July 2008.\n\
    \   [RFC5348]   Floyd, S., Handley, M., Padhye, J., and J. Widmer, \"TCP\n   \
    \            Friendly Rate Control (TFRC): Protocol Specification\",\n       \
    \        RFC 5348, September 2008.\n   [RFC5405]   Eggert, L. and G. Fairhurst,\
    \ \"Unicast UDP Usage\n               Guidelines for Application Designers\",\
    \ BCP 145, RFC 5405,\n               November 2008.\n   [RFC5622]   Floyd, S.\
    \ and E. Kohler, \"Profile for Datagram Congestion\n               Control Protocol\
    \ (DCCP) Congestion ID 4: TCP-Friendly\n               Rate Control for Small\
    \ Packets (TFRC-SP)\", RFC 5622,\n               August 2009.\n   [RFC5681]  \
    \ Allman, M., Paxson, V., and E. Blanton, \"TCP Congestion\n               Control\"\
    , RFC 5681 (Obsoletes RFC 2581), September 2009.\n   [RFC5783]   Welzl, M. and\
    \ W. Eddy, \"Congestion Control in the RFC\n               Series\", RFC 5783,\
    \ February 2010.\n   [RFC6040]   Briscoe, B., \"Tunnelling of Explicit Congestion\n\
    \               Notification\", RFC 6040, November 2010.\n   [Rossi06]   Rossi,\
    \ M., \"Evaluating TCP with Corruption Notification\n               in an IEEE\
    \ 802.11 Wireless LAN\", Master Thesis,\n               University of Innsbruck,\
    \ November 2006.  Available from\n               http://heim.ifi.uio.no/michawe/research/projects/\n\
    \               corruption/.\n   [Saltzer84] Saltzer, J., Reed, D., and D. Clark,\
    \ \"End-to-end\n               arguments in system design\", ACM Transactions\
    \ on Computer\n               Systems, Vol. 2, No. 4, November 1984.\n   [Sarola02]\
    \  Sarolahti, P. and A. Kuznetsov, \"Congestion Control in\n               Linux\
    \ TCP\", Proceedings of the USENIX Annual Technical\n               Conference,\
    \ 2002.\n   [Sarola07]  Sarolahti, P., Floyd, S., and M. Kojo, \"Transport-layer\n\
    \               Considerations for Explicit Cross-layer Indications\",\n     \
    \          Work in Progress, March 2007.\n   [Savage99]  Savage, S., Cardwell,\
    \ N., Wetherall, D., and T.\n               Anderson, \"TCP Congestion Control\
    \ with a Misbehaving\n               Receiver\", ACM SIGCOMM Computer Communication\
    \ Review,\n               1999.\n   [Shal10]    Shalunov, S., Hazel, G., and J.\
    \ Iyengar, \"Low Extra Delay\n               Background Transport (LEDBAT)\",\
    \ Work in Progress, October\n               2010.\n   [Shen08]    Shen, C., Schulzrinne,\
    \ H., and E. Nahum, \"Session\n               Initiation Protocol (SIP) Server\
    \ Overload Control: Design\n               and Evaluation, Principles\", Systems\
    \ and Applications of\n               IP Telecommunications (IPTComm'08), Heidelberg,\
    \ Germany,\n               July 2008.\n   [Shin08]    Shin, M., Chong, S., and\
    \ I. Rhee, \"Dual-Resource TCP/AQM\n               for Processing-Constrained\
    \ Networks\", IEEE/ACM\n               Transactions on Networking, Vol. 16, No.\
    \ 2, pp. 435-449,\n               April 2008.\n   [Thaler07]  Thaler, D., Sridharan,\
    \ M., and D. Bansal, \"Implementation\n               Report on Experiences with\
    \ Various TCP RFCs\",\n               Presentation to the IETF Transport Area,\
    \ March 2007.\n               <http://www.ietf.org/proceedings/07mar/\n      \
    \         slides/tsvarea-3/>.\n   [Tickoo05]  Tickoo, O., Subramanian, V., Kalyanaraman,\
    \ S., and K.K.\n               Ramakrishnan, \"LT-TCP: End-to-End Framework to\
    \ Improve\n               TCP Performance over Networks with Lossy Channels\"\
    ,\n               Proceedings of International Workshop on QoS (IWQoS),\n    \
    \           Passau, Germany, June 2005.\n   [TRILOGY]   \"Trilogy Project\", European\
    \ Commission Seventh Framework\n               Program (FP7), Grant No: 216372,\
    \ <http://www.trilogy-\n               project.org>.\n   [Vinnic02]  Vinnicombe,\
    \ G., \"On the stability of networks operating\n               TCP-like congestion\
    \ control,\" Proceedings of IFAC World\n               Congress, Barcelona, Spain,\
    \ 2002.\n   [Welzl03]   Welzl, M., \"Scalable Performance Signalling and\n   \
    \            Congestion Avoidance\", Springer (ISBN 1-4020-7570-7),\n        \
    \       September 2003.\n   [Welzl08]   Welzl, M., Rossi, M., Fumagalli, A., and\
    \ M. Tacca,\n               \"TCP/IP over IEEE 802.11b WLAN: the Challenge of\n\
    \               Harnessing Known-Corrupt Data\", Proceedings of IEEE\n       \
    \        International Conference on Communications (ICC) 2008,\n            \
    \   Beijing, China, May 2008.\n   [Xia05]     Xia, Y., Subramanian, L., Stoica,\
    \ I., and S.\n               Kalyanaraman, \"One more bit is enough\", ACM SIGCOMM\n\
    \               Computer Communication Review, Vol. 35, No. 4, pp. 37-48,\n  \
    \             2005.\n   [Zhang03]   Zhang, H., Towsley, D., Hollot, C., and V.\
    \ Misra, \"A\n               Self-Tuning Structure for Adaptation in TCP/AQM\n\
    \               Networks\", Proceedings of ACM SIGMETRICS'03 Conference,\n   \
    \            San Diego (California), USA, June 2003.\n"
- title: 6.  Acknowledgments
  contents:
  - "6.  Acknowledgments\n   The authors would like to thank the following people\
    \ whose feedback\n   and comments contributed to this document: Keith Moore, Jan\n\
    \   Vandenabeele, and Larry Dunn (his comments at the Manchester ICCRG\n   and\
    \ discussions with him helped with the section on packet-\n   congestibility).\n\
    \   Dimitri Papadimitriou's contribution was partly funded by [ECODE], a\n   Seventh\
    \ Framework Program (FP7) research project sponsored by the\n   European Commission.\n\
    \   Bob Briscoe's contribution was partly funded by [TRILOGY], a research\n  \
    \ project supported by the European Commission.\n   Michael Scharf is now with\
    \ Alcatel-Lucent.\n"
- title: 7.  Contributors
  contents:
  - "7.  Contributors\n   The following additional people have contributed to this\
    \ document:\n   - Wesley Eddy <weddy@grc.nasa.gov>\n   - Bela Berde <bela.berde@gmx.de>\n\
    \   - Paulo Loureiro <loureiro.pjg@gmail.com>\n   - Chris Christou <christou_chris@bah.com>\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Dimitri Papadimitriou (editor)\n   Alcatel-Lucent\n  \
    \ Copernicuslaan, 50\n   2018 Antwerpen, Belgium\n   Phone: +32 3 240 8491\n \
    \  EMail: dimitri.papadimitriou@alcatel-lucent.com\n   Michael Welzl\n   University\
    \ of Oslo, Department of Informatics\n   PO Box 1080 Blindern\n   N-0316 Oslo,\
    \ Norway\n   EMail: michawe@ifi.uio.no\n   Michael Scharf\n   University of Stuttgart\n\
    \   Pfaffenwaldring 47\n   70569 Stuttgart, Germany\n   EMail: michael.scharf@googlemail.com\n\
    \   Bob Briscoe\n   BT & UCL\n   B54/77, Adastral Park\n   Martlesham Heath\n\
    \   Ipswich IP5 3RE, UK\n   EMail: bob.briscoe@bt.com\n"
