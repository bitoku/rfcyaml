- title: __initial_text__
  contents:
  - '           Use of BGP for Routing in Large-Scale Data Centers

    '
- title: Abstract
  contents:
  - "Abstract\n   Some network operators build and operate data centers that support\n\
    \   over one hundred thousand servers.  In this document, such data\n   centers\
    \ are referred to as \"large-scale\" to differentiate them from\n   smaller infrastructures.\
    \  Environments of this scale have a unique\n   set of network requirements with\
    \ an emphasis on operational\n   simplicity and network stability.  This document\
    \ summarizes\n   operational experience in designing and operating large-scale\
    \ data\n   centers using BGP as the only routing protocol.  The intent is to\n\
    \   report on a proven and stable routing design that could be leveraged\n   by\
    \ others in the industry.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7938.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2016 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n   2.  Network Design Requirements . . . . . . . . . . . . .\
    \ . . . .   4\n     2.1.  Bandwidth and Traffic Patterns  . . . . . . . . . .\
    \ . . .   4\n     2.2.  CAPEX Minimization  . . . . . . . . . . . . . . . . .\
    \ . .   4\n     2.3.  OPEX Minimization . . . . . . . . . . . . . . . . . . .\
    \ .   5\n     2.4.  Traffic Engineering . . . . . . . . . . . . . . . . . . .\
    \   5\n     2.5.  Summarized Requirements . . . . . . . . . . . . . . . . .  \
    \ 6\n   3.  Data Center Topologies Overview . . . . . . . . . . . . . . .   6\n\
    \     3.1.  Traditional DC Topology . . . . . . . . . . . . . . . . .   6\n  \
    \   3.2.  Clos Network Topology . . . . . . . . . . . . . . . . . .   7\n    \
    \   3.2.1.  Overview  . . . . . . . . . . . . . . . . . . . . . .   7\n      \
    \ 3.2.2.  Clos Topology Properties  . . . . . . . . . . . . . .   8\n       3.2.3.\
    \  Scaling the Clos Topology . . . . . . . . . . . . . .   9\n       3.2.4.  Managing\
    \ the Size of Clos Topology Tiers  . . . . . .  10\n   4.  Data Center Routing\
    \ Overview  . . . . . . . . . . . . . . . .  11\n     4.1.  L2-Only Designs .\
    \ . . . . . . . . . . . . . . . . . . . .  11\n     4.2.  Hybrid L2/L3 Designs\
    \  . . . . . . . . . . . . . . . . . .  12\n     4.3.  L3-Only Designs . . . .\
    \ . . . . . . . . . . . . . . . . .  12\n   5.  Routing Protocol Design . . .\
    \ . . . . . . . . . . . . . . . .  13\n     5.1.  Choosing EBGP as the Routing\
    \ Protocol . . . . . . . . . .  13\n     5.2.  EBGP Configuration for Clos Topology\
    \  . . . . . . . . . .  15\n       5.2.1.  EBGP Configuration Guidelines and Example\
    \ ASN Scheme   15\n       5.2.2.  Private Use ASNs  . . . . . . . . . . . . .\
    \ . . . . .  16\n       5.2.3.  Prefix Advertisement  . . . . . . . . . . . .\
    \ . . . .  17\n       5.2.4.  External Connectivity . . . . . . . . . . . . .\
    \ . . .  18\n       5.2.5.  Route Summarization at the Edge . . . . . . . . .\
    \ . .  19\n   6.  ECMP Considerations . . . . . . . . . . . . . . . . . . . .\
    \ .  20\n     6.1.  Basic ECMP  . . . . . . . . . . . . . . . . . . . . . . .\
    \  20\n     6.2.  BGP ECMP over Multiple ASNs . . . . . . . . . . . . . . .  21\n\
    \     6.3.  Weighted ECMP . . . . . . . . . . . . . . . . . . . . . .  21\n  \
    \   6.4.  Consistent Hashing  . . . . . . . . . . . . . . . . . . .  22\n   7.\
    \  Routing Convergence Properties  . . . . . . . . . . . . . . .  22\n     7.1.\
    \  Fault Detection Timing  . . . . . . . . . . . . . . . . .  22\n     7.2.  Event\
    \ Propagation Timing  . . . . . . . . . . . . . . . .  23\n     7.3.  Impact of\
    \ Clos Topology Fan-Outs  . . . . . . . . . . . .  24\n     7.4.  Failure Impact\
    \ Scope  . . . . . . . . . . . . . . . . . .  24\n     7.5.  Routing Micro-Loops\
    \ . . . . . . . . . . . . . . . . . . .  26\n   8.  Additional Options for Design\
    \ . . . . . . . . . . . . . . . .  26\n     8.1.  Third-Party Route Injection\
    \ . . . . . . . . . . . . . . .  26\n     8.2.  Route Summarization within Clos\
    \ Topology  . . . . . . . .  27\n       8.2.1.  Collapsing Tier 1 Devices Layer\
    \ . . . . . . . . . . .  27\n       8.2.2.  Simple Virtual Aggregation  . . .\
    \ . . . . . . . . . .  29\n     8.3.  ICMP Unreachable Message Masquerading .\
    \ . . . . . . . . .  29\n   9.  Security Considerations . . . . . . . . . . .\
    \ . . . . . . . .  30\n   10. References  . . . . . . . . . . . . . . . . . .\
    \ . . . . . . .  30\n     10.1.  Normative References . . . . . . . . . . . .\
    \ . . . . . .  30\n     10.2.  Informative References . . . . . . . . . . . .\
    \ . . . . .  31\n   Acknowledgements  . . . . . . . . . . . . . . . . . . . .\
    \ . . . .  35\n   Authors' Addresses  . . . . . . . . . . . . . . . . . . . .\
    \ . . .  35\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document describes a practical routing design that\
    \ can be used\n   in a large-scale data center (DC) design.  Such data centers,\
    \ also\n   known as \"hyper-scale\" or \"warehouse-scale\" data centers, have\
    \ a\n   unique attribute of supporting over a hundred thousand servers.  In\n\
    \   order to accommodate networks of this scale, operators are revisiting\n  \
    \ networking designs and platforms to address this need.\n   The design presented\
    \ in this document is based on operational\n   experience with data centers built\
    \ to support large-scale distributed\n   software infrastructure, such as a web\
    \ search engine.  The primary\n   requirements in such an environment are operational\
    \ simplicity and\n   network stability so that a small group of people can effectively\n\
    \   support a significantly sized network.\n   Experimentation and extensive testing\
    \ have shown that External BGP\n   (EBGP) [RFC4271] is well suited as a stand-alone\
    \ routing protocol for\n   these types of data center applications.  This is in\
    \ contrast with\n   more traditional DC designs, which may use simple tree topologies\
    \ and\n   rely on extending Layer 2 (L2) domains across multiple network\n   devices.\
    \  This document elaborates on the requirements that led to\n   this design choice\
    \ and presents details of the EBGP routing design as\n   well as exploring ideas\
    \ for further enhancements.\n   This document first presents an overview of network\
    \ design\n   requirements and considerations for large-scale data centers.  Then,\n\
    \   traditional hierarchical data center network topologies are\n   contrasted\
    \ with Clos networks [CLOS1953] that are horizontally scaled\n   out.  This is\
    \ followed by arguments for selecting EBGP with a Clos\n   topology as the most\
    \ appropriate routing protocol to meet the\n   requirements and the proposed design\
    \ is described in detail.\n   Finally, this document reviews some additional considerations\
    \ and\n   design options.  A thorough understanding of BGP is assumed by a\n \
    \  reader planning on deploying the design described within the\n   document.\n"
- title: 2.  Network Design Requirements
  contents:
  - "2.  Network Design Requirements\n   This section describes and summarizes network\
    \ design requirements for\n   large-scale data centers.\n"
- title: 2.1.  Bandwidth and Traffic Patterns
  contents:
  - "2.1.  Bandwidth and Traffic Patterns\n   The primary requirement when building\
    \ an interconnection network for\n   a large number of servers is to accommodate\
    \ application bandwidth and\n   latency requirements.  Until recently it was quite\
    \ common to see the\n   majority of traffic entering and leaving the data center,\
    \ commonly\n   referred to as \"north-south\" traffic.  Traditional \"tree\" topologies\n\
    \   were sufficient to accommodate such flows, even with high\n   oversubscription\
    \ ratios between the layers of the network.  If more\n   bandwidth was required,\
    \ it was added by \"scaling up\" the network\n   elements, e.g., by upgrading\
    \ the device's linecards or fabrics or\n   replacing the device with one with\
    \ higher port density.\n   Today many large-scale data centers host applications\
    \ generating\n   significant amounts of server-to-server traffic, which does not\n\
    \   egress the DC, commonly referred to as \"east-west\" traffic.  Examples\n\
    \   of such applications could be computer clusters such as Hadoop\n   [HADOOP],\
    \ massive data replication between clusters needed by certain\n   applications,\
    \ or virtual machine migrations.  Scaling traditional\n   tree topologies to match\
    \ these bandwidth demands becomes either too\n   expensive or impossible due to\
    \ physical limitations, e.g., port\n   density in a switch.\n"
- title: 2.2.  CAPEX Minimization
  contents:
  - "2.2.  CAPEX Minimization\n   The Capital Expenditures (CAPEX) associated with\
    \ the network\n   infrastructure alone constitutes about 10-15% of total data\
    \ center\n   expenditure (see [GREENBERG2009]).  However, the absolute cost is\n\
    \   significant, and hence there is a need to constantly drive down the\n   cost\
    \ of individual network elements.  This can be accomplished in two\n   ways:\n\
    \   o  Unifying all network elements, preferably using the same hardware\n   \
    \   type or even the same device.  This allows for volume pricing on\n      bulk\
    \ purchases and reduced maintenance and inventory costs.\n   o  Driving costs\
    \ down using competitive pressures, by introducing\n      multiple network equipment\
    \ vendors.\n   In order to allow for good vendor diversity, it is important to\n\
    \   minimize the software feature requirements for the network elements.\n   This\
    \ strategy provides maximum flexibility of vendor equipment\n   choices while\
    \ enforcing interoperability using open standards.\n"
- title: 2.3.  OPEX Minimization
  contents:
  - "2.3.  OPEX Minimization\n   Operating large-scale infrastructure can be expensive\
    \ as a larger\n   amount of elements will statistically fail more often.  Having\
    \ a\n   simpler design and operating using a limited software feature set\n  \
    \ minimizes software issue-related failures.\n   An important aspect of Operational\
    \ Expenditure (OPEX) minimization is\n   reducing the size of failure domains\
    \ in the network.  Ethernet\n   networks are known to be susceptible to broadcast\
    \ or unicast traffic\n   storms that can have a dramatic impact on network performance\
    \ and\n   availability.  The use of a fully routed design significantly reduces\n\
    \   the size of the data-plane failure domains, i.e., limits them to the\n   lowest\
    \ level in the network hierarchy.  However, such designs\n   introduce the problem\
    \ of distributed control-plane failures.  This\n   observation calls for simpler\
    \ and less control-plane protocols to\n   reduce protocol interaction issues,\
    \ reducing the chance of a network\n   meltdown.  Minimizing software feature\
    \ requirements as described in\n   the CAPEX section above also reduces testing\
    \ and training\n   requirements.\n"
- title: 2.4.  Traffic Engineering
  contents:
  - "2.4.  Traffic Engineering\n   In any data center, application load balancing\
    \ is a critical function\n   performed by network devices.  Traditionally, load\
    \ balancers are\n   deployed as dedicated devices in the traffic forwarding path.\
    \  The\n   problem arises in scaling load balancers under growing traffic\n  \
    \ demand.  A preferable solution would be able to scale the load-\n   balancing\
    \ layer horizontally, by adding more of the uniform nodes and\n   distributing\
    \ incoming traffic across these nodes.  In situations like\n   this, an ideal\
    \ choice would be to use network infrastructure itself\n   to distribute traffic\
    \ across a group of load balancers.  The\n   combination of anycast prefix advertisement\
    \ [RFC4786] and Equal Cost\n   Multipath (ECMP) functionality can be used to accomplish\
    \ this goal.\n   To allow for more granular load distribution, it is beneficial\
    \ for\n   the network to support the ability to perform controlled per-hop\n \
    \  traffic engineering.  For example, it is beneficial to directly\n   control\
    \ the ECMP next-hop set for anycast prefixes at every level of\n   the network\
    \ hierarchy.\n"
- title: 2.5.  Summarized Requirements
  contents:
  - "2.5.  Summarized Requirements\n   This section summarizes the list of requirements\
    \ outlined in the\n   previous sections:\n   o  REQ1: Select a topology that can\
    \ be scaled \"horizontally\" by\n      adding more links and network devices of\
    \ the same type without\n      requiring upgrades to the network elements themselves.\n\
    \   o  REQ2: Define a narrow set of software features/protocols supported\n  \
    \    by a multitude of networking equipment vendors.\n   o  REQ3: Choose a routing\
    \ protocol that has a simple implementation\n      in terms of programming code\
    \ complexity and ease of operational\n      support.\n   o  REQ4: Minimize the\
    \ failure domain of equipment or protocol issues\n      as much as possible.\n\
    \   o  REQ5: Allow for some traffic engineering, preferably via explicit\n   \
    \   control of the routing prefix next hop using built-in protocol\n      mechanics.\n"
- title: 3.  Data Center Topologies Overview
  contents:
  - "3.  Data Center Topologies Overview\n   This section provides an overview of\
    \ two general types of data center\n   designs -- hierarchical (also known as\
    \ \"tree-based\") and Clos-based\n   network designs.\n"
- title: 3.1.  Traditional DC Topology
  contents:
  - "3.1.  Traditional DC Topology\n   In the networking industry, a common design\
    \ choice for data centers\n   typically looks like an (upside down) tree with\
    \ redundant uplinks and\n   three layers of hierarchy namely; core, aggregation/distribution,\
    \ and\n   access layers (see Figure 1).  To accommodate bandwidth demands, each\n\
    \   higher layer, from the server towards DC egress or WAN, has higher\n   port\
    \ density and bandwidth capacity where the core functions as the\n   \"trunk\"\
    \ of the tree-based design.  To keep terminology uniform and\n   for comparison\
    \ with other designs, in this document these layers will\n   be referred to as\
    \ Tier 1, Tier 2 and Tier 3 \"tiers\", instead of core,\n   aggregation, or access\
    \ layers.\n             +------+  +------+\n             |      |  |      |\n\
    \             |      |--|      |           Tier 1\n             |      |  |  \
    \    |\n             +------+  +------+\n               |  |      |  |\n     +---------+\
    \  |      |  +----------+\n     | +-------+--+------+--+-------+  |\n     | |\
    \       |  |      |  |       |  |\n   +----+     +----+    +----+     +----+\n\
    \   |    |     |    |    |    |     |    |\n   |    |-----|    |    |    |-----|\
    \    | Tier 2\n   |    |     |    |    |    |     |    |\n   +----+     +----+\
    \    +----+     +----+\n      |         |          |         |\n      |      \
    \   |          |         |\n      | +-----+ |          | +-----+ |\n      +-|\
    \     |-+          +-|     |-+    Tier 3\n        +-----+              +-----+\n\
    \         | | |                | | |\n     <- Servers ->        <- Servers ->\n\
    \                   Figure 1: Typical DC Network Topology\n   Unfortunately, as\
    \ noted previously, it is not possible to scale a\n   tree-based design to a large\
    \ enough degree for handling large-scale\n   designs due to the inability to be\
    \ able to acquire Tier 1 devices\n   with a large enough port density to sufficiently\
    \ scale Tier 2.  Also,\n   continuous upgrades or replacement of the upper-tier\
    \ devices are\n   required as deployment size or bandwidth requirements increase,\
    \ which\n   is operationally complex.  For this reason, REQ1 is in place,\n  \
    \ eliminating this type of design from consideration.\n"
- title: 3.2.  Clos Network Topology
  contents:
  - "3.2.  Clos Network Topology\n   This section describes a common design for horizontally\
    \ scalable\n   topology in large-scale data centers in order to meet REQ1.\n"
- title: 3.2.1.  Overview
  contents:
  - "3.2.1.  Overview\n   A common choice for a horizontally scalable topology is\
    \ a folded Clos\n   topology, sometimes called \"fat-tree\" (for example, [INTERCON]\
    \ and\n   [ALFARES2008]).  This topology features an odd number of stages\n  \
    \ (sometimes known as \"dimensions\") and is commonly made of uniform\n   elements,\
    \ e.g., network switches with the same port count.\n   Therefore, the choice of\
    \ folded Clos topology satisfies REQ1 and\n   facilitates REQ2.  See Figure 2\
    \ below for an example of a folded\n   3-stage Clos topology (3 stages counting\
    \ Tier 2 stage twice, when\n   tracing a packet flow):\n   +-------+\n   |   \
    \    |----------------------------+\n   |       |------------------+         |\n\
    \   |       |--------+         |         |\n   +-------+        |         |  \
    \       |\n   +-------+        |         |         |\n   |       |--------+---------+-------+\
    \ |\n   |       |--------+-------+ |       | |\n   |       |------+ |       |\
    \ |       | |\n   +-------+      | |       | |       | |\n   +-------+      |\
    \ |       | |       | |\n   |       |------+-+-------+-+-----+ | |\n   |     \
    \  |------+-+-----+ | |     | | |\n   |       |----+ | |     | | |     | | |\n\
    \   +-------+    | | |     | | |   ---------> M links\n    Tier 1      | | | \
    \    | | |     | | |\n              +-------+ +-------+ +-------+\n          \
    \    |       | |       | |       |\n              |       | |       | |      \
    \ | Tier 2\n              |       | |       | |       |\n              +-------+\
    \ +-------+ +-------+\n                | | |     | | |     | | |\n           \
    \     | | |     | | |   ---------> N Links\n                | | |     | | |  \
    \   | | |\n                O O O     O O O     O O O   Servers\n             \
    \     Figure 2: 3-Stage Folded Clos Topology\n   This topology is often also referred\
    \ to as a \"Leaf and Spine\"\n   network, where \"Spine\" is the name given to\
    \ the middle stage of the\n   Clos topology (Tier 1) and \"Leaf\" is the name\
    \ of input/output stage\n   (Tier 2).  For uniformity, this document will refer\
    \ to these layers\n   using the \"Tier n\" notation.\n"
- title: 3.2.2.  Clos Topology Properties
  contents:
  - "3.2.2.  Clos Topology Properties\n   The following are some key properties of\
    \ the Clos topology:\n   o  The topology is fully non-blocking, or more accurately\
    \ non-\n      interfering, if M >= N and oversubscribed by a factor of N/M\n \
    \     otherwise.  Here M and N is the uplink and downlink port count\n      respectively,\
    \ for a Tier 2 switch as shown in Figure 2.\n   o  Utilizing this topology requires\
    \ control and data-plane support\n      for ECMP with a fan-out of M or more.\n\
    \   o  Tier 1 switches have exactly one path to every server in this\n      topology.\
    \  This is an important property that makes route\n      summarization dangerous\
    \ in this topology (see Section 8.2 below).\n   o  Traffic flowing from server\
    \ to server is load balanced over all\n      available paths using ECMP.\n"
- title: 3.2.3.  Scaling the Clos Topology
  contents:
  - "3.2.3.  Scaling the Clos Topology\n   A Clos topology can be scaled either by\
    \ increasing network element\n   port density or by adding more stages, e.g.,\
    \ moving to a 5-stage\n   Clos, as illustrated in Figure 3 below:\n          \
    \                            Tier 1\n                                     +-----+\n\
    \          Cluster                    |     |\n +----------------------------+\
    \   +--|     |--+\n |                            |   |  +-----+  |\n |       \
    \             Tier 2  |   |           |   Tier 2\n |                   +-----+\
    \  |   |  +-----+  |  +-----+\n |     +-------------| DEV |------+--|     |--+--|\
    \     |-------------+\n |     |       +-----|  C  |------+  |     |  +--|    \
    \ |-----+       |\n |     |       |     +-----+  |      +-----+     +-----+  \
    \   |       |\n |     |       |              |                              |\
    \       |\n |     |       |     +-----+  |      +-----+     +-----+     |    \
    \   |\n |     | +-----------| DEV |------+  |     |  +--|     |-----------+ |\n\
    \ |     | |     | +---|  D  |------+--|     |--+--|     |---+ |     | |\n |  \
    \   | |     | |   +-----+  |   |  +-----+  |  +-----+   | |     | |\n |     |\
    \ |     | |            |   |           |            | |     | |\n |   +-----+\
    \ +-----+          |   |  +-----+  |          +-----+ +-----+\n |   | DEV | |\
    \ DEV |          |   +--|     |--+          |     | |     |\n |   |  A  | |  B\
    \  | Tier 3   |      |     |      Tier 3 |     | |     |\n |   +-----+ +-----+\
    \          |      +-----+             +-----+ +-----+\n |     | |     | |    \
    \        |                            | |     | |\n |     O O     O O        \
    \    |                            O O     O O\n |       Servers              |\
    \                              Servers\n +----------------------------+\n    \
    \                  Figure 3: 5-Stage Clos Topology\n   The small example of topology\
    \ in Figure 3 is built from devices with\n   a port count of 4.  In this document,\
    \ one set of directly connected\n   Tier 2 and Tier 3 devices along with their\
    \ attached servers will be\n   referred to as a \"cluster\".  For example, DEV\
    \ A, B, C, D, and the\n   servers that connect to DEV A and B, on Figure 3 form\
    \ a cluster.  The\n   concept of a cluster may also be a useful concept as a single\n\
    \   deployment or maintenance unit that can be operated on at a different\n  \
    \ frequency than the entire topology.\n   In practice, Tier 3 of the network,\
    \ which is typically Top-of-Rack\n   switches (ToRs), is where oversubscription\
    \ is introduced to allow for\n   packaging of more servers in the data center\
    \ while meeting the\n   bandwidth requirements for different types of applications.\
    \  The main\n   reason to limit oversubscription at a single layer of the network\
    \ is\n   to simplify application development that would otherwise need to\n  \
    \ account for multiple bandwidth pools: within rack (Tier 3), between\n   racks\
    \ (Tier 2), and between clusters (Tier 1).  Since\n   oversubscription does not\
    \ have a direct relationship to the routing\n   design, it is not discussed further\
    \ in this document.\n"
- title: 3.2.4.  Managing the Size of Clos Topology Tiers
  contents:
  - "3.2.4.  Managing the Size of Clos Topology Tiers\n   If a data center network\
    \ size is small, it is possible to reduce the\n   number of switches in Tier 1\
    \ or Tier 2 of a Clos topology by a factor\n   of two.  To understand how this\
    \ could be done, take Tier 1 as an\n   example.  Every Tier 2 device connects\
    \ to a single group of Tier 1\n   devices.  If half of the ports on each of the\
    \ Tier 1 devices are not\n   being used, then it is possible to reduce the number\
    \ of Tier 1\n   devices by half and simply map two uplinks from a Tier 2 device\
    \ to\n   the same Tier 1 device that were previously mapped to different Tier\n\
    \   1 devices.  This technique maintains the same bandwidth while\n   reducing\
    \ the number of elements in Tier 1, thus saving on CAPEX.  The\n   tradeoff, in\
    \ this example, is the reduction of maximum DC size in\n   terms of overall server\
    \ count by half.\n   In this example, Tier 2 devices will be using two parallel\
    \ links to\n   connect to each Tier 1 device.  If one of these links fails, the\n\
    \   other will pick up all traffic of the failed link, possibly resulting\n  \
    \ in heavy congestion and quality of service degradation if the path\n   determination\
    \ procedure does not take bandwidth amount into account,\n   since the number\
    \ of upstream Tier 1 devices is likely wider than two.\n   To avoid this situation,\
    \ parallel links can be grouped in link\n   aggregation groups (LAGs), e.g., [IEEE8023AD],\
    \ with widely available\n   implementation settings that take the whole \"bundle\"\
    \ down upon a\n   single link failure.  Equivalent techniques that enforce \"\
    fate\n   sharing\" on the parallel links can be used in place of LAGs to\n   achieve\
    \ the same effect.  As a result of such fate-sharing, traffic\n   from two or\
    \ more failed links will be rebalanced over the multitude\n   of remaining paths\
    \ that equals the number of Tier 1 devices.  This\n   example is using two links\
    \ for simplicity, having more links in a\n   bundle will have less impact on capacity\
    \ upon a member-link failure.\n"
- title: 4.  Data Center Routing Overview
  contents:
  - "4.  Data Center Routing Overview\n   This section provides an overview of three\
    \ general types of data\n   center protocol designs -- Layer 2 only, Hybrid Layer\
    \ L2/L3, and\n   Layer 3 only.\n"
- title: 4.1.  L2-Only Designs
  contents:
  - "4.1.  L2-Only Designs\n   Originally, most data center designs used Spanning\
    \ Tree Protocol\n   (STP) originally defined in [IEEE8021D-1990] for loop-free\
    \ topology\n   creation, typically utilizing variants of the traditional DC topology\n\
    \   described in Section 3.1.  At the time, many DC switches either did\n   not\
    \ support Layer 3 routing protocols or supported them with\n   additional licensing\
    \ fees, which played a part in the design choice.\n   Although many enhancements\
    \ have been made through the introduction of\n   Rapid Spanning Tree Protocol\
    \ (RSTP) in the latest revision of\n   [IEEE8021D-2004] and Multiple Spanning\
    \ Tree Protocol (MST) specified\n   in [IEEE8021Q] that increase convergence,\
    \ stability, and load-\n   balancing in larger topologies, many of the fundamentals\
    \ of the\n   protocol limit its applicability in large-scale DCs.  STP and its\n\
    \   newer variants use an active/standby approach to path selection, and\n   are\
    \ therefore hard to deploy in horizontally scaled topologies as\n   described\
    \ in Section 3.2.  Further, operators have had many\n   experiences with large\
    \ failures due to issues caused by improper\n   cabling, misconfiguration, or\
    \ flawed software on a single device.\n   These failures regularly affected the\
    \ entire spanning-tree domain and\n   were very hard to troubleshoot due to the\
    \ nature of the protocol.\n   For these reasons, and since almost all DC traffic\
    \ is now IP,\n   therefore requiring a Layer 3 routing protocol at the network\
    \ edge\n   for external connectivity, designs utilizing STP usually fail all of\n\
    \   the requirements of large-scale DC operators.  Various enhancements\n   to\
    \ link-aggregation protocols such as [IEEE8023AD], generally known\n   as Multi-Chassis\
    \ Link-Aggregation (M-LAG) made it possible to use\n   Layer 2 designs with active-active\
    \ network paths while relying on STP\n   as the backup for loop prevention.  The\
    \ major downsides of this\n   approach are the lack of ability to scale linearly\
    \ past two in most\n   implementations, lack of standards-based implementations,\
    \ and the\n   added failure domain risk of syncing state between the devices.\n\
    \   It should be noted that building large, horizontally scalable,\n   L2-only\
    \ networks without STP is possible recently through the\n   introduction of the\
    \ Transparent Interconnection of Lots of Links\n   (TRILL) protocol in [RFC6325].\
    \  TRILL resolves many of the issues STP\n   has for large-scale DC design however,\
    \ due to the limited number of\n   implementations, and often the requirement\
    \ for specific equipment\n   that supports it, this has limited its applicability\
    \ and increased\n   the cost of such designs.\n   Finally, neither the base TRILL\
    \ specification nor the M-LAG approach\n   totally eliminate the problem of the\
    \ shared broadcast domain that is\n   so detrimental to the operations of any\
    \ Layer 2, Ethernet-based\n   solution.  Later TRILL extensions have been proposed\
    \ to solve the\n   this problem statement, primarily based on the approaches outlined\
    \ in\n   [RFC7067], but this even further limits the number of available\n   interoperable\
    \ implementations that can be used to build a fabric.\n   Therefore, TRILL-based\
    \ designs have issues meeting REQ2, REQ3, and\n   REQ4.\n"
- title: 4.2.  Hybrid L2/L3 Designs
  contents:
  - "4.2.  Hybrid L2/L3 Designs\n   Operators have sought to limit the impact of data-plane\
    \ faults and\n   build large-scale topologies through implementing routing protocols\n\
    \   in either the Tier 1 or Tier 2 parts of the network and dividing the\n   Layer\
    \ 2 domain into numerous, smaller domains.  This design has\n   allowed data centers\
    \ to scale up, but at the cost of complexity in\n   managing multiple network\
    \ protocols.  For the following reasons,\n   operators have retained Layer 2 in\
    \ either the access (Tier 3) or both\n   access and aggregation (Tier 3 and Tier\
    \ 2) parts of the network:\n   o  Supporting legacy applications that may require\
    \ direct Layer 2\n      adjacency or use non-IP protocols.\n   o  Seamless mobility\
    \ for virtual machines that require the\n      preservation of IP addresses when\
    \ a virtual machine moves to a\n      different Tier 3 switch.\n   o  Simplified\
    \ IP addressing = less IP subnets are required for the\n      data center.\n \
    \  o  Application load balancing may require direct Layer 2 reachability\n   \
    \   to perform certain functions such as Layer 2 Direct Server Return\n      (DSR).\
    \  See [L3DSR].\n   o  Continued CAPEX differences between L2- and L3-capable\
    \ switches.\n"
- title: 4.3.  L3-Only Designs
  contents:
  - "4.3.  L3-Only Designs\n   Network designs that leverage IP routing down to Tier\
    \ 3 of the\n   network have gained popularity as well.  The main benefit of these\n\
    \   designs is improved network stability and scalability, as a result of\n  \
    \ confining L2 broadcast domains.  Commonly, an Interior Gateway\n   Protocol\
    \ (IGP) such as Open Shortest Path First (OSPF) [RFC2328] is\n   used as the primary\
    \ routing protocol in such a design.  As data\n   centers grow in scale, and server\
    \ count exceeds tens of thousands,\n   such fully routed designs have become more\
    \ attractive.\n   Choosing a L3-only design greatly simplifies the network,\n\
    \   facilitating the meeting of REQ1 and REQ2, and has widespread\n   adoption\
    \ in networks where large Layer 2 adjacency and larger size\n   Layer 3 subnets\
    \ are not as critical compared to network scalability\n   and stability.  Application\
    \ providers and network operators continue\n   to develop new solutions to meet\
    \ some of the requirements that\n   previously had driven large Layer 2 domains\
    \ by using various overlay\n   or tunneling techniques.\n"
- title: 5.  Routing Protocol Design
  contents:
  - "5.  Routing Protocol Design\n   In this section, the motivations for using External\
    \ BGP (EBGP) as the\n   single routing protocol for data center networks having\
    \ a Layer 3\n   protocol design and Clos topology are reviewed.  Then, a practical\n\
    \   approach for designing an EBGP-based network is provided.\n"
- title: 5.1.  Choosing EBGP as the Routing Protocol
  contents:
  - "5.1.  Choosing EBGP as the Routing Protocol\n   REQ2 would give preference to\
    \ the selection of a single routing\n   protocol to reduce complexity and interdependencies.\
    \  While it is\n   common to rely on an IGP in this situation, sometimes with\
    \ either the\n   addition of EBGP at the device bordering the WAN or Internal\
    \ BGP\n   (IBGP) throughout, this document proposes the use of an EBGP-only\n\
    \   design.\n   Although EBGP is the protocol used for almost all Inter-Domain\n\
    \   Routing in the Internet and has wide support from both vendor and\n   service\
    \ provider communities, it is not generally deployed as the\n   primary routing\
    \ protocol within the data center for a number of\n   reasons (some of which are\
    \ interrelated):\n   o  BGP is perceived as a \"WAN-only, protocol-only\" and\
    \ not often\n      considered for enterprise or data center applications.\n  \
    \ o  BGP is believed to have a \"much slower\" routing convergence\n      compared\
    \ to IGPs.\n   o  Large-scale BGP deployments typically utilize an IGP for BGP\
    \ next-\n      hop resolution as all nodes in the IBGP topology are not directly\n\
    \      connected.\n   o  BGP is perceived to require significant configuration\
    \ overhead and\n      does not support neighbor auto-discovery.\n   This document\
    \ discusses some of these perceptions, especially as\n   applicable to the proposed\
    \ design, and highlights some of the\n   advantages of using the protocol such\
    \ as:\n   o  BGP has less complexity in parts of its protocol design --\n    \
    \  internal data structures and state machine are simpler as compared\n      to\
    \ most link-state IGPs such as OSPF.  For example, instead of\n      implementing\
    \ adjacency formation, adjacency maintenance and/or\n      flow-control, BGP simply\
    \ relies on TCP as the underlying\n      transport.  This fulfills REQ2 and REQ3.\n\
    \   o  BGP information flooding overhead is less when compared to link-\n    \
    \  state IGPs.  Since every BGP router calculates and propagates only\n      the\
    \ best-path selected, a network failure is masked as soon as the\n      BGP speaker\
    \ finds an alternate path, which exists when highly\n      symmetric topologies,\
    \ such as Clos, are coupled with an EBGP-only\n      design.  In contrast, the\
    \ event propagation scope of a link-state\n      IGP is an entire area, regardless\
    \ of the failure type.  In this\n      way, BGP better meets REQ3 and REQ4.  It\
    \ is also worth mentioning\n      that all widely deployed link-state IGPs feature\
    \ periodic\n      refreshes of routing information while BGP does not expire routing\n\
    \      state, although this rarely impacts modern router control planes.\n   o\
    \  BGP supports third-party (recursively resolved) next hops.  This\n      allows\
    \ for manipulating multipath to be non-ECMP-based or\n      forwarding-based on\
    \ application-defined paths, through\n      establishment of a peering session\
    \ with an application\n      \"controller\" that can inject routing information\
    \ into the system,\n      satisfying REQ5.  OSPF provides similar functionality\
    \ using\n      concepts such as \"Forwarding Address\", but with more difficulty\
    \ in\n      implementation and far less control of information propagation\n \
    \     scope.\n   o  Using a well-defined Autonomous System Number (ASN) allocation\n\
    \      scheme and standard AS_PATH loop detection, \"BGP path hunting\"\n    \
    \  (see [JAKMA2008]) can be controlled and complex unwanted paths\n      will\
    \ be ignored.  See Section 5.2 for an example of a working ASN\n      allocation\
    \ scheme.  In a link-state IGP, accomplishing the same\n      goal would require\
    \ multi-(instance/topology/process) support,\n      typically not available in\
    \ all DC devices and quite complex to\n      configure and troubleshoot.  Using\
    \ a traditional single flooding\n      domain, which most DC designs utilize,\
    \ under certain failure\n      conditions may pick up unwanted lengthy paths,\
    \ e.g., traversing\n      multiple Tier 2 devices.\n   o  EBGP configuration that\
    \ is implemented with minimal routing policy\n      is easier to troubleshoot\
    \ for network reachability issues.  In\n      most implementations, it is straightforward\
    \ to view contents of\n      the BGP Loc-RIB and compare it to the router's Routing\
    \ Information\n      Base (RIB).  Also, in most implementations, an operator can\
    \ view\n      every BGP neighbors Adj-RIB-In and Adj-RIB-Out structures, and\n\
    \      therefore incoming and outgoing Network Layer Reachability\n      Information\
    \ (NLRI) information can be easily correlated on both\n      sides of a BGP session.\
    \  Thus, BGP satisfies REQ3.\n"
- title: 5.2.  EBGP Configuration for Clos Topology
  contents:
  - "5.2.  EBGP Configuration for Clos Topology\n   Clos topologies that have more\
    \ than 5 stages are very uncommon due to\n   the large numbers of interconnects\
    \ required by such a design.\n   Therefore, the examples below are made with reference\
    \ to the 5-stage\n   Clos topology (in unfolded state).\n"
- title: 5.2.1.  EBGP Configuration Guidelines and Example ASN Scheme
  contents:
  - "5.2.1.  EBGP Configuration Guidelines and Example ASN Scheme\n   The diagram\
    \ below illustrates an example of an ASN allocation scheme.\n   The following\
    \ is a list of guidelines that can be used:\n   o  EBGP single-hop sessions are\
    \ established over direct point-to-\n      point links interconnecting the network\
    \ nodes, no multi-hop or\n      loopback sessions are used, even in the case of\
    \ multiple links\n      between the same pair of nodes.\n   o  Private Use ASNs\
    \ from the range 64512-65534 are used to avoid ASN\n      conflicts.\n   o  A\
    \ single ASN is allocated to all of the Clos topology's Tier 1\n      devices.\n\
    \   o  A unique ASN is allocated to each set of Tier 2 devices in the\n      same\
    \ cluster.\n   o  A unique ASN is allocated to every Tier 3 device (e.g., ToR)\
    \ in\n      this topology.\n                                ASN 65534\n      \
    \                         +---------+\n                               | +-----+\
    \ |\n                               | |     | |\n                            \
    \ +-|-|     |-|-+\n                             | | +-----+ | |\n            \
    \      ASN 646XX  | |         | |  ASN 646XX\n                 +---------+ | |\
    \         | | +---------+\n                 | +-----+ | | | +-----+ | | | +-----+\
    \ |\n     +-----------|-|     |-|-+-|-|     |-|-+-|-|     |-|-----------+\n  \
    \   |       +---|-|     |-|-+ | |     | | +-|-|     |-|---+       |\n     |  \
    \     |   | +-----+ |   | +-----+ |   | +-----+ |   |       |\n     |       |\
    \   |         |   |         |   |         |   |       |\n     |       |   |  \
    \       |   |         |   |         |   |       |\n     |       |   | +-----+\
    \ |   | +-----+ |   | +-----+ |   |       |\n     | +-----+---|-|     |-|-+ |\
    \ |     | | +-|-|     |-|---+-----+ |\n     | |     | +-|-|     |-|-+-|-|    \
    \ |-|-+-|-|     |-|-+ |     | |\n     | |     | | | +-----+ | | | +-----+ | |\
    \ | +-----+ | | |     | |\n     | |     | | +---------+ | |         | | +---------+\
    \ | |     | |\n     | |     | |             | |         | |             | |  \
    \   | |\n   +-----+ +-----+           | | +-----+ | |           +-----+ +-----+\n\
    \   | ASN | |     |           +-|-|     |-|-+           |     | |     |\n   |65YYY|\
    \ | ... |             | |     | |             | ... | | ... |\n   +-----+ +-----+\
    \             | +-----+ |             +-----+ +-----+\n     | |     | |      \
    \         +---------+               | |     | |\n     O O     O O            \
    \  <- Servers ->              O O     O O\n                 Figure 4: BGP ASN\
    \ Layout for 5-Stage Clos\n"
- title: 5.2.2.  Private Use ASNs
  contents:
  - "5.2.2.  Private Use ASNs\n   The original range of Private Use ASNs [RFC6996]\
    \ limited operators to\n   1023 unique ASNs.  Since it is quite likely that the\
    \ number of\n   network devices may exceed this number, a workaround is required.\n\
    \   One approach is to re-use the ASNs assigned to the Tier 3 devices\n   across\
    \ different clusters.  For example, Private Use ASNs 65001,\n   65002 ... 65032\
    \ could be used within every individual cluster and\n   assigned to Tier 3 devices.\n\
    \   To avoid route suppression due to the AS_PATH loop detection\n   mechanism\
    \ in BGP, upstream EBGP sessions on Tier 3 devices must be\n   configured with\
    \ the \"Allowas-in\" feature [ALLOWASIN] that allows\n   accepting a device's\
    \ own ASN in received route advertisements.\n   Although this feature is not standardized,\
    \ it is widely available\n   across multiple vendors implementations.  Introducing\
    \ this feature\n   does not make routing loops more likely in the design since\
    \ the\n   AS_PATH is being added to by routers at each of the topology tiers\n\
    \   and AS_PATH length is an early tie breaker in the BGP path selection\n   process.\
    \  Further loop protection is still in place at the Tier 1\n   device, which will\
    \ not accept routes with a path including its own\n   ASN.  Tier 2 devices do\
    \ not have direct connectivity with each other.\n   Another solution to this problem\
    \ would be to use Four-Octet ASNs\n   ([RFC6793]), where there are additional\
    \ Private Use ASNs available,\n   see [IANA.AS].  Use of Four-Octet ASNs puts\
    \ additional protocol\n   complexity in the BGP implementation and should be balanced\
    \ against\n   the complexity of re-use when considering REQ3 and REQ4.  Perhaps\n\
    \   more importantly, they are not yet supported by all BGP\n   implementations,\
    \ which may limit vendor selection of DC equipment.\n   When supported, ensure\
    \ that deployed implementations are able to\n   remove the Private Use ASNs when\
    \ external connectivity\n   (Section 5.2.4) to these ASNs is required.\n"
- title: 5.2.3.  Prefix Advertisement
  contents:
  - "5.2.3.  Prefix Advertisement\n   A Clos topology features a large number of point-to-point\
    \ links and\n   associated prefixes.  Advertising all of these routes into BGP\
    \ may\n   create Forwarding Information Base (FIB) overload in the network\n \
    \  devices.  Advertising these links also puts additional path\n   computation\
    \ stress on the BGP control plane for little benefit.\n   There are two possible\
    \ solutions:\n   o  Do not advertise any of the point-to-point links into BGP.\
    \  Since\n      the EBGP-based design changes the next-hop address at every\n\
    \      device, distant networks will automatically be reachable via the\n    \
    \  advertising EBGP peer and do not require reachability to these\n      prefixes.\
    \  However, this may complicate operations or monitoring:\n      e.g., using the\
    \ popular \"traceroute\" tool will display IP\n      addresses that are not reachable.\n\
    \   o  Advertise point-to-point links, but summarize them on every\n      device.\
    \  This requires an address allocation scheme such as\n      allocating a consecutive\
    \ block of IP addresses per Tier 1 and Tier\n      2 device to be used for point-to-point\
    \ interface addressing to the\n      lower layers (Tier 2 uplinks will be allocated\
    \ from Tier 1 address\n      blocks and so forth).\n   Server subnets on Tier\
    \ 3 devices must be announced into BGP without\n   using route summarization on\
    \ Tier 2 and Tier 1 devices.  Summarizing\n   subnets in a Clos topology results\
    \ in route black-holing under a\n   single link failure (e.g., between Tier 2\
    \ and Tier 3 devices), and\n   hence must be avoided.  The use of peer links within\
    \ the same tier to\n   resolve the black-holing problem by providing \"bypass\
    \ paths\" is\n   undesirable due to O(N^2) complexity of the peering-mesh and\
    \ waste of\n   ports on the devices.  An alternative to the full mesh of peer\
    \ links\n   would be to use a simpler bypass topology, e.g., a \"ring\" as\n \
    \  described in [FB4POST], but such a topology adds extra hops and has\n   limited\
    \ bandwidth.  It may require special tweaks to make BGP routing\n   work, e.g.,\
    \ splitting every device into an ASN of its own.  Later in\n   this document,\
    \ Section 8.2 introduces a less intrusive method for\n   performing a limited\
    \ form of route summarization in Clos networks and\n   discusses its associated\
    \ tradeoffs.\n"
- title: 5.2.4.  External Connectivity
  contents:
  - "5.2.4.  External Connectivity\n   A dedicated cluster (or clusters) in the Clos\
    \ topology could be used\n   for the purpose of connecting to the Wide Area Network\
    \ (WAN) edge\n   devices, or WAN Routers.  Tier 3 devices in such a cluster would\
    \ be\n   replaced with WAN routers, and EBGP peering would be used again,\n  \
    \ though WAN routers are likely to belong to a public ASN if Internet\n   connectivity\
    \ is required in the design.  The Tier 2 devices in such a\n   dedicated cluster\
    \ will be referred to as \"Border Routers\" in this\n   document.  These devices\
    \ have to perform a few special functions:\n   o  Hide network topology information\
    \ when advertising paths to WAN\n      routers, i.e., remove Private Use ASNs\
    \ [RFC6996] from the AS_PATH\n      attribute.  This is typically done to avoid\
    \ ASN number collisions\n      between different data centers and also to provide\
    \ a uniform\n      AS_PATH length to the WAN for purposes of WAN ECMP to anycast\n\
    \      prefixes originated in the topology.  An implementation-specific\n    \
    \  BGP feature typically called \"Remove Private AS\" is commonly used\n     \
    \ to accomplish this.  Depending on implementation, the feature\n      should\
    \ strip a contiguous sequence of Private Use ASNs found in an\n      AS_PATH attribute\
    \ prior to advertising the path to a neighbor.\n      This assumes that all ASNs\
    \ used for intra data center numbering\n      are from the Private Use ranges.\
    \  The process for stripping the\n      Private Use ASNs is not currently standardized,\
    \ see [REMOVAL].\n      However, most implementations at least follow the logic\
    \ described\n      in this vendor's document [VENDOR-REMOVE-PRIVATE-AS], which\
    \ is\n      enough for the design specified.\n   o  Originate a default route\
    \ to the data center devices.  This is the\n      only place where a default route\
    \ can be originated, as route\n      summarization is risky for the unmodified\
    \ Clos topology.\n      Alternatively, Border Routers may simply relay the default\
    \ route\n      learned from WAN routers.  Advertising the default route from\n\
    \      Border Routers requires that all Border Routers be fully connected\n  \
    \    to the WAN Routers upstream, to provide resistance to a single-\n      link\
    \ failure causing the black-holing of traffic.  To prevent\n      black-holing\
    \ in the situation when all of the EBGP sessions to the\n      WAN routers fail\
    \ simultaneously on a given device, it is more\n      desirable to readvertise\
    \ the default route rather than originating\n      the default route via complicated\
    \ conditional route origination\n      schemes provided by some implementations\
    \ [CONDITIONALROUTE].\n"
- title: 5.2.5.  Route Summarization at the Edge
  contents:
  - "5.2.5.  Route Summarization at the Edge\n   It is often desirable to summarize\
    \ network reachability information\n   prior to advertising it to the WAN network\
    \ due to the high amount of\n   IP prefixes originated from within the data center\
    \ in a fully routed\n   network design.  For example, a network with 2000 Tier\
    \ 3 devices will\n   have at least 2000 servers subnets advertised into BGP, along\
    \ with\n   the infrastructure prefixes.  However, as discussed in Section 5.2.3,\n\
    \   the proposed network design does not allow for route summarization\n   due\
    \ to the lack of peer links inside every tier.\n   However, it is possible to\
    \ lift this restriction for the Border\n   Routers by devising a different connectivity\
    \ model for these devices.\n   There are two options possible:\n   o  Interconnect\
    \ the Border Routers using a full-mesh of physical\n      links or using any other\
    \ \"peer-mesh\" topology, such as ring or\n      hub-and-spoke.  Configure BGP\
    \ accordingly on all Border Leafs to\n      exchange network reachability information,\
    \ e.g., by adding a mesh\n      of IBGP sessions.  The interconnecting peer links\
    \ need to be\n      appropriately sized for traffic that will be present in the\
    \ case\n      of a device or link failure in the mesh connecting the Border\n\
    \      Routers.\n   o  Tier 1 devices may have additional physical links provisioned\n\
    \      toward the Border Routers (which are Tier 2 devices from the\n      perspective\
    \ of Tier 1).  Specifically, if protection from a single\n      link or node failure\
    \ is desired, each Tier 1 device would have to\n      connect to at least two\
    \ Border Routers.  This puts additional\n      requirements on the port count\
    \ for Tier 1 devices and Border\n      Routers, potentially making it a nonuniform,\
    \ larger port count,\n      device compared with the other devices in the Clos.\
    \  This also\n      reduces the number of ports available to \"regular\" Tier\
    \ 2\n      switches, and hence the number of clusters that could be\n      interconnected\
    \ via Tier 1.\n   If any of the above options are implemented, it is possible\
    \ to\n   perform route summarization at the Border Routers toward the WAN\n  \
    \ network core without risking a routing black-hole condition under a\n   single\
    \ link failure.  Both of the options would result in nonuniform\n   topology as\
    \ additional links have to be provisioned on some network\n   devices.\n"
- title: 6.  ECMP Considerations
  contents:
  - "6.  ECMP Considerations\n   This section covers the Equal Cost Multipath (ECMP)\
    \ functionality for\n   Clos topology and discusses a few special requirements.\n"
- title: 6.1.  Basic ECMP
  contents:
  - "6.1.  Basic ECMP\n   ECMP is the fundamental load-sharing mechanism used by a\
    \ Clos\n   topology.  Effectively, every lower-tier device will use all of its\n\
    \   directly attached upper-tier devices to load-share traffic destined\n   to\
    \ the same IP prefix.  The number of ECMP paths between any two Tier\n   3 devices\
    \ in Clos topology is equal to the number of the devices in\n   the middle stage\
    \ (Tier 1).  For example, Figure 5 illustrates a\n   topology where Tier 3 device\
    \ A has four paths to reach servers X and\n   Y, via Tier 2 devices B and C and\
    \ then Tier 1 devices 1, 2, 3, and 4,\n   respectively.\n                    \
    \            Tier 1\n                               +-----+\n                \
    \               | DEV |\n                            +->|  1  |--+\n         \
    \                   |  +-----+  |\n                    Tier 2  |           | \
    \  Tier 2\n                   +-----+  |  +-----+  |  +-----+\n     +------------>|\
    \ DEV |--+->| DEV |--+--|     |-------------+\n     |       +-----|  B  |--+ \
    \ |  2  |  +--|     |-----+       |\n     |       |     +-----+     +-----+  \
    \   +-----+     |       |\n     |       |                                    \
    \     |       |\n     |       |     +-----+     +-----+     +-----+     |    \
    \   |\n     | +-----+---->| DEV |--+  | DEV |  +--|     |-----+-----+ |\n    \
    \ | |     | +---|  C  |--+->|  3  |--+--|     |---+ |     | |\n     | |     |\
    \ |   +-----+  |  +-----+  |  +-----+   | |     | |\n     | |     | |        \
    \    |           |            | |     | |\n   +-----+ +-----+          |  +-----+\
    \  |          +-----+ +-----+\n   | DEV | |     | Tier 3   +->| DEV |--+   Tier\
    \ 3 |     | |     |\n   |  A  | |     |             |  4  |             |    \
    \ | |     |\n   +-----+ +-----+             +-----+             +-----+ +-----+\n\
    \     | |     | |                                     | |     | |\n     O O  \
    \   O O            <- Servers ->            X Y     O O\n               Figure\
    \ 5: ECMP Fan-Out Tree from A to X and Y\n   The ECMP requirement implies that\
    \ the BGP implementation must support\n   multipath fan-out for up to the maximum\
    \ number of devices directly\n   attached at any point in the topology in the\
    \ upstream or downstream\n   direction.  Normally, this number does not exceed\
    \ half of the ports\n   found on a device in the topology.  For example, an ECMP\
    \ fan-out of\n   32 would be required when building a Clos network using 64-port\n\
    \   devices.  The Border Routers may need to have wider fan-out to be\n   able\
    \ to connect to a multitude of Tier 1 devices if route\n   summarization at Border\
    \ Router level is implemented as described in\n   Section 5.2.5.  If a device's\
    \ hardware does not support wider ECMP,\n   logical link-grouping (link-aggregation\
    \ at Layer 2) could be used to\n   provide \"hierarchical\" ECMP (Layer 3 ECMP\
    \ coupled with Layer 2 ECMP)\n   to compensate for fan-out limitations.  However,\
    \ this approach\n   increases the risk of flow polarization, as less entropy will\
    \ be\n   available at the second stage of ECMP.\n   Most BGP implementations declare\
    \ paths to be equal from an ECMP\n   perspective if they match up to and including\
    \ step (e) in\n   Section 9.1.2.2 of [RFC4271].  In the proposed network design\
    \ there\n   is no underlying IGP, so all IGP costs are assumed to be zero or\n\
    \   otherwise the same value across all paths and policies may be applied\n  \
    \ as necessary to equalize BGP attributes that vary in vendor defaults,\n   such\
    \ as the MULTI_EXIT_DISC (MED) attribute and origin code.  For\n   historical\
    \ reasons, it is also useful to not use 0 as the equalized\n   MED value; this\
    \ and some other useful BGP information is available in\n   [RFC4277].  Routing\
    \ loops are unlikely due to the BGP best-path\n   selection process (which prefers\
    \ shorter AS_PATH length), and longer\n   paths through the Tier 1 devices (which\
    \ don't allow their own ASN in\n   the path) are not possible.\n"
- title: 6.2.  BGP ECMP over Multiple ASNs
  contents:
  - "6.2.  BGP ECMP over Multiple ASNs\n   For application load-balancing purposes,\
    \ it is desirable to have the\n   same prefix advertised from multiple Tier 3\
    \ devices.  From the\n   perspective of other devices, such a prefix would have\
    \ BGP paths with\n   different AS_PATH attribute values, while having the same\
    \ AS_PATH\n   attribute lengths.  Therefore, BGP implementations must support\
    \ load-\n   sharing over the above-mentioned paths.  This feature is sometimes\n\
    \   known as \"multipath relax\" or \"multipath multiple-AS\" and effectively\n\
    \   allows for ECMP to be done across different neighboring ASNs if all\n   other\
    \ attributes are equal as already described in the previous\n   section.\n"
- title: 6.3.  Weighted ECMP
  contents:
  - "6.3.  Weighted ECMP\n   It may be desirable for the network devices to implement\
    \ \"weighted\"\n   ECMP, to be able to send more traffic over some paths in ECMP\
    \ fan-\n   out.  This could be helpful to compensate for failures in the network\n\
    \   and send more traffic over paths that have more capacity.  The\n   prefixes\
    \ that require weighted ECMP would have to be injected using\n   remote BGP speaker\
    \ (central agent) over a multi-hop session as\n   described further in Section\
    \ 8.1.  If support in implementations is\n   available, weight distribution for\
    \ multiple BGP paths could be\n   signaled using the technique described in [LINK].\n"
- title: 6.4.  Consistent Hashing
  contents:
  - "6.4.  Consistent Hashing\n   It is often desirable to have the hashing function\
    \ used for ECMP to\n   be consistent (see [CONS-HASH]), to minimize the impact\
    \ on flow to\n   next-hop affinity changes when a next hop is added or removed\
    \ to an\n   ECMP group.  This could be used if the network device is used as a\n\
    \   load balancer, mapping flows toward multiple destinations -- in this\n   case,\
    \ losing or adding a destination will not have a detrimental\n   effect on currently\
    \ established flows.  One particular recommendation\n   on implementing consistent\
    \ hashing is provided in [RFC2992], though\n   other implementations are possible.\
    \  This functionality could be\n   naturally combined with weighted ECMP, with\
    \ the impact of the next\n   hop changes being proportional to the weight of the\
    \ given next hop.\n   The downside of consistent hashing is increased load on\
    \ hardware\n   resource utilization, as typically more resources (e.g., Ternary\n\
    \   Content-Addressable Memory (TCAM) space) are required to implement a\n   consistent-hashing\
    \ function.\n"
- title: 7.  Routing Convergence Properties
  contents:
  - "7.  Routing Convergence Properties\n   This section reviews routing convergence\
    \ properties in the proposed\n   design.  A case is made that sub-second convergence\
    \ is achievable if\n   the implementation supports fast EBGP peering session deactivation\n\
    \   and timely RIB and FIB updates upon failure of the associated link.\n"
- title: 7.1.  Fault Detection Timing
  contents:
  - "7.1.  Fault Detection Timing\n   BGP typically relies on an IGP to route around\
    \ link/node failures\n   inside an AS, and implements either a polling-based or\
    \ an event-\n   driven mechanism to obtain updates on IGP state changes.  The\n\
    \   proposed routing design does not use an IGP, so the remaining\n   mechanisms\
    \ that could be used for fault detection are BGP keep-alive\n   time-out (or any\
    \ other type of keep-alive mechanism) and link-failure\n   triggers.\n   Relying\
    \ solely on BGP keep-alive packets may result in high\n   convergence delays,\
    \ on the order of multiple seconds (on many BGP\n   implementations the minimum\
    \ configurable BGP hold timer value is\n   three seconds).  However, many BGP\
    \ implementations can shut down\n   local EBGP peering sessions in response to\
    \ the \"link down\" event for\n   the outgoing interface used for BGP peering.\
    \  This feature is\n   sometimes called \"fast fallover\".  Since links in modern\
    \ data centers\n   are predominantly point-to-point fiber connections, a physical\n\
    \   interface failure is often detected in milliseconds and subsequently\n   triggers\
    \ a BGP reconvergence.\n   Ethernet links may support failure signaling or detection\
    \ standards\n   such as Connectivity Fault Management (CFM) as described in\n\
    \   [IEEE8021Q]; this may make failure detection more robust.\n   Alternatively,\
    \ some platforms may support Bidirectional Forwarding\n   Detection (BFD) [RFC5880]\
    \ to allow for sub-second failure detection\n   and fault signaling to the BGP\
    \ process.  However, the use of either\n   of these presents additional requirements\
    \ to vendor software and\n   possibly hardware, and may contradict REQ1.  Until\
    \ recently with\n   [RFC7130], BFD also did not allow detection of a single member\
    \ link\n   failure on a LAG, which would have limited its usefulness in some\n\
    \   designs.\n"
- title: 7.2.  Event Propagation Timing
  contents:
  - "7.2.  Event Propagation Timing\n   In the proposed design, the impact of the\
    \ BGP\n   MinRouteAdvertisementIntervalTimer (MRAI timer), as specified in\n \
    \  Section 9.2.1.1 of [RFC4271], should be considered.  Per the\n   standard,\
    \ it is required for BGP implementations to space out\n   consecutive BGP UPDATE\
    \ messages by at least MRAI seconds, which is\n   often a configurable value.\
    \  The initial BGP UPDATE messages after an\n   event carrying withdrawn routes\
    \ are commonly not affected by this\n   timer.  The MRAI timer may present significant\
    \ convergence delays\n   when a BGP speaker \"waits\" for the new path to be learned\
    \ from its\n   peers and has no local backup path information.\n   In a Clos topology,\
    \ each EBGP speaker typically has either one path\n   (Tier 2 devices don't accept\
    \ paths from other Tier 2 in the same\n   cluster due to same ASN) or N paths\
    \ for the same prefix, where N is a\n   significantly large number, e.g., N=32\
    \ (the ECMP fan-out to the next\n   tier).  Therefore, if a link fails to another\
    \ device from which a\n   path is received there is either no backup path at all\
    \ (e.g., from\n   the perspective of a Tier 2 switch losing the link to a Tier\
    \ 3\n   device), or the backup is readily available in BGP Loc-RIB (e.g.,\n  \
    \ from the perspective of a Tier 2 device losing the link to a Tier 1\n   switch).\
    \  In the former case, the BGP withdrawal announcement will\n   propagate without\
    \ delay and trigger reconvergence on affected\n   devices.  In the latter case,\
    \ the best path will be re-evaluated, and\n   the local ECMP group corresponding\
    \ to the new next-hop set will be\n   changed.  If the BGP path was the best path\
    \ selected previously, an\n   \"implicit withdraw\" will be sent via a BGP UPDATE\
    \ message as\n   described as Option b in Section 3.1 of [RFC4271] due to the\
    \ BGP\n   AS_PATH attribute changing.\n"
- title: 7.3.  Impact of Clos Topology Fan-Outs
  contents:
  - "7.3.  Impact of Clos Topology Fan-Outs\n   Clos topology has large fan-outs,\
    \ which may impact the \"Up->Down\"\n   convergence in some cases, as described\
    \ in this section.  In a\n   situation when a link between Tier 3 and Tier 2 device\
    \ fails, the\n   Tier 2 device will send BGP UPDATE messages to all upstream Tier\
    \ 1\n   devices, withdrawing the affected prefixes.  The Tier 1 devices, in\n\
    \   turn, will relay these messages to all downstream Tier 2 devices\n   (except\
    \ for the originator).  Tier 2 devices other than the one\n   originating the\
    \ UPDATE should then wait for ALL upstream Tier 1\n   devices to send an UPDATE\
    \ message before removing the affected\n   prefixes and sending corresponding\
    \ UPDATE downstream to connected\n   Tier 3 devices.  If the original Tier 2 device\
    \ or the relaying Tier 1\n   devices introduce some delay into their UPDATE message\
    \ announcements,\n   the result could be UPDATE message \"dispersion\", that could\
    \ be as\n   long as multiple seconds.  In order to avoid such a behavior, BGP\n\
    \   implementations must support \"update groups\".  The \"update group\" is\n\
    \   defined as a collection of neighbors sharing the same outbound policy\n  \
    \ -- the local speaker will send BGP updates to the members of the\n   group synchronously.\n\
    \   The impact of such \"dispersion\" grows with the size of topology fan-\n \
    \  out and could also grow under network convergence churn.  Some\n   operators\
    \ may be tempted to introduce \"route flap dampening\" type\n   features that\
    \ vendors include to reduce the control-plane impact of\n   rapidly flapping prefixes.\
    \  However, due to issues described with\n   false positives in these implementations\
    \ especially under such\n   \"dispersion\" events, it is not recommended to enable\
    \ this feature in\n   this design.  More background and issues with \"route flap\
    \ dampening\"\n   and possible implementation changes that could affect this are\
    \ well\n   described in [RFC7196].\n"
- title: 7.4.  Failure Impact Scope
  contents:
  - "7.4.  Failure Impact Scope\n   A network is declared to converge in response\
    \ to a failure once all\n   devices within the failure impact scope are notified\
    \ of the event and\n   have recalculated their RIBs and consequently updated their\
    \ FIBs.\n   Larger failure impact scope typically means slower convergence since\n\
    \   more devices have to be notified, and results in a less stable\n   network.\
    \  In this section, we describe BGP's advantages over link-\n   state routing\
    \ protocols in reducing failure impact scope for a Clos\n   topology.\n   BGP\
    \ behaves like a distance-vector protocol in the sense that only\n   the best\
    \ path from the point of view of the local router is sent to\n   neighbors.  As\
    \ such, some failures are masked if the local node can\n   immediately find a\
    \ backup path and does not have to send any updates\n   further.  Notice that\
    \ in the worst case, all devices in a data center\n   topology have to either\
    \ withdraw a prefix completely or update the\n   ECMP groups in their FIBs.  However,\
    \ many failures will not result in\n   such a wide impact.  There are two main\
    \ failure types where impact\n   scope is reduced:\n   o  Failure of a link between\
    \ Tier 2 and Tier 1 devices: In this case,\n      a Tier 2 device will update\
    \ the affected ECMP groups, removing the\n      failed link.  There is no need\
    \ to send new information to\n      downstream Tier 3 devices, unless the path\
    \ was selected as best by\n      the BGP process, in which case only an \"implicit\
    \ withdraw\" needs\n      to be sent and this should not affect forwarding.  The\
    \ affected\n      Tier 1 device will lose the only path available to reach a\n\
    \      particular cluster and will have to withdraw the associated\n      prefixes.\
    \  Such a prefix withdrawal process will only affect Tier\n      2 devices directly\
    \ connected to the affected Tier 1 device.  The\n      Tier 2 devices receiving\
    \ the BGP UPDATE messages withdrawing\n      prefixes will simply have to update\
    \ their ECMP groups.  The Tier 3\n      devices are not involved in the reconvergence\
    \ process.\n   o  Failure of a Tier 1 device: In this case, all Tier 2 devices\n\
    \      directly attached to the failed node will have to update their\n      ECMP\
    \ groups for all IP prefixes from a non-local cluster.  The\n      Tier 3 devices\
    \ are once again not involved in the reconvergence\n      process, but may receive\
    \ \"implicit withdraws\" as described above.\n   Even in the case of such failures\
    \ where multiple IP prefixes will\n   have to be reprogrammed in the FIB, it is\
    \ worth noting that all of\n   these prefixes share a single ECMP group on a Tier\
    \ 2 device.\n   Therefore, in the case of implementations with a hierarchical\
    \ FIB,\n   only a single change has to be made to the FIB.  \"Hierarchical FIB\"\
    \n   here means FIB structure where the next-hop forwarding information is\n \
    \  stored separately from the prefix lookup table, and the latter only\n   stores\
    \ pointers to the respective forwarding information.  See\n   [BGP-PIC] for discussion\
    \ of FIB hierarchies and fast convergence.\n   Even though BGP offers reduced\
    \ failure scope for some cases, further\n   reduction of the fault domain using\
    \ summarization is not always\n   possible with the proposed design, since using\
    \ this technique may\n   create routing black-holes as mentioned previously. \
    \ Therefore, the\n   worst failure impact scope on the control plane is the network\
    \ as a\n   whole -- for instance, in the case of a link failure between Tier 2\n\
    \   and Tier 3 devices.  The amount of impacted prefixes in this case\n   would\
    \ be much less than in the case of a failure in the upper layers\n   of a Clos\
    \ network topology.  The property of having such large\n   failure scope is not\
    \ a result of choosing EBGP in the design but\n   rather a result of using the\
    \ Clos topology.\n"
- title: 7.5.  Routing Micro-Loops
  contents:
  - "7.5.  Routing Micro-Loops\n   When a downstream device, e.g., Tier 2 device,\
    \ loses all paths for a\n   prefix, it normally has the default route pointing\
    \ toward the\n   upstream device -- in this case, the Tier 1 device.  As a result,\
    \ it\n   is possible to get in the situation where a Tier 2 switch loses a\n \
    \  prefix, but a Tier 1 switch still has the path pointing to the Tier 2\n   device;\
    \ this results in a transient micro-loop, since the Tier 1\n   switch will keep\
    \ passing packets to the affected prefix back to the\n   Tier 2 device, and the\
    \ Tier 2 will bounce them back again using the\n   default route.  This micro-loop\
    \ will last for the time it takes the\n   upstream device to fully update its\
    \ forwarding tables.\n   To minimize impact of such micro-loops, Tier 2 and Tier\
    \ 1 switches\n   can be configured with static \"discard\" or \"null\" routes\
    \ that will be\n   more specific than the default route for prefixes missing during\n\
    \   network convergence.  For Tier 2 switches, the discard route should\n   be\
    \ a summary route, covering all server subnets of the underlying\n   Tier 3 devices.\
    \  For Tier 1 devices, the discard route should be a\n   summary covering the\
    \ server IP address subnets allocated for the\n   whole data center.  Those discard\
    \ routes will only take precedence\n   for the duration of network convergence,\
    \ until the device learns a\n   more specific prefix via a new path.\n"
- title: 8.  Additional Options for Design
  contents:
  - '8.  Additional Options for Design

    '
- title: 8.1.  Third-Party Route Injection
  contents:
  - "8.1.  Third-Party Route Injection\n   BGP allows for a \"third-party\", i.e.,\
    \ a directly attached BGP\n   speaker, to inject routes anywhere in the network\
    \ topology, meeting\n   REQ5.  This can be achieved by peering via a multi-hop\
    \ BGP session\n   with some or even all devices in the topology.  Furthermore,\
    \ BGP\n   diverse path distribution [RFC6774] could be used to inject multiple\n\
    \   BGP next hops for the same prefix to facilitate load balancing, or\n   using\
    \ the BGP ADD-PATH capability [RFC7911] if supported by the\n   implementation.\
    \  Unfortunately, in many implementations, ADD-PATH has\n   been found to only\
    \ support IBGP properly in the use cases for which\n   it was originally optimized;\
    \ this limits the \"third-party\" peering to\n   IBGP only.\n   To implement route\
    \ injection in the proposed design, a third-party\n   BGP speaker may peer with\
    \ Tier 3 and Tier 1 switches, injecting the\n   same prefix, but using a special\
    \ set of BGP next hops for Tier 1\n   devices.  Those next hops are assumed to\
    \ resolve recursively via BGP,\n   and could be, for example, IP addresses on\
    \ Tier 3 devices.  The\n   resulting forwarding table programming could provide\
    \ desired traffic\n   proportion distribution among different clusters.\n"
- title: 8.2.  Route Summarization within Clos Topology
  contents:
  - "8.2.  Route Summarization within Clos Topology\n   As mentioned previously, route\
    \ summarization is not possible within\n   the proposed Clos topology since it\
    \ makes the network susceptible to\n   route black-holing under single link failures.\
    \  The main problem is\n   the limited number of redundant paths between network\
    \ elements, e.g.,\n   there is only a single path between any pair of Tier 1 and\
    \ Tier 3\n   devices.  However, some operators may find route aggregation\n  \
    \ desirable to improve control-plane stability.\n   If any technique to summarize\
    \ within the topology is planned,\n   modeling of the routing behavior and potential\
    \ for black-holing\n   should be done not only for single or multiple link failures,\
    \ but\n   also for fiber pathway failures or optical domain failures when the\n\
    \   topology extends beyond a physical location.  Simple modeling can be\n   done\
    \ by checking the reachability on devices doing summarization\n   under the condition\
    \ of a link or pathway failure between a set of\n   devices in every tier as well\
    \ as to the WAN routers when external\n   connectivity is present.\n   Route summarization\
    \ would be possible with a small modification to\n   the network topology, though\
    \ the tradeoff would be reduction of the\n   total size of the network as well\
    \ as network congestion under\n   specific failures.  This approach is very similar\
    \ to the technique\n   described above, which allows Border Routers to summarize\
    \ the entire\n   data center address space.\n"
- title: 8.2.1.  Collapsing Tier 1 Devices Layer
  contents:
  - "8.2.1.  Collapsing Tier 1 Devices Layer\n   In order to add more paths between\
    \ Tier 1 and Tier 3 devices, group\n   Tier 2 devices into pairs, and then connect\
    \ the pairs to the same\n   group of Tier 1 devices.  This is logically equivalent\
    \ to\n   \"collapsing\" Tier 1 devices into a group of half the size, merging\n\
    \   the links on the \"collapsed\" devices.  The result is illustrated in\n  \
    \ Figure 6.  For example, in this topology DEV C and DEV D connect to\n   the\
    \ same set of Tier 1 devices (DEV 1 and DEV 2), whereas before they\n   were connecting\
    \ to different groups of Tier 1 devices.\n                    Tier 2       Tier\
    \ 1       Tier 2\n                   +-----+      +-----+      +-----+\n     +-------------|\
    \ DEV |------| DEV |------|     |-------------+\n     |       +-----|  C  |--++--|\
    \  1  |--++--|     |-----+       |\n     |       |     +-----+  ||  +-----+  ||\
    \  +-----+     |       |\n     |       |              ||           ||        \
    \      |       |\n     |       |     +-----+  ||  +-----+  ||  +-----+     | \
    \      |\n     | +-----+-----| DEV |--++--| DEV |--++--|     |-----+-----+ |\n\
    \     | |     | +---|  D  |------|  2  |------|     |---+ |     | |\n     | |\
    \     | |   +-----+      +-----+      +-----+   | |     | |\n     | |     | |\
    \                                       | |     | |\n   +-----+ +-----+      \
    \                             +-----+ +-----+\n   | DEV | | DEV |            \
    \                       |     | |     |\n   |  A  | |  B  | Tier 3           \
    \          Tier 3 |     | |     |\n   +-----+ +-----+                        \
    \           +-----+ +-----+\n     | |     | |                                \
    \       | |     | |\n     O O     O O             <- Servers ->             O\
    \ O     O O\n                      Figure 6: 5-Stage Clos Topology\n   Having\
    \ this design in place, Tier 2 devices may be configured to\n   advertise only\
    \ a default route down to Tier 3 devices.  If a link\n   between Tier 2 and Tier\
    \ 3 fails, the traffic will be re-routed via\n   the second available path known\
    \ to a Tier 2 switch.  It is still not\n   possible to advertise a summary route\
    \ covering prefixes for a single\n   cluster from Tier 2 devices since each of\
    \ them has only a single path\n   down to this prefix.  It would require dual-homed\
    \ servers to\n   accomplish that.  Also note that this design is only resilient\
    \ to\n   single link failures.  It is possible for a double link failure to\n\
    \   isolate a Tier 2 device from all paths toward a specific Tier 3\n   device,\
    \ thus causing a routing black-hole.\n   A result of the proposed topology modification\
    \ would be a reduction\n   of the port capacity of Tier 1 devices.  This limits\
    \ the maximum\n   number of attached Tier 2 devices, and therefore will limit\
    \ the\n   maximum DC network size.  A larger network would require different\n\
    \   Tier 1 devices that have higher port density to implement this\n   change.\n\
    \   Another problem is traffic rebalancing under link failures.  Since\n   there\
    \ are two paths from Tier 1 to Tier 3, a failure of the link\n   between Tier\
    \ 1 and Tier 2 switch would result in all traffic that was\n   taking the failed\
    \ link to switch to the remaining path.  This will\n   result in doubling the\
    \ link utilization on the remaining link.\n"
- title: 8.2.2.  Simple Virtual Aggregation
  contents:
  - "8.2.2.  Simple Virtual Aggregation\n   A completely different approach to route\
    \ summarization is possible,\n   provided that the main goal is to reduce the\
    \ FIB size, while allowing\n   the control plane to disseminate full routing information.\
    \  Firstly,\n   it could be easily noted that in many cases multiple prefixes,\
    \ some\n   of which are less specific, share the same set of the next hops (same\n\
    \   ECMP group).  For example, from the perspective of Tier 3 devices,\n   all\
    \ routes learned from upstream Tier 2 devices, including the\n   default route,\
    \ will share the same set of BGP next hops, provided\n   that there are no failures\
    \ in the network.  This makes it possible to\n   use the technique similar to\
    \ that described in [RFC6769] and only\n   install the least specific route in\
    \ the FIB, ignoring more specific\n   routes if they share the same next-hop set.\
    \  For example, under\n   normal network conditions, only the default route needs\
    \ to be\n   programmed into the FIB.\n   Furthermore, if the Tier 2 devices are\
    \ configured with summary\n   prefixes covering all of their attached Tier 3 device's\
    \ prefixes, the\n   same logic could be applied in Tier 1 devices as well and,\
    \ by\n   induction to Tier 2/Tier 3 switches in different clusters.  These\n \
    \  summary routes should still allow for more specific prefixes to leak\n   to\
    \ Tier 1 devices, to enable detection of mismatches in the next-hop\n   sets if\
    \ a particular link fails, thus changing the next-hop set for a\n   specific prefix.\n\
    \   Restating once again, this technique does not reduce the amount of\n   control-plane\
    \ state (i.e., BGP UPDATEs, BGP Loc-RIB size), but only\n   allows for more efficient\
    \ FIB utilization, by detecting more specific\n   prefixes that share their next-hop\
    \ set with a subsuming less specific\n   prefix.\n"
- title: 8.3.  ICMP Unreachable Message Masquerading
  contents:
  - "8.3.  ICMP Unreachable Message Masquerading\n   This section discusses some operational\
    \ aspects of not advertising\n   point-to-point link subnets into BGP, as previously\
    \ identified as an\n   option in Section 5.2.3.  The operational impact of this\
    \ decision\n   could be seen when using the well-known \"traceroute\" tool.\n\
    \   Specifically, IP addresses displayed by the tool will be the link's\n   point-to-point\
    \ addresses, and hence will be unreachable for\n   management connectivity.  This\
    \ makes some troubleshooting more\n   complicated.\n   One way to overcome this\
    \ limitation is by using the DNS subsystem to\n   create the \"reverse\" entries\
    \ for these point-to-point IP addresses\n   pointing to the same name as the loopback\
    \ address.  The connectivity\n   then can be made by resolving this name to the\
    \ \"primary\" IP address\n   of the device, e.g., its Loopback interface, which\
    \ is always\n   advertised into BGP.  However, this creates a dependency on the\
    \ DNS\n   subsystem, which may be unavailable during an outage.\n   Another option\
    \ is to make the network device perform IP address\n   masquerading, that is,\
    \ rewriting the source IP addresses of the\n   appropriate ICMP messages sent\
    \ by the device with the \"primary\" IP\n   address of the device.  Specifically,\
    \ the ICMP Destination\n   Unreachable Message (type 3) code 3 (port unreachable)\
    \ and ICMP Time\n   Exceeded (type 11) code 0 are required for correct operation\
    \ of the\n   \"traceroute\" tool.  With this modification, the \"traceroute\"\
    \ probes\n   sent to the devices will always be sent back with the \"primary\"\
    \ IP\n   address as the source, allowing the operator to discover the\n   \"reachable\"\
    \ IP address of the box.  This has the downside of hiding\n   the address of the\
    \ \"entry point\" into the device.  If the devices\n   support [RFC5837], this\
    \ may allow the best of both worlds by\n   providing the information about the\
    \ incoming interface even if the\n   return address is the \"primary\" IP address.\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   The design does not introduce any additional\
    \ security concerns.\n   General BGP security considerations are discussed in\
    \ [RFC4271] and\n   [RFC4272].  Since a DC is a single-operator domain, this document\n\
    \   assumes that edge filtering is in place to prevent attacks against\n   the\
    \ BGP sessions themselves from outside the perimeter of the DC.\n   This may be\
    \ a more feasible option for most deployments than having\n   to deal with key\
    \ management for TCP MD5 as described in [RFC2385] or\n   dealing with the lack\
    \ of implementations of the TCP Authentication\n   Option [RFC5925] available\
    \ at the time of publication of this\n   document.  The Generalized TTL Security\
    \ Mechanism [RFC5082] could\n   also be used to further reduce the risk of BGP\
    \ session spoofing.\n"
- title: 10.  References
  contents:
  - '10.  References

    '
- title: 10.1.  Normative References
  contents:
  - "10.1.  Normative References\n   [RFC4271]  Rekhter, Y., Ed., Li, T., Ed., and\
    \ S. Hares, Ed., \"A\n              Border Gateway Protocol 4 (BGP-4)\", RFC 4271,\n\
    \              DOI 10.17487/RFC4271, January 2006,\n              <http://www.rfc-editor.org/info/rfc4271>.\n\
    \   [RFC6996]  Mitchell, J., \"Autonomous System (AS) Reservation for\n      \
    \        Private Use\", BCP 6, RFC 6996, DOI 10.17487/RFC6996, July\n        \
    \      2013, <http://www.rfc-editor.org/info/rfc6996>.\n"
- title: 10.2.  Informative References
  contents:
  - "10.2.  Informative References\n   [ALFARES2008]\n              Al-Fares, M.,\
    \ Loukissas, A., and A. Vahdat, \"A Scalable,\n              Commodity Data Center\
    \ Network Architecture\",\n              DOI 10.1145/1402958.1402967, August 2008,\n\
    \              <http://dl.acm.org/citation.cfm?id=1402967>.\n   [ALLOWASIN]\n\
    \              Cisco Systems, \"Allowas-in Feature in BGP Configuration\n    \
    \          Example\", February 2015,\n              <http://www.cisco.com/c/en/us/support/docs/ip/\n\
    \              border-gateway-protocol-bgp/112236-allowas-in-bgp-config-\n   \
    \           example.html>.\n   [BGP-PIC]  Bashandy, A., Ed., Filsfils, C., and\
    \ P. Mohapatra, \"BGP\n              Prefix Independent Convergence\", Work in\
    \ Progress,\n              draft-ietf-rtgwg-bgp-pic-02, August 2016.\n   [CLOS1953]\
    \ Clos, C., \"A Study of Non-Blocking Switching Networks\",\n              The\
    \ Bell System Technical Journal, Vol. 32(2),\n              DOI 10.1002/j.1538-7305.1953.tb01433.x,\
    \ March 1953.\n   [CONDITIONALROUTE]\n              Cisco Systems, \"Configuring\
    \ and Verifying the BGP\n              Conditional Advertisement Feature\", August\
    \ 2005,\n              <http://www.cisco.com/c/en/us/support/docs/ip/\n      \
    \        border-gateway-protocol-bgp/16137-cond-adv.html>.\n   [CONS-HASH]\n \
    \             Wikipedia, \"Consistent Hashing\", July 2016,\n              <https://en.wikipedia.org/w/\n\
    \              index.php?title=Consistent_hashing&oldid=728825684>.\n   [FB4POST]\
    \  Farrington, N. and A. Andreyev, \"Facebook's Data Center\n              Network\
    \ Architecture\", May 2013,\n              <http://nathanfarrington.com/papers/facebook-oic13.pdf>.\n\
    \   [GREENBERG2009]\n              Greenberg, A., Hamilton, J., and D. Maltz,\
    \ \"The Cost of a\n              Cloud: Research Problems in Data Center Networks\"\
    ,\n              DOI 10.1145/1496091.1496103, January 2009,\n              <http://dl.acm.org/citation.cfm?id=1496103>.\n\
    \   [HADOOP]   Apache, \"Apache Hadoop\", April 2016,\n              <https://hadoop.apache.org/>.\n\
    \   [IANA.AS]  IANA, \"Autonomous System (AS) Numbers\",\n              <http://www.iana.org/assignments/as-numbers>.\n\
    \   [IEEE8021D-1990]\n              IEEE, \"IEEE Standard for Local and Metropolitan\
    \ Area\n              Networks: Media Access Control (MAC) Bridges\", IEEE\n \
    \             Std 802.1D, DOI 10.1109/IEEESTD.1991.101050, 1991,\n           \
    \   <http://ieeexplore.ieee.org/servlet/opac?punumber=2255>.\n   [IEEE8021D-2004]\n\
    \              IEEE, \"IEEE Standard for Local and Metropolitan Area\n       \
    \       Networks: Media Access Control (MAC) Bridges\", IEEE\n              Std\
    \ 802.1D, DOI 10.1109/IEEESTD.2004.94569, June 2004,\n              <http://ieeexplore.ieee.org/servlet/opac?punumber=9155>.\n\
    \   [IEEE8021Q]\n              IEEE, \"IEEE Standard for Local and Metropolitan\
    \ Area\n              Networks: Bridges and Bridged Networks\", IEEE Std 802.1Q,\n\
    \              DOI 10.1109/IEEESTD.2014.6991462,\n              <http://ieeexplore.ieee.org/servlet/\n\
    \              opac?punumber=6991460>.\n   [IEEE8023AD]\n              IEEE, \"\
    Amendment to Carrier Sense Multiple Access With\n              Collision Detection\
    \ (CSMA/CD) Access Method and Physical\n              Layer Specifications - Aggregation\
    \ of Multiple Link\n              Segments\", IEEE Std 802.3ad,\n            \
    \  DOI 10.1109/IEEESTD.2000.91610, October 2000,\n              <http://ieeexplore.ieee.org/servlet/opac?punumber=6867>.\n\
    \   [INTERCON] Dally, W. and B. Towles, \"Principles and Practices of\n      \
    \        Interconnection Networks\", ISBN 978-0122007514, January\n          \
    \    2004, <http://dl.acm.org/citation.cfm?id=995703>.\n   [JAKMA2008]\n     \
    \         Jakma, P., \"BGP Path Hunting\", 2008,\n              <https://blogs.oracle.com/paulj/entry/bgp_path_hunting>.\n\
    \   [L3DSR]    Schaumann, J., \"L3DSR - Overcoming Layer 2 Limitations of\n  \
    \            Direct Server Return Load Balancing\", 2011,\n              <https://www.nanog.org/meetings/nanog51/presentations/\n\
    \              Monday/NANOG51.Talk45.nanog51-Schaumann.pdf>.\n   [LINK]     Mohapatra,\
    \ P. and R. Fernando, \"BGP Link Bandwidth\n              Extended Community\"\
    , Work in Progress, draft-ietf-idr-\n              link-bandwidth-06, January\
    \ 2013.\n   [REMOVAL]  Mitchell, J., Rao, D., and R. Raszuk, \"Private Autonomous\n\
    \              System (AS) Removal Requirements\", Work in Progress,\n       \
    \       draft-mitchell-grow-remove-private-as-04, April 2015.\n   [RFC2328]  Moy,\
    \ J., \"OSPF Version 2\", STD 54, RFC 2328,\n              DOI 10.17487/RFC2328,\
    \ April 1998,\n              <http://www.rfc-editor.org/info/rfc2328>.\n   [RFC2385]\
    \  Heffernan, A., \"Protection of BGP Sessions via the TCP MD5\n             \
    \ Signature Option\", RFC 2385, DOI 10.17487/RFC2385, August\n              1998,\
    \ <http://www.rfc-editor.org/info/rfc2385>.\n   [RFC2992]  Hopps, C., \"Analysis\
    \ of an Equal-Cost Multi-Path\n              Algorithm\", RFC 2992, DOI 10.17487/RFC2992,\
    \ November 2000,\n              <http://www.rfc-editor.org/info/rfc2992>.\n  \
    \ [RFC4272]  Murphy, S., \"BGP Security Vulnerabilities Analysis\",\n        \
    \      RFC 4272, DOI 10.17487/RFC4272, January 2006,\n              <http://www.rfc-editor.org/info/rfc4272>.\n\
    \   [RFC4277]  McPherson, D. and K. Patel, \"Experience with the BGP-4\n     \
    \         Protocol\", RFC 4277, DOI 10.17487/RFC4277, January 2006,\n        \
    \      <http://www.rfc-editor.org/info/rfc4277>.\n   [RFC4786]  Abley, J. and\
    \ K. Lindqvist, \"Operation of Anycast\n              Services\", BCP 126, RFC\
    \ 4786, DOI 10.17487/RFC4786,\n              December 2006, <http://www.rfc-editor.org/info/rfc4786>.\n\
    \   [RFC5082]  Gill, V., Heasley, J., Meyer, D., Savola, P., Ed., and C.\n   \
    \           Pignataro, \"The Generalized TTL Security Mechanism\n            \
    \  (GTSM)\", RFC 5082, DOI 10.17487/RFC5082, October 2007,\n              <http://www.rfc-editor.org/info/rfc5082>.\n\
    \   [RFC5837]  Atlas, A., Ed., Bonica, R., Ed., Pignataro, C., Ed., Shen,\n  \
    \            N., and JR. Rivers, \"Extending ICMP for Interface and\n        \
    \      Next-Hop Identification\", RFC 5837, DOI 10.17487/RFC5837,\n          \
    \    April 2010, <http://www.rfc-editor.org/info/rfc5837>.\n   [RFC5880]  Katz,\
    \ D. and D. Ward, \"Bidirectional Forwarding Detection\n              (BFD)\"\
    , RFC 5880, DOI 10.17487/RFC5880, June 2010,\n              <http://www.rfc-editor.org/info/rfc5880>.\n\
    \   [RFC5925]  Touch, J., Mankin, A., and R. Bonica, \"The TCP\n             \
    \ Authentication Option\", RFC 5925, DOI 10.17487/RFC5925,\n              June\
    \ 2010, <http://www.rfc-editor.org/info/rfc5925>.\n   [RFC6325]  Perlman, R.,\
    \ Eastlake 3rd, D., Dutt, D., Gai, S., and A.\n              Ghanwani, \"Routing\
    \ Bridges (RBridges): Base Protocol\n              Specification\", RFC 6325,\
    \ DOI 10.17487/RFC6325, July 2011,\n              <http://www.rfc-editor.org/info/rfc6325>.\n\
    \   [RFC6769]  Raszuk, R., Heitz, J., Lo, A., Zhang, L., and X. Xu,\n        \
    \      \"Simple Virtual Aggregation (S-VA)\", RFC 6769,\n              DOI 10.17487/RFC6769,\
    \ October 2012,\n              <http://www.rfc-editor.org/info/rfc6769>.\n   [RFC6774]\
    \  Raszuk, R., Ed., Fernando, R., Patel, K., McPherson, D.,\n              and\
    \ K. Kumaki, \"Distribution of Diverse BGP Paths\",\n              RFC 6774, DOI\
    \ 10.17487/RFC6774, November 2012,\n              <http://www.rfc-editor.org/info/rfc6774>.\n\
    \   [RFC6793]  Vohra, Q. and E. Chen, \"BGP Support for Four-Octet\n         \
    \     Autonomous System (AS) Number Space\", RFC 6793,\n              DOI 10.17487/RFC6793,\
    \ December 2012,\n              <http://www.rfc-editor.org/info/rfc6793>.\n  \
    \ [RFC7067]  Dunbar, L., Eastlake 3rd, D., Perlman, R., and I.\n             \
    \ Gashinsky, \"Directory Assistance Problem and High-Level\n              Design\
    \ Proposal\", RFC 7067, DOI 10.17487/RFC7067, November\n              2013, <http://www.rfc-editor.org/info/rfc7067>.\n\
    \   [RFC7130]  Bhatia, M., Ed., Chen, M., Ed., Boutros, S., Ed.,\n           \
    \   Binderberger, M., Ed., and J. Haas, Ed., \"Bidirectional\n              Forwarding\
    \ Detection (BFD) on Link Aggregation Group (LAG)\n              Interfaces\"\
    , RFC 7130, DOI 10.17487/RFC7130, February\n              2014, <http://www.rfc-editor.org/info/rfc7130>.\n\
    \   [RFC7196]  Pelsser, C., Bush, R., Patel, K., Mohapatra, P., and O.\n     \
    \         Maennel, \"Making Route Flap Damping Usable\", RFC 7196,\n         \
    \     DOI 10.17487/RFC7196, May 2014,\n              <http://www.rfc-editor.org/info/rfc7196>.\n\
    \   [RFC7911]  Walton, D., Retana, A., Chen, E., and J. Scudder,\n           \
    \   \"Advertisement of Multiple Paths in BGP\", RFC 7911,\n              DOI 10.17487/RFC7911,\
    \ July 2016,\n              <http://www.rfc-editor.org/info/rfc7911>.\n   [VENDOR-REMOVE-PRIVATE-AS]\n\
    \              Cisco Systems, \"Removing Private Autonomous System Numbers\n \
    \             in BGP\", August 2005,\n              <http://www.cisco.com/en/US/tech/tk365/\n\
    \              technologies_tech_note09186a0080093f27.shtml>.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   This publication summarizes the work of many people who\
    \ participated\n   in developing, testing, and deploying the proposed network\
    \ design,\n   some of whom were George Chen, Parantap Lahiri, Dave Maltz, Edet\n\
    \   Nkposong, Robert Toomey, and Lihua Yuan.  The authors would also like\n  \
    \ to thank Linda Dunbar, Anoop Ghanwani, Susan Hares, Danny McPherson,\n   Robert\
    \ Raszuk, and Russ White for reviewing this document and\n   providing valuable\
    \ feedback, and Mary Mitchell for initial grammar\n   and style suggestions.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Petr Lapukhov\n   Facebook\n   1 Hacker Way\n   Menlo\
    \ Park, CA  94025\n   United States of America\n   Email: petr@fb.com\n   Ariff\
    \ Premji\n   Arista Networks\n   5453 Great America Parkway\n   Santa Clara, CA\
    \  95054\n   United States of America\n   Email: ariff@arista.com\n   URI:   http://arista.com/\n\
    \   Jon Mitchell (editor)\n   Email: jrmitche@puck.nether.net\n"
