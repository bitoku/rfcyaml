- title: __initial_text__
  contents:
  - '                  NFS Version 3 Protocol Specification

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\n\
    \   This memo does not specify an Internet standard of any kind.\n   Distribution\
    \ of this memo is unlimited.\n"
- title: IESG Note
  contents:
  - "IESG Note\n   Internet Engineering Steering Group comment: please note that\n\
    \   the IETF is not involved in creating or maintaining this\n   specification.\
    \  This is the significance of the specification\n   not being on the standards\
    \ track.\n"
- title: Abstract
  contents:
  - "Abstract\n   This paper describes the NFS version 3 protocol.  This paper is\n\
    \   provided so that people can write compatible implementations.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.    Introduction . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n   1.1     Scope of the NFS version 3 protocol  . . . . . . .\
    \ . . .   4\n   1.2     Useful terms . . . . . . . . . . . . . . . . . . . . .\
    \ .   5\n   1.3     Remote Procedure Call  . . . . . . . . . . . . . . . . . \
    \  5\n   1.4     External Data Representation . . . . . . . . . . . . . .   5\n\
    \   1.5     Authentication and Permission Checking . . . . . . . . .   7\n   1.6\
    \     Philosophy . . . . . . . . . . . . . . . . . . . . . . .   8\n   1.7   \
    \  Changes from the NFS version 2 protocol  . . . . . . . .  11\n   2.    RPC\
    \ Information  . . . . . . . . . . . . . . . . . . . . .  14\n   2.1     Authentication\
    \ . . . . . . . . . . . . . . . . . . . . .  14\n   2.2     Constants  . . . .\
    \ . . . . . . . . . . . . . . . . . . .  14\n   2.3     Transport address  . .\
    \ . . . . . . . . . . . . . . . . .  14\n   2.4     Sizes  . . . . . . . . . .\
    \ . . . . . . . . . . . . . . .  14\n   2.5     Basic Data Types . . . . . . .\
    \ . . . . . . . . . . . . .  15\n   2.6     Defined Error Numbers  . . . . . .\
    \ . . . . . . . . . . .  17\n   3.    Server Procedures  . . . . . . . . . . .\
    \ . . . . . . . . .  27\n   3.1     General comments on attributes . . . . . .\
    \ . . . . . . .  29\n   3.2     General comments on filenames  . . . . . . . .\
    \ . . . . .  30\n   3.3.0   NULL: Do nothing . . . . . . . . . . . . . . . . .\
    \ . . .  31\n   3.3.1   GETATTR: Get file attributes . . . . . . . . . . . . .\
    \ .  32\n   3.3.2   SETATTR: Set file attributes . . . . . . . . . . . . . . \
    \ 33\n   3.3.3   LOOKUP: Lookup filename  . . . . . . . . . . . . . . . .  37\n\
    \   3.3.4   ACCESS: Check access permission  . . . . . . . . . . . .  40\n   3.3.5\
    \   READLINK: Read from symbolic link  . . . . . . . . . . .  44\n   3.3.6   READ:\
    \ Read from file . . . . . . . . . . . . . . . . . .  46\n   3.3.7   WRITE: Write\
    \ to file . . . . . . . . . . . . . . . . . .  49\n   3.3.8   CREATE: Create a\
    \ file  . . . . . . . . . . . . . . . . .  54\n   3.3.9   MKDIR: Create a directory\
    \  . . . . . . . . . . . . . . .  58\n   3.3.10  SYMLINK: Create a symbolic link\
    \  . . . . . . . . . . . .  61\n   3.3.11  MKNOD: Create a special device . .\
    \ . . . . . . . . . . .  63\n   3.3.12  REMOVE: Remove a file  . . . . . . . .\
    \ . . . . . . . . .  67\n   3.3.13  RMDIR: Remove a directory  . . . . . . . .\
    \ . . . . . . .  69\n   3.3.14  RENAME: Rename a file or directory . . . . . .\
    \ . . . . .  71\n   3.3.15  LINK: Create link to an object . . . . . . . . . .\
    \ . . .  74\n   3.3.16  READDIR: Read From directory . . . . . . . . . . . . .\
    \ .  76\n   3.3.17  READDIRPLUS: Extended read from directory  . . . . . . . \
    \ 80\n   3.3.18  FSSTAT: Get dynamic file system information  . . . . . .  84\n\
    \   3.3.19  FSINFO: Get static file system information . . . . . . .  86\n   3.3.20\
    \  PATHCONF: Retrieve POSIX information . . . . . . . . . .  90\n   3.3.21  COMMIT:\
    \ Commit cached data on a server to stable storage  92\n   4.    Implementation\
    \ issues  . . . . . . . . . . . . . . . . . .  96\n   4.1     Multiple version\
    \ support . . . . . . . . . . . . . . . .  96\n   4.2     Server/client relationship\
    \ . . . . . . . . . . . . . . .  96\n   4.3     Path name interpretation . . .\
    \ . . . . . . . . . . . . .  97\n   4.4     Permission issues  . . . . . . . .\
    \ . . . . . . . . . . .  98\n   4.5     Duplicate request cache  . . . . . . .\
    \ . . . . . . . . .  99\n   4.6     File name component handling . . . . . . .\
    \ . . . . . . . 101\n   4.7     Synchronous modifying operations . . . . . . .\
    \ . . . . . 101\n   4.8     Stable storage . . . . . . . . . . . . . . . . . .\
    \ . . . 101\n   4.9     Lookups and name resolution  . . . . . . . . . . . . .\
    \ . 102\n   4.10    Adaptive retransmission  . . . . . . . . . . . . . . . . 102\n\
    \   4.11    Caching policies . . . . . . . . . . . . . . . . . . . . 102\n   4.12\
    \    Stable versus unstable writes. . . . . . . . . . . . . . 103\n   4.13   \
    \ 32 bit clients/servers and 64 bit clients/servers. . . . 104\n   5.    Appendix\
    \ I: Mount protocol . . . . . . . . . . . . . . . . 106\n   5.1     RPC Information\
    \  . . . . . . . . . . . . . . . . . . . . 106\n   5.1.1     Authentication .\
    \ . . . . . . . . . . . . . . . . . . . 106\n   5.1.2     Constants  . . . . .\
    \ . . . . . . . . . . . . . . . . . 106\n   5.1.3     Transport address  . . .\
    \ . . . . . . . . . . . . . . . 106\n   5.1.4     Sizes  . . . . . . . . . . .\
    \ . . . . . . . . . . . . . 106\n   5.1.5     Basic Data Types . . . . . . . .\
    \ . . . . . . . . . . . 106\n   5.2     Server Procedures  . . . . . . . . . .\
    \ . . . . . . . . . 107\n   5.2.0     NULL: Do nothing . . . . . . . . . . . .\
    \ . . . . . . . 108\n   5.2.1     MNT: Add mount entry . . . . . . . . . . . .\
    \ . . . . . 109\n   5.2.2     DUMP: Return mount entries . . . . . . . . . . .\
    \ . . . 110\n   5.2.3     UMNT: Remove mount entry . . . . . . . . . . . . . .\
    \ . 111\n   5.2.4     UMNTALL: Remove all mount entries  . . . . . . . . . . 112\n\
    \   5.2.5     EXPORT: Return export list . . . . . . . . . . . . . . 113\n   6.\
    \    Appendix II: Lock manager protocol . . . . . . . . . . . . 114\n   6.1  \
    \   RPC Information  . . . . . . . . . . . . . . . . . . . . 114\n   6.1.1   \
    \  Authentication . . . . . . . . . . . . . . . . . . . . 114\n   6.1.2     Constants\
    \  . . . . . . . . . . . . . . . . . . . . . . 114\n   6.1.3     Transport Address\
    \  . . . . . . . . . . . . . . . . . . 115\n   6.1.4     Basic Data Types . .\
    \ . . . . . . . . . . . . . . . . . 115\n   6.2     NLM Procedures . . . . . .\
    \ . . . . . . . . . . . . . . . 118\n   6.2.0     NULL: Do nothing . . . . . .\
    \ . . . . . . . . . . . . . 120\n   6.3     Implementation issues  . . . . . .\
    \ . . . . . . . . . . . 120\n   6.3.1     64-bit offsets and lengths . . . . .\
    \ . . . . . . . . . 120\n   6.3.2     File handles . . . . . . . . . . . . . .\
    \ . . . . . . . 120\n   7.    Appendix III: Bibliography . . . . . . . . . . .\
    \ . . . . . 122\n   8.    Security Considerations  . . . . . . . . . . . . . .\
    \ . . . 125\n   9.    Acknowledgements . . . . . . . . . . . . . . . . . . . .\
    \ . 125\n   10.   Authors' Addresses . . . . . . . . . . . . . . . . . . . . 126\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   Sun's NFS protocol provides transparent remote access to\
    \ shared\n   file systems across networks. The NFS protocol is designed to be\n\
    \   machine, operating system, network architecture, and transport\n   protocol\
    \ independent. This independence is achieved through the\n   use of Remote Procedure\
    \ Call (RPC) primitives built on top of an\n   eXternal Data Representation (XDR).\
    \  Implementations of the NFS\n   version 2 protocol exist for a variety of machines,\
    \ from personal\n   computers to supercomputers. The initial version of the NFS\n\
    \   protocol is specified in the Network File System Protocol\n   Specification\
    \ [RFC1094]. A description of the initial\n   implementation can be found in [Sandberg].\n\
    \   The supporting MOUNT protocol performs the operating\n   system-specific functions\
    \ that allow clients to attach remote\n   directory trees to a point within the\
    \ local file system. The\n   mount process also allows the server to grant remote\
    \ access\n   privileges to a restricted set of clients via export control.\n \
    \  The Lock Manager provides support for file locking when used in\n   the NFS\
    \ environment. The Network Lock Manager (NLM) protocol\n   isolates the inherently\
    \ stateful aspects of file locking into a\n   separate protocol.\n   A complete\
    \ description of the above protocols and their\n   implementation is to be found\
    \ in [X/OpenNFS].\n   The purpose of this document is to:\n        o Specify the\
    \ NFS version 3 protocol.\n        o Describe semantics of the protocol through\
    \ annotation\n          and description of intended implementation.\n        o\
    \ Specify the MOUNT version 3 protocol.\n        o Briefly describe the changes\
    \ between the NLM version 3\n          protocol and the NLM version 4 protocol.\n\
    \   The normative text is the description of the RPC procedures and\n   arguments\
    \ and results, which defines the over-the-wire protocol,\n   and the semantics\
    \ of those procedures. The material describing\n   implementation practice aids\
    \ the understanding of the protocol\n   specification and describes some possible\
    \ implementation issues\n   and solutions. It is not possible to describe all\
    \ implementations\n   and the UNIX operating system implementation of the NFS\
    \ version 3\n   protocol is most often used to provide examples. Given that, the\n\
    \   implementation discussion does not bear the authority of the\n   description\
    \ of the over-the-wire protocol itself.\n"
- title: 1.1 Scope of the NFS version 3 protocol
  contents:
  - "1.1 Scope of the NFS version 3 protocol\n   This revision of the NFS protocol\
    \ addresses new requirements.\n   The need to support larger files and file systems\
    \ has prompted\n   extensions to allow 64 bit file sizes and offsets. The revision\n\
    \   enhances security by adding support for an access check to be\n   done on\
    \ the server. Performance modifications are of three\n   types:\n   1. The number\
    \ of over-the-wire packets for a given\n      set of file operations is reduced\
    \ by returning file\n      attributes on every operation, thus decreasing the\
    \ number\n      of calls to get modified attributes.\n   2. The write throughput\
    \ bottleneck caused by the synchronous\n      definition of write in the NFS version\
    \ 2 protocol has been\n      addressed by adding support so that the NFS server\
    \ can do\n      unsafe writes. Unsafe writes are writes which have not\n     \
    \ been committed to stable storage before the operation\n      returns.  This\
    \ specification defines a method for\n      committing these unsafe writes to\
    \ stable storage in a\n      reliable way.\n   3. Limitations on transfer sizes\
    \ have been relaxed.\n   The ability to support multiple versions of a protocol\
    \ in RPC\n   will allow implementors of the NFS version 3 protocol to define\n\
    \   clients and servers that provide backwards compatibility with\n   the existing\
    \ installed base of NFS version 2 protocol\n   implementations.\n   The extensions\
    \ described here represent an evolution of the\n   existing NFS protocol and most\
    \ of the design features of the\n   NFS protocol described in [Sandberg] persist.\
    \ See Changes\n   from the NFS version 2 protocol on page 11 for a more\n   detailed\
    \ summary of the changes introduced by this revision.\n"
- title: 1.2 Useful terms
  contents:
  - "1.2 Useful terms\n   In this specification, a \"server\" is a machine that provides\n\
    \   resources to the network; a \"client\" is a machine that accesses\n   resources\
    \ over the network; a \"user\" is a person logged in on a\n   client; an \"application\"\
    \ is a program that executes on a client.\n"
- title: 1.3 Remote Procedure Call
  contents:
  - "1.3 Remote Procedure Call\n   The Sun Remote Procedure Call specification provides\
    \ a\n   procedure-oriented interface to remote services. Each server\n   supplies\
    \ a program, which is a set of procedures. The NFS\n   service is one such program.\
    \ The combination of host address,\n   program number, version number, and procedure\
    \ number specify one\n   remote service procedure.  Servers can support multiple\
    \ versions\n   of a program by using different protocol version numbers.\n   The\
    \ NFS protocol was designed to not require any specific level\n   of reliability\
    \ from its lower levels so it could potentially be\n   used on many underlying\
    \ transport protocols. The NFS service is\n   based on RPC which provides the\
    \ abstraction above lower level\n   network and transport protocols.\n   The rest\
    \ of this document assumes the NFS environment is\n   implemented on top of Sun\
    \ RPC, which is specified in [RFC1057].\n   A complete discussion is found in\
    \ [Corbin].\n"
- title: 1.4 External Data Representation
  contents:
  - "1.4 External Data Representation\n   The eXternal Data Representation (XDR) specification\
    \ provides a\n   standard way of representing a set of data types on a network.\n\
    \   This solves the problem of different byte orders, structure\n   alignment,\
    \ and data type representation on different,\n   communicating machines.\n   In\
    \ this document, the RPC Data Description Language is used to\n   specify the\
    \ XDR format parameters and results to each of the RPC\n   service procedures\
    \ that an NFS server provides. The RPC Data\n   Description Language is similar\
    \ to declarations in the C\n   programming language. A few new constructs have\
    \ been added.\n   The notation:\n      string  name[SIZE];\n      string  data<DSIZE>;\n\
    \   defines name, which is a fixed size block of SIZE bytes, and\n   data, which\
    \ is a variable sized block of up to DSIZE bytes. This\n   notation indicates\
    \ fixed-length arrays and arrays with a\n   variable number of elements up to\
    \ a fixed maximum. A\n   variable-length definition with no size specified means\
    \ there is\n   no maximum size for the field.\n   The discriminated union definition:\n\
    \      union example switch (enum status) {\n           case OK:\n           \
    \   struct {\n                 filename      file1;\n                 filename\
    \      file2;\n                 integer       count;\n              }\n      \
    \     case ERROR:\n              struct {\n                 errstat       error;\n\
    \                 integer       errno;\n              }\n           default:\n\
    \              void;\n      }\n   defines a structure where the first thing over\
    \ the network is an\n   enumeration type called status. If the value of status\
    \ is OK,\n   the next thing on the network will be the structure containing\n\
    \   file1, file2, and count. Else, if the value of status is ERROR,\n   the next\
    \ thing on the network will be a structure containing\n   error and errno.  If\
    \ the value of status is neither OK nor\n   ERROR, then there is no more data\
    \ in the structure.\n   The XDR type, hyper, is an 8 byte (64 bit) quantity. It\
    \ is used\n   in the same way as the integer type. For example:\n      hyper \
    \         foo;\n      unsigned hyper bar;\n   foo is an 8 byte signed value, while\
    \ bar is an 8 byte unsigned\n   value.\n   Although RPC/XDR compilers exist to\
    \ generate client and server\n   stubs from RPC Data Description Language input,\
    \ NFS\n   implementations do not require their use. Any software that\n   provides\
    \ equivalent encoding and decoding to the canonical\n   network order of data\
    \ defined by XDR can be used to interoperate\n   with other NFS implementations.\n\
    \   XDR is described in [RFC1014].\n"
- title: 1.5 Authentication and Permission Checking
  contents:
  - "1.5 Authentication and Permission Checking\n   The RPC protocol includes a slot\
    \ for authentication parameters\n   on every call. The contents of the authentication\
    \ parameters are\n   determined by the type of authentication used by the server\
    \ and\n   client. A server may support several different flavors of\n   authentication\
    \ at once. The AUTH_NONE flavor provides null\n   authentication, that is, no\
    \ authentication information is\n   passed. The AUTH_UNIX flavor provides UNIX-style\
    \ user ID, group\n   ID, and groups with each call. The AUTH_DES flavor provides\n\
    \   DES-encrypted authentication parameters based on a network-wide\n   name,\
    \ with session keys exchanged via a public key scheme. The\n   AUTH_KERB flavor\
    \ provides DES encrypted authentication\n   parameters based on a network-wide\
    \ name with session keys\n   exchanged via Kerberos secret keys.\n   The NFS server\
    \ checks permissions by taking the credentials from\n   the RPC authentication\
    \ information in each remote request. For\n   example, using the AUTH_UNIX flavor\
    \ of authentication, the\n   server gets the user's effective user ID, effective\
    \ group ID and\n   groups on each call, and uses them to check access. Using user\n\
    \   ids and group ids implies that the client and server either\n   share the\
    \ same ID list or do local user and group ID mapping.\n   Servers and clients\
    \ must agree on the mapping from user to uid\n   and from group to gid, for those\
    \ sites that do not implement a\n   consistent user ID and group ID space. In\
    \ practice, such mapping\n   is typically performed on the server, following a\
    \ static mapping\n   scheme or a mapping established by the user from a client\
    \ at\n   mount time.\n   The AUTH_DES and AUTH_KERB style of authentication is\
    \ based on a\n   network-wide name. It provides greater security through the use\n\
    \   of DES encryption and public keys in the case of AUTH_DES, and\n   DES encryption\
    \ and Kerberos secret keys (and tickets) in the\n   AUTH_KERB case. Again, the\
    \ server and client must agree on the\n   identity of a particular name on the\
    \ network, but the name to\n   identity mapping is more operating system independent\
    \ than the\n   uid and gid mapping in AUTH_UNIX. Also, because the\n   authentication\
    \ parameters are encrypted, a malicious user must\n   know another users network\
    \ password or private key to masquerade\n   as that user. Similarly, the server\
    \ returns a verifier that is\n   also encrypted so that masquerading as a server\
    \ requires knowing\n   a network password.\n   The NULL procedure typically requires\
    \ no authentication.\n"
- title: 1.6 Philosophy
  contents:
  - "1.6 Philosophy\n   This specification defines the NFS version 3 protocol, that\
    \ is\n   the over-the-wire protocol by which a client accesses a server.\n   The\
    \ protocol provides a well-defined interface to a server's\n   file resources.\
    \ A client or server implements the protocol and\n   provides a mapping of the\
    \ local file system semantics and\n   actions into those defined in the NFS version\
    \ 3 protocol.\n   Implementations may differ to varying degrees, depending on\
    \ the\n   extent to which a given environment can support all the\n   operations\
    \ and semantics defined in the NFS version 3 protocol.\n   Although implementations\
    \ exist and are used to illustrate\n   various aspects of the NFS version 3 protocol,\
    \ the protocol\n   specification itself is the final description of how clients\n\
    \   access server resources.\n   Because the NFS version 3 protocol is designed\
    \ to be\n   operating-system independent, it does not necessarily match the\n\
    \   semantics of any existing system. Server implementations are\n   expected\
    \ to make a best effort at supporting the protocol.  If a\n   server cannot support\
    \ a particular protocol procedure, it may\n   return the error, NFS3ERR_NOTSUP,\
    \ that indicates that the\n   operation is not supported.  For example, many operating\
    \ systems\n   do not support the notion of a hard link. A server that cannot\n\
    \   support hard links should return NFS3ERR_NOTSUP in response to a\n   LINK\
    \ request. FSINFO describes the most commonly unsupported\n   procedures in the\
    \ properties bit map.  Alternatively, a server\n   may not natively support a\
    \ given operation, but can emulate it\n   in the NFS version 3 protocol implementation\
    \ to provide greater\n   functionality.\n   In some cases, a server can support\
    \ most of the semantics\n   described by the protocol but not all. For example,\
    \ the ctime\n   field in the fattr structure gives the time that a file's\n  \
    \ attributes were last modified. Many systems do not keep this\n   information.\
    \ In this case, rather than not support the GETATTR\n   operation, a server could\
    \ simulate it by returning the last\n   modified time in place of ctime.  Servers\
    \ must be careful when\n   simulating attribute information because of possible\
    \ side\n   effects on clients. For example, many clients use file\n   modification\
    \ times as a basis for their cache consistency\n   scheme.\n   NFS servers are\
    \ dumb and NFS clients are smart. It is the\n   clients that do the work required\
    \ to convert the generalized\n   file access that servers provide into a file\
    \ access method that\n   is useful to applications and users. In the LINK example\
    \ given\n   above, a UNIX client that received an NFS3ERR_NOTSUP error from\n\
    \   a server would do the recovery necessary to either make it look\n   to the\
    \ application like the link request had succeeded or return\n   a reasonable error.\
    \ In general, it is the burden of the client\n   to recover.\n   The NFS version\
    \ 3 protocol assumes a stateless server\n   implementation.  Statelessness means\
    \ that the server does not\n   need to maintain state about any of its clients\
    \ in order to\n   function correctly. Stateless servers have a distinct advantage\n\
    \   over stateful servers in the event of a crash. With stateless\n   servers,\
    \ a client need only retry a request until the server\n   responds; the client\
    \ does not even need to know that the server\n   has crashed. See additional comments\
    \ in Duplicate request cache\n   on page 99.\n   For a server to be useful, it\
    \ holds nonvolatile state: data\n   stored in the file system. Design assumptions\
    \ in the NFS version\n   3 protocol regarding flushing of modified data to stable\
    \ storage\n   reduce the number of failure modes in which data loss can occur.\n\
    \   In this way, NFS version 3 protocol implementations can tolerate\n   transient\
    \ failures, including transient failures of the network.\n   In general, server\
    \ implementations of the NFS version 3 protocol\n   cannot tolerate a non-transient\
    \ failure of the stable storage\n   itself. However, there exist fault tolerant\
    \ implementations\n   which attempt to address such problems.\n   That is not\
    \ to say that an NFS version 3 protocol server can't\n   maintain noncritical\
    \ state. In many cases, servers will maintain\n   state (cache) about previous\
    \ operations to increase performance.\n   For example, a client READ request might\
    \ trigger a read-ahead of\n   the next block of the file into the server's data\
    \ cache in the\n   anticipation that the client is doing a sequential read and\
    \ the\n   next client READ request will be satisfied from the server's\n   data\
    \ cache instead of from the disk. Read-ahead on the server\n   increases performance\
    \ by overlapping server disk I/O with client\n   requests. The important point\
    \ here is that the read-ahead block\n   is not necessary for correct server behavior.\
    \ If the server\n   crashes and loses its memory cache of read buffers, recovery\
    \ is\n   simple on reboot - clients will continue read operations\n   retrieving\
    \ data from the server disk.\n   Most data-modifying operations in the NFS protocol\
    \ are\n   synchronous.  That is, when a data modifying procedure returns\n   to\
    \ the client, the client can assume that the operation has\n   completed and any\
    \ modified data associated with the request is\n   now on stable storage. For\
    \ example, a synchronous client WRITE\n   request may cause the server to update\
    \ data blocks, file system\n   information blocks, and file attribute information\
    \ - the latter\n   information is usually referred to as metadata. When the WRITE\n\
    \   operation completes, the client can assume that the write data\n   is safe\
    \ and discard it.  This is a very important part of the\n   stateless nature of\
    \ the server. If the server did not flush\n   dirty data to stable storage before\
    \ returning to the client, the\n   client would have no way of knowing when it\
    \ was safe to discard\n   modified data. The following data modifying procedures\
    \ are\n   synchronous: WRITE (with stable flag set to FILE_SYNC), CREATE,\n  \
    \ MKDIR, SYMLINK, MKNOD, REMOVE, RMDIR, RENAME, LINK, and COMMIT.\n   The NFS\
    \ version 3 protocol introduces safe asynchronous writes\n   on the server, when\
    \ the WRITE procedure is used in conjunction\n   with the COMMIT procedure. The\
    \ COMMIT procedure provides a way\n   for the client to flush data from previous\
    \ asynchronous WRITE\n   requests on the server to stable storage and to detect\
    \ whether\n   it is necessary to retransmit the data. See the procedure\n   descriptions\
    \ of WRITE on page 49 and COMMIT on page 92.\n   The LOOKUP procedure is used\
    \ by the client to traverse\n   multicomponent file names (pathnames). Each call\
    \ to LOOKUP is\n   used to resolve one segment of a pathname. There are two reasons\n\
    \   for restricting LOOKUP to a single segment: it is hard to\n   standardize\
    \ a common format for hierarchical file names and the\n   client and server may\
    \ have different mappings of pathnames to\n   file systems. This would imply that\
    \ either the client must break\n   the path name at file system attachment points,\
    \ or the server\n   must know about the client's file system attachment points.\
    \ In\n   NFS version 3 protocol implementations, it is the client that\n   constructs\
    \ the hierarchical file name space using mounts to\n   build a hierarchy. Support\
    \ utilities, such as the Automounter,\n   provide a way to manage a shared, consistent\
    \ image of the file\n   name space while still being driven by the client mount\n\
    \   process.\n   Clients can perform caching in varied manner. The general\n \
    \  practice with the NFS version 2 protocol was to implement a\n   time-based\
    \ client-server cache consistency mechanism. It is\n   expected NFS version 3\
    \ protocol implementations will use a\n   similar mechanism. The NFS version 3\
    \ protocol has some explicit\n   support, in the form of additional attribute\
    \ information to\n   eliminate explicit attribute checks. However, caching is\
    \ not\n   required, nor is any caching policy defined by the protocol.\n   Neither\
    \ the NFS version 2 protocol nor the NFS version 3\n   protocol provide a means\
    \ of maintaining strict client-server\n   consistency (and, by implication, consistency\
    \ across client\n   caches).\n"
- title: 1.7 Changes from the NFS Version 2 Protocol
  contents:
  - "1.7 Changes from the NFS Version 2 Protocol\n   The ROOT and WRITECACHE procedures\
    \ have been removed. A MKNOD\n   procedure has been defined to allow the creation\
    \ of special\n   files, eliminating the overloading of CREATE. Caching on the\n\
    \   client is not defined nor dictated by the NFS version 3\n   protocol, but\
    \ additional information and hints have been added\n   to the protocol to allow\
    \ clients that implement caching to\n   manage their caches more effectively.\
    \ Procedures that affect the\n   attributes of a file or directory may now return\
    \ the new\n   attributes after the operation has completed to optimize out a\n\
    \   subsequent GETATTR used in validating attribute caches. In\n   addition, operations\
    \ that modify the directory in which the\n   target object resides return the\
    \ old and new attributes of the\n   directory to allow clients to implement more\
    \ intelligent cache\n   invalidation procedures.  The ACCESS procedure provides\
    \ access\n   permission checking on the server, the FSSTAT procedure returns\n\
    \   dynamic information about a file system, the FSINFO procedure\n   returns\
    \ static information about a file system and server, the\n   READDIRPLUS procedure\
    \ returns file handles and attributes in\n   addition to directory entries, and\
    \ the PATHCONF procedure\n   returns POSIX pathconf information about a file.\n\
    \   Below is a list of the important changes between the NFS version\n   2 protocol\
    \ and the NFS version 3 protocol.\n   File handle size\n         The file handle\
    \ has been increased to a variable-length\n         array of 64 bytes maximum\
    \ from a fixed array of 32\n         bytes. This addresses some known requirements\
    \ for a\n         slightly larger file handle size. The file handle was\n    \
    \     converted from fixed length to variable length to\n         reduce local\
    \ storage and network bandwidth requirements\n         for systems which do not\
    \ utilize the full 64 bytes of\n         length.\n   Maximum data sizes\n    \
    \     The maximum size of a data transfer used in the READ\n         and WRITE\
    \ procedures is now set by values in the FSINFO\n         return structure. In\
    \ addition, preferred transfer sizes\n         are returned by FSINFO. The protocol\
    \ does not place any\n         artificial limits on the maximum transfer sizes.\n\
    \         Filenames and pathnames are now specified as strings of\n         variable\
    \ length. The actual length restrictions are\n         determined by the client\
    \ and server implementations as\n         appropriate.  The protocol does not\
    \ place any\n         artificial limits on the length. The error,\n         NFS3ERR_NAMETOOLONG,\
    \ is provided to allow the server to\n         return an indication to the client\
    \ that it received a\n         pathname that was too long for it to handle.\n\
    \   Error return\n         Error returns in some instances now return data (for\n\
    \         example, attributes). nfsstat3 now defines the full set\n         of\
    \ errors that can be returned by a server. No other\n         values are allowed.\n\
    \   File type\n         The file type now includes NF3CHR and NF3BLK for\n   \
    \      special files. Attributes for these types include\n         subfields for\
    \ UNIX major and minor devices numbers.\n         NF3SOCK and NF3FIFO are now\
    \ defined for sockets and\n         fifos in the file system.\n   File attributes\n\
    \         The blocksize (the size in bytes of a block in the\n         file) field\
    \ has been removed. The mode field no longer\n         contains file type information.\
    \ The size and fileid\n         fields have been widened to eight-byte unsigned\n\
    \         integers from four-byte integers. Major and minor\n         device information\
    \ is now presented in a distinct\n         structure.  The blocks field name has\
    \ been changed to\n         used and now contains the total number of bytes used\
    \ by\n         the file. It is also an eight-byte unsigned integer.\n   Set file\
    \ attributes\n         In the NFS version 2 protocol, the settable attributes\n\
    \         were represented by a subset of the file attributes\n         structure;\
    \ the client indicated those attributes which\n         were not to be modified\
    \ by setting the corresponding\n         field to -1, overloading some unsigned\
    \ fields. The set\n         file attributes structure now uses a discriminated\n\
    \         union for each field to tell whether or how to set that\n         field.\
    \ The atime and mtime fields can be set to either\n         the server's current\
    \ time or a time supplied by the\n         client.\n   LOOKUP\n         The LOOKUP\
    \ return structure now includes the attributes\n         for the directory searched.\n\
    \   ACCESS\n         An ACCESS procedure has been added to allow an explicit\n\
    \         over-the-wire permissions check. This addresses known\n         problems\
    \ with the superuser ID mapping feature in many\n         server implementations\
    \ (where, due to mapping of root\n         user, unexpected permission denied\
    \ errors could occur\n         while reading from or writing to a file).  This\
    \ also\n         removes the assumption which was made in the NFS\n         version\
    \ 2 protocol that access to files was based\n         solely on UNIX style mode\
    \ bits.\n   READ\n         The reply structure includes a Boolean that is TRUE\
    \ if\n         the end-of-file was encountered during the READ.  This\n      \
    \   allows the client to correctly detect end-of-file.\n   WRITE\n         The\
    \ beginoffset and totalcount fields were removed from\n         the WRITE arguments.\
    \ The reply now includes a count so\n         that the server can write less than\
    \ the requested\n         amount of data, if required. An indicator was added\
    \ to\n         the arguments to instruct the server as to the level of\n     \
    \    cache synchronization that is required by the client.\n   CREATE\n      \
    \   An exclusive flag and a create verifier was added for\n         the exclusive\
    \ creation of regular files.\n   MKNOD\n         This procedure was added to support\
    \ the creation of\n         special files. This avoids overloading fields of CREATE\n\
    \         as was done in some NFS version 2 protocol\n         implementations.\n\
    \   READDIR\n         The READDIR arguments now include a verifier to allow\n\
    \         the server to validate the cookie. The cookie is now a\n         64\
    \ bit unsigned integer instead of the 4 byte array\n         which was used in\
    \ the NFS version 2 protocol.  This\n         will help to reduce interoperability\
    \ problems.\n   READDIRPLUS\n         This procedure was added to return file\
    \ handles and\n         attributes in an extended directory list.\n   FSINFO\n\
    \         FSINFO was added to provide nonvolatile information\n         about\
    \ a file system. The reply includes preferred and\n         maximum read transfer\
    \ size, preferred and maximum write\n         transfer size, and flags stating\
    \ whether links or\n         symbolic links are supported.  Also returned are\n\
    \         preferred transfer size for READDIR procedure replies,\n         server\
    \ time granularity, and whether times can be set\n         in a SETATTR request.\n\
    \   FSSTAT\n         FSSTAT was added to provide volatile information about\n\
    \         a file system, for use by utilities such as the Unix\n         system\
    \ df command. The reply includes the total size\n         and free space in the\
    \ file system specified in bytes,\n         the total number of files and number\
    \ of free file slots\n         in the file system, and an estimate of time between\n\
    \         file system modifications (for use in cache consistency\n         checking\
    \ algorithms).\n   COMMIT\n         The COMMIT procedure provides the synchronization\n\
    \         mechanism to be used with asynchronous WRITE\n         operations.\n"
- title: 2. RPC Information
  contents:
  - '2. RPC Information

    '
- title: 2.1 Authentication
  contents:
  - "2.1 Authentication\n   The NFS service uses AUTH_NONE in the NULL procedure.\
    \ AUTH_UNIX,\n   AUTH_DES, or AUTH_KERB are used for all other procedures. Other\n\
    \   authentication types may be supported in the future.\n"
- title: 2.2 Constants
  contents:
  - "2.2 Constants\n   These are the RPC constants needed to call the NFS Version\
    \ 3\n   service.  They are given in decimal.\n      PROGRAM  100003\n      VERSION\
    \  3\n"
- title: 2.3 Transport address
  contents:
  - "2.3 Transport address\n   The NFS protocol is normally supported over the TCP\
    \ and UDP\n   protocols.  It uses port 2049, the same as the NFS version 2\n \
    \  protocol.\n"
- title: 2.4 Sizes
  contents:
  - "2.4 Sizes\n   These are the sizes, given in decimal bytes, of various XDR\n \
    \  structures used in the NFS version 3 protocol:\n   NFS3_FHSIZE 64\n      The\
    \ maximum size in bytes of the opaque file handle.\n   NFS3_COOKIEVERFSIZE 8\n\
    \      The size in bytes of the opaque cookie verifier passed by\n      READDIR\
    \ and READDIRPLUS.\n   NFS3_CREATEVERFSIZE 8\n      The size in bytes of the opaque\
    \ verifier used for\n      exclusive CREATE.\n   NFS3_WRITEVERFSIZE 8\n      The\
    \ size in bytes of the opaque verifier used for\n      asynchronous WRITE.\n"
- title: 2.5 Basic Data Types
  contents:
  - "2.5 Basic Data Types\n   The following XDR definitions are basic definitions\
    \ that are\n   used in other structures.\n   uint64\n         typedef unsigned\
    \ hyper uint64;\n   int64\n         typedef hyper int64;\n   uint32\n        \
    \ typedef unsigned long uint32;\n   int32\n         typedef long int32;\n   filename3\n\
    \         typedef string filename3<>;\n   nfspath3\n         typedef string nfspath3<>;\n\
    \   fileid3\n         typedef uint64 fileid3;\n   cookie3\n         typedef uint64\
    \ cookie3;\n   cookieverf3\n         typedef opaque cookieverf3[NFS3_COOKIEVERFSIZE];\n\
    \   createverf3\n         typedef opaque createverf3[NFS3_CREATEVERFSIZE];\n \
    \  writeverf3\n         typedef opaque writeverf3[NFS3_WRITEVERFSIZE];\n   uid3\n\
    \         typedef uint32 uid3;\n   gid3\n         typedef uint32 gid3;\n   size3\n\
    \         typedef uint64 size3;\n   offset3\n         typedef uint64 offset3;\n\
    \   mode3\n         typedef uint32 mode3;\n   count3\n         typedef uint32\
    \ count3;\n   nfsstat3\n      enum nfsstat3 {\n         NFS3_OK             =\
    \ 0,\n         NFS3ERR_PERM        = 1,\n         NFS3ERR_NOENT       = 2,\n \
    \        NFS3ERR_IO          = 5,\n         NFS3ERR_NXIO        = 6,\n       \
    \  NFS3ERR_ACCES       = 13,\n         NFS3ERR_EXIST       = 17,\n         NFS3ERR_XDEV\
    \        = 18,\n         NFS3ERR_NODEV       = 19,\n         NFS3ERR_NOTDIR  \
    \    = 20,\n         NFS3ERR_ISDIR       = 21,\n         NFS3ERR_INVAL       =\
    \ 22,\n         NFS3ERR_FBIG        = 27,\n         NFS3ERR_NOSPC       = 28,\n\
    \         NFS3ERR_ROFS        = 30,\n         NFS3ERR_MLINK       = 31,\n    \
    \     NFS3ERR_NAMETOOLONG = 63,\n         NFS3ERR_NOTEMPTY    = 66,\n        \
    \ NFS3ERR_DQUOT       = 69,\n         NFS3ERR_STALE       = 70,\n         NFS3ERR_REMOTE\
    \      = 71,\n         NFS3ERR_BADHANDLE   = 10001,\n         NFS3ERR_NOT_SYNC\
    \    = 10002,\n         NFS3ERR_BAD_COOKIE  = 10003,\n         NFS3ERR_NOTSUPP\
    \     = 10004,\n         NFS3ERR_TOOSMALL    = 10005,\n         NFS3ERR_SERVERFAULT\
    \ = 10006,\n         NFS3ERR_BADTYPE     = 10007,\n         NFS3ERR_JUKEBOX  \
    \   = 10008\n      };\n   The nfsstat3 type is returned with every procedure's\
    \ results\n   except for the NULL procedure. A value of NFS3_OK indicates that\n\
    \   the call completed successfully. Any other value indicates that\n   some error\
    \ occurred on the call, as identified by the error\n   code. Note that the precise\
    \ numeric encoding must be followed.\n   No other values may be returned by a\
    \ server. Servers are\n   expected to make a best effort mapping of error conditions\
    \ to\n   the set of error codes defined. In addition, no error\n   precedences\
    \ are specified by this specification.  Error\n   precedences determine the error\
    \ value that should be returned\n   when more than one error applies in a given\
    \ situation. The error\n   precedence will be determined by the individual server\n\
    \   implementation. If the client requires specific error\n   precedences, it\
    \ should check for the specific errors for\n   itself.\n"
- title: 2.6 Defined Error Numbers
  contents:
  - "2.6 Defined Error Numbers\n   A description of each defined error follows:\n\
    \   NFS3_OK\n       Indicates the call completed successfully.\n   NFS3ERR_PERM\n\
    \       Not owner. The operation was not allowed because the\n       caller is\
    \ either not a privileged user (root) or not the\n       owner of the target of\
    \ the operation.\n   NFS3ERR_NOENT\n       No such file or directory. The file\
    \ or directory name\n       specified does not exist.\n   NFS3ERR_IO\n       I/O\
    \ error. A hard error (for example, a disk error)\n       occurred while processing\
    \ the requested operation.\n   NFS3ERR_NXIO\n       I/O error. No such device\
    \ or address.\n   NFS3ERR_ACCES\n       Permission denied. The caller does not\
    \ have the correct\n       permission to perform the requested operation. Contrast\n\
    \       this with NFS3ERR_PERM, which restricts itself to owner\n       or privileged\
    \ user permission failures.\n   NFS3ERR_EXIST\n       File exists. The file specified\
    \ already exists.\n   NFS3ERR_XDEV\n       Attempt to do a cross-device hard link.\n\
    \   NFS3ERR_NODEV\n       No such device.\n   NFS3ERR_NOTDIR\n       Not a directory.\
    \ The caller specified a non-directory in\n       a directory operation.\n   NFS3ERR_ISDIR\n\
    \       Is a directory. The caller specified a directory in a\n       non-directory\
    \ operation.\n   NFS3ERR_INVAL\n       Invalid argument or unsupported argument\
    \ for an\n       operation. Two examples are attempting a READLINK on an\n   \
    \    object other than a symbolic link or attempting to\n       SETATTR a time\
    \ field on a server that does not support\n       this operation.\n   NFS3ERR_FBIG\n\
    \       File too large. The operation would have caused a file to\n       grow\
    \ beyond the server's limit.\n   NFS3ERR_NOSPC\n       No space left on device.\
    \ The operation would have caused\n       the server's file system to exceed its\
    \ limit.\n   NFS3ERR_ROFS\n       Read-only file system. A modifying operation\
    \ was\n       attempted on a read-only file system.\n   NFS3ERR_MLINK\n      \
    \ Too many hard links.\n   NFS3ERR_NAMETOOLONG\n       The filename in an operation\
    \ was too long.\n   NFS3ERR_NOTEMPTY\n       An attempt was made to remove a directory\
    \ that was not\n       empty.\n   NFS3ERR_DQUOT\n       Resource (quota) hard\
    \ limit exceeded. The user's resource\n       limit on the server has been exceeded.\n\
    \   NFS3ERR_STALE\n       Invalid file handle. The file handle given in the\n\
    \       arguments was invalid. The file referred to by that file\n       handle\
    \ no longer exists or access to it has been\n       revoked.\n   NFS3ERR_REMOTE\n\
    \       Too many levels of remote in path. The file handle given\n       in the\
    \ arguments referred to a file on a non-local file\n       system on the server.\n\
    \   NFS3ERR_BADHANDLE\n       Illegal NFS file handle. The file handle failed\
    \ internal\n       consistency checks.\n   NFS3ERR_NOT_SYNC\n       Update synchronization\
    \ mismatch was detected during a\n       SETATTR operation.\n   NFS3ERR_BAD_COOKIE\n\
    \       READDIR or READDIRPLUS cookie is stale.\n   NFS3ERR_NOTSUPP\n       Operation\
    \ is not supported.\n   NFS3ERR_TOOSMALL\n       Buffer or request is too small.\n\
    \   NFS3ERR_SERVERFAULT\n       An error occurred on the server which does not\
    \ map to any\n       of the legal NFS version 3 protocol error values.  The\n\
    \       client should translate this into an appropriate error.\n       UNIX clients\
    \ may choose to translate this to EIO.\n   NFS3ERR_BADTYPE\n       An attempt\
    \ was made to create an object of a type not\n       supported by the server.\n\
    \   NFS3ERR_JUKEBOX\n       The server initiated the request, but was not able\
    \ to\n       complete it in a timely fashion. The client should wait\n       and\
    \ then try the request with a new RPC transaction ID.\n       For example, this\
    \ error should be returned from a server\n       that supports hierarchical storage\
    \ and receives a request\n       to process a file that has been migrated. In\
    \ this case,\n       the server should start the immigration process and\n   \
    \    respond to client with this error.\n   ftype3\n      enum ftype3 {\n    \
    \     NF3REG    = 1,\n         NF3DIR    = 2,\n         NF3BLK    = 3,\n     \
    \    NF3CHR    = 4,\n         NF3LNK    = 5,\n         NF3SOCK   = 6,\n      \
    \   NF3FIFO   = 7\n      };\n   The enumeration, ftype3, gives the type of a file.\
    \ The type,\n   NF3REG, is a regular file, NF3DIR is a directory, NF3BLK is a\n\
    \   block special device file, NF3CHR is a character special device\n   file,\
    \ NF3LNK is a symbolic link, NF3SOCK is a socket, and\n   NF3FIFO is a named pipe.\
    \ Note that the precise enum encoding\n   must be followed.\n   specdata3\n  \
    \    struct specdata3 {\n           uint32     specdata1;\n           uint32 \
    \    specdata2;\n      };\n   The interpretation of the two words depends on the\
    \ type of file\n   system object. For a block special (NF3BLK) or character special\n\
    \   (NF3CHR) file, specdata1 and specdata2 are the major and minor\n   device\
    \ numbers, respectively.  (This is obviously a\n   UNIX-specific interpretation.)\
    \ For all other file types, these\n   two elements should either be set to 0 or\
    \ the values should be\n   agreed upon by the client and server. If the client\
    \ and server\n   do not agree upon the values, the client should treat these\n\
    \   fields as if they are set to 0. This data field is returned as\n   part of\
    \ the fattr3 structure and so is available from all\n   replies returning attributes.\
    \ Since these fields are otherwise\n   unused for objects which are not devices,\
    \ out of band\n   information can be passed from the server to the client.\n \
    \  However, once again, both the server and the client must agree\n   on the values\
    \ passed.\n   nfs_fh3\n      struct nfs_fh3 {\n         opaque       data<NFS3_FHSIZE>;\n\
    \      };\n   The nfs_fh3 is the variable-length opaque object returned by the\n\
    \   server on LOOKUP, CREATE, SYMLINK, MKNOD, LINK, or READDIRPLUS\n   operations,\
    \ which is used by the client on subsequent operations\n   to reference the file.\
    \ The file handle contains all the\n   information the server needs to distinguish\
    \ an individual file.\n   To the client, the file handle is opaque. The client\
    \ stores file\n   handles for use in a later request and can compare two file\n\
    \   handles from the same server for equality by doing a\n   byte-by-byte comparison,\
    \ but cannot otherwise interpret the\n   contents of file handles. If two file\
    \ handles from the same\n   server are equal, they must refer to the same file,\
    \ but if they\n   are not equal, no conclusions can be drawn. Servers should try\n\
    \   to maintain a one-to-one correspondence between file handles and\n   files,\
    \ but this is not required. Clients should use file handle\n   comparisons only\
    \ to improve performance, not for correct\n   behavior.\n   Servers can revoke\
    \ the access provided by a file handle at any\n   time.  If the file handle passed\
    \ in a call refers to a file\n   system object that no longer exists on the server\
    \ or access for\n   that file handle has been revoked, the error, NFS3ERR_STALE,\n\
    \   should be returned.\n   nfstime3\n      struct nfstime3 {\n         uint32\
    \   seconds;\n         uint32   nseconds;\n      };\n   The nfstime3 structure\
    \ gives the number of seconds and\n   nanoseconds since midnight January 1, 1970\
    \ Greenwich Mean Time.\n   It is used to pass time and date information. The times\n\
    \   associated with files are all server times except in the case of\n   a SETATTR\
    \ operation where the client can explicitly set the file\n   time. A server converts\
    \ to and from local time when processing\n   time values, preserving as much accuracy\
    \ as possible. If the\n   precision of timestamps stored for a file is less than\
    \ that\n   defined by NFS version 3 protocol, loss of precision can occur.\n \
    \  An adjunct time maintenance protocol is recommended to reduce\n   client and\
    \ server time skew.\n   fattr3\n      struct fattr3 {\n         ftype3     type;\n\
    \         mode3      mode;\n         uint32     nlink;\n         uid3       uid;\n\
    \         gid3       gid;\n         size3      size;\n         size3      used;\n\
    \         specdata3  rdev;\n         uint64     fsid;\n         fileid3    fileid;\n\
    \         nfstime3   atime;\n         nfstime3   mtime;\n         nfstime3   ctime;\n\
    \      };\n   This structure defines the attributes of a file system object.\n\
    \   It is returned by most operations on an object; in the case of\n   operations\
    \ that affect two objects (for example, a MKDIR that\n   modifies the target directory\
    \ attributes and defines new\n   attributes for the newly created directory),\
    \ the attributes for\n   both may be returned. In some cases, the attributes are\
    \ returned\n   in the structure, wcc_data, which is defined below; in other\n\
    \   cases the attributes are returned alone.  The main changes from\n   the NFS\
    \ version 2 protocol are that many of the fields have been\n   widened and the\
    \ major/minor device information is now presented\n   in a distinct structure\
    \ rather than being packed into a word.\n   The fattr3 structure contains the\
    \ basic attributes of a file.\n   All servers should support this set of attributes\
    \ even if they\n   have to simulate some of the fields. Type is the type of the\n\
    \   file. Mode is the protection mode bits. Nlink is the number of\n   hard links\
    \ to the file - that is, the number of different names\n   for the same file.\
    \ Uid is the user ID of the owner of the file.\n   Gid is the group ID of the\
    \ group of the file. Size is the size\n   of the file in bytes. Used is the number\
    \ of bytes of disk space\n   that the file actually uses (which can be smaller\
    \ than the size\n   because the file may have holes or it may be larger due to\n\
    \   fragmentation). Rdev describes the device file if the file type\n   is NF3CHR\
    \ or NF3BLK - see specdata3 on page 20. Fsid is the file\n   system identifier\
    \ for the file system. Fileid is a number which\n   uniquely identifies the file\
    \ within its file system (on UNIX\n   this would be the inumber). Atime is the\
    \ time when the file data\n   was last accessed. Mtime is the time when the file\
    \ data was last\n   modified.  Ctime is the time when the attributes of the file\n\
    \   were last changed.  Writing to the file changes the ctime in\n   addition\
    \ to the mtime.\n   The mode bits are defined as follows:\n      0x00800 Set user\
    \ ID on execution.\n      0x00400 Set group ID on execution.\n      0x00200 Save\
    \ swapped text (not defined in POSIX).\n      0x00100 Read permission for owner.\n\
    \      0x00080 Write permission for owner.\n      0x00040 Execute permission for\
    \ owner on a file. Or lookup\n              (search) permission for owner in directory.\n\
    \      0x00020 Read permission for group.\n      0x00010 Write permission for\
    \ group.\n      0x00008 Execute permission for group on a file. Or lookup\n  \
    \            (search) permission for group in directory.\n      0x00004 Read permission\
    \ for others.\n      0x00002 Write permission for others.\n      0x00001 Execute\
    \ permission for others on a file. Or lookup\n              (search) permission\
    \ for others in directory.\n   post_op_attr\n      union post_op_attr switch (bool\
    \ attributes_follow) {\n      case TRUE:\n         fattr3   attributes;\n    \
    \  case FALSE:\n         void;\n      };\n   This structure is used for returning\
    \ attributes in those\n   operations that are not directly involved with manipulating\n\
    \   attributes. One of the principles of this revision of the NFS\n   protocol\
    \ is to return the real value from the indicated\n   operation and not an error\
    \ from an incidental operation. The\n   post_op_attr structure was designed to\
    \ allow the server to\n   recover from errors encountered while getting attributes.\n\
    \   This appears to make returning attributes optional. However,\n   server implementors\
    \ are strongly encouraged to make best effort\n   to return attributes whenever\
    \ possible, even when returning an\n   error.\n   wcc_attr\n      struct wcc_attr\
    \ {\n         size3       size;\n         nfstime3    mtime;\n         nfstime3\
    \    ctime;\n      };\n   This is the subset of pre-operation attributes needed\
    \ to better\n   support the weak cache consistency semantics. Size is the file\n\
    \   size in bytes of the object before the operation. Mtime is the\n   time of\
    \ last modification of the object before the operation.\n   Ctime is the time\
    \ of last change to the attributes of the object\n   before the operation. See\
    \ discussion in wcc_attr on page 24.\n   The use of mtime by clients to detect\
    \ changes to file system\n   objects residing on a server is dependent on the\
    \ granularity of\n   the time base on the server.\n   pre_op_attr\n      union\
    \ pre_op_attr switch (bool attributes_follow) {\n      case TRUE:\n          \
    \ wcc_attr  attributes;\n      case FALSE:\n           void;\n      };\n   wcc_data\n\
    \      struct wcc_data {\n         pre_op_attr    before;\n         post_op_attr\
    \   after;\n      };\n   When a client performs an operation that modifies the\
    \ state of a\n   file or directory on the server, it cannot immediately determine\n\
    \   from the post-operation attributes whether the operation just\n   performed\
    \ was the only operation on the object since the last\n   time the client received\
    \ the attributes for the object. This is\n   important, since if an intervening\
    \ operation has changed the\n   object, the client will need to invalidate any\
    \ cached data for\n   the object (except for the data that it just wrote).\n \
    \  To deal with this, the notion of weak cache consistency data or\n   wcc_data\
    \ is introduced. A wcc_data structure consists of certain\n   key fields from\
    \ the object attributes before the operation,\n   together with the object attributes\
    \ after the operation. This\n   information allows the client to manage its cache\
    \ more\n   accurately than in NFS version 2 protocol implementations. The\n  \
    \ term, weak cache consistency, emphasizes the fact that this\n   mechanism does\
    \ not provide the strict server-client consistency\n   that a cache consistency\
    \ protocol would provide.\n   In order to support the weak cache consistency model,\
    \ the server\n   will need to be able to get the pre-operation attributes of the\n\
    \   object, perform the intended modify operation, and then get the\n   post-operation\
    \ attributes atomically. If there is a window for\n   the object to get modified\
    \ between the operation and either of\n   the get attributes operations, then\
    \ the client will not be able\n   to determine whether it was the only entity\
    \ to modify the\n   object. Some information will have been lost, thus weakening\
    \ the\n   weak cache consistency guarantees.\n   post_op_fh3\n      union post_op_fh3\
    \ switch (bool handle_follows) {\n      case TRUE:\n           nfs_fh3  handle;\n\
    \      case FALSE:\n           void;\n      };\n   One of the principles of this\
    \ revision of the NFS protocol is to\n   return the real value from the indicated\
    \ operation and not an\n   error from an incidental operation. The post_op_fh3\
    \ structure\n   was designed to allow the server to recover from errors\n   encountered\
    \ while constructing a file handle.\n   This is the structure used to return a\
    \ file handle from the\n   CREATE, MKDIR, SYMLINK, MKNOD, and READDIRPLUS requests.\
    \ In each\n   case, the client can get the file handle by issuing a LOOKUP\n \
    \  request after a successful return from one of the listed\n   operations. Returning\
    \ the file handle is an optimization so that\n   the client is not forced to immediately\
    \ issue a LOOKUP request\n   to get the file handle.\n   sattr3\n      enum time_how\
    \ {\n         DONT_CHANGE        = 0,\n         SET_TO_SERVER_TIME = 1,\n    \
    \     SET_TO_CLIENT_TIME = 2\n      };\n      union set_mode3 switch (bool set_it)\
    \ {\n      case TRUE:\n         mode3    mode;\n      default:\n         void;\n\
    \      };\n      union set_uid3 switch (bool set_it) {\n      case TRUE:\n   \
    \      uid3     uid;\n      default:\n         void;\n      };\n      union set_gid3\
    \ switch (bool set_it) {\n      case TRUE:\n         gid3     gid;\n      default:\n\
    \         void;\n      };\n      union set_size3 switch (bool set_it) {\n    \
    \  case TRUE:\n         size3    size;\n      default:\n         void;\n     \
    \ };\n      union set_atime switch (time_how set_it) {\n      case SET_TO_CLIENT_TIME:\n\
    \         nfstime3  atime;\n      default:\n         void;\n      };\n      union\
    \ set_mtime switch (time_how set_it) {\n      case SET_TO_CLIENT_TIME:\n     \
    \    nfstime3  mtime;\n      default:\n         void;\n      };\n      struct\
    \ sattr3 {\n         set_mode3   mode;\n         set_uid3    uid;\n         set_gid3\
    \    gid;\n         set_size3   size;\n         set_atime   atime;\n         set_mtime\
    \   mtime;\n      };\n   The sattr3 structure contains the file attributes that\
    \ can be\n   set from the client. The fields are the same as the similarly\n \
    \  named fields in the fattr3 structure. In the NFS version 3\n   protocol, the\
    \ settable attributes are described by a structure\n   containing a set of discriminated\
    \ unions. Each union indicates\n   whether the corresponding attribute is to be\
    \ updated, and if so,\n   how.\n   There are two forms of discriminated unions\
    \ used. In setting the\n   mode, uid, gid, or size, the discriminated union is\
    \ switched on\n   a boolean, set_it; if it is TRUE, a value of the appropriate\n\
    \   type is then encoded.\n   In setting the atime or mtime, the union is switched\
    \ on an\n   enumeration type, set_it. If set_it has the value DONT_CHANGE,\n \
    \  the corresponding attribute is unchanged. If it has the value,\n   SET_TO_SERVER_TIME,\
    \ the corresponding attribute is set by the\n   server to its local time; no data\
    \ is provided by the client.\n   Finally, if set_it has the value, SET_TO_CLIENT_TIME,\
    \ the\n   attribute is set to the time passed by the client in an nfstime3\n \
    \  structure. (See FSINFO on page 86, which addresses the issue of\n   time granularity).\n\
    \   diropargs3\n      struct diropargs3 {\n         nfs_fh3     dir;\n       \
    \  filename3   name;\n      };\n   The diropargs3 structure is used in directory\
    \ operations. The\n   file handle, dir, identifies the directory in which to\n\
    \   manipulate or access the file, name. See additional comments in\n   File name\
    \ component handling on page 101.\n"
- title: 3. Server Procedures
  contents:
  - "3. Server Procedures\n   The following sections define the RPC procedures that\
    \ are\n   supplied by an NFS version 3 protocol server. The RPC\n   procedure\
    \ number is given at the top of the page with the\n   name. The SYNOPSIS provides\
    \ the name of the procedure, the\n   list of the names of the arguments, the list\
    \ of the names of\n   the results, followed by the XDR argument declarations and\n\
    \   results declarations. The information in the SYNOPSIS is\n   specified in\
    \ RPC Data Description Language as defined in\n   [RFC1014]. The DESCRIPTION section\
    \ tells what the procedure\n   is expected to do and how its arguments and results\
    \ are used.\n   The ERRORS section lists the errors returned for specific\n  \
    \ types of failures. These lists are not intended to be the\n   definitive statement\
    \ of all of the errors which can be\n   returned by any specific procedure, but\
    \ as a guide for the\n   more common errors which may be returned.  Client\n \
    \  implementations should be prepared to deal with unexpected\n   errors coming\
    \ from a server. The IMPLEMENTATION field gives\n   information about how the\
    \ procedure is expected to work and\n   how it should be used by clients.\n  \
    \    program NFS_PROGRAM {\n         version NFS_V3 {\n            void\n    \
    \         NFSPROC3_NULL(void)                    = 0;\n            GETATTR3res\n\
    \             NFSPROC3_GETATTR(GETATTR3args)         = 1;\n            SETATTR3res\n\
    \             NFSPROC3_SETATTR(SETATTR3args)         = 2;\n            LOOKUP3res\n\
    \             NFSPROC3_LOOKUP(LOOKUP3args)           = 3;\n            ACCESS3res\n\
    \             NFSPROC3_ACCESS(ACCESS3args)           = 4;\n            READLINK3res\n\
    \             NFSPROC3_READLINK(READLINK3args)       = 5;\n            READ3res\n\
    \             NFSPROC3_READ(READ3args)               = 6;\n            WRITE3res\n\
    \             NFSPROC3_WRITE(WRITE3args)             = 7;\n            CREATE3res\n\
    \             NFSPROC3_CREATE(CREATE3args)           = 8;\n            MKDIR3res\n\
    \             NFSPROC3_MKDIR(MKDIR3args)             = 9;\n            SYMLINK3res\n\
    \             NFSPROC3_SYMLINK(SYMLINK3args)         = 10;\n            MKNOD3res\n\
    \             NFSPROC3_MKNOD(MKNOD3args)             = 11;\n            REMOVE3res\n\
    \             NFSPROC3_REMOVE(REMOVE3args)           = 12;\n            RMDIR3res\n\
    \             NFSPROC3_RMDIR(RMDIR3args)             = 13;\n            RENAME3res\n\
    \             NFSPROC3_RENAME(RENAME3args)           = 14;\n            LINK3res\n\
    \             NFSPROC3_LINK(LINK3args)               = 15;\n            READDIR3res\n\
    \             NFSPROC3_READDIR(READDIR3args)         = 16;\n            READDIRPLUS3res\n\
    \             NFSPROC3_READDIRPLUS(READDIRPLUS3args) = 17;\n            FSSTAT3res\n\
    \             NFSPROC3_FSSTAT(FSSTAT3args)           = 18;\n            FSINFO3res\n\
    \             NFSPROC3_FSINFO(FSINFO3args)           = 19;\n            PATHCONF3res\n\
    \             NFSPROC3_PATHCONF(PATHCONF3args)       = 20;\n            COMMIT3res\n\
    \             NFSPROC3_COMMIT(COMMIT3args)           = 21;\n         } = 3;\n\
    \      } = 100003;\n   Out of range (undefined) procedure numbers result in RPC\n\
    \   errors.  Refer to [RFC1057] for more detail.\n"
- title: 3.1 General comments on attributes and consistency data on failure
  contents:
  - "3.1 General comments on attributes and consistency data on failure\n   For those\
    \ procedures that return either post_op_attr or wcc_data\n   structures on failure,\
    \ the discriminated union may contain the\n   pre-operation attributes of the\
    \ object or object parent\n   directory.  This depends on the error encountered\
    \ and may also\n   depend on the particular server implementation. Implementors\
    \ are\n   strongly encouraged to return as much attribute data as possible\n \
    \  upon failure, but client implementors need to be aware that\n   their implementation\
    \ must correctly handle the variant return\n   instance where no attributes or\
    \ consistency data is returned.\n"
- title: 3.2 General comments on filenames
  contents:
  - "3.2 General comments on filenames\n   The following comments apply to all NFS\
    \ version 3 protocol\n   procedures in which the client provides one or more filenames\
    \ in\n   the arguments: LOOKUP, CREATE, MKDIR, SYMLINK, MKNOD, REMOVE,\n   RMDIR,\
    \ RENAME, and LINK.\n   1. The filename must not be null nor may it be the null\n\
    \      string.  The server should return the error, NFS3ERR_ACCES, if\n      it\
    \ receives such a filename. On some clients, the filename, ``''\n      or a null\
    \ string, is assumed to be an alias for the current\n      directory. Clients\
    \ which require this functionality should\n      implement it for themselves and\
    \ not depend upon the server to\n      support such semantics.\n   2. A filename\
    \ having the value of \".\" is assumed to be an\n      alias for the current directory.\
    \ Clients which require this\n      functionality should implement it for themselves\
    \ and not depend\n      upon the server to support such semantics. However, the\
    \ server\n      should be able to handle such a filename correctly.\n   3. A filename\
    \ having the value of \"..\" is assumed to be an\n      alias for the parent of\
    \ the current directory, i.e. the\n      directory which contains the current\
    \ directory. The server\n      should be prepared to handle this semantic, if\
    \ it supports\n      directories, even if those directories do not contain UNIX-style\n\
    \      \".\" or \"..\" entries.\n   4. If the filename is longer than the maximum\
    \ for the file\n      system (see PATHCONF on page 90, specifically name_max),\
    \ the\n      result depends on the value of the PATHCONF flag, no_trunc. If\n\
    \      no_trunc is FALSE, the filename will be silently truncated to\n      name_max\
    \ bytes. If no_trunc is TRUE and the filename exceeds the\n      server's file\
    \ system maximum filename length, the operation will\n      fail with the error,\
    \ NFS3ERR_NAMETOOLONG.\n   5. In general, there will be characters that a server\
    \ will\n      not be able to handle as part of a filename. This set of\n     \
    \ characters will vary from server to server and from\n      implementation to\
    \ implementation.  In most cases, it is the\n      server which will control the\
    \ client's view of the file system.\n      If the server receives a filename containing\
    \ characters that it\n      can not handle, the error, NFS3ERR_EACCES, should\
    \ be returned.\n      Client implementations should be prepared to handle this\
    \ side\n      affect of heterogeneity.\n   See also comments in File name component\
    \ handling on page 101.\n"
- title: '3.3.0 Procedure 0: NULL - Do nothing'
  contents:
  - "3.3.0 Procedure 0: NULL - Do nothing\n   SYNOPSIS\n      void NFSPROC3_NULL(void)\
    \ = 0;\n   DESCRIPTION\n      Procedure NULL does not do any work. It is made\
    \ available to\n      allow server response testing and timing.\n   IMPLEMENTATION\n\
    \      It is important that this procedure do no work at all so\n      that it\
    \ can be used to measure the overhead of processing\n      a service request.\
    \ By convention, the NULL procedure\n      should never require any authentication.\
    \ A server may\n      choose to ignore this convention, in a more secure\n   \
    \   implementation, where responding to the NULL procedure\n      call acknowledges\
    \ the existence of a resource to an\n      unauthenticated client.\n   ERRORS\n\
    \      Since the NULL procedure takes no NFS version 3 protocol\n      arguments\
    \ and returns no NFS version 3 protocol response,\n      it can not return an\
    \ NFS version 3 protocol error.\n      However, it is possible that some server\
    \ implementations\n      may return RPC errors based on security and authentication\n\
    \      requirements.\n"
- title: '3.3.1 Procedure 1: GETATTR - Get file attributes'
  contents:
  - "3.3.1 Procedure 1: GETATTR - Get file attributes\n   SYNOPSIS\n      GETATTR3res\
    \ NFSPROC3_GETATTR(GETATTR3args) = 1;\n      struct GETATTR3args {\n         nfs_fh3\
    \  object;\n      };\n      struct GETATTR3resok {\n         fattr3   obj_attributes;\n\
    \      };\n      union GETATTR3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \         GETATTR3resok  resok;\n      default:\n         void;\n      };\n  \
    \ DESCRIPTION\n      Procedure GETATTR retrieves the attributes for a specified\n\
    \      file system object. The object is identified by the file\n      handle\
    \ that the server returned as part of the response\n      from a LOOKUP, CREATE,\
    \ MKDIR, SYMLINK, MKNOD, or\n      READDIRPLUS procedure (or from the MOUNT service,\n\
    \      described elsewhere). On entry, the arguments in\n      GETATTR3args are:\n\
    \      object\n         The file handle of an object whose attributes are to be\n\
    \         retrieved.\n      On successful return, GETATTR3res.status is NFS3_OK\
    \ and\n      GETATTR3res.resok contains:\n      obj_attributes\n         The attributes\
    \ for the object.\n      Otherwise, GETATTR3res.status contains the error on failure\
    \ and\n      no other results are returned.\n   IMPLEMENTATION\n      The attributes\
    \ of file system objects is a point of major\n      disagreement between different\
    \ operating systems. Servers\n      should make a best attempt to support all\
    \ of the\n      attributes in the fattr3 structure so that clients can\n     \
    \ count on this as a common ground. Some mapping may be\n      required to map\
    \ local attributes to those in the fattr3\n      structure.\n      Today, most\
    \ client NFS version 3 protocol implementations\n      implement a time-bounded\
    \ attribute caching scheme to\n      reduce over-the-wire attribute checks.\n\
    \   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n \
    \     NFS3ERR_SERVERFAULT\n   SEE ALSO\n      ACCESS.\n"
- title: '3.3.2 Procedure 2: SETATTR - Set file attributes'
  contents:
  - "3.3.2 Procedure 2: SETATTR - Set file attributes\n   SYNOPSIS\n      SETATTR3res\
    \ NFSPROC3_SETATTR(SETATTR3args) = 2;\n      union sattrguard3 switch (bool check)\
    \ {\n      case TRUE:\n         nfstime3  obj_ctime;\n      case FALSE:\n    \
    \     void;\n      };\n      struct SETATTR3args {\n         nfs_fh3      object;\n\
    \         sattr3       new_attributes;\n         sattrguard3  guard;\n      };\n\
    \      struct SETATTR3resok {\n         wcc_data  obj_wcc;\n      };\n      struct\
    \ SETATTR3resfail {\n         wcc_data  obj_wcc;\n      };\n      union SETATTR3res\
    \ switch (nfsstat3 status) {\n      case NFS3_OK:\n         SETATTR3resok   resok;\n\
    \      default:\n         SETATTR3resfail resfail;\n      };\n   DESCRIPTION\n\
    \      Procedure SETATTR changes one or more of the attributes of\n      a file\
    \ system object on the server. The new attributes are\n      specified by a sattr3\
    \ structure. On entry, the arguments\n      in SETATTR3args are:\n      object\n\
    \         The file handle for the object.\n      new_attributes\n         A sattr3\
    \ structure containing booleans and\n         enumerations describing the attributes\
    \ to be set and the new\n         values for those attributes.\n      guard\n\
    \         A sattrguard3 union:\n         check\n            TRUE if the server\
    \ is to verify that guard.obj_ctime\n            matches the ctime for the object;\
    \ FALSE otherwise.\n      A client may request that the server check that the\
    \ object\n      is in an expected state before performing the SETATTR\n      operation.\
    \ To do this, it sets the argument guard.check to\n      TRUE and the client passes\
    \ a time value in guard.obj_ctime.\n      If guard.check is TRUE, the server must\
    \ compare the value of\n      guard.obj_ctime to the current ctime of the object.\
    \ If the\n      values are different, the server must preserve the object\n  \
    \    attributes and must return a status of NFS3ERR_NOT_SYNC.\n      If guard.check\
    \ is FALSE, the server will not perform this\n      check.\n      On successful\
    \ return, SETATTR3res.status is NFS3_OK and\n      SETATTR3res.resok contains:\n\
    \         obj_wcc\n            A wcc_data structure containing the old and new\n\
    \            attributes for the object.\n      Otherwise, SETATTR3res.status contains\
    \ the error on\n      failure and SETATTR3res.resfail contains the following:\n\
    \         obj_wcc\n            A wcc_data structure containing the old and new\n\
    \            attributes for the object.\n   IMPLEMENTATION\n      The guard.check\
    \ mechanism allows the client to avoid\n      changing the attributes of an object\
    \ on the basis of stale\n      attributes. It does not guarantee exactly-once\
    \ semantics.\n      In particular, if a reply is lost and the server does not\n\
    \      detect the retransmission of the request, the procedure\n      can fail\
    \ with the error, NFS3ERR_NOT_SYNC, even though the\n      attribute setting was\
    \ previously performed successfully.\n      The client can attempt to recover\
    \ from this error by\n      getting fresh attributes from the server and sending\
    \ a new\n      SETATTR request using the new ctime.  The client can\n      optionally\
    \ check the attributes to avoid the second\n      SETATTR request if the new attributes\
    \ show that the\n      attributes have already been set as desired (though it\
    \ may\n      not have been the issuing client that set the\n      attributes).\n\
    \      The new_attributes.size field is used to request changes\n      to the\
    \ size of a file. A value of 0 causes the file to be\n      truncated, a value\
    \ less than the current size of the file\n      causes data from new size to the\
    \ end of the file to be\n      discarded, and a size greater than the current\
    \ size of the\n      file causes logically zeroed data bytes to be added to the\n\
    \      end of the file.  Servers are free to implement this using\n      holes\
    \ or actual zero data bytes. Clients should not make\n      any assumptions regarding\
    \ a server's implementation of\n      this feature, beyond that the bytes returned\
    \ will be\n      zeroed. Servers must support extending the file size via\n  \
    \    SETATTR.\n      SETATTR is not guaranteed atomic. A failed SETATTR may\n\
    \      partially change a file's attributes.\n      Changing the size of a file\
    \ with SETATTR indirectly\n      changes the mtime. A client must account for\
    \ this as size\n      changes can result in data deletion.\n      If server and\
    \ client times differ, programs that compare\n      client time to file times\
    \ can break. A time maintenance\n      protocol should be used to limit client/server\
    \ time skew.\n      In a heterogeneous environment, it is quite possible that\n\
    \      the server will not be able to support the full range of\n      SETATTR\
    \ requests. The error, NFS3ERR_INVAL, may be\n      returned if the server can\
    \ not store a uid or gid in its\n      own representation of uids or gids, respectively.\
    \  If the\n      server can only support 32 bit offsets and sizes, a\n      SETATTR\
    \ request to set the size of a file to larger than\n      can be represented in\
    \ 32 bits will be rejected with this\n      same error.\n   ERRORS\n      NFS3ERR_PERM\n\
    \      NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_INVAL\n      NFS3ERR_NOSPC\n\
    \      NFS3ERR_ROFS\n      NFS3ERR_DQUOT\n      NFS3ERR_NOT_SYNC\n      NFS3ERR_STALE\n\
    \      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      CREATE,\
    \ MKDIR, SYMLINK, and MKNOD.\n"
- title: '3.3.3 Procedure 3: LOOKUP -  Lookup filename'
  contents:
  - "3.3.3 Procedure 3: LOOKUP -  Lookup filename\n   SYNOPSIS\n      LOOKUP3res NFSPROC3_LOOKUP(LOOKUP3args)\
    \ = 3;\n      struct LOOKUP3args {\n           diropargs3  what;\n      };\n \
    \     struct LOOKUP3resok {\n           nfs_fh3      object;\n           post_op_attr\
    \ obj_attributes;\n           post_op_attr dir_attributes;\n      };\n      struct\
    \ LOOKUP3resfail {\n           post_op_attr dir_attributes;\n      };\n      union\
    \ LOOKUP3res switch (nfsstat3 status) {\n      case NFS3_OK:\n           LOOKUP3resok\
    \    resok;\n      default:\n           LOOKUP3resfail  resfail;\n      };\n \
    \  DESCRIPTION\n      Procedure LOOKUP searches a directory for a specific name\n\
    \      and returns the file handle for the corresponding file\n      system object.\
    \ On entry, the arguments in LOOKUP3args\n      are:\n      what\n         Object\
    \ to look up:\n         dir\n            The file handle for the directory to\
    \ search.\n         name\n            The filename to be searched for. Refer to\
    \ General\n            comments on filenames on page 30.\n      On successful\
    \ return, LOOKUP3res.status is NFS3_OK and\n      LOOKUP3res.resok contains:\n\
    \      object\n         The file handle of the object corresponding to\n     \
    \    what.name.\n      obj_attributes\n         The attributes of the object corresponding\
    \ to\n         what.name.\n      dir_attributes\n         The post-operation attributes\
    \ of the directory,\n         what.dir.\n      Otherwise, LOOKUP3res.status contains\
    \ the error on failure and\n      LOOKUP3res.resfail contains the following:\n\
    \      dir_attributes\n         The post-operation attributes for the directory,\n\
    \         what.dir.\n   IMPLEMENTATION\n      At first glance, in the case where\
    \ what.name refers to a\n      mount point on the server, two different replies\
    \ seem\n      possible. The server can return either the file handle for\n   \
    \   the underlying directory that is mounted on or the file\n      handle of the\
    \ root of the mounted directory.  This\n      ambiguity is simply resolved. A\
    \ server will not allow a\n      LOOKUP operation to cross a mountpoint to the\
    \ root of a\n      different filesystem, even if the filesystem is exported.\n\
    \      This does not prevent a client from accessing a hierarchy\n      of filesystems\
    \ exported by a server, but the client must\n      mount each of the filesystems\
    \ individually so that the\n      mountpoint crossing takes place on the client.\
    \  A given\n      server implementation may refine these rules given\n      capabilities\
    \ or limitations particular to that\n      implementation. Refer to [X/OpenNFS]\
    \ for a discussion on\n      exporting file systems.\n      Two filenames are\
    \ distinguished, as in the NFS version 2\n      protocol.  The name, \".\", is\
    \ an alias for the current\n      directory and the name, \"..\", is an alias\
    \ for the parent\n      directory; that is, the directory that includes the\n\
    \      specified directory as a member. There is no facility for\n      dealing\
    \ with a multiparented directory and the NFS\n      protocol assumes a hierarchical\
    \ organization, organized as\n      a single-rooted tree.\n      Note that this\
    \ procedure does not follow symbolic links.\n      The client is responsible for\
    \ all parsing of filenames\n      including filenames that are modified by symbolic\
    \ links\n      encountered during the lookup process.\n   ERRORS\n      NFS3ERR_IO\n\
    \      NFS3ERR_NOENT\n      NFS3ERR_ACCES\n      NFS3ERR_NOTDIR\n      NFS3ERR_NAMETOOLONG\n\
    \      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE\
    \ ALSO\n      CREATE, MKDIR, SYMLINK, MKNOD, READDIRPLUS, and PATHCONF.\n"
- title: '3.3.4 Procedure 4: ACCESS - Check Access Permission'
  contents:
  - "3.3.4 Procedure 4: ACCESS - Check Access Permission\n   SYNOPSIS\n      ACCESS3res\
    \ NFSPROC3_ACCESS(ACCESS3args) = 4;\n      const ACCESS3_READ    = 0x0001;\n \
    \     const ACCESS3_LOOKUP  = 0x0002;\n      const ACCESS3_MODIFY  = 0x0004;\n\
    \      const ACCESS3_EXTEND  = 0x0008;\n      const ACCESS3_DELETE  = 0x0010;\n\
    \      const ACCESS3_EXECUTE = 0x0020;\n      struct ACCESS3args {\n         \
    \  nfs_fh3  object;\n           uint32   access;\n      };\n      struct ACCESS3resok\
    \ {\n           post_op_attr   obj_attributes;\n           uint32         access;\n\
    \      };\n      struct ACCESS3resfail {\n           post_op_attr   obj_attributes;\n\
    \      };\n      union ACCESS3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           ACCESS3resok   resok;\n      default:\n           ACCESS3resfail resfail;\n\
    \      };\n   DESCRIPTION\n      Procedure ACCESS determines the access rights\
    \ that a user,\n      as identified by the credentials in the request, has with\n\
    \      respect to a file system object. The client encodes the\n      set of permissions\
    \ that are to be checked in a bit mask.\n      The server checks the permissions\
    \ encoded in the bit mask.\n      A status of NFS3_OK is returned along with a\
    \ bit mask\n      encoded with the permissions that the client is allowed.\n \
    \     The results of this procedure are necessarily advisory in\n      nature.\
    \  That is, a return status of NFS3_OK and the\n      appropriate bit set in the\
    \ bit mask does not imply that\n      such access will be allowed to the file\
    \ system object in\n      the future, as access rights can be revoked by the server\n\
    \      at any time.\n      On entry, the arguments in ACCESS3args are:\n     \
    \ object\n         The file handle for the file system object to which\n     \
    \    access is to be checked.\n      access\n         A bit mask of access permissions\
    \ to check.\n      The following access permissions may be requested:\n      \
    \   ACCESS3_READ\n            Read data from file or read a directory.\n     \
    \    ACCESS3_LOOKUP\n            Look up a name in a directory (no meaning for\n\
    \            non-directory objects).\n         ACCESS3_MODIFY\n            Rewrite\
    \ existing file data or modify existing\n            directory entries.\n    \
    \     ACCESS3_EXTEND\n            Write new data or add directory entries.\n \
    \        ACCESS3_DELETE\n            Delete an existing directory entry.\n   \
    \      ACCESS3_EXECUTE\n            Execute file (no meaning for a directory).\n\
    \      On successful return, ACCESS3res.status is NFS3_OK. The\n      server should\
    \ return a status of NFS3_OK if no errors\n      occurred that prevented the server\
    \ from making the\n      required access checks. The results in ACCESS3res.resok\n\
    \      are:\n      obj_attributes\n         The post-operation attributes of object.\n\
    \      access\n         A bit mask of access permissions indicating access\n \
    \        rights for the authentication credentials provided with\n         the\
    \ request.\n      Otherwise, ACCESS3res.status contains the error on failure\n\
    \      and ACCESS3res.resfail contains the following:\n      obj_attributes\n\
    \         The attributes of object - if access to attributes is\n         permitted.\n\
    \   IMPLEMENTATION\n      In general, it is not sufficient for the client to attempt\n\
    \      to deduce access permissions by inspecting the uid, gid,\n      and mode\
    \ fields in the file attributes, since the server\n      may perform uid or gid\
    \ mapping or enforce additional\n      access control restrictions. It is also\
    \ possible that the\n      NFS version 3 protocol server may not be in the same\
    \ ID\n      space as the NFS version 3 protocol client. In these cases\n     \
    \ (and perhaps others), the NFS version 3 protocol client\n      can not reliably\
    \ perform an access check with only current\n      file attributes.\n      In\
    \ the NFS version 2 protocol, the only reliable way to\n      determine whether\
    \ an operation was allowed was to try it\n      and see if it succeeded or failed.\
    \ Using the ACCESS\n      procedure in the NFS version 3 protocol, the client\
    \ can\n      ask the server to indicate whether or not one or more\n      classes\
    \ of operations are permitted.  The ACCESS operation\n      is provided to allow\
    \ clients to check before doing a\n      series of operations. This is useful\
    \ in operating systems\n      (such as UNIX) where permission checking is done\
    \ only when\n      a file or directory is opened. This procedure is also\n   \
    \   invoked by NFS client access procedure (called possibly\n      through access(2)).\
    \ The intent is to make the behavior of\n      opening a remote file more consistent\
    \ with the behavior of\n      opening a local file.\n      The information returned\
    \ by the server in response to an\n      ACCESS call is not permanent. It was\
    \ correct at the exact\n      time that the server performed the checks, but not\n\
    \      necessarily afterwards. The server can revoke access\n      permission\
    \ at any time.\n      The NFS version 3 protocol client should use the effective\n\
    \      credentials of the user to build the authentication\n      information\
    \ in the ACCESS request used to determine access\n      rights. It is the effective\
    \ user and group credentials\n      that are used in subsequent read and write\
    \ operations. See\n      the comments in Permission issues on page 98 for more\n\
    \      information on this topic.\n      Many implementations do not directly\
    \ support the\n      ACCESS3_DELETE permission. Operating systems like UNIX\n\
    \      will ignore the ACCESS3_DELETE bit if set on an access\n      request on\
    \ a non-directory object. In these systems,\n      delete permission on a file\
    \ is determined by the access\n      permissions on the directory in which the\
    \ file resides,\n      instead of being determined by the permissions of the file\n\
    \      itself.  Thus, the bit mask returned for such a request\n      will have\
    \ the ACCESS3_DELETE bit set to 0, indicating that\n      the client does not\
    \ have this permission.\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_STALE\n  \
    \    NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      GETATTR.\n"
- title: '3.3.5 Procedure 5: READLINK - Read from symbolic link'
  contents:
  - "3.3.5 Procedure 5: READLINK - Read from symbolic link\n   SYNOPSIS\n      READLINK3res\
    \ NFSPROC3_READLINK(READLINK3args) = 5;\n      struct READLINK3args {\n      \
    \     nfs_fh3  symlink;\n      };\n      struct READLINK3resok {\n           post_op_attr\
    \   symlink_attributes;\n           nfspath3       data;\n      };\n      struct\
    \ READLINK3resfail {\n           post_op_attr   symlink_attributes;\n      };\n\
    \      union READLINK3res switch (nfsstat3 status) {\n      case NFS3_OK:\n  \
    \         READLINK3resok   resok;\n      default:\n           READLINK3resfail\
    \ resfail;\n      };\n   DESCRIPTION\n      Procedure READLINK reads the data\
    \ associated with a\n      symbolic link.  The data is an ASCII string that is\
    \ opaque\n      to the server.  That is, whether created by the NFS\n      version\
    \ 3 protocol software from a client or created\n      locally on the server, the\
    \ data in a symbolic link is not\n      interpreted when created, but is simply\
    \ stored. On entry,\n      the arguments in READLINK3args are:\n      symlink\n\
    \         The file handle for a symbolic link (file system object\n         of\
    \ type NF3LNK).\n      On successful return, READLINK3res.status is NFS3_OK and\n\
    \      READLINK3res.resok contains:\n      data\n         The data associated\
    \ with the symbolic link.\n      symlink_attributes\n         The post-operation\
    \ attributes for the symbolic link.\n      Otherwise, READLINK3res.status contains\
    \ the error on\n      failure and READLINK3res.resfail contains the following:\n\
    \      symlink_attributes\n         The post-operation attributes for the symbolic\
    \ link.\n   IMPLEMENTATION\n      A symbolic link is nominally a pointer to another\
    \ file.\n      The data is not necessarily interpreted by the server,\n      just\
    \ stored in the file.  It is possible for a client\n      implementation to store\
    \ a path name that is not meaningful\n      to the server operating system in\
    \ a symbolic link.  A\n      READLINK operation returns the data to the client\
    \ for\n      interpretation. If different implementations want to share\n    \
    \  access to symbolic links, then they must agree on the\n      interpretation\
    \ of the data in the symbolic link.\n      The READLINK operation is only allowed\
    \ on objects of type,\n      NF3LNK.  The server should return the error,\n  \
    \    NFS3ERR_INVAL, if the object is not of type, NF3LNK.\n      (Note: The X/Open\
    \ XNFS Specification for the NFS version 2\n      protocol defined the error status\
    \ in this case as\n      NFSERR_NXIO. This is inconsistent with existing server\n\
    \      practice.)\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_INVAL\n      NFS3ERR_ACCES\n\
    \      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n      NFS3ERR_SERVERFAULT\n\
    \   SEE ALSO\n      READLINK, SYMLINK.\n"
- title: '3.3.6 Procedure 6: READ - Read From file'
  contents:
  - "3.3.6 Procedure 6: READ - Read From file\n   SYNOPSIS\n      READ3res NFSPROC3_READ(READ3args)\
    \ = 6;\n      struct READ3args {\n           nfs_fh3  file;\n           offset3\
    \  offset;\n           count3   count;\n      };\n      struct READ3resok {\n\
    \           post_op_attr   file_attributes;\n           count3         count;\n\
    \           bool           eof;\n           opaque         data<>;\n      };\n\
    \      struct READ3resfail {\n           post_op_attr   file_attributes;\n   \
    \   };\n      union READ3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           READ3resok   resok;\n      default:\n           READ3resfail resfail;\n\
    \      };\n   DESCRIPTION\n      Procedure READ reads data from a file.  On entry,\
    \ the\n      arguments in READ3args are:\n      file\n         The file handle\
    \ of the file from which data is to be\n         read.  This must identify a file\
    \ system object of type,\n         NF3REG.\n      offset\n         The position\
    \ within the file at which the read is to\n         begin.  An offset of 0 means\
    \ to read data starting at\n         the beginning of the file. If offset is greater\
    \ than or\n         equal to the size of the file, the status, NFS3_OK, is\n \
    \        returned with count set to 0 and eof set to TRUE,\n         subject to\
    \ access permissions checking.\n      count\n         The number of bytes of data\
    \ that are to be read. If\n         count is 0, the READ will succeed and return\
    \ 0 bytes of\n         data, subject to access permissions checking. count\n \
    \        must be less than or equal to the value of the rtmax\n         field\
    \ in the FSINFO reply structure for the file system\n         that contains file.\
    \ If greater, the server may return\n         only rtmax bytes, resulting in a\
    \ short read.\n      On successful return, READ3res.status is NFS3_OK and\n  \
    \    READ3res.resok contains:\n      file_attributes\n         The attributes\
    \ of the file on completion of the read.\n      count\n         The number of\
    \ bytes of data returned by the read.\n      eof\n         If the read ended at\
    \ the end-of-file (formally, in a\n         correctly formed READ request, if\
    \ READ3args.offset plus\n         READ3resok.count is equal to the size of the\
    \ file), eof\n         is returned as TRUE; otherwise it is FALSE. A\n       \
    \  successful READ of an empty file will always return eof\n         as TRUE.\n\
    \      data\n         The counted data read from the file.\n      Otherwise, READ3res.status\
    \ contains the error on failure\n      and READ3res.resfail contains the following:\n\
    \      file_attributes\n         The post-operation attributes of the file.\n\
    \   IMPLEMENTATION\n      The nfsdata type used for the READ and WRITE operations\
    \ in\n      the NFS version 2 protocol defining the data portion of a\n      request\
    \ or reply has been changed to a variable-length\n      opaque byte array.  The\
    \ maximum size allowed by the\n      protocol is now limited by what XDR and underlying\n\
    \      transports will allow. There are no artificial limits\n      imposed by\
    \ the NFS version 3 protocol. Consult the FSINFO\n      procedure description\
    \ for details.\n      It is possible for the server to return fewer than count\n\
    \      bytes of data. If the server returns less than the count\n      requested\
    \ and eof set to FALSE, the client should issue\n      another READ to get the\
    \ remaining data. A server may\n      return less data than requested under several\n\
    \      circumstances. The file may have been truncated by another\n      client\
    \ or perhaps on the server itself, changing the file\n      size from what the\
    \ requesting client believes to be the\n      case. This would reduce the actual\
    \ amount of data\n      available to the client. It is possible that the server\n\
    \      may back off the transfer size and reduce the read request\n      return.\
    \ Server resource exhaustion may also occur\n      necessitating a smaller read\
    \ return.\n      Some NFS version 2 protocol client implementations chose\n  \
    \    to interpret a short read response as indicating EOF. The\n      addition\
    \ of the eof flag in the NFS version 3 protocol\n      provides a correct way\
    \ of handling EOF.\n      Some NFS version 2 protocol server implementations\n\
    \      incorrectly returned NFSERR_ISDIR if the file system\n      object type\
    \ was not a regular file. The correct return\n      value for the NFS version\
    \ 3 protocol is NFS3ERR_INVAL.\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_NXIO\n\
    \      NFS3ERR_ACCES\n      NFS3ERR_INVAL\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n\
    \      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      READLINK.\n"
- title: '3.3.7 Procedure 7: WRITE - Write to file'
  contents:
  - "3.3.7 Procedure 7: WRITE - Write to file\n   SYNOPSIS\n      WRITE3res NFSPROC3_WRITE(WRITE3args)\
    \ = 7;\n      enum stable_how {\n           UNSTABLE  = 0,\n           DATA_SYNC\
    \ = 1,\n           FILE_SYNC = 2\n      };\n      struct WRITE3args {\n      \
    \     nfs_fh3     file;\n           offset3     offset;\n           count3   \
    \   count;\n           stable_how  stable;\n           opaque      data<>;\n \
    \     };\n      struct WRITE3resok {\n           wcc_data    file_wcc;\n     \
    \      count3      count;\n           stable_how  committed;\n           writeverf3\
    \  verf;\n      };\n      struct WRITE3resfail {\n           wcc_data    file_wcc;\n\
    \      };\n      union WRITE3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           WRITE3resok    resok;\n      default:\n           WRITE3resfail  resfail;\n\
    \      };\n   DESCRIPTION\n      Procedure WRITE writes data to a file. On entry,\
    \ the\n      arguments in WRITE3args are:\n      file\n         The file handle\
    \ for the file to which data is to be\n         written.  This must identify a\
    \ file system object of\n         type, NF3REG.\n      offset\n         The position\
    \ within the file at which the write is to\n         begin.  An offset of 0 means\
    \ to write data starting at\n         the beginning of the file.\n      count\n\
    \         The number of bytes of data to be written. If count is\n         0,\
    \ the WRITE will succeed and return a count of 0,\n         barring errors due\
    \ to permissions checking. The size of\n         data must be less than or equal\
    \ to the value of the\n         wtmax field in the FSINFO reply structure for\
    \ the file\n         system that contains file. If greater, the server may\n \
    \        write only wtmax bytes, resulting in a short write.\n      stable\n \
    \        If stable is FILE_SYNC, the server must commit the data\n         written\
    \ plus all file system metadata to stable storage\n         before returning results.\
    \ This corresponds to the NFS\n         version 2 protocol semantics. Any other\
    \ behavior\n         constitutes a protocol violation. If stable is\n        \
    \ DATA_SYNC, then the server must commit all of the data\n         to stable storage\
    \ and enough of the metadata to\n         retrieve the data before returning.\
    \  The server\n         implementor is free to implement DATA_SYNC in the same\n\
    \         fashion as FILE_SYNC, but with a possible performance\n         drop.\
    \  If stable is UNSTABLE, the server is free to\n         commit any part of the\
    \ data and the metadata to stable\n         storage, including all or none, before\
    \ returning a\n         reply to the client. There is no guarantee whether or\n\
    \         when any uncommitted data will subsequently be\n         committed to\
    \ stable storage. The only guarantees made\n         by the server are that it\
    \ will not destroy any data\n         without changing the value of verf and that\
    \ it will not\n         commit the data and metadata at a level less than that\n\
    \         requested by the client. See the discussion on COMMIT\n         on page\
    \ 92 for more information on if and when\n         data is committed to stable\
    \ storage.\n      data\n         The data to be written to the file.\n      On\
    \ successful return, WRITE3res.status is NFS3_OK and\n      WRITE3res.resok contains:\n\
    \      file_wcc\n         Weak cache consistency data for the file. For a client\n\
    \         that requires only the post-write file attributes,\n         these can\
    \ be found in file_wcc.after.\n      count\n         The number of bytes of data\
    \ written to the file. The\n         server may write fewer bytes than requested.\
    \ If so, the\n         actual number of bytes written starting at location,\n\
    \         offset, is returned.\n      committed\n         The server should return\
    \ an indication of the level of\n         commitment of the data and metadata\
    \ via committed. If\n         the server committed all data and metadata to stable\n\
    \         storage, committed should be set to FILE_SYNC. If the\n         level\
    \ of commitment was at least as strong as\n         DATA_SYNC, then committed\
    \ should be set to DATA_SYNC.\n         Otherwise, committed must be returned\
    \ as UNSTABLE. If\n         stable was FILE_SYNC, then committed must also be\n\
    \         FILE_SYNC: anything else constitutes a protocol\n         violation.\
    \ If stable was DATA_SYNC, then committed may\n         be FILE_SYNC or DATA_SYNC:\
    \ anything else constitutes a\n         protocol violation. If stable was UNSTABLE,\
    \ then\n         committed may be either FILE_SYNC, DATA_SYNC, or\n         UNSTABLE.\n\
    \      verf\n         This is a cookie that the client can use to determine\n\
    \         whether the server has changed state between a call to\n         WRITE\
    \ and a subsequent call to either WRITE or COMMIT.\n         This cookie must\
    \ be consistent during a single instance\n         of the NFS version 3 protocol\
    \ service and must be\n         unique between instances of the NFS version 3\
    \ protocol\n         server, where uncommitted data may be lost.\n      Otherwise,\
    \ WRITE3res.status contains the error on failure\n      and WRITE3res.resfail\
    \ contains the following:\n      file_wcc\n         Weak cache consistency data\
    \ for the file. For a client\n         that requires only the post-write file\
    \ attributes,\n         these can be found in file_wcc.after. Even though the\n\
    \         write failed, full wcc_data is returned to allow the\n         client\
    \ to determine whether the failed write resulted\n         in any change to the\
    \ file.\n      If a client writes data to the server with the stable\n      argument\
    \ set to UNSTABLE and the reply yields a committed\n      response of DATA_SYNC\
    \ or UNSTABLE, the client will follow\n      up some time in the future with a\
    \ COMMIT operation to\n      synchronize outstanding asynchronous data and metadata\n\
    \      with the server's stable storage, barring client error. It\n      is possible\
    \ that due to client crash or other error that a\n      subsequent COMMIT will\
    \ not be received by the server.\n   IMPLEMENTATION\n      The nfsdata type used\
    \ for the READ and WRITE operations in\n      the NFS version 2 protocol defining\
    \ the data portion of a\n      request or reply has been changed to a variable-length\n\
    \      opaque byte array.  The maximum size allowed by the\n      protocol is\
    \ now limited by what XDR and underlying\n      transports will allow. There are\
    \ no artificial limits\n      imposed by the NFS version 3 protocol. Consult the\
    \ FSINFO\n      procedure description for details.\n      It is possible for the\
    \ server to write fewer than count\n      bytes of data. In this case, the server\
    \ should not return\n      an error unless no data was written at all. If the\
    \ server\n      writes less than count bytes, the client should issue\n      another\
    \ WRITE to write the remaining data.\n      It is assumed that the act of writing\
    \ data to a file will\n      cause the mtime of the file to be updated. However,\
    \ the\n      mtime of the file should not be changed unless the\n      contents\
    \ of the file are changed.  Thus, a WRITE request\n      with count set to 0 should\
    \ not cause the mtime of the file\n      to be updated.\n      The NFS version\
    \ 3 protocol introduces safe asynchronous\n      writes.  The combination of WRITE\
    \ with stable set to\n      UNSTABLE followed by a COMMIT addresses the performance\n\
    \      bottleneck found in the NFS version 2 protocol, the need\n      to synchronously\
    \ commit all writes to stable storage.\n      The definition of stable storage\
    \ has been historically a\n      point of contention. The following expected properties\
    \ of\n      stable storage may help in resolving design issues in the\n      implementation.\
    \ Stable storage is persistent storage that\n      survives:\n      1. Repeated\
    \ power failures.\n      2. Hardware failures (of any board, power supply, and\
    \ so on.).\n      3. Repeated software crashes, including reboot cycle.\n    \
    \  This definition does not address failure of the stable\n      storage module\
    \ itself.\n      A cookie, verf, is defined to allow a client to detect\n    \
    \  different instances of an NFS version 3 protocol server\n      over which cached,\
    \ uncommitted data may be lost. In the\n      most likely case, the verf allows\
    \ the client to detect\n      server reboots. This information is required so\
    \ that the\n      client can safely determine whether the server could have\n\
    \      lost cached data. If the server fails unexpectedly and the\n      client\
    \ has uncommitted data from previous WRITE requests\n      (done with the stable\
    \ argument set to UNSTABLE and in\n      which the result committed was returned\
    \ as UNSTABLE as\n      well) it may not have flushed cached data to stable\n\
    \      storage. The burden of recovery is on the client and the\n      client\
    \ will need to retransmit the data to the server.\n      A suggested verf cookie\
    \ would be to use the time that the\n      server was booted or the time the server\
    \ was last started\n      (if restarting the server without a reboot results in\
    \ lost\n      buffers).\n      The committed field in the results allows the client\
    \ to do\n      more effective caching. If the server is committing all\n     \
    \ WRITE requests to stable storage, then it should return\n      with committed\
    \ set to FILE_SYNC, regardless of the value\n      of the stable field in the\
    \ arguments. A server that uses\n      an NVRAM accelerator may choose to implement\
    \ this policy.\n      The client can use this to increase the effectiveness of\n\
    \      the cache by discarding cached data that has already been\n      committed\
    \ on the server.\n      Some implementations may return NFS3ERR_NOSPC instead\
    \ of\n      NFS3ERR_DQUOT when a user's quota is exceeded.\n      Some NFS version\
    \ 2 protocol server implementations\n      incorrectly returned NFSERR_ISDIR if\
    \ the file system\n      object type was not a regular file. The correct return\n\
    \      value for the NFS version 3 protocol is NFS3ERR_INVAL.\n   ERRORS\n   \
    \   NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_FBIG\n      NFS3ERR_DQUOT\n\
    \      NFS3ERR_NOSPC\n      NFS3ERR_ROFS\n      NFS3ERR_INVAL\n      NFS3ERR_STALE\n\
    \      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      COMMIT.\n"
- title: '3.3.8 Procedure 8: CREATE - Create a file'
  contents:
  - "3.3.8 Procedure 8: CREATE - Create a file\n   SYNOPSIS\n      CREATE3res NFSPROC3_CREATE(CREATE3args)\
    \ = 8;\n      enum createmode3 {\n           UNCHECKED = 0,\n           GUARDED\
    \   = 1,\n           EXCLUSIVE = 2\n      };\n      union createhow3 switch (createmode3\
    \ mode) {\n      case UNCHECKED:\n      case GUARDED:\n           sattr3     \
    \  obj_attributes;\n      case EXCLUSIVE:\n           createverf3  verf;\n   \
    \   };\n      struct CREATE3args {\n           diropargs3   where;\n         \
    \  createhow3   how;\n      };\n      struct CREATE3resok {\n           post_op_fh3\
    \   obj;\n           post_op_attr  obj_attributes;\n           wcc_data      dir_wcc;\n\
    \      };\n      struct CREATE3resfail {\n           wcc_data      dir_wcc;\n\
    \      };\n      union CREATE3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           CREATE3resok    resok;\n      default:\n           CREATE3resfail\
    \  resfail;\n      };\n   DESCRIPTION\n      Procedure CREATE creates a regular\
    \ file. On entry, the\n      arguments in CREATE3args are:\n      where\n    \
    \     The location of the file to be created:\n         dir\n            The file\
    \ handle for the directory in which the file\n            is to be created.\n\
    \         name\n            The name that is to be associated with the created\n\
    \            file.  Refer to General comments on filenames on\n            page\
    \ 30.\n      When creating a regular file, there are three ways to\n      create\
    \ the file as defined by:\n      how\n         A discriminated union describing\
    \ how the server is to\n         handle the file creation along with the appropriate\n\
    \         attributes:\n      mode\n         One of UNCHECKED, GUARDED, and EXCLUSIVE.\
    \ UNCHECKED\n         means that the file should be created without checking\n\
    \         for the existence of a duplicate file in the same\n         directory.\
    \ In this case, how.obj_attributes is a sattr3\n         describing the initial\
    \ attributes for the file. GUARDED\n         specifies that the server should\
    \ check for the presence\n         of a duplicate file before performing the create\
    \ and\n         should fail the request with NFS3ERR_EXIST if a\n         duplicate\
    \ file exists. If the file does not exist, the\n         request is performed\
    \ as described for UNCHECKED.\n         EXCLUSIVE specifies that the server is\
    \ to follow\n         exclusive creation semantics, using the verifier to\n  \
    \       ensure exclusive creation of the target. No attributes\n         may be\
    \ provided in this case, since the server may use\n         the target file metadata\
    \ to store the createverf3\n         verifier.\n      On successful return, CREATE3res.status\
    \ is NFS3_OK and the\n      results in CREATE3res.resok are:\n      obj\n    \
    \     The file handle of the newly created regular file.\n      obj_attributes\n\
    \         The attributes of the regular file just created.\n      dir_wcc\n  \
    \       Weak cache consistency data for the directory,\n         where.dir. For\
    \ a client that requires on the\n         post-CREATE directory attributes, these\
    \ can be found in\n         dir_wcc.after.\n      Otherwise, CREATE3res.status\
    \ contains the error on failure\n      and CREATE3res.resfail contains the following:\n\
    \      dir_wcc\n         Weak cache consistency data for the directory,\n    \
    \     where.dir. For a client that requires only the\n         post-CREATE directory\
    \ attributes, these can be found in\n         dir_wcc.after. Even though the CREATE\
    \ failed, full\n         wcc_data is returned to allow the client to determine\n\
    \         whether the failing CREATE resulted in any change to\n         the directory.\n\
    \   IMPLEMENTATION\n      Unlike the NFS version 2 protocol, in which certain\
    \ fields\n      in the initial attributes structure were overloaded to\n     \
    \ indicate creation of devices and FIFOs in addition to\n      regular files,\
    \ this procedure only supports the creation\n      of regular files. The MKNOD\
    \ procedure was introduced in\n      the NFS version 3 protocol to handle creation\
    \ of devices\n      and FIFOs. Implementations should have no reason in the\n\
    \      NFS version 3 protocol to overload CREATE semantics.\n      One aspect\
    \ of the NFS version 3 protocol CREATE procedure\n      warrants particularly\
    \ careful consideration: the mechanism\n      introduced to support the reliable\
    \ exclusive creation of\n      regular files. The mechanism comes into play when\
    \ how.mode\n      is EXCLUSIVE.  In this case, how.verf contains a verifier\n\
    \      that can reasonably be expected to be unique.  A\n      combination of\
    \ a client identifier, perhaps the client\n      network address, and a unique\
    \ number generated by the\n      client, perhaps the RPC transaction identifier,\
    \ may be\n      appropriate.\n      If the file does not exist, the server creates\
    \ the file\n      and stores the verifier in stable storage. For file\n      systems\
    \ that do not provide a mechanism for the storage of\n      arbitrary file attributes,\
    \ the server may use one or more\n      elements of the file metadata to store\
    \ the verifier. The\n      verifier must be stored in stable storage to prevent\n\
    \      erroneous failure on retransmission of the request. It is\n      assumed\
    \ that an exclusive create is being performed\n      because exclusive semantics\
    \ are critical to the\n      application. Because of the expected usage, exclusive\n\
    \      CREATE does not rely solely on the normally volatile\n      duplicate request\
    \ cache for storage of the verifier. The\n      duplicate request cache in volatile\
    \ storage does not\n      survive a crash and may actually flush on a long network\n\
    \      partition, opening failure windows.  In the UNIX local\n      file system\
    \ environment, the expected storage location for\n      the verifier on creation\
    \ is the metadata (time stamps) of\n      the file. For this reason, an exclusive\
    \ file create may\n      not include initial attributes because the server would\n\
    \      have nowhere to store the verifier.\n      If the server can not support\
    \ these exclusive create\n      semantics, possibly because of the requirement\
    \ to commit\n      the verifier to stable storage, it should fail the CREATE\n\
    \      request with the error, NFS3ERR_NOTSUPP.\n      During an exclusive CREATE\
    \ request, if the file already\n      exists, the server reconstructs the file's\
    \ verifier and\n      compares it with the verifier in the request. If they\n\
    \      match, the server treats the request as a success. The\n      request is\
    \ presumed to be a duplicate of an earlier,\n      successful request for which\
    \ the reply was lost and that\n      the server duplicate request cache mechanism\
    \ did not\n      detect. If the verifiers do not match, the request is\n     \
    \ rejected with the status, NFS3ERR_EXIST.\n      Once the client has performed\
    \ a successful exclusive\n      create, it must issue a SETATTR to set the correct\
    \ file\n      attributes.  Until it does so, it should not rely upon any\n   \
    \   of the file attributes, since the server implementation\n      may need to\
    \ overload file metadata to store the verifier.\n      Use of the GUARDED attribute\
    \ does not provide exactly-once\n      semantics.  In particular, if a reply is\
    \ lost and the\n      server does not detect the retransmission of the request,\n\
    \      the procedure can fail with NFS3ERR_EXIST, even though the\n      create\
    \ was performed successfully.\n      Refer to General comments on filenames on\
    \ page 30.\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_EXIST\n\
    \      NFS3ERR_NOTDIR\n      NFS3ERR_NOSPC\n      NFS3ERR_ROFS\n      NFS3ERR_NAMETOOLONG\n\
    \      NFS3ERR_DQUOT\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n\
    \      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      MKDIR, SYMLINK, MKNOD, and PATHCONF.\n"
- title: '3.3.9 Procedure 9: MKDIR - Create a directory'
  contents:
  - "3.3.9 Procedure 9: MKDIR - Create a directory\n   SYNOPSIS\n      MKDIR3res NFSPROC3_MKDIR(MKDIR3args)\
    \ = 9;\n      struct MKDIR3args {\n           diropargs3   where;\n          \
    \ sattr3       attributes;\n      };\n      struct MKDIR3resok {\n           post_op_fh3\
    \   obj;\n           post_op_attr  obj_attributes;\n           wcc_data      dir_wcc;\n\
    \      };\n      struct MKDIR3resfail {\n           wcc_data      dir_wcc;\n \
    \     };\n      union MKDIR3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           MKDIR3resok   resok;\n      default:\n           MKDIR3resfail resfail;\n\
    \      };\n   DESCRIPTION\n      Procedure MKDIR creates a new subdirectory. On\
    \ entry, the\n      arguments in MKDIR3args are:\n      where\n         The location\
    \ of the subdirectory to be created:\n         dir\n            The file handle\
    \ for the directory in which the\n            subdirectory is to be created.\n\
    \         name\n            The name that is to be associated with the created\n\
    \            subdirectory. Refer to General comments on filenames\n          \
    \  on page 30.\n      attributes\n         The initial attributes for the subdirectory.\n\
    \      On successful return, MKDIR3res.status is NFS3_OK and the\n      results\
    \ in MKDIR3res.resok are:\n      obj\n         The file handle for the newly created\
    \ directory.\n      obj_attributes\n         The attributes for the newly created\
    \ subdirectory.\n      dir_wcc\n         Weak cache consistency data for the directory,\n\
    \         where.dir. For a client that requires only the\n         post-MKDIR\
    \ directory attributes, these can be found in\n         dir_wcc.after.\n     \
    \ Otherwise, MKDIR3res.status contains the error on failure\n      and MKDIR3res.resfail\
    \ contains the following:\n      dir_wcc\n         Weak cache consistency data\
    \ for the directory,\n         where.dir. For a client that requires only the\n\
    \         post-MKDIR directory attributes, these can be found in\n         dir_wcc.after.\
    \ Even though the MKDIR failed, full\n         wcc_data is returned to allow the\
    \ client to determine\n         whether the failing MKDIR resulted in any change\
    \ to the\n         directory.\n   IMPLEMENTATION\n      Many server implementations\
    \ will not allow the filenames,\n      \".\" or \"..\", to be used as targets\
    \ in a MKDIR operation.\n      In this case, the server should return NFS3ERR_EXIST.\n\
    \      Refer to General comments on filenames on page 30.\n   ERRORS\n      NFS3ERR_IO\n\
    \      NFS3ERR_ACCES\n      NFS3ERR_EXIST\n      NFS3ERR_NOTDIR\n      NFS3ERR_NOSPC\n\
    \      NFS3ERR_ROFS\n      NFS3ERR_NAMETOOLONG\n      NFS3ERR_DQUOT\n      NFS3ERR_STALE\n\
    \      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n      NFS3ERR_SERVERFAULT\n \
    \  SEE ALSO\n      CREATE, SYMLINK, MKNOD, and PATHCONF.\n"
- title: '3.3.10 Procedure 10: SYMLINK - Create a symbolic link'
  contents:
  - "3.3.10 Procedure 10: SYMLINK - Create a symbolic link\n   SYNOPSIS\n      SYMLINK3res\
    \ NFSPROC3_SYMLINK(SYMLINK3args) = 10;\n      struct symlinkdata3 {\n        \
    \   sattr3    symlink_attributes;\n           nfspath3  symlink_data;\n      };\n\
    \      struct SYMLINK3args {\n           diropargs3    where;\n           symlinkdata3\
    \  symlink;\n      };\n      struct SYMLINK3resok {\n           post_op_fh3  \
    \ obj;\n           post_op_attr  obj_attributes;\n           wcc_data      dir_wcc;\n\
    \      };\n      struct SYMLINK3resfail {\n           wcc_data      dir_wcc;\n\
    \      };\n      union SYMLINK3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           SYMLINK3resok   resok;\n      default:\n           SYMLINK3resfail\
    \ resfail;\n      };\n   DESCRIPTION\n      Procedure SYMLINK creates a new symbolic\
    \ link. On entry,\n      the arguments in SYMLINK3args are:\n      where\n   \
    \      The location of the symbolic link to be created:\n         dir\n      \
    \      The file handle for the directory in which the\n            symbolic link\
    \ is to be created.\n         name\n            The name that is to be associated\
    \ with the created\n            symbolic link. Refer to General comments on\n\
    \            filenames on page 30.\n      symlink\n         The symbolic link\
    \ to create:\n         symlink_attributes\n            The initial attributes\
    \ for the symbolic link.\n         symlink_data\n            The string containing\
    \ the symbolic link data.\n      On successful return, SYMLINK3res.status is NFS3_OK\
    \ and\n      SYMLINK3res.resok contains:\n      obj\n         The file handle\
    \ for the newly created symbolic link.\n      obj_attributes\n         The attributes\
    \ for the newly created symbolic link.\n      dir_wcc\n         Weak cache consistency\
    \ data for the directory,\n         where.dir. For a client that requires only\
    \ the\n         post-SYMLINK directory attributes, these can be found\n      \
    \   in dir_wcc.after.\n      Otherwise, SYMLINK3res.status contains the error\
    \ on\n      failure and SYMLINK3res.resfail contains the following:\n      dir_wcc\n\
    \         Weak cache consistency data for the directory,\n         where.dir.\
    \ For a client that requires only the\n         post-SYMLINK directory attributes,\
    \ these can be found\n         in dir_wcc.after. Even though the SYMLINK failed,\
    \ full\n         wcc_data is returned to allow the client to determine\n     \
    \    whether the failing SYMLINK changed the directory.\n   IMPLEMENTATION\n \
    \     Refer to General comments on filenames on page 30.\n      For symbolic links,\
    \ the actual file system node and its\n      contents are expected to be created\
    \ in a single atomic\n      operation.  That is, once the symbolic link is visible,\n\
    \      there must not be a window where a READLINK would fail or\n      return\
    \ incorrect data.\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_EXIST\n\
    \      NFS3ERR_NOTDIR\n      NFS3ERR_NOSPC\n      NFS3ERR_ROFS\n      NFS3ERR_NAMETOOLONG\n\
    \      NFS3ERR_DQUOT\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n\
    \      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      READLINK, CREATE, MKDIR, MKNOD,\
    \ FSINFO, and PATHCONF.\n"
- title: '3.3.11 Procedure 11: MKNOD - Create a special device'
  contents:
  - "3.3.11 Procedure 11: MKNOD - Create a special device\n   SYNOPSIS\n      MKNOD3res\
    \ NFSPROC3_MKNOD(MKNOD3args) = 11;\n      struct devicedata3 {\n           sattr3\
    \     dev_attributes;\n           specdata3  spec;\n      };\n      union mknoddata3\
    \ switch (ftype3 type) {\n      case NF3CHR:\n      case NF3BLK:\n           devicedata3\
    \  device;\n      case NF3SOCK:\n      case NF3FIFO:\n           sattr3      \
    \ pipe_attributes;\n      default:\n           void;\n      };\n      struct MKNOD3args\
    \ {\n           diropargs3   where;\n           mknoddata3   what;\n      };\n\
    \      struct MKNOD3resok {\n           post_op_fh3   obj;\n           post_op_attr\
    \  obj_attributes;\n           wcc_data      dir_wcc;\n      };\n      struct\
    \ MKNOD3resfail {\n           wcc_data      dir_wcc;\n      };\n      union MKNOD3res\
    \ switch (nfsstat3 status) {\n      case NFS3_OK:\n           MKNOD3resok   resok;\n\
    \      default:\n           MKNOD3resfail resfail;\n      };\n   DESCRIPTION\n\
    \      Procedure MKNOD creates a new special file of the type,\n      what.type.\
    \  Special files can be device files or named\n      pipes.  On entry, the arguments\
    \ in MKNOD3args are:\n      where\n         The location of the special file to\
    \ be created:\n         dir\n            The file handle for the directory in\
    \ which the\n            special file is to be created.\n         name\n     \
    \       The name that is to be associated with the created\n            special\
    \ file. Refer to General comments on filenames\n            on page 30.\n    \
    \  what\n         A discriminated union identifying the type of the\n        \
    \ special file to be created along with the data and\n         attributes appropriate\
    \ to the type of the special\n         file:\n         type\n            The type\
    \ of the object to be created.\n      When creating a character special file (what.type\
    \ is\n      NF3CHR) or a block special file (what.type is NF3BLK),\n      what\
    \ includes:\n      device\n         A structure devicedata3 with the following\
    \ components:\n         dev_attributes\n            The initial attributes for\
    \ the special file.\n         spec\n            The major number stored in device.spec.specdata1\
    \ and\n            the minor number stored in device.spec.specdata2.\n      When\
    \ creating a socket (what.type is NF3SOCK) or a FIFO\n      (what.type is NF3FIFO),\
    \ what includes:\n         pipe_attributes\n            The initial attributes\
    \ for the special file.\n      On successful return, MKNOD3res.status is NFS3_OK\
    \ and\n      MKNOD3res.resok contains:\n      obj\n         The file handle for\
    \ the newly created special file.\n      obj_attributes\n         The attributes\
    \ for the newly created special file.\n      dir_wcc\n         Weak cache consistency\
    \ data for the directory,\n         where.dir. For a client that requires only\
    \ the\n         post-MKNOD directory attributes, these can be found in\n     \
    \    dir_wcc.after.\n      Otherwise, MKNOD3res.status contains the error on failure\n\
    \      and MKNOD3res.resfail contains the following:\n      dir_wcc\n        \
    \ Weak cache consistency data for the directory,\n         where.dir. For a client\
    \ that requires only the\n         post-MKNOD directory attributes, these can\
    \ be found in\n         dir_wcc.after. Even though the MKNOD failed, full\n  \
    \       wcc_data is returned to allow the client to determine\n         whether\
    \ the failing MKNOD changed the directory.\n   IMPLEMENTATION\n      Refer to\
    \ General comments on filenames on page 30.\n      Without explicit support for\
    \ special file type creation in\n      the NFS version 2 protocol, fields in the\
    \ CREATE arguments\n      were overloaded to indicate creation of certain types\
    \ of\n      objects.  This overloading is not necessary in the NFS\n      version\
    \ 3 protocol.\n      If the server does not support any of the defined types,\n\
    \      the error, NFS3ERR_NOTSUPP, should be returned. Otherwise,\n      if the\
    \ server does not support the target type or the\n      target type is illegal,\
    \ the error, NFS3ERR_BADTYPE, should\n      be returned. Note that NF3REG, NF3DIR,\
    \ and NF3LNK are\n      illegal types for MKNOD. The procedures, CREATE, MKDIR,\n\
    \      and SYMLINK should be used to create these file types,\n      respectively,\
    \ instead of MKNOD.\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_EXIST\n\
    \      NFS3ERR_NOTDIR\n      NFS3ERR_NOSPC\n      NFS3ERR_ROFS\n      NFS3ERR_NAMETOOLONG\n\
    \      NFS3ERR_DQUOT\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n\
    \      NFS3ERR_SERVERFAULT\n      NFS3ERR_BADTYPE\n   SEE ALSO\n      CREATE,\
    \ MKDIR, SYMLINK, and PATHCONF.\n"
- title: '3.3.12 Procedure 12: REMOVE - Remove a File'
  contents:
  - "3.3.12 Procedure 12: REMOVE - Remove a File\n   SYNOPSIS\n      REMOVE3res NFSPROC3_REMOVE(REMOVE3args)\
    \ = 12;\n      struct REMOVE3args {\n           diropargs3  object;\n      };\n\
    \      struct REMOVE3resok {\n           wcc_data    dir_wcc;\n      };\n    \
    \  struct REMOVE3resfail {\n           wcc_data    dir_wcc;\n      };\n      union\
    \ REMOVE3res switch (nfsstat3 status) {\n      case NFS3_OK:\n           REMOVE3resok\
    \   resok;\n      default:\n           REMOVE3resfail resfail;\n      };\n   DESCRIPTION\n\
    \      Procedure REMOVE removes (deletes) an entry from a\n      directory. If\
    \ the entry in the directory was the last\n      reference to the corresponding\
    \ file system object, the\n      object may be destroyed.  On entry, the arguments\
    \ in\n      REMOVE3args are:\n      object\n         A diropargs3 structure identifying\
    \ the entry to be\n         removed:\n      dir\n         The file handle for\
    \ the directory from which the entry\n         is to be removed.\n      name\n\
    \         The name of the entry to be removed. Refer to General\n         comments\
    \ on filenames on page 30.\n      On successful return, REMOVE3res.status is NFS3_OK\
    \ and\n      REMOVE3res.resok contains:\n      dir_wcc\n         Weak cache consistency\
    \ data for the directory,\n         object.dir.  For a client that requires only\
    \ the\n         post-REMOVE directory attributes, these can be found in\n    \
    \     dir_wcc.after.\n      Otherwise, REMOVE3res.status contains the error on\
    \ failure\n      and REMOVE3res.resfail contains the following:\n      dir_wcc\n\
    \         Weak cache consistency data for the directory,\n         object.dir.\
    \  For a client that requires only the\n         post-REMOVE directory attributes,\
    \ these can be found in\n         dir_wcc.after. Even though the REMOVE failed,\
    \ full\n         wcc_data is returned to allow the client to determine\n     \
    \    whether the failing REMOVE changed the directory.\n   IMPLEMENTATION\n  \
    \    In general, REMOVE is intended to remove non-directory\n      file objects\
    \ and RMDIR is to be used to remove\n      directories.  However, REMOVE can be\
    \ used to remove\n      directories, subject to restrictions imposed by either\
    \ the\n      client or server interfaces.  This had been a source of\n      confusion\
    \ in the NFS version 2 protocol.\n      The concept of last reference is server\
    \ specific. However,\n      if the nlink field in the previous attributes of the\n\
    \      object had the value 1, the client should not rely on\n      referring\
    \ to the object via a file handle. Likewise, the\n      client should not rely\
    \ on the resources (disk space,\n      directory entry, and so on.) formerly associated\
    \ with the\n      object becoming immediately available. Thus, if a client\n \
    \     needs to be able to continue to access a file after using\n      REMOVE\
    \ to remove it, the client should take steps to make\n      sure that the file\
    \ will still be accessible. The usual\n      mechanism used is to use RENAME to\
    \ rename the file from\n      its old name to a new hidden name.\n      Refer\
    \ to General comments on filenames on page 30.\n   ERRORS\n      NFS3ERR_NOENT\n\
    \      NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_NOTDIR\n      NFS3ERR_NAMETOOLONG\n\
    \      NFS3ERR_ROFS\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n\
    \   SEE ALSO\n      RMDIR and RENAME.\n"
- title: '3.3.13 Procedure 13: RMDIR - Remove a Directory'
  contents:
  - "3.3.13 Procedure 13: RMDIR - Remove a Directory\n   SYNOPSIS\n      RMDIR3res\
    \ NFSPROC3_RMDIR(RMDIR3args) = 13;\n      struct RMDIR3args {\n           diropargs3\
    \  object;\n      };\n      struct RMDIR3resok {\n           wcc_data    dir_wcc;\n\
    \      };\n      struct RMDIR3resfail {\n           wcc_data    dir_wcc;\n   \
    \   };\n      union RMDIR3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           RMDIR3resok   resok;\n      default:\n           RMDIR3resfail resfail;\n\
    \      };\n   DESCRIPTION\n      Procedure RMDIR removes (deletes) a subdirectory\
    \ from a\n      directory. If the directory entry of the subdirectory is\n   \
    \   the last reference to the subdirectory, the subdirectory\n      may be destroyed.\
    \ On entry, the arguments in RMDIR3args\n      are:\n      object\n         A\
    \ diropargs3 structure identifying the directory entry\n         to be removed:\n\
    \         dir\n            The file handle for the directory from which the\n\
    \            subdirectory is to be removed.\n         name\n            The name\
    \ of the subdirectory to be removed. Refer to\n            General comments on\
    \ filenames on page 30.\n      On successful return, RMDIR3res.status is NFS3_OK\
    \ and\n      RMDIR3res.resok contains:\n      dir_wcc\n         Weak cache consistency\
    \ data for the directory,\n         object.dir.  For a client that requires only\
    \ the\n         post-RMDIR directory attributes, these can be found in\n     \
    \    dir_wcc.after.\n      Otherwise, RMDIR3res.status contains the error on failure\n\
    \      and RMDIR3res.resfail contains the following:\n      dir_wcc\n        \
    \ Weak cache consistency data for the directory,\n         object.dir.  For a\
    \ client that requires only the\n         post-RMDIR directory attributes, these\
    \ can be found in\n         dir_wcc.after. Note that even though the RMDIR failed,\n\
    \         full wcc_data is returned to allow the client to\n         determine\
    \ whether the failing RMDIR changed the\n         directory.\n   IMPLEMENTATION\n\
    \      Note that on some servers, removal of a non-empty\n      directory is disallowed.\n\
    \      On some servers, the filename, \".\", is illegal. These\n      servers\
    \ will return the error, NFS3ERR_INVAL. On some\n      servers, the filename,\
    \ \"..\", is illegal. These servers\n      will return the error, NFS3ERR_EXIST.\
    \ This would seem\n      inconsistent, but allows these servers to comply with\n\
    \      their own specific interface definitions.  Clients should\n      be prepared\
    \ to handle both cases.\n      The client should not rely on the resources (disk\
    \ space,\n      directory entry, and so on.) formerly associated with the\n  \
    \    directory becoming immediately available.\n   ERRORS\n      NFS3ERR_NOENT\n\
    \      NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_INVAL\n      NFS3ERR_EXIST\n\
    \      NFS3ERR_NOTDIR\n      NFS3ERR_NAMETOOLONG\n      NFS3ERR_ROFS\n      NFS3ERR_NOTEMPTY\n\
    \      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n      NFS3ERR_SERVERFAULT\n\
    \   SEE ALSO\n      REMOVE.\n"
- title: '3.3.14 Procedure 14: RENAME - Rename a File or Directory'
  contents:
  - "3.3.14 Procedure 14: RENAME - Rename a File or Directory\n   SYNOPSIS\n     \
    \ RENAME3res NFSPROC3_RENAME(RENAME3args) = 14;\n      struct RENAME3args {\n\
    \           diropargs3   from;\n           diropargs3   to;\n      };\n      struct\
    \ RENAME3resok {\n           wcc_data     fromdir_wcc;\n           wcc_data  \
    \   todir_wcc;\n      };\n      struct RENAME3resfail {\n           wcc_data \
    \    fromdir_wcc;\n           wcc_data     todir_wcc;\n      };\n      union RENAME3res\
    \ switch (nfsstat3 status) {\n      case NFS3_OK:\n           RENAME3resok   resok;\n\
    \      default:\n           RENAME3resfail resfail;\n      };\n   DESCRIPTION\n\
    \      Procedure RENAME renames the file identified by from.name\n      in the\
    \ directory, from.dir, to to.name in the di- rectory,\n      to.dir. The operation\
    \ is required to be atomic to the\n      client. To.dir and from.dir must reside\
    \ on the same file\n      system and server. On entry, the arguments in RENAME3args\n\
    \      are:\n      from\n         A diropargs3 structure identifying the source\
    \ (the file\n         system object to be re-named):\n         from.dir\n    \
    \        The file handle for the directory from which the\n            entry is\
    \ to be renamed.\n         from.name\n            The name of the entry that identifies\
    \ the object to\n            be renamed. Refer to General comments on filenames\n\
    \            on page 30.\n      to\n         A diropargs3 structure identifying\
    \ the target (the new\n         name of the object):\n         to.dir\n      \
    \      The file handle for the directory to which the\n            object is to\
    \ be renamed.\n         to.name\n            The new name for the object. Refer\
    \ to General\n            comments on filenames on page 30.\n      If the directory,\
    \ to.dir, already contains an entry with\n      the name, to.name, the source\
    \ object must be compatible\n      with the target: either both are non-directories\
    \ or both\n      are directories and the target must be empty. If\n      compatible,\
    \ the existing target is removed before the\n      rename occurs. If they are\
    \ not compatible or if the target\n      is a directory but not empty, the server\
    \ should return the\n      error, NFS3ERR_EXIST.\n      On successful return,\
    \ RENAME3res.status is NFS3_OK and\n      RENAME3res.resok contains:\n      fromdir_wcc\n\
    \         Weak cache consistency data for the directory,\n         from.dir.\n\
    \      todir_wcc\n         Weak cache consistency data for the directory, to.dir.\n\
    \      Otherwise, RENAME3res.status contains the error on failure\n      and RENAME3res.resfail\
    \ contains the following:\n      fromdir_wcc\n         Weak cache consistency\
    \ data for the directory,\n         from.dir.\n      todir_wcc\n         Weak\
    \ cache consistency data for the directory, to.dir.\n   IMPLEMENTATION\n     \
    \ The RENAME operation must be atomic to the client. The\n      message \"to.dir\
    \ and from.dir must reside on the same file\n      system on the server, [or the\
    \ operation will fail]\" means\n      that the fsid fields in the attributes for\
    \ the directories\n      are the same. If they reside on different file systems,\n\
    \      the error, NFS3ERR_XDEV, is returned. Even though the\n      operation\
    \ is atomic, the status, NFS3ERR_MLINK, may be\n      returned if the server used\
    \ a \"unlink/link/unlink\"\n      sequence internally.\n      A file handle may\
    \ or may not become stale on a rename.\n      However, server implementors are\
    \ strongly encouraged to\n      attempt to keep file handles from becoming stale\
    \ in this\n      fashion.\n      On some servers, the filenames, \".\" and \"\
    ..\", are illegal\n      as either from.name or to.name. In addition, neither\n\
    \      from.name nor to.name can be an alias for from.dir. These\n      servers\
    \ will return the error, NFS3ERR_INVAL, in these\n      cases.\n      If from\
    \ and to both refer to the same file (they might\n      be hard links of each\
    \ other), then RENAME should perform\n      no action and return NFS3_OK.\n  \
    \    Refer to General comments on filenames on page 30.\n   ERRORS\n      NFS3ERR_NOENT\n\
    \      NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_EXIST\n      NFS3ERR_XDEV\n\
    \      NFS3ERR_NOTDIR\n      NFS3ERR_ISDIR\n      NFS3ERR_INVAL\n      NFS3ERR_NOSPC\n\
    \      NFS3ERR_ROFS\n      NFS3ERR_MLINK\n      NFS3ERR_NAMETOOLONG\n      NFS3ERR_NOTEMPTY\n\
    \      NFS3ERR_DQUOT\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n\
    \      NFS3ERR_SERVERFAULT\n   SEE ALSO\n   REMOVE and LINK.\n"
- title: '3.3.15 Procedure 15: LINK - Create Link to an object'
  contents:
  - "3.3.15 Procedure 15: LINK - Create Link to an object\n   SYNOPSIS\n      LINK3res\
    \ NFSPROC3_LINK(LINK3args) = 15;\n      struct LINK3args {\n           nfs_fh3\
    \     file;\n           diropargs3  link;\n      };\n      struct LINK3resok {\n\
    \           post_op_attr   file_attributes;\n           wcc_data       linkdir_wcc;\n\
    \      };\n      struct LINK3resfail {\n           post_op_attr   file_attributes;\n\
    \           wcc_data       linkdir_wcc;\n      };\n      union LINK3res switch\
    \ (nfsstat3 status) {\n      case NFS3_OK:\n           LINK3resok    resok;\n\
    \      default:\n           LINK3resfail  resfail;\n      };\n   DESCRIPTION\n\
    \      Procedure LINK creates a hard link from file to link.name,\n      in the\
    \ directory, link.dir. file and link.dir must reside\n      on the same file system\
    \ and server. On entry, the\n      arguments in LINK3args are:\n      file\n \
    \        The file handle for the existing file system object.\n      link\n  \
    \       The location of the link to be created:\n         link.dir\n         \
    \   The file handle for the directory in which the link\n            is to be\
    \ created.\n         link.name\n            The name that is to be associated\
    \ with the created\n            link. Refer to General comments on filenames on\
    \ page\n            17.\n      On successful return, LINK3res.status is NFS3_OK\
    \ and\n      LINK3res.resok contains:\n      file_attributes\n         The post-operation\
    \ attributes of the file system object\n         identified by file.\n      linkdir_wcc\n\
    \         Weak cache consistency data for the directory,\n         link.dir.\n\
    \      Otherwise, LINK3res.status contains the error on failure\n      and LINK3res.resfail\
    \ contains the following:\n      file_attributes\n         The post-operation\
    \ attributes of the file system object\n         identified by file.\n      linkdir_wcc\n\
    \         Weak cache consistency data for the directory,\n         link.dir.\n\
    \   IMPLEMENTATION\n      Changes to any property of the hard-linked files are\n\
    \      reflected in all of the linked files. When a hard link is\n      made to\
    \ a file, the attributes for the file should have a\n      value for nlink that\
    \ is one greater than the value before\n      the LINK.\n      The comments under\
    \ RENAME regarding object and target\n      residing on the same file system apply\
    \ here as well. The\n      comments regarding the target name applies as well.\
    \ Refer\n      to General comments on filenames on page 30.\n   ERRORS\n     \
    \ NFS3ERR_IO\n      NFS3ERR_ACCES\n      NFS3ERR_EXIST\n      NFS3ERR_XDEV\n \
    \     NFS3ERR_NOTDIR\n      NFS3ERR_INVAL\n      NFS3ERR_NOSPC\n      NFS3ERR_ROFS\n\
    \      NFS3ERR_MLINK\n      NFS3ERR_NAMETOOLONG\n      NFS3ERR_DQUOT\n      NFS3ERR_STALE\n\
    \      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n      NFS3ERR_SERVERFAULT\n \
    \  SEE ALSO\n      SYMLINK, RENAME and FSINFO.\n"
- title: '3.3.16 Procedure 16: READDIR - Read From Directory'
  contents:
  - "3.3.16 Procedure 16: READDIR - Read From Directory\n   SYNOPSIS\n      READDIR3res\
    \ NFSPROC3_READDIR(READDIR3args) = 16;\n      struct READDIR3args {\n        \
    \   nfs_fh3      dir;\n           cookie3      cookie;\n           cookieverf3\
    \  cookieverf;\n           count3       count;\n      };\n      struct entry3\
    \ {\n           fileid3      fileid;\n           filename3    name;\n        \
    \   cookie3      cookie;\n           entry3       *nextentry;\n      };\n    \
    \  struct dirlist3 {\n           entry3       *entries;\n           bool     \
    \    eof;\n      };\n      struct READDIR3resok {\n           post_op_attr dir_attributes;\n\
    \           cookieverf3  cookieverf;\n           dirlist3     reply;\n      };\n\
    \      struct READDIR3resfail {\n           post_op_attr dir_attributes;\n   \
    \   };\n      union READDIR3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           READDIR3resok   resok;\n      default:\n           READDIR3resfail\
    \ resfail;\n      };\n   DESCRIPTION\n      Procedure READDIR retrieves a variable\
    \ number of entries,\n      in sequence, from a directory and returns the name\
    \ and\n      file identifier for each, with information to allow the\n      client\
    \ to request additional directory entries in a\n      subsequent READDIR request.\
    \ On entry, the arguments in\n      READDIR3args are:\n      dir\n         The\
    \ file handle for the directory to be read.\n      cookie\n         This should\
    \ be set to 0 in the first request to read\n         the directory. On subsequent\
    \ requests, it should be a\n         cookie as returned by the server.\n     \
    \ cookieverf\n         This should be set to 0 in the first request to read\n\
    \         the directory. On subsequent requests, it should be a\n         cookieverf\
    \ as returned by the server. The cookieverf\n         must match that returned\
    \ by the READDIR in which the\n         cookie was acquired.\n      count\n  \
    \       The maximum size of the READDIR3resok structure, in\n         bytes. \
    \ The size must include all XDR overhead. The\n         server is free to return\
    \ less than count bytes of\n         data.\n      On successful return, READDIR3res.status\
    \ is NFS3_OK and\n      READDIR3res.resok contains:\n      dir_attributes\n  \
    \       The attributes of the directory, dir.\n      cookieverf\n         The\
    \ cookie verifier.\n      reply\n         The directory list:\n         entries\n\
    \            Zero or more directory (entry3) entries.\n         eof\n        \
    \    TRUE if the last member of reply.entries is the last\n            entry in\
    \ the directory or the list reply.entries is\n            empty and the cookie\
    \ corresponded to the end of the\n            directory. If FALSE, there may be\
    \ more entries to\n            read.\n      Otherwise, READDIR3res.status contains\
    \ the error on\n      failure and READDIR3res.resfail contains the following:\n\
    \      dir_attributes\n         The attributes of the directory, dir.\n   IMPLEMENTATION\n\
    \      In the NFS version 2 protocol, each directory entry\n      returned included\
    \ a cookie identifying a point in the\n      directory. By including this cookie\
    \ in a subsequent\n      READDIR, the client could resume the directory read at\
    \ any\n      point in the directory.  One problem with this scheme was\n     \
    \ that there was no easy way for a server to verify that a\n      cookie was valid.\
    \ If two READDIRs were separated by one or\n      more operations that changed\
    \ the directory in some way\n      (for example, reordering or compressing it),\
    \ it was\n      possible that the second READDIR could miss entries, or\n    \
    \  process entries more than once. If the cookie was no\n      longer usable,\
    \ for example, pointing into the middle of a\n      directory entry, the server\
    \ would have to either round the\n      cookie down to the cookie of the previous\
    \ entry or round\n      it up to the cookie of the next entry in the directory.\n\
    \      Either way would possibly lead to incorrect results and\n      the client\
    \ would be unaware that any problem existed.\n      In the NFS version 3 protocol,\
    \ each READDIR request\n      includes both a cookie and a cookie verifier. For\
    \ the\n      first call, both are set to 0.  The response includes a\n      new\
    \ cookie verifier, with a cookie per entry.  For\n      subsequent READDIRs, the\
    \ client must present both the\n      cookie and the corresponding cookie verifier.\
    \  If the\n      server detects that the cookie is no longer valid, the\n    \
    \  server will reject the READDIR request with the status,\n      NFS3ERR_BAD_COOKIE.\
    \ The client should be careful to\n      avoid holding directory entry cookies\
    \ across operations\n      that modify the directory contents, such as REMOVE\
    \ and\n      CREATE.\n      One implementation of the cookie-verifier mechanism\
    \ might\n      be for the server to use the modification time of the\n      directory.\
    \ This might be overly restrictive, however. A\n      better approach would be\
    \ to record the time of the last\n      directory modification that changed the\
    \ directory\n      organization in a way that would make it impossible to\n  \
    \    reliably interpret a cookie. Servers in which directory\n      cookies are\
    \ always valid are free to use zero as the\n      verifier always.\n      The\
    \ server may return fewer than count bytes of\n      XDR-encoded entries.  The\
    \ count specified by the client in\n      the request should be greater than or\
    \ equal to FSINFO\n      dtpref.\n      Since UNIX clients give a special meaning\
    \ to the fileid\n      value zero, UNIX clients should be careful to map zero\n\
    \      fileid values to some other value and servers should try\n      to avoid\
    \ sending a zero fileid.\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_ACCES\n \
    \     NFS3ERR_NOTDIR\n      NFS3ERR_BAD_COOKIE\n      NFS3ERR_TOOSMALL\n     \
    \ NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE ALSO\n\
    \      READDIRPLUS and FSINFO.\n"
- title: '3.3.17 Procedure 17: READDIRPLUS - Extended read from directory'
  contents:
  - "3.3.17 Procedure 17: READDIRPLUS - Extended read from directory\n   SYNOPSIS\n\
    \      READDIRPLUS3res NFSPROC3_READDIRPLUS(READDIRPLUS3args) = 17;\n      struct\
    \ READDIRPLUS3args {\n           nfs_fh3      dir;\n           cookie3      cookie;\n\
    \           cookieverf3  cookieverf;\n           count3       dircount;\n    \
    \       count3       maxcount;\n      };\n      struct entryplus3 {\n        \
    \   fileid3      fileid;\n           filename3    name;\n           cookie3  \
    \    cookie;\n           post_op_attr name_attributes;\n           post_op_fh3\
    \  name_handle;\n           entryplus3   *nextentry;\n      };\n      struct dirlistplus3\
    \ {\n           entryplus3   *entries;\n           bool         eof;\n      };\n\
    \      struct READDIRPLUS3resok {\n           post_op_attr dir_attributes;\n \
    \          cookieverf3  cookieverf;\n           dirlistplus3 reply;\n      };\n\
    \      struct READDIRPLUS3resfail {\n           post_op_attr dir_attributes;\n\
    \      };\n      union READDIRPLUS3res switch (nfsstat3 status) {\n      case\
    \ NFS3_OK:\n           READDIRPLUS3resok   resok;\n      default:\n          \
    \ READDIRPLUS3resfail resfail;\n      };\n   DESCRIPTION\n      Procedure READDIRPLUS\
    \ retrieves a variable number of\n      entries from a file system directory and\
    \ returns complete\n      information about each along with information to allow\
    \ the\n      client to request additional directory entries in a\n      subsequent\
    \ READDIRPLUS.  READDIRPLUS differs from READDIR\n      only in the amount of\
    \ information returned for each\n      entry.  In READDIR, each entry returns\
    \ the filename and\n      the fileid.  In READDIRPLUS, each entry returns the\
    \ name,\n      the fileid, attributes (including the fileid), and file\n     \
    \ handle. On entry, the arguments in READDIRPLUS3args are:\n      dir\n      \
    \   The file handle for the directory to be read.\n      cookie\n         This\
    \ should be set to 0 on the first request to read a\n         directory. On subsequent\
    \ requests, it should be a\n         cookie as returned by the server.\n     \
    \ cookieverf\n         This should be set to 0 on the first request to read a\n\
    \         directory. On subsequent requests, it should be a\n         cookieverf\
    \ as returned by the server. The cookieverf\n         must match that returned\
    \ by the READDIRPLUS call in\n         which the cookie was acquired.\n      dircount\n\
    \         The maximum number of bytes of directory information\n         returned.\
    \ This number should not include the size of\n         the attributes and file\
    \ handle portions of the result.\n      maxcount\n         The maximum size of\
    \ the READDIRPLUS3resok structure, in\n         bytes. The size must include all\
    \ XDR overhead. The\n         server is free to return fewer than maxcount bytes\
    \ of\n         data.\n      On successful return, READDIRPLUS3res.status is NFS3_OK\n\
    \      and READDIRPLUS3res.resok contains:\n      dir_attributes\n         The\
    \ attributes of the directory, dir.\n      cookieverf\n         The cookie verifier.\n\
    \      reply\n         The directory list:\n         entries\n            Zero\
    \ or more directory (entryplus3) entries.\n         eof\n            TRUE if the\
    \ last member of reply.entries is the last\n            entry in the directory\
    \ or the list reply.entries is\n            empty and the cookie corresponded\
    \ to the end of the\n            directory. If FALSE, there may be more entries\
    \ to\n            read.\n      Otherwise, READDIRPLUS3res.status contains the\
    \ error on\n      failure and READDIRPLUS3res.resfail contains the following:\n\
    \      dir_attributes\n         The attributes of the directory, dir.\n   IMPLEMENTATION\n\
    \      Issues that need to be understood for this procedure\n      include increased\
    \ cache flushing activity on the client\n      (as new file handles are returned\
    \ with names which are\n      entered into caches) and over-the-wire overhead\
    \ versus\n      expected subsequent LOOKUP elimination. It is thought that\n \
    \     this procedure may improve performance for directory\n      browsing where\
    \ attributes are always required as on the\n      Apple Macintosh operating system\
    \ and for MS-DOS.\n      The dircount and maxcount fields are included as an\n\
    \      optimization.  Consider a READDIRPLUS call on a UNIX\n      operating system\
    \ implementation for 1048 bytes; the reply\n      does not contain many entries\
    \ because of the overhead due\n      to attributes and file handles. An alternative\
    \ is to issue\n      a READDIRPLUS call for 8192 bytes and then only use the\n\
    \      first 1048 bytes of directory information. However, the\n      server doesn't\
    \ know that all that is needed is 1048 bytes\n      of directory information (as\
    \ would be returned by\n      READDIR). It sees the 8192 byte request and issues\
    \ a\n      VOP_READDIR for 8192 bytes. It then steps through all of\n      those\
    \ directory entries, obtaining attributes and file\n      handles for each entry.\
    \  When it encodes the result, the\n      server only encodes until it gets 8192\
    \ bytes of results\n      which include the attributes and file handles. Thus,\
    \ it\n      has done a larger VOP_READDIR and many more attribute\n      fetches\
    \ than it needed to. The ratio of the directory\n      entry size to the size\
    \ of the attributes plus the size of\n      the file handle is usually at least\
    \ 8 to 1. The server has\n      done much more work than it needed to.\n     \
    \ The solution to this problem is for the client to provide\n      two counts\
    \ to the server. The first is the number of bytes\n      of directory information\
    \ that the client really wants,\n      dircount.  The second is the maximum number\
    \ of bytes in\n      the result, including the attributes and file handles,\n\
    \      maxcount. Thus, the server will issue a VOP_READDIR for\n      only the\
    \ number of bytes that the client really wants to\n      get, not an inflated\
    \ number.  This should help to reduce\n      the size of VOP_READDIR requests\
    \ on the server, thus\n      reducing the amount of work done there, and to reduce\
    \ the\n      number of VOP_LOOKUP, VOP_GETATTR, and other calls done by\n    \
    \  the server to construct attributes and file handles.\n   ERRORS\n      NFS3ERR_IO\n\
    \      NFS3ERR_ACCES\n      NFS3ERR_NOTDIR\n      NFS3ERR_BAD_COOKIE\n      NFS3ERR_TOOSMALL\n\
    \      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_NOTSUPP\n      NFS3ERR_SERVERFAULT\n\
    \   SEE ALSO\n      READDIR.\n"
- title: '3.3.18 Procedure 18: FSSTAT - Get dynamic file system information'
  contents:
  - "3.3.18 Procedure 18: FSSTAT - Get dynamic file system information\n   SYNOPSIS\n\
    \      FSSTAT3res NFSPROC3_FSSTAT(FSSTAT3args) = 18;\n      struct FSSTAT3args\
    \ {\n           nfs_fh3   fsroot;\n      };\n      struct FSSTAT3resok {\n   \
    \        post_op_attr obj_attributes;\n           size3        tbytes;\n     \
    \      size3        fbytes;\n           size3        abytes;\n           size3\
    \        tfiles;\n           size3        ffiles;\n           size3        afiles;\n\
    \           uint32       invarsec;\n      };\n      struct FSSTAT3resfail {\n\
    \           post_op_attr obj_attributes;\n      };\n      union FSSTAT3res switch\
    \ (nfsstat3 status) {\n      case NFS3_OK:\n           FSSTAT3resok   resok;\n\
    \      default:\n           FSSTAT3resfail resfail;\n      };\n   DESCRIPTION\n\
    \      Procedure FSSTAT retrieves volatile file system state\n      information.\
    \ On entry, the arguments in FSSTAT3args are:\n      fsroot\n         A file handle\
    \ identifying a object in the file system.\n         This is normally a file handle\
    \ for a mount point for a\n         file system, as originally obtained from the\
    \ MOUNT\n         service on the server.\n      On successful return, FSSTAT3res.status\
    \ is NFS3_OK and\n      FSSTAT3res.resok contains:\n      obj_attributes\n   \
    \      The attributes of the file system object specified in\n         fsroot.\n\
    \      tbytes\n         The total size, in bytes, of the file system.\n      fbytes\n\
    \         The amount of free space, in bytes, in the file\n         system.\n\
    \      abytes\n         The amount of free space, in bytes, available to the\n\
    \         user identified by the authentication information in\n         the RPC.\
    \  (This reflects space that is reserved by the\n         file system; it does\
    \ not reflect any quota system\n         implemented by the server.)\n      tfiles\n\
    \         The total number of file slots in the file system. (On\n         a UNIX\
    \ server, this often corresponds to the number of\n         inodes configured.)\n\
    \      ffiles\n         The number of free file slots in the file system.\n  \
    \    afiles\n         The number of free file slots that are available to the\n\
    \         user corresponding to the authentication information in\n         the\
    \ RPC.  (This reflects slots that are reserved by the\n         file system; it\
    \ does not reflect any quota system\n         implemented by the server.)\n  \
    \    invarsec\n         A measure of file system volatility: this is the number\n\
    \         of seconds for which the file system is not expected to\n         change.\
    \ For a volatile, frequently updated file system,\n         this will be 0. For\
    \ an immutable file system, such as a\n         CD-ROM, this would be the largest\
    \ unsigned integer. For\n         file systems that are infrequently modified,\
    \ for\n         example, one containing local executable programs and\n      \
    \   on-line documentation, a value corresponding to a few\n         hours or days\
    \ might be used. The client may use this as\n         a hint in tuning its cache\
    \ management. Note however,\n         this measure is assumed to be dynamic and\
    \ may change at\n         any time.\n      Otherwise, FSSTAT3res.status contains\
    \ the error on failure\n      and FSSTAT3res.resfail contains the following:\n\
    \      obj_attributes\n         The attributes of the file system object specified\
    \ in\n         fsroot.\n   IMPLEMENTATION\n      Not all implementations can support\
    \ the entire list of\n      attributes. It is expected that servers will make\
    \ a best\n      effort at supporting all the attributes.\n   ERRORS\n      NFS3ERR_IO\n\
    \      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE\
    \ ALSO\n      FSINFO.\n"
- title: '3.3.19 Procedure 19: FSINFO - Get static file system Information'
  contents:
  - "3.3.19 Procedure 19: FSINFO - Get static file system Information\n   SYNOPSIS\n\
    \      FSINFO3res NFSPROC3_FSINFO(FSINFO3args) = 19;\n      const FSF3_LINK  \
    \      = 0x0001;\n      const FSF3_SYMLINK     = 0x0002;\n      const FSF3_HOMOGENEOUS\
    \ = 0x0008;\n      const FSF3_CANSETTIME  = 0x0010;\n      struct FSINFOargs {\n\
    \           nfs_fh3   fsroot;\n      };\n      struct FSINFO3resok {\n       \
    \    post_op_attr obj_attributes;\n           uint32       rtmax;\n          \
    \ uint32       rtpref;\n           uint32       rtmult;\n           uint32   \
    \    wtmax;\n           uint32       wtpref;\n           uint32       wtmult;\n\
    \           uint32       dtpref;\n           size3        maxfilesize;\n     \
    \      nfstime3     time_delta;\n           uint32       properties;\n      };\n\
    \      struct FSINFO3resfail {\n           post_op_attr obj_attributes;\n    \
    \  };\n      union FSINFO3res switch (nfsstat3 status) {\n      case NFS3_OK:\n\
    \           FSINFO3resok   resok;\n      default:\n           FSINFO3resfail resfail;\n\
    \      };\n   DESCRIPTION\n      Procedure FSINFO retrieves nonvolatile file system\
    \ state\n      information and general information about the NFS version\n   \
    \   3 protocol server implementation. On entry, the arguments\n      in FSINFO3args\
    \ are:\n      fsroot\n         A file handle identifying a file object. Normal\
    \ usage\n         is to provide a file handle for a mount point for a\n      \
    \   file system, as originally obtained from the MOUNT\n         service on the\
    \ server.\n      On successful return, FSINFO3res.status is NFS3_OK and\n    \
    \  FSINFO3res.resok contains:\n      obj_attributes\n         The attributes of\
    \ the file system object specified in\n         fsroot.\n      rtmax\n       \
    \  The maximum size in bytes of a READ request supported\n         by the server.\
    \ Any READ with a number greater than\n         rtmax will result in a short read\
    \ of rtmax bytes or\n         less.\n      rtpref\n         The preferred size\
    \ of a READ request. This should be\n         the same as rtmax unless there is\
    \ a clear benefit in\n         performance or efficiency.\n      rtmult\n    \
    \     The suggested multiple for the size of a READ request.\n      wtmax\n  \
    \       The maximum size of a WRITE request supported by the\n         server.\
    \  In general, the client is limited by wtmax\n         since there is no guarantee\
    \ that a server can handle a\n         larger write. Any WRITE with a count greater\
    \ than wtmax\n         will result in a short write of at most wtmax bytes.\n\
    \      wtpref\n         The preferred size of a WRITE request. This should be\n\
    \         the same as wtmax unless there is a clear benefit in\n         performance\
    \ or efficiency.\n      wtmult\n         The suggested multiple for the size of\
    \ a WRITE\n         request.\n      dtpref\n         The preferred size of a READDIR\
    \ request.\n      maxfilesize\n         The maximum size of a file on the file\
    \ system.\n      time_delta\n         The server time granularity. When setting\
    \ a file time\n         using SETATTR, the server guarantees only to preserve\n\
    \         times to this accuracy. If this is {0, 1}, the server\n         can\
    \ support nanosecond times, {0, 1000000} denotes\n         millisecond precision,\
    \ and {1, 0} indicates that times\n         are accurate only to the nearest second.\n\
    \      properties\n         A bit mask of file system properties. The following\n\
    \         values are defined:\n         FSF_LINK\n            If this bit is 1\
    \ (TRUE), the file system supports\n            hard links.\n         FSF_SYMLINK\n\
    \            If this bit is 1 (TRUE), the file system supports\n            symbolic\
    \ links.\n         FSF_HOMOGENEOUS\n            If this bit is 1 (TRUE), the information\
    \ returned by\n            PATHCONF is identical for every file and directory\n\
    \            in the file system. If it is 0 (FALSE), the client\n            should\
    \ retrieve PATHCONF information for each file\n            and directory as required.\n\
    \         FSF_CANSETTIME\n            If this bit is 1 (TRUE), the server will\
    \ set the\n            times for a file via SETATTR if requested (to the\n   \
    \         accuracy indicated by time_delta). If it is 0\n            (FALSE),\
    \ the server cannot set times as requested.\n      Otherwise, FSINFO3res.status\
    \ contains the error on failure\n      and FSINFO3res.resfail contains the following:\n\
    \      attributes\n         The attributes of the file system object specified\
    \ in\n         fsroot.\n   IMPLEMENTATION\n      Not all implementations can support\
    \ the entire list of\n      attributes. It is expected that a server will make\
    \ a best\n      effort at supporting all the attributes.\n      The file handle\
    \ provided is expected to be the file handle\n      of the file system root, as\
    \ returned to the MOUNT\n      operation.  Since mounts may occur anywhere within\
    \ an\n      exported tree, the server should expect FSINFO requests\n      specifying\
    \ file handles within the exported file system.\n      A server may export different\
    \ types of file systems with\n      different attributes returned to the FSINFO\
    \ call. The\n      client should retrieve FSINFO information for each mount\n\
    \      completed. Though a server may return different FSINFO\n      information\
    \ for different files within a file system,\n      there is no requirement that\
    \ a client obtain FSINFO\n      information for other than the file handle returned\
    \ at\n      mount.\n      The maxfilesize field determines whether a server's\n\
    \      particular file system uses 32 bit sizes and offsets or 64\n      bit file\
    \ sizes and offsets. This may affect a client's\n      processing.\n      The\
    \ preferred sizes for requests are nominally tied to an\n      exported file system\
    \ mounted by a client. A surmountable\n      issue arises in that the transfer\
    \ size for an NFS version\n      3 protocol request is not only dependent on\n\
    \      characteristics of the file system but also on\n      characteristics of\
    \ the network interface, particularly the\n      maximum transfer unit (MTU).\
    \ A server implementation can\n      advertise different transfer sizes (for the\
    \ fields, rtmax,\n      rtpref, wtmax, wtpref, and dtpref) depending on the\n\
    \      interface on which the FSINFO request is received. This is\n      an implementation\
    \ issue.\n   ERRORS\n      NFS3ERR_STALE\n      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n\
    \   SEE ALSO\n      READLINK, WRITE, READDIR, FSSTAT and PATHCONF.\n"
- title: '3.3.20 Procedure 20: PATHCONF - Retrieve POSIX information'
  contents:
  - "3.3.20 Procedure 20: PATHCONF - Retrieve POSIX information\n   SYNOPSIS\n   \
    \   PATHCONF3res NFSPROC3_PATHCONF(PATHCONF3args) = 20;\n      struct PATHCONF3args\
    \ {\n           nfs_fh3   object;\n      };\n      struct PATHCONF3resok {\n \
    \          post_op_attr obj_attributes;\n           uint32       linkmax;\n  \
    \         uint32       name_max;\n           bool         no_trunc;\n        \
    \   bool         chown_restricted;\n           bool         case_insensitive;\n\
    \           bool         case_preserving;\n      };\n      struct PATHCONF3resfail\
    \ {\n           post_op_attr obj_attributes;\n      };\n      union PATHCONF3res\
    \ switch (nfsstat3 status) {\n      case NFS3_OK:\n           PATHCONF3resok \
    \  resok;\n      default:\n           PATHCONF3resfail resfail;\n      };\n  \
    \ DESCRIPTION\n      Procedure PATHCONF retrieves the pathconf information for\n\
    \      a file or directory. If the FSF_HOMOGENEOUS bit is set in\n      FSFINFO3resok.properties,\
    \ the pathconf information will be\n      the same for all files and directories\
    \ in the exported\n      file system in which this file or directory resides.\
    \ On\n      entry, the arguments in PATHCONF3args are:\n      object\n       \
    \  The file handle for the file system object.\n      On successful return, PATHCONF3res.status\
    \ is NFS3_OK and\n      PATHCONF3res.resok contains:\n      obj_attributes\n \
    \        The attributes of the object specified by object.\n      linkmax\n  \
    \       The maximum number of hard links to an object.\n      name_max\n     \
    \    The maximum length of a component of a filename.\n      no_trunc\n      \
    \   If TRUE, the server will reject any request that\n         includes a name\
    \ longer than name_max with the error,\n         NFS3ERR_NAMETOOLONG. If FALSE,\
    \ any length name over\n         name_max bytes will be silently truncated to\
    \ name_max\n         bytes.\n      chown_restricted\n         If TRUE, the server\
    \ will reject any request to change\n         either the owner or the group associated\
    \ with a file if\n         the caller is not the privileged user. (Uid 0.)\n \
    \     case_insensitive\n         If TRUE, the server file system does not distinguish\n\
    \         case when interpreting filenames.\n      case_preserving\n         If\
    \ TRUE, the server file system will preserve the case\n         of a name during\
    \ a CREATE, MKDIR, MKNOD, SYMLINK,\n         RENAME, or LINK operation.\n    \
    \  Otherwise, PATHCONF3res.status contains the error on\n      failure and PATHCONF3res.resfail\
    \ contains the following:\n      obj_attributes\n         The attributes of the\
    \ object specified by object.\n   IMPLEMENTATION\n      In some implementations\
    \ of the NFS version 2 protocol,\n      pathconf information was obtained at mount\
    \ time through\n      the MOUNT protocol.  The proper place to obtain it, is as\n\
    \      here, in the NFS version 3 protocol itself.\n   ERRORS\n      NFS3ERR_STALE\n\
    \      NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      LOOKUP,\
    \ CREATE, MKDIR, SYMLINK, MKNOD, RENAME, LINK and FSINFO.\n"
- title: '3.3.21 Procedure 21: COMMIT - Commit cached data on a server to stable'
  contents:
  - "3.3.21 Procedure 21: COMMIT - Commit cached data on a server to stable\n    \
    \   storage\n   SYNOPSIS\n      COMMIT3res NFSPROC3_COMMIT(COMMIT3args) = 21;\n\
    \      struct COMMIT3args {\n           nfs_fh3    file;\n           offset3 \
    \   offset;\n           count3     count;\n      };\n      struct COMMIT3resok\
    \ {\n           wcc_data   file_wcc;\n           writeverf3 verf;\n      };\n\
    \      struct COMMIT3resfail {\n           wcc_data   file_wcc;\n      };\n  \
    \    union COMMIT3res switch (nfsstat3 status) {\n      case NFS3_OK:\n      \
    \     COMMIT3resok   resok;\n      default:\n           COMMIT3resfail resfail;\n\
    \      };\n   DESCRIPTION\n      Procedure COMMIT forces or flushes data to stable\
    \ storage\n      that was previously written with a WRITE procedure call\n   \
    \   with the stable field set to UNSTABLE. On entry, the\n      arguments in COMMIT3args\
    \ are:\n      file\n         The file handle for the file to which data is to\
    \ be\n         flushed (committed). This must identify a file system\n       \
    \  object of type, NF3REG.\n      offset\n         The position within the file\
    \ at which the flush is to\n         begin.  An offset of 0 means to flush data\
    \ starting at\n         the beginning of the file.\n      count\n         The\
    \ number of bytes of data to flush. If count is 0, a\n         flush from offset\
    \ to the end of file is done.\n      On successful return, COMMIT3res.status is\
    \ NFS3_OK and\n      COMMIT3res.resok contains:\n      file_wcc\n         Weak\
    \ cache consistency data for the file. For a client\n         that requires only\
    \ the post-operation file attributes,\n         these can be found in file_wcc.after.\n\
    \      verf\n         This is a cookie that the client can use to determine\n\
    \         whether the server has rebooted between a call to WRITE\n         and\
    \ a subsequent call to COMMIT. This cookie must be\n         consistent during\
    \ a single boot session and must be\n         unique between instances of the\
    \ NFS version 3 protocol\n         server where uncommitted data may be lost.\n\
    \      Otherwise, COMMIT3res.status contains the error on failure\n      and COMMIT3res.resfail\
    \ contains the following:\n      file_wcc\n         Weak cache consistency data\
    \ for the file. For a client\n         that requires only the post-write file\
    \ attributes,\n         these can be found in file_wcc.after. Even though the\n\
    \         COMMIT failed, full wcc_data is returned to allow the\n         client\
    \ to determine whether the file changed on the\n         server between calls\
    \ to WRITE and COMMIT.\n   IMPLEMENTATION\n      Procedure COMMIT is similar in\
    \ operation and semantics to\n      the POSIX fsync(2) system call that synchronizes\
    \ a file's\n      state with the disk, that is it flushes the file's data\n  \
    \    and metadata to disk. COMMIT performs the same operation\n      for a client,\
    \ flushing any unsynchronized data and\n      metadata on the server to the server's\
    \ disk for the\n      specified file. Like fsync(2), it may be that there is\n\
    \      some modified data or no modified data to synchronize. The\n      data\
    \ may have been synchronized by the server's normal\n      periodic buffer synchronization\
    \ activity. COMMIT will\n      always return NFS3_OK, unless there has been an\
    \ unexpected\n      error.\n      COMMIT differs from fsync(2) in that it is possible\
    \ for\n      the client to flush a range of the file (most likely\n      triggered\
    \ by a buffer-reclamation scheme on the client\n      before file has been completely\
    \ written).\n      The server implementation of COMMIT is reasonably simple.\n\
    \      If the server receives a full file COMMIT request, that is\n      starting\
    \ at offset 0 and count 0, it should do the\n      equivalent of fsync()'ing the\
    \ file. Otherwise, it should\n      arrange to have the cached data in the range\
    \ specified by\n      offset and count to be flushed to stable storage.  In both\n\
    \      cases, any metadata associated with the file must be\n      flushed to\
    \ stable storage before returning. It is not an\n      error for there to be nothing\
    \ to flush on the server.\n      This means that the data and metadata that needed\
    \ to be\n      flushed have already been flushed or lost during the last\n   \
    \   server failure.\n      The client implementation of COMMIT is a little more\n\
    \      complex.  There are two reasons for wanting to commit a\n      client buffer\
    \ to stable storage. The first is that the\n      client wants to reuse a buffer.\
    \ In this case, the offset\n      and count of the buffer are sent to the server\
    \ in the\n      COMMIT request. The server then flushes any cached data\n    \
    \  based on the offset and count, and flushes any metadata\n      associated with\
    \ the file. It then returns the status of\n      the flush and the verf verifier.\
    \  The other reason for the\n      client to generate a COMMIT is for a full file\
    \ flush, such\n      as may be done at close. In this case, the client would\n\
    \      gather all of the buffers for this file that contain\n      uncommitted\
    \ data, do the COMMIT operation with an offset\n      of 0 and count of 0, and\
    \ then free all of those buffers.\n      Any other dirty buffers would be sent\
    \ to the server in the\n      normal fashion.\n      This implementation will\
    \ require some modifications to the\n      buffer cache on the client. After a\
    \ buffer is written with\n      stable UNSTABLE, it must be considered as dirty\
    \ by the\n      client system until it is either flushed via a COMMIT\n      operation\
    \ or written via a WRITE operation with stable set\n      to FILE_SYNC or DATA_SYNC.\
    \ This is done to prevent the\n      buffer from being freed and reused before\
    \ the data can be\n      flushed to stable storage on the server.\n      When\
    \ a response comes back from either a WRITE or a COMMIT\n      operation that\
    \ contains an unexpected verf, the client\n      will need to retransmit all of\
    \ the buffers containing\n      uncommitted cached data to the server.  How this\
    \ is to be\n      done is up to the implementor. If there is only one buffer\n\
    \      of interest, then it should probably be sent back over in\n      a WRITE\
    \ request with the appropriate stable flag. If there\n      more than one, it\
    \ might be worthwhile retransmitting all\n      of the buffers in WRITE requests\
    \ with stable set to\n      UNSTABLE and then retransmitting the COMMIT operation\
    \ to\n      flush all of the data on the server to stable storage. The\n     \
    \ timing of these retransmissions is left to the\n      implementor.\n      The\
    \ above description applies to page-cache-based systems\n      as well as buffer-cache-based\
    \ systems. In those systems,\n      the virtual memory system will need to be\
    \ modified instead\n      of the buffer cache.\n      See additional comments\
    \ on WRITE on page 49.\n   ERRORS\n      NFS3ERR_IO\n      NFS3ERR_STALE\n   \
    \   NFS3ERR_BADHANDLE\n      NFS3ERR_SERVERFAULT\n   SEE ALSO\n      WRITE.\n"
- title: 4. Implementation issues
  contents:
  - "4. Implementation issues\n   The NFS version 3 protocol was designed to allow\
    \ different\n   operating systems to share files. However, since it was\n   designed\
    \ in a UNIX environment, many operations have\n   semantics similar to the operations\
    \ of the UNIX file system.\n   This section discusses some of the general\n  \
    \ implementation-specific details and semantic issues.\n   Procedure descriptions\
    \ have implementation comments specific\n   to that procedure.\n   A number of\
    \ papers have been written describing issues\n   encountered when constructing\
    \ an NFS version 2 protocol\n   implementation. The best overview paper is still\
    \ [Sandberg].\n   [Israel], [Macklem], and [Pawlowski] describe other\n   implementations.\
    \ [X/OpenNFS] provides a complete description\n   of the NFS version 2 protocol\
    \ and supporting protocols, as\n   well as a discussion on implementation issues\
    \ and procedure\n   and error semantics. Many of the issues encountered when\n\
    \   constructing an NFS version 2 protocol implementation will be\n   encountered\
    \ when constructing an NFS version 3 protocol\n   implementation.\n"
- title: 4.1 Multiple version support
  contents:
  - "4.1 Multiple version support\n   The RPC protocol provides explicit support for\
    \ versioning of\n   a service. Client and server implementations of NFS version\
    \ 3\n   protocol should support both versions, for full backwards\n   compatibility,\
    \ when possible. Default behavior of the RPC\n   binding protocol is the client\
    \ and server bind using the\n   highest version number they both support. Client\
    \ or server\n   implementations that cannot easily support both versions (for\n\
    \   example, because of memory restrictions) will have to choose\n   what version\
    \ to support. The NFS version 2 protocol would be\n   a safe choice since fully\
    \ capable clients and servers should\n   support both versions. However, this\
    \ choice would need to be\n   made keeping all requirements in mind.\n"
- title: 4.2 Server/client relationship
  contents:
  - "4.2 Server/client relationship\n   The NFS version 3 protocol is designed to\
    \ allow servers to be\n   as simple and general as possible. Sometimes the simplicity\n\
    \   of the server can be a problem, if the client implements\n   complicated file\
    \ system semantics.\n   For example, some operating systems allow removal of open\n\
    \   files.  A process can open a file and, while it is open,\n   remove it from\
    \ the directory. The file can be read and\n   written as long as the process keeps\
    \ it open, even though the\n   file has no name in the file system.  It is impossible\
    \ for a\n   stateless server to implement these semantics.  The client\n   can\
    \ do some tricks such as renaming the file on remove (to a\n   hidden name), and\
    \ only physically deleting it on close. The\n   NFS version 3 protocol provides\
    \ sufficient functionality to\n   implement most file system semantics on a client.\n\
    \   Every NFS version 3 protocol client can also potentially be a\n   server,\
    \ and remote and local mounted file systems can be\n   freely mixed. This leads\
    \ to some problems when a client\n   travels down the directory tree of a remote\
    \ file system and\n   reaches the mount point on the server for another remote\
    \ file\n   system. Allowing the server to follow the second remote mount\n   would\
    \ require loop detection, server lookup, and user\n   revalidation. Instead, both\
    \ NFS version 2 protocol and NFS\n   version 3 protocol implementations do not\
    \ typically let\n   clients cross a server's mount point. When a client does a\n\
    \   LOOKUP on a directory on which the server has mounted a file\n   system, the\
    \ client sees the underlying directory instead of\n   the mounted directory.\n\
    \   For example, if a server has a file system called /usr and\n   mounts another\
    \ file system on /usr/src, if a client mounts\n   /usr, it does not see the mounted\
    \ version of /usr/src. A\n   client could do remote mounts that match the server's\
    \ mount\n   points to maintain the server's view.  In this example, the\n   client\
    \ would also have to mount /usr/src in addition to /usr,\n   even if they are\
    \ from the same server.\n"
- title: 4.3 Path name interpretation
  contents:
  - "4.3 Path name interpretation\n   There are a few complications to the rule that\
    \ path names are\n   always parsed on the client. For example, symbolic links\n\
    \   could have different interpretations on different clients.\n   There is no\
    \ answer to this problem in this specification.\n   Another common problem for\
    \ non-UNIX implementations is the\n   special interpretation of the pathname,\
    \ \"..\", to mean the\n   parent of a given directory. A future revision of the\n\
    \   protocol may use an explicit flag to indicate the parent\n   instead - however\
    \ it is not a problem as many working\n   non-UNIX implementations exist.\n"
- title: 4.4 Permission issues
  contents:
  - "4.4 Permission issues\n   The NFS version 3 protocol, strictly speaking, does\
    \ not\n   define the permission checking used by servers. However, it\n   is expected\
    \ that a server will do normal operating system\n   permission checking using\
    \ AUTH_UNIX style authentication as\n   the basis of its protection mechanism,\
    \ or another stronger\n   form of authentication such as AUTH_DES or AUTH_KERB.\
    \ With\n   AUTH_UNIX authentication, the server gets the client's\n   effective\
    \ uid, effective gid, and groups on each call and\n   uses them to check permission.\
    \ These are the so-called UNIX\n   credentials. AUTH_DES and AUTH_KERB use a network\
    \ name, or\n   netname, as the basis for identification (from which a UNIX\n \
    \  server derives the necessary standard UNIX credentials).\n   There are problems\
    \ with this method that have been solved.\n   Using uid and gid implies that the\
    \ client and server share\n   the same uid list. Every server and client pair\
    \ must have the\n   same mapping from user to uid and from group to gid. Since\n\
    \   every client can also be a server, this tends to imply that\n   the whole\
    \ network shares the same uid/gid space. If this is\n   not the case, then it\
    \ usually falls upon the server to\n   perform some custom mapping of credentials\
    \ from one\n   authentication domain into another. A discussion of\n   techniques\
    \ for managing a shared user space or for providing\n   mechanisms for user ID\
    \ mapping is beyond the scope of this\n   specification.\n   Another problem arises\
    \ due to the usually stateful open\n   operation.  Most operating systems check\
    \ permission at open\n   time, and then check that the file is open on each read\
    \ and\n   write request. With stateless servers, the server cannot\n   detect\
    \ that the file is open and must do permission checking\n   on each read and write\
    \ call. UNIX client semantics of access\n   permission checking on open can be\
    \ provided with the ACCESS\n   procedure call in this revision, which allows a\
    \ client to\n   explicitly check access permissions without resorting to\n   trying\
    \ the operation. On a local file system, a user can open\n   a file and then change\
    \ the permissions so that no one is\n   allowed to touch it, but will still be\
    \ able to write to the\n   file because it is open. On a remote file system, by\n\
    \   contrast, the write would fail. To get around this problem,\n   the server's\
    \ permission checking algorithm should allow the\n   owner of a file to access\
    \ it regardless of the permission\n   setting. This is needed in a practical NFS\
    \ version 3 protocol\n   server implementation, but it does depart from correct\
    \ local\n   file system semantics. This should not affect the return\n   result\
    \ of access permissions as returned by the ACCESS\n   procedure, however.\n  \
    \ A similar problem has to do with paging in an executable\n   program over the\
    \ network. The operating system usually checks\n   for execute permission before\
    \ opening a file for demand\n   paging, and then reads blocks from the open file.\
    \ In a local\n   UNIX file system, an executable file does not need read\n   permission\
    \ to execute (pagein). An NFS version 3 protocol\n   server can not tell the difference\
    \ between a normal file read\n   (where the read permission bit is meaningful)\
    \ and a demand\n   pagein read (where the server should allow access to the\n\
    \   executable file if the execute bit is set for that user or\n   group or public).\
    \ To make this work, the server allows\n   reading of files if the uid given in\
    \ the call has either\n   execute or read permission on the file, through ownership,\n\
    \   group membership or public access. Again, this departs from\n   correct local\
    \ file system semantics.\n   In most operating systems, a particular user (on\
    \ UNIX, the\n   uid 0) has access to all files, no matter what permission and\n\
    \   ownership they have. This superuser permission may not be\n   allowed on the\
    \ server, since anyone who can become superuser\n   on their client could gain\
    \ access to all remote files. A UNIX\n   server by default maps uid 0 to a distinguished\
    \ value\n   (UID_NOBODY), as well as mapping the groups list, before\n   doing\
    \ its access checking. A server implementation may\n   provide a mechanism to\
    \ change this mapping. This works except\n   for NFS version 3 protocol root file\
    \ systems (required for\n   diskless NFS version 3 protocol client support), where\n\
    \   superuser access cannot be avoided.  Export options are used,\n   on the server,\
    \ to restrict the set of clients allowed\n   superuser access.\n"
- title: 4.5 Duplicate request cache
  contents:
  - "4.5 Duplicate request cache\n   The typical NFS version 3 protocol failure recovery\
    \ model\n   uses client time-out and retry to handle server crashes,\n   network\
    \ partitions, and lost server replies. A retried\n   request is called a duplicate\
    \ of the original.\n   When used in a file server context, the term idempotent\
    \ can\n   be used to distinguish between operation types. An idempotent\n   request\
    \ is one that a server can perform more than once with\n   equivalent results\
    \ (though it may in fact change, as a side\n   effect, the access time on a file,\
    \ say for READ). Some NFS\n   operations are obviously non-idempotent. They cannot\
    \ be\n   reprocessed without special attention simply because they may\n   fail\
    \ if tried a second time. The CREATE request, for example,\n   can be used to\
    \ create a file for which the owner does not\n   have write permission. A duplicate\
    \ of this request cannot\n   succeed if the original succeeded. Likewise, a file\
    \ can be\n   removed only once.\n   The side effects caused by performing a duplicate\n\
    \   non-idempotent request can be destructive (for example, a\n   truncate operation\
    \ causing lost writes). The combination of a\n   stateless design with the common\
    \ choice of an unreliable\n   network transport (UDP) implies the possibility\
    \ of\n   destructive replays of non-idempotent requests. Though to be\n   more\
    \ accurate, it is the inherent stateless design of the NFS\n   version 3 protocol\
    \ on top of an unreliable RPC mechanism that\n   yields the possibility of destructive\
    \ replays of\n   non-idempotent requests, since even in an implementation of\n\
    \   the NFS version 3 protocol over a reliable\n   connection-oriented transport,\
    \ a connection break with\n   automatic reestablishment requires duplicate request\n\
    \   processing (the client will retransmit the request, and the\n   server needs\
    \ to deal with a potential duplicate\n   non-idempotent request).\n   Most NFS\
    \ version 3 protocol server implementations use a\n   cache of recent requests\
    \ (called the duplicate request cache)\n   for the processing of duplicate non-idempotent\
    \ requests. The\n   duplicate request cache provides a short-term memory\n   mechanism\
    \ in which the original completion status of a\n   request is remembered and the\
    \ operation attempted only once.\n   If a duplicate copy of this request is received,\
    \ then the\n   original completion status is returned.\n   The duplicate-request\
    \ cache mechanism has been useful in\n   reducing destructive side effects caused\
    \ by duplicate NFS\n   version 3 protocol requests. This mechanism, however, does\n\
    \   not guarantee against these destructive side effects in all\n   failure modes.\
    \ Most servers store the duplicate request cache\n   in RAM, so the contents are\
    \ lost if the server crashes.  The\n   exception to this may possibly occur in\
    \ a redundant server\n   approach to high availability, where the file system\
    \ itself\n   may be used to share the duplicate request cache state. Even\n  \
    \ if the cache survives server reboots (or failovers in the\n   high availability\
    \ case), its effectiveness is a function of\n   its size. A network partition\
    \ can cause a cache entry to be\n   reused before a client receives a reply for\
    \ the corresponding\n   request. If this happens, the duplicate request will be\n\
    \   processed as a new one, possibly with destructive side\n   effects.\n   A\
    \ good description of the implementation and use of a\n   duplicate request cache\
    \ can be found in [Juszczak].\n"
- title: 4.6 File name component handling
  contents:
  - "4.6 File name component handling\n   Server implementations of NFS version 3\
    \ protocol will\n   frequently impose restrictions on the names which can be\n\
    \   created. Many servers will also forbid the use of names that\n   contain certain\
    \ characters, such as the path component\n   separator used by the server operating\
    \ system. For example,\n   the UFS file system will reject a name which contains\
    \ \"/\",\n   while \".\" and \"..\" are distinguished in UFS, and may not be\n\
    \   specified as the name when creating a file system object.\n   The exact error\
    \ status values return for these errors is\n   specified in the description of\
    \ each procedure argument. The\n   values (which conform to NFS version 2 protocol\
    \ server\n   practice) are not necessarily obvious, nor are they\n   consistent\
    \ from one procedure to the next.\n"
- title: 4.7 Synchronous modifying operations
  contents:
  - "4.7 Synchronous modifying operations\n   Data-modifying operations in the NFS\
    \ version 3 protocol are\n   synchronous. When a procedure returns to the client,\
    \ the\n   client can assume that the operation has completed and any\n   data\
    \ associated with the request is now on stable storage.\n"
- title: 4.8 Stable storage
  contents:
  - "4.8 Stable storage\n   NFS version 3 protocol servers must be able to recover\n\
    \   without data loss from multiple power failures (including\n   cascading power\
    \ failures, that is, several power failures in\n   quick succession), operating\
    \ system failures, and hardware\n   failure of components other than the storage\
    \ medium itself\n   (for example, disk, nonvolatile RAM).\n   Some examples of\
    \ stable storage that are allowable for an NFS\n   server include:\n   1. Media\
    \ commit of data, that is, the modified data has\n      been successfully written\
    \ to the disk media, for example,\n      the disk platter.\n   2. An immediate\
    \ reply disk drive with battery-backed\n      on-drive intermediate storage or\
    \ uninterruptible power\n      system (UPS).\n   3. Server commit of data with\
    \ battery-backed intermediate\n      storage and recovery software.\n   4. Cache\
    \ commit with uninterruptible power system (UPS) and\n      recovery software.\n\
    \   Conversely, the following are not examples of stable\n   storage:\n   1. An\
    \ immediate reply disk drive without battery-backed\n      on-drive intermediate\
    \ storage or uninterruptible power\n      system (UPS).\n   2. Cache commit without\
    \ both uninterruptible power system\n      (UPS) and recovery software.\n   The\
    \ only exception to this (introduced in this protocol\n   revision) is as described\
    \ under the WRITE procedure on the\n   handling of the stable bit, and the use\
    \ of the COMMIT\n   procedure.  It is the use of the synchronous COMMIT procedure\n\
    \   that provides the necessary semantic support in the NFS\n   version 3 protocol.\n"
- title: 4.9 Lookups and name resolution
  contents:
  - "4.9 Lookups and name resolution\n   A common objection to the NFS version 3 protocol\
    \ is the\n   philosophy of component-by-component LOOKUP by the client in\n  \
    \ resolving a name. The objection is that this is inefficient,\n   as latencies\
    \ for component-by-component LOOKUP would be\n   unbearable.\n   Implementation\
    \ practice solves this issue. A name cache,\n   providing component to file-handle\
    \ mapping, is kept on the\n   client to short circuit actual LOOKUP invocations\
    \ over the\n   wire.  The cache is subject to cache timeout parameters that\n\
    \   bound attributes.\n"
- title: 4.10 Adaptive retransmission
  contents:
  - "4.10 Adaptive retransmission\n   Most client implementations use either an exponential\n\
    \   back-off strategy to some maximum retransmission value, or a\n   more adaptive\
    \ strategy that attempts congestion avoidance.\n   Congestion avoidance schemes\
    \ in NFS request retransmission\n   are modelled on the work presented in [Jacobson].\
    \ [Nowicki]\n   and [Macklem] describe congestion avoidance schemes to be\n  \
    \ applied to the NFS protocol over UDP.\n"
- title: 4.11 Caching policies
  contents:
  - "4.11 Caching policies\n   The NFS version 3 protocol does not define a policy\
    \ for\n   caching on the client or server. In particular, there is no\n   support\
    \ for strict cache consistency between a client and\n   server, nor between different\
    \ clients. See [Kazar] for a\n   discussion of the issues of cache synchronization\
    \ and\n   mechanisms in several distributed file systems.\n"
- title: 4.12 Stable versus unstable writes
  contents:
  - "4.12 Stable versus unstable writes\n   The setting of the stable field in the\
    \ WRITE arguments, that\n   is whether or not to do asynchronous WRITE requests,\
    \ is\n   straightforward on a UNIX client. If the NFS version 3\n   protocol client\
    \ receives a write request that is not marked\n   as being asynchronous, it should\
    \ generate the RPC with stable\n   set to TRUE. If the request is marked as being\
    \ asynchronous,\n   the RPC should be generated with stable set to FALSE. If the\n\
    \   response comes back with the committed field set to TRUE, the\n   client should\
    \ just mark the write request as done and no\n   further action is required. If\
    \ committed is set to FALSE,\n   indicating that the buffer was not synchronized\
    \ with the\n   server's disk, the client will need to mark the buffer in\n   some\
    \ way which indicates that a copy of the buffer lives on\n   the server and that\
    \ a new copy does not need to be sent to\n   the server, but that a commit is\
    \ required.\n   Note that this algorithm introduces a new state for buffers,\n\
    \   thus there are now three states for buffers. The three states\n   are dirty,\
    \ done but needs to be committed, and done. This\n   extra state on the client\
    \ will likely require modifications\n   to the system outside of the NFS version\
    \ 3 protocol client.\n   One proposal that was rejected was the addition of a\
    \ boolean\n   commit argument to the WRITE operation. It would be used to\n  \
    \ indicate whether the server should do a full file commit\n   after doing the\
    \ write. This seems as if it could be useful if\n   the client knew that it was\
    \ doing the last write on the file.\n   It is difficult to see how this could\
    \ be used, given existing\n   client architectures though.\n   The asynchronous\
    \ write opens up the window of problems\n   associated with write sharing. For\
    \ example: client A writes\n   some data asynchronously. Client A is still holding\
    \ the\n   buffers cached, waiting to commit them later. Client B reads\n   the\
    \ modified data and writes it back to the server. The\n   server then crashes.\
    \ When it comes back up, client A issues a\n   COMMIT operation which returns\
    \ with a different cookie as\n   well as changed attributes. In this case, the\
    \ correct action\n   may or may not be to retransmit the cached buffers.\n   Unfortunately,\
    \ client A can't tell for sure, so it will need\n   to retransmit the buffers,\
    \ thus overwriting the changes from\n   client B.  Fortunately, write sharing\
    \ is rare and the\n   solution matches the current write sharing situation. Without\n\
    \   using locking for synchronization, the behaviour will be\n   indeterminate.\n\
    \   In a high availability (redundant system) server\n   implementation, two cases\
    \ exist which relate to the verf\n   changing.  If the high availability server\
    \ implementation\n   does not use a shared-memory scheme, then the verf should\n\
    \   change on failover, since the unsynchronized data is not\n   available to\
    \ the second processor and there is no guarantee\n   that the system which had\
    \ the data cached was able to flush\n   it to stable storage before going down.\
    \ The client will need\n   to retransmit the data to be safe. In a shared-memory\
    \ high\n   availability server implementation, the verf would not need\n   to\
    \ change because the server would still have the cached data\n   available to\
    \ it to be flushed. The exact policy regarding the\n   verf in a shared memory\
    \ high availability implementation,\n   however, is up to the server implementor.\n"
- title: 4.13 32 bit clients/servers and 64 bit clients/servers
  contents:
  - "4.13 32 bit clients/servers and 64 bit clients/servers\n   The 64 bit nature\
    \ of the NFS version 3 protocol introduces\n   several compatibility problems.\
    \ The most notable two are\n   mismatched clients and servers, that is, a 32 bit\
    \ client and\n   a 64 bit server or a 64 bit client and a 32 bit server.\n   The\
    \ problems of a 64 bit client and a 32 bit server are easy\n   to handle. The\
    \ client will never encounter a file that it can\n   not handle. If it sends a\
    \ request to the server that the\n   server can not handle, the server should\
    \ reject the request\n   with an appropriate error.\n   The problems of a 32 bit\
    \ client and a 64 bit server are much\n   harder to handle. In this situation,\
    \ the server does not have\n   a problem because it can handle anything that the\
    \ client can\n   generate. However, the client may encounter a file that it\n\
    \   can not handle. The client will not be able to handle a file\n   whose size\
    \ can not be expressed in 32 bits. Thus, the client\n   will not be able to properly\
    \ decode the size of the file into\n   its local attributes structure. Also, a\
    \ file can grow beyond\n   the limit of the client while the client is accessing\
    \ the\n   file.\n   The solutions to these problems are left up to the individual\n\
    \   implementor. However, there are two common approaches used to\n   resolve\
    \ this situation. The implementor can choose between\n   them or even can invent\
    \ a new solution altogether.\n   The most common solution is for the client to\
    \ deny access to\n   any file whose size can not be expressed in 32 bits. This\
    \ is\n   probably the safest, but does introduce some strange\n   semantics when\
    \ the file grows beyond the limit of the client\n   while it is being access by\
    \ that client. The file becomes\n   inaccessible even while it is being accessed.\n\
    \   The second solution is for the client to map any size greater\n   than it\
    \ can handle to the maximum size that it can handle.\n   Effectively, it is lying\
    \ to the application program. This\n   allows the application access as much of\
    \ the file as possible\n   given the 32 bit offset restriction. This eliminates\
    \ the\n   strange semantic of the file effectively disappearing after\n   it has\
    \ been accessed, but does introduce other problems. The\n   client will not be\
    \ able to access the entire file.\n   Currently, the first solution is the recommended\
    \ solution.\n   However, client implementors are encouraged to do the best\n \
    \  that they can to reduce the effects of this situation.\n"
- title: '5.0 Appendix I: Mount protocol'
  contents:
  - "5.0 Appendix I: Mount protocol\n   The changes from the NFS version 2 protocol\
    \ to the NFS version 3\n   protocol have required some changes to be made in the\
    \ MOUNT\n   protocol.  To meet the needs of the NFS version 3 protocol, a\n  \
    \ new version of the MOUNT protocol has been defined. This new\n   protocol satisfies\
    \ the requirements of the NFS version 3\n   protocol and addresses several other\
    \ current market\n   requirements.\n"
- title: 5.1 RPC Information
  contents:
  - '5.1 RPC Information

    '
- title: 5.1.1 Authentication
  contents:
  - "5.1.1 Authentication\n   The MOUNT service uses AUTH_NONE in the NULL procedure.\n\
    \   AUTH_UNIX, AUTH_SHORT, AUTH_DES, or AUTH_KERB are used for all\n   other procedures.\
    \  Other authentication types may be supported\n   in the future.\n"
- title: 5.1.2 Constants
  contents:
  - "5.1.2 Constants\n   These are the RPC constants needed to call the MOUNT service.\n\
    \   They are given in decimal.\n      PROGRAM  100005\n      VERSION  3\n"
- title: 5.1.3 Transport address
  contents:
  - "5.1.3 Transport address\n   The MOUNT service is normally supported over the\
    \ TCP and UDP\n   protocols. The rpcbind daemon should be queried for the correct\n\
    \   transport address.\n"
- title: 5.1.4 Sizes
  contents:
  - "5.1.4 Sizes\n   const MNTPATHLEN = 1024;  /* Maximum bytes in a path name */\n\
    \   const MNTNAMLEN  = 255;   /* Maximum bytes in a name */\n   const FHSIZE3\
    \    = 64;    /* Maximum bytes in a V3 file handle */\n"
- title: 5.1.5 Basic Data Types
  contents:
  - "5.1.5 Basic Data Types\n   typedef opaque fhandle3<FHSIZE3>;\n   typedef string\
    \ dirpath<MNTPATHLEN>;\n   typedef string name<MNTNAMLEN>;\n   enum mountstat3\
    \ {\n      MNT3_OK = 0,                 /* no error */\n      MNT3ERR_PERM = 1,\
    \            /* Not owner */\n      MNT3ERR_NOENT = 2,           /* No such file\
    \ or directory */\n      MNT3ERR_IO = 5,              /* I/O error */\n      MNT3ERR_ACCES\
    \ = 13,          /* Permission denied */\n      MNT3ERR_NOTDIR = 20,         /*\
    \ Not a directory */\n      MNT3ERR_INVAL = 22,          /* Invalid argument */\n\
    \      MNT3ERR_NAMETOOLONG = 63,    /* Filename too long */\n      MNT3ERR_NOTSUPP\
    \ = 10004,     /* Operation not supported */\n      MNT3ERR_SERVERFAULT = 10006\
    \  /* A failure on the server */\n   };\n"
- title: 5.2 Server Procedures
  contents:
  - "5.2 Server Procedures\n   The following sections define the RPC procedures  supplied\
    \ by a\n   MOUNT version 3 protocol server. The RPC procedure number is\n   given\
    \ at the top of the page with the name and version. The\n   SYNOPSIS provides\
    \ the name of the procedure, the list of the\n   names of the arguments, the list\
    \ of the names of the results,\n   followed by the XDR argument declarations and\
    \ results\n   declarations. The information in the SYNOPSIS is specified in\n\
    \   RPC Data Description Language as defined in [RFC1014]. The\n   DESCRIPTION\
    \ section tells what the procedure is expected to do\n   and how its arguments\
    \ and results are used. The ERRORS section\n   lists the errors returned for specific\
    \ types of failures. The\n   IMPLEMENTATION field describes how the procedure\
    \ is expected to\n   work and how it should be used by clients.\n      program\
    \ MOUNT_PROGRAM {\n         version MOUNT_V3 {\n            void      MOUNTPROC3_NULL(void)\
    \    = 0;\n            mountres3 MOUNTPROC3_MNT(dirpath)  = 1;\n            mountlist\
    \ MOUNTPROC3_DUMP(void)    = 2;\n            void      MOUNTPROC3_UMNT(dirpath)\
    \ = 3;\n            void      MOUNTPROC3_UMNTALL(void) = 4;\n            exports\
    \   MOUNTPROC3_EXPORT(void)  = 5;\n         } = 3;\n      } = 100005;\n"
- title: '5.2.0 Procedure 0: Null - Do nothing'
  contents:
  - "5.2.0 Procedure 0: Null - Do nothing\n   SYNOPSIS\n      void MOUNTPROC3_NULL(void)\
    \ = 0;\n   DESCRIPTION\n      Procedure NULL does not do any work. It is made\
    \ available\n      to allow server response testing and timing.\n   IMPLEMENTATION\n\
    \      It is important that this procedure do no work at all so\n      that it\
    \ can be used to measure the overhead of processing\n      a service request.\
    \ By convention, the NULL procedure\n      should never require any authentication.\
    \ A server may\n      choose to ignore this convention, in a more secure\n   \
    \   implementation, where responding to the NULL procedure\n      call acknowledges\
    \ the existence of a resource to an\n      unauthenticated client.\n   ERRORS\n\
    \      Since the NULL procedure takes no MOUNT protocol arguments\n      and returns\
    \ no MOUNT protocol response, it can not return\n      a MOUNT protocol error.\
    \ However, it is possible that some\n      server implementations may return RPC\
    \ errors based on\n      security and authentication requirements.\n"
- title: '5.2.1 Procedure 1: MNT - Add mount entry'
  contents:
  - "5.2.1 Procedure 1: MNT - Add mount entry\n   SYNOPSIS\n      mountres3 MOUNTPROC3_MNT(dirpath)\
    \ = 1;\n      struct mountres3_ok {\n           fhandle3   fhandle;\n        \
    \   int        auth_flavors<>;\n      };\n      union mountres3 switch (mountstat3\
    \ fhs_status) {\n      case MNT_OK:\n           mountres3_ok  mountinfo;\n   \
    \   default:\n           void;\n      };\n   DESCRIPTION\n      Procedure MNT\
    \ maps a pathname on the server to a file\n      handle.  The pathname is an ASCII\
    \ string that describes a\n      directory on the server. If the call is successful\n\
    \      (MNT3_OK), the server returns an NFS version 3 protocol\n      file handle\
    \ and a vector of RPC authentication flavors\n      that are supported with the\
    \ client's use of the file\n      handle (or any file handles derived from it).\
    \  The\n      authentication flavors are defined in Section 7.2 and\n      section\
    \ 9 of [RFC1057].\n   IMPLEMENTATION\n      If mountres3.fhs_status is MNT3_OK,\
    \ then\n      mountres3.mountinfo contains the file handle for the\n      directory\
    \ and a list of acceptable authentication\n      flavors.  This file handle may\
    \ only be used in the NFS\n      version 3 protocol.  This procedure also results\
    \ in the\n      server adding a new entry to its mount list recording that\n \
    \     this client has mounted the directory. AUTH_UNIX\n      authentication or\
    \ better is required.\n   ERRORS\n      MNT3ERR_NOENT\n      MNT3ERR_IO\n    \
    \  MNT3ERR_ACCES\n      MNT3ERR_NOTDIR\n      MNT3ERR_NAMETOOLONG\n"
- title: '5.2.2 Procedure 2: DUMP - Return mount entries'
  contents:
  - "5.2.2 Procedure 2: DUMP - Return mount entries\n   SYNOPSIS\n      mountlist\
    \ MOUNTPROC3_DUMP(void) = 2;\n      typedef struct mountbody *mountlist;\n   \
    \   struct mountbody {\n           name       ml_hostname;\n           dirpath\
    \    ml_directory;\n           mountlist  ml_next;\n      };\n   DESCRIPTION\n\
    \      Procedure DUMP returns the list of remotely mounted file\n      systems.\
    \ The mountlist contains one entry for each client\n      host name and directory\
    \ pair.\n   IMPLEMENTATION\n      This list is derived from a list maintained\
    \ on the server\n      of clients that have requested file handles with the MNT\n\
    \      procedure.  Entries are removed from this list only when a\n      client\
    \ calls the UMNT or UMNTALL procedure. Entries may\n      become stale if a client\
    \ crashes and does not issue either\n      UMNT calls for all of the file systems\
    \ that it had\n      previously mounted or a UMNTALL to remove all entries that\n\
    \      existed for it on the server.\n   ERRORS\n      There are no MOUNT protocol\
    \ errors which can be returned\n      from this procedure. However, RPC errors\
    \ may be returned\n      for authentication or other RPC failures.\n"
- title: '5.2.3 Procedure 3: UMNT - Remove mount entry'
  contents:
  - "5.2.3 Procedure 3: UMNT - Remove mount entry\n   SYNOPSIS\n      void MOUNTPROC3_UMNT(dirpath)\
    \ = 3;\n   DESCRIPTION\n      Procedure UMNT removes the mount list entry for\
    \ the\n      directory that was previously the subject of a MNT call\n      from\
    \ this client.  AUTH_UNIX authentication or better is\n      required.\n   IMPLEMENTATION\n\
    \      Typically, server implementations have maintained a list\n      of clients\
    \ which have file systems mounted. In the past,\n      this list has been used\
    \ to inform clients that the server\n      was going to be shutdown.\n   ERRORS\n\
    \      There are no MOUNT protocol errors which can be returned\n      from this\
    \ procedure. However, RPC errors may be returned\n      for authentication or\
    \ other RPC failures.\n"
- title: '5.2.4 Procedure 4: UMNTALL - Remove all mount entries'
  contents:
  - "5.2.4 Procedure 4: UMNTALL - Remove all mount entries\n   SYNOPSIS\n      void\
    \ MOUNTPROC3_UMNTALL(void) = 4;\n   DESCRIPTION\n      Procedure UMNTALL removes\
    \ all of the mount entries for\n      this client previously recorded by calls\
    \ to MNT. AUTH_UNIX\n      authentication or better is required.\n   IMPLEMENTATION\n\
    \      This procedure should be used by clients when they are\n      recovering\
    \ after a system shutdown. If the client could\n      not successfully unmount\
    \ all of its file systems before\n      being shutdown or the client crashed because\
    \ of a software\n      or hardware problem, there may be servers which still have\n\
    \      mount entries for this client. This is an easy way for the\n      client\
    \ to inform all servers at once that it does not have\n      any mounted file\
    \ systems.  However, since this procedure\n      is generally implemented using\
    \ broadcast RPC, it is only\n      of limited usefullness.\n   ERRORS\n      There\
    \ are no MOUNT protocol errors which can be returned\n      from this procedure.\
    \ However, RPC errors may be returned\n      for authentication or other RPC failures.\n"
- title: '5.2.5 Procedure 5: EXPORT - Return export list'
  contents:
  - "5.2.5 Procedure 5: EXPORT - Return export list\n   SYNOPSIS\n      exports MOUNTPROC3_EXPORT(void)\
    \ = 5;\n      typedef struct groupnode *groups;\n      struct groupnode {\n  \
    \         name     gr_name;\n           groups   gr_next;\n      };\n      typedef\
    \ struct exportnode *exports;\n      struct exportnode {\n           dirpath \
    \ ex_dir;\n           groups   ex_groups;\n           exports  ex_next;\n    \
    \  };\n   DESCRIPTION\n      Procedure EXPORT returns a list of all the exported\
    \ file\n      systems and which clients are allowed to mount each one.\n     \
    \ The names in the group list are implementation-specific\n      and cannot be\
    \ directly interpreted by clients. These names\n      can represent hosts or groups\
    \ of hosts.\n   IMPLEMENTATION\n      This procedure generally returns the contents\
    \ of a list of\n      shared or exported file systems. These are the file\n  \
    \    systems which are made available to NFS version 3 protocol\n      clients.\n\
    \   ERRORS\n      There are no MOUNT protocol errors which can be returned\n \
    \     from this procedure. However, RPC errors may be returned\n      for authentication\
    \ or other RPC failures.\n"
- title: '6.0 Appendix II: Lock manager protocol'
  contents:
  - "6.0 Appendix II: Lock manager protocol\n   Because the NFS version 2 protocol\
    \ as well as the NFS version 3\n   protocol is stateless, an additional Network\
    \ Lock Manager (NLM)\n   protocol is required to support locking of NFS-mounted\
    \ files.\n   The NLM version 3 protocol, which is used with the NFS version 2\n\
    \   protocol, is documented in [X/OpenNFS].\n   Some of the changes in the NFS\
    \ version 3 protocol require a\n   new version of the NLM protocol. This new protocol\
    \ is the NLM\n   version 4 protocol. The following table summarizes the\n   correspondence\
    \ between versions of the NFS protocol and NLM\n   protocol.\n       NFS and NLM\
    \ protocol compatibility\n               +---------+---------+\n             \
    \  |   NFS   |   NLM   |\n               | Version | Version |\n             \
    \  +===================+\n               |    2    |   1,3   |\n             \
    \  +---------+---------+\n               |    3    |    4    |\n             \
    \  +---------+---------+\n   This appendix only discusses the differences between\
    \ the NLM\n   version 3 protocol and the NLM version 4 protocol.  As in the\n\
    \   NFS version 3 protocol, almost all the names in the NLM version\n   4 protocol\
    \ have been changed to include a version number. This\n   appendix does not discuss\
    \ changes that consist solely of a name\n   change.\n"
- title: 6.1 RPC Information
  contents:
  - '6.1 RPC Information

    '
- title: 6.1.1 Authentication
  contents:
  - "6.1.1 Authentication\n   The NLM service uses AUTH_NONE in the NULL procedure.\n\
    \   AUTH_UNIX, AUTH_SHORT, AUTH_DES, and AUTH_KERB are used for\n   all other\
    \ procedures. Other authentication types may be\n   supported in the future.\n"
- title: 6.1.2 Constants
  contents:
  - "6.1.2 Constants\n   These are the RPC constants needed to call the NLM service.\n\
    \   They are given in decimal.\n      PROGRAM    100021\n      VERSION    4\n"
- title: 6.1.3 Transport Address
  contents:
  - "6.1.3 Transport Address\n   The NLM service is normally supported over the TCP\
    \ and UDP\n   protocols.  The rpcbind daemon should be queried for the\n   correct\
    \ transport address.\n"
- title: 6.1.4 Basic Data Types
  contents:
  - "6.1.4 Basic Data Types\n   uint64\n      typedef unsigned hyper uint64;\n   int64\n\
    \      typedef hyper int64;\n   uint32\n      typedef unsigned long uint32;\n\
    \   int32\n      typedef long int32;\n   These types are new for the NLM version\
    \ 4 protocol. They are\n   the same as in the NFS version 3 protocol.\n   nlm4_stats\n\
    \      enum nlm4_stats {\n         NLM4_GRANTED = 0,\n         NLM4_DENIED = 1,\n\
    \         NLM4_DENIED_NOLOCKS = 2,\n         NLM4_BLOCKED = 3,\n         NLM4_DENIED_GRACE_PERIOD\
    \ = 4,\n         NLM4_DEADLCK = 5,\n         NLM4_ROFS = 6,\n         NLM4_STALE_FH\
    \ = 7,\n         NLM4_FBIG = 8,\n         NLM4_FAILED = 9\n      };\n   Nlm4_stats\
    \ indicates the success or failure of a call. This\n   version contains several\
    \ new error codes, so that clients can\n   provide more precise failure information\
    \ to applications.\n   NLM4_GRANTED\n      The call completed successfully.\n\
    \   NLM4_DENIED\n      The call failed. For attempts to set a lock, this status\n\
    \      implies that if the client retries the call later, it may\n      succeed.\n\
    \   NLM4_DENIED_NOLOCKS\n      The call failed because the server could not allocate\
    \ the\n      necessary resources.\n   NLM4_BLOCKED\n      Indicates that a blocking\
    \ request cannot be granted\n      immediately. The server will issue an NLMPROC4_GRANTED\n\
    \      callback to the client when the lock is granted.\n   NLM4_DENIED_GRACE_PERIOD\n\
    \      The call failed because the server is reestablishing old\n      locks after\
    \ a reboot and is not yet ready to resume normal\n      service.\n   NLM4_DEADLCK\n\
    \      The request could not be granted and blocking would cause\n      a deadlock.\n\
    \   NLM4_ROFS\n      The call failed because the remote file system is\n     \
    \ read-only.  For example, some server implementations might\n      not support\
    \ exclusive locks on read-only file systems.\n   NLM4_STALE_FH\n      The call\
    \ failed because it uses an invalid file handle.\n      This can happen if the\
    \ file has been removed or if access\n      to the file has been revoked on the\
    \ server.\n   NLM4_FBIG\n      The call failed because it specified a length or\
    \ offset\n      that exceeds the range supported by the server.\n   NLM4_FAILED\n\
    \      The call failed for some reason not already listed.  The\n      client\
    \ should take this status as a strong hint not to\n      retry the request.\n\
    \   nlm4_holder\n      struct nlm4_holder {\n           bool     exclusive;\n\
    \           int32    svid;\n           netobj   oh;\n           uint64   l_offset;\n\
    \           uint64   l_len;\n      };\n   This structure indicates the holder\
    \ of a lock. The exclusive\n   field tells whether the holder has an exclusive\
    \ lock or a\n   shared lock. The svid field identifies the process that is\n \
    \  holding the lock. The oh field is an opaque object that\n   identifies the\
    \ host or process that is holding the lock. The\n   l_len and l_offset fields\
    \ identify the region that is locked.\n   The only difference between the NLM\
    \ version 3 protocol and\n   the NLM version 4 protocol is that in the NLM version\
    \ 3\n   protocol, the l_len and l_offset fields are 32 bits wide,\n   while they\
    \ are 64 bits wide in the NLM version 4 protocol.\n   nlm4_lock\n      struct\
    \ nlm4_lock {\n           string   caller_name<LM_MAXSTRLEN>;\n           netobj\
    \   fh;\n           netobj   oh;\n           int32    svid;\n           uint64\
    \   l_offset;\n           uint64   l_len;\n      };\n   This structure describes\
    \ a lock request. The caller_name\n   field identifies the host that is making\
    \ the request. The fh\n   field identifies the file to lock. The oh field is an\
    \ opaque\n   object that identifies the host or process that is making the\n \
    \  request, and the svid field identifies the process that is\n   making the request.\
    \  The l_offset and l_len fields identify\n   the region of the file that the\
    \ lock controls.  A l_len of 0\n   means \"to end of file\".\n   There are two\
    \ differences between the NLM version 3 protocol\n   and the NLM version 4 protocol\
    \ versions of this structure.\n   First, in the NLM version 3 protocol, the length\
    \ and offset\n   are 32 bits wide, while they are 64 bits wide in the NLM\n  \
    \ version 4 protocol.  Second, in the NLM version 3 protocol,\n   the file handle\
    \ is a fixed-length NFS version 2 protocol file\n   handle, which is encoded as\
    \ a byte count followed by a byte\n   array. In the NFS version 3 protocol, the\
    \ file handle is\n   already variable-length, so it is copied directly into the\
    \ fh\n   field.  That is, the first four bytes of the fh field are the\n   same\
    \ as the byte count in an NFS version 3 protocol nfs_fh3.\n   The rest of the\
    \ fh field contains the byte array from the NFS\n   version 3 protocol nfs_fh3.\n\
    \   nlm4_share\n      struct nlm4_share {\n           string      caller_name<LM_MAXSTRLEN>;\n\
    \           netobj      fh;\n           netobj      oh;\n           fsh4_mode\
    \   mode;\n           fsh4_access access;\n      };\n   This structure is used\
    \ to support DOS file sharing. The\n   caller_name field identifies the host making\
    \ the request.\n   The fh field identifies the file to be operated on. The oh\n\
    \   field is an opaque object that identifies the host or process\n   that is\
    \ making the request. The mode and access fields\n   specify the file-sharing\
    \ and access modes. The encoding of fh\n   is a byte count, followed by the file\
    \ handle byte array. See\n   the description of nlm4_lock for more details.\n"
- title: 6.2 NLM Procedures
  contents:
  - "6.2 NLM Procedures\n   The procedures in the NLM version 4 protocol are semantically\n\
    \   the same as those in the NLM version 3 protocol. The only\n   semantic difference\
    \ is the addition of a NULL procedure that\n   can be used to test for server\
    \ responsiveness.  The procedure\n   names with _MSG and _RES suffixes denote\
    \ asynchronous\n   messages; for these the void response implies no reply.  A\n\
    \   syntactic change is that the procedures were renamed to avoid\n   name conflicts\
    \ with the values of nlm4_stats. Thus the\n   procedure definition is as follows.\n\
    \      version NLM4_VERS {\n         void\n            NLMPROC4_NULL(void)   \
    \               = 0;\n         nlm4_testres\n            NLMPROC4_TEST(nlm4_testargs)\
    \         = 1;\n         nlm4_res\n            NLMPROC4_LOCK(nlm4_lockargs)  \
    \       = 2;\n         nlm4_res\n            NLMPROC4_CANCEL(nlm4_cancargs)  \
    \     = 3;\n         nlm4_res\n            NLMPROC4_UNLOCK(nlm4_unlockargs)  \
    \   = 4;\n         nlm4_res\n            NLMPROC4_GRANTED(nlm4_testargs)     \
    \ = 5;\n         void\n            NLMPROC4_TEST_MSG(nlm4_testargs)     = 6;\n\
    \         void\n            NLMPROC4_LOCK_MSG(nlm4_lockargs)     = 7;\n      \
    \   void\n            NLMPROC4_CANCEL_MSG(nlm4_cancargs)   = 8;\n         void\n\
    \            NLMPROC4_UNLOCK_MSG(nlm4_unlockargs) = 9;\n         void\n      \
    \      NLMPROC4_GRANTED_MSG(nlm4_testargs) = 10;\n         void\n            NLMPROC4_TEST_RES(nlm4_testres)\
    \     = 11;\n         void\n            NLMPROC4_LOCK_RES(nlm4_res)         =\
    \ 12;\n         void\n            NLMPROC4_CANCEL_RES(nlm4_res)       = 13;\n\
    \         void\n            NLMPROC4_UNLOCK_RES(nlm4_res)       = 14;\n      \
    \   void\n            NLMPROC4_GRANTED_RES(nlm4_res)      = 15;\n         nlm4_shareres\n\
    \            NLMPROC4_SHARE(nlm4_shareargs)      = 20;\n         nlm4_shareres\n\
    \            NLMPROC4_UNSHARE(nlm4_shareargs)    = 21;\n         nlm4_res\n  \
    \          NLMPROC4_NM_LOCK(nlm4_lockargs)     = 22;\n         void\n        \
    \    NLMPROC4_FREE_ALL(nlm4_notify)      = 23;\n      } = 4;\n"
- title: '6.2.0 Procedure 0: NULL - Do nothing'
  contents:
  - "6.2.0 Procedure 0: NULL - Do nothing\n   SYNOPSIS\n      void NLMPROC4_NULL(void)\
    \ = 0;\n   DESCRIPTION\n      The NULL procedure does no work. It is made available\
    \ in\n      all RPC services to allow server response testing and\n      timing.\n\
    \   IMPLEMENTATION\n      It is important that this procedure do no work at all\
    \ so\n      that it can be used to measure the overhead of processing\n      a\
    \ service request. By convention, the NULL procedure\n      should never require\
    \ any authentication.\n   ERRORS\n      It is possible that some server implementations\
    \ may return\n      RPC errors based on security and authentication\n      requirements.\n"
- title: 6.3 Implementation issues
  contents:
  - '6.3 Implementation issues

    '
- title: 6.3.1 64-bit offsets and lengths
  contents:
  - "6.3.1 64-bit offsets and lengths\n      Some NFS version 3 protocol servers can\
    \ only support\n      requests where the file offset or length fits in 32 or\n\
    \      fewer bits.  For these servers, the lock manager will have\n      the same\
    \ restriction.  If such a lock manager receives a\n      request that it cannot\
    \ handle (because the offset or\n      length uses more than 32 bits), it should\
    \ return the\n      error, NLM4_FBIG.\n"
- title: 6.3.2 File handles
  contents:
  - "6.3.2 File handles\n      The change in the file handle format from the NFS version\n\
    \      2 protocol to the NFS version 3 protocol complicates the\n      lock manager.\
    \ First, the lock manager needs some way to\n      tell when an NFS version 2\
    \ protocol file handle refers to\n      the same file as an NFS version 3 protocol\
    \ file handle.\n      (This is assuming that the lock manager supports both NLM\n\
    \      version 3 protocol clients and NLM version 4 protocol\n      clients.)\
    \ Second, if the lock manager runs the file handle\n      through a hashing function,\
    \ the hashing function may need\n      to be retuned to work with NFS version\
    \ 3 protocol file\n      handles as well as NFS version 2 protocol file handles.\n"
- title: '7.0 Appendix III: Bibliography'
  contents:
  - '7.0 Appendix III: Bibliography

    '
- title: '[Corbin]        Corbin, John, "The Art of Distributed'
  contents:
  - "[Corbin]        Corbin, John, \"The Art of Distributed\n                Programming-Programming\
    \ Techniques for Remote\n                Procedure Calls.\" Springer-Verlag, New\
    \ York, New\n                York. 1991.  Basic description of RPC and XDR\n \
    \               and how to program distributed applications\n                using\
    \ them.\n"
- title: '[Glover]        Glover, Fred, "TNFS Protocol Specification,"'
  contents:
  - "[Glover]        Glover, Fred, \"TNFS Protocol Specification,\"\n            \
    \    Trusted System Interest Group, Work in\n                Progress.\n"
- title: '[Israel]        Israel, Robert K., Sandra Jett, James Pownell,'
  contents:
  - "[Israel]        Israel, Robert K., Sandra Jett, James Pownell,\n            \
    \    George M. Ericson, \"Eliminating Data Copies in\n                UNIX-based\
    \ NFS Servers,\" Uniforum Conference\n                Proceedings, San Francisco,\
    \ CA,\n                February 27 - March 2, 1989.  Describes two\n         \
    \       methods for reducing data copies in NFS server\n                code.\n"
- title: '[Jacobson]      Jacobson, V., "Congestion Control and'
  contents:
  - "[Jacobson]      Jacobson, V., \"Congestion Control and\n                Avoidance,\"\
    \ Proc. ACM SIGCOMM `88, Stanford, CA,\n                August 1988.  The paper\
    \ describing improvements\n                to TCP to allow use over Wide Area\
    \ Networks and\n                through gateways connecting networks of varying\n\
    \                capacity. This work was a starting point for the\n          \
    \      NFS Dynamic Retransmission work.\n"
- title: '[Juszczak]      Juszczak, Chet, "Improving the Performance and'
  contents:
  - "[Juszczak]      Juszczak, Chet, \"Improving the Performance and\n           \
    \     Correctness of an NFS Server,\" USENIX Conference\n                Proceedings,\
    \ USENIX Association, Berkeley, CA,\n                June 1990, pages 53-63. \
    \ Describes reply cache\n                implementation that avoids work in the\
    \ server by\n                handling duplicate requests. More important,\n  \
    \              though listed as a side-effect, the reply cache\n             \
    \   aids in the avoidance of destructive\n                non-idempotent operation\
    \ re-application --\n                improving correctness.\n"
- title: '[Kazar]         Kazar, Michael Leon, "Synchronization and Caching'
  contents:
  - "[Kazar]         Kazar, Michael Leon, \"Synchronization and Caching\n        \
    \        Issues in the Andrew File System,\" USENIX Conference\n             \
    \   Proceedings, USENIX Association, Berkeley, CA,\n                Dallas Winter\
    \ 1988, pages 27-36.  A description\n                of the cache consistency\
    \ scheme in AFS.\n                Contrasted with other distributed file systems.\n"
- title: '[Macklem]       Macklem, Rick, "Lessons Learned Tuning the'
  contents:
  - "[Macklem]       Macklem, Rick, \"Lessons Learned Tuning the\n               \
    \ 4.3BSD Reno Implementation of the NFS Protocol,\"\n                Winter USENIX\
    \ Conference Proceedings, USENIX\n                Association, Berkeley, CA, January\
    \ 1991.\n                Describes performance work in tuning the 4.3BSD\n   \
    \             Reno NFS implementation. Describes performance\n               \
    \ improvement (reduced CPU loading) through\n                elimination of data\
    \ copies.\n"
- title: '[Mogul]         Mogul, Jeffrey C., "A Recovery Protocol for Spritely'
  contents:
  - "[Mogul]         Mogul, Jeffrey C., \"A Recovery Protocol for Spritely\n     \
    \           NFS,\" USENIX File System Workshop Proceedings,\n                Ann\
    \ Arbor, MI, USENIX Association, Berkeley, CA,\n                May 1992.  Second\
    \ paper on Spritely NFS proposes\n                a lease-based scheme for recovering\
    \ state of\n                consistency protocol.\n"
- title: '[Nowicki]       Nowicki, Bill, "Transport Issues in the Network'
  contents:
  - "[Nowicki]       Nowicki, Bill, \"Transport Issues in the Network\n          \
    \      File System,\" ACM SIGCOMM newsletter Computer\n                Communication\
    \ Review, April 1989.  A brief\n                description of the basis for the\
    \ dynamic\n                retransmission work.\n"
- title: '[Pawlowski]     Pawlowski, Brian, Ron Hixon, Mark Stein, Joseph'
  contents:
  - "[Pawlowski]     Pawlowski, Brian, Ron Hixon, Mark Stein, Joseph\n           \
    \     Tumminaro, \"Network Computing in the UNIX and\n                IBM Mainframe\
    \ Environment,\" Uniforum `89 Conf.\n                Proc., (1989) Description\
    \ of an NFS server\n                implementation for IBM's MVS operating system.\n"
- title: '[RFC1014]       Sun Microsystems, Inc., "XDR: External Data'
  contents:
  - "[RFC1014]       Sun Microsystems, Inc., \"XDR: External Data\n              \
    \  Representation Standard\", RFC 1014,\n                Sun Microsystems, Inc.,\
    \ June 1987.\n                Specification for canonical format for data\n  \
    \              exchange, used with RPC.\n"
- title: '[RFC1057]       Sun Microsystems, Inc., "RPC: Remote Procedure'
  contents:
  - "[RFC1057]       Sun Microsystems, Inc., \"RPC: Remote Procedure\n           \
    \     Call Protocol Specification\", RFC 1057,\n                Sun Microsystems,\
    \ Inc., June 1988.\n                Remote procedure protocol specification.\n"
- title: '[RFC1094]       Sun Microsystems, Inc., "Network Filesystem'
  contents:
  - "[RFC1094]       Sun Microsystems, Inc., \"Network Filesystem\n              \
    \  Specification\", RFC 1094, Sun Microsystems, Inc.,\n                March 1989.\
    \  NFS version 2 protocol\n                specification.\n"
- title: '[Sandberg]      Sandberg, R., D. Goldberg, S. Kleiman, D. Walsh,'
  contents:
  - "[Sandberg]      Sandberg, R., D. Goldberg, S. Kleiman, D. Walsh,\n          \
    \      B.  Lyon, \"Design and Implementation of the Sun\n                Network\
    \ Filesystem,\" USENIX Conference\n                Proceedings, USENIX Association,\
    \ Berkeley, CA,\n                Summer 1985.  The basic paper describing the\n\
    \                SunOS implementation of the NFS version 2\n                protocol,\
    \ and discusses the goals, protocol\n                specification and trade-offs.\n"
- title: '[Srinivasan]    Srinivasan, V., Jeffrey C. Mogul, "Spritely'
  contents:
  - "[Srinivasan]    Srinivasan, V., Jeffrey C. Mogul, \"Spritely\n              \
    \  NFS:  Implementation and Performance of Cache\n                Consistency\
    \ Protocols\", WRL Research Report\n                89/5, Digital Equipment Corporation\
    \ Western\n                Research Laboratory, 100 Hamilton Ave., Palo\n    \
    \            Alto, CA, 94301, May 1989.  This paper analyzes\n               \
    \ the effect of applying a Sprite-like consistency\n                protocol applied\
    \ to standard NFS. The issues of\n                recovery in a stateful environment\
    \ are covered\n                in [Mogul].\n"
- title: '[X/OpenNFS]     X/Open Company, Ltd., X/Open CAE Specification:'
  contents:
  - "[X/OpenNFS]     X/Open Company, Ltd., X/Open CAE Specification:\n           \
    \     Protocols for X/Open Internetworking: XNFS,\n                X/Open Company,\
    \ Ltd., Apex Plaza, Forbury Road,\n                Reading Berkshire, RG1 1AX,\
    \ United Kingdom,\n                1991.  This is an indispensable reference for\n\
    \                NFS version 2 protocol and accompanying\n                protocols,\
    \ including the Lock Manager and the\n                Portmapper.\n"
- title: '[X/OpenPCNFS]   X/Open Company, Ltd., X/Open CAE Specification:'
  contents:
  - "[X/OpenPCNFS]   X/Open Company, Ltd., X/Open CAE Specification:\n           \
    \     Protocols for X/Open Internetworking: (PC)NFS,\n                Developer's\
    \ Specification, X/Open Company, Ltd.,\n                Apex Plaza, Forbury Road,\
    \ Reading Berkshire, RG1\n                1AX, United Kingdom, 1991.  This is\
    \ an\n                indispensable reference for NFS version 2\n            \
    \    protocol and accompanying protocols, including\n                the Lock\
    \ Manager and the Portmapper.\n"
- title: 8. Security Considerations
  contents:
  - "8. Security Considerations\n   Since sensitive file data may be transmitted or\
    \ received\n   from a server by the NFS protocol, authentication, privacy,\n \
    \  and data integrity issues should be addressed by implementations\n   of this\
    \ protocol.\n   As with the previous protocol revision (version 2), NFS\n   version\
    \ 3 defers to the authentication provisions of the\n   supporting RPC protocol\
    \ [RFC1057], and assumes that data\n   privacy and integrity are provided by underlying\
    \ transport\n   layers as available in each implementation of the protocol.\n\
    \   See section 4.4 for a discussion relating to file access\n   permissions.\n"
- title: 9. Acknowledgements
  contents:
  - "9. Acknowledgements\n   This description of the protocol is derived from an original\n\
    \   document written by Brian Pawlowski and revised by Peter\n   Staubach.  This\
    \ protocol is the result of a co-operative\n   effort that comprises the contributions\
    \ of Geoff Arnold,\n   Brent Callaghan, John Corbin, Fred Glover, Chet Juszczak,\n\
    \   Mike Eisler, John Gillono, Dave Hitz, Mike Kupfer, Rick\n   Macklem, Ron Minnich,\
    \ Brian Pawlowski, David Robinson, Rusty\n   Sandberg, Craig Schamp, Spencer Shepler,\
    \ Carl Smith, Mark\n   Stein, Peter Staubach, Tom Talpey, Rob Thurlow, and Mark\n\
    \   Wittle.\n"
- title: 10. Authors' Addresses
  contents:
  - "10. Authors' Addresses\n   Address comments related to this protocol to:\n  \
    \    nfs3@eng.sun.com\n   Brent Callaghan\n   Sun Microsystems, Inc.\n   2550\
    \ Garcia Avenue\n   Mailstop UMTV05-44\n   Mountain View, CA 94043-1100\n   Phone:\
    \ 1-415-336-1051\n   Fax:   1-415-336-6015\n   EMail: brent.callaghan@eng.sun.com\n\
    \   Brian Pawlowski\n   Network Appliance Corp.\n   319 North Bernardo Ave.\n\
    \   Mountain View, CA 94043\n   Phone: 1-415-428-5136\n   Fax:   1-415-428-5151\n\
    \   EMail: beepy@netapp.com\n   Peter Staubach\n   Sun Microsystems, Inc.\n  \
    \ 2550 Garcia Avenue\n   Mailstop UMTV05-44\n   Mountain View, CA 94043-1100\n\
    \   Phone: 1-415-336-5615\n   Fax:   1-415-336-6015\n   EMail: peter.staubach@eng.sun.com\n"
