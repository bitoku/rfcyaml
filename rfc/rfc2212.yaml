- contents:
  - '             Specification of Guaranteed Quality of Service

    '
  title: __initial_text__
- contents:
  - "Status of this Memo\n   This document specifies an Internet standards track protocol
    for the\n   Internet community, and requests discussion and suggestions for\n
    \  improvements.  Please refer to the current edition of the \"Internet\n   Official
    Protocol Standards\" (STD 1) for the standardization state\n   and status of this
    protocol.  Distribution of this memo is unlimited.\n"
  title: Status of this Memo
- contents:
  - "Abstract\n   This memo describes the network element behavior required to deliver\n
    \  a guaranteed service (guaranteed delay and bandwidth) in the\n   Internet.
    \ Guaranteed service provides firm (mathematically provable)\n   bounds on end-to-end
    datagram queueing delays.  This service makes it\n   possible to provide a service
    that guarantees both delay and\n   bandwidth.  This specification follows the
    service specification\n   template described in [1].\n"
  title: Abstract
- contents:
  - "Introduction\n   This document defines the requirements for network elements
    that\n   support guaranteed service.  This memo is one of a series of\n   documents
    that specify the network element behavior required to\n   support various qualities
    of service in IP internetworks.  Services\n   described in these documents are
    useful both in the global Internet\n   and private IP networks.\n   The key words
    \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\",
    \"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in this\n   document
    are to be interpreted as described in RFC 2119.\n   This document is based on
    the service specification template given in\n   [1]. Please refer to that document
    for definitions and additional\n   information about the specification of qualities
    of service within\n   the IP protocol family.\n   In brief, the concept behind
    this memo is that a flow is described\n   using a token bucket and given this
    description of a flow, a service\n   element (a router, a subnet, etc) computes
    various parameters\n   describing how the service element will handle the flow's
    data.  By\n   combining the parameters from the various service elements in a
    path,\n   it is possible to compute the maximum delay a piece of data will\n   experience
    when transmitted via that path.\n   It is important to note three characteristics
    of this memo and the\n   service it specifies:\n      1. While the requirements
    a setup mechanism must follow to achieve\n      a guaranteed reservation are carefully
    specified, neither the\n      setup mechanism itself nor the method for identifying
    flows is\n      specified.  One can create a guaranteed reservation using a\n
    \     protocol like RSVP, manual configuration of relevant routers or a\n      network
    management protocol like SNMP.  This specification is\n      intentionally independent
    of setup mechanism.\n      2. To achieve a bounded delay requires that every service
    element\n      in the path supports guaranteed service or adequately mimics\n
    \     guaranteed service.  However this requirement does not imply that\n      guaranteed
    service must be deployed throughout the Internet to be\n      useful.  Guaranteed
    service can have clear benefits even when\n      partially deployed.  If fully
    deployed in an intranet, that\n      intranet can support guaranteed service internally.
    \ And an ISP\n      can put guaranteed service in its backbone and provide guaranteed\n
    \     service between customers (or between POPs).\n      3. Because service elements
    produce a delay bound as a result\n      rather than take a delay bound as an
    input to be achieved, it is\n      sometimes assumed that applications cannot
    control the delay.  In\n      reality, guaranteed service gives applications considerable\n
    \     control over their delay.\n      In brief, delay has two parts: a fixed
    delay (transmission delays,\n      etc) and a queueing delay.  The fixed delay
    is a property of the\n      chosen path, which is determined not by guaranteed
    service but by\n      the setup mechanism.  Only queueing delay is determined
    by\n      guaranteed service.  And (as the equations later in this memo\n      show)
    the queueing delay is primarily a function of two\n      parameters: the token
    bucket (in particular, the bucket size b)\n      and the data rate (R) the application
    requests.  These two values\n      are completely under the application's control.
    \ In other words,\n      an application can usually accurately estimate, a priori,
    what\n      queueing delay guaranteed service will likely promise.\n      Furthermore,
    if the delay is larger than expected, the application\n      can modify its token
    bucket and data rate in predictable ways to\n      achieve a lower delay.\n"
  title: Introduction
- contents:
  - "End-to-End Behavior\n   The end-to-end behavior provided by a series of network
    elements that\n   conform to this document is an assured level of bandwidth that,
    when\n   used by a policed flow, produces a delay-bounded service with no\n   queueing
    loss for all conforming datagrams (assuming no failure of\n   network components
    or changes in routing during the life of the\n   flow).\n   The end-to-end behavior
    conforms to the fluid model (described under\n   Network Element Data Handling
    below) in that the delivered queueing\n   delays do not exceed the fluid delays
    by more than the specified\n   error bounds.  More precisely, the end-to-end delay
    bound is [(b-\n   M)/R*(p-R)/(p-r)]+(M+Ctot)/R+Dtot for p>R>=r, and (M+Ctot)/R+Dtot
    for\n   r<=p<=R, (where b, r, p, M, R, Ctot, and Dtot are defined later in\n   this
    document).\n      NOTE: While the per-hop error terms needed to compute the end-to-\n
    \     end delays are exported by the service module (see Exported\n      Information
    below), the mechanisms needed to collect per-hop\n      bounds and make the end-to-end
    quantities Ctot and Dtot known to\n      the applications are not described in
    this specification.  These\n      functions are provided by reservation setup
    protocols, routing\n      protocols or other network management functions and
    are outside\n      the scope of this document.\n   The maximum end-to-end queueing
    delay (as characterized by Ctot and\n   Dtot) and bandwidth (characterized by
    R) provided along a path will\n   be stable.  That is, they will not change as
    long as the end-to-end\n   path does not change.\n   Guaranteed service does not
    control the minimal or average delay of\n   datagrams, merely the maximal queueing
    delay.  Furthermore, to\n   compute the maximum delay a datagram will experience,
    the latency of\n   the path MUST be determined and added to the guaranteed queueing\n
    \  delay.  (However, as noted below, a conservative bound of the latency\n   can
    be computed by observing the delay experienced by any one\n   packet).\n   This
    service is subject to admission control.\n"
  title: End-to-End Behavior
- contents:
  - "Motivation\n   Guaranteed service guarantees that datagrams will arrive within
    the\n   guaranteed delivery time and will not be discarded due to queue\n   overflows,
    provided the flow's traffic stays within its specified\n   traffic parameters.
    \ This service is intended for applications which\n   need a firm guarantee that
    a datagram will arrive no later than a\n   certain time after it was transmitted
    by its source.  For example,\n   some audio and video \"play-back\" applications
    are intolerant of any\n   datagram arriving after their play-back time.  Applications
    that have\n   hard real-time requirements will also require guaranteed service.\n
    \  This service does not attempt to minimize the jitter (the difference\n   between
    the minimal and maximal datagram delays); it merely controls\n   the maximal queueing
    delay.  Because the guaranteed delay bound is a\n   firm one, the delay has to
    be set large enough to cover extremely\n   rare cases of long queueing delays.
    \ Several studies have shown that\n   the actual delay for the vast majority of
    datagrams can be far lower\n   than the guaranteed delay.  Therefore, authors
    of playback\n   applications should note that datagrams will often arrive far
    earlier\n   than the delivery deadline and will have to be buffered at the\n   receiving
    system until it is time for the application to process\n   them.\n   This service
    represents one extreme end of delay control for\n   networks.  Most other services
    providing delay control provide much\n   weaker assurances about the resulting
    delays.  In order to provide\n   this high level of assurance, guaranteed service
    is typically only\n   useful if provided by every network element along the path
    (i.e. by\n   both routers and the links that interconnect the routers).  Moreover,\n
    \  as described in the Exported Information section, effective provision\n   and
    use of the service requires that the set-up protocol or other\n   mechanism used
    to request service provides service characterizations\n   to intermediate routers
    and to the endpoints.\n"
  title: Motivation
- contents:
  - "Network Element Data Handling Requirements\n   The network element MUST ensure
    that the service approximates the\n   \"fluid model\" of service.  The fluid model
    at service rate R is\n   essentially the service that would be provided by a dedicated
    wire of\n   bandwidth R between the source and receiver.  Thus, in the fluid\n
    \  model of service at a fixed rate R, the flow's service is completely\n   independent
    of that of any other flow.\n   The flow's level of service is characterized at
    each network element\n   by a bandwidth (or service rate) R and a buffer size
    B.  R represents\n   the share of the link's bandwidth the flow is entitled to
    and B\n   represents the buffer space in the network element that the flow may\n
    \  consume.  The network element MUST ensure that its service matches\n   the
    fluid model at that same rate to within a sharp error bound.\n   The definition
    of guaranteed service relies on the result that the\n   fluid delay of a flow
    obeying a token bucket (r,b) and being served\n   by a line with bandwidth R is
    bounded by b/R as long as R is no less\n   than r.  Guaranteed service with a
    service rate R, where now R is a\n   share of bandwidth rather than the bandwidth
    of a dedicated line,\n   approximates this behavior.\n   Consequently, the network
    element MUST ensure that the queueing delay\n   of any datagram be less than b/R+C/R+D,
    where C and D describe the\n   maximal local deviation away from the fluid model.
    \ It is important\n   to emphasize that C and D are maximums.  So, for instance,
    if an\n   implementation has occasional gaps in service (perhaps due to\n   processing
    routing updates), D needs to be large enough to account\n   for the time a datagram
    may lose during the gap in service.  (C and D\n   are described in more detail
    in the section on Exported Information).\n      NOTE: Strictly speaking, this
    memo requires only that the service\n      a flow receives is never worse than
    it would receive under this\n      approximation of the fluid model.  It is perfectly
    acceptable to\n      give better service.  For instance, if a flow is currently
    not\n      using its share, R, algorithms such as Weighted Fair Queueing that\n
    \     temporarily give other flows the unused bandwidth, are perfectly\n      acceptable
    (indeed, are encouraged).\n   Links are not permitted to fragment datagrams as
    part of guaranteed\n   service.  Datagrams larger than the MTU of the link MUST
    be policed\n   as nonconformant which means that they will be policed according
    to\n   the rules described in the Policing section below.\n"
  title: Network Element Data Handling Requirements
- contents:
  - "Invocation Information\n   Guaranteed service is invoked by specifying the traffic
    (TSpec) and\n   the desired service (RSpec) to the network element.  A service\n
    \  request for an existing flow that has a new TSpec and/or RSpec SHOULD\n   be
    treated as a new invocation, in the sense that admission control\n   SHOULD be
    reapplied to the flow.  Flows that reduce their TSpec\n   and/or their RSpec (i.e.,
    their new TSpec/RSpec is strictly smaller\n   than the old TSpec/RSpec according
    to the ordering rules described in\n   the section on Ordering below) SHOULD never
    be denied service.\n   The TSpec takes the form of a token bucket plus a peak
    rate (p), a\n   minimum policed unit (m), and a maximum datagram size (M).\n   The
    token bucket has a bucket depth, b, and a bucket rate, r.  Both b\n   and r MUST
    be positive.  The rate, r, is measured in bytes of IP\n   datagrams per second,
    and can range from 1 byte per second to as\n   large as 40 terabytes per second
    (or close to what is believed to be\n   the maximum theoretical bandwidth of a
    single strand of fiber).\n   Clearly, particularly for large bandwidths, only
    the first few digits\n   are significant and so the use of floating point representations,\n
    \  accurate to at least 0.1% is encouraged.\n   The bucket depth, b, is also measured
    in bytes and can range from 1\n   byte to 250 gigabytes.  Again, floating point
    representations\n   accurate to at least 0.1% are encouraged.\n   The range of
    values is intentionally large to allow for the future\n   bandwidths.  The range
    is not intended to imply that a network\n   element has to support the entire
    range.\n   The peak rate, p, is measured in bytes of IP datagrams per second and\n
    \  has the same range and suggested representation as the bucket rate.\n   The
    peak rate is the maximum rate at which the source and any\n   reshaping points
    (reshaping points are defined below) may inject\n   bursts of traffic into the
    network.  More precisely, it is a\n   requirement that for all time periods the
    amount of data sent cannot\n   exceed M+pT where M is the maximum datagram size
    and T is the length\n   of the time period.  Furthermore, p MUST be greater than
    or equal to\n   the token bucket rate, r.  If the peak rate is unknown or\n   unspecified,
    then p MUST be set to infinity.\n   The minimum policed unit, m, is an integer
    measured in bytes.  All IP\n   datagrams less than size m will be counted, when
    policed and tested\n   for conformance to the TSpec, as being of size m.  The
    maximum\n   datagram size, M, is the biggest datagram that will conform to the\n
    \  traffic specification; it is also measured in bytes.  The flow MUST\n   be
    rejected if the requested maximum datagram size is larger than the\n   MTU of
    the link.  Both m and M MUST be positive, and m MUST be less\n   than or equal
    to M.\n      The guaranteed service uses the general TOKEN_BUCKET_TSPEC\n      parameter
    defined in Reference [8] to describe a data flow's\n      traffic characteristics.
    The description above is of that\n      parameter.  The TOKEN_BUCKET_TSPEC is
    general parameter number\n      127. Use of this parameter for the guaranteed
    service TSpec\n      simplifies the use of guaranteed Service in a multi-service\n
    \     environment.\n   The RSpec is a rate R and a slack term S, where R MUST
    be greater\n   than or equal to r and S MUST be nonnegative.  The rate R is again\n
    \  measured in bytes of IP datagrams per second and has the same range\n   and
    suggested representation as the bucket and the peak rates.  The\n   slack term
    S is in microseconds.  The RSpec rate can be bigger than\n   the TSpec rate because
    higher rates will reduce queueing delay.  The\n   slack term signifies the difference
    between the desired delay and the\n   delay obtained by using a reservation level
    R.  This slack term can\n   be utilized by the network element to reduce its resource
    reservation\n   for this flow. When a network element chooses to utilize some
    of the\n   slack in the RSpec, it MUST follow specific rules in updating the R\n
    \  and S fields of the RSpec; these rules are specified in the Ordering\n   and
    Merging section.  If at the time of service invocation no slack\n   is specified,
    the slack term, S, is set to zero.  No buffer\n   specification is included in
    the RSpec because the network element is\n   expected to derive the required buffer
    space to ensure no queueing\n   loss from the token bucket and peak rate in the
    TSpec, the reserved\n   rate and slack in the RSpec, the exported information
    received at the\n   network element, i.e., Ctot and Dtot or Csum and Dsum, combined
    with\n   internal information about how the element manages its traffic.\n   The
    TSpec can be represented by three floating point numbers in\n   single-precision
    IEEE floating point format followed by two 32-bit\n   integers in network byte
    order.  The first floating point value is\n   the rate (r), the second floating
    point value is the bucket size (b),\n   the third floating point is the peak rate
    (p), the first integer is\n   the minimum policed unit (m), and the second integer
    is the maximum\n   datagram size (M).\n   The RSpec rate term, R, can also be
    represented using single-\n   precision IEEE floating point.\n   The Slack term,
    S, can be represented as a 32-bit integer.  Its value\n   can range from 0 to
    (2**32)-1 microseconds.\n   When r, b, p, and R terms are represented as IEEE
    floating point\n   values, the sign bit MUST be zero (all values MUST be non-negative).\n
    \  Exponents less than 127 (i.e., 0) are prohibited.  Exponents greater\n   than
    162 (i.e., positive 35) are discouraged, except for specifying a\n   peak rate
    of infinity.  Infinity is represented with an exponent of\n   all ones (255) and
    a sign bit and mantissa of all zeroes.\n"
  title: Invocation Information
- contents:
  - "Exported Information\n   Each guaranteed service module MUST export at least
    the following\n   information.  All of the parameters described below are\n   characterization
    parameters.\n   A network element's implementation of guaranteed service is\n
    \  characterized by two error terms, C and D, which represent how the\n   element's
    implementation of the guaranteed service deviates from the\n   fluid model.  These
    two parameters have an additive composition rule.\n   The error term C is the
    rate-dependent error term.  It represents the\n   delay a datagram in the flow
    might experience due to the rate\n   parameters of the flow.  An example of such
    an error term is the need\n   to account for the time taken serializing a datagram
    broken up into\n   ATM cells, with the cells sent at a frequency of 1/r.\n      NOTE:
    It is important to observe that when computing the delay\n      bound, parameter
    C is divided by the reservation rate R.  This\n      division is done because,
    as with the example of serializing the\n      datagram, the effect of the C term
    is a function of the\n      transmission rate.  Implementors should take care
    to confirm that\n      their C values, when divided by various rates, give appropriate\n
    \     results.  Delay values that are not dependent on the rate SHOULD\n      be
    incorporated into the value for the D parameter.\n   The error term D is the rate-independent,
    per-element error term and\n   represents the worst case non-rate-based transit
    time variation\n   through the service element.  It is generally determined or
    set at\n   boot or configuration time.  An example of D is a slotted network,
    in\n   which guaranteed flows are assigned particular slots in a cycle of\n   slots.
    \ Some part of the per-flow delay may be determined by which\n   slots in the
    cycle are allocated to the flow.  In this case, D would\n   measure the maximum
    amount of time a flow's data, once ready to be\n   sent, might have to wait for
    a slot.  (Observe that this value can be\n   computed before slots are assigned
    and thus can be advertised.  For\n   instance, imagine there are 100 slots.  In
    the worst case, a flow\n   might get all of its N slots clustered together, such
    that if a\n   packet was made ready to send just after the cluster ended, the\n
    \  packet might have to wait 100-N slot times before transmitting.  In\n   this
    case one can easily approximate this delay by setting D to 100\n   slot times).\n
    \  If the composition function is applied along the entire path to\n   compute
    the end-to-end sums of C and D (Ctot and Dtot) and the\n   resulting values are
    then provided to the end nodes (by presumably\n   the setup protocol), the end
    nodes can compute the maximal datagram\n   queueing delays.  Moreover, if the
    partial sums (Csum and Dsum) from\n   the most recent reshaping point (reshaping
    points are defined below)\n   downstream towards receivers are handed to each
    network element then\n   these network elements can compute the buffer allocations
    necessary\n   to achieve no datagram loss, as detailed in the section Guidelines\n
    \  for Implementors.  The proper use and provision of this service\n   requires
    that the quantities Ctot and Dtot, and the quantities Csum\n   and Dsum be computed.
    \ Therefore, we assume that usage of guaranteed\n   service will be primarily
    in contexts where these quantities are made\n   available to end nodes and network
    elements.\n   The error term C is measured in units of bytes.  An individual\n
    \  element can advertise a C value between 1 and 2**28 (a little over\n   250
    megabytes) and the total added over all elements can range as\n   high as (2**32)-1.
    \ Should the sum of the different elements delay\n   exceed (2**32)-1, the end-to-end
    error term MUST be set to (2**32)-1.\n   The error term D is measured in units
    of one microsecond.  An\n   individual element can advertise a delay value between
    1 and 2**28\n   (somewhat over two minutes) and the total delay added over all\n
    \  elements can range as high as (2**32)-1.  Should the sum of the\n   different
    elements delay exceed (2**32)-1, the end-to-end delay MUST\n   be set to (2**32)-1.\n
    \  The guaranteed service is service_name 2.\n   The RSpec parameter is numbered
    130.\n   Error characterization parameters C and D are numbered 131 and 132.\n
    \  The end-to-end composed values for C and D (Ctot and Dtot) are\n   numbered
    133 and 134.  The since-last-reshaping point composed values\n   for C and D (Csum
    and Dsum) are numbered 135 and 136.\n"
  title: Exported Information
- contents:
  - "Policing\n   There are two forms of policing in guaranteed service.  One form
    is\n   simple policing (hereafter just called policing to be consistent with\n
    \  other documents), in which arriving traffic is compared against a\n   TSpec.
    \ The other form is reshaping, where an attempt is made to\n   restore (possibly
    distorted) traffic's shape to conform to the TSpec,\n   and the fact that traffic
    is in violation of the TSpec is discovered\n   because the reshaping fails (the
    reshaping buffer overflows).\n   Policing is done at the edge of the network.
    \ Reshaping is done at\n   all heterogeneous source branch points and at all source
    merge\n   points.  A heterogeneous source branch point is a spot where the\n   multicast
    distribution tree from a source branches to multiple\n   distinct paths, and the
    TSpec's of the reservations on the various\n   outgoing links are not all the
    same.  Reshaping need only be done if\n   the TSpec on the outgoing link is \"less
    than\" (in the sense described\n   in the Ordering section) the TSpec reserved
    on the immediately\n   upstream link.  A source merge point is where the distribution
    paths\n   or trees from two different sources (sharing the same reservation)\n
    \  merge.  It is the responsibility of the invoker of the service (a\n   setup
    protocol, local configuration tool, or similar mechanism) to\n   identify points
    where policing is required.  Reshaping may be done at\n   other points as well
    as those described above.  Policing MUST not be\n   done except at the edge of
    the network.\n   The token bucket and peak rate parameters require that traffic
    MUST\n   obey the rule that over all time periods, the amount of data sent\n   cannot
    exceed M+min[pT, rT+b-M], where r and b are the token bucket\n   parameters, M
    is the maximum datagram size, and T is the length of\n   the time period (note
    that when p is infinite this reduces to the\n   standard token bucket requirement).
    \ For the purposes of this\n   accounting, links MUST count datagrams which are
    smaller than the\n   minimum policing unit to be of size m.  Datagrams which arrive
    at an\n   element and cause a violation of the the M+min[pT, rT+b-M] bound are\n
    \  considered non-conformant.\n   At the edge of the network, traffic is policed
    to ensure it conforms\n   to the token bucket.  Non-conforming datagrams SHOULD
    be treated as\n   best-effort datagrams.  [If and when a marking ability becomes\n
    \  available, these non-conformant datagrams SHOULD be ''marked'' as\n   being
    non-compliant and then treated as best effort datagrams at all\n   subsequent
    routers.]\n   Best effort service is defined as the default service a network\n
    \  element would give to a datagram that is not part of a flow and was\n   sent
    between the flow's source and destination.  Among other\n   implications, this
    definition means that if a flow's datagram is\n   changed to a best effort datagram,
    all flow control (e.g., RED [2])\n   that is normally applied to best effort datagrams
    is applied to that\n   datagram too.\n      NOTE: There may be situations outside
    the scope of this document,\n      such as when a service module's implementation
    of guaranteed\n      service is being used to implement traffic sharing rather
    than a\n      quality of service, where the desired action is to discard non-\n
    \     conforming datagrams.  To allow for such uses, implementors SHOULD\n      ensure
    that the action to be taken for non-conforming datagrams is\n      configurable.\n
    \  Inside the network, policing does not produce the desired results,\n   because
    queueing effects will occasionally cause a flow's traffic\n   that entered the
    network as conformant to be no longer conformant at\n   some downstream network
    element.  Therefore, inside the network,\n   network elements that wish to police
    traffic MUST do so by reshaping\n   traffic to the token bucket.  Reshaping entails
    delaying datagrams\n   until they are within conformance of the TSpec.\n   Reshaping
    is done by combining a buffer with a token bucket and peak\n   rate regulator
    and buffering data until it can be sent in conformance\n   with the token bucket
    and peak rate parameters.  (The token bucket\n   regulator MUST start with its
    token bucket full of tokens).  Under\n   guaranteed service, the amount of buffering
    required to reshape any\n   conforming traffic back to its original token bucket
    shape is\n   b+Csum+(Dsum*r), where Csum and Dsum are the sums of the parameters
    C\n   and D between the last reshaping point and the current reshaping\n   point.
    \ Note that the knowledge of the peak rate at the reshapers can\n   be used to
    reduce these buffer requirements (see the section on\n   \"Guidelines for Implementors\"
    below).  A network element MUST provide\n   the necessary buffers to ensure that
    conforming traffic is not lost\n   at the reshaper.\n      NOTE: Observe that
    a router that is not reshaping can still\n      identify non-conforming datagrams
    (and discard them or schedule\n      them at lower priority) by observing when
    queued traffic for the\n      flow exceeds b+Csum+(Dsum*r).\n   If a datagram
    arrives to discover the reshaping buffer is full, then\n   the datagram is non-conforming.
    \ Observe this means that a reshaper\n   is effectively policing too.  As with
    a policer, the reshaper SHOULD\n   relegate non-conforming datagrams to best effort.
    \ [If marking is\n   available, the non-conforming datagrams SHOULD be marked]\n
    \     NOTE: As with policers, it SHOULD be possible to configure how\n      reshapers
    handle non-conforming datagrams.\n   Note that while the large buffer makes it
    appear that reshapers add\n   considerable delay, this is not the case.  Given
    a valid TSpec that\n   accurately describes the traffic, reshaping will cause
    little extra\n   actual delay at the reshaping point (and will not affect the
    delay\n   bound at all).  Furthermore, in the normal case, reshaping will not\n
    \  cause the loss of any data.\n   However, (typically at merge or branch points),
    it may happen that\n   the TSpec is smaller than the actual traffic.  If this
    happens,\n   reshaping will cause a large queue to develop at the reshaping point,\n
    \  which both causes substantial additional delays and forces some\n   datagrams
    to be treated as non-conforming.  This scenario makes an\n   unpleasant denial
    of service attack possible, in which a receiver who\n   is successfully receiving
    a flow's traffic via best effort service is\n   pre-empted by a new receiver who
    requests a reservation for the flow,\n   but with an inadequate TSpec and RSpec.
    \ The flow's traffic will now\n   be policed and possibly reshaped.  If the policing
    function was\n   chosen to discard datagrams, the best-effort receiver would stop\n
    \  receiving traffic.  For this reason, in the normal case, policers are\n   simply
    to treat non-conforming datagrams as best effort (and marking\n   them if marking
    is implemented).  While this protects against denial\n   of service, it is still
    true that the bad TSpec may cause queueing\n   delays to increase.\n      NOTE:
    To minimize problems of reordering datagrams, reshaping\n      points may wish
    to forward a best-effort datagram from the front\n      of the reshaping queue
    when a new datagram arrives and the\n      reshaping buffer is full.\n      Readers
    should also observe that reclassifying datagrams as best\n      effort (as opposed
    to dropping the datagrams) also makes support\n      for elastic flows easier.
    \ They can reserve a modest token bucket\n      and when their traffic exceeds
    the token bucket, the excess\n      traffic will be sent best effort.\n   A related
    issue is that at all network elements, datagrams bigger\n   than the MTU of the
    network element MUST be considered non-conformant\n   and SHOULD be classified
    as best effort (and will then either be\n   fragmented or dropped according to
    the element's handling of best\n   effort traffic).  [Again, if marking is available,
    these reclassified\n   datagrams SHOULD be marked.]\n"
  title: Policing
- contents:
  - "Ordering and Merging\n   TSpec's are ordered according to the following rules.\n
    \  TSpec A is a substitute (\"as good or better than\") for TSpec B if (1)\n   both
    the token rate r and bucket depth b for TSpec A are greater than\n   or equal
    to those of TSpec B; (2) the peak rate p is at least as\n   large in TSpec A as
    it is in TSpec B; (3) the minimum policed unit m\n   is at least as small for
    TSpec A as it is for TSpec B; and (4) the\n   maximum datagram size M is at least
    as large for TSpec A as it is for\n   TSpec B.\n   TSpec A is \"less than or equal\"
    to TSpec B if (1) both the token rate\n   r and bucket depth b for TSpec A are
    less than or equal to those of\n   TSpec B; (2) the peak rate p in TSpec A is
    at least as small as the\n   peak rate in TSpec B; (3) the minimum policed unit
    m is at least as\n   large for TSpec A as it is for TSpec B; and (4) the maximum
    datagram\n   size M is at least as small for TSpec A as it is for TSpec B.\n   A
    merged TSpec may be calculated over a set of TSpecs by taking (1)\n   the largest
    token bucket rate, (2) the largest bucket size, (3) the\n   largest peak rate,
    (4) the smallest minimum policed unit, and (5) the\n   smallest maximum datagram
    size across all members of the set.  This\n   use of the word \"merging\" is similar
    to that in the RSVP protocol\n   [10]; a merged TSpec is one which is adequate
    to describe the traffic\n   from any one of constituent TSpecs.\n   A summed TSpec
    may be calculated over a set of TSpecs by computing\n   (1) the sum of the token
    bucket rates, (2) the sum of the bucket\n   sizes, (3) the sum of the peak rates,
    (4) the smallest minimum\n   policed unit, and (5) the maximum datagram size parameter.\n
    \  A least common TSpec is one that is sufficient to describe the\n   traffic
    of any one in a set of traffic flows.  A least common TSpec\n   may be calculated
    over a set of TSpecs by computing: (1) the largest\n   token bucket rate, (2)
    the largest bucket size, (3) the largest peak\n   rate, (4) the smallest minimum
    policed unit, and (5) the largest\n   maximum datagram size across all members
    of the set.\n   The minimum of two TSpecs differs according to whether the TSpecs
    can\n   be ordered.  If one TSpec is less than the other TSpec, the smaller\n
    \  TSpec is the minimum.  Otherwise, the minimum TSpec of two TSpecs is\n   determined
    by comparing the respective values in the two TSpecs and\n   choosing (1) the
    smaller token bucket rate, (2) the larger token\n   bucket size (3) the smaller
    peak rate, (4) the smaller minimum\n   policed unit, and (5) the smaller maximum
    datagram size.\n   The RSpec's are merged in a similar manner as the TSpecs, i.e.
    a set\n   of RSpecs is merged onto a single RSpec by taking the largest rate R,\n
    \  and the smallest slack S.  More precisely, RSpec A is a substitute\n   for
    RSpec B if the value of reserved service rate, R, in RSpec A is\n   greater than
    or equal to the value in RSpec B, and the value of the\n   slack, S, in RSpec
    A is smaller than or equal to that in RSpec B.\n   Each network element receives
    a service request of the form (TSpec,\n   RSpec), where the RSpec is of the form
    (Rin, Sin).  The network\n   element processes this request and performs one of
    two actions:\n    a. it accepts the request and returns a new Rspec of the form\n
    \      (Rout, Sout);\n    b. it rejects the request.\n   The processing rules
    for generating the new RSpec are governed by the\n   delay constraint:\n          Sout
    + b/Rout + Ctoti/Rout <= Sin + b/Rin + Ctoti/Rin,\n   where Ctoti is the cumulative
    sum of the error terms, C, for all the\n   network elements that are upstream
    of and including the current\n   element, i.  In other words, this element consumes
    (Sin - Sout) of\n   slack and can use it to reduce its reservation level, provided
    that\n   the above inequality is satisfied.  Rin and Rout MUST also satisfy\n
    \  the constraint:\n                             r <= Rout <= Rin.\n   When several
    RSpec's, each with rate Rj, j=1,2..., are to be merged\n   at a split point, the
    value of Rout is the maximum over all the rates\n   Rj, and the value of Sout
    is the minimum over all the slack terms Sj.\n      NOTE: The various TSpec functions
    described above are used by\n      applications which desire to combine TSpecs.
    \ It is important to\n      observe, however, that the properties of the actual
    reservation\n      are determined by combining the TSpec with the RSpec rate (R).\n
    \     Because the guaranteed reservation requires both the TSpec and the\n      RSpec
    rate, there exist some difficult problems for shared\n      reservations in RSVP,
    particularly where two or more source\n      streams meet.  Upstream of the meeting
    point, it would be\n      desirable to reduce the TSpec and RSpec to use only
    as much\n      bandwidth and buffering as is required by the individual source's\n
    \     traffic.  (Indeed, it may be necessary if the sender is\n      transmitting
    over a low bandwidth link).\n      However, the RSpec's rate is set to achieve
    a particular delay\n      bound (and is notjust a function of the TSpec), so changing
    the\n      RSpec may cause the reservation to fail to meet the receiver's\n      delay
    requirements.  At the same time, not adjusting the RSpec\n      rate means that
    \"shared\" RSVP reservations using guaranteed\n      service will fail whenever
    the bandwidth available at a particular\n      link is less than the receiver's
    requested rate R, even if the\n      bandwidth is adequate to support the number
    of senders actually\n      using the link.  At this time, this limitation is an
    open problem\n      in using the guaranteed service with RSVP.\n"
  title: Ordering and Merging
- contents:
  - "Guidelines for Implementors\n   This section discusses a number of important
    implementation issues in\n   no particular order.\n   It is important to note
    that individual subnetworks are network\n   elements and both routers and subnetworks
    MUST support the guaranteed\n   service model to achieve guaranteed service.  Since
    subnetworks\n   typically are not capable of negotiating service using IP-based\n
    \  protocols, as part of providing guaranteed service, routers will have\n   to
    act as proxies for the subnetworks they are attached to.\n   In some cases, this
    proxy service will be easy.  For instance, on\n   leased line managed by a WFQ
    scheduler on the upstream node, the\n   proxy need simply ensure that the sum
    of all the flows' RSpec rates\n   does not exceed the bandwidth of the line, and
    needs to advertise the\n   rate-based and non-rate-based delays of the link as
    the values of C\n   and D.\n   In other cases, this proxy service will be complex.
    \ In an ATM\n   network, for example, it may require establishing an ATM VC for
    the\n   flow and computing the C and D terms for that VC.  Readers may\n   observe
    that the token bucket and peak rate used by guaranteed\n   service map directly
    to the Sustained Cell Rate, Burst Size, and Peak\n   Cell Rate of ATM's Q.2931
    QoS parameters for Variable Bit Rate\n   traffic.\n   The assurance that datagrams
    will not be lost is obtained by setting\n   the router buffer space B to be equal
    to the token bucket b plus some\n   error term (described below).\n   Another
    issue related to subnetworks is that the TSpec's token bucket\n   rates measure
    IP traffic and do not (and cannot) account for link\n   level headers.  So the
    subnetwork network elements MUST adjust the\n   rate and possibly the bucket size
    to account for adding link level\n   headers.  Tunnels MUST also account for the
    additional IP headers\n   that they add.\n   For datagram networks, a maximum
    header rate can usually be computed\n   by dividing the rate and bucket sizes
    by the minimum policed unit.\n   For networks that do internal fragmentation,
    such as ATM, the\n   computation may be more complex, since one MUST account for
    both\n   per-fragment overhead and any wastage (padding bytes transmitted) due\n
    \  to mismatches between datagram sizes and fragment sizes.  For\n   instance,
    a conservative estimate of the additional data rate imposed\n   by ATM AAL5 plus
    ATM segmentation and reassembly is\n                         ((r/48)*5)+((r/m)*(8+52))\n
    \  which represents the rate divided into 48-byte cells multiplied by\n   the
    5-byte ATM header, plus the maximum datagram rate (r/m)\n   multiplied by the
    cost of the 8-byte AAL5 header plus the maximum\n   space that can be wasted by
    ATM segmentation of a datagram (which is\n   the 52 bytes wasted in a cell that
    contains one byte).  But this\n   estimate is likely to be wildly high, especially
    if m is small, since\n   ATM wastage is usually much less than 52 bytes.  (ATM
    implementors\n   should be warned that the token bucket may also have to be scaled\n
    \  when setting the VC parameters for call setup and that this example\n   does
    not account for overhead incurred by encapsulations such as\n   those specified
    in RFC 1483).\n   To ensure no loss, network elements will have to allocate some\n
    \  buffering for bursts.  If every hop implemented the fluid model\n   perfectly,
    this buffering would simply be b (the token bucket size).\n   However, as noted
    in the discussion of reshaping earlier,\n   implementations are approximations
    and we expect that traffic will\n   become more bursty as it goes through the
    network.  However, as with\n   shaping the amount of buffering required to handle
    the burstiness is\n   bounded by b+Csum+Dsum*R.  If one accounts for the peak
    rate, this\n   can be further reduced to\n                  M + (b-M)(p-X)/(p-r)
    + (Csum/R + Dsum)X\n   where X is set to r if (b-M)/(p-r) is less than Csum/R+Dsum
    and X is\n   R if (b-M)/(p-r) is greater than or equal to Csum/R+Dsum and p>R;\n
    \  otherwise, X is set to p.  This reduction comes from the fact that\n   the
    peak rate limits the rate at which the burst, b, can be placed in\n   the network.
    \ Conversely, if a non-zero slack term, Sout, is returned\n   by the network element,
    the buffer requirements are increased by\n   adding Sout to Dsum.\n   While sending
    applications are encouraged to set the peak rate\n   parameter and reshaping points
    are required to conform to it, it is\n   always acceptable to ignore the peak
    rate for the purposes of\n   computing buffer requirements and end-to-end delays.
    \ The result is\n   simply an overestimate of the buffering and delay.  As noted
    above,\n   if the peak rate is unknown (and thus potentially infinite), the\n
    \  buffering required is b+Csum+Dsum*R.  The end-to-end delay without\n   the
    peak rate is b/R+Ctot/R+Dtot.\n   The parameter D for each network element SHOULD
    be set to the maximum\n   datagram transfer delay variation (independent of rate
    and bucket\n   size) through the network element.  For instance, in a simple router,\n
    \  one might compute the difference between the worst case and best case\n   times
    it takes for a datagram to get through the input interface to\n   the processor,
    and add it to any variation that may occur in how long\n   it would take to get
    from the processor to the outbound link\n   scheduler (assuming the queueing schemes
    work correctly).\n   For weighted fair queueing in a datagram environment, D is
    set to the\n   link MTU divided by the link bandwidth, to account for the\n   possibility
    that a packet arrives just as a maximum-sized packet\n   begins to be transmitted,
    and that the arriving packet should have\n   departed before the maximum-sized
    packet.  For a frame-based, slotted\n   system such as Stop and Go queueing, D
    is the maximum number of slots\n   a datagram may have to wait before getting
    a chance to be\n   transmitted.\n   Note that multicasting may make determining
    D more difficult.  In\n   many subnets, ATM being one example, the properties
    of the subnet may\n   depend on the path taken from the multicast sender to the
    receiver.\n   There are a number of possible approaches to this problem.  One
    is to\n   choose a representative latency for the overall subnet and set D to\n
    \  the (non-negative) difference from that latency.  Another is to\n   estimate
    subnet properties at exit points from the subnet, since the\n   exit point presumably
    is best placed to compute the properties of its\n   path from the source.\n      NOTE:
    It is important to note that there is no fixed set of rules\n      about how a
    subnet determines its properties, and each subnet\n      technology will have
    to develop its own set of procedures to\n      accurately compute C and D and
    slack values.\n   D is intended to be distinct from the latency through the network\n
    \  element.  Latency is the minimum time through the device (the speed\n   of
    light delay in a fiber or the absolute minimum time it would take\n   to move
    a packet through a router), while parameter D is intended to\n   bound the variability
    in non-rate-based delay.  In practice, this\n   distinction is sometimes arbitrary
    (the latency may be minimal) -- in\n   such cases it is perfectly reasonable to
    combine the latency with D\n   and to advertise any latency as zero.\n      NOTE:
    It is implicit in this scheme that to get a complete\n      guarantee of the maximum
    delay a packet might experience, a user\n      of this service will need to know
    both the queueing delay\n      (provided by C and D) and the latency.  The latency
    is not\n      advertised by this service but is a general characterization\n      parameter
    (advertised as specified in [8]).\n      However, even if latency is not advertised,
    this service can still\n      be used.  The simplest approach is to measure the
    delay\n      experienced by the first packet (or the minimum delay of the first\n
    \     few packets) received and treat this delay value as an upper bound\n      on
    the latency.\n   The parameter C is the data backlog resulting from the vagaries
    of\n   how a specific implementation deviates from a strict bit-by-bit\n   service.
    So, for instance, for datagramized weighted fair queueing, C\n   is set to M to
    account for packetization effects.\n   If a network element uses a certain amount
    of slack, Si, to reduce\n   the amount of resources that it has reserved for a
    particular flow,\n   i, the value Si SHOULD be stored at the network element.\n
    \  Subsequently, if reservation refreshes are received for flow i, the\n   network
    element MUST use the same slack Si without any further\n   computation. This guarantees
    consistency in the reservation process.\n   As an example for the use of the slack
    term, consider the case where\n   the required end-to-end delay, Dreq, is larger
    than the maximum delay\n   of the fluid flow system. The latter is obtained by
    setting R=r in\n   the fluid delay formula (for stability, R>=r must be true),
    and is\n   given by\n                           b/r + Ctot/r + Dtot.\n   In this
    case the slack term is\n                     S = Dreq - (b/r + Ctot/r + Dtot).\n
    \  The slack term may be used by the network elements to adjust their\n   local
    reservations, so that they can admit flows that would otherwise\n   have been
    rejected. A network element at an intermediate network\n   element that can internally
    differentiate between delay and rate\n   guarantees can now take advantage of
    this information to lower the\n   amount of resources allocated to this flow.
    For example, by taking an\n   amount of slack s <= S, an RCSD scheduler [5] can
    increase the local\n   delay bound, d, assigned to the flow, to d+s. Given an
    RSpec, (Rin,\n   Sin), it would do so by setting Rout = Rin and Sout = Sin - s.\n
    \  Similarly, a network element using a WFQ scheduler can decrease its\n   local
    reservation from Rin to Rout by using some of the slack in the\n   RSpec. This
    can be accomplished by using the transformation rules\n   given in the previous
    section, that ensure that the reduced\n   reservation level will not increase
    the overall end-to-end delay.\n"
  title: Guidelines for Implementors
- contents:
  - "Evaluation Criteria\n   The scheduling algorithm and admission control algorithm
    of the\n   element MUST ensure that the delay bounds are never violated and\n
    \  datagrams are not lost, when a source's traffic conforms to the\n   TSpec.
    \ Furthermore, the element MUST ensure that misbehaving flows\n   do not affect
    the service given to other flows.  Vendors are\n   encouraged to formally prove
    that their implementation is an\n   approximation of the fluid model.\n"
  title: Evaluation Criteria
- contents:
  - "Examples of Implementation\n   Several algorithms and implementations exist that
    approximate the\n   fluid model.  They include Weighted Fair Queueing (WFQ) [2],
    Jitter-\n   EDD [3], Virtual Clock [4] and a scheme proposed by IBM [5].  A nice\n
    \  theoretical presentation that shows these schemes are part of a large\n   class
    of algorithms can be found in [6].\n"
  title: Examples of Implementation
- contents:
  - "Examples of Use\n   Consider an application that is intolerant of any lost or
    late\n   datagrams.  It uses the advertised values Ctot and Dtot and the TSpec\n
    \  of the flow, to compute the resulting delay bound from a service\n   request
    with rate R. Assuming R < p, it then sets its playback point\n   to [(b-M)/R*(p-R)/(p-r)]+(M+Ctot)/R+Dtot.\n"
  title: Examples of Use
- contents:
  - "Security Considerations\n   This memo discusses how this service could be abused
    to permit denial\n   of service attacks.  The service, as defined, does not allow
    denial\n   of service (although service may degrade under certain\n   circumstances).\n"
  title: Security Considerations
- contents:
  - "Appendix 1: Use of the Guaranteed service with RSVP\n   The use of guaranteed
    service in conjunction with the RSVP resource\n   reservation setup protocol is
    specified in reference [9]. This\n   document gives the format of RSVP FLOWSPEC,
    SENDER_TSPEC, and ADSPEC\n   objects needed to support applications desiring guaranteed
    service\n   and gives information about how RSVP processes those objects. The\n
    \  RSVP protocol itself is specified in Reference [10].\n"
  title: 'Appendix 1: Use of the Guaranteed service with RSVP'
- contents:
  - "References\n   [1] Shenker, S., and J. Wroclawski, \"Network Element Service\n
    \  Specification Template\", RFC 2216, September 1997.\n   [2] A. Demers, S. Keshav
    and S. Shenker, \"Analysis and Simulation of\n   a Fair Queueing Algorithm,\"
    in Internetworking: Research and\n   Experience, Vol 1, No. 1., pp. 3-26.\n   [3]
    L. Zhang, \"Virtual Clock: A New Traffic Control Algorithm for\n   Packet Switching
    Networks,\" in Proc. ACM SIGCOMM '90, pp. 19-29.\n   [4] D. Verma, H. Zhang, and
    D. Ferrari, \"Guaranteeing Delay Jitter\n   Bounds in Packet Switching Networks,\"
    in Proc. Tricomm '91.\n   [5] L. Georgiadis, R. Guerin, V. Peris, and K. N. Sivarajan,\n
    \  \"Efficient Network QoS Provisioning Based on per Node Traffic\n   Shaping,\"
    IBM Research Report No. RC-20064.\n   [6] P. Goyal, S.S. Lam and H.M. Vin, \"Determining
    End-to-End Delay\n   Bounds in Heterogeneous Networks,\" in Proc. 5th Intl. Workshop
    on\n   Network and Operating System Support for Digital Audio and Video,\n   April
    1995.\n   [7] A.K.J. Parekh, A Generalized Processor Sharing Approach to Flow\n
    \  Control in Integrated Services Networks, MIT Laboratory for\n   Information
    and Decision Systems, Report LIDS-TH-2089, February 1992.\n   [8] Shenker, S.,
    and J. Wroclawski, \"General Characterization\n   Parameters for Integrated Service
    Network Elements\", RFC 2215,\n   September 1997.\n   [9] Wroclawski, J., \"Use
    of RSVP with IETF Integrated Services\", RFC\n   2210, September 1997.\n   [10]
    Braden, R., Ed., et. al., \"Resource Reservation Protocol (RSVP)\n   - Version
    1 Functional Specification\", RFC 2205, September 1997.\n"
  title: References
- contents:
  - "Authors' Addresses\n   Scott Shenker\n   Xerox PARC\n   3333 Coyote Hill Road\n
    \  Palo Alto, CA  94304-1314\n   Phone: 415-812-4840\n   Fax:   415-812-4471\n
    \  EMail: shenker@parc.xerox.com\n   Craig Partridge\n   BBN\n   2370 Amherst
    St\n   Palo Alto CA 94306\n   EMail: craig@bbn.com\n   Roch Guerin\n   IBM T.J.
    Watson Research Center\n   Yorktown Heights, NY 10598\n   Phone: 914-784-7038\n
    \  Fax:   914-784-6318\n   EMail: guerin@watson.ibm.com\n"
  title: Authors' Addresses
