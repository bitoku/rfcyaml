- title: __initial_text__
  contents:
  - "                   Network Time Protocol (Version 1)\n                    Specification\
    \ and Implementation\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo describes the Network Time Protocol (NTP),\
    \ specifies its\n   formal structure and summarizes information useful for its\n\
    \   implementation.  NTP provides the mechanisms to synchronize time and\n   coordinate\
    \ time distribution in a large, diverse internet operating\n   at rates from mundane\
    \ to lightwave.  It uses a returnable-time design\n   in which a distributed subnet\
    \ of time servers operating in a self-\n   organizing, hierarchical master-slave\
    \ configuration synchronizes\n   logical clocks within the subnet and to national\
    \ time standards via\n   wire or radio.  The servers can also redistribute reference\
    \ time via\n   local routing algorithms and time daemons.\n   The NTP architectures,\
    \ algorithms and protocols which have evolved\n   over several years of implementation\
    \ and refinement are described in\n   this document.  The prototype system, which\
    \ has been in regular\n   operation in the Internet for the last two years, is\
    \ described in an\n   Appendix along with performance data which shows that timekeeping\n\
    \   accuracy throughout most portions of the Internet can be ordinarily\n   maintained\
    \ to within a few tens of milliseconds, even in cases of\n   failure or disruption\
    \ of clocks, time servers or nets.  This is a\n   Draft Standard for an Elective\
    \ protocol.  Distribution of this memo\n   is unlimited.\n                   \
    \          Table of Contents\n   1.      Introduction                        \
    \                       3\n   1.1.    Related Technology                     \
    \                    4\n   2.      System Architecture                       \
    \                 6\n   2.1.    Implementation Model                         \
    \              7\n   2.2.    Network Configurations                          \
    \           9\n   2.3.    Time Scales                                        \
    \       10\n   3.      Network Time Protocol                                 \
    \    12\n   3.1.    Data Formats                                             \
    \ 12\n   3.2.    State Variables and Parameters                            13\n\
    \   3.2.1.  Common Variables                                          15\n   3.2.2.\
    \  System Variables                                          17\n   3.2.3.  Peer\
    \ Variables                                            18\n   3.2.4.  Packet Variables\
    \                                          19\n   3.2.5.  Clock Filter Variables\
    \                                    19\n   3.2.6.  Parameters               \
    \                                 20\n   3.3.    Modes of Operation          \
    \                              21\n   3.4.    Event Processing               \
    \                           22\n   3.4.1.  Timeout Procedure                 \
    \                        23\n   3.4.2.  Receive Procedure                    \
    \                     24\n   3.4.3.  Update Procedure                        \
    \                  27\n   3.4.4.  Initialization Procedures                  \
    \               29\n   4.      Filtering and Selection Algorithms            \
    \            29\n   4.1.    Clock Filter Algorithm                           \
    \         29\n   4.2     Clock Selection Algorithm                           \
    \      30\n   4.3.    Variable-Rate Polling                                  \
    \   32\n   5.      Logical Clocks                                            33\n\
    \   5.1.    Uniform Phase Adjustments                                 35\n   5.2.\
    \    Nonuniform Phase Adjustments                              36\n   5.3.   \
    \ Maintaining Date and Time                                 37\n   5.4.    Calculating\
    \ Estimates                                     37\n   6.      References    \
    \                                            40\n   Appendices\n   Appendix A.\
    \ UDP Header Format                                     43\n   Appendix B. NTP\
    \ Data Format                                       44\n   Appendix C. Timeteller\
    \ Experiments                                47\n   Appendix D. Evaluation of\
    \ Filtering Algorithms                    49\n   Appendix E. NTP Synchronization\
    \ Networks                          56\n   List of Figures\n   Figure 2.1. Implementation\
    \ Model                                   8\n   Figure 3.1. Calculating Delay\
    \ and Offset                          26\n   Figure 5.1. Clock Registers     \
    \                                  34\n   Figure D.1. Calculating Delay and Offset\
    \                          50\n   Figure E.1. Primary Service Network        \
    \                       57\n   List of Tables\n   Table 2.1. Dates of Leap-Second\
    \ Insertion                         11\n   Table 3.1. System Variables       \
    \                                14\n   Table 3.2. Peer Variables            \
    \                             14\n   Table 3.3. Packet Variables             \
    \                          15\n   Table 3.4. Parameters                      \
    \                       15\n   Table 4.1. Outlyer Selection Procedure        \
    \                    32\n   Table 5.1. Clock Parameters                      \
    \                 35\n   Table C.1. Distribution Functions                   \
    \              47\n   Table D.1. Delay and Offset Measurements (UMD)         \
    \           52\n   Table D.2.a Delay and Offset Measurements (UDEL)          \
    \        52\n   Table D.2.b Offset Measurements (UDEL)                       \
    \     53\n   Table D.3. Minimum Filter (UMD - NCAR)                          \
    \  54\n   Table D.4. Median Filter (UMD - NCAR)                             54\n\
    \   Table D.5. Minimum Filter (UDEL - NCAR)                           55\n   Table\
    \ E.1. Primary Servers                                        56\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document describes the Network Time Protocol (NTP),\
    \ including\n   the architectures, algorithms and protocols to synchronize local\n\
    \   clocks in a set of distributed clients and servers.  The protocol was\n  \
    \ first described in RFC-958 [24], but has evolved in significant ways\n   since\
    \ publication of that document.  NTP is built on the Internet\n   Protocol (IP)\
    \ [10] and User Datagram Protocol (UDP) [6], which\n   provide a connectionless\
    \ transport mechanism;  however, it is readily\n   adaptable to other protocol\
    \ suites.  It is evolved from the Time\n   Protocol [13] and the ICMP Timestamp\
    \ message [11], but is\n   specifically designed to maintain accuracy and robustness,\
    \ even when\n   used over typical Internet paths involving multiple gateways and\n\
    \   unreliable nets.\n   The service environment consists of the implementation\
    \ model, service\n   model and time scale described in Section 2.  The implementation\n\
    \   model is based on a multiple-process operating system architecture,\n   although\
    \ other architectures could be used as well.  The service\n   model is based on\
    \ a returnable-time design which depends only on\n   measured offsets, or skews,\
    \ but does not require reliable message\n   delivery.  The subnet is a self-organizing,\
    \ hierarchical master-slave\n   configuration, with synchronization paths determined\
    \ by a minimum-\n   weight spanning tree.  While multiple masters (primary servers)\
    \ may\n   exist, there is no requirement for an election protocol.\n   NTP itself\
    \ is described in Section 3.  It provides the protocol\n   mechanisms to synchronize\
    \ time in principle to precisions in the\n   order of nanoseconds while preserving\
    \ a non-ambiguous date well into\n   the next century.  The protocol includes\
    \ provisions to specify the\n   characteristics and estimate the error of the\
    \ local clock and the\n   time server to which it may be synchronized.  It also\
    \ includes\n   provisions for operation with a number of mutually suspicious,\n\
    \   hierarchically distributed primary reference sources such as radio\n   clocks.\n\
    \   Section 4 describes algorithms useful for deglitching and smoothing\n   clock-offset\
    \ samples collected on a continuous basis.  These\n   algorithms began with those\
    \ suggested in [22], were refined as the\n   results of experiments described\
    \ in [23] and further evolved under\n   typical operating conditions over the\
    \ last two years.  In addition,\n   as the result of experience in operating multiple-server\
    \ nets\n   including radio-synchronized clocks at several sites in the US and\n\
    \   with clients in the US and Europe, reliable algorithms for selecting\n   good\
    \ clocks from a population possibly including broken ones have\n   been developed\
    \ and are described in Section 4.\n   The accuracies achievable by NTP depend\
    \ strongly on the precision of\n   the local clock hardware and stringent control\
    \ of device and process\n   latencies.  Provisions must be included to adjust\
    \ the software\n   logical clock time and frequency in response to corrections\
    \ produced\n   by NTP.  Section 5 describes a logical clock design evolved from\
    \ the\n   Fuzzball implementation described in [15].  This design includes\n \
    \  offset-slewing, drift-compensation and deglitching mechanisms capable\n   of\
    \ accuracies in order of a millisecond, even after extended periods\n   when synchronization\
    \ to primary reference sources has been lost.\n   The UDP and NTP packet formats\
    \ are shown in Appendices A and B.\n   Appendix C presents the results of a survey\
    \ of about 5500 Internet\n   hosts showing how their clocks compare with primary\
    \ reference sources\n   using three different time protocols, including NTP. \
    \ Appendix D\n   presents experimental results using several different deglitching\
    \ and\n   smoothing algorithms.  Appendix E describes the prototype NTP primary\n\
    \   service net, as well as proposed rules of engagement for its use.\n"
- title: 1.1.  Related Technology
  contents:
  - "1.1.  Related Technology\n   Other mechanisms have been specified in the Internet\
    \ protocol suite\n   to record and transmit the time at which an event takes place,\n\
    \   including the Daytime protocol [12], Time Protocol [13], ICMP\n   Timestamp\
    \ message [11] and IP Timestamp option [9].  Experimental\n   results on measured\
    \ times and roundtrip delays in the Internet are\n   discussed in [14], [23] and\
    \ [31].  Other synchronization protocols\n   are discussed in [7], [17], [20]\
    \ and [28].  NTP uses techniques\n   evolved from both linear and nonlinear synchronization\
    \ methodology.\n   Linear methods used for digital telephone network synchronization\
    \ are\n   summarized in [3], while nonlinear methods used for process\n   synchronization\
    \ are summarized in [27].\n   The Fuzzball routing protocol [15], sometimes called\
    \ Hellospeak,\n   incorporates time synchronization directly into the routing\
    \ protocol\n   design.  One or more processes synchronize to an external reference\n\
    \   source, such as a radio clock or NTP daemon, and the routing\n   algorithm\
    \ constructs a minimum-weight spanning tree rooted on these\n   processes.  The\
    \ clock offsets are then distributed along the arcs of\n   the spanning tree to\
    \ all processes in the system and the various\n   process clocks corrected using\
    \ the procedure described in Section 5\n   of this document.  While it can be\
    \ seen that the design of Hellospeak\n   strongly influenced the design of NTP,\
    \ Hellospeak itself is not an\n   Internet protocol and is unsuited for use outside\
    \ its local-net\n   environment.\n   The Unix 4.3bsd model [20] uses a single\
    \ master time daemon to\n   measure offsets of a number of slave hosts and send\
    \ periodic\n   corrections to them.  In this model the master is determined using\
    \ an\n   election algorithm [25] designed to avoid situations where either no\n\
    \   master is elected or more than one master is elected.  The election\n   process\
    \ requires a broadcast capability, which is not a ubiquitous\n   feature of the\
    \ Internet.  While this model has been extended to\n   support hierarchical configurations\
    \ in which a slave on one network\n   serves as a master on the other [28], the\
    \ model requires handcrafted\n   configuration tables in order to establish the\
    \ hierarchy and avoid\n   loops.  In addition to the burdensome, but presumably\
    \ infrequent,\n   overheads of the election process, the offset measurement/correction\n\
    \   process requires twice as many messages as NTP per update.\n   A good deal\
    \ of research has gone into the issue of maintaining\n   accurate time in a community\
    \ where some clocks cannot be trusted.  A\n   truechimer is a clock that maintains\
    \ timekeeping accuracy to a\n   previously published (and trusted) standard, while\
    \ a falseticker is a\n   clock that does not.  Determining whether a particular\
    \ clock is a\n   truechimer or falseticker is an interesting abstract problem\
    \ which\n   can be attacked using methods summarized in [19] and [27].\n   A convergence\
    \ function operates upon the offsets between the clocks\n   in a system to increase\
    \ the accuracy by reducing or eliminating\n   errors caused by falsetickers. \
    \ There are two classes of convergence\n   functions, those involving interactive\
    \ convergence algorithms and\n   those involving interactive consistency algorithms.\
    \  Interactive\n   convergence algorithms use statistical clustering techniques\
    \ such as\n   the fault-tolerant average algorithm of [17], the CNV algorithm\
    \ of\n   [19], the majority-subset algorithm of [22], the egocentric algorithm\n\
    \   of [27] and the algorithms in Section 4 of this document.\n   Interactive\
    \ consistency algorithms are designed to detect faulty\n   clock processes which\
    \ might indicate grossly inconsistent offsets in\n   successive readings or to\
    \ different readers.  These algorithms use an\n   agreement protocol involving\
    \ successive rounds of readings, possibly\n   relayed and possibly augmented by\
    \ digital signatures.  Examples\n   include the fireworks algorithm of [17] and\
    \ the optimum algorithm of\n   [30].  However, these algorithms require large\
    \ numbers of messages,\n   especially when large numbers of clocks are involved,\
    \ and are\n   designed to detect faults that have rarely been found in the Internet\n\
    \   experience.  For these reasons they are not considered further in\n   this\
    \ document.\n   In practice it is not possible to determine the truechimers from\
    \ the\n   falsetickers on other than a statistical basis, especially with\n  \
    \ hierarchical configurations and a statistically noisy Internet.\n   Thus, the\
    \ approach taken in this document and its predecessors\n   involves mutually coupled\
    \ oscillators and maximum-likelihood\n   estimation and selection procedures.\
    \  From the analytical point of\n   view, the system of distributed NTP peers\
    \ operates as a set of\n   coupled phase-locked oscillators, with the update algorithm\n\
    \   functioning as a phase detector and the logical clock as a voltage-\n   controlled\
    \ oscillator.  This similarity is not accidental, since\n   systems like this\
    \ have been studied extensively [3], [4] and [5].\n   The particular choice of\
    \ offset measurement and computation procedure\n   described in Section 3 is a\
    \ variant of the returnable-time system\n   used in some digital telephone networks\
    \ [3].  The clock filter and\n   selection algorithms are designed so that the\
    \ clock synchronization\n   subnet self-organizes into a hierarchical master-slave\
    \ configuration\n   [5].  What makes the NTP model unique is the adaptive configuration,\n\
    \   polling, filtering and selection functions which tailor the dynamics\n   of\
    \ the system to fit the ubiquitous Internet environment.\n"
- title: 2.  System Architecture
  contents:
  - "2.  System Architecture\n   The purpose of NTP is to connect a number of primary\
    \ reference\n   sources, synchronized to national standards by wire or radio,\
    \ to\n   widely accessible resources such as backbone gateways.  These\n   gateways,\
    \ acting as primary time servers, use NTP between them to\n   cross-check the\
    \ clocks and mitigate errors due to equipment or\n   propagation failures.  Some\
    \ number of local-net hosts or gateways,\n   acting as secondary time servers,\
    \ run NTP with one or more of the\n   primary servers.  In order to reduce the\
    \ protocol overhead the\n   secondary servers distribute time via NTP to the remaining\
    \ local-net\n   hosts.  In the interest of reliability, selected hosts can be\n\
    \   equipped with less accurate but less expensive radio clocks and used\n   for\
    \ backup in case of failure of the primary and/or secondary servers\n   or communication\
    \ paths between them.\n   There is no provision for peer discovery, acquisition,\
    \ or\n   authentication in NTP.  Data integrity is provided by the IP and UDP\n\
    \   checksums.  No circuit-management, duplicate-detection or\n   retransmission\
    \ facilities are provided or necessary.  The service can\n   operate in a symmetric\
    \ mode, in which servers and clients are\n   indistinguishable, yet maintain a\
    \ small amount of state information,\n   or in client/server mode, in which servers\
    \ need maintain no state\n   other than that contained in the client request.\
    \  A lightweight\n   association-management capability, including dynamic reachability\
    \ and\n   variable polling rate mechanisms, is included only to manage the\n \
    \  state information and reduce resource requirements.  Since only a\n   single\
    \ NTP message format is used, the protocol is easily implemented\n   and can be\
    \ used in a variety of solicited or unsolicited polling\n   mechanisms.\n   It\
    \ should be recognized that clock synchronization requires by its\n   nature long\
    \ periods and multiple comparisons in order to maintain\n   accurate timekeeping.\
    \  While only a few measurements are usually\n   adequate to reliably determine\
    \ local time to within a second or so,\n   periods of many hours and dozens of\
    \ measurements are required to\n   resolve oscillator drift and maintain local\
    \ time to the order of a\n   millisecond.  Thus, the accuracy achieved is directly\
    \ dependent on\n   the time taken to achieve it.  Fortunately, the frequency of\n\
    \   measurements can be quite low and almost always non-intrusive to\n   normal\
    \ net operations.\n"
- title: 2.1.  Implementation Model
  contents:
  - "2.1.  Implementation Model\n   In what may be the most common client/server model\
    \ a client sends an\n   NTP message to one or more servers and processes the replies\
    \ as\n   received.  The server interchanges addresses and ports, overwrites\n\
    \   certain fields in the message, recalculates the checksum and returns\n   the\
    \ message immediately.  Information included in the NTP message\n   allows the\
    \ client to determine the server time with respect to local\n   time and adjust\
    \ the logical clock accordingly.  In addition, the\n   message includes information\
    \ to calculate the expected timekeeping\n   accuracy and reliability, thus select\
    \ the best from possibly several\n   servers.\n   While the client/server model\
    \ may suffice for use on local nets\n   involving a public server and perhaps\
    \ many workstation clients, the\n   full generality of NTP requires distributed\
    \ participation of a number\n   of client/servers or peers arranged in a dynamically\
    \ reconfigurable,\n   hierarchically distributed configuration.  It also requires\n\
    \   sophisticated algorithms for association management, data\n   manipulation\
    \ and logical clock control.  Figure 2.1 shows a possible\n   implementation model\
    \ including four processes sharing a partitioned\n   data base, with a partition\
    \ dedicated to each peer and interconnected\n   by a message-passing system.\n\
    \                                +---------+\n                               \
    \ | Update  |\n                     +--------->|         +----------+\n      \
    \               |          |Algorithm|          |\n                     |    \
    \      +----+----+          |\n                     |               |        \
    \       |\n                     |               V               V\n          \
    \      +----+----+     +---------+     +---------+\n                |        \
    \ |     |  Local  |     |         |\n                | Receive |     |       \
    \  +---->| Timeout |\n                |         |     |  Clock  |     |      \
    \   |\n                +---------+     +---------+     +-+-----+-+\n         \
    \         A     A                         |     |\n                  |     | \
    \                        V     V\n                ===========================================\n\
    \                   Peers          Network          Peers\n                  \
    \   Figure 2.1. Implementation Model\n   The timeout process, driven by independent\
    \ timers for each peer,\n   collects information in the data base and sends NTP\
    \ messages to other\n   peers in the net.  Each message contains the local time\
    \ the message\n   is sent, together with previously received information and other\n\
    \   information necessary to compute the estimated error and manage the\n   association.\
    \  The message transmission rate is determined by the\n   accuracy expected of\
    \ the local system, as well as its peers.\n   The receive process receives NTP\
    \ messages and perhaps messages in\n   other protocols as well, including ICMP,\
    \ other UDP or TCP time\n   protocols, local-net protocols and directly connected\
    \ radio clocks.\n   When an NTP message is received the offset between the sender\
    \ clock\n   and the local clock is computed and incorporated into the data base\n\
    \   along with other information useful for error estimation and clock\n   selection.\n\
    \   The update algorithm is initiated upon receipt of a message and at\n   other\
    \ times.  It processes the offset data from each peer and selects\n   the best\
    \ peer using algorithms such as those described in Section 4.\n   This may involve\
    \ many observations of a few clocks or a few\n   observations of many clocks,\
    \ depending on the accuracies required.\n   The local clock process operates upon\
    \ the offset data produced by the\n   update algorithm and adjusts the phase and\
    \ frequency of the logical\n   clock using mechanisms such as described in Section\
    \ 5.  This may\n   result in either a step change or a gradual slew adjustment\
    \ of the\n   logical clock to reduce the offset to zero.  The logical clock\n\
    \   provides a stable source of time information to other users of the\n   system\
    \ and for subsequent reference by NTP itself.\n"
- title: 2.2.  Network Configurations
  contents:
  - "2.2.  Network Configurations\n   A primary time server is connected to a primary\
    \ reference source,\n   usually a radio clock synchronized to national standard\
    \ time.  A\n   secondary time server derives time synchronization, possibly via\n\
    \   other secondary servers, from a primary server.  Under normal\n   circumstances\
    \ it is intended that a subnet of primary and secondary\n   servers assumes a\
    \ hierarchical master-slave configuration with the\n   more accurate servers near\
    \ the top and the less accurate below.\n   Following conventions established by\
    \ the telephone industry, the\n   accuracy of each server is defined by a number\
    \ called its stratum,\n   with the stratum of a primary server assigned as one\
    \ and each level\n   downwards in the hierarchy assigned as one greater than the\
    \ preceding\n   level.  With current technology and available receiving equipment,\n\
    \   single-sample accuracies in the order of a millisecond can be\n   achieved\
    \ at the radio clock interface and in the order of a few\n   milliseconds at the\
    \ packet interface to the net.  Accuracies of this\n   order require special care\
    \ in the design and implementation of the\n   operating system, such as described\
    \ in [15], and the logical clock\n   mechanism, such as described in Section 5.\n\
    \   As the stratum increases from one, the single-sample accuracies\n   achievable\
    \ will degrade depending on the communication paths and\n   local clock stabilities.\
    \  In order to avoid the tedious calculations\n   [4] necessary to estimate errors\
    \ in each specific configuration, it\n   is useful to assume the errors accumulate\
    \ approximately in proportion\n   to the minimum total roundtrip path delay between\
    \ each server and the\n   primary reference source to which it is synchronized.\
    \  This is called\n   the synchronization distance.\n   Again drawing from the\
    \ experience of the telephone industry, who\n   learned such lessons at considerable\
    \ cost, the synchronization paths\n   should be organized to produce the highest\
    \ accuracies, but must never\n   be allowed to form a loop.  The clock filter\
    \ and selection algorithms\n   used in NTP accomplish this by using a variant\
    \ of the Bellman-Ford\n   distributed routing algorithm [29] to compute the minimum-weight\n\
    \   spanning trees rooted on the primary servers.  This results in each\n   server\
    \ operating at the lowest stratum and, in case of multiple peers\n   at the same\
    \ stratum, at the lowest synchronization distance.\n   As a result of the above\
    \ design, the subnet reconfigures\n   automatically in a hierarchical master-slave\
    \ configuration to produce\n   the most accurate time, even when one or more primary\
    \ or secondary\n   servers or the communication paths between them fail.  This\
    \ includes\n   the case where all normal primary servers (e.g.,  backbone WWVB\n\
    \   clocks) on a possibly partitioned subnet fail, but one or more backup\n  \
    \ primary servers (e.g., local WWV clocks) continue operation.\n   However, should\
    \ all primary servers throughout the subnet fail, the\n   remaining secondary\
    \ servers will synchronize among themselves for\n   some time and then gradually\
    \ drop off the subnet and coast using\n   their last offset and frequency computations.\
    \  Since these\n   computations are expected to be very precise, especially in\n\
    \   frequency, even extend outage periods of a day or more should result\n   in\
    \ timekeeping errors of not over a few tens of milliseconds.\n   In the case of\
    \ multiple primary servers, the spanning-tree\n   computation will usually select\
    \ the server at minimum synchronization\n   distance.  However, when these servers\
    \ are at approximately the same\n   distance, the computation may result in random\
    \ selections among them\n   as the result of normal dispersive delays.  Ordinarily\
    \ this does not\n   degrade accuracy as long as any discrepancy between the primary\n\
    \   servers is small compared to the synchronization distance.  If not,\n   the\
    \ filter and selection algorithms will select the best of the\n   available servers\
    \ and cast out outlyers as intended.\n"
- title: 2.3.  Time Scales
  contents:
  - "2.3.  Time Scales\n   Since 1972 the various national time scales have been based\
    \ on\n   International Atomic Time (TA), which is currently maintained using\n\
    \   multiple cesium-beam clocks to an accuracy of a few parts in 10^12.\n   The\
    \ Bureau International de l'Heure (BIH) uses astronomical\n   observations provided\
    \ by the US Naval Observatory and other\n   observatories to determine corrections\
    \ for small changes in the mean\n   rotation period of the Earth.  This results\
    \ in Universal Coordinated\n   Time (UTC), which is presently decreasing from\
    \ TA at a fraction of a\n   second per year.  When the magnitude of the correction\
    \ approaches 0.7\n   second, a leap second is inserted or deleted in the UTC time\
    \ scale on\n   the last day of June or December.  Further information on time\
    \ scales\n   can be found in [26].\n   For the most precise coordination and timestamping\
    \ of events since\n   1972 it is necessary to know when leap seconds were inserted\
    \ or\n   deleted in UTC and how the seconds are numbered.  A leap second is\n\
    \   inserted following second 23:59:59 on the last day of June or\n   December\
    \ and becomes second 23:59:60 of that day.  A leap second\n   would be deleted\
    \ by omitting second 23:59:59 on one of these days,\n   although this has never\
    \ happened.  Leap seconds were inserted on the\n   following fourteen occasions\
    \ prior to January 1988 (courtesy US Naval\n   Observatory):\n           1  June\
    \ 1972                    8  December 1978\n           2  December 1972      \
    \          9  December 1979\n           3  December 1973                10 June\
    \ 1981\n           4  December 1974                11 June 1982\n           5\
    \  December 1975                12 June 1983\n           6  December 1976    \
    \            13 June 1985\n           7  December 1977                14 December\
    \ 1987\n                 Table 2.1. Dates of Leap-Second Insertion\n   Like UTC,\
    \ NTP operates with an abstract oscillator synchronized in\n   frequency to the\
    \ TA time scale.  At 0000 hours on 1 January 1972 the\n   NTP time scale was set\
    \ to 2,272,060,800, representing the number of\n   TA seconds since 0000 hours\
    \ on 1 January 1900.  The insertion of leap\n   seconds in UTC does not affect\
    \ the oscillator itself, only the\n   translation between TA and UTC, or conventional\
    \ civil time.  However,\n   since the only institutional memory assumed by NTP\
    \ is the UTC radio\n   broadcast service, the NTP time scale is in effect reset\
    \ to UTC as\n   each offset estimate is computed.  When a leap second is inserted\
    \ in\n   UTC and subsequently in NTP, knowledge of all previous leap seconds\n\
    \   is lost.  Thus, if a clock synchronized to NTP in early 1988 was used\n  \
    \ to establish the time of an event that occured in early 1972, it\n   would be\
    \ fourteen seconds early.\n   When NTP is used to measure intervals between events\
    \ that straddle a\n   leap second, special considerations apply.  When it is necessary\
    \ to\n   determine the elapsed time between events, such as the half life of a\n\
    \   proton, NTP timestamps of these events can be used directly.  When it\n  \
    \ is necessary to establish the order of events relative to UTC, such\n   as the\
    \ order of funds transfers, NTP timestamps can also be used\n   directly; however,\
    \ if it is necessary to establish the elapsed time\n   between events relative\
    \ to UTC, such as the intervals between\n   payments on a mortgage, NTP timestamps\
    \ must be converted to UTC using\n   the above table and its successors.\n   The\
    \ current formats used by NBS radio broadcast services [2] do not\n   include\
    \ provisions for advance notice of leap seconds, so this\n   information must\
    \ be determined from other sources.  NTP includes\n   provisions to distribute\
    \ advance warnings of leap seconds using the\n   Leap Indicator bits described\
    \ in Section 3.  The protocol is designed\n   so that these bits can be set manually\
    \ at the primary clocks and then\n   automatically distributed throughout the\
    \ system for delivery to all\n   logical clocks and then effected as described\
    \ in Section 5.\n"
- title: 3.  Network Time Protocol
  contents:
  - "3.  Network Time Protocol\n   This section consists of a formal definition of\
    \ the Network Time\n   Protocol, including its data formats, entities, state variables,\n\
    \   events and event-processing procedures.  The specification model is\n   based\
    \ on the implementation model illustrated in Figure 2.1, but it\n   is not intended\
    \ that this model is the only one upon which a\n   specification can be based.\
    \  In particular, the specification is\n   intended to illustrate and clarify\
    \ the intrinsic operations of NTP\n   and serve as a foundation for a more rigorous,\
    \ comprehensive and\n   verifiable specification.\n"
- title: 3.1.  Data Formats
  contents:
  - "3.1.  Data Formats\n   All mathematical operations expressed or implied herein\
    \ are in\n   two's-complement arithmetic.  Data are specified as integer or\n\
    \   fixed-point quantities.  Since various implementations would be\n   expected\
    \ to scale externally derived quantities for internal use,\n   neither the precision\
    \ nor decimal-point placement for fixed-point\n   quantities is specified.  Unless\
    \ specified otherwise, all quantities\n   are unsigned and may occupy the full\
    \ field width, if designated, with\n   an implied zero preceding the most significant\
    \ (leftmost) bit.\n   Hardware and software packages designed to work with signed\n\
    \   quantities will thus yield surprising results when the most\n   significant\
    \ (sign) bit is set.  It is suggested that externally\n   derived, unsigned fixed-point\
    \ quantities such as timestamps be\n   shifted right one bit for internal use,\
    \ since the precision\n   represented by the full field width is seldom justified.\n\
    \   Since NTP timestamps are cherished data and, in fact, represent the\n   main\
    \ product of the protocol, a special timestamp format has been\n   established.\
    \  NTP timestamps are represented as a 64-bit unsigned\n   fixed-point number,\
    \ in seconds relative to 0000 UT on 1 January 1900.\n   The integer part is in\
    \ the first 32 bits and the fraction part in the\n   last 32 bits, as shown in\
    \ the following diagram.\n      0                   1                   2    \
    \               3\n      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\
    \ 7 8 9 0 1\n     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \     |                         Integer Part                          |\n    \
    \ +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n     |  \
    \                       Fraction Part                         |\n     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   This format allows convenient multiple-precision arithmetic and\n   conversion\
    \ to Time Protocol representation (seconds), but does\n   complicate the conversion\
    \ to ICMP Timestamp message representation\n   (milliseconds).  The precision\
    \ of this representation is about 0.2\n   nanosecond, which should be adequate\
    \ for even the most exotic\n   requirements.\n   Timestamps are determined by\
    \ copying the current value of the logical\n   clock to a timestamp variable when\
    \ some significant event, such as\n   the arrival of a message, occurs.  In order\
    \ to maintain the highest\n   accuracy, it is important that this be done as close\
    \ to the hardware\n   or software driver associated with the event as possible.\
    \  In\n   particular, departure timestamps should be redetermined for each\n \
    \  link-level retransmission.  In some cases a particular timestamp may\n   not\
    \ be available, such as when the host is rebooted or the protocol\n   first starts\
    \ up.  In these cases the 64-bit field is set to zero,\n   indicating the value\
    \ is invalid or undefined.\n   Note that since some time in 1968 the most significant\
    \ bit (bit 0 of\n   the Integer Part) has been set and that the 64-bit field will\n\
    \   overflow some time in 2036.  Should NTP be in use in 2036, some\n   external\
    \ means will be necessary to qualify time relative to 1900 and\n   time relative\
    \ to 2036 (and other multiples of 136 years).\n   Timestamped data requiring such\
    \ qualification will be so precious\n   that appropriate means should be readily\
    \ available.  There will exist\n   an 0.2-nanosecond interval, henceforth ignored,\
    \ every 136 years when\n   the 64-bit field will be zero and thus considered invalid.\n"
- title: 3.2.  State Variables and Parameters
  contents:
  - "3.2.  State Variables and Parameters\n   Following is a tabular summary of the\
    \ various state variables and\n   parameters used by the protocol.  They are separated\
    \ into classes of\n   system variables, which relate to the operating system environment\n\
    \   and logical clock mechanism;  peer variables, which are specific to\n   each\
    \ peer operating in symmetric mode or client mode;  packet\n   variables, which\
    \ represent the contents of the NTP message;  and\n   parameters, which are fixed\
    \ in all implementations of the current\n   version.  For each class the description\
    \ of the variable is followed\n   by its name and the procedure or value which\
    \ controls it.  Note that\n   variables are in lower case, while parameters are\
    \ in upper case.\n        System Variables                Name            Control\n\
    \        -------------------------------------------------------\n        Logical\
    \ Clock                   sys.clock       update\n        Clock Source       \
    \             sys.peer        selection\n                                    \
    \                    algorithm\n        Leap Indicator                  sys.leap\
    \        update\n        Stratum                         sys.stratum     update\n\
    \        Precision                       sys.precision   system\n        Synchronizing\
    \ Distance          sys.distance    update\n        Estimated Drift Rate     \
    \       sys.drift       system\n        Reference Clock Identifier      sys.refid\
    \       update\n        Reference Timestamp             sys.reftime     update\n\
    \                        Table 3.1. System Variables\n        Peer Variables \
    \                 Name            Control\n        -------------------------------------------------------\n\
    \        Peer Address                    peer.srcadr     system\n        Peer\
    \ Port                       peer.srcport    system\n        Local Address   \
    \                peer.dstadr     system\n        Local Port                  \
    \    peer.dstport    system\n        Peer State                      peer.state\
    \      receive,\n                                                        transmit\n\
    \        Reachability Register           peer.reach      receive,\n          \
    \                                              transmit\n        Peer Timer  \
    \                    peer.timer      system\n        Timer Threshold         \
    \        peer.threshold  system\n        Leap Indicator                  peer.leap\
    \       receive\n        Stratum                         peer.stratum    receive\n\
    \        Peer Poll Interval              peer.ppoll      receive\n        Host\
    \ Poll Interval              peer.hpoll      receive,\n                      \
    \                                  transmit\n        Precision               \
    \        peer.precision  receive\n        Synchronizing Distance          peer.distance\
    \   receive\n        Estimated Drift Rate            peer.drift      receive\n\
    \        Reference Clock Identifier      peer.refid      receive\n        Reference\
    \ Timestamp             peer.reftime    receive\n        Originate Timestamp \
    \            peer.org        receive\n        Receive Timestamp              \
    \ peer.rec        receive\n        Filter Register                 peer.filter\
    \     filter\n                                                        algorithm\n\
    \        Delay Estimate                  peer.delay      filter\n            \
    \                                            algorithm\n        Offset Estimate\
    \                 peer.offset     filter\n                                   \
    \                     algorithm\n        Dispersion Estimate             peer.dispersion\
    \ filter\n                         Table 3.2. Peer Variables\n        Packet Variables\
    \                Name            Control\n        -------------------------------------------------------\n\
    \        Peer Address                    pkt.srcadr      transmit\n        Peer\
    \ Port                       pkt.srcport     transmit\n        Local Address \
    \                  pkt.dstadr      transmit\n        Local Port              \
    \        pkt.dstport     transmit\n        Leap Indicator                  pkt.leap\
    \        transmit\n        Version Number                  pkt.version     transmit\n\
    \        Stratum                         pkt.stratum     transmit\n        Poll\
    \                            pkt.poll        transmit\n        Precision     \
    \                  pkt.precision   transmit\n        Synchronizing Distance  \
    \        pkt.distance    transmit\n        Estimated Drift Rate            pkt.drift\
    \       transmit\n        Reference Clock Identifier      pkt.refid       transmit\n\
    \        Reference Timestamp             pkt.reftime     transmit\n        Originate\
    \ Timestamp             pkt.org         transmit\n        Receive Timestamp  \
    \             pkt.rec         transmit\n        Transmit Timestamp           \
    \   pkt.xmt         transmit\n                        Table 3.3. Packet Variables\n\
    \        Parameters                      Name            Value\n        -------------------------------------------------------\n\
    \        NTP Version                     NTP.VERSION     1\n        NTP Port \
    \                       NTP.PORT        123\n        Minimum Polling Interval\
    \        NTP.MINPOLL     6 (64 sec)\n        Maximum Polling Interval        NTP.MAXPOLL\
    \     10 (1024\n                                                        sec)\n\
    \        Maximum Dispersion              NTP.MAXDISP     65535 ms\n        Reachability\
    \ Register Size      PEER.WINDOW     8\n        Shift Register Size          \
    \   PEER.SHIFT      4/8\n        Dispersion Threshold            PEER.THRESHOLD\
    \  500 ms\n        Filter Weight                   PEER.FILTER     .5\n      \
    \  Select Weight                   PEER.SELECT     .75\n                     \
    \      Table 3.4. Parameters\n   Following is a description of the various variables\
    \ used in the\n   protocol.  Additional details on formats and use are presented\
    \ in\n   later sections and appendices.\n"
- title: 3.2.1.  Common Variables
  contents:
  - "3.2.1.  Common Variables\n   The following variables are common to the system,\
    \ peer and packet\n   classes.\n   Peer Address (peer.srcadr, pkt.srcadr) Peer\
    \ Port (peer.srcport,\n   pkt.srcport)\n      These are the 32-bit Internet address\
    \ and 16-bit port number of\n      the remote host.\n   Local Address (peer.dstadr,\
    \ pkt.dstadr) Local Port (peer.dstport,\n   pkt.dstport)\n      These are the\
    \ 32-bit Internet address and 16-bit port number of\n      the local host.  They\
    \ are included among the state variables to\n      support multi-homing.\n   Leap\
    \ Indicator (sys.leap, peer.leap, pkt.leap)\n      This is a two-bit code warning\
    \ of an impending leap second to be\n      inserted in the NTP time scale.  The\
    \ bits are set before 23:59 on\n      the day of insertion and reset after 00:01\
    \ on the following day.\n      This causes the number of seconds (rollover interval)\
    \ in the day\n      of insertion to be increased or decreased by one.  In the\
    \ case of\n      primary servers the bits are set by operator intervention, while\n\
    \      in the case of secondary servers the bits are set by the protocol.\n  \
    \    The two bits are coded as follows:\n                   00      no warning\
    \ (day has 86400 seconds)\n                   01      +1 second (day has 86401\
    \ seconds)\n                           seconds)\n                   10      -1\
    \ second (day has 86399 seconds)\n                           seconds)\n      \
    \             11      alarm condition (clock not synchronized)\n      In all except\
    \ the alarm condition (11) NTP itself does nothing\n      with these bits, except\
    \ pass them on to the time-conversion\n      routines that are not part of NTP.\
    \  The alarm condition occurs\n      when, for whatever reason, the logical clock\
    \ is not synchronized,\n      such as when first coming up or after an extended\
    \ period when no\n      outside reference source is available.\n   Stratum (sys.stratum,\
    \ peer.stratum, pkt.stratum)\n      This is an integer indicating the stratum\
    \ of the logical clock.  A\n      value of zero is interpreted as unspecified,\
    \ one as a primary\n      clock (synchronized by outside means) and remaining\
    \ values as the\n      stratum level (synchronized by NTP).  For comparison purposes\
    \ a\n      value of zero is considered greater than any other value.\n   Peer\
    \ Poll Interval (peer.ppoll, pkt.poll)\n      This is a signed integer used only\
    \ in symmetric mode and\n      indicating the minimum interval between messages\
    \ sent to the peer,\n      in seconds as a power of two.  For instance, a value\
    \ of six\n      indicates a minimum interval of 64 seconds.  The value of this\n\
    \      variable must not be less than NTP.MINPOLL and must not be greater\n  \
    \    than NTP.MAXPOLL.\n   Precision (sys.precision, peer.precision, pkt.precision)\n\
    \      This is a signed integer indicating the precision of the logical\n    \
    \  clock, in seconds to the nearest power of two.  For instance, a\n      60-Hz\
    \ line-frequency clock would be assigned the value -6, while a\n      1000-Hz\
    \ crystal-derived clock would be assigned the value -10.\n   Synchronizing Distance\
    \ (sys.distance, peer.distance, pkt.distance)\n      This is a fixed-point number\
    \ indicating the estimated roundtrip\n      delay to the primary clock, in seconds.\n\
    \   Estimated Drift Rate (sys.drift, peer.drift, pkt.drift)\n      This is a fixed-point\
    \ number indicating the estimated drift rate\n      of the local clock, in dimensionless\
    \ units.\n   Reference Clock Identifier (sys.refid, peer.refid, pkt.refid)\n \
    \     This is a code identifying the particular reference clock or\n      server.\
    \  The interpretation of the value depends on the stratum.\n      For stratum\
    \ values of zero (unspecified) or one (primary clock),\n      the value is an\
    \ ASCII string identifying the reason or clock,\n      respectively.  For stratum\
    \ values greater than one (synchronized\n      by NTP), the value is the 32-bit\
    \ Internet address of the reference\n      server.\n   Reference Timestamp (sys.reftime,\
    \ peer.reftime, pkt.reftime)\n      This is the local time, in timestamp format,\
    \ when the logical\n      clock was last updated.  If the logical clock has never\
    \ been\n      synchronized, the value is zero.\n"
- title: 3.2.2.  System Variables
  contents:
  - "3.2.2.  System Variables\n   The following variables are used by the operating\
    \ system in order to\n   synchronize the logical clock.\n   Logical Clock (sys.clock)\n\
    \      This is the current local time, in timestamp format.  Local time\n    \
    \  is derived from the hardware clock of the particular machine and\n      increments\
    \ at intervals depending on the design used.  An\n      appropriate design, including\
    \ slewing and drift-compensation\n      mechanisms, is described in Section 5.\n\
    \   Clock Source (sys.peer)\n      This is a selector identifying the current\
    \ clock source.  Usually\n      this will be a pointer to a structure containing\
    \ the peer\n      variables.\n"
- title: 3.2.3.  Peer Variables
  contents:
  - "3.2.3.  Peer Variables\n   Following is a list of state variables used by the\
    \ peer management\n   and measurement functions.  There is one set of these variables\
    \ for\n   every peer operating in client mode or symmetric mode.\n   Peer State\
    \ (peer.state)\n      This is a bit-encoded quantity used for various control\
    \ functions.\n   Host Poll Interval (peer.hpoll)\n      This is a signed integer\
    \ used only in symmetric mode and\n      indicating the minimum interval between\
    \ messages expected from the\n      peer, in seconds as a power of two.  For instance,\
    \ a value of six\n      indicates a minimum interval of 64 seconds.  The value\
    \ of this\n      variable must not be less than NTP.MINPOLL and must not be greater\n\
    \      than NTP.MAXPOLL.\n   Reachability Register (peer.reach)\n      This is\
    \ a code used to determine the reachability status of the\n      peer.  It is\
    \ used as a shift register, with bits entering from the\n      least significant\
    \ (rightmost) end.  The size of this register is\n      specified as PEER.SHIFT\
    \ bits.\n   Peer Timer (peer.timer)\n      This is an integer counter used to\
    \ control the interval between\n      transmitted NTP messages.\n   Timer Threshold\
    \ (peer.threshold)\n      This is the timer value which, when reached, causes\
    \ the timeout\n      procedure to be executed.\n   Originate Timestamp (peer.org,\
    \ pkt.org)\n      This is the local time, in timestamp format, at the peer when\
    \ its\n      latest NTP message was sent.  If the peer becomes unreachable the\n\
    \      value is set to zero.\n   Receive Timestamp (peer.rec, pkt.rec)\n     \
    \ This is the local time, in timestamp format, when the latest NTP\n      message\
    \ from the peer arrived.  If the peer becomes unreachable\n      the value is\
    \ set to zero.\n"
- title: 3.2.4.  Packet Variables
  contents:
  - "3.2.4.  Packet Variables\n   Following is a list of variables used in NTP messages\
    \ in addition to\n   the common variables above.\n   Version Number (pkt.version)\n\
    \      This is an integer indicating the version number of the sender.\n     \
    \ NTP messages will always be sent with the current version number\n      NTP.VERSION\
    \ and will always be accepted if the version number\n      matches NTP.VERSION.\
    \  Exceptions may be advised on a case-by-case\n      basis at times when the\
    \ version number is changed.\n   Transmit Timestamp (pkt.xmt)\n      This is the\
    \ local time, in timestamp format, at which the NTP\n      message departed the\
    \ sender.\n"
- title: 3.2.5.  Clock Filter Variables
  contents:
  - "3.2.5.  Clock Filter Variables\n   When the filter and selection algorithms suggested\
    \ in Section 4 are\n   used, the following state variables are defined.  There\
    \ is one set of\n   these variables for every peer operating in client mode or\
    \ symmetric\n   mode.\n   Filter Register (peer.filter)\n      This is a shift\
    \ register of PEER.WINDOW bits, where each stage is\n      a tuple consisting\
    \ of the measured delay concatenated with the\n      measured offset associated\
    \ with a single observation.\n      Delay/offset observations enter from the least\
    \ significant\n      (rightmost) right and are shifted towards the most significant\n\
    \      (leftmost) end and eventually discarded as new observations\n      arrive.\
    \  The register is cleared to zeros when (a) the peer\n      becomes unreachable\
    \ or (b) the logical clock has just been reset\n      so as to cause a significant\
    \ discontinuity in local time.\n   Delay Estimate (peer.delay)\n      This is\
    \ a signed, fixed-point number indicating the latest delay\n      estimate output\
    \ from the filter, in seconds.  While the number is\n      signed, only those\
    \ values greater than zero represent valid delay\n      estimates.\n   Offset\
    \ Estimate (peer.offset)\n      This is a signed, fixed-point number indicating\
    \ the latest offset\n      estimate output from the filter, in seconds.\n   Dispersion\
    \ Estimate (peer.dispersion)\n      This is a fixed-point number indicating the\
    \ latest dispersion\n      estimate output from the filter, in scrambled units.\n"
- title: 3.2.6.  Parameters
  contents:
  - "3.2.6.  Parameters\n   Following is a list of parameters assumed for all implementations\n\
    \   operating in the Internet system.  It is necessary to agree on the\n   values\
    \ for these parameters in order to avoid unnecessary network\n   overheads and\
    \ stable peer associations.\n   Version Number (NTP.VERSION)\n      This is the\
    \ NTP version number, currently one (1).\n   NTP Port (NTP.PORT)\n      This is\
    \ the port number (123) assigned by the Internet Number Czar\n      to NTP.\n\
    \   Minimum Polling Interval (NTP.MINPOLL)\n      This is the minimum polling\
    \ interval allowed by any peer of the\n      Internet system, currently set to\
    \ 6 (64 seconds).\n   Maximum Polling Interval (NTP.MAXPOLL)\n      This is the\
    \ maximum polling interval allowed by any peer of the\n      Internet system,\
    \ currently set to 10 (1024 seconds).\n   Maximum Dispersion (NTP.MAXDISP)\n \
    \     This is the maximum dispersion assumed by the filter algorithms,\n     \
    \ currently set to 65535 milliseconds.\n   Reachability Register Size (PEER.WINDOW)\n\
    \      This is the size of the Reachability Register (peer.reach),\n      currently\
    \ set to eight (8) bits.\n   Shift Register Size (PEER.SHIFT)\n      When the\
    \ filter and selection algorithms suggested in Section 4\n      are used, this\
    \ is the size of the Clock Filter (peer.filter) shift\n      register, in bits.\
    \  For crystal-stabilized oscillators a value of\n      eight (8) is suggested,\
    \ while for mains-frequency oscillators a\n      value of four (4) is suggested.\
    \  Additional considerations are\n      given in Section 5.\n   Dispersion Threshold\
    \ (PEER.THRESHOLD)\n      When the filter and selection algorithms suggested in\
    \ Section 4\n      are used, this is the threshold used to discard noisy data.\
    \  While\n      a value of 500 milliseconds is suggested, the value may be changed\n\
    \      to suit local conditions on particular peer paths.\n   Filter Weight (PEER.FILTER)\n\
    \      When the filter algorithm suggested in Section 4 is used, this is\n   \
    \   the filter weight used to discard noisy data.  While a value of\n      0.5\
    \ is suggested, the value may be changed to suit local\n      conditions on particular\
    \ peer paths.\n   Select Weight (PEER.SELECT)\n      When the selection algorithm\
    \ suggested in Section 4 is used, this\n      is the select weight used to discard\
    \ outlyers.  data.  While a\n      value of 0.75 is suggested, the value may be\
    \ changed to suit local\n      conditions on particular peer paths.\n"
- title: 3.3.  Modes of Operation
  contents:
  - "3.3.  Modes of Operation\n   An NTP host can operate in three modes:  client,\
    \ server and\n   symmetric.  The mode of operation is determined by whether the\
    \ source\n   port (peer.srcport) or destination port (peer.dstport) peer variables\n\
    \   contain the assigned NTP service port number NTP.PORT (123) as shown\n   in\
    \ the following table.\n           peer.srcport    peer.dstport    Mode\n    \
    \       -------------------------------------------\n           not NTP.PORT \
    \   not NTP.PORT    not possible\n           not NTP.PORT    NTP.PORT        server\n\
    \           NTP.PORT        not NTP.PORT    client\n           NTP.PORT      \
    \  NTP.PORT        symmetric\n   A host operating in client mode occasionally\
    \ sends an NTP message to\n   a host operating in server mode.  The server responds\
    \ by simply\n   interchanging addresses and ports, filling in the required\n \
    \  information and returning the message to the client.  Servers then\n   need\
    \ retain no state information between client requests.  Clients\n   are free to\
    \ manage the intervals between sending NTP messages to suit\n   local conditions.\n\
    \   In symmetric mode the client/server distinction disappears.  Each\n   host\
    \ maintains a table with as many entries as active peers.  Each\n   entry includes\
    \ a code uniquely identifying the peer (e.g.,  Internet\n   address and port),\
    \ together with status information and a copy of the\n   timestamps last received.\
    \  A host operating in symmetric mode\n   periodically sends NTP messages to each\
    \ peer including the latest\n   copy of the timestamps.  The intervals between\
    \ sending NTP messages\n   are managed jointly by the host and each peer using\
    \ the polling\n   variables peer.ppoll and peer.hpoll.\n   When a pair of peers\
    \ operating in symmetric mode exchange NTP\n   messages and each determines that\
    \ the other is reachable, an\n   association is formed.  One or both peers must\
    \ be in active state;\n   that is, sending messages to the other regardless of\
    \ reachability\n   status.  A peer not in active state is in passive state.  If\
    \ a peer\n   operating in passive state discovers that the other peer is no longer\n\
    \   reachable, it ceases sending messages and reclaims the storage and\n   timer\
    \ resources used by the association.  A peer operating in client\n   mode is always\
    \ in active state, while a peer operating in server mode\n   is always in passive\
    \ state.\n"
- title: 3.4.  Event Processing
  contents:
  - "3.4.  Event Processing\n   The significant events of interest in NTP occur upon\
    \ expiration of\n   the peer timer, one of which is dedicated to each peer operating\
    \ in\n   symmetric or client modes, and upon arrival of an NTP message from\n\
    \   the various peers.  An event can also occur as the result of an\n   operator\
    \ command or detected system fault, such as a primary clock\n   failure.  This\
    \ section describes the procedures invoked when these\n   events occur.\n"
- title: 3.4.1.  Timeout Procedure
  contents:
  - "3.4.1.  Timeout Procedure\n   The timeout procedure is called in client and symmetric\
    \ modes when\n   the peer timer (peer.timer) reaches the value of the timer threshold\n\
    \   (peer.threshold) variable.  First, the reachability register\n   (peer.reach)\
    \ is shifted one position to the left and a zero replaces\n   the vacated bit.\
    \  Then an NTP message is constructed and sent to the\n   peer.  If operating\
    \ in active state or in passive state and\n   peer.reach is nonzero (reachable),\
    \ the peer.timer is reinitialized\n   (resumes counting from zero) and the value\
    \ of peer.threshold is set\n   to:\n           peer.threshold <- max( min( peer.ppoll,\
    \ peer.hpoll,\n                           NTP.MAXPOLL), NTP.MINPOLL) .\n   If\
    \ operating in active state and peer.reach is zero (unreachable),\n   the peer\
    \ variables are updated as follows:\n                   peer.hpoll <- NTP.MINPOLL\n\
    \                   peer.disp <- NTP.MAXDISP\n                   peer.filter <-\
    \ 0 (cleared)\n                   peer.org <- 0\n                   peer.rec <-\
    \ 0\n   Then the clock selection algorithm is called, which may result in a\n\
    \   new clock source (sys.peer).  In other cases the protocol ceases\n   operation\
    \ and the storage and timer resources are reclaimed for\n   subsequent use.\n\
    \   An NTP message is constructed as follows (see Appendices A and B for\n   formats).\
    \  First, the IP and UDP packet variables are copied from the\n   peer variables\
    \ (note the interchange of source and destination\n   addresses and ports):\n\
    \           pkt.srcadr <- peer.dstadr       pkt.srcport <- peer.dstport\n    \
    \       pkt.dstadr <- peer.srcadr       pkt.dstport <- peer.srcport\n   Next,\
    \ the NTP packet variables are copied (rescaled as necessary)\n   from the system\
    \ and peer variables:\n           pkt.leap <- sys.leap            pkt.distance\
    \ <- sys.distance\n           pkt.version <- NTP.VERSION      pkt.drift <- sys.drift\n\
    \           pkt.stratum <- sys.stratum      pkt.refid <- sys.refid\n         \
    \  pkt.poll <- peer.hpoll          pkt.reftime <- sys.reftime\n           pkt.precision\
    \ <- sys.precision\n   Finally, the NTP packet timestamp variables are copied,\
    \ depending on\n   whether the peer is operating in symmetric mode and reachable,\
    \ in\n   symmetric mode and not reachable (but active) or in client mode:\n  \
    \ Symmetric Reachable     Symmetric Active        Client\n   - -------------------\
    \     -------------------     -------------------\n   pkt.org <- peer.org    \
    \ pkt.org <- 0            pkt.org <- sys.clock\n   pkt.rec <- peer.rec     pkt.rec\
    \ <- 0            pkt.rec <- sys.clock\n   pkt.xmt <- sys.clock    pkt.xmt <-\
    \ sys.clock    pkt.xmt <- sys.clock\n   Note that the order of copying should\
    \ be designed so that the time to\n   perform the copy operations themselves does\
    \ not degrade the\n   measurement accuracy, which implies that the sys.clock values\
    \ should\n   be copied last.  The reason for the choice of zeros to fill the\n\
    \   pkt.org and pkt.rec packet variables in the symmetric unreachable\n   case\
    \ is to avoid the use of old data after a possibly extensive\n   period of unreachability.\
    \  The reason for the choice of sys.clock to\n   fill these variables in the client\
    \ case is that, if for some reason\n   the NTP message is returned by the recipient\
    \ unaltered, as when\n   testing with an Internet-echo server, this convention\
    \ still allows at\n   least the roundtrip time to be accurately determined without\
    \ special\n   handling.\n"
- title: 3.4.2.  Receive Procedure
  contents:
  - "3.4.2.  Receive Procedure\n   The receive procedure is executed upon arrival\
    \ of an NTP message.  If\n   the version number of the message (pkt.version) does\
    \ not match the\n   current version number (NTP.VERSION), the message is discarded;\n\
    \   however, exceptions may be advised on a case-by-case basis at times\n   when\
    \ the version number is changed.\n   If the clock of the sender is unsynchronized\
    \ (pkt.leap = 11), or the\n   receiver is in server mode or the receiver is in\
    \ symmetric mode and\n   the stratum of the sender is greater than the stratum\
    \ of the receiver\n   (pkt.stratum > sys.stratum), the message is simply returned\
    \ to the\n   sender along with the timestamps.  In this case the addresses and\n\
    \   ports are interchanged in the IP and UDP headers:\n        pkt.srcadr <->\
    \ pkt.dstadr       pkt.srcport <-> pkt.dstport\n   The following packet variables\
    \ are updated from the system variables:\n        pkt.leap <- sys.leap       \
    \     pkt.distance <- sys.distance\n        pkt.version <- NTP.VERSION      pkt.drift\
    \ <- sys.drift\n        pkt.stratum <- sys.stratum      pkt.refid <- sys.refid\n\
    \        pkt.precision <- sys.precision  pkt.reftime <- sys.reftime\n   Note that\
    \ the pkt.poll packet variable is unchanged.  The timestamps\n   are updated in\
    \ the order shown:\n                        pkt.org <- pkt.xmt\n             \
    \           pkt.rec <- sys.clock\n                        pkt.xmt <- sys.clock\n\
    \   Finally, the message is forwarded to the sender and the server\n   receive\
    \ procedure terminated at this point.\n   If the above is not the case, the source\
    \ and destination Internet\n   addresses and ports in the IP and UDP headers are\
    \ matched to the\n   correct peer.  If there is a match, processing continues\
    \ at the next\n   step below.  If there is no match and symmetric mode is not\
    \ indicated\n   (either pkt.srcport or pkt.dstport not equal to NTP.PORT), the\n\
    \   message must be a reply to a previously sent message from a client\n   which\
    \ is no longer in operation.  In this case the message is dropped\n   and the\
    \ receive procedure terminated at this point.\n   If there is no match and symmetric\
    \ mode is indicated, (both\n   pkt.srcport and pkt.dstport equal to NTP.PORT),\
    \ an implementation-\n   specific instantiation procedure is called to create\
    \ and initialize a\n   new set of peer variables and start the peer timer.  The\
    \ following\n   peer variables are set from the IP and UDP headers:\n        \
    \   peer.srcadr <- pkt.srcadr       peer.srcport <- pkt.srcport\n           peer.dstadr\
    \ <- pkt.dstadr       peer.dstport <- pkt.dstport\n   The following peer variables\
    \ are initialized:\n                   peer.state <- symmetric (passive)\n   \
    \                peer.timer <- 0 (enabled)\n                   peer.hpoll <- NTP.MINPOLL\n\
    \                   peer.disp <- NTP.MAXDISP\n   The remaining peer variables\
    \ are undefined and set to zero.\n   Assuming that instantiation is complete and\
    \ that match occurs, the\n   least significant bit of the reachability register\
    \ (peer.reach) is\n   set, indicating the peer is now reachable.  The following\
    \ peer\n   variables are copied (rescaled as necessary) from the NTP packet\n\
    \   variables and system variables:\n           peer.leap <- pkt.leap        \
    \   peer.distance <- pkt.distance\n           peer.stratum <- pkt.stratum    \
    \ peer.drift <- pkt.drift\n           peer.ppoll <- pkt.poll          peer.refid\
    \ <- pkt.refid\n           peer.precision <- pkt.precision peer.reftime <- pkt.reftime\n\
    \           peer.org <- pkt.xmt             peer.rec <- sys.clock\n          \
    \ peer.threshold <- max( min( peer.ppoll, peer.hpoll,\n                      \
    \     NTP.MAXPOLL), NTP.MINPOLL)\n   If either or both the pkt.org or pkt.rec\
    \ packet variables are zero,\n   the sender did not have reliable values for them,\
    \ so the receive\n   procedure is terminated at this point.  If both of these\
    \ variables\n   are nonzero, the roundtrip delay and clock offset relative to\
    \ the\n   peer are calculated as follows.  Number the times of sending and\n \
    \  receiving NTP messages as shown in Figure 3.1 and let i be an even\n   integer.\
    \  Then t(i-3), t(i-2) and t(i-1) and t(i) are the contents of\n   the pkt.org,\
    \ pkt.rec, pkt.xmt and peer.rec variables respectively.\n                    \
    \    |                    |\n                   t(1) |------------------->| t(2)\n\
    \                        |                    |\n                   t(4) |<-------------------|\
    \ t(3)\n                        |                    |\n                   t(5)\
    \ |------------------->| t(6)\n                        |                    |\n\
    \                   t(8) |<-------------------| t(7)\n                       \
    \ |                    |\n                                 ...\n             \
    \   Figure 3.1. Calculating Delay and Offset\n   The roundtrip delay d and clock\
    \ offset c of the receiving peer\n   relative to the sending peer is:\n      \
    \             d = (t(i) - t(i-3)) - (t(i-1) - t(i-2))\n                c = [(t(i-2)\
    \ - t(i-3)) + (t(i-1) - t(i))]/2 .\n   This method amounts to a continuously sampled,\
    \ returnable-time\n   system, which is used in some digital telephone networks.\
    \  Among the\n   advantages are that the order and timing of the messages is\n\
    \   unimportant and that reliable delivery is not required.  Obviously,\n   the\
    \ accuracies achievable depend upon the statistical properties of\n   the outbound\
    \ and inbound net paths.  Further analysis and\n   experimental results bearing\
    \ on this issue can be found in\n   Appendix D.\n   The c and d values are then\
    \ input to the clock filter algorithm to\n   produce the delay estimate (peer.delay)\
    \ and offset estimate\n   (peer.offset) for the peer involved.  If d becomes nonpositive\
    \ due to\n   low delays, long polling intervals and high drift rates, it should\
    \ be\n   considered invalid;  however, even under these conditions it may\n  \
    \ still be useful to update the local clock and reduce the drift rate\n   to the\
    \ point that d becomes positive again.  Specification of the\n   clock filter\
    \ algorithm is not an integral part of the NTP\n   specification;  however, one\
    \ found to work well in the Internet\n   environment is described in Section 4.\n\
    \   When a primary clock is connected to the host, it is convenient to\n   incorporate\
    \ its information into the data base as if the clock were\n   represented by an\
    \ ordinary peer.  The clocks are usually polled once\n   or twice a minute and\
    \ the returned timecheck used to produce a new\n   update for the logical clock.\
    \  The update procedure is then called\n   with the following assumed peer variables:\n\
    \                   peer.offset <- timecheck - sys.clock\n                   peer.delay\
    \ <- as determined\n                   peer.dispersion <- 0\n                \
    \   peer.leap <- selected by operator, ordinarily 00\n                   peer.stratum\
    \ <- 0\n                   peer.distance <- 0\n                   peer.refid <-\
    \ ASCII identifier\n                   peer.reftime <- timecheck\n   In this case\
    \ the peer.delay and peer.refid can be constants\n   reflecting the type and accuracy\
    \ of the clock.  By convention, the\n   value for peer.delay is ten times the\
    \ expected mean error of the\n   clock, for instance, 10 milliseconds for a WWVB\
    \ clock and 1000\n   milliseconds for a less accurate WWV clock, but with a floor\
    \ of 100\n   milliseconds.  Other peer variables such as the peer timer and\n\
    \   reachability register can be used to control the polling interval and\n  \
    \ to confirm the clock is operating correctly.  In this way the clock\n   filter\
    \ and selection algorithms operate in the usual way and can be\n   used to mitigate\
    \ the clock itself, should it appear to be operating\n   correctly, yet deliver\
    \ bogus time.\n"
- title: 3.4.3.  Update Procedure
  contents:
  - "3.4.3.  Update Procedure\n   The update procedure is called when a new delay/offset\
    \ estimate is\n   available.  First, the clock selection algorithm determines\
    \ the best\n   peer on the basis of estimated accuracy and reliability, which\
    \ may\n   result in a new clock source (sys.peer).  If sys.peer points to the\n\
    \   peer data structure with the just-updated estimates, the state\n   variables\
    \ of that peer are used to update the system state variables\n   as follows:\n\
    \                   sys.leap <- peer.leap\n                   sys.stratum <- peer.stratum\
    \ + 1\n                   sys.distance <- peer.distance + peer.delay\n       \
    \            sys.refid <- peer.srcadr\n                   sys.reftime <- peer.rec\n\
    \   Finally, the logical clock procedure is called with peer.offset as\n   argument\
    \ to update the logical clock (sys.clock) and recompute the\n   estimated drift\
    \ rate (sys.drift).  It may happen that the logical\n   clock may be reset, rather\
    \ than slewed to its final value.  In this\n   case the peer variables of all\
    \ reachable peers are are updated as\n   follows:\n                   peer.hpoll\
    \ <- NTP.MINPOLL\n                   peer.disp <- NTP.MAXDISP\n              \
    \     peer.filter <- 0 (cleared)\n                   peer.org <- 0\n         \
    \          peer.rec <- 0\n   and the clock selection algorithm is called again,\
    \ which results in a\n   null clock source (sys.peer = 0).  A new selection will\
    \ occur when\n   the filters fill up again and the dispersion settles down.\n\
    \   Specification of the clock selection algorithm and logical clock\n   procedure\
    \ is not an integral part of the NTP specification.  A clock\n   selection algorithm\
    \ found to work well in the Internet environment is\n   described in Section 4,\
    \ while a logical clock procedure is described\n   in Section 5.  The clock selection\
    \ algorithm described in Section 4\n   usually picks the server at the highest\
    \ stratum and minimum delay\n   among all those available, unless that server\
    \ appears to be a\n   falseticker.  The result is that the algorithms all work\
    \ to build a\n   minimum-weight spanning tree relative to the primary servers\
    \ and thus\n   a hierarchical master-slave system similar to those used by some\n\
    \   digital telephone networks.\n"
- title: 3.4.4.  Initialization Procedures
  contents:
  - "3.4.4.  Initialization Procedures\n   Upon reboot the NTP host initializes all\
    \ system variables as follows:\n                   sys.clock <- best available\
    \ estimate\n                   sys.leap <- 11 (unsynchronized)\n             \
    \      sys.stratum <- 0 (undefined)\n                   sys.precision <- as required\n\
    \                   sys.distance <- 0 (undefined)\n                   sys.drift\
    \ <- as determined\n                   sys.refid <- 0 (undefined)\n          \
    \         sys.reftime <- 0 (undefined)\n   The logical clock sys.clock is presumably\
    \ undefined at reboot;\n   however, in some designs such as the Fuzzball an estimate\
    \ is\n   available from the reboot environment.  The sys.precision variable is\n\
    \   determined by the intrinsic architecture of the local hardware clock.\n  \
    \ The sys.drift variable is determined as a side effect of subsequent\n   logical\
    \ clock updates, from whatever source.\n   Next, an implementation-specific instantiation\
    \ procedure is called\n   repeatedly to establish the set of client peers or symmetric\
    \ (active)\n   peers which will actively probe other servers during regular\n\
    \   operation.  The mode and addresses of these peers is determined using\n  \
    \ information read during the reboot procedure or as the result of\n   operator\
    \ commands.\n"
- title: 4.  Filtering Algorithms
  contents:
  - "4.  Filtering Algorithms\n   A very important factor affecting the accuracy and\
    \ reliability of\n   time distribution is the complex of algorithms used to deglitch\
    \ and\n   smooth the offset estimates and to cast out outlyers due to failure\n\
    \   of the primary reference sources or propagation media.  The\n   algorithms\
    \ suggested in this section were developed and refined over\n   several years\
    \ of operation in the Internet under widely varying net\n   configurations and\
    \ utilizations.  While these algorithms are believed\n   the best available at\
    \ the present time, they are not an integral part\n   of the NTP specification.\n\
    \   There are two algorithms described in the following, the clock filter\n  \
    \ algorithm, which is used to select the best offset samples from a\n   given\
    \ clock, and the clock selection algorithm, which is used to\n   select the best\
    \ clock among a hierarchical set of clocks.\n"
- title: 4.1.  Clock Filter Algorithm
  contents:
  - "4.1.  Clock Filter Algorithm\n   The clock filter algorithm is executed upon\
    \ arrival of each NTP\n   message that results in new delay/offset sample pairs.\
    \  New sample\n   pairs are shifted into the filter register (peer.filter) from\
    \ the\n   left end, causing first zeros then old sample pairs to shift off the\n\
    \   right end.  Then those sample pairs in peer.filter with nonzero delay\n  \
    \ are inserted on a temporary list and sorted in order of increasing\n   delay.\
    \  The delay estimate (peer.delay) and offset estimate\n   (peer.offset) are chosen\
    \ as the delay/offset values corresponding to\n   the minimum-delay sample.  In\
    \ case of ties an arbitrary choice is\n   made.\n   The dispersion estimate (peer.dispersion)\
    \ is then computed as the\n   weighted sum of the offsets in the list.  Assume\
    \ the list has\n   PEER.SHIFT entries, the first m of which contain valid samples\
    \ in\n   order of increasing delay.  If X(i) (0 =< i < PEER.SHIFT) is the\n  \
    \ offset of the ith sample, then,\n           d(i) = |X(i) - X(0)|    if i < m\
    \ and |X(i) - X(0)| < 2^15\n           d(i) = 2^15 - 1         otherwise\n   \
    \                peer.dispersion = Sum(d(i)*w^i) ,\n                         \
    \  (0 =< i < PEER.SHIFT)\n   where w < 1 is a weighting factor experimentally\
    \ adjusted to match\n   typical offset distributions.  The peer.dispersion variable\
    \ is\n   intended for use as a quality indicator, with increasing values\n   associated\
    \ with decreasing quality.  The intent is that samples with\n   a peer.dispersion\
    \ exceeding a configuration threshold will not be\n   used in subsequent processing.\
    \  The prototype implementation uses a\n   weighting factor w = 0.5, also called\
    \ PEER.FILTER, and a threshold\n   PEER.THRESHOLD of 500 ms, which insures that\
    \ all stages of\n   peer.filter are filled and contain offsets within a few seconds\
    \ of\n   each other.\n"
- title: 4.2.  Clock Selection Algorithm
  contents:
  - "4.2.  Clock Selection Algorithm\n   The clock selection algorithm uses the values\
    \ of peer.delay,\n   peer.offset and peer.dispersion calculated by the clock filter\n\
    \   algorithm and is called when these values change or when the\n   reachability\
    \ status changes.  It constructs a list of candidate\n   estimates according to\
    \ a set of criteria designed to maximize\n   accuracy and reliability, then sorts\
    \ the list in order of estimated\n   precision.  Finally, it repeatedly casts\
    \ out outlyers on the basis of\n   dispersion until only a single candidate is\
    \ left.\n   The selection process operates on each peer in turn and inspects the\n\
    \   various data captured from the last received NTP message header, as\n   well\
    \ as the latest clock filter estimates.  It selects only those\n   peers for which\
    \ the following criteria are satisfied:\n   1.  The peer must be reachable and\
    \ operating in client or symmetric\n       modes.\n   2.  The peer logical clock\
    \ must be synchronized, as indicated by the\n       Leap Indicator bits being\
    \ other than 11.\n   3.  If the peer is operating at stratum two or greater, it\
    \ must not\n       be synchronized to this host, which means its reference clock\n\
    \       identifier (peer.refid) must not match the Internet address of\n     \
    \  this host.  This is analogous to the split-horizon rule used in\n       some\
    \ variants of the Bellman-Ford routing algorithm.\n   4.  The sum of the peer\
    \ synchronizing distance (peer.distance) plus\n       peer.delay must be less\
    \ than 2^13 (8192) milliseconds.  Also, the\n       peer stratum (peer.stratum)\
    \ must be less than eight and\n       peer.dispersion must be less than a configured\
    \ threshold\n       PEER.THRESHOLD (currently 500 ms).  These range checks were\n\
    \       established through experience with the prototype implementation,\n  \
    \     but may be changed in future.\n   For each peer which satisfies the above\
    \ criteria, a sixteen-bit\n   keyword is constructed, with the low-order thirteen\
    \ bits the sum of\n   peer.distance plus peer.delay and the high-order three bits\
    \ the\n   peer.stratum reduced by one and truncated to three bits (thus mapping\n\
    \   zero to seven).  The keyword together with a pointer to the peer data\n  \
    \ structure are inserted according to increasing keyword values and\n   truncated\
    \ at a maximum of eight entries.  The resulting list\n   represents the order\
    \ in which peers should be chosen according to the\n   estimated precision of\
    \ measurement.  If no keywords are found, the\n   clock source variable (sys.peer)\
    \ is set to zero and the algorithm\n   terminates.\n   The final procedure is\
    \ designed to detect falsetickers or other\n   conditions which might result in\
    \ gross errors.  Let m be the number\n   of samples remaining in the list.  For\
    \ each i (0 =< i < m) compute\n   the dispersion d(i) of the list relative to\
    \ i:\n                   d(i) = Sum(|X(j) - X(i)|*w^j) ,\n                   \
    \    (0 =< j < m)\n   where w < 1 is a weighting factor experimentally adjusted\
    \ for the\n   desired characteristic (see below).  Then cast out the entry with\n\
    \   maximum d(i) or, in case of ties, the maximum i, and repeat the\n   procedure.\
    \  When only a single entry remains in the list, sys.peer is\n   set as its peer\
    \ data structure pointer and the peer.hpoll variable in\n   that structure is\
    \ set to NTP.MINPOLL as required by the logical clock\n   mechanism described\
    \ in Section 5.\n   This procedure is designed to favor those peers near the head\
    \ of the\n   list, which are at the highest stratum and lowest delay and\n   presumably\
    \ can provide the most precise time.  With proper selection\n   of weighting factor\
    \ w, also called PEER.SELECT, entries will be\n   trimmed from the tail of the\
    \ list, unless a few outlyers disagree\n   significantly with respect to the remaining\
    \ entries, in which case\n   the outlyers are discarded first.\n   In order to\
    \ see how this procedure works to select outlyers, consider\n   the case of three\
    \ entries and assume that one or more of the offsets\n   are clustered about zero\
    \ and others are clustered about one.  For w =\n   0.75 as used in the prototype\
    \ implementations and multiplying by 16\n   for convenience, the first entry has\
    \ weight w^0 = 16, the second w^1\n   = 12 and the third w^2 = 9.  Table X shows\
    \ for all combinations of\n   peer offsets the calculated dispersion about each\
    \ of the three\n   entries, along with the results of the procedure.\n      Peer\
    \ 0    1    2         Dispersion          Cast    Result\n    Weight 16   12 \
    \  9     0       1       2       Out\n           ------------------------------------------------------\n\
    \           0    0    0     0       0       0       2       0    0\n         \
    \  0    0    1     9       9       28      2       0    0\n           0    1 \
    \   0     12      25      12      1       0    0\n           0    1    1     21\
    \      16      16      0       1    1\n           1    0    0     21      16 \
    \     16      0       0    0\n           1    0    1     12      25      12  \
    \    1       1    1\n           1    1    0     9       9       28      2    \
    \   1    1\n           1    1    1     0       0       0       2       1    1\n\
    \                  Table 4.1. Outlyer Selection Procedure\n   In the four cases\
    \ where peer 0 and peer 1 disagree, the outcome is\n   determined by peer 2. \
    \ Similar outcomes occur in the case of four\n   peers.  While these outcomes\
    \ depend on judicious choice of w, the\n   behavior of the algorithm is substantially\
    \ the same for values of w\n   between 0.5 and 1.0.\n"
- title: 4.3.  Variable-Rate Polling
  contents:
  - "4.3.  Variable-Rate Polling\n   As NTP service matures in the Internet, the resulting\
    \ network traffic\n   can become burdensome, especially in the primary service\
    \ net.  In\n   this expectation, it is useful to explore variable-rate polling,\
    \ in\n   which the intervals between NTP messages can be adjusted to fit\n   prevailing\
    \ network conditions of delay dispersion and loss rate.  The\n   prototype NTP\
    \ implementation uses this technique to reduce the\n   network overheads to one-sixteenth\
    \ the maximum rate, depending on\n   observed dispersion and loss.\n   The prototype\
    \ implementation adjusts the polling interval peer.hpoll\n   in response to the\
    \ reachability register (peer.reach) variable along\n   with the dispersion (peer.dispersion)\
    \ variable.  So long as the clock\n   source variable (sys.peer) does not point\
    \ to the peer data structure,\n   peer.reach is nonzero (reachable) and peer.dispersion\
    \ is less than\n   the PEER.THRESHOLD parameter, the value of peer.hpoll is increased\
    \ by\n   one for each call on the update procedure, subject to a maximum of\n\
    \   NTP.MAXPOLL.  Following the timeout procedure, if peer.reach\n   indicates\
    \ messages have not been received for the preceding two\n   polling intervals\
    \ (low-order two bits are zero), the value of\n   peer.hpoll is decreased by one,\
    \ subject to a minimum of NTP.MINPOLL.\n   If peer.reach becomes zero (unreachable),\
    \ the value of peer.hpoll is\n   set to NTP.MINPOLL.\n   The result of the above\
    \ mechanism is that the polling intervals for\n   peers not selected for synchronization\
    \ and in symmetric mode creep\n   upwards once the filter register (peer.filter)\
    \ has filled and the\n   peer.dispersion has settled down, but decrease again\
    \ in case\n   peer.dispersion increases or the loss rate increases or the peer\n\
    \   becomes unreachable.\n"
- title: 5.  Logical Clocks
  contents:
  - "5.  Logical Clocks\n   In order to implement a logical clock, the host must be\
    \ equipped with\n   a hardware clock consisting of an oscillator and interface\
    \ and\n   capable of the required precision and stability.  The logical clock\n\
    \   is adjusted by means of periodic offset corrections computed by NTP\n   or\
    \ some other time-synchronization protocol such as Hellospeak [15]\n   or the\
    \ Unix 4.3bsd TSP [20].  Following is a description of the\n   Fuzzball logical\
    \ clock, which includes provisions for precise time\n   and frequency adjustment\
    \ and can maintain time to within a\n   millisecond and frequency to within a\
    \ day per millisecond.\n   The logical clock is implemented using a 48-bit Clock\
    \ Register, which\n   increments at 1000-Hz (at the decimal point), a 32-bit Clock-Adjust\n\
    \   Register, which is used to slew the Clock Register in response to\n   offset\
    \ corrections, and a Drift-Compensation Register, which is used\n   to trim the\
    \ oscillator frequency.  In some interface designs such as\n   the DEC KWV11,\
    \ an additional hardware register, the Counter Register,\n   is used as an auxiliary\
    \ counter.  The configuration and decimal point\n   of these registers are shown\
    \ in Figure 5.1.\n           Clock Register\n           0               16   \
    \            32\n           +---------------+---------------+---------------+\n\
    \           |               |               |               |\n           +---------------+---------------+---------------+\n\
    \                                           A\n                              \
    \       decimal point\n           Clock-Adjust Register\n                    \
    \       0               16\n                           +---------------+---------------+\n\
    \                           |               |               |\n              \
    \             +---------------+---------------+\n                            \
    \               A\n                                     decimal point\n      \
    \     Drift-Compensation Register\n                           0              \
    \ 16\n                           +---------------+\n                         \
    \  |               |\n                           +---------------+\n         \
    \                                  A\n                                     decimal\
    \ point\n           Counter Register\n                           0           \
    \    16\n                           +---------------+\n                      \
    \     |               |\n                           +---------------+\n      \
    \                                     A\n                                    \
    \ decimal point\n                        Figure 5.1. Clock Registers\n   The Clock\
    \ Register, Clock-Adjust Register and Drift-Compensation\n   Register are implemented\
    \ in memory.  In typical clock interface\n   designs such as the DEC KWV11, the\
    \ Counter Register is implemented as\n   a buffered counter driven by a crystal\
    \ oscillator.  A counter\n   overflow is signalled by an interrupt, which results\
    \ in an increment\n   of the Clock Register at bit 15 and the propagation of carries\
    \ as\n   required.  The time of day is determined by reading the Counter\n   Register,\
    \ which does not disturb the counting process, and adding its\n   value to that\
    \ of the Clock Register with decimal points aligned.\n   In other interface designs\
    \ such as the LSI-11 event-line mechanism,\n   each tick of the clock is signalled\
    \ by an interrupt at intervals of\n   16-2/3 or 20 ms, depending on interface\
    \ and mains frequency.  When\n   this occurs the appropriate increment in milliseconds,\
    \ expressed to\n   32 bits in precision, is added to the Clock Register with decimal\n\
    \   points aligned.\n"
- title: 5.1.  Uniform Phase Adjustments
  contents:
  - "5.1.  Uniform Phase Adjustments\n   Left uncorrected, the logical clock runs\
    \ at the rate of its intrinsic\n   oscillator.  A correction is introduced as\
    \ a signed 32-bit integer in\n   milliseconds, which is added to the Drift-Compensation\
    \ Register and\n   also replaces bits 0-15 of the Clock-Adjust Register, with\
    \ bits 16-31\n   set to zero.  At adjustment intervals of CLOCK.ADJ a correction\n\
    \   consisting of two components is computed.  The first (phase)\n   component\
    \ consists of the Clock-Adjust Register shifted right\n   CLOCK.PHASE bits, which\
    \ is then subtracted from the Clock-Adjust\n   Register.  The second (frequency)\
    \ component consists of the Drift-\n   Compensation Register shifted right CLOCK.FREQ\
    \ bits.  The sum of the\n   phase and frequency components is the correction,\
    \ which is then added\n   to the Clock Register.  Operation continues in this\
    \ way until a new\n   correction is introduced.\n   Care is required in the implementation\
    \ to insure monotonicity of the\n   Clock Register and to preserve the highest\
    \ precision while minimizing\n   the propagation of roundoff errors.  This can\
    \ be done by buffering\n   the corrections and adding them to the increment at\
    \ the time the\n   Clock Register is next updated.  Monotonicity is insured with\
    \ the\n   parameters shown in Table 5.1, as long as the increment is at least\
    \ 2\n   ms.  This table shows the above parameters and others discussed below\n\
    \   for both a crystal-stabilized oscillator and a mains-frequency\n   oscillator.\n\
    \   Parameter               Name            Crystal         Mains\n   -------------------------------------------------------------------\n\
    \   Update Interval         CLOCK.ADJ       4 sec           1 sec\n   Phase Shift\
    \             CLOCK.PHASE     -8              -9\n   Frequency Shift         CLOCK.FREQ\
    \      -16             -16\n   Maximum Aperture        CLOCK.MAX       +-128 ms\
    \        +-256 ms\n   Shift Register Size     PEER.SHIFT      8              \
    \ 4\n   Host Poll Interval      peer.hpoll      NTP.MINPOLL     NTP.MINPOLL\n\
    \                                            (64 sec)        (64 sec)\n      \
    \                  Table 5.1. Clock Parameters\n   The above design constitutes\
    \ a second-order phase-lock loop which\n   adjusts the logical clock phase and\
    \ frequency to compensate for the\n   intrinsic oscillator jitter, wander and\
    \ drift.  Simulation of a loop\n   with parameters chosen from Table 5.1 for a\
    \ crystal-stabilized\n   oscillator and the clock filter described in Section\
    \ 4 results in the\n   following transient response:  For a phase correction of\
    \ 100 ms the\n   loop reaches zero error in 34 minutes, overshoots 7 ms in 76\
    \ minutes\n   and settles to less than 1 ms in about four hours.  The maximum\n\
    \   frequency error is about 6 ppm at 40 minutes and returns to less than\n  \
    \ 1 ppm in about eight hours.  For a frequency correction of 10 ppm the\n   loop\
    \ settles to within 1 ppm in about nine hours and to within 0.1\n   ppm in about\
    \ a day.  These characteristics are appropriate for\n   typical computing equipment\
    \ using board-mounted crystals without oven\n   temperature control.\n   In those\
    \ cases where mains-frequency oscillators must be used, the\n   loop parameters\
    \ must be adapted for the relatively high jitter and\n   wander characteristics\
    \ of the national power grid, in which diurnal\n   peak-to-peak phase excursions\
    \ can exceed four seconds.  Simulation of\n   a loop with parameters chosen from\
    \ Table 5.1 for a mains-frequency\n   oscillator and the clock filter described\
    \ in Section 4 results in a\n   transient response similar to the crystal-stabilized\
    \ case, but with\n   time constants only one-fourth those in that case.  When\
    \ presented\n   with actual phase-offset data for typical Summer days when the\
    \ jitter\n   and wander are the largest, the loop errors are in the order of a\
    \ few\n   tens of milliseconds, but not greater than 150 ms.\n   The above simulations\
    \ assume the clock filter algorithm operates to\n   select the oldest sample in\
    \ the shift register at each step;  that\n   is, the filter operates as a delay\
    \ line with delay equal to the\n   polling interval times the number of stages.\
    \  This is a worst-case\n   scenario, since the larger the overall delay the harder\
    \ it is to\n   maintain low loop errors together with good transient response.\
    \  The\n   parameters in Table 5.1 were experimentally determined with this\n\
    \   scenario and the constraint that the polling interval could not be\n   reduced\
    \ below 64 seconds.  With these parameters it is not possible\n   to increase\
    \ the polling interval above 64 seconds without significant\n   increase in loop\
    \ error or degradation of transient response.  Thus,\n   when a clock is selected\
    \ according to the algorithms of Section 4,\n   the polling interval peer.hpoll\
    \ is always set at NTP.MINPOLL.\n"
- title: 5.2.  Nonuniform Phase Adjustments
  contents:
  - "5.2.  Nonuniform Phase Adjustments\n   When the magnitude of a correction exceeds\
    \ a maximum aperture\n   CLOCK.MAX, the possibility exists that the clock is so\
    \ far out of\n   synchronization with the reference source that the best action\
    \ is an\n   immediate and wholesale replacement of Clock Register contents,\n\
    \   rather than a graduated slewing as described above.  In practice the\n   necessity\
    \ to do this is rare and occurs when the local host or\n   reference source is\
    \ rebooted, for example.  This is fortunate, since\n   step changes in the clock\
    \ can result in the clock apparently running\n   backward, as well as incorrect\
    \ delay and offset measurements of the\n   synchronization mechanism itself.\n\
    \   Considerable experience with the Internet environment suggests the\n   values\
    \ of CLOCK.MAX tabulated in Table 5.1 as appropriate.  In\n   practice, these\
    \ values are exceeded with a single time-server source\n   only under conditions\
    \ of the most extreme congestion or when multiple\n   failures of nodes or links\
    \ have occured.  The most common case when\n   the maximum is exceeded is when\
    \ the time-server source is changed and\n   the time indicated by the new and\
    \ old sources exceeds the maximum due\n   to systematic errors in the primary\
    \ reference source or large\n   differences in the synchronizing path delays.\n"
- title: 5.3.  Maintaining Date and Time
  contents:
  - "5.3.  Maintaining Date and Time\n   Conversion from NTP format to the common\
    \ date and time formats used\n   by application programs is simplified if the\
    \ internal local-clock\n   format uses separate date and time registers.  The\
    \ time register is\n   designed to roll over at 24 hours, give or take a leap\
    \ second as\n   determined by the Leap Indicator bits, with its overflows\n  \
    \ (underflows) incrementing (decrementing) the date register.  The date\n   and\
    \ time registers then indicate the number of days and seconds since\n   some previous\
    \ reference time, but uncorrected for leap seconds.\n   On the day prior to the\
    \ insertion of a leap second the Leap Indicator\n   bits are set at the primary\
    \ servers, presumably by manual means.\n   Subsequently, these bits show up at\
    \ the local host and are passed to\n   the logical clock procedure.  This causes\
    \ the modulus of the time\n   register, which is the length of the current day,\
    \ to be increased or\n   decreased by one second as appropriate.  On the day following\n\
    \   insertion the bits are turned off at the primary servers.  While it\n   is\
    \ possible to turn the bits off automatically, the procedure\n   suggested here\
    \ insures that all clocks have rolled over and will not\n   be reset incorrectly\
    \ to the previous day as the result of possible\n   corrections near the instant\
    \ of rollover.\n"
- title: 5.4.  Estimating Errors
  contents:
  - "5.4.  Estimating Errors\n   After an NTP message is received and until the next\
    \ one is received,\n   the accuracy of the local clock can be expected to degrade\
    \ somewhat.\n   The magnitude of this degradation depends on the error at the\
    \ last\n   update time together with the drift of the local oscillator with\n\
    \   respect to time.  It is possible to estimate both the error and drift\n  \
    \ rate from data collected during regular operation.  These data can be\n   used\
    \ to determine the rate at which NTP neighbors should exchange NTP\n   messages\
    \ and thus control net overheads.\n   NTP messages include the local-clock precision\
    \ of the sender, as well\n   as the reference time, estimated drift and a quantity\
    \ called the\n   synchronizing distance.  The precision of the local clock, together\n\
    \   with its peer clocks, establishes the short-term jitter\n   characteristics\
    \ of the offset estimates.  The reference time and\n   estimated drift of the\
    \ sender provide an error estimate at the time\n   the latest update was received.\
    \  The synchronizing distance provides\n   an estimate of error relative to the\
    \ primary reference source and is\n   used by the filtering algorithms to improve\
    \ the quality and\n   reliability of the offset estimates.\n   Estimates of error\
    \ and drift rate are not essential for the correct\n   functioning of the clock\
    \ algorithms, but do improve the accuracy and\n   adjustment with respect to net\
    \ overheads.  The estimated error allows\n   the recipient to compute the rate\
    \ at which independent samples are\n   required in order to maintain a specified\
    \ estimated error.  The\n   estimated drift rate allows the recipient to estimate\
    \ the optimum\n   polling interval.\n   It is possible to compute the estimated\
    \ drift rate of the local clock\n   to a high degree of precision by simply adding\
    \ the n offsets received\n   during an interval T to an accumulator.  If X1 and\
    \ X2 are the values\n   of the accumulator at the beginning and end of T, then\
    \ the estimated\n   drift rate r is:\n                               X2 - X1 \
    \ n\n                           r = ------- --- .\n                          \
    \        n     T\n   The intrinsic (uncorrected) drift rate of typical crystal\
    \ oscillators\n   under room-temperature conditions is in the order of from a\
    \ few parts\n   per million (ppm) to as much as 100 ppm, or up to a few seconds\
    \ per\n   day.  For most purposes the drift of a particular crystal oscillator\n\
    \   is constant to within perhaps one ppm.  Assuming T can be estimated\n   to\
    \ within 100 ms, for example, it would take about a day of\n   accumulation to\
    \ estimate r to an uncertainty in the order of one ppm.\n   Some idea of the estimated\
    \ error of the local clock can be derived\n   from the variance of the offsets\
    \ about the mean per unit time.  This\n   can be computed by adding the n offset\
    \ squares received during T to\n   an accumulator.  If Y1 and Y2 are the values\
    \ of the accumulator at\n   the beginning and end of T, then the estimated error\
    \ s is:\n                         Y2 - Y1   (X2 - X1)^2    n\n               \
    \    s = ( ------- - ----------- ) --- .\n                            n      \
    \   n * n       T\n   The quantities r and s have direct utility to the peer as\
    \ noted\n   above.  However, they also have indirect utility to the recipient\
    \ of\n   an NTP message sent by that peer, since they can be used as weights\n\
    \   in such algorithms as described in [22], as well as to improve the\n   estimates\
    \ during periods when offsets are not available.  It is most\n   useful if the\
    \ latest estimate of these quantities are available in\n   each NTP message sent;\
    \  however, considerable latitude remains in the\n   details of computation and\
    \ storage.\n   The above formulae for r and s imply equal weighting for offsets\n\
    \   received throughout the accumulation interval T.  One way to do this\n   is\
    \ using a software shift register implemented as a circular buffer.\n   A single\
    \ pointer points to the active entry in the buffer and\n   advances around one\
    \ entry as each new offset is stored.  There are\n   two accumulators, one for\
    \ the offset and the other for its squares.\n   When a new offset arrives, a quantity\
    \ equal to the new offset minus\n   the old (active) entry is added to the first\
    \ accumulator and the\n   square of this quantity is added to the second.  Finally,\
    \ the offset\n   is stored in the circular buffer.\n   The size of the circular\
    \ buffer depends on the accumulation interval\n   T and the rate offsets are produced.\
    \  In many reachability and\n   routing algorithms, such as GGP, EGP and local-net\
    \ control\n   algorithms, peers exchange messages on the order of once or twice\
    \ a\n   minute.  If NTP peers exchanged messages at a rate of one per minute\n\
    \   and if T were one day, the circular buffer would have to be 1440\n   words\
    \ long;  however, a less costly design might aggregate the data\n   in something\
    \ like half-hour segments, which would reduce the length\n   of the buffer to\
    \ 48 words while not significantly affecting the\n   quality of the data.\n"
- title: 6.  References
  contents:
  - "6.  References\n   1.  Lamport, L., \"Time, Clocks and the Ordering of Events\
    \ in a\n       Distributed System\", Communications of the ACM, Vol. 21, No. 7,\n\
    \       pgs.  558-565, July 1978.\n   2.  \"Time and Frequency Dissemination Services\"\
    , NBS Special\n       Publication No. 432, US Department of Commerce, 1979.\n\
    \   3.  Lindsay, W., and A.  Kantak, \"Network Synchronization of Random\n   \
    \    Signals\", IEEE Trans. Comm., COM-28, No. 8, pgs. 1260-1266,\n       August\
    \ 1980.\n   4.  Braun, W., \"Short Term Frequency Effects in Networks of Coupled\n\
    \       Oscillators\", IEEE Trans. Comm., COM-28, No. 8, pgs. 1269-1275,\n   \
    \    August 1980.\n   5.  Mitra, D., \"Network Synchronization:  Analysis of a\
    \ Hybrid of\n       Master-Slave and Mutual Synchronization\", IEEE Trans. Comm.\n\
    \       COM-28, No. 8, pgs. 1245-1259, August 1980.\n   6.  Postel, J., \"User\
    \ Datagram Protocol\", RFC-768, USC/Information\n       Sciences Institute, August\
    \ 1980.\n   7.  Mills, D., \"Time Synchronization in DCNET Hosts\", IEN-173, COMSAT\n\
    \       Laboratories, February 1981.\n   8.  Mills, D., \"DCNET Internet Clock\
    \ Service\", RFC-778, COMSAT\n       Laboratories, April 1981.\n   9.  Su, Z.,\
    \ \"A Specification of the Internet Protocol (IP) Timestamp\n       Option\",\
    \ RFC-781, SRI International, May 1981.\n   10. Defense Advanced Research Projects\
    \ Agency, \"Internet Protocol\",\n       RFC-791, USC/Information Sciences Institute,\
    \ September 1981.\n   11. Defense Advanced Research Projects Agency, \"Internet\
    \ Control\n       Message Protocol\", RFC-792, USC/Information Sciences Institute,\n\
    \       September 1981.\n   12. Postel, J., \"Daytime Protocol\", RFC-867, USC/Information\
    \ Sciences\n       Institute, May 1983.\n   13. Postel, J., \"Time Protocol\"\
    , RFC-868, USC/Information Sciences\n       Institute, May 1983.\n   14. Mills,\
    \ D., \"Internet Delay Experiments\", RFC-889, M/A-COM\n       Linkabit, December\
    \ 1983.\n   15. Mills, D., \"DCN Local-Network Protocols\", RFC-891, M/A-COM\n\
    \       Linkabit, December 1983.\n   16. Gusella, R., and S. Zatti, \"TEMPO -\
    \ A Network Time Controller for\n       a Distributed Berkeley UNIX System\",\
    \ IEEE Distributed Processing\n       Technical Committee Newsletter 6, No. SI-2,\
    \ pgs. 7-15, June 1984.\n       Also in: Proc.  Summer 1984 USENIX, Salt Lake\
    \ City, June 1984.\n   17. Halpern, J., Simons, B., Strong, R., and D. Dolly,\
    \ \"Fault-\n       Tolerant Clock Synchronization\", Proc. Third Annual ACM Symposium\n\
    \       on Principles of Distributed Computing, pgs. 89-102, August 1984.\n  \
    \ 18. Lundelius, J., and N. Lynch, \"A New Fault-Tolerant Algorithm for\n    \
    \   Clock Synchronization:, Proc. Third Annual ACM Symposium on\n       Principles\
    \ of Distributed Computing, pgs. 75-88, August 1984.\n   19. Lamport, L., and\
    \ P. Melliar-Smith \"Synchronizing Clocks in the\n       Presence of Faults\"\
    , JACM 32, No. 1, pgs. 52-78, January 1985.\n   20. Gusella, R., and S. Zatti,\
    \ \"The Berkeley UNIX 4.3BSD Time\n       Synchronization Protocol: Protocol Specification\"\
    , Technical\n       Report UCB/CSD 85/250, University of California, Berkeley,\
    \ June\n       1985.\n   21. Marzullo, K., and S. Owicki, \"Maintaining the Time\
    \ in a\n       Distributed System\", ACM Operating Systems Review 19, No. 3, pgs.\n\
    \       44-54, July 1985.\n   22. Mills, D., \"Algorithms for Synchronizing Network\
    \ Clocks\", RFC-\n       956, M/A-COM Linkabit, September 1985.\n   23. Mills,\
    \ D., \"Experiments in Network Clock Synchronization\", RFC-\n       957, M/A-COM\
    \ Linkabit, September 1985.\n   24. Mills, D., \"Network Time Protocol (NTP)\"\
    , RFC-958, M/A-COM\n       Linkabit, September 1985.\n   25. Gusella, R., and\
    \ S. Zatti, \"An Election Algorithm for a\n       Distributed Clock Synchronization\
    \ Program\", Technical Report\n       UCB/CSD 86/275, University of California,\
    \ Berkeley, December\n       1985.\n   26. Sams, H., \"Reference Data for Engineers:\
    \  Radio, Electronics,\n       Computer and Communications (Seventh Edition)\"\
    , Indianapolis,\n       1985.\n   27. Schneider, F., \"A Paradigm for Reliable\
    \ Clock Synchronization\",\n       Technical Report TR 86-735, Cornell University,\
    \ February 1986.\n   28. Tripathi, S., and S. Chang, \"ETempo:  A Clock Synchronization\n\
    \       Algorithm for Hierarchical LANs - Implementation and\n       Measurements\"\
    , Systems Research Center Technical Report TR-86-48,\n       University of Maryland,\
    \ 1986.\n   29. Bertsekas, D., and R.  Gallager, \"Data Networks\", Prentice-Hall,\n\
    \       Englewood Cliffs, NJ, 1987.\n   30. Srikanth, T., and S. Toueg. \"Optimal\
    \ Clock Synchronization\", JACM\n       34, No. 3, pgs. 626-645, July 1987.\n\
    \   31. Rickert, N., \"Non Byzantine Clock Synchronization - A Programming\n \
    \      Experiment\", ACM Operating Systems Review 22, No. 1, pgs. 73-78,\n   \
    \    January 1988.\n"
- title: Appendix A.  UDP Header Format
  contents:
  - "Appendix A.  UDP Header Format\n   An NTP packet consists of the UDP header followed\
    \ by the NTP data\n   portion.  The format of the UDP header and the interpretation\
    \ of its\n   fields are described in [6] and are not part of the NTP\n   specification.\
    \  They are shown below for completeness.\n    0                   1         \
    \          2                   3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9\
    \ 0 1 2 3 4 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |          Source Port          |       Destination Port        |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |            Length             |           Checksum            |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   Source Port\n          UDP source port number. In the case of a client request\
    \ this\n          field is assigned by the client host, while for a server reply\n\
    \          it is copied from the Destination Port field of the client\n      \
    \    request. In the case of symmetric mode, both the Source Port\n          and\
    \ Destination Port fields are assigned the NTP service-port\n          number\
    \ 123.\n   Destination Port\n          UDP destination port number. In the case\
    \ of a client request\n          this field is assigned the NTP service-port number\
    \ 123, while\n          for a server reply it is copied from the Source Port field\
    \ of\n          the client request. In the case of symmetric mode, both the\n\
    \          Source Port and Destination Port fields are assigned the NTP\n    \
    \      service-port number 123.\n   Length\n          Length of the request or\
    \ reply, including UDP header, in\n          octets\n   Checksum\n          Standard\
    \ UDP checksum\n"
- title: Appendix B.  NTP Data Format - Version 1
  contents:
  - "Appendix B.  NTP Data Format - Version 1\n   The format of the NTP data portion,\
    \ which immediately follows the UDP\n   header, is shown below along with a description\
    \ of its fields.\n    0                   1                   2              \
    \     3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n\
    \   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |LI\
    \ | VN  |0 0 0|    Stratum    |      Poll     |   Precision   |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                     Synchronizing Distance                    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                     Estimated Drift Rate                      |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                  Reference Clock Identifier                   |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                                                               |\n   |  \
    \               Reference Timestamp (64 bits)                 |\n   |        \
    \                                                       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                                                               |\n   |  \
    \               Originate Timestamp (64 bits)                 |\n   |        \
    \                                                       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                                                               |\n   |  \
    \                Receive Timestamp (64 bits)                  |\n   |        \
    \                                                       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                                                               |\n   |  \
    \                Transmit Timestamp (64 bits)                 |\n   |        \
    \                                                       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   Leap Indicator (LI)\n          Two-bit code warning of impending leap-second\
    \ to be inserted\n          at the end of the last day of the current month. Bits\
    \ are\n          coded as follows:\n                    00      no warning\n \
    \                   01      +1 second (following minute has 61 seconds)\n    \
    \                10      -1 second (following minute has 59 seconds)\n       \
    \             11      alarm condition (clock not synchronized)\n   Version Number\
    \ (VN)\n          Three-bit code indicating the version number, currently one\n\
    \          (1).\n   Reserved\n          Three-bit field consisting of all zeros\
    \ and reserved for\n          future use.\n   Stratum\n          Integer identifying\
    \ stratum level of local clock. Values are\n          defined as follows:\n  \
    \                  0       unspecified\n                    1       primary reference\
    \ (e.g., radio clock)\n                    2...n   secondary reference (via NTP)\n\
    \   Poll\n          Signed integer indicating the maximum interval between\n \
    \         successive messages, in seconds to the nearest power of two.\n   Precision\n\
    \          Signed integer indicating the precision of the local clock, in\n  \
    \        seconds to the nearest power of two.\n   Synchronizing Distance\n   \
    \       Fixed-point number indicating the estimated roundtrip delay to\n     \
    \     the primary synchronizing source, in seconds with fraction\n          point\
    \ between bits 15 and 16.\n   Estimated Drift Rate\n          Fixed-point number\
    \ indicating the estimated drift rate of the\n          local clock, in dimensionless\
    \ units with fraction point to the\n          left of the most significant bit.\n\
    \   Reference Clock Identifier\n          Code identifying the particular reference\
    \ clock. In the case\n          of type 0 (unspecified) or type 1 (primary reference),\
    \ this is\n          a left-justified, zero-filled ASCII string, for example:\n\
    \                    Type    Code    Meaning\n                    ---------------------------------------------------\n\
    \                    0       DCN     Determined by DCN routing algorithm\n   \
    \                 1       WWVB    WWVB radio clock (60 kHz)\n                \
    \    1       GOES    GOES satellite clock (468 MHz)\n                    1   \
    \    WWV     WWV radio clock (5/10/15 MHz)\n                    (and others as\
    \ necessary)\n          In the case of type 2 and greater (secondary reference),\
    \ this\n          is the 32-bit Internet address of the reference host.\n   Reference\
    \ Timestamp\n          Local time at which the local clock was last set or corrected.\n\
    \   Originate Timestamp\n          Local time at which the request departed the\
    \ client host for\n          the service host.\n   Receive Timestamp\n       \
    \   Local time at which the request arrived at the service host.\n   Transmit\
    \ Timestamp\n          Local time at which the reply departed the service host\
    \ for\n          the client host.\n"
- title: Appendix C.  Timeteller Experiments
  contents:
  - "Appendix C.  Timeteller Experiments\n   In order to update data collected in\
    \ June 1985 and reported in RFC-\n   957, a glorious three-day experiment was\
    \ carried out in January 1988\n   with all the hosts and gateways listed in the\
    \ NIC data base.  Four\n   packets were sent at five-second intervals to each\
    \ host and gateway\n   using UDP/NTP, UDP/TIME and ICMP/TIMESTAMP protocols and\
    \ the clock\n   offsets (in milliseconds) for each protocol averaged with respect\
    \ to\n   local time, which is synchronized via NTP to a radio-clock host.\n  \
    \ While the ICMP/TIMESTAMP protocol has much finer granularity\n   (milliseconds)\
    \ than UDP/TIME (seconds), it has no provisions for the\n   date, so is not suitable\
    \ as a time-synchronization protocol;\n   however, it was included in the experiments\
    \ both as a sanity check\n   and in order to assess the precision of measurement.\n\
    \   In the latest survey of 5498 hosts and 224 gateways, 46 responded to\n   UDP/NTP\
    \ requests, 1158 to UDP/TIME and 1963 to ICMP/TIMESTAMP.  By\n   contrast, in\
    \ the 1985 survey of 1775 hosts and 110 gateways, 163\n   responded to UDP/TIME\
    \ requests and 504 to ICMP/TIMESTAMP.  At that\n   time there were no UDP/NTP\
    \ implementations.  There are many more\n   hosts and gateways listed in the rapidly\
    \ growing domain-name system,\n   but not listed in the NIC data base, and therefore\
    \ not surveyed.  The\n   results of the survey are given in Table C.1, which shows\
    \ for each of\n   the three protocols the error X for which the distribution function\n\
    \   P[x =< X] has the value shown.\n           P[x=<X] UDP/NTP         UDP/TIME\
    \        ICMP/TIMESTAMP\n           ------------------------------------------------------\n\
    \           .1      11              4632            5698\n           .2      37\
    \              18238           27965\n           .3      66              38842\
    \           68596\n           .4      177             68213           127367\n\
    \           .5      364             126232          201908\n           .6    \
    \  567             195950          285092\n           .7      3466           \
    \ 267119          525509\n           .8      20149           422129          2.91426E+06\n\
    \           .9      434634          807135          5.02336E+07\n           1\
    \       1.17971E+09     1.59524E+09     2.11591E+09\n                     Table\
    \ C.1. Distribution Functions\n   It can be seen that ten percent of the UDP/NTP\
    \ responses show errors\n   of 11 milliseconds or less and that ten percent of\
    \ the UDP/TIME\n   responses show errors greater than 807135 milliseconds (about\
    \ 13\n   minutes).  Fifty percent of the UDP/NTP timetellers are within 364\n\
    \   milliseconds, while fifty percent of the UDP/TIME tellers are within\n   126232\
    \ milliseconds (just over two minutes).  Surprisingly,\n   ICMP/TIMESTAMP responses\
    \ show errors even larger than UDP/TIME.\n   However, the maximum error shown\
    \ in all three protocols exceeded the\n   range that could be recorded, in this\
    \ case about 12 days.  Clearly,\n   there are good timetellers and bad.\n"
- title: Appendix D.  Evaluation of Filtering Algorithms
  contents:
  - "Appendix D.  Evaluation of Filtering Algorithms\n   A number of algorithms for\
    \ deglitching and filtering time-offset data\n   were described in RFC-956.  These\
    \ fall in two classes:  majority-\n   subset algorithms, which attempt to separate\
    \ good subsets from bad by\n   comparing their means, and clustering algorithms,\
    \ which attempt to\n   improve the estimate by repeatedly casting out outlyers.\
    \  The former\n   class was suggested as a technique to select the best (i.e.\
    \  the most\n   reliable) clocks from a population, while the latter class was\n\
    \   suggested as a technique to improve the offset estimate for a single\n   clock\
    \ given a series of observations.\n   Following publication of RFC-956 and after\
    \ further development and\n   experimentation using typical Internet paths, a\
    \ better algorithm was\n   found for casting out outlyers from a continuous stream\
    \ of offset\n   observations spaced at intervals in the order of minutes.  The\n\
    \   algorithm is described as a variant of a median filter, in which a\n   window\
    \ consisting of the last n sample offsets is continuously\n   updated and the\
    \ median sample selected as the estimate.  However, in\n   the modified algorithm\
    \ the outlyer (sample furthest from the median)\n   is then discarded and the\
    \ entire process repeated until only a single\n   sample offset is left, which\
    \ is then selected as the estimate.\n   The modified algorithm was found to be\
    \ more resistant to glitches and\n   to provide a more accurate estimate than\
    \ the unmodified one.  It has\n   been implemented in the NTP daemons developed\
    \ for the Fuzzball and\n   Unix operating systems and been in regular operation\
    \ for about two\n   years.  However, recent experiments have shown there is an\
    \ even\n   better one which provides comparable accuracy together with a much\n\
    \   lower computational burden.  The key to the new algorithm became\n   evident\
    \ through an examination of scatter diagrams plotting sample\n   offset versus\
    \ roundtrip delay.\n   To see how a scatter diagram is constructed, it will be\
    \ useful to\n   consider how offsets and delays are computed.  Number the times\
    \ of\n   sending and receiving NTP messages as shown in Figure D.1 and let i\n\
    \   be an even integer.  Then the timestamps t(i-3), t(i-2) and t(i-1)\n   and\
    \ t(i) are sufficient to calculate the offset and delay of each\n   peer relative\
    \ to the other.\n                   Peer 1                    Peer 2\n       \
    \                 |                    |\n                   t(1) |------------------->|\
    \ t(2)\n                        |                    |\n                   t(4)\
    \ |<-------------------| t(3)\n                        |                    |\n\
    \                   t(5) |------------------->| t(6)\n                       \
    \ |                    |\n                   t(8) |<-------------------| t(7)\n\
    \                        |                    |\n                            \
    \     ...\n                 Figure D.1. Calculating Delay and Offset\n   The roundtrip\
    \ delay d and clock offset c of the receiving peer\n   relative to the sending\
    \ peer are:\n                   d = (t(i) - t(i-3)) - (t(i-1) - t(i-2))\n    \
    \            c = [(t(i-2) - t(i-3)) + (t(i-1) - t(i))]/2 .\n   Two implicit assumptions\
    \ in the above are that the delay distribution\n   is independent of direction\
    \ and that the intrinsic drift rates of the\n   client and server clocks are small\
    \ and close to the same value.  If\n   this is the case the scatter diagram would\
    \ show the samples\n   concentrated about a horizontal line extending from the\
    \ point (d,c)\n   to the right.  However, this is not generally the case.  The\
    \ typical\n   diagram shows the samples dispersed in a wedge with apex (d,c) and\n\
    \   opening to the right.  The limits of the wedge are determined by\n   lines\
    \ extending from (d,c) with slopes +0.5 and -0.5, which\n   correspond to the\
    \ locus of points as the delay in one direction\n   increases while the delay\
    \ in the other direction does not.  In some\n   cases the points are concentrated\
    \ along these two extrema lines, with\n   relatively few points remaining within\
    \ the opening of the wedge,\n   which would correspond to increased delays on\
    \ both directions.\n   Upon reflection, the reason for the particular dispersion\
    \ shown in\n   the scatter diagram is obvious.  Packet-switching nets are most\
    \ often\n   operated with relatively small mean queue lengths in the order of\n\
    \   one, which means the queues are often idle for relatively long\n   periods.\
    \  In addition, the routing algorithm most often operates to\n   minimize the\
    \ number of packet-switch hops and thus the number of\n   queues.  Thus, not only\
    \ is the probability that an arriving NTP\n   packet finds a busy queue in one\
    \ direction reasonably low, but the\n   probability of it finding a busy queue\
    \ in both directions is even\n   lower.\n   From the above discussion one would\
    \ expect that, at low utilizations\n   and hop counts the points should be concentrated\
    \ about the apex of\n   the wedge and begin to extend rightward along the extrema\
    \ lines as\n   the utilizations and hop counts increase.  As the utilizations\
    \ and\n   hop counts continue to increase, the points should begin to fill in\n\
    \   the wedge as it expands even further rightward.  This behavior is in\n   fact\
    \ what is observed on typical Internet paths involving ARPANET,\n   NSFNET and\
    \ other nets.\n   These observations cast doubt on the median-filter approach\
    \ as a good\n   way to cast out offset outlyers and suggests another approach\
    \ which\n   might be called a minimum filter.  From the scatter diagrams it is\n\
    \   obvious that the best offset samples occur at the lower delays.\n   Therefore,\
    \ an appropriate technique would be simply to select from\n   the n most recent\
    \ samples the sample with lowest delay and use its\n   associated offset as the\
    \ estimate.  An experiment was designed to\n   test this technique using measurements\
    \ between selected hosts\n   equipped with radio clocks, so that delays and offsets\
    \ could be\n   determined independent of the measurement procedure itself.\n \
    \  The raw delays and offsets were measured by NTP from hosts at U\n   Maryland\
    \ (UMD) and U Delaware (UDEL) via net paths to each other and\n   other hosts\
    \ at Ford Research (FORD), Information Sciences Institute\n   (ISI) and National\
    \ Center for Atmospheric Research (NCAR).  For the\n   purposes here, all hosts\
    \ can be assumed synchronized to within a few\n   milliseconds to NBS time, so\
    \ that the delays and offsets reflect only\n   the net paths themselves.\n   The\
    \ results of the measurements are given in Table D.1 (UMD) and\n   Table D.2 (UDEL),\
    \ which show for each of the paths the error X for\n   which the distribution\
    \ function P[x =< X] has the value shown.  Note\n   that the values of the distribution\
    \ function are shown by intervals\n   of decreasing size as the function increases,\
    \ so that its behavior in\n   the interesting regime of low error probability\
    \ can be more\n   accurately determined.\n    UMD    FORD    ISI     NCAR    \
    \      UMD    FORD    ISI     NCAR\n    Delay  1525    2174    1423          Offset\
    \ 1525    2174    1423\n    ---------------------------          ---------------------------\n\
    \    .1     493     688     176           .1     2       17      1\n    .2   \
    \  494     748     179           .2     4       33      2\n    .3     495    \
    \ 815     187           .3     9       62      3\n    .4     495     931     205\
    \           .4     18      96      8\n    .5     497     1013    224         \
    \  .5     183     127     13\n    .6     503     1098    243           .6    \
    \ 4.88E+8 151     20\n    .7     551     1259    265           .7     4.88E+8\
    \ 195     26\n    .8     725     1658    293           .8     4.88E+8 347    \
    \ 35\n    .9     968     2523    335           .9     4.88E+8 775     53\n   \
    \ .99    1409    6983    472           .99    4.88E+8 2785    114\n    .999  \
    \ 14800   11464   22731         .999   4.88E+8 5188    11279\n    1      18395\
    \   15892   25647         1      4.88E+8 6111    12733\n              Table D.1.\
    \ Delay and Offset Measurements (UMD)\n           UDEL   FORD    UMD     ISI \
    \    NCAR\n           Delay  2986    3442    3215    2756\n           -----------------------------------\n\
    \           .1     650     222     411     476\n           .2     666     231\
    \     436     512\n           .3     692     242     471     554\n           .4\
    \     736     256     529     594\n           .5     787     272     618     648\n\
    \           .6     873     298     681     710\n           .7     1013    355\
    \     735     815\n           .8     1216    532     845     1011\n          \
    \ .9     1836    1455    1019    1992\n           .99    4690    3920    1562\
    \    4334\n           .999   15371   6132    2387    11234\n           1     \
    \ 21984   8942    4483    21427\n                   Table D.2.a Delay Measurements\
    \ (UDEL)\n           UDEL   FORD    UMD     ISI     NCAR\n           Offset 2986\
    \    3442    3215    2756\n           -----------------------------------\n  \
    \         .1     83      2       16      12\n           .2     96      5     \
    \  27      24\n           .3     108     9       36      36\n           .4   \
    \  133     13      48      51\n           .5     173     20      67      69\n\
    \           .6     254     30      93      93\n           .7     429     51  \
    \    130     133\n           .8     1824    133     165     215\n           .9\
    \     4.88E+8 582     221     589\n           .99    4.88E+8 1757    539     1640\n\
    \           .999   4.88E+8 2945    929     5278\n           1      5.63E+8 4374\
    \    1263    10425\n                  Table D.2.b Offset Measurements (UDEL)\n\
    \   The results suggest that accuracies less than a few seconds can\n   usually\
    \ be achieved for all but one percent of the measurements, but\n   that accuracies\
    \ degrade drastically when the remaining measurements\n   are included.  Note\
    \ that in the case of the UMD measurements to FORD\n   almost half the measurements\
    \ showed gross errors, which was due to\n   equipment failure at that site.  These\
    \ data were intentionally left\n   in the sample set to see how well the algorithms\
    \ dealt with the\n   problem.\n   The next two tables compare the results of minimum\
    \ filters (Table\n   D.3) and median filters (Table D.4) for various n when presented\
    \ with\n   the UMD - - NCAR raw sample data.  The results show consistently\n\
    \   lower errors for the minimum filter when compared with the median\n   filter\
    \ of nearest value of n.  Perhaps the most dramatic result of\n   both filters\
    \ is the greatly reduced error at the upper end of the\n   range.  In fact, using\
    \ either filter with n at least three results in\n   no errors greater than 100\
    \ milliseconds.\n                           Filter Samples\n                 \
    \  1       2       4       8       16\n           P[x=<X] 1423    1422    1422\
    \    1420    1416\n           - --------------------------------------------\n\
    \            .1     1       1       1       0       0\n            .2     2  \
    \     1       1       1       1\n            .3     3       2       1       1\
    \       1\n            .4     8       2       2       1       1\n            .5\
    \     13      5       2       2       1\n            .6     20      10      3\
    \       2       2\n            .7     26      15      6       2       2\n    \
    \        .8     35      23      11      4       2\n            .9     53     \
    \ 33      20      9       3\n            .99    114     62      43      28   \
    \   23\n            .999   11279   82      57      37      23\n            1 \
    \     12733   108     59      37      23\n                         Table D.3.\
    \ Minimum Filter\n                               (UMD - NCAR)\n              \
    \             Filter Samples\n                           3       7       15\n\
    \                   P[x=<X] 1423    1423    1423\n                     ----------------------------\n\
    \                    .1     2       2       2\n                    .2     2  \
    \     4       5\n                    .3     5       8       8\n              \
    \      .4     10      11      11\n                    .5     13      14      14\n\
    \                    .6     18      17      16\n                    .7     23\
    \      21      19\n                    .8     28      25      23\n           \
    \         .9     36      30      27\n                    .99    64      46   \
    \   35\n                    .999   82      53      44\n                    1 \
    \     82      60      44\n                         Table D.4. Median Filter\n\
    \                               (UMD - NCAR)\n   While the UMD - NCAR data above\
    \ represented a path across the NSFNET\n   Backbone, which normally involves only\
    \ a few hops via Ethernets and\n   56-Kbps links, the UDEL - NCAR path involves\
    \ additional ARPANET hops,\n   which can contribute substantial additional delay\
    \ dispersion.  The\n   following Table D.5.  shows the results of a minimum filter\
    \ for\n   various n when presented with the UDEL - NCAR raw sample data.  The\n\
    \   range of error is markedly greater than the UMD - NCAR path above,\n   especially\
    \ near the upper end of the distribution function.\n                         \
    \       Filter Samples\n                        1       2       4       8    \
    \   16\n                P[x=<X] 2756    2755    2755    2753    2749\n       \
    \         --------------------------------------------\n                 .1  \
    \   12      9       8       7       6\n                 .2     24      19    \
    \  16      14      14\n                 .3     36      27      22      20    \
    \  19\n                 .4     51      36      29      25      23\n          \
    \       .5     69      47      36      30      27\n                 .6     93\
    \      61      44      35      32\n                 .7     133     80      56\
    \      43      35\n                 .8     215     112     75      53      43\n\
    \                 .9     589     199     111     76      63\n                \
    \ .99    1640    1002    604     729     315\n                 .999   5278   \
    \ 1524    884     815     815\n                 1      10425   5325    991   \
    \  835     815\n                   Table D.5. Minimum Filter (UDEL - NCAR)\n \
    \  Based on these data, the minimum filter was selected as the standard\n   algorithm.\
    \  Since its performance did not seem to much improve for\n   values of n above\
    \ eight, this value was chosen as the standard.\n   Network Time Protocol (Version\
    \ 1): Specification and Implementation.\n"
- title: Appendix E.  NTP Synchronization Networks
  contents:
  - "Appendix E.  NTP Synchronization Networks\n   This section discusses net configuration\
    \ issues for implementing a\n   ubiquitous NTP service in the Internet system.\
    \  Section E.1 describes\n   the NTP primary service net now in operation, including\
    \ an analysis\n   of failure scenarios.  Section E.2 suggests how secondary service\n\
    \   nets, which obtain wholesale time from the primary service net, can\n   be\
    \ configured to deliver accurate and reliable retail time to the\n   general host\
    \ population.\n"
- title: E.1.  Primary Service Network
  contents:
  - "E.1.  Primary Service Network\n   The primary service net consists of five primary\
    \ servers, each of\n   which is synchronized via radio or satellite to a national\
    \ time\n   standard and thus operates at stratum one.  Each server consists of\n\
    \   an LSI-11 Fuzzball, a WWVB or GOES radio clock and one or more net\n   interfaces.\
    \  Some servers provide switching and gateway services as\n   well.  Table E.1\
    \ shows the name, Internet address, type of clock,\n   operating institution and\
    \ identifying code.\n"
- title: Name          Address         Clock   Operating Institution and (Code)
  contents:
  - 'Name          Address         Clock   Operating Institution and (Code)

    '
- title: '----------------------------------------------------------------------'
  contents:
  - '----------------------------------------------------------------------

    '
- title: DCN5.ARPA     128.4.0.5       WWVB    U Delaware, Newark, DE (UDEL)
  contents:
  - 'DCN5.ARPA     128.4.0.5       WWVB    U Delaware, Newark, DE (UDEL)

    '
- title: FORD1.ARPA    128.5.0.1       GOES    Ford Research, Dearborn, MI
  contents:
  - "FORD1.ARPA    128.5.0.1       GOES    Ford Research, Dearborn, MI\n         \
    \                               (FORD)\n"
- title: NCAR.NSF.NET  128.116.64.3    WWVB    National Center for Atmospheric
  contents:
  - "NCAR.NSF.NET  128.116.64.3    WWVB    National Center for Atmospheric\n     \
    \                                   Research, Boulder, CO (NCAR)\n"
- title: UMD1.UMD.EDU  128.8.10.1      WWVB    U Maryland, College Park, MD
  contents:
  - "UMD1.UMD.EDU  128.8.10.1      WWVB    U Maryland, College Park, MD\n        \
    \                                (UMD)\n"
- title: WWVB.ISI.EDU  128.9.2.129     WWVB    USC Information Sciences
  contents:
  - "WWVB.ISI.EDU  128.9.2.129     WWVB    USC Information Sciences\n            \
    \                            Institute, Marina del Rey, CA\n                 \
    \                       (ISI)\n                       Table E.1. Primary Servers\n\
    \   Figure E.1 shows how the five primary servers are interconnected as\n   NTP\
    \ peers.  Note that each server actively probes two other servers\n   (along the\
    \ direction of the arrows), which means these probes will\n   continue even if\
    \ one or both of the two probed servers are down.  On\n   the other hand, each\
    \ server is probed by two other servers, so that\n   the result, assuming all\
    \ servers are up, is that every server peers\n   with every other server.\n  \
    \             +------------------------------------------------+\n           \
    \    V                                                |\n           +--------+\
    \              +--------+              +--------+\n           |        |<-------------|\
    \        |<-------------|        |\n           |  NCAR  |              |  ISI\
    \   |              |  FORD  |\n           |        |----+      +--|        |<--+\
    \    +---->|        |\n           +--------+    |      |  +--------+   |    |\
    \     +--------+\n               |         |      |               |    |     \
    \     A\n               |     +---|------|---------------|----+          |\n \
    \              |     |   |      |               |               |\n          \
    \     |     |   +------|---------------|---------+     |\n               |   \
    \  |          |               |         |     |\n               |     |      \
    \    |               |         V     |\n               |   +--------+   |    \
    \           |  +--------+   |\n               |   |        |<--+             \
    \  +--|        |   |\n               +-->|  UMD   |                      |  UDEL\
    \  |---+\n                   |        |--------------------->|        |\n    \
    \               +--------+                      +--------+\n                 \
    \   Figure E.1. Primary Service Network\n   All of the five primary servers shown\
    \ are directly connected to a\n   radio clock and thus normally operate at stratum\
    \ one.  However, if\n   the radio clock itself becomes disabled or the propagation\
    \ path to\n   its synchronizing source fails, then the server drops to stratum\
    \ two\n   and synchronizes via NTP with its neighbor at the smallest\n   synchronizing\
    \ distance.  If a radio clock appears to operate\n   correctly but delivers incorrect\
    \ time (falseticker), the server may\n   remain synchronized to the clock.  However,\
    \ gross discrepancies will\n   become apparent via the NTP peer paths, which will\
    \ ordinarily result\n   in an operator alarm.\n   Assume that, if a radio clock\
    \ appears up, it is a truechimer;\n   otherwise, the clock appears down.  Then\
    \ the above configuration will\n   continue to provide correct time at all primary\
    \ servers as long as at\n   least one radio clock is up, all servers are up and\
    \ the servers\n   remain connected to each other through the net.  The fact that\
    \ the\n   graph and all of its subgraphs are completely connected lends an\n \
    \  incredible resilience to the configuration.\n   If some radio clocks appear\
    \ up but are in fact falsetickers, the\n   primary servers connected to those\
    \ clocks will not provide correct\n   time.  However, as the consequents of the\
    \ voting procedure and\n   complete connectivity of the graph and its subgraphs,\
    \ any combination\n   of two falsetickers or of one falseticker and one down server\
    \ will be\n   detected by their truechimer neighbors.\n"
- title: E.2.  Secondary Service Networks
  contents:
  - "E.2.  Secondary Service Networks\n   A secondary server operating at stratum\
    \ n > 1 ordinarily obtains\n   synchronization using at least three peer paths,\
    \ two with servers at\n   stratum n-1 and one or more with servers at stratum\
    \ n.  In the most\n   robust configurations a set of servers agree to provide\
    \ backup\n   service for each other, so distribute some of their peer paths over\n\
    \   stratum-(n-1) servers and others over stratum-n servers in the same\n   set.\
    \  For instance, in the case of a stratum-2 service net with two\n   secondary\
    \ servers and the primary service net of Figure E.1, there\n   are five possible\
    \ configurations where each stratum-1 path ends on a\n   different primary server.\
    \  Such configurations can survive the loss\n   of three out of the four stratum-1\
    \ servers or net paths and will\n   reject a single falseticker on one of the\
    \ two stratum-1 paths for\n   each server.\n   Ordinary hosts can obtain retail\
    \ time from primary or secondary\n   service net using NTP in client/server mode,\
    \ which does not require\n   dedicated server resources as does symmetric mode.\
    \  It is anticipated\n   that ordinary hosts will be quite close to a secondary\
    \ server,\n   perhaps on the same cable or local net, so that the frequency of\
    \ NTP\n   request messages need only be high enough, perhaps one per hour or\n\
    \   two, to trim the drift from the local clock.\n"
