- title: __initial_text__
  contents:
  - '    Remote Direct Memory Access Transport for Remote Procedure Call

    '
- title: Abstract
  contents:
  - "Abstract\n   This document describes a protocol providing Remote Direct Memory\n\
    \   Access (RDMA) as a new transport for Remote Procedure Call (RPC).\n   The\
    \ RDMA transport binding conveys the benefits of efficient, bulk-\n   data transport\
    \ over high-speed networks, while providing for minimal\n   change to RPC applications\
    \ and with no required revision of the\n   application RPC protocol, or the RPC\
    \ protocol itself.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This\
    \ document is a product of the Internet Engineering Task Force\n   (IETF).  It\
    \ represents the consensus of the IETF community.  It has\n   received public\
    \ review and has been approved for publication by the\n   Internet Engineering\
    \ Steering Group (IESG).  Further information on\n   Internet Standards is available\
    \ in Section 2 of RFC 5741.\n   Information about the current status of this document,\
    \ any errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc5666.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2010 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n   This document\
    \ may contain material from IETF Documents or IETF\n   Contributions published\
    \ or made publicly available before November\n   10, 2008.  The person(s) controlling\
    \ the copyright in some of this\n   material may not have granted the IETF Trust\
    \ the right to allow\n   modifications of such material outside the IETF Standards\
    \ Process.\n   Without obtaining an adequate license from the person(s) controlling\n\
    \   the copyright in such materials, this document may not be modified\n   outside\
    \ the IETF Standards Process, and derivative works of it may\n   not be created\
    \ outside the IETF Standards Process, except to format\n   it for publication\
    \ as an RFC or to translate it into languages other\n   than English.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Requirements Language ......................................4\n  \
    \ 2. Abstract RDMA Requirements ......................................4\n   3.\
    \ Protocol Outline ................................................5\n      3.1.\
    \ Short Messages .............................................6\n      3.2. Data\
    \ Chunks ................................................6\n      3.3. Flow Control\
    \ ...............................................7\n      3.4. XDR Encoding with\
    \ Chunks ...................................8\n      3.5. XDR Decoding with Read\
    \ Chunks .............................11\n      3.6. XDR Decoding with Write Chunks\
    \ ............................12\n      3.7. XDR Roundup and Chunks ....................................13\n\
    \      3.8. RPC Call and Reply ........................................14\n  \
    \    3.9. Padding ...................................................17\n   4.\
    \ RPC RDMA Message Layout ........................................18\n      4.1.\
    \ RPC-over-RDMA Header ......................................18\n      4.2. RPC-over-RDMA\
    \ Header Errors ...............................20\n      4.3. XDR Language Description\
    \ ..................................20\n   5. Long Messages ..................................................22\n\
    \      5.1. Message as an RDMA Read Chunk .............................23\n  \
    \    5.2. RDMA Write of Long Replies (Reply Chunks) .................24\n   6.\
    \ Connection Configuration Protocol ..............................25\n      6.1.\
    \ Initial Connection State ..................................26\n      6.2. Protocol\
    \ Description ......................................26\n   7. Memory Registration\
    \ Overhead ...................................28\n   8. Errors and Error Recovery\
    \ ......................................28\n   9. Node Addressing ................................................28\n\
    \   10. RPC Binding ...................................................29\n  \
    \ 11. Security Considerations .......................................30\n   12.\
    \ IANA Considerations ...........................................31\n   13. Acknowledgments\
    \ ...............................................32\n   14. References ....................................................33\n\
    \      14.1. Normative References .....................................33\n  \
    \    14.2. Informative References ...................................33\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Remote Direct Memory Access (RDMA) [RFC5040, RFC5041], [IB]\
    \ is a\n   technique for efficient movement of data between end nodes, which\n\
    \   becomes increasingly compelling over high-speed transports.  By\n   directing\
    \ data into destination buffers as it is sent on a network,\n   and placing it\
    \ via direct memory access by hardware, the double\n   benefit of faster transfers\
    \ and reduced host overhead is obtained.\n   Open Network Computing Remote Procedure\
    \ Call (ONC RPC, or simply,\n   RPC) [RFC5531] is a remote procedure call protocol\
    \ that has been run\n   over a variety of transports.  Most RPC implementations\
    \ today use UDP\n   or TCP.  RPC messages are defined in terms of an eXternal\
    \ Data\n   Representation (XDR) [RFC4506], which provides a canonical data\n \
    \  representation across a variety of host architectures.  An XDR data\n   stream\
    \ is conveyed differently on each type of transport.  On UDP,\n   RPC messages\
    \ are encapsulated inside datagrams, while on a TCP byte\n   stream, RPC messages\
    \ are delineated by a record marking protocol.  An\n   RDMA transport also conveys\
    \ RPC messages in a unique fashion that\n   must be fully described if client\
    \ and server implementations are to\n   interoperate.\n   RDMA transports present\
    \ new semantics unlike the behaviors of either\n   UDP or TCP alone.  They retain\
    \ message delineations like UDP while\n   also providing a reliable, sequenced\
    \ data transfer like TCP.  Also,\n   they provide the new efficient, bulk-transfer\
    \ service of RDMA.  RDMA\n   transports are therefore naturally viewed as a new\
    \ transport type by\n   RPC.\n   RDMA as a transport will benefit the performance\
    \ of RPC protocols\n   that move large \"chunks\" of data, since RDMA hardware\
    \ excels at\n   moving data efficiently between host memory and a high-speed network\n\
    \   with little or no host CPU involvement.  In this context, the Network\n  \
    \ File System (NFS) protocol, in all its versions [RFC1094] [RFC1813]\n   [RFC3530]\
    \ [RFC5661], is an obvious beneficiary of RDMA.  A complete\n   problem statement\
    \ is discussed in [RFC5532], and related NFSv4 issues\n   are discussed in [RFC5661].\
    \  Many other RPC-based protocols will also\n   benefit.\n   Although the RDMA\
    \ transport described here provides relatively\n   transparent support for any\
    \ RPC application, the proposal goes\n   further in describing mechanisms that\
    \ can optimize the use of RDMA\n   with more active participation by the RPC application.\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in [RFC2119].\n"
- title: 2.  Abstract RDMA Requirements
  contents:
  - "2.  Abstract RDMA Requirements\n   An RPC transport is responsible for conveying\
    \ an RPC message from a\n   sender to a receiver.  An RPC message is either an\
    \ RPC call from a\n   client to a server, or an RPC reply from the server back\
    \ to the\n   client.  An RPC message contains an RPC call header followed by\n\
    \   arguments if the message is an RPC call, or an RPC reply header\n   followed\
    \ by results if the message is an RPC reply.  The call header\n   contains a transaction\
    \ ID (XID) followed by the program and procedure\n   number as well as a security\
    \ credential.  An RPC reply header begins\n   with an XID that matches that of\
    \ the RPC call message, followed by a\n   security verifier and results.  All\
    \ data in an RPC message is XDR\n   encoded.  For a complete description of the\
    \ RPC protocol and XDR\n   encoding, see [RFC5531] and [RFC4506].\n   This protocol\
    \ assumes the following abstract model for RDMA\n   transports.  These terms,\
    \ common in the RDMA lexicon, are used in\n   this document.  A more complete\
    \ glossary of RDMA terms can be found\n   in [RFC5040].\n   o Registered Memory\n\
    \        All data moved via tagged RDMA operations is resident in\n        registered\
    \ memory at its destination.  This protocol assumes\n        that each segment\
    \ of registered memory MUST be identified with a\n        steering tag of no more\
    \ than 32 bits and memory addresses of up\n        to 64 bits in length.\n   o\
    \ RDMA Send\n        The RDMA provider supports an RDMA Send operation with\n\
    \        completion signaled at the receiver when data is placed in a\n      \
    \  pre-posted buffer.  The amount of transferred data is limited\n        only\
    \ by the size of the receiver's buffer.  Sends complete at\n        the receiver\
    \ in the order they were issued at the sender.\n   o RDMA Write\n        The RDMA\
    \ provider supports an RDMA Write operation to directly\n        place data in\
    \ the receiver's buffer.  An RDMA Write is initiated\n        by the sender and\
    \ completion is signaled at the sender.  No\n        completion is signaled at\
    \ the receiver.  The sender uses a\n        steering tag, memory address, and\
    \ length of the remote\n        destination buffer.  RDMA Writes are not necessarily\
    \ ordered\n        with respect to one another, but are ordered with respect to\n\
    \        RDMA Sends; a subsequent RDMA Send completion obtained at the\n     \
    \   receiver guarantees that prior RDMA Write data has been\n        successfully\
    \ placed in the receiver's memory.\n   o RDMA Read\n        The RDMA provider\
    \ supports an RDMA Read operation to directly\n        place peer source data\
    \ in the requester's buffer.  An RDMA Read\n        is initiated by the receiver\
    \ and completion is signaled at the\n        receiver.  The receiver provides\
    \ steering tags, memory\n        addresses, and a length for the remote source\
    \ and local\n        destination buffers.  Since the peer at the data source receives\n\
    \        no notification of RDMA Read completion, there is an assumption\n   \
    \     that on receiving the data, the receiver will signal completion\n      \
    \  with an RDMA Send message, so that the peer can free the source\n        buffers\
    \ and the associated steering tags.\n   This protocol is designed to be carried\
    \ over all RDMA transports\n   meeting the stated requirements.  This protocol\
    \ conveys to the RPC\n   peer information sufficient for that RPC peer to direct\
    \ an RDMA layer\n   to perform transfers containing RPC data and to communicate\
    \ their\n   result(s).  For example, it is readily carried over RDMA transports\n\
    \   such as Internet Wide Area RDMA Protocol (iWARP) [RFC5040, RFC5041],\n   or\
    \ InfiniBand [IB].\n"
- title: 3.  Protocol Outline
  contents:
  - "3.  Protocol Outline\n   An RPC message can be conveyed in identical fashion,\
    \ whether it is a\n   call or reply message.  In each case, the transmission of\
    \ the message\n   proper is preceded by transmission of a transport-specific header\
    \ for\n   use by RPC-over-RDMA transports.  This header is analogous to the\n\
    \   record marking used for RPC over TCP, but is more extensive, since\n   RDMA\
    \ transports support several modes of data transfer; it is\n   important to allow\
    \ the upper-layer protocol to specify the most\n   efficient mode for each of\
    \ the segments in a message.  Multiple\n   segments of a message may thereby be\
    \ transferred in different ways to\n   different remote memory destinations.\n\
    \   All transfers of a call or reply begin with an RDMA Send that\n   transfers\
    \ at least the RPC-over-RDMA header, usually with the call or\n   reply message\
    \ appended, or at least some part thereof.  Because the\n   size of what may be\
    \ transmitted via RDMA Send is limited by the size\n   of the receiver's pre-posted\
    \ buffer, the RPC-over-RDMA transport\n   provides a number of methods to reduce\
    \ the amount transferred by\n   means of the RDMA Send, when necessary, by transferring\
    \ various parts\n   of the message using RDMA Read and RDMA Write.\n   RPC-over-RDMA\
    \ framing replaces all other RPC framing (such as TCP\n   record marking) when\
    \ used atop an RPC/RDMA association, even though\n   the underlying RDMA protocol\
    \ may itself be layered atop a protocol\n   with a defined RPC framing (such as\
    \ TCP).  It is however possible for\n   RPC/RDMA to be dynamically enabled, in\
    \ the course of negotiating the\n   use of RDMA via an upper-layer exchange. \
    \ Because RPC framing\n   delimits an entire RPC request or reply, the resulting\
    \ shift in\n   framing must occur between distinct RPC messages, and in concert\
    \ with\n   the transport.\n"
- title: 3.1.  Short Messages
  contents:
  - "3.1.  Short Messages\n   Many RPC messages are quite short.  For example, the\
    \ NFS version 3\n   GETATTR request, is only 56 bytes: 20 bytes of RPC header,\
    \ plus a\n   32-byte file handle argument and 4 bytes of length.  The reply to\n\
    \   this common request is about 100 bytes.\n   There is no benefit in transferring\
    \ such small messages with an RDMA\n   Read or Write operation.  The overhead\
    \ in transferring steering tags\n   and memory addresses is justified only by\
    \ large transfers.  The\n   critical message size that justifies RDMA transfer\
    \ will vary\n   depending on the RDMA implementation and network, but is typically\
    \ of\n   the order of a few kilobytes.  It is appropriate to transfer a short\n\
    \   message with an RDMA Send to a pre-posted buffer.  The RPC-over-RDMA\n   header\
    \ with the short message (call or reply) immediately following\n   is transferred\
    \ using a single RDMA Send operation.\n   Short RPC messages over an RDMA transport:\n\
    \        RPC Client                           RPC Server\n            |      \
    \         RPC Call              |\n       Send |   ------------------------------>\
    \   |\n            |                                     |\n            |    \
    \           RPC Reply             |\n            |   <------------------------------\
    \   | Send\n"
- title: 3.2.  Data Chunks
  contents:
  - "3.2.  Data Chunks\n   Some protocols, like NFS, have RPC procedures that can\
    \ transfer very\n   large chunks of data in the RPC call or reply and would cause\
    \ the\n   maximum send size to be exceeded if one tried to transfer them as\n\
    \   part of the RDMA Send.  These large chunks typically range from a\n   kilobyte\
    \ to a megabyte or more.  An RDMA transport can transfer large\n   chunks of data\
    \ more efficiently via the direct placement of an RDMA\n   Read or RDMA Write\
    \ operation.  Using direct placement instead of\n   inline transfer not only avoids\
    \ expensive data copies, but provides\n   correct data alignment at the destination.\n"
- title: 3.3.  Flow Control
  contents:
  - "3.3.  Flow Control\n   It is critical to provide RDMA Send flow control for an\
    \ RDMA\n   connection.  RDMA receive operations will fail if a pre-posted\n  \
    \ receive buffer is not available to accept an incoming RDMA Send, and\n   repeated\
    \ occurrences of such errors can be fatal to the connection.\n   This is a departure\
    \ from conventional TCP/IP networking where buffers\n   are allocated dynamically\
    \ on an as-needed basis, and where\n   pre-posting is not required.\n   It is\
    \ not practical to provide for fixed credit limits at the RPC\n   server.  Fixed\
    \ limits scale poorly, since posted buffers are\n   dedicated to the associated\
    \ connection until consumed by receive\n   operations.  Additionally, for protocol\
    \ correctness, the RPC server\n   must always be able to reply to client requests,\
    \ whether or not new\n   buffers have been posted to accept future receives. \
    \ (Note that the\n   RPC server may in fact be a client at some other layer. \
    \ For example,\n   NFSv4 callbacks are processed by the NFSv4 client, acting as\
    \ an RPC\n   server.  The credit discussions apply equally in either case.)\n\
    \   Flow control for RDMA Send operations is implemented as a simple\n   request/grant\
    \ protocol in the RPC-over-RDMA header associated with\n   each RPC message. \
    \ The RPC-over-RDMA header for RPC call messages\n   contains a requested credit\
    \ value for the RPC server, which MAY be\n   dynamically adjusted by the caller\
    \ to match its expected needs.  The\n   RPC-over-RDMA header for the RPC reply\
    \ messages provides the granted\n   result, which MAY have any value except it\
    \ MUST NOT be zero when no\n   in-progress operations are present at the server,\
    \ since such a value\n   would result in deadlock.  The value MAY be adjusted\
    \ up or down at\n   each opportunity to match the server's needs or policies.\n\
    \   The RPC client MUST NOT send unacknowledged requests in excess of\n   this\
    \ granted RPC server credit limit.  If the limit is exceeded, the\n   RDMA layer\
    \ may signal an error, possibly terminating the connection.\n   Even if an error\
    \ does not occur, it is OPTIONAL that the server\n   handle the excess request(s),\
    \ and it MAY return an RPC error to the\n   client.  Also note that the never-zero\
    \ requirement implies that an\n   RPC server MUST always provide at least one\
    \ credit to each connected\n   RPC client from which no requests are outstanding.\
    \  The client would\n   deadlock otherwise, unable to send another request.\n\
    \   While RPC calls complete in any order, the current flow control limit\n  \
    \ at the RPC server is known to the RPC client from the Send ordering\n   properties.\
    \  It is always the most recent server-granted credit value\n   minus the number\
    \ of requests in flight.\n   Certain RDMA implementations may impose additional\
    \ flow control\n   restrictions, such as limits on RDMA Read operations in progress\
    \ at\n   the responder.  Because these operations are outside the scope of\n \
    \  this protocol, they are not addressed and SHOULD be provided for by\n   other\
    \ layers.  For example, a simple upper-layer RPC consumer might\n   perform single-issue\
    \ RDMA Read requests, while a more sophisticated,\n   multithreaded RPC consumer\
    \ might implement its own First In, First\n   Out (FIFO) queue of such operations.\
    \  For further discussion of\n   possible protocol implementations capable of\
    \ negotiating these\n   values, see Section 6 \"Connection Configuration Protocol\"\
    \ of this\n   document, or [RFC5661].\n"
- title: 3.4.  XDR Encoding with Chunks
  contents:
  - "3.4.  XDR Encoding with Chunks\n   The data comprising an RPC call or reply message\
    \ is marshaled or\n   serialized into a contiguous stream by an XDR routine. \
    \ XDR data\n   types such as integers, strings, arrays, and linked lists are\n\
    \   commonly implemented over two very simple functions that encode\n   either\
    \ an XDR data unit (32 bits) or an array of bytes.\n   Normally, the separate\
    \ data items in an RPC call or reply are encoded\n   as a contiguous sequence\
    \ of bytes for network transmission over UDP\n   or TCP.  However, in the case\
    \ of an RDMA transport, local routines\n   such as XDR encode can determine that\
    \ (for instance) an opaque byte\n   array is large enough to be more efficiently\
    \ moved via an RDMA data\n   transfer operation like RDMA Read or RDMA Write.\n\
    \   Semantically speaking, the protocol has no restriction regarding data\n  \
    \ types that may or may not be represented by a read or write chunk.\n   In practice\
    \ however, efficiency considerations lead to the conclusion\n   that certain data\
    \ types are not generally \"chunkable\".  Typically,\n   only those opaque and\
    \ aggregate data types that may attain\n   substantial size are considered to\
    \ be eligible.  With today's\n   hardware, this size may be a kilobyte or more.\
    \  However, any object\n   MAY be chosen for chunking in any given message.\n\
    \   The eligibility of XDR data items to be candidates for being moved as\n  \
    \ data chunks (as opposed to being marshaled inline) is not specified\n   by the\
    \ RPC-over-RDMA protocol.  Chunk eligibility criteria MUST be\n   determined by\
    \ each upper-layer in order to provide for an\n   interoperable specification.\
    \  One such example with rationale, for\n   the NFS protocol family, is provided\
    \ in [RFC5667].\n   The interface by which an upper-layer implementation communicates\
    \ the\n   eligibility of a data item locally to RPC for chunking is out of\n \
    \  scope for this specification.  In many implementations, it is\n   possible\
    \ to implement a transparent RPC chunking facility.  However,\n   such implementations\
    \ may lead to inefficiencies, either because they\n   require the RPC layer to\
    \ perform expensive registration and\n   de-registration of memory \"on the fly\"\
    , or they may require using\n   RDMA chunks in reply messages, along with the\
    \ resulting additional\n   handshaking with the RPC-over-RDMA peer.  However,\
    \ these issues are\n   internal and generally confined to the local interface\
    \ between RPC\n   and its upper layers, one in which implementations are free\
    \ to\n   innovate.  The only requirement is that the resulting RPC RDMA\n   protocol\
    \ sent to the peer is valid for the upper layer.  See, for\n   example, [RFC5667].\n\
    \   When sending any message (request or reply) that contains an eligible\n  \
    \ large data chunk, the XDR encoding routine avoids moving the data\n   into the\
    \ XDR stream.  Instead, it does not encode the data portion,\n   but records the\
    \ address and size of each chunk in a separate \"read\n   chunk list\" encoded\
    \ within RPC RDMA transport-specific headers.  Such\n   chunks will be transferred\
    \ via RDMA Read operations initiated by the\n   receiver.\n   When the read chunks\
    \ are to be moved via RDMA, the memory for each\n   chunk is registered.  This\
    \ registration may take place within XDR\n   itself, providing for full transparency\
    \ to upper layers, or it may be\n   performed by any other specific local implementation.\n\
    \   Additionally, when making an RPC call that can result in bulk data\n   transferred\
    \ in the reply, write chunks MAY be provided to accept the\n   data directly via\
    \ RDMA Write.  These write chunks will therefore be\n   pre-filled by the RPC\
    \ server prior to responding, and XDR decode of\n   the data at the client will\
    \ not be required.  These chunks undergo a\n   similar registration and advertisement\
    \ via \"write chunk lists\" built\n   as a part of XDR encoding.\n   Some RPC\
    \ client implementations are not able to determine where an\n   RPC call's results\
    \ reside during the \"encode\" phase.  This makes it\n   difficult or impossible\
    \ for the RPC client layer to encode the write\n   chunk list at the time of building\
    \ the request.  In this case, it is\n   difficult for the RPC implementation to\
    \ provide transparency to the\n   RPC consumer, which may require recoding to\
    \ provide result\n   information at this earlier stage.\n   Therefore, if the\
    \ RPC client does not make a write chunk list\n   available to receive the result,\
    \ then the RPC server MAY return data\n   inline in the reply, or if the upper-layer\
    \ specification permits, it\n   MAY be returned via a read chunk list.  It is\
    \ NOT RECOMMENDED that\n   upper-layer RPC client protocol specifications omit\
    \ write chunk lists\n   for eligible replies, due to the lower performance of\
    \ the additional\n   handshaking to perform data transfer, and the requirement\
    \ that the\n   RPC server must expose (and preserve) the reply data for a period\
    \ of\n   time.  In the absence of a server-provided read chunk list in the\n \
    \  reply, if the encoded reply overflows the posted receive buffer, the\n   RPC\
    \ will fail with an RDMA transport error.\n   When any data within a message is\
    \ provided via either read or write\n   chunks, the chunk itself refers only to\
    \ the data portion of the XDR\n   stream element.  In particular, for counted\
    \ fields (e.g., a \"<>\"\n   encoding) the byte count that is encoded as part\
    \ of the field remains\n   in the XDR stream, and is also encoded in the chunk\
    \ list.  The data\n   portion is however elided from the encoded XDR stream, and\
    \ is\n   transferred as part of chunk list processing.  It is important to\n \
    \  maintain upper-layer implementation compatibility -- both the count\n   and\
    \ the data must be transferred as part of the logical XDR stream.\n   While the\
    \ chunk list processing results in the data being available\n   to the upper-layer\
    \ peer for XDR decoding, the length present in the\n   chunk list entries is not.\
    \  Any byte count in the XDR stream MUST\n   match the sum of the byte counts\
    \ present in the corresponding read or\n   write chunk list.  If they do not agree,\
    \ an RPC protocol encoding\n   error results.\n   The following items are contained\
    \ in a chunk list entry.\n   Handle\n        Steering tag or handle obtained when\
    \ the chunk memory is\n        registered for RDMA.\n   Length\n        The length\
    \ of the chunk in bytes.\n   Offset\n        The offset or beginning memory address\
    \ of the chunk.  In order\n        to support the widest array of RDMA implementations,\
    \ as well as\n        the most general steering tag scheme, this field is\n  \
    \      unconditionally included in each chunk list entry.\n        While zero-based\
    \ offset schemes are available in many RDMA\n        implementations, their use\
    \ by RPC requires individual\n        registration of each read or write chunk.\
    \  On many such\n        implementations, this can be a significant overhead.\
    \  By\n        providing an offset in each chunk, many pre-registration or\n \
    \       region-based registrations can be readily supported, and by\n        using\
    \ a single, universal chunk representation, the RPC RDMA\n        protocol implementation\
    \ is simplified to its most general form.\n   Position\n        For data that\
    \ is to be encoded, the position in the XDR stream\n        where the chunk would\
    \ normally reside.  Note that the chunk\n        therefore inserts its data into\
    \ the XDR stream at this position,\n        but its transfer is no longer \"inline\"\
    .  Also note therefore\n        that all chunks belonging to a single RPC argument\
    \ or result\n        will have the same position.  For data that is to be decoded,\
    \ no\n        position is used.\n   When XDR marshaling is complete, the chunk\
    \ list is XDR encoded, then\n   sent to the receiver prepended to the RPC message.\
    \  Any source data\n   for a read chunk, or the destination of a write chunk,\
    \ remain behind\n   in the sender's registered memory, and their actual payload\
    \ is not\n   marshaled into the request or reply.\n      +----------------+----------------+-------------\n\
    \      | RPC-over-RDMA  |                |\n      |    header w/   |   RPC Header\
    \   | Non-chunk args/results\n      |     chunks     |                |\n    \
    \  +----------------+----------------+-------------\n   Read chunk lists and write\
    \ chunk lists are structured somewhat\n   differently.  This is due to the different\
    \ usage -- read chunks are\n   decoded and indexed by their argument's or result's\
    \ position in the\n   XDR data stream;  their size is always known.  Write chunks,\
    \ on the\n   other hand, are used only for results, and have neither a preassigned\n\
    \   offset in the XDR stream nor a size until the results are produced,\n   since\
    \ the buffers may be only partially filled, or may not be used\n   for results\
    \ at all.  Their presence in the XDR stream is therefore\n   not known until the\
    \ reply is processed.  The mapping of write chunks\n   onto designated NFS procedures\
    \ and their results is described in\n   [RFC5667].\n   Therefore, read chunks\
    \ are encoded into a read chunk list as a single\n   array, with each entry tagged\
    \ by its (known) size and its argument's\n   or result's position in the XDR stream.\
    \  Write chunks are encoded as\n   a list of arrays of RDMA buffers, with each\
    \ list element (an array)\n   providing buffers for a separate result.  Individual\
    \ write chunk list\n   elements MAY thereby result in being partially or fully\
    \ filled, or in\n   fact not being filled at all.  Unused write chunks, or unused\
    \ bytes\n   in write chunk buffer lists, are not returned as results, and their\n\
    \   memory is returned to the upper layer as part of RPC completion.\n   However,\
    \ the RPC layer MUST NOT assume that the buffers have not been\n   modified.\n"
- title: 3.5.  XDR Decoding with Read Chunks
  contents:
  - "3.5.  XDR Decoding with Read Chunks\n   The XDR decode process moves data from\
    \ an XDR stream into a data\n   structure provided by the RPC client or server\
    \ application.  Where\n   elements of the destination data structure are buffers\
    \ or strings,\n   the RPC application can either pre-allocate storage to receive\
    \ the\n   data or leave the string or buffer fields null and allow the XDR\n \
    \  decode stage of RPC processing to automatically allocate storage of\n   sufficient\
    \ size.\n   When decoding a message from an RDMA transport, the receiver first\n\
    \   XDR decodes the chunk lists from the RPC-over-RDMA header, then\n   proceeds\
    \ to decode the body of the RPC message (arguments or\n   results).  Whenever\
    \ the XDR offset in the decode stream matches that\n   of a chunk in the read\
    \ chunk list, the XDR routine initiates an RDMA\n   Read to bring over the chunk\
    \ data into locally registered memory for\n   the destination buffer.\n   When\
    \ processing an RPC request, the RPC receiver (RPC server)\n   acknowledges its\
    \ completion of use of the source buffers by simply\n   replying to the RPC sender\
    \ (client), and the peer may then free all\n   source buffers advertised by the\
    \ request.\n   When processing an RPC reply, after completing such a transfer,\
    \ the\n   RPC receiver (client) MUST issue an RDMA_DONE message (described in\n\
    \   Section 3.8) to notify the peer (server) that the source buffers can\n   be\
    \ freed.\n   The read chunk list is constructed and used entirely within the\n\
    \   RPC/XDR layer.  Other than specifying the minimum chunk size, the\n   management\
    \ of the read chunk list is automatic and transparent to an\n   RPC application.\n"
- title: 3.6.  XDR Decoding with Write Chunks
  contents:
  - "3.6.  XDR Decoding with Write Chunks\n   When a write chunk list is provided\
    \ for the results of the RPC call,\n   the RPC server MUST provide any corresponding\
    \ data via RDMA Write to\n   the memory referenced in the chunk list entries.\
    \  The RPC reply\n   conveys this by returning the write chunk list to the client\
    \ with the\n   lengths rewritten to match the actual transfer.  The XDR decode\
    \ of\n   the reply therefore performs no local data transfer but merely\n   returns\
    \ the length obtained from the reply.\n   Each decoded result consumes one entry\
    \ in the write chunk list, which\n   in turn consists of an array of RDMA segments.\
    \  The length is\n   therefore the sum of all returned lengths in all segments\
    \ comprising\n   the corresponding list entry.  As each list entry is decoded,\
    \ the\n   entire entry is consumed.\n   The write chunk list is constructed and\
    \ used by the RPC application.\n   The RPC/XDR layer simply conveys the list between\
    \ client and server\n   and initiates the RDMA Writes back to the client.  The\
    \ mapping of\n   write chunk list entries to procedure arguments MUST be determined\n\
    \   for each protocol.  An example of a mapping is described in\n   [RFC5667].\n"
- title: 3.7.  XDR Roundup and Chunks
  contents:
  - "3.7.  XDR Roundup and Chunks\n   The XDR protocol requires 4-byte alignment of\
    \ each new encoded\n   element in any XDR stream.  This requirement is for efficiency\
    \ and\n   ease of decode/unmarshaling at the receiver -- if the XDR stream\n \
    \  buffer begins on a native machine boundary, then the XDR elements\n   will\
    \ lie on similarly predictable offsets in memory.\n   Within XDR, when non-4-byte\
    \ encodes (such as an odd-length string or\n   bulk data) are marshaled, their\
    \ length is encoded literally, while\n   their data is padded to begin the next\
    \ element at a 4-byte boundary\n   in the XDR stream.  For TCP or RDMA inline\
    \ encoding, this minimal\n   overhead is required because the transport-specific\
    \ framing relies on\n   the fact that the relative offset of the elements in the\
    \ XDR stream\n   from the start of the message determines the XDR position during\n\
    \   decode.\n   On the other hand, RPC/RDMA Read chunks carry the XDR position\
    \ of\n   each chunked element and length of the Chunk segment, and can be\n  \
    \ placed by the receiver exactly where they belong in the receiver's\n   memory\
    \ without regard to the alignment of their position in the XDR\n   stream.  Since\
    \ any rounded-up data is not actually part of the upper\n   layer's message, the\
    \ receiver will not reference it, and there is no\n   reason to set it to any\
    \ particular value in the receiver's memory.\n   When roundup is present at the\
    \ end of a sequence of chunks, the\n   length of the sequence will terminate it\
    \ at a non-4-byte XDR\n   position.  When the receiver proceeds to decode the\
    \ remaining part of\n   the XDR stream, it inspects the XDR position indicated\
    \ by the next\n   chunk.  Because this position will not match (else roundup would\
    \ not\n   have occurred), the receiver decoding will fall back to inspecting\n\
    \   the remaining inline portion.  If in turn, no data remains to be\n   decoded\
    \ from the inline portion, then the receiver MUST conclude that\n   roundup is\
    \ present, and therefore it advances the XDR decode position\n   to that indicated\
    \ by the next chunk (if any).  In this way, roundup\n   is passed without ever\
    \ actually transferring additional XDR bytes.\n   Some protocol operations over\
    \ RPC/RDMA, for instance NFS writes of\n   data encountered at the end of a file\
    \ or in direct I/O situations,\n   commonly yield these roundups within RDMA Read\
    \ Chunks.  Because any\n   roundup bytes are not actually present in the data\
    \ buffers being\n   written, memory for these bytes would come from noncontiguous\n\
    \   buffers, either as an additional memory registration segment or as an\n  \
    \ additional Chunk.  The overhead of these operations can be\n   significant to\
    \ both the sender to marshal them and even higher to the\n   receiver to which\
    \ to transfer them.  Senders SHOULD therefore avoid\n   encoding individual RDMA\
    \ Read Chunks for roundup whenever possible.\n   It is acceptable, but not necessary,\
    \ to include roundup data in an\n   existing RDMA Read Chunk, but only if it is\
    \ already present in the\n   XDR stream to carry upper-layer data.\n   Note that\
    \ there is no exposure of additional data at the sender due\n   to eliding roundup\
    \ data from the XDR stream, since any additional\n   sender buffers are never\
    \ exposed to the peer.  The data is literally\n   not there to be transferred.\n\
    \   For RDMA Write Chunks, a simpler encoding method applies.  Again,\n   roundup\
    \ bytes are not transferred, instead the chunk length sent to\n   the receiver\
    \ in the reply is simply increased to include any roundup.\n   Because of the\
    \ requirement that the RDMA Write Chunks are filled\n   sequentially without gaps,\
    \ this situation can only occur on the final\n   chunk receiving data.  Therefore,\
    \ there is no opportunity for roundup\n   data to insert misalignment or positional\
    \ gaps into the XDR stream.\n"
- title: 3.8.  RPC Call and Reply
  contents:
  - "3.8.  RPC Call and Reply\n   The RDMA transport for RPC provides three methods\
    \ of moving data\n   between RPC client and server:\n   Inline\n        Data is\
    \ moved between RPC client and server within an RDMA Send.\n   RDMA Read\n   \
    \     Data is moved between RPC client and server via an RDMA Read\n        operation\
    \ via steering tag; address and offset obtained from a\n        read chunk list.\n\
    \   RDMA Write\n        Result data is moved from RPC server to client via an\
    \ RDMA Write\n        operation via steering tag; address and offset obtained\
    \ from a\n        write chunk list or reply chunk in the client's RPC call\n \
    \       message.\n   These methods of data movement may occur in combinations\
    \ within a\n   single RPC.  For instance, an RPC call may contain some inline\
    \ data\n   along with some large chunks to be transferred via RDMA Read to the\n\
    \   server.  The reply to that call may have some result chunks that the\n   server\
    \ RDMA Writes back to the client.  The following protocol\n   interactions illustrate\
    \ RPC calls that use these methods to move RPC\n   message data:\n   An RPC with\
    \ write chunks in the call message:\n       RPC Client                       \
    \    RPC Server\n           |     RPC Call + Write Chunk list     |\n      Send\
    \ |   ------------------------------>   |\n           |                      \
    \               |\n           |               Chunk 1               |\n      \
    \     |   <------------------------------   | Write\n           |            \
    \      :                  |\n           |               Chunk n              \
    \ |\n           |   <------------------------------   | Write\n           |  \
    \                                   |\n           |               RPC Reply  \
    \           |\n           |   <------------------------------   | Send\n   In\
    \ the presence of write chunks, RDMA ordering provides the guarantee\n   that\
    \ all data in the RDMA Write operations has been placed in memory\n   prior to\
    \ the client's RPC reply processing.\n   An RPC with read chunks in the call message:\n\
    \       RPC Client                           RPC Server\n           |     RPC\
    \ Call + Read Chunk list      |\n      Send |   ------------------------------>\
    \   |\n           |                                     |\n           |      \
    \         Chunk 1               |\n           |   +------------------------------\
    \   | Read\n           |   v----------------------------->   |\n           | \
    \                 :                  |\n           |               Chunk n   \
    \            |\n           |   +------------------------------   | Read\n    \
    \       |   v----------------------------->   |\n           |                \
    \                     |\n           |               RPC Reply             |\n\
    \           |   <------------------------------   | Send\n   An RPC with read\
    \ chunks in the reply message:\n       RPC Client                           RPC\
    \ Server\n           |               RPC Call              |\n      Send |   ------------------------------>\
    \   |\n           |                                     |\n           |     RPC\
    \ Reply + Read Chunk list     |\n           |   <------------------------------\
    \   | Send\n           |                                     |\n           | \
    \              Chunk 1               |\n      Read |   ------------------------------+\
    \   |\n           |   <-----------------------------v   |\n           |      \
    \            :                  |\n           |               Chunk n        \
    \       |\n      Read |   ------------------------------+   |\n           |  \
    \ <-----------------------------v   |\n           |                          \
    \           |\n           |                 Done                |\n      Send\
    \ |   ------------------------------>   |\n   The final Done message allows the\
    \ RPC client to signal the server\n   that it has received the chunks, so the\
    \ server can de-register and\n   free the memory holding the chunks.  A Done completion\
    \ is not\n   necessary for an RPC call, since the RPC reply Send is itself a\n\
    \   receive completion notification.  In the event that the client fails\n   to\
    \ return the Done message within some timeout period, the server MAY\n   conclude\
    \ that a protocol violation has occurred and close the RPC\n   connection, or\
    \ it MAY proceed with a de-register and free its chunk\n   buffers.  This may\
    \ result in a fatal RDMA error if the client later\n   attempts to perform an\
    \ RDMA Read operation, which amounts to the same\n   thing.\n   The use of read\
    \ chunks in RPC reply messages is much less efficient\n   than providing write\
    \ chunks in the originating RPC calls, due to the\n   additional message exchanges,\
    \ the need for the RPC server to\n   advertise buffers to the peer, the necessity\
    \ of the server\n   maintaining a timer for the purpose of recovery from misbehaving\n\
    \   clients, and the need for additional memory registration.  Their use\n   is\
    \ NOT RECOMMENDED by upper layers where efficiency is a primary\n   concern [RFC5667].\
    \  However, they MAY be employed by upper-layer\n   protocol bindings that are\
    \ primarily concerned with transparency,\n   since they can frequently be implemented\
    \ completely within the RPC\n   lower layers.\n   It is important to note that\
    \ the Done message consumes a credit at\n   the RPC server.  The RPC server SHOULD\
    \ provide sufficient credits to\n   the client to allow the Done message to be\
    \ sent without deadlock\n   (driving the outstanding credit count to zero).  The\
    \ RPC client MUST\n   account for its required Done messages to the server in\
    \ its\n   accounting of available credits, and the server SHOULD replenish any\n\
    \   credit consumed by its use of such exchanges at its earliest\n   opportunity.\n\
    \   Finally, it is possible to conceive of RPC exchanges that involve any\n  \
    \ or all combinations of write chunks in the RPC call, read chunks in\n   the\
    \ RPC call, and read chunks in the RPC reply.  Support for such\n   exchanges\
    \ is straightforward from a protocol perspective, but in\n   practice such exchanges\
    \ would be quite rare, limited to upper-layer\n   protocol exchanges that transferred\
    \ bulk data in both the call and\n   corresponding reply.\n"
- title: 3.9.  Padding
  contents:
  - "3.9.  Padding\n   Alignment of specific opaque data enables certain scatter/gather\n\
    \   optimizations.  Padding leverages the useful property that RDMA\n   transfers\
    \ preserve alignment of data, even when they are placed into\n   pre-posted receive\
    \ buffers by Sends.\n   Many servers can make good use of such padding.  Padding\
    \ allows the\n   chaining of RDMA receive buffers such that any data transferred\
    \ by\n   RDMA on behalf of RPC requests will be placed into appropriately\n  \
    \ aligned buffers on the system that receives the transfer.  In this\n   way,\
    \ the need for servers to perform RDMA Read to satisfy all but the\n   largest\
    \ client writes is obviated.\n   The effect of padding is demonstrated below showing\
    \ prior bytes on an\n   XDR stream (\"XXX\" in the figure below) followed by an\
    \ opaque field\n   consisting of four length bytes (\"LLLL\") followed by data\
    \ bytes\n   (\"DDD\").  The receiver of the RDMA Send has posted two chained\n\
    \   receive buffers.  Without padding, the opaque data is split across\n   the\
    \ two buffers.  With the addition of padding bytes (\"ppp\") prior to\n   the\
    \ first data byte, the data can be forced to align correctly in the\n   second\
    \ buffer.\n                                            Buffer 1       Buffer 2\n\
    \      Unpadded                           --------------  --------------\n   \
    \    XXXXXXXLLLLDDDDDDDDDDDDDD    ---> XXXXXXXLLLLDDD  DDDDDDDDDDD\n      Padded\n\
    \       XXXXXXXLLLLpppDDDDDDDDDDDDDD ---> XXXXXXXLLLLppp  DDDDDDDDDDDDDD\n   Padding\
    \ is implemented completely within the RDMA transport encoding,\n   flagged with\
    \ a specific message type.  Where padding is applied, two\n   values are passed\
    \ to the peer:  an \"rdma_align\", which is the padding\n   value used, and \"\
    rdma_thresh\", which is the opaque data size at or\n   above which padding is\
    \ applied.  For instance, if the server is using\n   chained 4 KB receive buffers,\
    \ then up to (4 KB - 1) padding bytes\n   could be used to achieve alignment of\
    \ the data.  The XDR routine at\n   the peer MUST consult these values when decoding\
    \ opaque values.\n   Where the decoded length exceeds the rdma_thresh, the XDR\
    \ decode MUST\n   skip over the appropriate padding as indicated by rdma_align\
    \ and the\n   current XDR stream position.\n"
- title: 4.  RPC RDMA Message Layout
  contents:
  - "4.  RPC RDMA Message Layout\n   RPC call and reply messages are conveyed across\
    \ an RDMA transport\n   with a prepended RPC-over-RDMA header.  The RPC-over-RDMA\
    \ header\n   includes data for RDMA flow control credits, padding parameters,\
    \ and\n   lists of addresses that provide direct data placement via RDMA Read\n\
    \   and Write operations.  The layout of the RPC message itself is\n   unchanged\
    \ from that described in [RFC5531] except for the possible\n   exclusion of large\
    \ data chunks that will be moved by RDMA Read or\n   Write operations.  If the\
    \ RPC message (along with the RPC-over-RDMA\n   header) is too long for the posted\
    \ receive buffer (even after any\n   large chunks are removed), then the entire\
    \ RPC message MAY be moved\n   separately as a chunk, leaving just the RPC-over-RDMA\
    \ header in the\n   RDMA Send.\n"
- title: 4.1.  RPC-over-RDMA Header
  contents:
  - "4.1.  RPC-over-RDMA Header\n   The RPC-over-RDMA header begins with four 32-bit\
    \ fields that are\n   always present and that control the RDMA interaction including\
    \ RDMA-\n   specific flow control.  These are then followed by a number of items\n\
    \   such as chunk lists and padding that MAY or MUST NOT be present\n   depending\
    \ on the type of transmission.  The four fields that are\n   always present are:\n\
    \   1. Transaction ID (XID).\n      The XID generated for the RPC call and reply.\
    \  Having the XID at\n      the beginning of the message makes it easy to establish\
    \ the\n      message context.  This XID MUST be the same as the XID in the RPC\n\
    \      header.  The receiver MAY perform its processing based solely on\n    \
    \  the XID in the RPC-over-RDMA header, and thereby ignore the XID in\n      the\
    \ RPC header, if it so chooses.\n   2. Version number.\n      This version of\
    \ the RPC RDMA message protocol is 1.  The version\n      number MUST be increased\
    \ by 1 whenever the format of the RPC RDMA\n      messages is changed.\n   3.\
    \ Flow control credit value.\n      When sent in an RPC call message, the requested\
    \ value is provided.\n      When sent in an RPC reply message, the granted value\
    \ is returned.\n      RPC calls SHOULD NOT be sent in excess of the currently\
    \ granted\n      limit.\n   4. Message type.\n      o  RDMA_MSG = 0 indicates\
    \ that chunk lists and RPC message follow.\n      o  RDMA_NOMSG = 1 indicates\
    \ that after the chunk lists there is no\n         RPC message.  In this case,\
    \ the chunk lists provide information\n         to allow the message proper to\
    \ be transferred using RDMA Read\n         or Write and thus is not appended to\
    \ the RPC-over-RDMA header.\n      o  RDMA_MSGP = 2 indicates that a chunk list\
    \ and RPC message with\n         some padding follow.\n      o  RDMA_DONE = 3\
    \ indicates that the message signals the completion\n         of a chunk transfer\
    \ via RDMA Read.\n      o  RDMA_ERROR = 4 is used to signal any detected error(s)\
    \ in the\n         RPC RDMA chunk encoding.\n   Because the version number is\
    \ encoded as part of this header, and the\n   RDMA_ERROR message type is used\
    \ to indicate errors, these first four\n   fields and the start of the following\
    \ message body MUST always remain\n   aligned at these fixed offsets for all versions\
    \ of the RPC-over-RDMA\n   header.\n   For a message of type RDMA_MSG or RDMA_NOMSG,\
    \ the Read and Write\n   chunk lists follow.  If the Read chunk list is null (a\
    \ 32-bit word of\n   zeros), then there are no chunks to be transferred separately\
    \ and the\n   RPC message follows in its entirety.  If non-null, then it's the\n\
    \   beginning of an XDR encoded sequence of Read chunk list entries.  If\n   the\
    \ Write chunk list is non-null, then an XDR encoded sequence of\n   Write chunk\
    \ entries follows.\n   If the message type is RDMA_MSGP, then two additional fields\
    \ that\n   specify the padding alignment and threshold are inserted prior to the\n\
    \   Read and Write chunk lists.\n   A header of message type RDMA_MSG or RDMA_MSGP\
    \ MUST be followed by\n   the RPC call or RPC reply message body, beginning with\
    \ the XID.  The\n   XID in the RDMA_MSG or RDMA_MSGP header MUST match this.\n\
    \   +--------+---------+---------+-----------+-------------+----------\n   | \
    \       |         |         | Message   |   NULLs     | RPC Call\n   |  XID  \
    \ | Version | Credits |  Type     |    or       |    or\n   |        |       \
    \  |         |           | Chunk Lists | Reply Msg\n   +--------+---------+---------+-----------+-------------+----------\n\
    \   Note that in the case of RDMA_DONE and RDMA_ERROR, no chunk list or\n   RPC\
    \ message follows.  As an implementation hint: a gather operation\n   on the Send\
    \ of the RDMA RPC message can be used to marshal the\n   initial header, the chunk\
    \ list, and the RPC message itself.\n"
- title: 4.2.  RPC-over-RDMA Header Errors
  contents:
  - "4.2.  RPC-over-RDMA Header Errors\n   When a peer receives an RPC RDMA message,\
    \ it MUST perform the\n   following basic validity checks on the header and chunk\
    \ contents.  If\n   such errors are detected in the request, an RDMA_ERROR reply\
    \ MUST be\n   generated.\n   Two types of errors are defined, version mismatch\
    \ and invalid chunk\n   format.  When the peer detects an RPC-over-RDMA header\
    \ version that\n   it does not support (currently this document defines only version\
    \ 1),\n   it replies with an error code of ERR_VERS, and provides the low and\n\
    \   high inclusive version numbers it does, in fact, support.  The\n   version\
    \ number in this reply MUST be any value otherwise valid at the\n   receiver.\
    \  When other decoding errors are detected in the header or\n   chunks, either\
    \ an RPC decode error MAY be returned or the RPC/RDMA\n   error code ERR_CHUNK\
    \ MUST be returned.\n"
- title: 4.3.  XDR Language Description
  contents:
  - "4.3.  XDR Language Description\n   Here is the message layout in XDR language.\n\
    \      struct xdr_rdma_segment {\n         uint32 handle;          /* Registered\
    \ memory handle */\n         uint32 length;          /* Length of the chunk in\
    \ bytes */\n         uint64 offset;          /* Chunk virtual address or offset\
    \ */\n      };\n      struct xdr_read_chunk {\n         uint32 position;     \
    \   /* Position in XDR stream */\n         struct xdr_rdma_segment target;\n \
    \     };\n      struct xdr_read_list {\n         struct xdr_read_chunk entry;\n\
    \         struct xdr_read_list  *next;\n      };\n      struct xdr_write_chunk\
    \ {\n         struct xdr_rdma_segment target<>;\n      };\n      struct xdr_write_list\
    \ {\n         struct xdr_write_chunk entry;\n         struct xdr_write_list  *next;\n\
    \      };\n      struct rdma_msg {\n         uint32    rdma_xid;     /* Mirrors\
    \ the RPC header xid */\n         uint32    rdma_vers;    /* Version of this protocol\
    \ */\n         uint32    rdma_credit;  /* Buffers requested/granted */\n     \
    \    rdma_body rdma_body;\n      };\n      enum rdma_proc {\n         RDMA_MSG=0,\
    \   /* An RPC call or reply msg */\n         RDMA_NOMSG=1, /* An RPC call or reply\
    \ msg - separate body */\n         RDMA_MSGP=2,  /* An RPC call or reply msg with\
    \ padding */\n         RDMA_DONE=3,  /* Client signals reply completion */\n \
    \        RDMA_ERROR=4  /* An RPC RDMA encoding error */\n      };\n      union\
    \ rdma_body switch (rdma_proc proc) {\n         case RDMA_MSG:\n           rpc_rdma_header\
    \ rdma_msg;\n         case RDMA_NOMSG:\n           rpc_rdma_header_nomsg rdma_nomsg;\n\
    \         case RDMA_MSGP:\n           rpc_rdma_header_padded rdma_msgp;\n    \
    \     case RDMA_DONE:\n           void;\n         case RDMA_ERROR:\n         \
    \  rpc_rdma_error rdma_error;\n      };\n      struct rpc_rdma_header {\n    \
    \     struct xdr_read_list   *rdma_reads;\n         struct xdr_write_list  *rdma_writes;\n\
    \         struct xdr_write_chunk *rdma_reply;\n         /* rpc body follows */\n\
    \      };\n      struct rpc_rdma_header_nomsg {\n         struct xdr_read_list\
    \   *rdma_reads;\n         struct xdr_write_list  *rdma_writes;\n         struct\
    \ xdr_write_chunk *rdma_reply;\n      };\n      struct rpc_rdma_header_padded\
    \ {\n         uint32                 rdma_align;   /* Padding alignment */\n \
    \        uint32                 rdma_thresh;  /* Padding threshold */\n      \
    \   struct xdr_read_list   *rdma_reads;\n         struct xdr_write_list  *rdma_writes;\n\
    \         struct xdr_write_chunk *rdma_reply;\n         /* rpc body follows */\n\
    \      };\n      enum rpc_rdma_errcode {\n         ERR_VERS = 1,\n         ERR_CHUNK\
    \ = 2\n      };\n      union rpc_rdma_error switch (rpc_rdma_errcode err) {\n\
    \         case ERR_VERS:\n           uint32               rdma_vers_low;\n   \
    \        uint32               rdma_vers_high;\n         case ERR_CHUNK:\n    \
    \       void;\n         default:\n           uint32               rdma_extra[8];\n\
    \      };\n"
- title: 5.  Long Messages
  contents:
  - "5.  Long Messages\n   The receiver of RDMA Send messages is required by RDMA\
    \ to have\n   previously posted one or more adequately sized buffers.  The RPC\n\
    \   client can inform the server of the maximum size of its RDMA Send\n   messages\
    \ via the Connection Configuration Protocol described later in\n   this document.\n\
    \   Since RPC messages are frequently small, memory savings can be\n   achieved\
    \ by posting small buffers.  Even large messages like NFS READ\n   or WRITE will\
    \ be quite small once the chunks are removed from the\n   message.  However, there\
    \ may be large messages that would demand a\n   very large buffer be posted, where\
    \ the contents of the buffer may not\n   be a chunkable XDR element.  A good example\
    \ is an NFS READDIR reply,\n   which may contain a large number of small filename\
    \ strings.  Also,\n   the NFS version 4 protocol [RFC3530] features COMPOUND request\
    \ and\n   reply messages of unbounded length.\n   Ideally, each upper layer will\
    \ negotiate these limits.  However, it\n   is frequently necessary to provide\
    \ a transparent solution.\n"
- title: 5.1.  Message as an RDMA Read Chunk
  contents:
  - "5.1.  Message as an RDMA Read Chunk\n   One relatively simple method is to have\
    \ the client identify any RPC\n   message that exceeds the RPC server's posted\
    \ buffer size and move it\n   separately as a chunk, i.e., reference it as the\
    \ first entry in the\n   read chunk list with an XDR position of zero.\n   Normal\
    \ Message\n   +--------+---------+---------+------------+-------------+----------\n\
    \   |        |         |         |            |             | RPC Call\n   | \
    \ XID   | Version | Credits |  RDMA_MSG  | Chunk Lists |    or\n   |        |\
    \         |         |            |             | Reply Msg\n   +--------+---------+---------+------------+-------------+----------\n\
    \   Long Message\n   +--------+---------+---------+------------+-------------+\n\
    \   |        |         |         |            |             |\n   |  XID   | Version\
    \ | Credits | RDMA_NOMSG | Chunk Lists |\n   |        |         |         |  \
    \          |             |\n   +--------+---------+---------+------------+-------------+\n\
    \                                                |\n                         \
    \                       |  +----------\n                                     \
    \           |  | Long RPC Call\n                                             \
    \   +->|    or\n                                                   | Reply Message\n\
    \                                                   +----------\n   If the receiver\
    \ gets an RPC-over-RDMA header with a message type of\n   RDMA_NOMSG and finds\
    \ an initial read chunk list entry with a zero XDR\n   position, it allocates\
    \ a registered buffer and issues an RDMA Read of\n   the long RPC message into\
    \ it.  The receiver then proceeds to XDR\n   decode the RPC message as if it had\
    \ received it inline with the Send\n   data.  Further decoding may issue additional\
    \ RDMA Reads to bring over\n   additional chunks.\n   Although the handling of\
    \ long messages requires one extra network\n   turnaround, in practice these messages\
    \ will be rare if the posted\n   receive buffers are correctly sized, and of course\
    \ they will be\n   non-existent for RDMA-aware upper layers.\n   A long call RPC\
    \ with request supplied via RDMA Read\n       RPC Client                     \
    \      RPC Server\n           |        RDMA-over-RPC Header         |\n      Send\
    \ |   ------------------------------>   |\n           |                      \
    \               |\n           |          Long RPC Call Msg          |\n      \
    \     |   +------------------------------   | Read\n           |   v----------------------------->\
    \   |\n           |                                     |\n           |      \
    \   RDMA-over-RPC Reply         |\n           |   <------------------------------\
    \   | Send\n   An RPC with long reply returned via RDMA Read\n       RPC Client\
    \                           RPC Server\n           |             RPC Call    \
    \            |\n      Send |   ------------------------------>   |\n         \
    \  |                                     |\n           |         RDMA-over-RPC\
    \ Header        |\n           |   <------------------------------   | Send\n \
    \          |                                     |\n           |          Long\
    \ RPC Reply Msg         |\n      Read |   ------------------------------+   |\n\
    \           |   <-----------------------------v   |\n           |            \
    \                         |\n           |                Done                \
    \ |\n      Send |   ------------------------------>   |\n   It is possible for\
    \ a single RPC procedure to employ both a long call\n   for its arguments and\
    \ a long reply for its results.  However, such an\n   operation is atypical, as\
    \ few upper layers define such exchanges.\n"
- title: 5.2.  RDMA Write of Long Replies (Reply Chunks)
  contents:
  - "5.2.  RDMA Write of Long Replies (Reply Chunks)\n   A superior method of handling\
    \ long RPC replies is to have the RPC\n   client post a large buffer into which\
    \ the server can write a large\n   RPC reply.  This has the advantage that an\
    \ RDMA Write may be slightly\n   faster in network latency than an RDMA Read,\
    \ and does not require the\n   server to wait for the completion as it must for\
    \ RDMA Read.\n   Additionally, for a reply it removes the need for an RDMA_DONE\n\
    \   message if the large reply is returned as a Read chunk.\n   This protocol\
    \ supports direct return of a large reply via the\n   inclusion of an OPTIONAL\
    \ rdma_reply write chunk after the read chunk\n   list and the write chunk list.\
    \  The client allocates a buffer sized\n   to receive a large reply and enters\
    \ its steering tag, address and\n   length in the rdma_reply write chunk.  If\
    \ the reply message is too\n   long to return inline with an RDMA Send (exceeds\
    \ the size of the\n   client's posted receive buffer), even with read chunks removed,\
    \ then\n   the RPC server performs an RDMA Write of the RPC reply message into\n\
    \   the buffer indicated by the rdma_reply chunk.  If the client doesn't\n   provide\
    \ an rdma_reply chunk, or if it's too small, then if the upper-\n   layer specification\
    \ permits, the message MAY be returned as a Read\n   chunk.\n   An RPC with long\
    \ reply returned via RDMA Write\n    RPC Client                           RPC\
    \ Server\n        |      RPC Call with rdma_reply       |\n   Send |   ------------------------------>\
    \   |\n        |                                     |\n        |          Long\
    \ RPC Reply Msg         |\n        |   <------------------------------   | Write\n\
    \        |                                     |\n        |         RDMA-over-RPC\
    \ Header        |\n        |   <------------------------------   | Send\n   The\
    \ use of RDMA Write to return long replies requires that the client\n   applications\
    \ anticipate a long reply and have some knowledge of its\n   size so that an adequately\
    \ sized buffer can be allocated.  This is\n   certainly true of NFS READDIR replies;\
    \ where the client already\n   provides an upper bound on the size of the encoded\
    \ directory fragment\n   to be returned by the server.\n   The use of these \"\
    reply chunks\" is highly efficient and convenient\n   for both RPC client and\
    \ server.  Their use is encouraged for eligible\n   RPC operations such as NFS\
    \ READDIR, which would otherwise require\n   extensive chunk management within\
    \ the results or use of RDMA Read and\n   a Done message [RFC5667].\n"
- title: 6.  Connection Configuration Protocol
  contents:
  - "6.  Connection Configuration Protocol\n   RDMA Send operations require the receiver\
    \ to post one or more buffers\n   at the RDMA connection endpoint, each large\
    \ enough to receive the\n   largest Send message.  Buffers are consumed as Send\
    \ messages are\n   received.  If a buffer is too small, or if there are no buffers\n\
    \   posted, the RDMA transport MAY return an error and break the RDMA\n   connection.\
    \  The receiver MUST post sufficient, adequately buffers to\n   avoid buffer overrun\
    \ or capacity errors.\n   The protocol described above includes only a mechanism\
    \ for managing\n   the number of such receive buffers and no explicit features\
    \ to allow\n   the RPC client and server to provision or control buffer sizing,\
    \ nor\n   any other session parameters.\n   In the past, this type of connection\
    \ management has not been\n   necessary for RPC.  RPC over UDP or TCP does not\
    \ have a protocol to\n   negotiate the link.  The server can get a rough idea\
    \ of the maximum\n   size of messages from the server protocol code.  However,\
    \ a protocol\n   to negotiate transport features on a more dynamic basis is desirable.\n\
    \   The Connection Configuration Protocol allows the client to pass its\n   connection\
    \ requirements to the server, and allows the server to\n   inform the client of\
    \ its connection limits.\n   Use of the Connection Configuration Protocol by an\
    \ upper layer is\n   OPTIONAL.\n"
- title: 6.1.  Initial Connection State
  contents:
  - "6.1.  Initial Connection State\n   This protocol MAY be used for connection setup\
    \ prior to the use of\n   another RPC protocol that uses the RDMA transport. \
    \ It operates\n   in-band, i.e., it uses the connection itself to negotiate the\n\
    \   connection parameters.  To provide a basis for connection\n   negotiation,\
    \ the connection is assumed to provide a basic level of\n   interoperability:\
    \ the ability to exchange at least one RPC message at\n   a time that is at least\
    \ 1 KB in size.  The server MAY exceed this\n   basic level of configuration,\
    \ but the client MUST NOT assume more\n   than one, and MUST receive a valid reply\
    \ from the server carrying the\n   actual number of available receive messages,\
    \ prior to sending its\n   next request.\n"
- title: 6.2.  Protocol Description
  contents:
  - "6.2.  Protocol Description\n   Version 1 of the Connection Configuration Protocol\
    \ consists of a\n   single procedure that allows the client to inform the server\
    \ of its\n   connection requirements and the server to return connection\n   information\
    \ to the client.\n   The maxcall_sendsize argument is the maximum size of an RPC\
    \ call\n   message that the client MAY send inline in an RDMA Send message to\n\
    \   the server.  The server MAY return a maxcall_sendsize value that is\n   smaller\
    \ or larger than the client's request.  The client MUST NOT\n   send an inline\
    \ call message larger than what the server will accept.\n   The maxcall_sendsize\
    \ limits only the size of inline RPC calls.  It\n   does not limit the size of\
    \ long RPC messages transferred as an\n   initial chunk in the Read chunk list.\n\
    \   The maxreply_sendsize is the maximum size of an inline RPC message\n   that\
    \ the client will accept from the server.\n   The maxrdmaread is the maximum number\
    \ of RDMA Reads that may be\n   active at the peer.  This number correlates to\
    \ the RDMA incoming RDMA\n   Read count (\"IRD\") configured into each originating\
    \ endpoint by the\n   client or server.  If more than this number of RDMA Read\
    \ operations\n   by the connected peer are issued simultaneously, connection loss\
    \ or\n   suboptimal flow control may result; therefore, the value SHOULD be\n\
    \   observed at all times.  The peers' values need not be equal.  If\n   zero,\
    \ the peer MUST NOT issue requests that require RDMA Read to\n   satisfy, as no\
    \ transfer will be possible.\n   The align value is the value recommended by the\
    \ server for opaque\n   data values such as strings and counted byte arrays. \
    \ The client MAY\n   use this value to compute the number of prepended pad bytes\
    \ when XDR\n   encoding opaque values in the RPC call message.\n      typedef\
    \ unsigned int uint32;\n      struct config_rdma_req {\n           uint32  maxcall_sendsize;\n\
    \                       /* max size of inline RPC call */\n           uint32 \
    \ maxreply_sendsize;\n                       /* max size of inline RPC reply */\n\
    \           uint32  maxrdmaread;\n                       /* max active RDMA Reads\
    \ at client */\n      };\n      struct config_rdma_reply {\n           uint32\
    \  maxcall_sendsize;\n                       /* max call size accepted by server\
    \ */\n           uint32  align;\n                       /* server's receive buffer\
    \ alignment */\n           uint32  maxrdmaread;\n                       /* max\
    \ active RDMA Reads at server */\n      };\n      program CONFIG_RDMA_PROG {\n\
    \         version VERS1 {\n            /*\n             * Config call/reply\n\
    \             */\n            config_rdma_reply CONF_RDMA(config_rdma_req) = 1;\n\
    \         } = 1;\n      } = 100417;\n"
- title: 7.  Memory Registration Overhead
  contents:
  - "7.  Memory Registration Overhead\n   RDMA requires that all data be transferred\
    \ between registered memory\n   regions at the source and destination.  All protocol\
    \ headers as well\n   as separately transferred data chunks use registered memory.\
    \  Since\n   the cost of registering and de-registering memory can be a large\n\
    \   proportion of the RDMA transaction cost, it is important to minimize\n   registration\
    \ activity.  This is easily achieved within RPC controlled\n   memory by allocating\
    \ chunk list data and RPC headers in a reusable\n   way from pre-registered pools.\n\
    \   The data chunks transferred via RDMA MAY occupy memory that persists\n   outside\
    \ the bounds of the RPC transaction.  Hence, the default\n   behavior of an RPC-over-RDMA\
    \ transport is to register and de-register\n   these chunks on every transaction.\
    \  However, this is not a limitation\n   of the protocol -- only of the existing\
    \ local RPC API.  The API is\n   easily extended through such functions as rpc_control(3)\
    \ to change\n   the default behavior so that the application can assume\n   responsibility\
    \ for controlling memory registration through an RPC-\n   provided registered\
    \ memory allocator.\n"
- title: 8.  Errors and Error Recovery
  contents:
  - "8.  Errors and Error Recovery\n   RPC RDMA protocol errors are described in Section\
    \ 4.  RPC errors and\n   RPC error recovery are not affected by the protocol,\
    \ and proceed as\n   for any RPC error condition.  RDMA transport error reporting\
    \ and\n   recovery are outside the scope of this protocol.\n   It is assumed that\
    \ the link itself will provide some degree of error\n   detection and retransmission.\
    \  iWARP's Marker PDU Aligned (MPA) layer\n   (when used over TCP), Stream Control\
    \ Transmission Protocol (SCTP), as\n   well as the InfiniBand link layer all provide\
    \ Cyclic Redundancy Check\n   (CRC) protection of the RDMA payload, and CRC-class\
    \ protection is a\n   general attribute of such transports.  Additionally, the\
    \ RPC layer\n   itself can accept errors from the link level and recover via\n\
    \   retransmission.  RPC recovery can handle complete loss and\n   re-establishment\
    \ of the link.\n   See Section 11 for further discussion of the use of RPC-level\n\
    \   integrity schemes to detect errors and related efficiency issues.\n"
- title: 9.  Node Addressing
  contents:
  - "9.  Node Addressing\n   In setting up a new RDMA connection, the first action\
    \ by an RPC\n   client will be to obtain a transport address for the server. \
    \ The\n   mechanism used to obtain this address, and to open an RDMA connection\n\
    \   is dependent on the type of RDMA transport, and is the responsibility\n  \
    \ of each RPC protocol binding and its local implementation.\n"
- title: 10.  RPC Binding
  contents:
  - "10.  RPC Binding\n   RPC services normally register with a portmap or rpcbind\
    \ [RFC1833]\n   service, which associates an RPC program number with a service\n\
    \   address.  (In the case of UDP or TCP, the service address for NFS is\n   normally\
    \ port 2049.)  This policy is no different with RDMA\n   interconnects, although\
    \ it may require the allocation of port numbers\n   appropriate to each upper-layer\
    \ binding that uses the RPC framing\n   defined here.\n   When mapped atop the\
    \ iWARP [RFC5040, RFC5041] transport, which uses\n   IP port addressing due to\
    \ its layering on TCP and/or SCTP, port\n   mapping is trivial and consists merely\
    \ of issuing the port in the\n   connection process.  The NFS/RDMA protocol service\
    \ address has been\n   assigned port 20049 by IANA, for both iWARP/TCP and iWARP/SCTP.\n\
    \   When mapped atop InfiniBand [IB], which uses a Group Identifier\n   (GID)-based\
    \ service endpoint naming scheme, a translation MUST be\n   employed.  One such\
    \ translation is defined in the InfiniBand Port\n   Addressing Annex [IBPORT],\
    \ which is appropriate for translating IP\n   port addressing to the InfiniBand\
    \ network.  Therefore, in this case,\n   IP port addressing may be readily employed\
    \ by the upper layer.\n   When a mapping standard or convention exists for IP\
    \ ports on an RDMA\n   interconnect, there are several possibilities for each\
    \ upper layer to\n   consider:\n      One possibility is to have an upper-layer\
    \ server register its\n      mapped IP port with the rpcbind service, under the\
    \ netid (or\n      netid's) defined here.  An RPC/RDMA-aware client can then resolve\n\
    \      its desired service to a mappable port, and proceed to connect.\n     \
    \ This is the most flexible and compatible approach, for those upper\n      layers\
    \ that are defined to use the rpcbind service.\n      A second possibility is\
    \ to have the server's portmapper register\n      itself on the RDMA interconnect\
    \ at a \"well known\" service address.\n      (On UDP or TCP, this corresponds\
    \ to port 111.)  A client could\n      connect to this service address and use\
    \ the portmap protocol to\n      obtain a service address in response to a program\
    \ number, e.g., an\n      iWARP port number, or an InfiniBand GID.\n      Alternatively,\
    \ the client could simply connect to the mapped well-\n      known port for the\
    \ service itself, if it is appropriately defined.\n      By convention, the NFS/RDMA\
    \ service, when operating atop such an\n      InfiniBand fabric, will use the\
    \ same 20049 assignment as for\n      iWARP.\n   Historically, different RPC protocols\
    \ have taken different approaches\n   to their port assignment; therefore, the\
    \ specific method is left to\n   each RPC/RDMA-enabled upper-layer binding, and\
    \ not addressed here.\n   In Section 12, \"IANA Considerations\", this specification\
    \ defines two\n   new \"netid\" values, to be used for registration of upper layers\
    \ atop\n   iWARP [RFC5040, RFC5041] and (when a suitable port translation\n  \
    \ service is available) InfiniBand [IB].  Additional RDMA-capable\n   networks\
    \ MAY define their own netids, or if they provide a port\n   translation, MAY\
    \ share the one defined here.\n"
- title: 11.  Security Considerations
  contents:
  - "11.  Security Considerations\n   RPC provides its own security via the RPCSEC_GSS\
    \ framework [RFC2203].\n   RPCSEC_GSS can provide message authentication, integrity\
    \ checking,\n   and privacy.  This security mechanism will be unaffected by the\
    \ RDMA\n   transport.  The data integrity and privacy features alter the body\
    \ of\n   the message, presenting it as a single chunk.  For large messages the\n\
    \   chunk may be large enough to qualify for RDMA Read transfer.\n   However,\
    \ there is much data movement associated with computation and\n   verification\
    \ of integrity, or encryption/decryption, so certain\n   performance advantages\
    \ may be lost.\n   For efficiency, a more appropriate security mechanism for RDMA\
    \ links\n   may be link-level protection, such as certain configurations of\n\
    \   IPsec, which may be co-located in the RDMA hardware.  The use of\n   link-level\
    \ protection MAY be negotiated through the use of the new\n   RPCSEC_GSS mechanism\
    \ defined in [RFC5403] in conjunction with the\n   Channel Binding mechanism [RFC5056]\
    \ and IPsec Channel Connection\n   Latching [RFC5660].  Use of such mechanisms\
    \ is REQUIRED where\n   integrity and/or privacy is desired, and where efficiency\
    \ is\n   required.\n   An additional consideration is the protection of the integrity\
    \ and\n   privacy of local memory by the RDMA transport itself.  The use of\n\
    \   RDMA by RPC MUST NOT introduce any vulnerabilities to system memory\n   contents,\
    \ or to memory owned by user processes.  These protections\n   are provided by\
    \ the RDMA layer specifications, and specifically their\n   security models. \
    \ It is REQUIRED that any RDMA provider used for RPC\n   transport be conformant\
    \ to the requirements of [RFC5042] in order to\n   satisfy these protections.\n\
    \   Once delivered securely by the RDMA provider, any RDMA-exposed\n   addresses\
    \ will contain only RPC payloads in the chunk lists,\n   transferred under the\
    \ protection of RPCSEC_GSS integrity and privacy.\n   By these means, the data\
    \ will be protected end-to-end, as required by\n   the RPC layer security model.\n\
    \   Where upper-layer protocols choose to supply results to the requester\n  \
    \ via read chunks, a server resource deficit can arise if the client\n   does\
    \ not promptly acknowledge their status via the RDMA_DONE message.\n   This can\
    \ potentially lead to a denial-of-service situation, with a\n   single client\
    \ unfairly (and unnecessarily) consuming server RDMA\n   resources.  Servers for\
    \ such upper-layer protocols MUST protect\n   against this situation, originating\
    \ from one or many clients.  For\n   example, a time-based window of buffer availability\
    \ may be offered,\n   if the client fails to obtain the data within the window,\
    \ it will\n   simply retry using ordinary RPC retry semantics.  Or, a more severe\n\
    \   method would be for the server to simply close the client's RDMA\n   connection,\
    \ freeing the RDMA resources and allowing the server to\n   reclaim them.\n  \
    \ A fairer and more useful method is provided by the protocol itself.\n   The\
    \ server MAY use the rdma_credit value to limit the number of\n   outstanding\
    \ requests for each client.  By including the number of\n   outstanding RDMA_DONE\
    \ completions in the computation of available\n   client credits, the server can\
    \ limit its exposure to each client, and\n   therefore provide uninterrupted service\
    \ as its resources permit.\n   However, the server must ensure that it does not\
    \ decrease the credit\n   count to zero with this method, since the RDMA_DONE\
    \ message is not\n   acknowledged.  If the credit count were to drop to zero solely\
    \ due to\n   outstanding RDMA_DONE messages, the client would deadlock since it\n\
    \   would never obtain a new credit with which to continue.  Therefore,\n   if\
    \ the server adjusts credits to zero for outstanding RDMA_DONE, it\n   MUST withhold\
    \ its reply to at least one message in order to provide\n   the next credit. \
    \ The time-based window (or any other appropriate\n   method) SHOULD be used by\
    \ the server to recover resources in the\n   event that the client never returns.\n\
    \   The Connection Configuration Protocol, when used, MUST be protected\n   by\
    \ an appropriate RPC security flavor, to ensure it is not attacked\n   in the\
    \ process of initiating an RPC/RDMA connection.\n"
- title: 12.  IANA Considerations
  contents:
  - "12.  IANA Considerations\n   Three new assignments are specified by this document:\n\
    \   - A new set of RPC \"netids\" for resolving RPC/RDMA services\n   - Optional\
    \ service port assignments for upper-layer bindings\n   - An RPC program number\
    \ assignment for the configuration protocol\n   These assignments have been established,\
    \ as below.\n   The new RPC transport has been assigned an RPC \"netid\", which\
    \ is an\n   rpcbind [RFC1833] string used to describe the underlying protocol\
    \ in\n   order for RPC to select the appropriate transport framing, as well as\n\
    \   the format of the service addresses and ports.\n   The following \"Netid\"\
    \ registry strings are defined for this purpose:\n      NC_RDMA \"rdma\"\n   \
    \   NC_RDMA6 \"rdma6\"\n   These netids MAY be used for any RDMA network satisfying\
    \ the\n   requirements of Section 2, and able to identify service endpoints\n\
    \   using IP port addressing, possibly through use of a translation\n   service\
    \ as described above in Section 10, \"RPC Binding\".  The \"rdma\"\n   netid is\
    \ to be used when IPv4 addressing is employed by the\n   underlying transport,\
    \ and \"rdma6\" for IPv6 addressing.\n   The netid assignment policy and registry\
    \ are defined in [RFC5665].\n   As a new RPC transport, this protocol has no effect\
    \ on RPC program\n   numbers or existing registered port numbers.  However, new\
    \ port\n   numbers MAY be registered for use by RPC/RDMA-enabled services, as\n\
    \   appropriate to the new networks over which the services will operate.\n  \
    \ For example, the NFS/RDMA service defined in [RFC5667] has been\n   assigned\
    \ the port 20049, in the IANA registry:\n      nfsrdma 20049/tcp Network File\
    \ System (NFS) over RDMA\n      nfsrdma 20049/udp Network File System (NFS) over\
    \ RDMA\n      nfsrdma 20049/sctp Network File System (NFS) over RDMA\n   The OPTIONAL\
    \ Connection Configuration Protocol described herein\n   requires an RPC program\
    \ number assignment.  The value \"100417\" has\n   been assigned:\n      rdmaconfig\
    \ 100417 rpc.rdmaconfig\n   The RPC program number assignment policy and registry\
    \ are defined in\n   [RFC5531].\n"
- title: 13.  Acknowledgments
  contents:
  - "13.  Acknowledgments\n   The authors wish to thank Rob Thurlow, John Howard,\
    \ Chet Juszczak,\n   Alex Chiu, Peter Staubach, Dave Noveck, Brian Pawlowski,\
    \ Steve\n   Kleiman, Mike Eisler, Mark Wittle, Shantanu Mehendale, David\n   Robinson,\
    \ and Mallikarjun Chadalapaka for their contributions to this\n   document.\n"
- title: 14.  References
  contents:
  - '14.  References

    '
- title: 14.1.  Normative References
  contents:
  - "14.1.  Normative References\n   [RFC1833]  Srinivasan, R., \"Binding Protocols\
    \ for ONC RPC Version 2\",\n              RFC 1833, August 1995.\n   [RFC2203]\
    \  Eisler, M., Chiu, A., and L. Ling, \"RPCSEC_GSS Protocol\n              Specification\"\
    , RFC 2203, September 1997.\n   [RFC2119]  Bradner, S., \"Key words for use in\
    \ RFCs to Indicate\n              Requirement Levels\", BCP 14, RFC 2119, March\
    \ 1997.\n   [RFC4506]  Eisler, M., Ed., \"XDR: External Data Representation\n\
    \              Standard\", STD 67, RFC 4506, May 2006.\n   [RFC5042]  Pinkerton,\
    \ J. and E. Deleganes, \"Direct Data Placement\n              Protocol (DDP) /\
    \ Remote Direct Memory Access Protocol\n              (RDMAP) Security\", RFC\
    \ 5042, October 2007.\n   [RFC5056]  Williams, N., \"On the Use of Channel Bindings\
    \ to Secure\n              Channels\", RFC 5056, November 2007.\n   [RFC5403]\
    \  Eisler, M., \"RPCSEC_GSS Version 2\", RFC 5403, February\n              2009.\n\
    \   [RFC5531]  Thurlow, R., \"RPC: Remote Procedure Call Protocol\n          \
    \    Specification Version 2\", RFC 5531, May 2009.\n   [RFC5660]  Williams, N.,\
    \ \"IPsec Channels: Connection Latching\", RFC\n              5660, October 2009.\n\
    \   [RFC5665]  Eisler, M., \"IANA Considerations for Remote Procedure Call\n \
    \             (RPC) Network Identifiers and Universal Address Formats\",\n   \
    \           RFC 5665, January 2010.\n"
- title: 14.2.  Informative References
  contents:
  - "14.2.  Informative References\n   [RFC1094]  Sun Microsystems, \"NFS: Network\
    \ File System Protocol\n              specification\", RFC 1094, March 1989.\n\
    \   [RFC1813]  Callaghan, B., Pawlowski, B., and P. Staubach, \"NFS\n        \
    \      Version 3 Protocol Specification\", RFC 1813, June 1995.\n   [RFC3530]\
    \  Shepler, S., Callaghan, B., Robinson, D., Thurlow, R.,\n              Beame,\
    \ C., Eisler, M., and D. Noveck, \"Network File System\n              (NFS) version\
    \ 4 Protocol\", RFC 3530, April 2003.\n   [RFC5040]  Recio, R., Metzler, B., Culley,\
    \ P., Hilland, J., and D.\n              Garcia, \"A Remote Direct Memory Access\
    \ Protocol\n              Specification\", RFC 5040, October 2007.\n   [RFC5041]\
    \  Shah, H., Pinkerton, J., Recio, R., and P. Culley, \"Direct\n             \
    \ Data Placement over Reliable Transports\", RFC 5041,\n              October\
    \ 2007.\n   [RFC5532]  Talpey, T. and C. Juszczak, \"Network File System (NFS)\n\
    \              Remote Direct Memory Access (RDMA) Problem Statement\", RFC\n \
    \             5532, May 2009.\n   [RFC5661]  Shepler, S., Ed., Eisler, M., Ed.,\
    \ and D. Noveck, Ed.,\n              \"Network File System Version 4 Minor Version\
    \ 1 Protocol\",\n              RFC 5661, January 2010.\n   [RFC5667]  Talpey,\
    \ T. and B. Callaghan, \"Network File System (NFS)\n              Direct Data\
    \ Placement\", RFC 5667, January 2010.\n   [IB]       InfiniBand Trade Association,\
    \ InfiniBand Architecture\n              Specifications, available from\n    \
    \          http://www.infinibandta.org.\n   [IBPORT]   InfiniBand Trade Association,\
    \ \"IP Addressing Annex\",\n              available from http://www.infinibandta.org.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Tom Talpey\n   170 Whitman St.\n   Stow, MA 01775 USA\n\
    \   EMail: tmtalpey@gmail.com\n   Brent Callaghan\n   Apple Computer, Inc.\n \
    \  MS: 302-4K\n   2 Infinite Loop\n   Cupertino, CA 95014 USA\n   EMail: brentc@apple.com\n"
