- title: __initial_text__
  contents:
  - "             PNG (Portable Network Graphics) Specification\n                \
    \              Version 1.0\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  This memo\n   does not specify an Internet standard of any kind.  Distribution\
    \ of\n   this memo is unlimited.\n   The IESG takes no position on the validity\
    \ of any Intellectual\n   Property Rights statements contained in this document.\n"
- title: Abstract
  contents:
  - "Abstract\n   This document describes PNG (Portable Network Graphics), an\n  \
    \ extensible file format for the lossless, portable, well-compressed\n   storage\
    \ of raster images.  PNG provides a patent-free replacement for\n   GIF and can\
    \ also replace many common uses of TIFF.  Indexed-color,\n   grayscale, and truecolor\
    \ images are supported, plus an optional alpha\n   channel.  Sample depths range\
    \ from 1 to 16 bits.\n   PNG is designed to work well in online viewing applications,\
    \ such as\n   the World Wide Web, so it is fully streamable with a progressive\n\
    \   display option.  PNG is robust, providing both full file integrity\n   checking\
    \ and simple detection of common transmission errors.  Also,\n   PNG can store\
    \ gamma and chromaticity data for improved color matching\n   on heterogeneous\
    \ platforms.\n   This specification defines the Internet Media Type image/png.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ..................................................\
    \  4\n   2. Data Representation ...........................................  5\n\
    \      2.1. Integers and byte order ..................................  5\n  \
    \    2.2. Color values .............................................  6\n    \
    \  2.3. Image layout .............................................  6\n      2.4.\
    \ Alpha channel ............................................  7\n      2.5. Filtering\
    \ ................................................  8\n      2.6. Interlaced data\
    \ order ....................................  8\n      2.7. Gamma correction .........................................\
    \ 10\n      2.8. Text strings ............................................. 10\n\
    \   3. File Structure ................................................ 11\n  \
    \    3.1. PNG file signature ....................................... 11\n    \
    \  3.2. Chunk layout ............................................. 11\n      3.3.\
    \ Chunk naming conventions ................................. 12\n      3.4. CRC\
    \ algorithm ............................................ 15\n   4. Chunk Specifications\
    \ .......................................... 15\n      4.1. Critical chunks ..........................................\
    \ 15\n          4.1.1. IHDR Image header .................................. 15\n\
    \          4.1.2. PLTE Palette ....................................... 17\n  \
    \        4.1.3. IDAT Image data .................................... 18\n    \
    \      4.1.4. IEND Image trailer ................................. 19\n      4.2.\
    \ Ancillary chunks ......................................... 19\n          4.2.1.\
    \ bKGD Background color .............................. 19\n          4.2.2. cHRM\
    \ Primary chromaticities and white point ........ 20\n          4.2.3. gAMA Image\
    \ gamma ................................... 21\n          4.2.4. hIST Image histogram\
    \ ............................... 21\n          4.2.5. pHYs Physical pixel dimensions\
    \ ..................... 22\n          4.2.6. sBIT Significant bits ..............................\
    \ 22\n          4.2.7. tEXt Textual data .................................. 24\n\
    \          4.2.8. tIME Image last-modification time .................. 25\n  \
    \        4.2.9. tRNS Transparency .................................. 26\n    \
    \      4.2.10. zTXt Compressed textual data ...................... 27\n      4.3.\
    \ Summary of standard chunks ............................... 28\n      4.4. Additional\
    \ chunk types ................................... 29\n   5. Deflate/Inflate Compression\
    \ ................................... 29\n   6. Filter Algorithms .............................................\
    \ 31\n      6.1. Filter types ............................................. 31\n\
    \      6.2. Filter type 0: None ...................................... 32\n  \
    \    6.3. Filter type 1: Sub ....................................... 33\n    \
    \  6.4. Filter type 2: Up ........................................ 33\n      6.5.\
    \ Filter type 3: Average ................................... 34\n      6.6. Filter\
    \ type 4: Paeth...................................... 35\n   7. Chunk Ordering\
    \ Rules .......................................... 36\n      7.1. Behavior of\
    \ PNG editors .................................. 37\n      7.2. Ordering of ancillary\
    \ chunks ............................. 38\n      7.3. Ordering of critical chunks\
    \ .............................. 38\n   8. Miscellaneous Topics ..........................................\
    \ 39\n      8.1. File name extension ...................................... 39\n\
    \      8.2. Internet media type ...................................... 39\n  \
    \    8.3. Macintosh file layout .................................... 39\n    \
    \  8.4. Multiple-image extension ................................. 39\n      8.5.\
    \ Security considerations .................................. 40\n   9. Recommendations\
    \ for Encoders .................................. 41\n      9.1. Sample depth\
    \ scaling ..................................... 41\n      9.2. Encoder gamma handling\
    \ ................................... 42\n      9.3. Encoder color handling ...................................\
    \ 45\n      9.4. Alpha channel creation ................................... 47\n\
    \      9.5. Suggested palettes ....................................... 48\n  \
    \    9.6. Filter selection ......................................... 49\n    \
    \  9.7. Text chunk processing .................................... 49\n      9.8.\
    \ Use of private chunks .................................... 50\n      9.9. Private\
    \ type and method codes ............................ 51\n   10. Recommendations\
    \ for Decoders ................................. 51\n      10.1. Error checking\
    \ .......................................... 52\n      10.2. Pixel dimensions\
    \ ........................................ 52\n      10.3. Truecolor image handling\
    \ ................................ 52\n      10.4. Sample depth rescaling ..................................\
    \ 53\n      10.5. Decoder gamma handling .................................. 54\n\
    \      10.6. Decoder color handling .................................. 56\n  \
    \    10.7. Background color ........................................ 57\n    \
    \  10.8. Alpha channel processing ................................ 58\n      10.9.\
    \ Progressive display ..................................... 62\n      10.10. Suggested-palette\
    \ and histogram usage .................. 63\n      10.11. Text chunk processing\
    \ .................................. 64\n   11. Glossary .....................................................\
    \ 65\n   12. Appendix: Rationale .......................................... 69\n\
    \      12.1. Why a new file format? .................................. 69\n  \
    \    12.2. Why these features? ..................................... 70\n    \
    \  12.3. Why not these features? ................................. 70\n      12.4.\
    \ Why not use format X? ................................... 72\n      12.5. Byte\
    \ order .............................................. 73\n      12.6. Interlacing\
    \ ............................................. 73\n      12.7. Why gamma? ..............................................\
    \ 73\n      12.8. Non-premultiplied alpha ................................. 75\n\
    \      12.9. Filtering ............................................... 75\n  \
    \    12.10. Text strings ........................................... 76\n    \
    \  12.11. PNG file signature ..................................... 77\n      12.12.\
    \ Chunk layout ........................................... 77\n      12.13. Chunk\
    \ naming conventions ............................... 78\n      12.14. Palette\
    \ histograms ..................................... 80\n   13. Appendix: Gamma\
    \ Tutorial ..................................... 81\n   14. Appendix: Color Tutorial\
    \ ..................................... 89\n   15. Appendix: Sample CRC Code ....................................\
    \ 94\n   16. Appendix: Online Resources ................................... 96\n\
    \   17. Appendix: Revision History ................................... 96\n  \
    \ 18. References ................................................... 97\n   19.\
    \ Credits ......................................................100\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   The PNG format provides a portable, legally unencumbered,\
    \ well-\n   compressed, well-specified standard for lossless bitmapped image\n\
    \   files.\n   Although the initial motivation for developing PNG was to replace\n\
    \   GIF, the design provides some useful new features not available in\n   GIF,\
    \ with minimal cost to developers.\n   GIF features retained in PNG include:\n\
    \       * Indexed-color images of up to 256 colors.\n       * Streamability: files\
    \ can be read and written serially, thus\n         allowing the file format to\
    \ be used as a communications\n         protocol for on-the-fly generation and\
    \ display of images.\n       * Progressive display: a suitably prepared image\
    \ file can be\n         displayed as it is received over a communications link,\n\
    \         yielding a low-resolution image very quickly followed by\n         gradual\
    \ improvement of detail.\n       * Transparency: portions of the image can be\
    \ marked as\n         transparent, creating the effect of a non-rectangular image.\n\
    \       * Ancillary information: textual comments and other data can be\n    \
    \     stored within the image file.\n       * Complete hardware and platform independence.\n\
    \       * Effective, 100% lossless compression.\n   Important new features of\
    \ PNG, not available in GIF, include:\n       * Truecolor images of up to 48 bits\
    \ per pixel.\n       * Grayscale images of up to 16 bits per pixel.\n       *\
    \ Full alpha channel (general transparency masks).\n       * Image gamma information,\
    \ which supports automatic display of\n         images with correct brightness/contrast\
    \ regardless of the\n         machines used to originate and display the image.\n\
    \       * Reliable, straightforward detection of file corruption.\n       * Faster\
    \ initial presentation in progressive display mode.\n   PNG is designed to be:\n\
    \       * Simple and portable: developers should be able to implement PNG\n  \
    \       easily.\n       * Legally unencumbered: to the best knowledge of the PNG\
    \ authors,\n         no algorithms under legal challenge are used.  (Some\n  \
    \       considerable effort has been spent to verify this.)\n       * Well compressed:\
    \ both indexed-color and truecolor images are\n         compressed as effectively\
    \ as in any other widely used lossless\n         format, and in most cases more\
    \ effectively.\n       * Interchangeable: any standard-conforming PNG decoder\
    \ must read\n         all conforming PNG files.\n       * Flexible: the format\
    \ allows for future extensions and private\n         add-ons, without compromising\
    \ interchangeability of basic PNG.\n       * Robust: the design supports full\
    \ file integrity checking as\n         well as simple, quick detection of common\
    \ transmission errors.\n   The main part of this specification gives the definition\
    \ of the file\n   format and recommendations for encoder and decoder behavior.\
    \  An\n   appendix gives the rationale for many design decisions.  Although the\n\
    \   rationale is not part of the formal specification, reading it can\n   help\
    \ implementors understand the design.  Cross-references in the\n   main text point\
    \ to relevant parts of the rationale.  Additional\n   appendixes, also not part\
    \ of the formal specification, provide\n   tutorials on gamma and color theory\
    \ as well as other supporting\n   material.\n   In this specification, the word\
    \ \"must\" indicates a mandatory\n   requirement, while \"should\" indicates recommended\
    \ behavior.\n   See Rationale: Why a new file format? (Section 12.1), Why these\n\
    \   features? (Section 12.2), Why not these features? (Section 12.3), Why\n  \
    \ not use format X? (Section 12.4).\n   Pronunciation\n      PNG is pronounced\
    \ \"ping\".\n"
- title: 2. Data Representation
  contents:
  - "2. Data Representation\n   This chapter discusses basic data representations\
    \ used in PNG files,\n   as well as the expected representation of the image data.\n\
    \   2.1. Integers and byte order\n      All integers that require more than one\
    \ byte must be in network\n      byte order: the most significant byte comes first,\
    \ then the less\n      significant bytes in descending order of significance (MSB\
    \ LSB for\n      two-byte integers, B3 B2 B1 B0 for four-byte integers).  The\n\
    \      highest bit (value 128) of a byte is numbered bit 7; the lowest\n     \
    \ bit (value 1) is numbered bit 0. Values are unsigned unless\n      otherwise\
    \ noted. Values explicitly noted as signed are represented\n      in two's complement\
    \ notation.\n      See Rationale: Byte order (Section 12.5).\n   2.2. Color values\n\
    \      Colors can be represented by either grayscale or RGB (red, green,\n   \
    \   blue) sample data.  Grayscale data represents luminance; RGB data\n      represents\
    \ calibrated color information (if the cHRM chunk is\n      present) or uncalibrated\
    \ device-dependent color (if cHRM is\n      absent).  All color values range from\
    \ zero (representing black) to\n      most intense at the maximum value for the\
    \ sample depth.  Note that\n      the maximum value at a given sample depth is\
    \ (2^sampledepth)-1,\n      not 2^sampledepth.\n      Sample values are not necessarily\
    \ linear; the gAMA chunk specifies\n      the gamma characteristic of the source\
    \ device, and viewers are\n      strongly encouraged to compensate properly. \
    \ See Gamma correction\n      (Section 2.7).\n      Source data with a precision\
    \ not directly supported in PNG (for\n      example, 5 bit/sample truecolor) must\
    \ be scaled up to the next\n      higher supported bit depth.  This scaling is\
    \ reversible with no\n      loss of data, and it reduces the number of cases that\
    \ decoders\n      have to cope with.  See Recommendations for Encoders: Sample\
    \ depth\n      scaling (Section 9.1) and Recommendations for Decoders: Sample\n\
    \      depth rescaling (Section 10.4).\n   2.3. Image layout\n      Conceptually,\
    \ a PNG image is a rectangular pixel array, with\n      pixels appearing left-to-right\
    \ within each scanline, and scanlines\n      appearing top-to-bottom.  (For progressive\
    \ display purposes, the\n      data may actually be transmitted in a different\
    \ order; see\n      Interlaced data order, Section 2.6.) The size of each pixel\
    \ is\n      determined by the bit depth, which is the number of bits per\n   \
    \   sample in the image data.\n      Three types of pixel are supported:\n   \
    \       * An indexed-color pixel is represented by a single sample\n         \
    \   that is an index into a supplied palette.  The image bit\n            depth\
    \ determines the maximum number of palette entries, but\n            not the color\
    \ precision within the palette.\n          * A grayscale pixel is represented\
    \ by a single sample that is\n            a grayscale level, where zero is black\
    \ and the largest value\n            for the bit depth is white.\n          *\
    \ A truecolor pixel is represented by three samples: red (zero\n            =\
    \ black, max = red) appears first, then green (zero = black,\n            max\
    \ = green), then blue (zero = black, max = blue).  The bit\n            depth\
    \ specifies the size of each sample, not the total pixel\n            size.\n\
    \      Optionally, grayscale and truecolor pixels can also include an\n      alpha\
    \ sample, as described in the next section.\n      Pixels are always packed into\
    \ scanlines with no wasted bits\n      between pixels.  Pixels smaller than a\
    \ byte never cross byte\n      boundaries; they are packed into bytes with the\
    \ leftmost pixel in\n      the high-order bits of a byte, the rightmost in the\
    \ low-order\n      bits.  Permitted bit depths and pixel types are restricted\
    \ so that\n      in all cases the packing is simple and efficient.\n      PNG\
    \ permits multi-sample pixels only with 8- and 16-bit samples,\n      so multiple\
    \ samples of a single pixel are never packed into one\n      byte.  16-bit samples\
    \ are stored in network byte order (MSB\n      first).\n      Scanlines always\
    \ begin on byte boundaries.  When pixels have fewer\n      than 8 bits and the\
    \ scanline width is not evenly divisible by the\n      number of pixels per byte,\
    \ the low-order bits in the last byte of\n      each scanline are wasted.  The\
    \ contents of these wasted bits are\n      unspecified.\n      An additional \"\
    filter type\" byte is added to the beginning of\n      every scanline (see Filtering,\
    \ Section 2.5).  The filter type byte\n      is not considered part of the image\
    \ data, but it is included in\n      the datastream sent to the compression step.\n\
    \   2.4. Alpha channel\n      An alpha channel, representing transparency information\
    \ on a per-\n      pixel basis, can be included in grayscale and truecolor PNG\n\
    \      images.\n      An alpha value of zero represents full transparency, and\
    \ a value\n      of (2^bitdepth)-1 represents a fully opaque pixel.  Intermediate\n\
    \      values indicate partially transparent pixels that can be combined\n   \
    \   with a background image to yield a composite image.  (Thus, alpha\n      is\
    \ really the degree of opacity of the pixel.  But most people\n      refer to\
    \ alpha as providing transparency information, not opacity\n      information,\
    \ and we continue that custom here.)\n      Alpha channels can be included with\
    \ images that have either 8 or\n      16 bits per sample, but not with images\
    \ that have fewer than 8\n      bits per sample.  Alpha samples are represented\
    \ with the same bit\n      depth used for the image samples.  The alpha sample\
    \ for each pixel\n      is stored immediately following the grayscale or RGB samples\
    \ of\n      the pixel.\n      The color values stored for a pixel are not affected\
    \ by the alpha\n      value assigned to the pixel.  This rule is sometimes called\n\
    \      \"unassociated\" or \"non-premultiplied\" alpha.  (Another common\n   \
    \   technique is to store sample values premultiplied by the alpha\n      fraction;\
    \ in effect, such an image is already composited against a\n      black background.\
    \  PNG does not use premultiplied alpha.)\n      Transparency control is also\
    \ possible without the storage cost of\n      a full alpha channel.  In an indexed-color\
    \ image, an alpha value\n      can be defined for each palette entry.  In grayscale\
    \ and truecolor\n      images, a single pixel value can be identified as being\n\
    \      \"transparent\".  These techniques are controlled by the tRNS\n      ancillary\
    \ chunk type.\n      If no alpha channel nor tRNS chunk is present, all pixels\
    \ in the\n      image are to be treated as fully opaque.\n      Viewers can support\
    \ transparency control partially, or not at all.\n      See Rationale: Non-premultiplied\
    \ alpha (Section 12.8),\n      Recommendations for Encoders: Alpha channel creation\
    \ (Section\n      9.4), and Recommendations for Decoders: Alpha channel processing\n\
    \      (Section 10.8).\n   2.5. Filtering\n      PNG allows the image data to\
    \ be filtered before it is compressed.\n      Filtering can improve the compressibility\
    \ of the data.  The filter\n      step itself does not reduce the size of the\
    \ data.  All PNG filters\n      are strictly lossless.\n      PNG defines several\
    \ different filter algorithms, including \"None\"\n      which indicates no filtering.\
    \  The filter algorithm is specified\n      for each scanline by a filter type\
    \ byte that precedes the filtered\n      scanline in the precompression datastream.\
    \  An intelligent encoder\n      can switch filters from one scanline to the next.\
    \  The method for\n      choosing which filter to employ is up to the encoder.\n\
    \      See Filter Algorithms (Chapter 6) and Rationale: Filtering\n      (Section\
    \ 12.9).\n   2.6. Interlaced data order\n      A PNG image can be stored in interlaced\
    \ order to allow progressive\n      display.  The purpose of this feature is to\
    \ allow images to \"fade\n      in\" when they are being displayed on-the-fly.\
    \  Interlacing\n      slightly expands the file size on average, but it gives\
    \ the user a\n      meaningful display much more rapidly.  Note that decoders\
    \ are\n      required to be able to read interlaced images, whether or not they\n\
    \      actually perform progressive display.\n      With interlace method 0, pixels\
    \ are stored sequentially from left\n      to right, and scanlines sequentially\
    \ from top to bottom (no\n      interlacing).\n      Interlace method 1, known\
    \ as Adam7 after its author, Adam M.\n      Costello, consists of seven distinct\
    \ passes over the image.  Each\n      pass transmits a subset of the pixels in\
    \ the image.  The pass in\n      which each pixel is transmitted is defined by\
    \ replicating the\n      following 8-by-8 pattern over the entire image, starting\
    \ at the\n      upper left corner:\n         1 6 4 6 2 6 4 6\n         7 7 7 7\
    \ 7 7 7 7\n         5 6 5 6 5 6 5 6\n         7 7 7 7 7 7 7 7\n         3 6 4\
    \ 6 3 6 4 6\n         7 7 7 7 7 7 7 7\n         5 6 5 6 5 6 5 6\n         7 7\
    \ 7 7 7 7 7 7\n      Within each pass, the selected pixels are transmitted left\
    \ to\n      right within a scanline, and selected scanlines sequentially from\n\
    \      top to bottom.  For example, pass 2 contains pixels 4, 12, 20,\n      etc.\
    \ of scanlines 0, 8, 16, etc. (numbering from 0,0 at the upper\n      left corner).\
    \  The last pass contains the entirety of scanlines 1,\n      3, 5, etc.\n   \
    \   The data within each pass is laid out as though it were a complete\n     \
    \ image of the appropriate dimensions.  For example, if the complete\n      image\
    \ is 16 by 16 pixels, then pass 3 will contain two scanlines,\n      each containing\
    \ four pixels.  When pixels have fewer than 8 bits,\n      each such scanline\
    \ is padded as needed to fill an integral number\n      of bytes (see Image layout,\
    \ Section 2.3).  Filtering is done on\n      this reduced image in the usual way,\
    \ and a filter type byte is\n      transmitted before each of its scanlines (see\
    \ Filter Algorithms,\n      Chapter 6).  Notice that the transmission order is\
    \ defined so that\n      all the scanlines transmitted in a pass will have the\
    \ same number\n      of pixels; this is necessary for proper application of some\
    \ of the\n      filters.\n      Caution: If the image contains fewer than five\
    \ columns or fewer\n      than five rows, some passes will be entirely empty.\
    \  Encoders and\n      decoders must handle this case correctly.  In particular,\
    \ filter\n      type bytes are only associated with nonempty scanlines; no filter\n\
    \      type bytes are present in an empty pass.\n      See Rationale: Interlacing\
    \ (Section 12.6) and Recommendations for\n      Decoders: Progressive display\
    \ (Section 10.9).\n   2.7. Gamma correction\n      PNG images can specify, via\
    \ the gAMA chunk, the gamma\n      characteristic of the image with respect to\
    \ the original scene.\n      Display programs are strongly encouraged to use this\
    \ information,\n      plus information about the display device they are using\
    \ and room\n      lighting, to present the image to the viewer in a way that\n\
    \      reproduces what the image's original author saw as closely as\n      possible.\
    \  See Gamma Tutorial (Chapter 13) if you aren't already\n      familiar with\
    \ gamma issues.\n      Gamma correction is not applied to the alpha channel, if\
    \ any.\n      Alpha samples always represent a linear fraction of full opacity.\n\
    \      For high-precision applications, the exact chromaticity of the RGB\n  \
    \    data in a PNG image can be specified via the cHRM chunk, allowing\n     \
    \ more accurate color matching than gamma correction alone will\n      provide.\
    \  See Color Tutorial (Chapter 14) if you aren't already\n      familiar with\
    \ color representation issues.\n      See Rationale: Why gamma? (Section 12.7),\
    \ Recommendations for\n      Encoders: Encoder gamma handling (Section 9.2), and\n\
    \      Recommendations for Decoders: Decoder gamma handling (Section\n      10.5).\n\
    \   2.8. Text strings\n      A PNG file can store text associated with the image,\
    \ such as an\n      image description or copyright notice.  Keywords are used\
    \ to\n      indicate what each text string represents.\n      ISO 8859-1 (Latin-1)\
    \ is the character set recommended for use in\n      text strings [ISO-8859].\
    \  This character set is a superset of 7-\n      bit ASCII.\n      Character codes\
    \ not defined in Latin-1 should not be used, because\n      they have no platform-independent\
    \ meaning.  If a non-Latin-1 code\n      does appear in a PNG text string, its\
    \ interpretation will vary\n      across platforms and decoders.  Some systems\
    \ might not even be\n      able to display all the characters in Latin-1, but\
    \ most modern\n      systems can.\n      Provision is also made for the storage\
    \ of compressed text.\n      See Rationale: Text strings (Section 12.10).\n"
- title: 3. File Structure
  contents:
  - "3. File Structure\n   A PNG file consists of a PNG signature followed by a series\
    \ of\n   chunks.  This chapter defines the signature and the basic properties\n\
    \   of chunks.  Individual chunk types are discussed in the next chapter.\n  \
    \ 3.1. PNG file signature\n      The first eight bytes of a PNG file always contain\
    \ the following\n      (decimal) values:\n         137 80 78 71 13 10 26 10\n\
    \      This signature indicates that the remainder of the file contains a\n  \
    \    single PNG image, consisting of a series of chunks beginning with\n     \
    \ an IHDR chunk and ending with an IEND chunk.\n      See Rationale: PNG file\
    \ signature (Section 12.11).\n   3.2. Chunk layout\n      Each chunk consists\
    \ of four parts:\n      Length\n         A 4-byte unsigned integer giving the\
    \ number of bytes in the\n         chunk's data field. The length counts only\
    \ the data field, not\n         itself, the chunk type code, or the CRC.  Zero\
    \ is a valid\n         length.  Although encoders and decoders should treat the\
    \ length\n         as unsigned, its value must not exceed (2^31)-1 bytes.\n  \
    \    Chunk Type\n         A 4-byte chunk type code.  For convenience in description\
    \ and\n         in examining PNG files, type codes are restricted to consist of\n\
    \         uppercase and lowercase ASCII letters (A-Z and a-z, or 65-90\n     \
    \    and 97-122 decimal).  However, encoders and decoders must treat\n       \
    \  the codes as fixed binary values, not character strings.  For\n         example,\
    \ it would not be correct to represent the type code\n         IDAT by the EBCDIC\
    \ equivalents of those letters.  Additional\n         naming conventions for chunk\
    \ types are discussed in the next\n         section.\n      Chunk Data\n     \
    \    The data bytes appropriate to the chunk type, if any.  This\n         field\
    \ can be of zero length.\n      CRC\n         A 4-byte CRC (Cyclic Redundancy\
    \ Check) calculated on the\n         preceding bytes in the chunk, including the\
    \ chunk type code and\n         chunk data fields, but not including the length\
    \ field. The CRC\n         is always present, even for chunks containing no data.\
    \  See CRC\n         algorithm (Section 3.4).\n      The chunk data length can\
    \ be any number of bytes up to the\n      maximum; therefore, implementors cannot\
    \ assume that chunks are\n      aligned on any boundaries larger than bytes.\n\
    \      Chunks can appear in any order, subject to the restrictions placed\n  \
    \    on each chunk type.  (One notable restriction is that IHDR must\n      appear\
    \ first and IEND must appear last; thus the IEND chunk serves\n      as an end-of-file\
    \ marker.)  Multiple chunks of the same type can\n      appear, but only if specifically\
    \ permitted for that type.\n      See Rationale: Chunk layout (Section 12.12).\n\
    \   3.3. Chunk naming conventions\n      Chunk type codes are assigned so that\
    \ a decoder can determine some\n      properties of a chunk even when it does\
    \ not recognize the type\n      code.  These rules are intended to allow safe,\
    \ flexible extension\n      of the PNG format, by allowing a decoder to decide\
    \ what to do when\n      it encounters an unknown chunk.  The naming rules are\
    \ not normally\n      of interest when the decoder does recognize the chunk's\
    \ type.\n      Four bits of the type code, namely bit 5 (value 32) of each byte,\n\
    \      are used to convey chunk properties.  This choice means that a\n      human\
    \ can read off the assigned properties according to whether\n      each letter\
    \ of the type code is uppercase (bit 5 is 0) or\n      lowercase (bit 5 is 1).\
    \  However, decoders should test the\n      properties of an unknown chunk by\
    \ numerically testing the\n      specified bits; testing whether a character is\
    \ uppercase or\n      lowercase is inefficient, and even incorrect if a locale-specific\n\
    \      case definition is used.\n      It is worth noting that the property bits\
    \ are an inherent part of\n      the chunk name, and hence are fixed for any chunk\
    \ type.  Thus,\n      TEXT and Text would be unrelated chunk type codes, not the\
    \ same\n      chunk with different properties.  Decoders must recognize type\n\
    \      codes by a simple four-byte literal comparison; it is incorrect to\n  \
    \    perform case conversion on type codes.\n      The semantics of the property\
    \ bits are:\n      Ancillary bit: bit 5 of first byte\n         0 (uppercase)\
    \ = critical, 1 (lowercase) = ancillary.\n         Chunks that are not strictly\
    \ necessary in order to meaningfully\n         display the contents of the file\
    \ are known as \"ancillary\"\n         chunks.  A decoder encountering an unknown\
    \ chunk in which the\n         ancillary bit is 1 can safely ignore the chunk\
    \ and proceed to\n         display the image. The time chunk (tIME) is an example\
    \ of an\n         ancillary chunk.\n         Chunks that are necessary for successful\
    \ display of the file's\n         contents are called \"critical\" chunks. A decoder\
    \ encountering\n         an unknown chunk in which the ancillary bit is 0 must\
    \ indicate\n         to the user that the image contains information it cannot\n\
    \         safely interpret.  The image header chunk (IHDR) is an example\n   \
    \      of a critical chunk.\n      Private bit: bit 5 of second byte\n       \
    \  0 (uppercase) = public, 1 (lowercase) = private.\n         A public chunk is\
    \ one that is part of the PNG specification or\n         is registered in the\
    \ list of PNG special-purpose public chunk\n         types.  Applications can\
    \ also define private (unregistered)\n         chunks for their own purposes.\
    \  The names of private chunks\n         must have a lowercase second letter,\
    \ while public chunks will\n         always be assigned names with uppercase second\
    \ letters.  Note\n         that decoders do not need to test the private-chunk\
    \ property\n         bit, since it has no functional significance; it is simply\
    \ an\n         administrative convenience to ensure that public and private\n\
    \         chunk names will not conflict.  See Additional chunk types\n       \
    \  (Section 4.4) and Recommendations for Encoders: Use of private\n         chunks\
    \ (Section 9.8).\n      Reserved bit: bit 5 of third byte\n         Must be 0\
    \ (uppercase) in files conforming to this version of\n         PNG.\n        \
    \ The significance of the case of the third letter of the chunk\n         name\
    \ is reserved for possible future expansion.  At the present\n         time all\
    \ chunk names must have uppercase third letters.\n         (Decoders should not\
    \ complain about a lowercase third letter,\n         however, as some future version\
    \ of the PNG specification could\n         define a meaning for this bit.  It\
    \ is sufficient to treat a\n         chunk with a lowercase third letter in the\
    \ same way as any\n         other unknown chunk type.)\n      Safe-to-copy bit:\
    \ bit 5 of fourth byte\n         0 (uppercase) = unsafe to copy, 1 (lowercase)\
    \ = safe to copy.\n         This property bit is not of interest to pure decoders,\
    \ but it\n         is needed by PNG editors (programs that modify PNG files).\n\
    \         This bit defines the proper handling of unrecognized chunks in\n   \
    \      a file that is being modified.\n         If a chunk's safe-to-copy bit\
    \ is 1, the chunk may be copied to\n         a modified PNG file whether or not\
    \ the software recognizes the\n         chunk type, and regardless of the extent\
    \ of the file\n         modifications.\n         If a chunk's safe-to-copy bit\
    \ is 0, it indicates that the chunk\n         depends on the image data.  If the\
    \ program has made any changes\n         to critical chunks, including addition,\
    \ modification, deletion,\n         or reordering of critical chunks, then unrecognized\
    \ unsafe\n         chunks must not be copied to the output PNG file.  (Of course,\n\
    \         if the program does recognize the chunk, it can choose to\n        \
    \ output an appropriately modified version.)\n         A PNG editor is always\
    \ allowed to copy all unrecognized chunks\n         if it has only added, deleted,\
    \ modified, or reordered ancillary\n         chunks.  This implies that it is\
    \ not permissible for ancillary\n         chunks to depend on other ancillary\
    \ chunks.\n         PNG editors that do not recognize a critical chunk must report\n\
    \         an error and refuse to process that PNG file at all. The\n         safe/unsafe\
    \ mechanism is intended for use with ancillary\n         chunks.  The safe-to-copy\
    \ bit will always be 0 for critical\n         chunks.\n         Rules for PNG\
    \ editors are discussed further in Chunk Ordering\n         Rules (Chapter 7).\n\
    \      For example, the hypothetical chunk type name \"bLOb\" has the\n      property\
    \ bits:\n         bLOb  <-- 32 bit chunk type code represented in text form\n\
    \         ||||\n         |||+- Safe-to-copy bit is 1 (lower case letter; bit 5\
    \ is 1)\n         ||+-- Reserved bit is 0     (upper case letter; bit 5 is 0)\n\
    \         |+--- Private bit is 0      (upper case letter; bit 5 is 0)\n      \
    \   +---- Ancillary bit is 1    (lower case letter; bit 5 is 1)\n      Therefore,\
    \ this name represents an ancillary, public, safe-to-copy\n      chunk.\n    \
    \  See Rationale: Chunk naming conventions (Section 12.13).\n   3.4. CRC algorithm\n\
    \      Chunk CRCs are calculated using standard CRC methods with pre and\n   \
    \   post conditioning, as defined by ISO 3309 [ISO-3309] or ITU-T V.42\n     \
    \ [ITU-V42].  The CRC polynomial employed is\n         x^32+x^26+x^23+x^22+x^16+x^12+x^11+x^10+x^8+x^7+x^5+x^4+x^2+x+1\n\
    \      The 32-bit CRC register is initialized to all 1's, and then the\n     \
    \ data from each byte is processed from the least significant bit\n      (1) to\
    \ the most significant bit (128).  After all the data bytes\n      are processed,\
    \ the CRC register is inverted (its ones complement\n      is taken).  This value\
    \ is transmitted (stored in the file) MSB\n      first.  For the purpose of separating\
    \ into bytes and ordering, the\n      least significant bit of the 32-bit CRC\
    \ is defined to be the\n      coefficient of the x^31 term.\n      Practical calculation\
    \ of the CRC always employs a precalculated\n      table to greatly accelerate\
    \ the computation. See Sample CRC Code\n      (Chapter 15).\n"
- title: 4. Chunk Specifications
  contents:
  - "4. Chunk Specifications\n   This chapter defines the standard types of PNG chunks.\n\
    \   4.1. Critical chunks\n      All implementations must understand and successfully\
    \ render the\n      standard critical chunks.  A valid PNG image must contain\
    \ an IHDR\n      chunk, one or more IDAT chunks, and an IEND chunk.\n      4.1.1.\
    \ IHDR Image header\n         The IHDR chunk must appear FIRST.  It contains:\n\
    \            Width:              4 bytes\n            Height:             4 bytes\n\
    \            Bit depth:          1 byte\n            Color type:         1 byte\n\
    \            Compression method: 1 byte\n            Filter method:      1 byte\n\
    \            Interlace method:   1 byte\n         Width and height give the image\
    \ dimensions in pixels.  They are\n         4-byte integers. Zero is an invalid\
    \ value. The maximum for each\n         is (2^31)-1 in order to accommodate languages\
    \ that have\n         difficulty with unsigned 4-byte values.\n         Bit depth\
    \ is a single-byte integer giving the number of bits\n         per sample or per\
    \ palette index (not per pixel).  Valid values\n         are 1, 2, 4, 8, and 16,\
    \ although not all values are allowed for\n         all color types.\n       \
    \  Color type is a single-byte integer that describes the\n         interpretation\
    \ of the image data.  Color type codes represent\n         sums of the following\
    \ values: 1 (palette used), 2 (color used),\n         and 4 (alpha channel used).\
    \ Valid values are 0, 2, 3, 4, and 6.\n         Bit depth restrictions for each\
    \ color type are imposed to\n         simplify implementations and to prohibit\
    \ combinations that do\n         not compress well.  Decoders must support all\
    \ legal\n         combinations of bit depth and color type.  The allowed\n   \
    \      combinations are:\n            Color    Allowed    Interpretation\n   \
    \         Type    Bit Depths\n            0       1,2,4,8,16  Each pixel is a\
    \ grayscale sample.\n            2       8,16        Each pixel is an R,G,B triple.\n\
    \            3       1,2,4,8     Each pixel is a palette index;\n            \
    \                    a PLTE chunk must appear.\n            4       8,16     \
    \   Each pixel is a grayscale sample,\n                                followed\
    \ by an alpha sample.\n            6       8,16        Each pixel is an R,G,B\
    \ triple,\n                                followed by an alpha sample.\n    \
    \     The sample depth is the same as the bit depth except in the\n         case\
    \ of color type 3, in which the sample depth is always 8\n         bits.\n   \
    \      Compression method is a single-byte integer that indicates the\n      \
    \   method used to compress the image data.  At present, only\n         compression\
    \ method 0 (deflate/inflate compression with a 32K\n         sliding window) is\
    \ defined.  All standard PNG images must be\n         compressed with this scheme.\
    \  The compression method field is\n         provided for possible future expansion\
    \ or proprietary variants.\n         Decoders must check this byte and report\
    \ an error if it holds\n         an unrecognized code.  See Deflate/Inflate Compression\
    \ (Chapter\n         5) for details.\n         Filter method is a single-byte\
    \ integer that indicates the\n         preprocessing method applied to the image\
    \ data before\n         compression.  At present, only filter method 0 (adaptive\n\
    \         filtering with five basic filter types) is defined.  As with\n     \
    \    the compression method field, decoders must check this byte and\n       \
    \  report an error if it holds an unrecognized code.  See Filter\n         Algorithms\
    \ (Chapter 6) for details.\n         Interlace method is a single-byte integer\
    \ that indicates the\n         transmission order of the image data.  Two values\
    \ are currently\n         defined: 0 (no interlace) or 1 (Adam7 interlace).  See\n\
    \         Interlaced data order (Section 2.6) for details.\n      4.1.2. PLTE\
    \ Palette\n         The PLTE chunk contains from 1 to 256 palette entries, each\
    \ a\n         three-byte series of the form:\n            Red:   1 byte (0 = black,\
    \ 255 = red)\n            Green: 1 byte (0 = black, 255 = green)\n           \
    \ Blue:  1 byte (0 = black, 255 = blue)\n         The number of entries is determined\
    \ from the chunk length.  A\n         chunk length not divisible by 3 is an error.\n\
    \         This chunk must appear for color type 3, and can appear for\n      \
    \   color types 2 and 6; it must not appear for color types 0 and\n         4.\
    \ If this chunk does appear, it must precede the first IDAT\n         chunk. \
    \ There must not be more than one PLTE chunk.\n         For color type 3 (indexed\
    \ color), the PLTE chunk is required.\n         The first entry in PLTE is referenced\
    \ by pixel value 0, the\n         second by pixel value 1, etc.  The number of\
    \ palette entries\n         must not exceed the range that can be represented\
    \ in the image\n         bit depth (for example, 2^4 = 16 for a bit depth of 4).\
    \  It is\n         permissible to have fewer entries than the bit depth would\n\
    \         allow.  In that case, any out-of-range pixel value found in the\n  \
    \       image data is an error.\n         For color types 2 and 6 (truecolor and\
    \ truecolor with alpha),\n         the PLTE chunk is optional.  If present, it\
    \ provides a\n         suggested set of from 1 to 256 colors to which the truecolor\n\
    \         image can be quantized if the viewer cannot display truecolor\n    \
    \     directly.  If PLTE is not present, such a viewer will need to\n        \
    \ select colors on its own, but it is often preferable for this\n         to be\
    \ done once by the encoder.  (See Recommendations for\n         Encoders: Suggested\
    \ palettes, Section 9.5.)\n         Note that the palette uses 8 bits (1 byte)\
    \ per sample\n         regardless of the image bit depth specification.  In\n\
    \         particular, the palette is 8 bits deep even when it is a\n         suggested\
    \ quantization of a 16-bit truecolor image.\n         There is no requirement\
    \ that the palette entries all be used by\n         the image, nor that they all\
    \ be different.\n      4.1.3. IDAT Image data\n         The IDAT chunk contains\
    \ the actual image data.  To create this\n         data:\n             * Begin\
    \ with image scanlines represented as described in\n               Image layout\
    \ (Section 2.3); the layout and total size of\n               this raw data are\
    \ determined by the fields of IHDR.\n             * Filter the image data according\
    \ to the filtering method\n               specified by the IHDR chunk.  (Note\
    \ that with filter\n               method 0, the only one currently defined, this\
    \ implies\n               prepending a filter type byte to each scanline.)\n \
    \            * Compress the filtered data using the compression method\n     \
    \          specified by the IHDR chunk.\n         The IDAT chunk contains the\
    \ output datastream of the\n         compression algorithm.\n         To read\
    \ the image data, reverse this process.\n         There can be multiple IDAT chunks;\
    \ if so, they must appear\n         consecutively with no other intervening chunks.\
    \  The compressed\n         datastream is then the concatenation of the contents\
    \ of all the\n         IDAT chunks.  The encoder can divide the compressed datastream\n\
    \         into IDAT chunks however it wishes.  (Multiple IDAT chunks are\n   \
    \      allowed so that encoders can work in a fixed amount of memory;\n      \
    \   typically the chunk size will correspond to the encoder's\n         buffer\
    \ size.) It is important to emphasize that IDAT chunk\n         boundaries have\
    \ no semantic significance and can occur at any\n         point in the compressed\
    \ datastream.  A PNG file in which each\n         IDAT chunk contains only one\
    \ data byte is legal, though\n         remarkably wasteful of space.  (For that\
    \ matter, zero-length\n         IDAT chunks are legal, though even more wasteful.)\n\
    \         See Filter Algorithms (Chapter 6) and Deflate/Inflate\n         Compression\
    \ (Chapter 5) for details.\n      4.1.4. IEND Image trailer\n         The IEND\
    \ chunk must appear LAST.  It marks the end of the PNG\n         datastream. \
    \ The chunk's data field is empty.\n   4.2. Ancillary chunks\n      All ancillary\
    \ chunks are optional, in the sense that encoders need\n      not write them and\
    \ decoders can ignore them.  However, encoders\n      are encouraged to write\
    \ the standard ancillary chunks when the\n      information is available, and\
    \ decoders are encouraged to interpret\n      these chunks when appropriate and\
    \ feasible.\n      The standard ancillary chunks are listed in alphabetical order.\n\
    \      This is not necessarily the order in which they would appear in a\n   \
    \   file.\n      4.2.1. bKGD Background color\n         The bKGD chunk specifies\
    \ a default background color to present\n         the image against.  Note that\
    \ viewers are not bound to honor\n         this chunk; a viewer can choose to\
    \ use a different background.\n         For color type 3 (indexed color), the\
    \ bKGD chunk contains:\n            Palette index:  1 byte\n         The value\
    \ is the palette index of the color to be used as\n         background.\n    \
    \     For color types 0 and 4 (grayscale, with or without alpha),\n         bKGD\
    \ contains:\n            Gray:  2 bytes, range 0 .. (2^bitdepth)-1\n         (For\
    \ consistency, 2 bytes are used regardless of the image bit\n         depth.)\
    \  The value is the gray level to be used as background.\n         For color types\
    \ 2 and 6 (truecolor, with or without alpha),\n         bKGD contains:\n     \
    \       Red:   2 bytes, range 0 .. (2^bitdepth)-1\n            Green: 2 bytes,\
    \ range 0 .. (2^bitdepth)-1\n            Blue:  2 bytes, range 0 .. (2^bitdepth)-1\n\
    \         (For consistency, 2 bytes per sample are used regardless of the\n  \
    \       image bit depth.)  This is the RGB color to be used as\n         background.\n\
    \         When present, the bKGD chunk must precede the first IDAT chunk,\n  \
    \       and must follow the PLTE chunk, if any.\n         See Recommendations\
    \ for Decoders: Background color (Section\n         10.7).\n      4.2.2. cHRM\
    \ Primary chromaticities and white point\n         Applications that need device-independent\
    \ specification of\n         colors in a PNG file can use the cHRM chunk to specify\
    \ the 1931\n         CIE x,y chromaticities of the red, green, and blue primaries\n\
    \         used in the image, and the referenced white point. See Color\n     \
    \    Tutorial (Chapter 14) for more information.\n         The cHRM chunk contains:\n\
    \            White Point x: 4 bytes\n            White Point y: 4 bytes\n    \
    \        Red x:         4 bytes\n            Red y:         4 bytes\n        \
    \    Green x:       4 bytes\n            Green y:       4 bytes\n            Blue\
    \ x:        4 bytes\n            Blue y:        4 bytes\n         Each value is\
    \ encoded as a 4-byte unsigned integer,\n         representing the x or y value\
    \ times 100000.  For example, a\n         value of 0.3127 would be stored as the\
    \ integer 31270.\n         cHRM is allowed in all PNG files, although it is of\
    \ little\n         value for grayscale images.\n         If the encoder does not\
    \ know the chromaticity values, it should\n         not write a cHRM chunk; the\
    \ absence of a cHRM chunk indicates\n         that the image's primary colors\
    \ are device-dependent.\n         If the cHRM chunk appears, it must precede the\
    \ first IDAT\n         chunk, and it must also precede the PLTE chunk if present.\n\
    \         See Recommendations for Encoders: Encoder color handling\n         (Section\
    \ 9.3), and Recommendations for Decoders: Decoder color\n         handling (Section\
    \ 10.6).\n      4.2.3. gAMA Image gamma\n         The gAMA chunk specifies the\
    \ gamma of the camera (or simulated\n         camera) that produced the image,\
    \ and thus the gamma of the\n         image with respect to the original scene.\
    \  More precisely, the\n         gAMA chunk encodes the file_gamma value, as defined\
    \ in Gamma\n         Tutorial (Chapter 13).\n         The gAMA chunk contains:\n\
    \            Image gamma: 4 bytes\n         The value is encoded as a 4-byte unsigned\
    \ integer, representing\n         gamma times 100000.  For example, a gamma of\
    \ 0.45 would be\n         stored as the integer 45000.\n         If the encoder\
    \ does not know the image's gamma value, it should\n         not write a gAMA\
    \ chunk; the absence of a gAMA chunk indicates\n         that the gamma is unknown.\n\
    \         If the gAMA chunk appears, it must precede the first IDAT\n        \
    \ chunk, and it must also precede the PLTE chunk if present.\n         See Gamma\
    \ correction (Section 2.7), Recommendations for\n         Encoders: Encoder gamma\
    \ handling (Section 9.2), and\n         Recommendations for Decoders: Decoder\
    \ gamma handling (Section\n         10.5).\n      4.2.4. hIST Image histogram\n\
    \         The hIST chunk gives the approximate usage frequency of each\n     \
    \    color in the color palette.  A histogram chunk can appear only\n        \
    \ when a palette chunk appears.  If a viewer is unable to provide\n         all\
    \ the colors listed in the palette, the histogram may help it\n         decide\
    \ how to choose a subset of the colors for display.\n         The hIST chunk contains\
    \ a series of 2-byte (16 bit) unsigned\n         integers.  There must be exactly\
    \ one entry for each entry in\n         the PLTE chunk.  Each entry is proportional\
    \ to the fraction of\n         pixels in the image that have that palette index;\
    \ the exact\n         scale factor is chosen by the encoder.\n         Histogram\
    \ entries are approximate, with the exception that a\n         zero entry specifies\
    \ that the corresponding palette entry is\n         not used at all in the image.\
    \  It is required that a histogram\n         entry be nonzero if there are any\
    \ pixels of that color.\n         When the palette is a suggested quantization\
    \ of a truecolor\n         image, the histogram is necessarily approximate, since\
    \ a\n         decoder may map pixels to palette entries differently than the\n\
    \         encoder did.  In this situation, zero entries should not\n         appear.\n\
    \         The hIST chunk, if it appears, must follow the PLTE chunk, and\n   \
    \      must precede the first IDAT chunk.\n         See Rationale: Palette histograms\
    \ (Section 12.14), and\n         Recommendations for Decoders: Suggested-palette\
    \ and histogram\n         usage (Section 10.10).\n      4.2.5. pHYs Physical pixel\
    \ dimensions\n         The pHYs chunk specifies the intended pixel size or aspect\n\
    \         ratio for display of the image.  It contains:\n            Pixels per\
    \ unit, X axis: 4 bytes (unsigned integer)\n            Pixels per unit, Y axis:\
    \ 4 bytes (unsigned integer)\n            Unit specifier:          1 byte\n  \
    \       The following values are legal for the unit specifier:\n            0:\
    \ unit is unknown\n            1: unit is the meter\n         When the unit specifier\
    \ is 0, the pHYs chunk defines pixel\n         aspect ratio only; the actual size\
    \ of the pixels remains\n         unspecified.\n         Conversion note: one\
    \ inch is equal to exactly 0.0254 meters.\n         If this ancillary chunk is\
    \ not present, pixels are assumed to\n         be square, and the physical size\
    \ of each pixel is unknown.\n         If present, this chunk must precede the\
    \ first IDAT chunk.\n         See Recommendations for Decoders: Pixel dimensions\
    \ (Section\n         10.2).\n      4.2.6. sBIT Significant bits\n         To simplify\
    \ decoders, PNG specifies that only certain sample\n         depths can be used,\
    \ and further specifies that sample values\n         should be scaled to the full\
    \ range of possible values at the\n         sample depth.  However, the sBIT chunk\
    \ is provided in order to\n         store the original number of significant bits.\
    \  This allows\n         decoders to recover the original data losslessly even\
    \ if the\n         data had a sample depth not directly supported by PNG.  We\n\
    \         recommend that an encoder emit an sBIT chunk if it has\n         converted\
    \ the data from a lower sample depth.\n         For color type 0 (grayscale),\
    \ the sBIT chunk contains a single\n         byte, indicating the number of bits\
    \ that were significant in\n         the source data.\n         For color type\
    \ 2 (truecolor), the sBIT chunk contains three\n         bytes, indicating the\
    \ number of bits that were significant in\n         the source data for the red,\
    \ green, and blue channels,\n         respectively.\n         For color type 3\
    \ (indexed color), the sBIT chunk contains three\n         bytes, indicating the\
    \ number of bits that were significant in\n         the source data for the red,\
    \ green, and blue components of the\n         palette entries, respectively.\n\
    \         For color type 4 (grayscale with alpha channel), the sBIT chunk\n  \
    \       contains two bytes, indicating the number of bits that were\n        \
    \ significant in the source grayscale data and the source alpha\n         data,\
    \ respectively.\n         For color type 6 (truecolor with alpha channel), the\
    \ sBIT chunk\n         contains four bytes, indicating the number of bits that\
    \ were\n         significant in the source data for the red, green, blue and\n\
    \         alpha channels, respectively.\n         Each depth specified in sBIT\
    \ must be greater than zero and less\n         than or equal to the sample depth\
    \ (which is 8 for indexed-color\n         images, and the bit depth given in IHDR\
    \ for other color types).\n         A decoder need not pay attention to sBIT:\
    \ the stored image is a\n         valid PNG file of the sample depth indicated\
    \ by IHDR.  However,\n         if the decoder wishes to recover the original data\
    \ at its\n         original precision, this can be done by right-shifting the\n\
    \         stored samples (the stored palette entries, for an indexed-\n      \
    \   color image).  The encoder must scale the data in such a way\n         that\
    \ the high-order bits match the original data.\n         If the sBIT chunk appears,\
    \ it must precede the first IDAT\n         chunk, and it must also precede the\
    \ PLTE chunk if present.\n         See Recommendations for Encoders: Sample depth\
    \ scaling (Section\n         9.1) and Recommendations for Decoders: Sample depth\
    \ rescaling\n         (Section 10.4).\n      4.2.7. tEXt Textual data\n      \
    \   Textual information that the encoder wishes to record with the\n         image\
    \ can be stored in tEXt chunks.  Each tEXt chunk contains a\n         keyword\
    \ and a text string, in the format:\n            Keyword:        1-79 bytes (character\
    \ string)\n            Null separator: 1 byte\n            Text:           n bytes\
    \ (character string)\n         The keyword and text string are separated by a\
    \ zero byte (null\n         character).  Neither the keyword nor the text string\
    \ can\n         contain a null character.  Note that the text string is not\n\
    \         null-terminated (the length of the chunk is sufficient\n         information\
    \ to locate the ending).  The keyword must be at\n         least one character\
    \ and less than 80 characters long.  The text\n         string can be of any length\
    \ from zero bytes up to the maximum\n         permissible chunk size less the\
    \ length of the keyword and\n         separator.\n         Any number of tEXt\
    \ chunks can appear, and more than one with\n         the same keyword is permissible.\n\
    \         The keyword indicates the type of information represented by\n     \
    \    the text string.  The following keywords are predefined and\n         should\
    \ be used where appropriate:\n            Title            Short (one line) title\
    \ or caption for image\n            Author           Name of image's creator\n\
    \            Description      Description of image (possibly long)\n         \
    \   Copyright        Copyright notice\n            Creation Time    Time of original\
    \ image creation\n            Software         Software used to create the image\n\
    \            Disclaimer       Legal disclaimer\n            Warning          Warning\
    \ of nature of content\n            Source           Device used to create the\
    \ image\n            Comment          Miscellaneous comment; conversion from\n\
    \                             GIF comment\n         For the Creation Time keyword,\
    \ the date format defined in\n         section 5.2.14 of RFC 1123 is suggested,\
    \ but not required\n         [RFC-1123].  Decoders should allow for free-format\
    \ text\n         associated with this or any other keyword.\n         Other keywords\
    \ may be invented for other purposes.  Keywords of\n         general interest\
    \ can be registered with the maintainers of the\n         PNG specification. \
    \ However, it is also permitted to use\n         private unregistered keywords.\
    \  (Private keywords should be\n         reasonably self-explanatory, in order\
    \ to minimize the chance\n         that the same keyword will be used for incompatible\
    \ purposes by\n         different people.)\n         Both keyword and text are\
    \ interpreted according to the ISO\n         8859-1 (Latin-1) character set [ISO-8859].\
    \  The text string can\n         contain any Latin-1 character.  Newlines in the\
    \ text string\n         should be represented by a single linefeed character (decimal\n\
    \         10); use of other control characters in the text is\n         discouraged.\n\
    \         Keywords must contain only printable Latin-1 characters and\n      \
    \   spaces; that is, only character codes 32-126 and 161-255\n         decimal\
    \ are allowed.  To reduce the chances for human\n         misreading of a keyword,\
    \ leading and trailing spaces are\n         forbidden, as are consecutive spaces.\
    \  Note also that the non-\n         breaking space (code 160) is not permitted\
    \ in keywords, since\n         it is visually indistinguishable from an ordinary\
    \ space.\n         Keywords must be spelled exactly as registered, so that\n \
    \        decoders can use simple literal comparisons when looking for\n      \
    \   particular keywords.  In particular, keywords are considered\n         case-sensitive.\n\
    \         See Recommendations for Encoders: Text chunk processing\n         (Section\
    \ 9.7) and Recommendations for Decoders: Text chunk\n         processing (Section\
    \ 10.11).\n      4.2.8. tIME Image last-modification time\n         The tIME chunk\
    \ gives the time of the last image modification\n         (not the time of initial\
    \ image creation).  It contains:\n            Year:   2 bytes (complete; for example,\
    \ 1995, not 95)\n            Month:  1 byte (1-12)\n            Day:    1 byte\
    \ (1-31)\n            Hour:   1 byte (0-23)\n            Minute: 1 byte (0-59)\n\
    \            Second: 1 byte (0-60)    (yes, 60, for leap seconds; not 61,\n  \
    \                                    a common error)\n         Universal Time\
    \ (UTC, also called GMT) should be specified\n         rather than local time.\n\
    \         The tIME chunk is intended for use as an automatically-applied\n   \
    \      time stamp that is updated whenever the image data is changed.\n      \
    \   It is recommended that tIME not be changed by PNG editors that\n         do\
    \ not change the image data.  See also the Creation Time tEXt\n         keyword,\
    \ which can be used for a user-supplied time.\n      4.2.9. tRNS Transparency\n\
    \         The tRNS chunk specifies that the image uses simple\n         transparency:\
    \ either alpha values associated with palette\n         entries (for indexed-color\
    \ images) or a single transparent\n         color (for grayscale and truecolor\
    \ images).  Although simple\n         transparency is not as elegant as the full\
    \ alpha channel, it\n         requires less storage space and is sufficient for\
    \ many common\n         cases.\n         For color type 3 (indexed color), the\
    \ tRNS chunk contains a\n         series of one-byte alpha values, corresponding\
    \ to entries in\n         the PLTE chunk:\n            Alpha for palette index\
    \ 0:  1 byte\n            Alpha for palette index 1:  1 byte\n            ...\
    \ etc ...\n         Each entry indicates that pixels of the corresponding palette\n\
    \         index must be treated as having the specified alpha value.\n       \
    \  Alpha values have the same interpretation as in an 8-bit full\n         alpha\
    \ channel: 0 is fully transparent, 255 is fully opaque,\n         regardless of\
    \ image bit depth. The tRNS chunk must not contain\n         more alpha values\
    \ than there are palette entries, but tRNS can\n         contain fewer values\
    \ than there are palette entries.  In this\n         case, the alpha value for\
    \ all remaining palette entries is\n         assumed to be 255.  In the common\
    \ case in which only palette\n         index 0 need be made transparent, only\
    \ a one-byte tRNS chunk is\n         needed.\n         For color type 0 (grayscale),\
    \ the tRNS chunk contains a single\n         gray level value, stored in the format:\n\
    \            Gray:  2 bytes, range 0 .. (2^bitdepth)-1\n         (For consistency,\
    \ 2 bytes are used regardless of the image bit\n         depth.) Pixels of the\
    \ specified gray level are to be treated as\n         transparent (equivalent\
    \ to alpha value 0); all other pixels are\n         to be treated as fully opaque\
    \ (alpha value (2^bitdepth)-1).\n         For color type 2 (truecolor), the tRNS\
    \ chunk contains a single\n         RGB color value, stored in the format:\n \
    \           Red:   2 bytes, range 0 .. (2^bitdepth)-1\n            Green: 2 bytes,\
    \ range 0 .. (2^bitdepth)-1\n            Blue:  2 bytes, range 0 .. (2^bitdepth)-1\n\
    \         (For consistency, 2 bytes per sample are used regardless of the\n  \
    \       image bit depth.) Pixels of the specified color value are to be\n    \
    \     treated as transparent (equivalent to alpha value 0); all other\n      \
    \   pixels are to be treated as fully opaque (alpha value\n         (2^bitdepth)-1).\n\
    \         tRNS is prohibited for color types 4 and 6, since a full alpha\n   \
    \      channel is already present in those cases.\n         Note: when dealing\
    \ with 16-bit grayscale or truecolor data, it\n         is important to compare\
    \ both bytes of the sample values to\n         determine whether a pixel is transparent.\
    \  Although decoders\n         may drop the low-order byte of the samples for\
    \ display, this\n         must not occur until after the data has been tested\
    \ for\n         transparency.  For example, if the grayscale level 0x0001 is\n\
    \         specified to be transparent, it would be incorrect to compare\n    \
    \     only the high-order byte and decide that 0x0002 is also\n         transparent.\n\
    \         When present, the tRNS chunk must precede the first IDAT chunk,\n  \
    \       and must follow the PLTE chunk, if any.\n      4.2.10. zTXt Compressed\
    \ textual data\n         The zTXt chunk contains textual data, just as tEXt does;\n\
    \         however, zTXt takes advantage of compression.  zTXt and tEXt\n     \
    \    chunks are semantically equivalent, but zTXt is recommended for\n       \
    \  storing large blocks of text.\n         A zTXt chunk contains:\n          \
    \  Keyword:            1-79 bytes (character string)\n            Null separator:\
    \     1 byte\n            Compression method: 1 byte\n            Compressed text:\
    \    n bytes\n         The keyword and null separator are exactly the same as\
    \ in the\n         tEXt chunk.  Note that the keyword is not compressed.  The\n\
    \         compression method byte identifies the compression method used\n   \
    \      in this zTXt chunk.  The only value presently defined for it is\n     \
    \    0 (deflate/inflate compression). The compression method byte is\n       \
    \  followed by a compressed datastream that makes up the remainder\n         of\
    \ the chunk.  For compression method 0, this datastream\n         adheres to the\
    \ zlib datastream format (see Deflate/Inflate\n         Compression, Chapter 5).\
    \  Decompression of this datastream\n         yields Latin-1 text that is identical\
    \ to the text that would be\n         stored in an equivalent tEXt chunk.\n  \
    \       Any number of zTXt and tEXt chunks can appear in the same file.\n    \
    \     See the preceding definition of the tEXt chunk for the\n         predefined\
    \ keywords and the recommended format of the text.\n         See Recommendations\
    \ for Encoders: Text chunk processing\n         (Section 9.7), and Recommendations\
    \ for Decoders: Text chunk\n         processing (Section 10.11).\n   4.3. Summary\
    \ of standard chunks\n      This table summarizes some properties of the standard\
    \ chunk types.\n         Critical chunks (must appear in this order, except PLTE\n\
    \                          is optional):\n                 Name  Multiple  Ordering\
    \ constraints\n                         OK?\n                 IHDR    No     \
    \ Must be first\n                 PLTE    No      Before IDAT\n              \
    \   IDAT    Yes     Multiple IDATs must be consecutive\n                 IEND\
    \    No      Must be last\n         Ancillary chunks (need not appear in this\
    \ order):\n                 Name  Multiple  Ordering constraints\n           \
    \              OK?\n                 cHRM    No      Before PLTE and IDAT\n  \
    \               gAMA    No      Before PLTE and IDAT\n                 sBIT  \
    \  No      Before PLTE and IDAT\n                 bKGD    No      After PLTE;\
    \ before IDAT\n                 hIST    No      After PLTE; before IDAT\n    \
    \             tRNS    No      After PLTE; before IDAT\n                 pHYs \
    \   No      Before IDAT\n                 tIME    No      None\n             \
    \    tEXt    Yes     None\n                 zTXt    Yes     None\n      Standard\
    \ keywords for tEXt and zTXt chunks:\n         Title            Short (one line)\
    \ title or caption for image\n         Author           Name of image's creator\n\
    \         Description      Description of image (possibly long)\n         Copyright\
    \        Copyright notice\n         Creation Time    Time of original image creation\n\
    \         Software         Software used to create the image\n         Disclaimer\
    \       Legal disclaimer\n         Warning          Warning of nature of content\n\
    \         Source           Device used to create the image\n         Comment \
    \         Miscellaneous comment; conversion from\n                          GIF\
    \ comment\n   4.4. Additional chunk types\n      Additional public PNG chunk types\
    \ are defined in the document \"PNG\n      Special-Purpose Public Chunks\" [PNG-EXTENSIONS].\
    \  Chunks described\n      there are expected to be less widely supported than\
    \ those defined\n      in this specification.  However, application authors are\n\
    \      encouraged to use those chunk types whenever appropriate for their\n  \
    \    applications.  Additional chunk types can be proposed for\n      inclusion\
    \ in that list by contacting the PNG specification\n      maintainers at png-info@uunet.uu.net\
    \ or at png-group@w3.org.\n      New public chunks will only be registered if\
    \ they are of use to\n      others and do not violate the design philosophy of\
    \ PNG. Chunk\n      registration is not automatic, although it is the intent of\
    \ the\n      authors that it be straightforward when a new chunk of potentially\n\
    \      wide application is needed.  Note that the creation of new\n      critical\
    \ chunk types is discouraged unless absolutely necessary.\n      Applications\
    \ can also use private chunk types to carry data that\n      is not of interest\
    \ to other applications.  See Recommendations for\n      Encoders: Use of private\
    \ chunks (Section 9.8).\n      Decoders must be prepared to encounter unrecognized\
    \ public or\n      private chunk type codes.  Unrecognized chunk types must be\n\
    \      handled as described in Chunk naming conventions (Section 3.3).\n"
- title: 5. Deflate/Inflate Compression
  contents:
  - "5. Deflate/Inflate Compression\n   PNG compression method 0 (the only compression\
    \ method presently\n   defined for PNG) specifies deflate/inflate compression\
    \ with a 32K\n   sliding window.  Deflate compression is an LZ77 derivative used\
    \ in\n   zip, gzip, pkzip and related programs.  Extensive research has been\n\
    \   done supporting its patent-free status.  Portable C implementations\n   are\
    \ freely available.\n   Deflate-compressed datastreams within PNG are stored in\
    \ the \"zlib\"\n   format, which has the structure:\n      Compression method/flags\
    \ code: 1 byte\n      Additional flags/check bits:   1 byte\n      Compressed\
    \ data blocks:        n bytes\n      Check value:                   4 bytes\n\
    \   Further details on this format are given in the zlib specification\n   [RFC-1950].\n\
    \   For PNG compression method 0, the zlib compression method/flags code\n   must\
    \ specify method code 8 (\"deflate\" compression) and an LZ77 window\n   size\
    \ of not more than 32K.  Note that the zlib compression method\n   number is not\
    \ the same as the PNG compression method number.  The\n   additional flags must\
    \ not specify a preset dictionary.\n   The compressed data within the zlib datastream\
    \ is stored as a series\n   of blocks, each of which can represent raw (uncompressed)\
    \ data,\n   LZ77-compressed data encoded with fixed Huffman codes, or LZ77-\n\
    \   compressed data encoded with custom Huffman codes.  A marker bit in\n   the\
    \ final block identifies it as the last block, allowing the decoder\n   to recognize\
    \ the end of the compressed datastream.  Further details\n   on the compression\
    \ algorithm and the encoding are given in the\n   deflate specification [RFC-1951].\n\
    \   The check value stored at the end of the zlib datastream is\n   calculated\
    \ on the uncompressed data represented by the datastream.\n   Note that the algorithm\
    \ used is not the same as the CRC calculation\n   used for PNG chunk check values.\
    \  The zlib check value is useful\n   mainly as a cross-check that the deflate\
    \ and inflate algorithms are\n   implemented correctly.  Verifying the chunk CRCs\
    \ provides adequate\n   confidence that the PNG file has been transmitted undamaged.\n\
    \   In a PNG file, the concatenation of the contents of all the IDAT\n   chunks\
    \ makes up a zlib datastream as specified above.  This\n   datastream decompresses\
    \ to filtered image data as described elsewhere\n   in this document.\n   It is\
    \ important to emphasize that the boundaries between IDAT chunks\n   are arbitrary\
    \ and can fall anywhere in the zlib datastream.  There is\n   not necessarily\
    \ any correlation between IDAT chunk boundaries and\n   deflate block boundaries\
    \ or any other feature of the zlib data.  For\n   example, it is entirely possible\
    \ for the terminating zlib check value\n   to be split across IDAT chunks.\n \
    \  In the same vein, there is no required correlation between the\n   structure\
    \ of the image data (i.e., scanline boundaries) and deflate\n   block boundaries\
    \ or IDAT chunk boundaries.  The complete image data\n   is represented by a single\
    \ zlib datastream that is stored in some\n   number of IDAT chunks; a decoder\
    \ that assumes any more than this is\n   incorrect.  (Of course, some encoder\
    \ implementations may emit files\n   in which some of these structures are indeed\
    \ related.  But decoders\n   cannot rely on this.)\n   PNG also uses zlib datastreams\
    \ in zTXt chunks.  In a zTXt chunk, the\n   remainder of the chunk following the\
    \ compression method byte is a\n   zlib datastream as specified above.  This datastream\
    \ decompresses to\n   the user-readable text described by the chunk's keyword.\
    \  Unlike the\n   image data, such datastreams are not split across chunks; each\
    \ zTXt\n   chunk contains an independent zlib datastream.\n   Additional documentation\
    \ and portable C code for deflate and inflate\n   are available from the Info-ZIP\
    \ archives at\n   <URL:ftp://ftp.uu.net/pub/archiving/zip/>.\n"
- title: 6. Filter Algorithms
  contents:
  - "6. Filter Algorithms\n   This chapter describes the filter algorithms that can\
    \ be applied\n   before compression.  The purpose of these filters is to prepare\
    \ the\n   image data for optimum compression.\n   6.1. Filter types\n      PNG\
    \ filter method 0 defines five basic filter types:\n         Type    Name\n  \
    \       0       None\n         1       Sub\n         2       Up\n         3  \
    \     Average\n         4       Paeth\n      (Note that filter method 0 in IHDR\
    \ specifies exactly this set of\n      five filter types.  If the set of filter\
    \ types is ever extended, a\n      different filter method number will be assigned\
    \ to the extended\n      set, so that decoders need not decompress the data to\
    \ discover\n      that it contains unsupported filter types.)\n      The encoder\
    \ can choose which of these filter algorithms to apply\n      on a scanline-by-scanline\
    \ basis.  In the image data sent to the\n      compression step, each scanline\
    \ is preceded by a filter type byte\n      that specifies the filter algorithm\
    \ used for that scanline.\n      Filtering algorithms are applied to bytes, not\
    \ to pixels,\n      regardless of the bit depth or color type of the image.  The\n\
    \      filtering algorithms work on the byte sequence formed by a\n      scanline\
    \ that has been represented as described in Image layout\n      (Section 2.3).\
    \  If the image includes an alpha channel, the alpha\n      data is filtered in\
    \ the same way as the image data.\n      When the image is interlaced, each pass\
    \ of the interlace pattern\n      is treated as an independent image for filtering\
    \ purposes.  The\n      filters work on the byte sequences formed by the pixels\
    \ actually\n      transmitted during a pass, and the \"previous scanline\" is\
    \ the one\n      previously transmitted in the same pass, not the one adjacent\
    \ in\n      the complete image.  Note that the subimage transmitted in any one\n\
    \      pass is always rectangular, but is of smaller width and/or height\n   \
    \   than the complete image.  Filtering is not applied when this\n      subimage\
    \ is empty.\n      For all filters, the bytes \"to the left of\" the first pixel\
    \ in a\n      scanline must be treated as being zero.  For filters that refer\
    \ to\n      the prior scanline, the entire prior scanline must be treated as\n\
    \      being zeroes for the first scanline of an image (or of a pass of\n    \
    \  an interlaced image).\n      To reverse the effect of a filter, the decoder\
    \ must use the\n      decoded values of the prior pixel on the same line, the\
    \ pixel\n      immediately above the current pixel on the prior line, and the\n\
    \      pixel just to the left of the pixel above.  This implies that at\n    \
    \  least one scanline's worth of image data will have to be stored by\n      the\
    \ decoder at all times.  Even though some filter types do not\n      refer to\
    \ the prior scanline, the decoder will always need to store\n      each scanline\
    \ as it is decoded, since the next scanline might use\n      a filter that refers\
    \ to it.\n      PNG imposes no restriction on which filter types can be applied\
    \ to\n      an image.  However, the filters are not equally effective on all\n\
    \      types of data.  See Recommendations for Encoders: Filter selection\n  \
    \    (Section 9.6).\n      See also Rationale: Filtering (Section 12.9).\n   6.2.\
    \ Filter type 0: None\n      With the None filter, the scanline is transmitted\
    \ unmodified; it\n      is only necessary to insert a filter type byte before\
    \ the data.\n   6.3. Filter type 1: Sub\n      The Sub filter transmits the difference\
    \ between each byte and the\n      value of the corresponding byte of the prior\
    \ pixel.\n      To compute the Sub filter, apply the following formula to each\n\
    \      byte of the scanline:\n         Sub(x) = Raw(x) - Raw(x-bpp)\n      where\
    \ x ranges from zero to the number of bytes representing the\n      scanline minus\
    \ one, Raw(x) refers to the raw data byte at that\n      byte position in the\
    \ scanline, and bpp is defined as the number of\n      bytes per complete pixel,\
    \ rounding up to one. For example, for\n      color type 2 with a bit depth of\
    \ 16, bpp is equal to 6 (three\n      samples, two bytes per sample); for color\
    \ type 0 with a bit depth\n      of 2, bpp is equal to 1 (rounding up); for color\
    \ type 4 with a bit\n      depth of 16, bpp is equal to 4 (two-byte grayscale\
    \ sample, plus\n      two-byte alpha sample).\n      Note this computation is\
    \ done for each byte, regardless of bit\n      depth.  In a 16-bit image, each\
    \ MSB is predicted from the\n      preceding MSB and each LSB from the preceding\
    \ LSB, because of the\n      way that bpp is defined.\n      Unsigned arithmetic\
    \ modulo 256 is used, so that both the inputs\n      and outputs fit into bytes.\
    \  The sequence of Sub values is\n      transmitted as the filtered scanline.\n\
    \      For all x < 0, assume Raw(x) = 0.\n      To reverse the effect of the Sub\
    \ filter after decompression,\n      output the following value:\n         Sub(x)\
    \ + Raw(x-bpp)\n      (computed mod 256), where Raw refers to the bytes already\
    \ decoded.\n   6.4. Filter type 2: Up\n      The Up filter is just like the Sub\
    \ filter except that the pixel\n      immediately above the current pixel, rather\
    \ than just to its left,\n      is used as the predictor.\n      To compute the\
    \ Up filter, apply the following formula to each byte\n      of the scanline:\n\
    \         Up(x) = Raw(x) - Prior(x)\n      where x ranges from zero to the number\
    \ of bytes representing the\n      scanline minus one, Raw(x) refers to the raw\
    \ data byte at that\n      byte position in the scanline, and Prior(x) refers\
    \ to the\n      unfiltered bytes of the prior scanline.\n      Note this is done\
    \ for each byte, regardless of bit depth.\n      Unsigned arithmetic modulo 256\
    \ is used, so that both the inputs\n      and outputs fit into bytes.  The sequence\
    \ of Up values is\n      transmitted as the filtered scanline.\n      On the first\
    \ scanline of an image (or of a pass of an interlaced\n      image), assume Prior(x)\
    \ = 0 for all x.\n      To reverse the effect of the Up filter after decompression,\
    \ output\n      the following value:\n         Up(x) + Prior(x)\n      (computed\
    \ mod 256), where Prior refers to the decoded bytes of the\n      prior scanline.\n\
    \   6.5. Filter type 3: Average\n      The Average filter uses the average of\
    \ the two neighboring pixels\n      (left and above) to predict the value of a\
    \ pixel.\n      To compute the Average filter, apply the following formula to\
    \ each\n      byte of the scanline:\n         Average(x) = Raw(x) - floor((Raw(x-bpp)+Prior(x))/2)\n\
    \      where x ranges from zero to the number of bytes representing the\n    \
    \  scanline minus one, Raw(x) refers to the raw data byte at that\n      byte\
    \ position in the scanline, Prior(x) refers to the unfiltered\n      bytes of\
    \ the prior scanline, and bpp is defined as for the Sub\n      filter.\n     \
    \ Note this is done for each byte, regardless of bit depth.  The\n      sequence\
    \ of Average values is transmitted as the filtered\n      scanline.\n      The\
    \ subtraction of the predicted value from the raw byte must be\n      done modulo\
    \ 256, so that both the inputs and outputs fit into\n      bytes.  However, the\
    \ sum Raw(x-bpp)+Prior(x) must be formed\n      without overflow (using at least\
    \ nine-bit arithmetic).  floor()\n      indicates that the result of the division\
    \ is rounded to the next\n      lower integer if fractional; in other words, it\
    \ is an integer\n      division or right shift operation.\n      For all x < 0,\
    \ assume Raw(x) = 0.  On the first scanline of an\n      image (or of a pass of\
    \ an interlaced image), assume Prior(x) = 0\n      for all x.\n      To reverse\
    \ the effect of the Average filter after decompression,\n      output the following\
    \ value:\n         Average(x) + floor((Raw(x-bpp)+Prior(x))/2)\n      where the\
    \ result is computed mod 256, but the prediction is\n      calculated in the same\
    \ way as for encoding.  Raw refers to the\n      bytes already decoded, and Prior\
    \ refers to the decoded bytes of\n      the prior scanline.\n   6.6. Filter type\
    \ 4: Paeth\n      The Paeth filter computes a simple linear function of the three\n\
    \      neighboring pixels (left, above, upper left), then chooses as\n      predictor\
    \ the neighboring pixel closest to the computed value.\n      This technique is\
    \ due to Alan W. Paeth [PAETH].\n      To compute the Paeth filter, apply the\
    \ following formula to each\n      byte of the scanline:\n         Paeth(x) =\
    \ Raw(x) - PaethPredictor(Raw(x-bpp), Prior(x),\n                            \
    \                Prior(x-bpp))\n      where x ranges from zero to the number of\
    \ bytes representing the\n      scanline minus one, Raw(x) refers to the raw data\
    \ byte at that\n      byte position in the scanline, Prior(x) refers to the unfiltered\n\
    \      bytes of the prior scanline, and bpp is defined as for the Sub\n      filter.\n\
    \      Note this is done for each byte, regardless of bit depth.\n      Unsigned\
    \ arithmetic modulo 256 is used, so that both the inputs\n      and outputs fit\
    \ into bytes.  The sequence of Paeth values is\n      transmitted as the filtered\
    \ scanline.\n      The PaethPredictor function is defined by the following\n \
    \     pseudocode:\n         function PaethPredictor (a, b, c)\n         begin\n\
    \              ; a = left, b = above, c = upper left\n              p := a + b\
    \ - c        ; initial estimate\n              pa := abs(p - a)      ; distances\
    \ to a, b, c\n              pb := abs(p - b)\n              pc := abs(p - c)\n\
    \              ; return nearest of a,b,c,\n              ; breaking ties in order\
    \ a,b,c.\n              if pa <= pb AND pa <= pc then return a\n             \
    \ else if pb <= pc then return b\n              else return c\n         end\n\
    \      The calculations within the PaethPredictor function must be\n      performed\
    \ exactly, without overflow.  Arithmetic modulo 256 is to\n      be used only\
    \ for the final step of subtracting the function result\n      from the target\
    \ byte value.\n      Note that the order in which ties are broken is critical\
    \ and must\n      not be altered.  The tie break order is: pixel to the left,\
    \ pixel\n      above, pixel to the upper left.  (This order differs from that\n\
    \      given in Paeth's article.)\n      For all x < 0, assume Raw(x) = 0 and\
    \ Prior(x) = 0.  On the first\n      scanline of an image (or of a pass of an\
    \ interlaced image), assume\n      Prior(x) = 0 for all x.\n      To reverse the\
    \ effect of the Paeth filter after decompression,\n      output the following\
    \ value:\n         Paeth(x) + PaethPredictor(Raw(x-bpp), Prior(x), Prior(x-bpp))\n\
    \      (computed mod 256), where Raw and Prior refer to bytes already\n      decoded.\
    \  Exactly the same PaethPredictor function is used by both\n      encoder and\
    \ decoder.\n"
- title: 7. Chunk Ordering Rules
  contents:
  - "7. Chunk Ordering Rules\n   To allow new chunk types to be added to PNG, it is\
    \ necessary to\n   establish rules about the ordering requirements for all chunk\
    \ types.\n   Otherwise a PNG editing program cannot know what to do when it\n\
    \   encounters an unknown chunk.\n   We define a \"PNG editor\" as a program that\
    \ modifies a PNG file and\n   wishes to preserve as much as possible of the ancillary\
    \ information\n   in the file.  Two examples of PNG editors are a program that\
    \ adds or\n   modifies text chunks, and a program that adds a suggested palette\
    \ to\n   a truecolor PNG file.  Ordinary image editors are not PNG editors in\n\
    \   this sense, because they usually discard all unrecognized information\n  \
    \ while reading in an image.  (Note: we strongly encourage programs\n   handling\
    \ PNG files to preserve ancillary information whenever\n   possible.)\n   As an\
    \ example of possible problems, consider a hypothetical new\n   ancillary chunk\
    \ type that is safe-to-copy and is required to appear\n   after PLTE if PLTE is\
    \ present.  If our program to add a suggested\n   PLTE does not recognize this\
    \ new chunk, it may insert PLTE in the\n   wrong place, namely after the new chunk.\
    \  We could prevent such\n   problems by requiring PNG editors to discard all\
    \ unknown chunks, but\n   that is a very unattractive solution.  Instead, PNG\
    \ requires\n   ancillary chunks not to have ordering restrictions like this.\n\
    \   To prevent this type of problem while allowing for future extension,\n   we\
    \ put some constraints on both the behavior of PNG editors and the\n   allowed\
    \ ordering requirements for chunks.\n   7.1. Behavior of PNG editors\n      The\
    \ rules for PNG editors are:\n          * When copying an unknown unsafe-to-copy\
    \ ancillary chunk, a\n            PNG editor must not move the chunk relative\
    \ to any critical\n            chunk.  It can relocate the chunk freely relative\
    \ to other\n            ancillary chunks that occur between the same pair of\n\
    \            critical chunks.  (This is well defined since the editor\n      \
    \      must not add, delete, modify, or reorder critical chunks if\n         \
    \   it is preserving unknown unsafe-to-copy chunks.)\n          * When copying\
    \ an unknown safe-to-copy ancillary chunk, a PNG\n            editor must not\
    \ move the chunk from before IDAT to after\n            IDAT or vice versa.  (This\
    \ is well defined because IDAT is\n            always present.)  Any other reordering\
    \ is permitted.\n          * When copying a known ancillary chunk type, an editor\
    \ need\n            only honor the specific chunk ordering rules that exist for\n\
    \            that chunk type.  However, it can always choose to apply the\n  \
    \          above general rules instead.\n          * PNG editors must give up\
    \ on encountering an unknown critical\n            chunk type, because there is\
    \ no way to be certain that a\n            valid file will result from modifying\
    \ a file containing such\n            a chunk.  (Note that simply discarding the\
    \ chunk is not good\n            enough, because it might have unknown implications\
    \ for the\n            interpretation of other chunks.)\n      These rules are\
    \ expressed in terms of copying chunks from an input\n      file to an output\
    \ file, but they apply in the obvious way if a PNG\n      file is modified in\
    \ place.\n      See also Chunk naming conventions (Section 3.3).\n   7.2. Ordering\
    \ of ancillary chunks\n      The ordering rules for an ancillary chunk type cannot\
    \ be any\n      stricter than this:\n          * Unsafe-to-copy chunks can have\
    \ ordering requirements\n            relative to critical chunks.\n          *\
    \ Safe-to-copy chunks can have ordering requirements relative\n            to\
    \ IDAT.\n      The actual ordering rules for any particular ancillary chunk type\n\
    \      may be weaker.  See for example the ordering rules for the\n      standard\
    \ ancillary chunk types (Summary of standard chunks,\n      Section 4.3).\n  \
    \    Decoders must not assume more about the positioning of any\n      ancillary\
    \ chunk than is specified by the chunk ordering rules.  In\n      particular,\
    \ it is never valid to assume that a specific ancillary\n      chunk type occurs\
    \ with any particular positioning relative to\n      other ancillary chunks. \
    \ (For example, it is unsafe to assume that\n      your private ancillary chunk\
    \ occurs immediately before IEND.  Even\n      if your application always writes\
    \ it there, a PNG editor might\n      have inserted some other ancillary chunk\
    \ after it.  But you can\n      safely assume that your chunk will remain somewhere\
    \ between IDAT\n      and IEND.)\n   7.3. Ordering of critical chunks\n      Critical\
    \ chunks can have arbitrary ordering requirements, because\n      PNG editors\
    \ are required to give up if they encounter unknown\n      critical chunks.  For\
    \ example, IHDR has the special ordering rule\n      that it must always appear\
    \ first.  A PNG editor, or indeed any\n      PNG-writing program, must know and\
    \ follow the ordering rules for\n      any critical chunk type that it can emit.\n"
- title: 8. Miscellaneous Topics
  contents:
  - "8. Miscellaneous Topics\n   8.1. File name extension\n      On systems where\
    \ file names customarily include an extension\n      signifying file type, the\
    \ extension \".png\" is recommended for PNG\n      files.  Lower case \".png\"\
    \ is preferred if file names are case-\n      sensitive.\n   8.2. Internet media\
    \ type\n      The Internet Assigned Numbers Authority (IANA) has registered\n\
    \      \"image/png\" as the Internet Media Type for PNG [RFC-2045, RFC-\n    \
    \  2048].  For robustness, decoders may choose to also support the\n      interim\
    \ media type \"image/x-png\" which was in use before\n      registration was complete.\n\
    \   8.3. Macintosh file layout\n      In the Apple Macintosh system, the following\
    \ conventions are\n      recommended:\n          * The four-byte file type code\
    \ for PNG files is \"PNGf\".  (This\n            code has been registered with\
    \ Apple for PNG files.) The\n            creator code will vary depending on the\
    \ creating\n            application.\n          * The contents of the data fork\
    \ must be a PNG file exactly as\n            described in the rest of this specification.\n\
    \          * The contents of the resource fork are unspecified.  It may\n    \
    \        be empty or may contain application-dependent resources.\n          *\
    \ When transferring a Macintosh PNG file to a non-Macintosh\n            system,\
    \ only the data fork should be transferred.\n   8.4. Multiple-image extension\n\
    \      PNG itself is strictly a single-image format.  However, it may be\n   \
    \   necessary to store multiple images within one file; for example,\n      this\
    \ is needed to convert some GIF files.  In the future, a\n      multiple-image\
    \ format based on PNG may be defined.  Such a format\n      will be considered\
    \ a separate file format and will have a\n      different signature.  PNG-supporting\
    \ applications may or may not\n      choose to support the multiple-image format.\n\
    \      See Rationale: Why not these features? (Section 12.3).\n   8.5. Security\
    \ considerations\n      A PNG file or datastream is composed of a collection of\
    \ explicitly\n      typed \"chunks\".  Chunks whose contents are defined by the\n\
    \      specification could actually contain anything, including malicious\n  \
    \    code.  But there is no known risk that such malicious code could\n      be\
    \ executed on the recipient's computer as a result of decoding\n      the PNG\
    \ image.\n      The possible security risks associated with future chunk types\n\
    \      cannot be specified at this time.  Security issues will be\n      considered\
    \ when evaluating chunks proposed for registration as\n      public chunks.  There\
    \ is no additional security risk associated\n      with unknown or unimplemented\
    \ chunk types, because such chunks\n      will be ignored, or at most be copied\
    \ into another PNG file.\n      The tEXt and zTXt chunks contain data that is\
    \ meant to be\n      displayed as plain text.  It is possible that if the decoder\n\
    \      displays such text without filtering out control characters,\n      especially\
    \ the ESC (escape) character, certain systems or\n      terminals could behave\
    \ in undesirable and insecure ways.  We\n      recommend that decoders filter\
    \ out control characters to avoid\n      this risk; see Recommendations for Decoders:\
    \ Text chunk processing\n      (Section 10.11).\n      Because every chunk's length\
    \ is available at its beginning, and\n      because every chunk has a CRC trailer,\
    \ there is a very robust\n      defense against corrupted data and against fraudulent\
    \ chunks that\n      attempt to overflow the decoder's buffers.  Also, the PNG\n\
    \      signature bytes provide early detection of common file\n      transmission\
    \ errors.\n      A decoder that fails to check CRCs could be subject to data\n\
    \      corruption.  The only likely consequence of such corruption is\n      incorrectly\
    \ displayed pixels within the image.  Worse things might\n      happen if the\
    \ CRC of the IHDR chunk is not checked and the width\n      or height fields are\
    \ corrupted.  See Recommendations for Decoders:\n      Error checking (Section\
    \ 10.1).\n      A poorly written decoder might be subject to buffer overflow,\n\
    \      because chunks can be extremely large, up to (2^31)-1 bytes long.\n   \
    \   But properly written decoders will handle large chunks without\n      difficulty.\n"
- title: 9. Recommendations for Encoders
  contents:
  - "9. Recommendations for Encoders\n   This chapter gives some recommendations for\
    \ encoder behavior.  The\n   only absolute requirement on a PNG encoder is that\
    \ it produce files\n   that conform to the format specified in the preceding chapters.\n\
    \   However, best results will usually be achieved by following these\n   recommendations.\n\
    \   9.1. Sample depth scaling\n      When encoding input samples that have a sample\
    \ depth that cannot\n      be directly represented in PNG, the encoder must scale\
    \ the samples\n      up to a sample depth that is allowed by PNG.  The most accurate\n\
    \      scaling method is the linear equation\n         output = ROUND(input *\
    \ MAXOUTSAMPLE / MAXINSAMPLE)\n      where the input samples range from 0 to MAXINSAMPLE\
    \ and the\n      outputs range from 0 to MAXOUTSAMPLE (which is (2^sampledepth)-1).\n\
    \      A close approximation to the linear scaling method can be achieved\n  \
    \    by \"left bit replication\", which is shifting the valid bits to\n      begin\
    \ in the most significant bit and repeating the most\n      significant bits into\
    \ the open bits.  This method is often faster\n      to compute than linear scaling.\
    \  As an example, assume that 5-bit\n      samples are being scaled up to 8 bits.\
    \  If the source sample value\n      is 27 (in the range from 0-31), then the\
    \ original bits are:\n         4 3 2 1 0\n         ---------\n         1 1 0 1\
    \ 1\n      Left bit replication gives a value of 222:\n         7 6 5 4 3  2 1\
    \ 0\n         ----------------\n         1 1 0 1 1  1 1 0\n         |=======|\
    \  |===|\n             |      Leftmost Bits Repeated to Fill Open Bits\n     \
    \        |\n         Original Bits\n      which matches the value computed by\
    \ the linear equation.  Left bit\n      replication usually gives the same value\
    \ as linear scaling, and is\n      never off by more than one.\n      A distinctly\
    \ less accurate approximation is obtained by simply\n      left-shifting the input\
    \ value and filling the low order bits with\n      zeroes.  This scheme cannot\
    \ reproduce white exactly, since it does\n      not generate an all-ones maximum\
    \ value; the net effect is to\n      darken the image slightly.  This method is\
    \ not recommended in\n      general, but it does have the effect of improving\
    \ compression,\n      particularly when dealing with greater-than-eight-bit sample\n\
    \      depths.  Since the relative error introduced by zero-fill scaling\n   \
    \   is small at high sample depths, some encoders may choose to use\n      it.\
    \  Zero-fill must not be used for alpha channel data, however,\n      since many\
    \ decoders will special-case alpha values of all zeroes\n      and all ones. \
    \ It is important to represent both those values\n      exactly in the scaled\
    \ data.\n      When the encoder writes an sBIT chunk, it is required to do the\n\
    \      scaling in such a way that the high-order bits of the stored\n      samples\
    \ match the original data.  That is, if the sBIT chunk\n      specifies a sample\
    \ depth of S, the high-order S bits of the stored\n      data must agree with\
    \ the original S-bit data values.  This allows\n      decoders to recover the\
    \ original data by shifting right.  The\n      added low-order bits are not constrained.\
    \  Note that all the above\n      scaling methods meet this restriction.\n   \
    \   When scaling up source data, it is recommended that the low-order\n      bits\
    \ be filled consistently for all samples; that is, the same\n      source value\
    \ should generate the same sample value at any pixel\n      position.  This improves\
    \ compression by reducing the number of\n      distinct sample values.  However,\
    \ this is not a requirement, and\n      some encoders may choose not to follow\
    \ it.  For example, an\n      encoder might instead dither the low-order bits,\
    \ improving\n      displayed image quality at the price of increasing file size.\n\
    \      In some applications the original source data may have a range\n      that\
    \ is not a power of 2.  The linear scaling equation still works\n      for this\
    \ case, although the shifting methods do not.  It is\n      recommended that an\
    \ sBIT chunk not be written for such images,\n      since sBIT suggests that the\
    \ original data range was exactly\n      0..2^S-1.\n   9.2. Encoder gamma handling\n\
    \      See Gamma Tutorial (Chapter 13) if you aren't already familiar\n      with\
    \ gamma issues.\n      Proper handling of gamma encoding and the gAMA chunk in\
    \ an encoder\n      depends on the prior history of the sample values and on whether\n\
    \      these values have already been quantized to integers.\n      If the encoder\
    \ has access to sample intensity values in floating-\n      point or high-precision\
    \ integer form (perhaps from a computer\n      image renderer), then it is recommended\
    \ that the encoder perform\n      its own gamma encoding before quantizing the\
    \ data to integer\n      values for storage in the file.  Applying gamma encoding\
    \ at this\n      stage results in images with fewer banding artifacts at a given\n\
    \      sample depth, or allows smaller samples while retaining the same\n    \
    \  visual quality.\n      A linear intensity level, expressed as a floating-point\
    \ value in\n      the range 0 to 1, can be converted to a gamma-encoded sample\
    \ value\n      by\n         sample = ROUND((intensity ^ encoder_gamma) * MAXSAMPLE)\n\
    \      The file_gamma value to be written in the PNG gAMA chunk is the\n     \
    \ same as encoder_gamma in this equation, since we are assuming the\n      initial\
    \ intensity value is linear (in effect, camera_gamma is\n      1.0).\n      If\
    \ the image is being written to a file only, the encoder_gamma\n      value can\
    \ be selected somewhat arbitrarily.  Values of 0.45 or 0.5\n      are generally\
    \ good choices because they are common in video\n      systems, and so most PNG\
    \ decoders should do a good job displaying\n      such images.\n      Some image\
    \ renderers may simultaneously write the image to a PNG\n      file and display\
    \ it on-screen.  The displayed pixels should be\n      gamma corrected for the\
    \ display system and viewing conditions in\n      use, so that the user sees a\
    \ proper representation of the intended\n      scene.  An appropriate gamma correction\
    \ value is\n         screen_gc = viewing_gamma / display_gamma\n      If the renderer\
    \ wants to write the same gamma-corrected sample\n      values to the PNG file,\
    \ avoiding a separate gamma-encoding step\n      for file output, then this screen_gc\
    \ value should be written in\n      the gAMA chunk.  This will allow a PNG decoder\
    \ to reproduce what\n      the file's originator saw on screen during rendering\
    \ (provided the\n      decoder properly supports arbitrary values in a gAMA chunk).\n\
    \      However, it is equally reasonable for a renderer to apply gamma\n     \
    \ correction for screen display using a gamma appropriate to the\n      viewing\
    \ conditions, and to separately gamma-encode the sample\n      values for file\
    \ storage using a standard value of gamma such as\n      0.5.  In fact, this is\
    \ preferable, since some PNG decoders may not\n      accurately display images\
    \ with unusual gAMA values.\n      Computer graphics renderers often do not perform\
    \ gamma encoding,\n      instead making sample values directly proportional to\
    \ scene light\n      intensity.  If the PNG encoder receives sample values that\
    \ have\n      already been quantized into linear-light integer values, there is\n\
    \      no point in doing gamma encoding on them; that would just result\n    \
    \  in further loss of information.  The encoder should just write the\n      sample\
    \ values to the PNG file.  This \"linear\" sample encoding is\n      equivalent\
    \ to gamma encoding with a gamma of 1.0, so graphics\n      programs that produce\
    \ linear samples should always emit a gAMA\n      chunk specifying a gamma of\
    \ 1.0.\n      When the sample values come directly from a piece of hardware, the\n\
    \      correct gAMA value is determined by the gamma characteristic of\n     \
    \ the hardware.  In the case of video digitizers (\"frame grabbers\"),\n     \
    \ gAMA should be 0.45 or 0.5 for NTSC (possibly less for PAL or\n      SECAM)\
    \ since video camera transfer functions are standardized.\n      Image scanners\
    \ are less predictable.  Their output samples may be\n      linear (gamma 1.0)\
    \ since CCD sensors themselves are linear, or the\n      scanner hardware may\
    \ have already applied gamma correction\n      designed to compensate for dot\
    \ gain in subsequent printing (gamma\n      of about 0.57), or the scanner may\
    \ have corrected the samples for\n      display on a CRT (gamma of 0.4-0.5). \
    \ You will need to refer to\n      the scanner's manual, or even scan a calibrated\
    \ gray wedge, to\n      determine what a particular scanner does.\n      File\
    \ format converters generally should not attempt to convert\n      supplied images\
    \ to a different gamma.  Store the data in the PNG\n      file without conversion,\
    \ and record the source gamma if it is\n      known.  Gamma alteration at file\
    \ conversion time causes re-\n      quantization of the set of intensity levels\
    \ that are represented,\n      introducing further roundoff error with little\
    \ benefit.  It's\n      almost always better to just copy the sample values intact\
    \ from\n      the input to the output file.\n      In some cases, the supplied\
    \ image may be in an image format (e.g.,\n      TIFF) that can describe the gamma\
    \ characteristic of the image.  In\n      such cases, a file format converter\
    \ is strongly encouraged to\n      write a PNG gAMA chunk that corresponds to\
    \ the known gamma of the\n      source image.  Note that some file formats specify\
    \ the gamma of\n      the display system, not the camera.  If the input file's\
    \ gamma\n      value is greater than 1.0, it is almost certainly a display system\n\
    \      gamma, and you should use its reciprocal for the PNG gAMA.\n      If the\
    \ encoder or file format converter does not know how an image\n      was originally\
    \ created, but does know that the image has been\n      displayed satisfactorily\
    \ on a display with gamma display_gamma\n      under lighting conditions where\
    \ a particular viewing_gamma is\n      appropriate, then the image can be marked\
    \ as having the\n      file_gamma:\n         file_gamma = viewing_gamma / display_gamma\n\
    \      This will allow viewers of the PNG file to see the same image that\n  \
    \    the person running the file format converter saw.  Although this\n      may\
    \ not be precisely the correct value of the image gamma, it's\n      better to\
    \ write a gAMA chunk with an approximately right value\n      than to omit the\
    \ chunk and force PNG decoders to guess at an\n      appropriate gamma.\n    \
    \  On the other hand, if the image file is being converted as part of\n      a\
    \ \"bulk\" conversion, with no one looking at each image, then it is\n      better\
    \ to omit the gAMA chunk entirely.  If the image gamma has to\n      be guessed\
    \ at, leave it to the decoder to do the guessing.\n      Gamma does not apply\
    \ to alpha samples; alpha is always represented\n      linearly.\n      See also\
    \ Recommendations for Decoders: Decoder gamma handling\n      (Section 10.5).\n\
    \   9.3. Encoder color handling\n      See Color Tutorial (Chapter 14) if you\
    \ aren't already familiar\n      with color issues.\n      If it is possible for\
    \ the encoder to determine the chromaticities\n      of the source display primaries,\
    \ or to make a strong guess based\n      on the origin of the image or the hardware\
    \ running it, then the\n      encoder is strongly encouraged to output the cHRM\
    \ chunk.  If it\n      does so, the gAMA chunk should also be written; decoders\
    \ can do\n      little with cHRM if gAMA is missing.\n      Video created with\
    \ recent video equipment probably uses the CCIR\n      709 primaries and D65 white\
    \ point [ITU-BT709], which are:\n                  R           G           B \
    \        White\n         x      0.640       0.300       0.150       0.3127\n \
    \        y      0.330       0.600       0.060       0.3290\n      An older but\
    \ still very popular video standard is SMPTE-C [SMPTE-\n      170M]:\n       \
    \           R           G           B         White\n         x      0.630   \
    \    0.310       0.155       0.3127\n         y      0.340       0.595       0.070\
    \       0.3290\n      The original NTSC color primaries have not been used in\
    \ decades.\n      Although you may still find the NTSC numbers listed in standards\n\
    \      documents, you won't find any images that actually use them.\n      Scanners\
    \ that produce PNG files as output should insert the filter\n      chromaticities\
    \ into a cHRM chunk and the camera_gamma into a gAMA\n      chunk.\n      In the\
    \ case of hand-drawn or digitally edited images, you have to\n      determine\
    \ what monitor they were viewed on when being produced.\n      Many image editing\
    \ programs allow you to specify what type of\n      monitor you are using.  This\
    \ is often because they are working in\n      some device-independent space internally.\
    \  Such programs have\n      enough information to write valid cHRM and gAMA chunks,\
    \ and should\n      do so automatically.\n      If the encoder is compiled as\
    \ a portion of a computer image\n      renderer that performs full-spectral rendering,\
    \ the monitor values\n      that were used to convert from the internal device-independent\n\
    \      color space to RGB should be written into the cHRM chunk. Any\n      colors\
    \ that are outside the gamut of the chosen RGB device should\n      be clipped\
    \ or otherwise constrained to be within the gamut; PNG\n      does not store out\
    \ of gamut colors.\n      If the computer image renderer performs calculations\
    \ directly in\n      device-dependent RGB space, a cHRM chunk should not be written\n\
    \      unless the scene description and rendering parameters have been\n     \
    \ adjusted to look good on a particular monitor.  In that case, the\n      data\
    \ for that monitor (if known) should be used to construct a\n      cHRM chunk.\n\
    \      There are often cases where an image's exact origins are unknown,\n   \
    \   particularly if it began life in some other format.  A few image\n      formats\
    \ store calibration information, which can be used to fill\n      in the cHRM\
    \ chunk.  For example, all PhotoCD images use the CCIR\n      709 primaries and\
    \ D65 whitepoint, so these values can be written\n      into the cHRM chunk when\
    \ converting a PhotoCD file.  PhotoCD also\n      uses the SMPTE-170M transfer\
    \ function, which is closely\n      approximated by a gAMA of 0.5.  (PhotoCD can\
    \ store colors outside\n      the RGB gamut, so the image data will require gamut\
    \ mapping before\n      writing to PNG format.)  TIFF 6.0 files can optionally\
    \ store\n      calibration information, which if present should be used to\n \
    \     construct the cHRM chunk.  GIF and most other formats do not store\n   \
    \   any calibration information.\n      It is not recommended that file format\
    \ converters attempt to\n      convert supplied images to a different RGB color\
    \ space.  Store the\n      data in the PNG file without conversion, and record\
    \ the source\n      primary chromaticities if they are known.  Color space\n \
    \     transformation at file conversion time is a bad idea because of\n      gamut\
    \ mismatches and rounding errors.  As with gamma conversions,\n      it's better\
    \ to store the data losslessly and incur at most one\n      conversion when the\
    \ image is finally displayed.\n      See also Recommendations for Decoders: Decoder\
    \ color handling\n      (Section 10.6).\n   9.4. Alpha channel creation\n    \
    \  The alpha channel can be regarded either as a mask that\n      temporarily\
    \ hides transparent parts of the image, or as a means\n      for constructing\
    \ a non-rectangular image.  In the first case, the\n      color values of fully\
    \ transparent pixels should be preserved for\n      future use.  In the second\
    \ case, the transparent pixels carry no\n      useful data and are simply there\
    \ to fill out the rectangular image\n      area required by PNG.  In this case,\
    \ fully transparent pixels\n      should all be assigned the same color value\
    \ for best compression.\n      Image authors should keep in mind the possibility\
    \ that a decoder\n      will ignore transparency control.  Hence, the colors assigned\
    \ to\n      transparent pixels should be reasonable background colors whenever\n\
    \      feasible.\n      For applications that do not require a full alpha channel,\
    \ or\n      cannot afford the price in compression efficiency, the tRNS\n    \
    \  transparency chunk is also available.\n      If the image has a known background\
    \ color, this color should be\n      written in the bKGD chunk.  Even decoders\
    \ that ignore transparency\n      may use the bKGD color to fill unused screen\
    \ area.\n      If the original image has premultiplied (also called \"associated\"\
    )\n      alpha data, convert it to PNG's non-premultiplied format by\n      dividing\
    \ each sample value by the corresponding alpha value, then\n      multiplying\
    \ by the maximum value for the image bit depth, and\n      rounding to the nearest\
    \ integer.  In valid premultiplied data, the\n      sample values never exceed\
    \ their corresponding alpha values, so\n      the result of the division should\
    \ always be in the range 0 to 1.\n      If the alpha value is zero, output black\
    \ (zeroes).\n   9.5. Suggested palettes\n      A PLTE chunk can appear in truecolor\
    \ PNG files.  In such files,\n      the chunk is not an essential part of the\
    \ image data, but simply\n      represents a suggested palette that viewers may\
    \ use to present the\n      image on indexed-color display hardware.  A suggested\
    \ palette is\n      of no interest to viewers running on truecolor hardware.\n\
    \      If an encoder chooses to provide a suggested palette, it is\n      recommended\
    \ that a hIST chunk also be written to indicate the\n      relative importance\
    \ of the palette entries.  The histogram values\n      are most easily computed\
    \ as \"nearest neighbor\" counts, that is,\n      the approximate usage of each\
    \ palette entry if no dithering is\n      applied.  (These counts will often be\
    \ available for free as a\n      consequence of developing the suggested palette.)\n\
    \      For images of color type 2 (truecolor without alpha channel), it\n    \
    \  is recommended that the palette and histogram be computed with\n      reference\
    \ to the RGB data only, ignoring any transparent-color\n      specification. \
    \ If the file uses transparency (has a tRNS chunk),\n      viewers can easily\
    \ adapt the resulting palette for use with their\n      intended background color.\
    \  They need only replace the palette\n      entry closest to the tRNS color with\
    \ their background color (which\n      may or may not match the file's bKGD color,\
    \ if any).\n      For images of color type 6 (truecolor with alpha channel), it\
    \ is\n      recommended that a bKGD chunk appear and that the palette and\n  \
    \    histogram be computed with reference to the image as it would\n      appear\
    \ after compositing against the specified background color.\n      This definition\
    \ is necessary to ensure that useful palette entries\n      are generated for\
    \ pixels having fractional alpha values.  The\n      resulting palette will probably\
    \ only be useful to viewers that\n      present the image against the same background\
    \ color.  It is\n      recommended that PNG editors delete or recompute the palette\
    \ if\n      they alter or remove the bKGD chunk in an image of color type 6.\n\
    \      If PLTE appears without bKGD in an image of color type 6, the\n      circumstances\
    \ under which the palette was computed are\n      unspecified.\n   9.6. Filter\
    \ selection\n      For images of color type 3 (indexed color), filter type 0 (None)\n\
    \      is usually the most effective.  Note that color images with 256 or\n  \
    \    fewer colors should almost always be stored in indexed color\n      format;\
    \ truecolor format is likely to be much larger.\n      Filter type 0 is also recommended\
    \ for images of bit depths less\n      than 8.  For low-bit-depth grayscale images,\
    \ it may be a net win\n      to expand the image to 8-bit representation and apply\
    \ filtering,\n      but this is rare.\n      For truecolor and grayscale images,\
    \ any of the five filters may\n      prove the most effective.  If an encoder\
    \ uses a fixed filter, the\n      Paeth filter is most likely to be the best.\n\
    \      For best compression of truecolor and grayscale images, we\n      recommend\
    \ an adaptive filtering approach in which a filter is\n      chosen for each scanline.\
    \  The following simple heuristic has\n      performed well in early tests: compute\
    \ the output scanline using\n      all five filters, and select the filter that\
    \ gives the smallest\n      sum of absolute values of outputs.  (Consider the\
    \ output bytes as\n      signed differences for this test.)  This method usually\n\
    \      outperforms any single fixed filter choice.  However, it is likely\n  \
    \    that much better heuristics will be found as more experience is\n      gained\
    \ with PNG.\n      Filtering according to these recommendations is effective on\n\
    \      interlaced as well as noninterlaced images.\n   9.7. Text chunk processing\n\
    \      A nonempty keyword must be provided for each text chunk.  The\n      generic\
    \ keyword \"Comment\" can be used if no better description of\n      the text\
    \ is available.  If a user-supplied keyword is used, be\n      sure to check that\
    \ it meets the restrictions on keywords.\n      PNG text strings are expected\
    \ to use the Latin-1 character set.\n      Encoders should avoid storing characters\
    \ that are not defined in\n      Latin-1, and should provide character code remapping\
    \ if the local\n      system's character set is not Latin-1.\n      Encoders should\
    \ discourage the creation of single lines of text\n      longer than 79 characters,\
    \ in order to facilitate easy reading.\n      It is recommended that text items\
    \ less than 1K (1024 bytes) in\n      size should be output using uncompressed\
    \ tEXt chunks. In\n      particular, it is recommended that the basic title and\
    \ author\n      keywords should always be output using uncompressed tEXt chunks.\n\
    \      Lengthy disclaimers, on the other hand, are ideal candidates for\n    \
    \  zTXt.\n      Placing large tEXt and zTXt chunks after the image data (after\n\
    \      IDAT) can speed up image display in some situations, since the\n      decoder\
    \ won't have to read over the text to get to the image data.\n      But it is\
    \ recommended that small text chunks, such as the image\n      title, appear before\
    \ IDAT.\n   9.8. Use of private chunks\n      Applications can use PNG private\
    \ chunks to carry information that\n      need not be understood by other applications.\
    \  Such chunks must be\n      given names with lowercase second letters, to ensure\
    \ that they can\n      never conflict with any future public chunk definition.\
    \  Note,\n      however, that there is no guarantee that some other application\n\
    \      will not use the same private chunk name.  If you use a private\n     \
    \ chunk type, it is prudent to store additional identifying\n      information\
    \ at the beginning of the chunk data.\n      Use an ancillary chunk type (lowercase\
    \ first letter), not a\n      critical chunk type, for all private chunks that\
    \ store information\n      that is not absolutely essential to view the image.\
    \  Creation of\n      private critical chunks is discouraged because they render\
    \ PNG\n      files unportable.  Such chunks should not be used in publicly\n \
    \     available software or files.  If private critical chunks are\n      essential\
    \ for your application, it is recommended that one appear\n      near the start\
    \ of the file, so that a standard decoder need not\n      read very far before\
    \ discovering that it cannot handle the file.\n      If you want others outside\
    \ your organization to understand a chunk\n      type that you invent, contact\
    \ the maintainers of the PNG\n      specification to submit a proposed chunk name\
    \ and definition for\n      addition to the list of special-purpose public chunks\
    \ (see\n      Additional chunk types, Section 4.4).  Note that a proposed public\n\
    \      chunk name (with uppercase second letter) must not be used in\n      publicly\
    \ available software or files until registration has been\n      approved.\n \
    \     If an ancillary chunk contains textual information that might be\n     \
    \ of interest to a human user, you should not create a special chunk\n      type\
    \ for it.  Instead use a tEXt chunk and define a suitable\n      keyword.  That\
    \ way, the information will be available to users not\n      using your software.\n\
    \      Keywords in tEXt chunks should be reasonably self-explanatory,\n      since\
    \ the idea is to let other users figure out what the chunk\n      contains.  If\
    \ of general usefulness, new keywords can be\n      registered with the maintainers\
    \ of the PNG specification.  But it\n      is permissible to use keywords without\
    \ registering them first.\n   9.9. Private type and method codes\n      This specification\
    \ defines the meaning of only some of the\n      possible values of some fields.\
    \  For example, only compression\n      method 0 and filter types 0 through 4\
    \ are defined.  Numbers\n      greater than 127 must be used when inventing experimental\
    \ or\n      private definitions of values for any of these fields.  Numbers\n\
    \      below 128 are reserved for possible future public extensions of\n     \
    \ this specification.  Note that use of private type codes may\n      render a\
    \ file unreadable by standard decoders.  Such codes are\n      strongly discouraged\
    \ except for experimental purposes, and should\n      not appear in publicly available\
    \ software or files.\n"
- title: 10. Recommendations for Decoders
  contents:
  - "10. Recommendations for Decoders\n   This chapter gives some recommendations\
    \ for decoder behavior.  The\n   only absolute requirement on a PNG decoder is\
    \ that it successfully\n   read any file conforming to the format specified in\
    \ the preceding\n   chapters.  However, best results will usually be achieved\
    \ by\n   following these recommendations.\n   10.1. Error checking\n      To ensure\
    \ early detection of common file-transfer problems,\n      decoders should verify\
    \ that all eight bytes of the PNG file\n      signature are correct.  (See Rationale:\
    \ PNG file signature,\n      Section 12.11.) A decoder can have additional confidence\
    \ in the\n      file's integrity if the next eight bytes are an IHDR chunk header\n\
    \      with the correct chunk length.\n      Unknown chunk types must be handled\
    \ as described in Chunk naming\n      conventions (Section 3.3).  An unknown chunk\
    \ type is not to be\n      treated as an error unless it is a critical chunk.\n\
    \      It is strongly recommended that decoders should verify the CRC on\n   \
    \   each chunk.\n      In some situations it is desirable to check chunk headers\
    \ (length\n      and type code) before reading the chunk data and CRC.  The chunk\n\
    \      type can be checked for plausibility by seeing whether all four\n     \
    \ bytes are ASCII letters (codes 65-90 and 97-122); note that this\n      need\
    \ only be done for unrecognized type codes.  If the total file\n      size is\
    \ known (from file system information, HTTP protocol, etc),\n      the chunk length\
    \ can be checked for plausibility as well.\n      If CRCs are not checked, dropped/added\
    \ data bytes or an erroneous\n      chunk length can cause the decoder to get\
    \ out of step and\n      misinterpret subsequent data as a chunk header.  Verifying\
    \ that\n      the chunk type contains letters is an inexpensive way of providing\n\
    \      early error detection in this situation.\n      For known-length chunks\
    \ such as IHDR, decoders should treat an\n      unexpected chunk length as an\
    \ error.  Future extensions to this\n      specification will not add new fields\
    \ to existing chunks; instead,\n      new chunk types will be added to carry new\
    \ information.\n      Unexpected values in fields of known chunks (for example,\
    \ an\n      unexpected compression method in the IHDR chunk) must be checked\n\
    \      for and treated as errors.  However, it is recommended that\n      unexpected\
    \ field values be treated as fatal errors only in\n      critical chunks.  An\
    \ unexpected value in an ancillary chunk can be\n      handled by ignoring the\
    \ whole chunk as though it were an unknown\n      chunk type.  (This recommendation\
    \ assumes that the chunk's CRC has\n      been verified.  In decoders that do\
    \ not check CRCs, it is safer to\n      treat any unexpected value as indicating\
    \ a corrupted file.)\n   10.2. Pixel dimensions\n      Non-square pixels can be\
    \ represented (see the pHYs chunk), but\n      viewers are not required to account\
    \ for them; a viewer can present\n      any PNG file as though its pixels are\
    \ square.\n      Conversely, viewers running on display hardware with non-square\n\
    \      pixels are strongly encouraged to rescale images for proper\n      display.\n\
    \   10.3. Truecolor image handling\n      To achieve PNG's goal of universal interchangeability,\
    \ decoders\n      are required to accept all types of PNG image: indexed-color,\n\
    \      truecolor, and grayscale.  Viewers running on indexed-color\n      display\
    \ hardware need to be able to reduce truecolor images to\n      indexed format\
    \ for viewing.  This process is usually called \"color\n      quantization\".\n\
    \      A simple, fast way of doing this is to reduce the image to a fixed\n  \
    \    palette.  Palettes with uniform color spacing (\"color cubes\") are\n   \
    \   usually used to minimize the per-pixel computation.  For\n      photograph-like\
    \ images, dithering is recommended to avoid ugly\n      contours in what should\
    \ be smooth gradients; however, dithering\n      introduces graininess that can\
    \ be objectionable.\n      The quality of rendering can be improved substantially\
    \ by using a\n      palette chosen specifically for the image, since a color cube\n\
    \      usually has numerous entries that are unused in any particular\n      image.\
    \  This approach requires more work, first in choosing the\n      palette, and\
    \ second in mapping individual pixels to the closest\n      available color. \
    \ PNG allows the encoder to supply a suggested\n      palette in a PLTE chunk,\
    \ but not all encoders will do so, and the\n      suggested palette may be unsuitable\
    \ in any case (it may have too\n      many or too few colors).  High-quality viewers\
    \ will therefore need\n      to have a palette selection routine at hand.  A large\
    \ lookup table\n      is usually the most feasible way of mapping individual pixels\
    \ to\n      palette entries with adequate speed.\n      Numerous implementations\
    \ of color quantization are available.  The\n      PNG reference implementation,\
    \ libpng, includes code for the\n      purpose.\n   10.4. Sample depth rescaling\n\
    \      Decoders may wish to scale PNG data to a lesser sample depth (data\n  \
    \    precision) for display.  For example, 16-bit data will need to be\n     \
    \ reduced to 8-bit depth for use on most present-day display\n      hardware.\
    \  Reduction of 8-bit data to 5-bit depth is also common.\n      The most accurate\
    \ scaling is achieved by the linear equation\n         output = ROUND(input *\
    \ MAXOUTSAMPLE / MAXINSAMPLE)\n      where\n         MAXINSAMPLE = (2^sampledepth)-1\n\
    \         MAXOUTSAMPLE = (2^desired_sampledepth)-1\n      A slightly less accurate\
    \ conversion is achieved by simply shifting\n      right by sampledepth-desired_sampledepth\
    \ places.  For example, to\n      reduce 16-bit samples to 8-bit, one need only\
    \ discard the low-\n      order byte.  In many situations the shift method is\
    \ sufficiently\n      accurate for display purposes, and it is certainly much\
    \ faster.\n      (But if gamma correction is being done, sample rescaling can\
    \ be\n      merged into the gamma correction lookup table, as is illustrated\n\
    \      in Decoder gamma handling, Section 10.5.)\n      When an sBIT chunk is\
    \ present, the original pre-PNG data can be\n      recovered by shifting right\
    \ to the sample depth specified by sBIT.\n      Note that linear scaling will\
    \ not necessarily reproduce the\n      original data, because the encoder is not\
    \ required to have used\n      linear scaling to scale the data up.  However,\
    \ the encoder is\n      required to have used a method that preserves the high-order\
    \ bits,\n      so shifting always works.  This is the only case in which shifting\n\
    \      might be said to be more accurate than linear scaling.\n      When comparing\
    \ pixel values to tRNS chunk values to detect\n      transparent pixels, it is\
    \ necessary to do the comparison exactly.\n      Therefore, transparent pixel\
    \ detection must be done before\n      reducing sample precision.\n   10.5. Decoder\
    \ gamma handling\n      See Gamma Tutorial (Chapter 13) if you aren't already\
    \ familiar\n      with gamma issues.\n      To produce correct tone reproduction,\
    \ a good image display program\n      should take into account the gammas of the\
    \ image file and the\n      display device, as well as the viewing_gamma appropriate\
    \ to the\n      lighting conditions near the display.  This can be done by\n \
    \     calculating\n         gbright = insample / MAXINSAMPLE\n         bright\
    \ = gbright ^ (1.0 / file_gamma)\n         vbright = bright ^ viewing_gamma\n\
    \         gcvideo = vbright ^ (1.0 / display_gamma)\n         fbval = ROUND(gcvideo\
    \ * MAXFBVAL)\n      where MAXINSAMPLE is the maximum sample value in the file\
    \ (255 for\n      8-bit, 65535 for 16-bit, etc), MAXFBVAL is the maximum value\
    \ of a\n      frame buffer sample (255 for 8-bit, 31 for 5-bit, etc), insample\n\
    \      is the value of the sample in the PNG file, and fbval is the value\n  \
    \    to write into the frame buffer. The first line converts from\n      integer\
    \ samples into a normalized 0 to 1 floating point value, the\n      second undoes\
    \ the gamma encoding of the image file to produce a\n      linear intensity value,\
    \ the third adjusts for the viewing\n      conditions, the fourth corrects for\
    \ the display system's gamma\n      value, and the fifth converts to an integer\
    \ frame buffer sample.\n      In practice, the second through fourth lines can\
    \ be merged into\n         gcvideo = gbright^(viewing_gamma / (file_gamma*display_gamma))\n\
    \      so as to perform only one power calculation. For color images, the\n  \
    \    entire calculation is performed separately for R, G, and B values.\n    \
    \  It is not necessary to perform transcendental math for every\n      pixel.\
    \  Instead, compute a lookup table that gives the correct\n      output value\
    \ for every possible sample value. This requires only\n      256 calculations\
    \ per image (for 8-bit accuracy), not one or three\n      calculations per pixel.\
    \  For an indexed-color image, a one-time\n      correction of the palette is\
    \ sufficient, unless the image uses\n      transparency and is being displayed\
    \ against a nonuniform\n      background.\n      In some cases even the cost of\
    \ computing a gamma lookup table may\n      be a concern.  In these cases, viewers\
    \ are encouraged to have\n      precomputed gamma correction tables for file_gamma\
    \ values of 1.0\n      and 0.5 with some reasonable choice of viewing_gamma and\n\
    \      display_gamma, and to use the table closest to the gamma indicated\n  \
    \    in the file. This will produce acceptable results for the majority\n    \
    \  of real files.\n      When the incoming image has unknown gamma (no gAMA chunk),\
    \ choose\n      a likely default file_gamma value, but allow the user to select\
    \ a\n      new one if the result proves too dark or too light.\n      In practice,\
    \ it is often difficult to determine what value of\n      display_gamma should\
    \ be used. In systems with no built-in gamma\n      correction, the display_gamma\
    \ is determined entirely by the CRT.\n      Assuming a CRT_gamma of 2.5 is recommended,\
    \ unless you have\n      detailed calibration measurements of this particular\
    \ CRT\n      available.\n      However, many modern frame buffers have lookup\
    \ tables that are\n      used to perform gamma correction, and on these systems\
    \ the\n      display_gamma value should be the gamma of the lookup table and\n\
    \      CRT combined. You may not be able to find out what the lookup\n      table\
    \ contains from within an image viewer application, so you may\n      have to\
    \ ask the user what the system's gamma value is.\n      Unfortunately, different\
    \ manufacturers use different ways of\n      specifying what should go into the\
    \ lookup table, so interpretation\n      of the system gamma value is system-dependent.\
    \  Gamma Tutorial\n      (Chapter 13) gives some examples.\n      The response\
    \ of real displays is actually more complex than can be\n      described by a\
    \ single number (display_gamma). If actual\n      measurements of the monitor's\
    \ light output as a function of\n      voltage input are available, the fourth\
    \ and fifth lines of the\n      computation above can be replaced by a lookup\
    \ in these\n      measurements, to find the actual frame buffer value that most\n\
    \      nearly gives the desired brightness.\n      The value of viewing_gamma\
    \ depends on lighting conditions; see\n      Gamma Tutorial (Chapter 13) for more\
    \ detail.  Ideally, a viewer\n      would allow the user to specify viewing_gamma,\
    \ either directly\n      numerically, or via selecting from \"bright surround\"\
    , \"dim\n      surround\", and \"dark surround\" conditions.  Viewers that don't\n\
    \      want to do this should just assume a value for viewing_gamma of\n     \
    \ 1.0, since most computer displays live in brightly-lit rooms.\n      When viewing\
    \ images that are digitized from video, or that are\n      destined to become\
    \ video frames, the user might want to set the\n      viewing_gamma to about 1.25\
    \ regardless of the actual level of room\n      lighting.  This value of viewing_gamma\
    \ is \"built into\" NTSC video\n      practice, and displaying an image with that\
    \ viewing_gamma allows\n      the user to see what a TV set would show under the\
    \ current room\n      lighting conditions.  (This is not the same thing as trying\
    \ to\n      obtain the most accurate rendition of the content of the scene,\n\
    \      which would require adjusting viewing_gamma to correspond to the\n    \
    \  room lighting level.)  This is another reason viewers might want\n      to\
    \ allow users to adjust viewing_gamma directly.\n   10.6. Decoder color handling\n\
    \      See Color Tutorial (Chapter 14) if you aren't already familiar\n      with\
    \ color issues.\n      In many cases, decoders will treat image data in PNG files\
    \ as\n      device-dependent RGB data and display it without modification\n  \
    \    (except for appropriate gamma correction). This provides the\n      fastest\
    \ display of PNG images.  But unless the viewer uses exactly\n      the same display\
    \ hardware as the original image author used, the\n      colors will not be exactly\
    \ the same as the original author saw,\n      particularly for darker or near-neutral\
    \ colors.  The cHRM chunk\n      provides information that allows closer color\
    \ matching than that\n      provided by gamma correction alone.\n      Decoders\
    \ can use the cHRM data to transform the image data from\n      RGB to XYZ and\
    \ thence into a perceptually linear color space such\n      as CIE LAB.  They\
    \ can then partition the colors to generate an\n      optimal palette, because\
    \ the geometric distance between two colors\n      in CIE LAB is strongly related\
    \ to how different those colors\n      appear (unlike, for example, RGB or XYZ\
    \ spaces).  The resulting\n      palette of colors, once transformed back into\
    \ RGB color space,\n      could be used for display or written into a PLTE chunk.\n\
    \      Decoders that are part of image processing applications might also\n  \
    \    transform image data into CIE LAB space for analysis.\n      In applications\
    \ where color fidelity is critical, such as product\n      design, scientific\
    \ visualization, medicine, architecture, or\n      advertising, decoders can transform\
    \ the image data from source_RGB\n      to the display_RGB space of the monitor\
    \ used to view the image.\n      This involves calculating the matrix to go from\
    \ source_RGB to XYZ\n      and the matrix to go from XYZ to display_RGB, then\
    \ combining them\n      to produce the overall transformation.  The decoder is\
    \ responsible\n      for implementing gamut mapping.\n      Decoders running on\
    \ platforms that have a Color Management System\n      (CMS) can pass the image\
    \ data, gAMA and cHRM values to the CMS for\n      display or further processing.\n\
    \      Decoders that provide color printing facilities can use the\n      facilities\
    \ in Level 2 PostScript to specify image data in\n      calibrated RGB space or\
    \ in a device-independent color space such\n      as XYZ.  This will provide better\
    \ color fidelity than a simple RGB\n      to CMYK conversion.  The PostScript\
    \ Language Reference manual\n      gives examples of this process [POSTSCRIPT].\
    \  Such decoders are\n      responsible for implementing gamut mapping between\
    \ source_RGB\n      (specified in the cHRM chunk) and the target printer. The\n\
    \      PostScript interpreter is then responsible for producing the\n      required\
    \ colors.\n      Decoders can use the cHRM data to calculate an accurate grayscale\n\
    \      representation of a color image.  Conversion from RGB to gray is\n    \
    \  simply a case of calculating the Y (luminance) component of XYZ,\n      which\
    \ is a weighted sum of the R G and B values.  The weights\n      depend on the\
    \ monitor type, i.e., the values in the cHRM chunk.\n      Decoders may wish to\
    \ do this for PNG files with no cHRM chunk.  In\n      that case, a reasonable\
    \ default would be the CCIR 709 primaries\n      [ITU-BT709].  Do not use the\
    \ original NTSC primaries, unless you\n      really do have an image color-balanced\
    \ for such a monitor.  Few\n      monitors ever used the NTSC primaries, so such\
    \ images are probably\n      nonexistent these days.\n   10.7. Background color\n\
    \      The background color given by bKGD will typically be used to fill\n   \
    \   unused screen space around the image, as well as any transparent\n      pixels\
    \ within the image.  (Thus, bKGD is valid and useful even\n      when the image\
    \ does not use transparency.)  If no bKGD chunk is\n      present, the viewer\
    \ will need to make its own decision about a\n      suitable background color.\n\
    \      Viewers that have a specific background against which to present\n    \
    \  the image (such as Web browsers) should ignore the bKGD chunk, in\n      effect\
    \ overriding bKGD with their preferred background color or\n      background image.\n\
    \      The background color given by bKGD is not to be considered\n      transparent,\
    \ even if it happens to match the color given by tRNS\n      (or, in the case\
    \ of an indexed-color image, refers to a palette\n      index that is marked as\
    \ transparent by tRNS).  Otherwise one would\n      have to imagine something\
    \ \"behind the background\" to composite\n      against.  The background color\
    \ is either used as background or\n      ignored; it is not an intermediate layer\
    \ between the PNG image and\n      some other background.\n      Indeed, it will\
    \ be common that bKGD and tRNS specify the same\n      color, since then a decoder\
    \ that does not implement transparency\n      processing will give the intended\
    \ display, at least when no\n      partially-transparent pixels are present.\n\
    \   10.8. Alpha channel processing\n      In the most general case, the alpha\
    \ channel can be used to\n      composite a foreground image against a background\
    \ image; the PNG\n      file defines the foreground image and the transparency\
    \ mask, but\n      not the background image.  Decoders are not required to support\n\
    \      this most general case.  It is expected that most will be able to\n   \
    \   support compositing against a single background color, however.\n      The\
    \ equation for computing a composited sample value is\n         output = alpha\
    \ * foreground + (1-alpha) * background\n      where alpha and the input and output\
    \ sample values are expressed\n      as fractions in the range 0 to 1.  This computation\
    \ should be\n      performed with linear (non-gamma-encoded) sample values.  For\n\
    \      color images, the computation is done separately for R, G, and B\n    \
    \  samples.\n      The following code illustrates the general case of compositing\
    \ a\n      foreground image over a background image.  It assumes that you\n  \
    \    have the original pixel data available for the background image,\n      and\
    \ that output is to a frame buffer for display.  Other variants\n      are possible;\
    \ see the comments below the code.  The code allows\n      the sample depths and\
    \ gamma values of foreground image, background\n      image, and frame buffer/CRT\
    \ all to be different.  Don't assume\n      they are the same without checking.\n\
    \      This code is standard C, with line numbers added for reference in\n   \
    \   the comments below.\n         01  int foreground[4];  /* image pixel: R, G,\
    \ B, A */\n         02  int background[3];  /* background pixel: R, G, B */\n\
    \         03  int fbpix[3];       /* frame buffer pixel */\n         04  int fg_maxsample;\
    \   /* foreground max sample */\n         05  int bg_maxsample;   /* background\
    \ max sample */\n         06  int fb_maxsample;   /* frame buffer max sample */\n\
    \         07  int ialpha;\n         08  float alpha, compalpha;\n         09 \
    \ float gamfg, linfg, gambg, linbg, comppix, gcvideo;\n             /* Get max\
    \ sample values in data and frame buffer */\n         10  fg_maxsample = (1 <<\
    \ fg_sample_depth) - 1;\n         11  bg_maxsample = (1 << bg_sample_depth) -\
    \ 1;\n         12  fb_maxsample = (1 << frame_buffer_sample_depth) - 1;\n    \
    \         /*\n              * Get integer version of alpha.\n              * Check\
    \ for opaque and transparent special cases;\n              * no compositing needed\
    \ if so.\n              *\n              * We show the whole gamma decode/correct\
    \ process in\n              * floating point, but it would more likely be done\n\
    \              * with lookup tables.\n              */\n         13  ialpha =\
    \ foreground[3];\n         14  if (ialpha == 0) {\n                 /*\n     \
    \             * Foreground image is transparent here.\n                  * If\
    \ the background image is already in the frame\n                  * buffer, there\
    \ is nothing to do.\n                  */\n         15      ;\n         16  }\
    \ else if (ialpha == fg_maxsample) {\n                 /*\n                  *\
    \ Copy foreground pixel to frame buffer.\n                  */\n         17  \
    \    for (i = 0; i < 3; i++) {\n         18          gamfg = (float) foreground[i]\
    \ / fg_maxsample;\n         19          linfg = pow(gamfg, 1.0/fg_gamma);\n  \
    \       20          comppix = linfg;\n         21          gcvideo = pow(comppix,viewing_gamma/display_gamma);\n\
    \         22          fbpix[i] = (int) (gcvideo * fb_maxsample + 0.5);\n     \
    \    23      }\n         24  } else {\n                 /*\n                 \
    \ * Compositing is necessary.\n                  * Get floating-point alpha and\
    \ its complement.\n                  * Note: alpha is always linear; gamma does\
    \ not\n                  * affect it.\n                  */\n         25     \
    \ alpha = (float) ialpha / fg_maxsample;\n         26      compalpha = 1.0 - alpha;\n\
    \         27      for (i = 0; i < 3; i++) {\n                     /*\n       \
    \               * Convert foreground and background to floating\n            \
    \          * point, then linearize (undo gamma encoding).\n                  \
    \    */\n         28          gamfg = (float) foreground[i] / fg_maxsample;\n\
    \         29          linfg = pow(gamfg, 1.0/fg_gamma);\n         30         \
    \ gambg = (float) background[i] / bg_maxsample;\n         31          linbg =\
    \ pow(gambg, 1.0/bg_gamma);\n                     /*\n                      *\
    \ Composite.\n                      */\n         32          comppix = linfg *\
    \ alpha + linbg * compalpha;\n                     /*\n                      *\
    \ Gamma correct for display.\n                      * Convert to integer frame\
    \ buffer pixel.\n                      */\n         33          gcvideo = pow(comppix,viewing_gamma/display_gamma);\n\
    \         34          fbpix[i] = (int) (gcvideo * fb_maxsample + 0.5);\n     \
    \    35      }\n         36  }\n      Variations:\n          * If output is to\
    \ another PNG image file instead of a frame\n            buffer, lines 21, 22,\
    \ 33, and 34 should be changed to be\n            something like\n           \
    \    /*\n                * Gamma encode for storage in output file.\n        \
    \        * Convert to integer sample value.\n                */\n            \
    \   gamout = pow(comppix, outfile_gamma);\n               outpix[i] = (int) (gamout\
    \ * out_maxsample + 0.5);\n            Also, it becomes necessary to process background\
    \ pixels when\n            alpha is zero, rather than just skipping pixels.  Thus,\
    \ line\n            15 will need to be replaced by copies of lines 17-23, but\n\
    \            processing background instead of foreground pixel values.\n     \
    \     * If the sample depths of the output file, foreground file,\n          \
    \  and background file are all the same, and the three gamma\n            values\
    \ also match, then the no-compositing code in lines\n            14-23 reduces\
    \ to nothing more than copying pixel values from\n            the input file to\
    \ the output file if alpha is one, or\n            copying pixel values from background\
    \ to output file if alpha\n            is zero.  Since alpha is typically either\
    \ zero or one for\n            the vast majority of pixels in an image, this is\
    \ a great\n            savings.  No gamma computations are needed for most pixels.\n\
    \          * When the sample depths and gamma values all match, it may\n     \
    \       appear attractive to skip the gamma decoding and encoding\n          \
    \  (lines 28-31, 33-34) and just perform line 32 using gamma-\n            encoded\
    \ sample values. Although this doesn't hurt image\n            quality too badly,\
    \ the time savings are small if alpha\n            values of zero and one are\
    \ special-cased as recommended\n            here.\n          * If the original\
    \ pixel values of the background image are no\n            longer available, only\
    \ processed frame buffer pixels left by\n            display of the background\
    \ image, then lines 30 and 31 need\n            to extract intensity from the\
    \ frame buffer pixel values\n            using code like\n               /*\n\
    \                * Decode frame buffer value back into linear space.\n       \
    \         */\n               gcvideo = (float) fbpix[i] / fb_maxsample;\n    \
    \           linbg = pow(gcvideo, display_gamma / viewing_gamma);\n           \
    \ However, some roundoff error can result, so it is better to\n            have\
    \ the original background pixels available if at all\n            possible.\n\
    \          * Note that lines 18-22 are performing exactly the same gamma\n   \
    \         computation that is done when no alpha channel is present.\n       \
    \     So, if you handle the no-alpha case with a lookup table, you\n         \
    \   can use the same lookup table here.  Lines 28-31 and 33-34\n            can\
    \ also be done with (different) lookup tables.\n          * Of course, everything\
    \ here can be done in integer\n            arithmetic.  Just be careful to maintain\
    \ sufficient\n            precision all the way through.\n      Note: in floating\
    \ point, no overflow or underflow checks are\n      needed, because the input\
    \ sample values are guaranteed to be\n      between 0 and 1, and compositing always\
    \ yields a result that is in\n      between the input values (inclusive).  With\
    \ integer arithmetic,\n      some roundoff-error analysis might be needed to guarantee\
    \ no\n      overflow or underflow.\n      When displaying a PNG image with full\
    \ alpha channel, it is\n      important to be able to composite the image against\
    \ some\n      background, even if it's only black.  Ignoring the alpha channel\n\
    \      will cause PNG images that have been converted from an\n      associated-alpha\
    \ representation to look wrong.  (Of course, if the\n      alpha channel is a\
    \ separate transparency mask, then ignoring alpha\n      is a useful option: it\
    \ allows the hidden parts of the image to be\n      recovered.)\n      Even if\
    \ the decoder author does not wish to implement true\n      compositing logic,\
    \ it is simple to deal with images that contain\n      only zero and one alpha\
    \ values.  (This is implicitly true for\n      grayscale and truecolor PNG files\
    \ that use a tRNS chunk; for\n      indexed-color PNG files, it is easy to check\
    \ whether tRNS contains\n      any values other than 0 and 255.)  In this simple\
    \ case,\n      transparent pixels are replaced by the background color, while\n\
    \      others are unchanged.  If a decoder contains only this much\n      transparency\
    \ capability, it should deal with a full alpha channel\n      by treating all\
    \ nonzero alpha values as fully opaque; that is, do\n      not replace partially\
    \ transparent pixels by the background.  This\n      approach will not yield very\
    \ good results for images converted\n      from associated-alpha formats, but\
    \ it's better than doing nothing.\n   10.9. Progressive display\n      When receiving\
    \ images over slow transmission links, decoders can\n      improve perceived performance\
    \ by displaying interlaced images\n      progressively.  This means that as each\
    \ pass is received, an\n      approximation to the complete image is displayed\
    \ based on the data\n      received so far.  One simple yet pleasing effect can\
    \ be obtained\n      by expanding each received pixel to fill a rectangle covering\
    \ the\n      yet-to-be-transmitted pixel positions below and to the right of\n\
    \      the received pixel.  This process can be described by the\n      following\
    \ pseudocode:\n         Starting_Row [1..7] =  { 0, 0, 4, 0, 2, 0, 1 }\n     \
    \    Starting_Col [1..7] =  { 0, 4, 0, 2, 0, 1, 0 }\n         Row_Increment [1..7]\
    \ = { 8, 8, 8, 4, 4, 2, 2 }\n         Col_Increment [1..7] = { 8, 8, 4, 4, 2,\
    \ 2, 1 }\n         Block_Height [1..7] =  { 8, 8, 4, 4, 2, 2, 1 }\n         Block_Width\
    \ [1..7] =   { 8, 4, 4, 2, 2, 1, 1 }\n         pass := 1\n         while pass\
    \ <= 7\n         begin\n             row := Starting_Row[pass]\n             while\
    \ row < height\n             begin\n                 col := Starting_Col[pass]\n\
    \                 while col < width\n                 begin\n                \
    \     visit (row, col,\n                            min (Block_Height[pass], height\
    \ - row),\n                            min (Block_Width[pass], width - col))\n\
    \                     col := col + Col_Increment[pass]\n                 end\n\
    \                 row := row + Row_Increment[pass]\n             end\n       \
    \      pass := pass + 1\n         end\n      Here, the function \"visit(row,column,height,width)\"\
    \ obtains the\n      next transmitted pixel and paints a rectangle of the specified\n\
    \      height and width, whose upper-left corner is at the specified row\n   \
    \   and column, using the color indicated by the pixel.  Note that row\n     \
    \ and column are measured from 0,0 at the upper left corner.\n      If the decoder\
    \ is merging the received image with a background\n      image, it may be more\
    \ convenient just to paint the received pixel\n      positions; that is, the \"\
    visit()\" function sets only the pixel at\n      the specified row and column,\
    \ not the whole rectangle.  This\n      produces a \"fade-in\" effect as the new\
    \ image gradually replaces\n      the old.  An advantage of this approach is that\
    \ proper alpha or\n      transparency processing can be done as each pixel is\
    \ replaced.\n      Painting a rectangle as described above will overwrite\n  \
    \    background-image pixels that may be needed later, if the pixels\n      eventually\
    \ received for those positions turn out to be wholly or\n      partially transparent.\
    \  Of course, this is only a problem if the\n      background image is not stored\
    \ anywhere offscreen.\n   10.10. Suggested-palette and histogram usage\n     \
    \ In truecolor PNG files, the encoder may have provided a suggested\n      PLTE\
    \ chunk for use by viewers running on indexed-color hardware.\n      If the image\
    \ has a tRNS chunk, the viewer will need to adapt the\n      suggested palette\
    \ for use with its desired background color.  To\n      do this, replace the palette\
    \ entry closest to the tRNS color with\n      the desired background color; or\
    \ just add a palette entry for the\n      background color, if the viewer can\
    \ handle more colors than there\n      are PLTE entries.\n      For images of\
    \ color type 6 (truecolor with alpha channel), any\n      suggested palette should\
    \ have been designed for display of the\n      image against a uniform background\
    \ of the color specified by bKGD.\n      Viewers should probably ignore the palette\
    \ if they intend to use a\n      different background, or if the bKGD chunk is\
    \ missing.  Viewers\n      can use a suggested palette for display against a different\n\
    \      background than it was intended for, but the results may not be\n     \
    \ very good.\n      If the viewer presents a transparent truecolor image against\
    \ a\n      background that is more complex than a single color, it is\n      unlikely\
    \ that the suggested palette will be optimal for the\n      composite image. \
    \ In this case it is best to perform a truecolor\n      compositing step on the\
    \ truecolor PNG image and background image,\n      then color-quantize the resulting\
    \ image.\n      The histogram chunk is useful when the viewer cannot provide as\n\
    \      many colors as are used in the image's palette.  If the viewer is\n   \
    \   only short a few colors, it is usually adequate to drop the\n      least-used\
    \ colors from the palette.  To reduce the number of\n      colors substantially,\
    \ it's best to choose entirely new\n      representative colors, rather than trying\
    \ to use a subset of the\n      existing palette.  This amounts to performing\
    \ a new color\n      quantization step; however, the existing palette and histogram\
    \ can\n      be used as the input data, thus avoiding a scan of the image data.\n\
    \      If no palette or histogram chunk is provided, a decoder can\n      develop\
    \ its own, at the cost of an extra pass over the image data.\n      Alternatively,\
    \ a default palette (probably a color cube) can be\n      used.\n      See also\
    \ Recommendations for Encoders: Suggested palettes (Section\n      9.5).\n   10.11.\
    \ Text chunk processing\n      If practical, decoders should have a way to display\
    \ to the user\n      all tEXt and zTXt chunks found in the file.  Even if the\
    \ decoder\n      does not recognize a particular text keyword, the user might\
    \ be\n      able to understand it.\n      PNG text is not supposed to contain\
    \ any characters outside the ISO\n      8859-1 \"Latin-1\" character set (that\
    \ is, no codes 0-31 or 127-\n      159), except for the newline character (decimal\
    \ 10).  But decoders\n      might encounter such characters anyway.  Some of these\
    \ characters\n      can be safely displayed (e.g., TAB, FF, and CR, decimal 9,\
    \ 12, and\n      13, respectively), but others, especially the ESC character\n\
    \      (decimal 27), could pose a security hazard because unexpected\n      actions\
    \ may be taken by display hardware or software.  To prevent\n      such hazards,\
    \ decoders should not attempt to directly display any\n      non-Latin-1 characters\
    \ (except for newline and perhaps TAB, FF,\n      CR) encountered in a tEXt or\
    \ zTXt chunk.  Instead, ignore them or\n      display them in a visible notation\
    \ such as \"\\nnn\".  See Security\n      considerations (Section 8.5).\n    \
    \  Even though encoders are supposed to represent newlines as LF, it\n      is\
    \ recommended that decoders not rely on this; it's best to\n      recognize all\
    \ the common newline combinations (CR, LF, and CR-LF)\n      and display each\
    \ as a single newline.  TAB can be expanded to the\n      proper number of spaces\
    \ needed to arrive at a column multiple of\n      8.\n      Decoders running on\
    \ systems with non-Latin-1 character set\n      encoding should provide character\
    \ code remapping so that Latin-1\n      characters are displayed correctly.  Some\
    \ systems may not provide\n      all the characters defined in Latin-1.  Mapping\
    \ unavailable\n      characters to a visible notation such as \"\\nnn\" is a good\n\
    \      fallback.  In particular, character codes 127-255 should be\n      displayed\
    \ only if they are printable characters on the decoding\n      system.  Some systems\
    \ may interpret such codes as control\n      characters; for security, decoders\
    \ running on such systems should\n      not display such characters literally.\n\
    \      Decoders should be prepared to display text chunks that contain\n     \
    \ any number of printing characters between newline characters, even\n      though\
    \ encoders are encouraged to avoid creating lines in excess\n      of 79 characters.\n"
- title: 11. Glossary
  contents:
  - "11. Glossary\n   a^b\n      Exponentiation; a raised to the power b.  C programmers\
    \ should be\n      careful not to misread this notation as exclusive-or.  Note\
    \ that\n      in gamma-related calculations, zero raised to any power is valid\n\
    \      and must give a zero result.\n   Alpha\n      A value representing a pixel's\
    \ degree of transparency.  The more\n      transparent a pixel, the less it hides\
    \ the background against\n      which the image is presented.  In PNG, alpha is\
    \ really the degree\n      of opacity: zero alpha represents a completely transparent\
    \ pixel,\n      maximum alpha represents a completely opaque pixel.  But most\n\
    \      people refer to alpha as providing transparency information, not\n    \
    \  opacity information, and we continue that custom here.\n   Ancillary chunk\n\
    \      A chunk that provides additional information.  A decoder can still\n  \
    \    produce a meaningful image, though not necessarily the best\n      possible\
    \ image, without processing the chunk.\n   Bit depth\n      The number of bits\
    \ per palette index (in indexed-color PNGs) or\n      per sample (in other color\
    \ types).  This is the same value that\n      appears in IHDR.\n   Byte\n    \
    \  Eight bits; also called an octet.\n   Channel\n      The set of all samples\
    \ of the same kind within an image; for\n      example, all the blue samples in\
    \ a truecolor image.  (The term\n      \"component\" is also used, but not in\
    \ this specification.)  A\n      sample is the intersection of a channel and a\
    \ pixel.\n   Chromaticity\n      A pair of values x,y that precisely specify the\
    \ hue, though not\n      the absolute brightness, of a perceived color.\n   Chunk\n\
    \      A section of a PNG file.  Each chunk has a type indicated by its\n    \
    \  chunk type name.  Most types of chunks also include some data.\n      The format\
    \ and meaning of the data within the chunk are determined\n      by the type name.\n\
    \   Composite\n      As a verb, to form an image by merging a foreground image\
    \ and a\n      background image, using transparency information to determine\n\
    \      where the background should be visible.  The foreground image is\n    \
    \  said to be \"composited against\" the background.\n   CRC\n      Cyclic Redundancy\
    \ Check.  A CRC is a type of check value designed\n      to catch most transmission\
    \ errors.  A decoder calculates the CRC\n      for the received data and compares\
    \ it to the CRC that the encoder\n      calculated, which is appended to the data.\
    \  A mismatch indicates\n      that the data was corrupted in transit.\n   Critical\
    \ chunk\n      A chunk that must be understood and processed by the decoder in\n\
    \      order to produce a meaningful image from a PNG file.\n   CRT\n      Cathode\
    \ Ray Tube: a common type of computer display hardware.\n   Datastream\n     \
    \ A sequence of bytes.  This term is used rather than \"file\" to\n      describe\
    \ a byte sequence that is only a portion of a file.  We\n      also use it to\
    \ emphasize that a PNG image might be generated and\n      consumed \"on the fly\"\
    , never appearing in a stored file at all.\n   Deflate\n      The name of the\
    \ compression algorithm used in standard PNG files,\n      as well as in zip,\
    \ gzip, pkzip, and other compression programs.\n      Deflate is a member of the\
    \ LZ77 family of compression methods.\n   Filter\n      A transformation applied\
    \ to image data in hopes of improving its\n      compressibility.  PNG uses only\
    \ lossless (reversible) filter\n      algorithms.\n   Frame buffer\n      The\
    \ final digital storage area for the image shown by a computer\n      display.\
    \  Software causes an image to appear onscreen by loading\n      it into the frame\
    \ buffer.\n   Gamma\n      The brightness of mid-level tones in an image.  More\
    \ precisely, a\n      parameter that describes the shape of the transfer function\
    \ for\n      one or more stages in an imaging pipeline.  The transfer function\n\
    \      is given by the expression\n         output = input ^ gamma\n      where\
    \ both input and output are scaled to the range 0 to 1.\n   Grayscale\n      An\
    \ image representation in which each pixel is represented by a\n      single sample\
    \ value representing overall luminance (on a scale\n      from black to white).\
    \  PNG also permits an alpha sample to be\n      stored for each pixel of a grayscale\
    \ image.\n   Indexed color\n      An image representation in which each pixel\
    \ is represented by a\n      single sample that is an index into a palette or\
    \ lookup table.\n      The selected palette entry defines the actual color of\
    \ the pixel.\n   Lossless compression\n      Any method of data compression that\
    \ guarantees the original data\n      can be reconstructed exactly, bit-for-bit.\n\
    \   Lossy compression\n      Any method of data compression that reconstructs\
    \ the original data\n      approximately, rather than exactly.\n   LSB\n     \
    \ Least Significant Byte of a multi-byte value.\n   Luminance\n      Perceived\
    \ brightness, or grayscale level, of a color.  Luminance\n      and chromaticity\
    \ together fully define a perceived color.\n   LUT\n      Look Up Table.  In general,\
    \ a table used to transform data.  In\n      frame buffer hardware, a LUT can\
    \ be used to map indexed-color\n      pixels into a selected set of truecolor\
    \ values, or to perform\n      gamma correction.  In software, a LUT can be used\
    \ as a fast way of\n      implementing any one-variable mathematical function.\n\
    \   MSB\n      Most Significant Byte of a multi-byte value.\n   Palette\n    \
    \  The set of colors available in an indexed-color image.  In PNG, a\n      palette\
    \ is an array of colors defined by red, green, and blue\n      samples.  (Alpha\
    \ values can also be defined for palette entries,\n      via the tRNS chunk.)\n\
    \   Pixel\n      The information stored for a single grid point in the image.\
    \  The\n      complete image is a rectangular array of pixels.\n   PNG editor\n\
    \      A program that modifies a PNG file and preserves ancillary\n      information,\
    \ including chunks that it does not recognize.  Such a\n      program must obey\
    \ the rules given in Chunk Ordering Rules (Chapter\n      7).\n   Sample\n   \
    \   A single number in the image data; for example, the red value of a\n     \
    \ pixel.  A pixel is composed of one or more samples.  When\n      discussing\
    \ physical data layout (in particular, in Image layout,\n      Section 2.3), we\
    \ use \"sample\" to mean a number stored in the image\n      array.  It would\
    \ be more precise but much less readable to say\n      \"sample or palette index\"\
    \ in that context.  Elsewhere in the\n      specification, \"sample\" means a\
    \ color value or alpha value.  In\n      the indexed-color case, these are palette\
    \ entries not palette\n      indexes.\n   Sample depth\n      The precision, in\
    \ bits, of color values and alpha values.  In\n      indexed-color PNGs the sample\
    \ depth is always 8 by definition of\n      the PLTE chunk.  In other color types\
    \ it is the same as the bit\n      depth.\n   Scanline\n      One horizontal row\
    \ of pixels within an image.\n   Truecolor\n      An image representation in which\
    \ pixel colors are defined by\n      storing three samples for each pixel, representing\
    \ red, green, and\n      blue intensities respectively.  PNG also permits an alpha\
    \ sample\n      to be stored for each pixel of a truecolor image.\n   White point\n\
    \      The chromaticity of a computer display's nominal white value.\n   zlib\n\
    \      A particular format for data that has been compressed using\n      deflate-style\
    \ compression.  Also the name of a library\n      implementing this method.  PNG\
    \ implementations need not use the\n      zlib library, but they must conform\
    \ to its format for compressed\n      data.\n"
- title: '12. Appendix: Rationale'
  contents:
  - "12. Appendix: Rationale\n   (This appendix is not part of the formal PNG specification.)\n\
    \   This appendix gives the reasoning behind some of the design decisions\n  \
    \ in PNG.  Many of these decisions were the subject of considerable\n   debate.\
    \  The authors freely admit that another group might have made\n   different decisions;\
    \ however, we believe that our choices are\n   defensible and consistent.\n  \
    \ 12.1. Why a new file format?\n      Does the world really need yet another graphics\
    \ format?  We\n      believe so.  GIF is no longer freely usable, but no other\
    \ commonly\n      used format can directly replace it, as is discussed in more\n\
    \      detail below.  We might have used an adaptation of an existing\n      format,\
    \ for example GIF with an unpatented compression scheme.\n      But this would\
    \ require new code anyway; it would not be all that\n      much easier to implement\
    \ than a whole new file format.  (PNG is\n      designed to be simple to implement,\
    \ with the exception of the\n      compression engine, which would be needed in\
    \ any case.)  We feel\n      that this is an excellent opportunity to design a\
    \ new format that\n      fixes some of the known limitations of GIF.\n   12.2.\
    \ Why these features?\n      The features chosen for PNG are intended to address\
    \ the needs of\n      applications that previously used the special strengths\
    \ of GIF.\n      In particular, GIF is well adapted for online communications\n\
    \      because of its streamability and progressive display capability.\n    \
    \  PNG shares those attributes.\n      We have also addressed some of the widely\
    \ known shortcomings of\n      GIF.  In particular, PNG supports truecolor images.\
    \  We know of no\n      widely used image format that losslessly compresses truecolor\n\
    \      images as effectively as PNG does.  We hope that PNG will make use\n  \
    \    of truecolor images more practical and widespread.\n      Some form of transparency\
    \ control is desirable for applications in\n      which images are displayed against\
    \ a background or together with\n      other images.  GIF provided a simple transparent-color\n\
    \      specification for this purpose.  PNG supports a full alpha channel\n  \
    \    as well as transparent-color specifications.  This allows both\n      highly\
    \ flexible transparency and compression efficiency.\n      Robustness against\
    \ transmission errors has been an important\n      consideration.  For example,\
    \ images transferred across Internet\n      are often mistakenly processed as\
    \ text, leading to file\n      corruption.  PNG is designed so that such errors\
    \ can be detected\n      quickly and reliably.\n      PNG has been expressly designed\
    \ not to be completely dependent on\n      a single compression technique. Although\
    \ deflate/inflate\n      compression is mentioned in this document, PNG would\
    \ still exist\n      without it.\n   12.3. Why not these features?\n      Some\
    \ features have been deliberately omitted from PNG.  These\n      choices were\
    \ made to simplify implementation of PNG, promote\n      portability and interchangeability,\
    \ and make the format as simple\n      and foolproof as possible for users.  In\
    \ particular:\n          * There is no uncompressed variant of PNG.  It is possible\
    \ to\n            store uncompressed data by using only uncompressed deflate\n\
    \            blocks (a feature normally used to guarantee that deflate\n     \
    \       does not make incompressible data much larger).  However,\n          \
    \  PNG software must support full deflate/inflate; any software\n            that\
    \ does not is not compliant with the PNG standard. The\n            two most important\
    \ features of PNG---portability and\n            compression---are absolute requirements\
    \ for online\n            applications, and users demand them. Failure to support\
    \ full\n            deflate/inflate compromises both of these objectives.\n  \
    \        * There is no lossy compression in PNG.  Existing formats such\n    \
    \        as JFIF already handle lossy compression well.  Furthermore,\n      \
    \      available lossy compression methods (e.g., JPEG) are far\n            from\
    \ foolproof --- a poor choice of quality level can ruin\n            an image.\
    \  To avoid user confusion and unintentional loss of\n            information,\
    \ we feel it is best to keep lossy and lossless\n            formats strictly\
    \ separate.  Also, lossy compression is\n            complex to implement.  Adding\
    \ JPEG support to a PNG decoder\n            might increase its size by an order\
    \ of magnitude.  This\n            would certainly cause some decoders to omit\
    \ support for the\n            feature, which would destroy our goal of interchangeability.\n\
    \          * There is no support for CMYK or other unusual color spaces.\n   \
    \         Again, this is in the name of promoting portability.  CMYK,\n      \
    \      in particular, is far too device-dependent to be useful as a\n        \
    \    portable image representation.\n          * There is no standard chunk for\
    \ thumbnail views of images.\n            In discussions with software vendors\
    \ who use thumbnails in\n            their products, it has become clear that\
    \ most would not use\n            a \"standard\" thumbnail chunk.  For one thing,\
    \ every vendor\n            has a different idea of what the dimensions and\n\
    \            characteristics of a thumbnail ought to be.  Also, some\n       \
    \     vendors keep thumbnails in separate files to accommodate\n            varied\
    \ image formats; they are not going to stop doing that\n            simply because\
    \ of a thumbnail chunk in one new format.\n            Proprietary chunks containing\
    \ vendor-specific thumbnails\n            appear to be more practical than a common\
    \ thumbnail format.\n      It is worth noting that private extensions to PNG could\
    \ easily add\n      these features.  We will not, however, include them as part\
    \ of the\n      basic PNG standard.\n      PNG also does not support multiple\
    \ images in one file.  This\n      restriction is a reflection of the reality\
    \ that many applications\n      do not need and will not support multiple images\
    \ per file.  In any\n      case, single images are a fundamentally different sort\
    \ of object\n      from sequences of images.  Rather than make false promises\
    \ of\n      interchangeability, we have drawn a clear distinction between\n  \
    \    single-image and multi-image formats.  PNG is a single-image\n      format.\
    \  (But see Multiple-image extension, Section 8.4.)\n   12.4. Why not use format\
    \ X?\n      Numerous existing formats were considered before deciding to\n   \
    \   develop PNG.  None could meet the requirements we felt were\n      important\
    \ for PNG.\n      GIF is no longer suitable as a universal standard because of\
    \ legal\n      entanglements.  Although just replacing GIF's compression method\n\
    \      would avoid that problem, GIF does not support truecolor images,\n    \
    \  alpha channels, or gamma correction.  The spec has more subtle\n      problems\
    \ too.  Only a small subset of the GIF89 spec is actually\n      portable across\
    \ a variety of implementations, but there is no\n      codification of the most\
    \ portable part of the spec.\n      TIFF is far too complex to meet our goals\
    \ of simplicity and\n      interchangeability.  Defining a TIFF subset would meet\
    \ that\n      objection, but would frustrate users making the reasonable\n   \
    \   assumption that a file saved as TIFF from their existing software\n      would\
    \ load into a program supporting our flavor of TIFF.\n      Furthermore, TIFF\
    \ is not designed for stream processing, has no\n      provision for progressive\
    \ display, and does not currently provide\n      any good, legally unencumbered,\
    \ lossless compression method.\n      IFF has also been suggested, but is not\
    \ suitable in detail:\n      available image representations are too machine-specific\
    \ or not\n      adequately compressed.  The overall chunk structure of IFF is\
    \ a\n      useful concept that PNG has liberally borrowed from, but we did\n \
    \     not attempt to be bit-for-bit compatible with IFF chunk structure.\n   \
    \   Again this is due to detailed issues, notably the fact that IFF\n      FORMs\
    \ are not designed to be serially writable.\n      Lossless JPEG is not suitable\
    \ because it does not provide for the\n      storage of indexed-color images.\
    \  Furthermore, its lossless\n      truecolor compression is often inferior to\
    \ that of PNG.\n   12.5. Byte order\n      It has been asked why PNG uses network\
    \ byte order.  We have\n      selected one byte ordering and used it consistently.\
    \ Which order\n      in particular is of little relevance, but network byte order\
    \ has\n      the advantage that routines to convert to and from it are already\n\
    \      available on any platform that supports TCP/IP networking,\n      including\
    \ all PC platforms.  The functions are trivial and will be\n      included in\
    \ the reference implementation.\n   12.6. Interlacing\n      PNG's two-dimensional\
    \ interlacing scheme is more complex to\n      implement than GIF's line-wise\
    \ interlacing.  It also costs a\n      little more in file size.  However, it\
    \ yields an initial image\n      eight times faster than GIF (the first pass transmits\
    \ only 1/64th\n      of the pixels, compared to 1/8th for GIF).  Although this\
    \ initial\n      image is coarse, it is useful in many situations.  For example,\
    \ if\n      the image is a World Wide Web imagemap that the user has seen\n  \
    \    before, PNG's first pass is often enough to determine where to\n      click.\
    \  The PNG scheme also looks better than GIF's, because\n      horizontal and\
    \ vertical resolution never differ by more than a\n      factor of two; this avoids\
    \ the odd \"stretched\" look seen when\n      interlaced GIFs are filled in by\
    \ replicating scanlines.\n      Preliminary results show that small text in an\
    \ interlaced PNG\n      image is typically readable about twice as fast as in\
    \ an\n      equivalent GIF, i.e., after PNG's fifth pass or 25% of the image\n\
    \      data, instead of after GIF's third pass or 50%.  This is again due\n  \
    \    to PNG's more balanced increase in resolution.\n   12.7. Why gamma?\n   \
    \   It might seem natural to standardize on storing sample values that\n     \
    \ are linearly proportional to light intensity (that is, have gamma\n      of\
    \ 1.0).  But in fact, it is common for images to have a gamma of\n      less than\
    \ 1.  There are three good reasons for this:\n          * For reasons detailed\
    \ in Gamma Tutorial (Chapter 13), all\n            video cameras apply a \"gamma\
    \ correction\" function to the\n            intensity information.  This causes\
    \ the video signal to have\n            a gamma of about 0.5 relative to the light\
    \ intensity in the\n            original scene.  Thus, images obtained by frame-grabbing\n\
    \            video already have a gamma of about 0.5.\n          * The human eye\
    \ has a nonlinear response to intensity, so\n            linear encoding of samples\
    \ either wastes sample codes in\n            bright areas of the image, or provides\
    \ too few sample codes\n            to avoid banding artifacts in dark areas of\
    \ the image, or\n            both.  At least 12 bits per sample are needed to\
    \ avoid\n            visible artifacts in linear encoding with a 100:1 image\n\
    \            intensity range.  An image gamma in the range 0.3 to 0.5\n      \
    \      allocates sample values in a way that roughly corresponds to\n        \
    \    the eye's response, so that 8 bits/sample are enough to\n            avoid\
    \ artifacts caused by insufficient sample precision in\n            almost all\
    \ images.  This makes \"gamma encoding\" a much\n            better way of storing\
    \ digital images than the simpler linear\n            encoding.\n          * Many\
    \ images are created on PCs or workstations with no gamma\n            correction\
    \ hardware and no software willing to provide gamma\n            correction either.\
    \  In these cases, the images have had\n            their lighting and color chosen\
    \ to look best on this\n            platform --- they can be thought of as having\
    \ \"manual\" gamma\n            correction built in.  To see what the image author\
    \ intended,\n            it is necessary to treat such images as having a file_gamma\n\
    \            value in the range 0.4-0.6, depending on the room lighting\n    \
    \        level that the author was working in.\n      In practice, image gamma\
    \ values around 1.0 and around 0.5 are both\n      widely found.  Older image\
    \ standards such as GIF often do not\n      account for this fact.  The JFIF standard\
    \ specifies that images in\n      that format should use linear samples, but many\
    \ JFIF images found\n      on the Internet actually have a gamma somewhere near\
    \ 0.4 or 0.5.\n      The variety of images found and the variety of systems that\
    \ people\n      display them on have led to widespread problems with images\n\
    \      appearing \"too dark\" or \"too light\".\n      PNG expects viewers to\
    \ compensate for image gamma at the time that\n      the image is displayed. Another\
    \ possible approach is to expect\n      encoders to convert all images to a uniform\
    \ gamma at encoding\n      time. While that method would speed viewers slightly,\
    \ it has\n      fundamental flaws:\n          * Gamma correction is inherently\
    \ lossy due to quantization and\n            roundoff error.  Requiring conversion\
    \ at encoding time thus\n            causes irreversible loss. Since PNG is intended\
    \ to be a\n            lossless storage format, this is undesirable; we should\n\
    \            store unmodified source data.\n          * The encoder might not\
    \ know the source gamma value. If the\n            decoder does gamma correction\
    \ at viewing time, it can adjust\n            the gamma (change the displayed\
    \ brightness) in response to\n            feedback from a human user. The encoder\
    \ has no such\n            recourse.\n          * Whatever \"standard\" gamma\
    \ we settled on would be wrong for\n            some displays. Hence viewers would\
    \ still need gamma\n            correction capability.\n      Since there will\
    \ always be images with no gamma or an incorrect\n      recorded gamma, good viewers\
    \ will need to incorporate gamma\n      adjustment code anyway. Gamma correction\
    \ at viewing time is thus\n      the right way to go.\n      See Gamma Tutorial\
    \ (Chapter 13) for more information.\n   12.8. Non-premultiplied alpha\n     \
    \ PNG uses \"unassociated\" or \"non-premultiplied\" alpha so that\n      images\
    \ with separate transparency masks can be stored losslessly.\n      Another common\
    \ technique, \"premultiplied alpha\", stores pixel\n      values premultiplied\
    \ by the alpha fraction; in effect, the image\n      is already composited against\
    \ a black background.  Any image data\n      hidden by the transparency mask is\
    \ irretrievably lost by that\n      method, since multiplying by a zero alpha\
    \ value always produces\n      zero.\n      Some image rendering techniques generate\
    \ images with premultiplied\n      alpha (the alpha value actually represents\
    \ how much of the pixel\n      is covered by the image).  This representation\
    \ can be converted to\n      PNG by dividing the sample values by alpha, except\
    \ where alpha is\n      zero.  The result will look good if displayed by a viewer\
    \ that\n      handles alpha properly, but will not look very good if the viewer\n\
    \      ignores the alpha channel.\n      Although each form of alpha storage has\
    \ its advantages, we did not\n      want to require all PNG viewers to handle\
    \ both forms.  We\n      standardized on non-premultiplied alpha as being the\
    \ lossless and\n      more general case.\n   12.9. Filtering\n      PNG includes\
    \ filtering capability because filtering can\n      significantly reduce the compressed\
    \ size of truecolor and\n      grayscale images.  Filtering is also sometimes\
    \ of value on\n      indexed-color images, although this is less common.\n   \
    \   The filter algorithms are defined to operate on bytes, rather than\n     \
    \ pixels; this gains simplicity and speed with very little cost in\n      compression\
    \ performance.  Tests have shown that filtering is\n      usually ineffective\
    \ for images with fewer than 8 bits per sample,\n      so providing pixelwise\
    \ filtering for such images would be\n      pointless.  For 16 bit/sample data,\
    \ bytewise filtering is nearly\n      as effective as pixelwise filtering, because\
    \ MSBs are predicted\n      from adjacent MSBs, and LSBs are predicted from adjacent\
    \ LSBs.\n      The encoder is allowed to change filters for each new scanline.\n\
    \      This creates no additional complexity for decoders, since a\n      decoder\
    \ is required to contain defiltering logic for every filter\n      type anyway.\
    \  The only cost is an extra byte per scanline in the\n      pre-compression datastream.\
    \  Our tests showed that when the same\n      filter is selected for all scanlines,\
    \ this extra byte compresses\n      away to almost nothing, so there is little\
    \ storage cost compared\n      to a fixed filter specified for the whole image.\
    \  And the\n      potential benefits of adaptive filtering are too great to ignore.\n\
    \      Even with the simplistic filter-choice heuristics so far\n      discovered,\
    \ adaptive filtering usually outperforms fixed filters.\n      In particular,\
    \ an adaptive filter can change behavior for\n      successive passes of an interlaced\
    \ image; a fixed filter cannot.\n   12.10. Text strings\n      Most graphics file\
    \ formats include the ability to store some\n      textual information along with\
    \ the image.  But many applications\n      need more than that: they want to be\
    \ able to store several\n      identifiable pieces of text.  For example, a database\
    \ using PNG\n      files to store medical X-rays would likely want to include\n\
    \      patient's name, doctor's name, etc.  A simple way to do this in\n     \
    \ PNG would be to invent new private chunks holding text.  The\n      disadvantage\
    \ of such an approach is that other applications would\n      have no idea what\
    \ was in those chunks, and would simply ignore\n      them.  Instead, we recommend\
    \ that textual information be stored in\n      standard tEXt chunks with suitable\
    \ keywords.  Use of tEXt tells\n      any PNG viewer that the chunk contains text\
    \ that might be of\n      interest to a human user.  Thus, a person looking at\
    \ the file with\n      another viewer will still be able to see the text, and\
    \ even\n      understand what it is if the keywords are reasonably self-\n   \
    \   explanatory.  (To this end, we recommend spelled-out keywords, not\n     \
    \ abbreviations that will be hard for a person to understand.\n      Saving a\
    \ few bytes on a keyword is false economy.)\n      The ISO 8859-1 (Latin-1) character\
    \ set was chosen as a compromise\n      between functionality and portability.\
    \  Some platforms cannot\n      display anything more than 7-bit ASCII characters,\
    \ while others\n      can handle characters beyond the Latin-1 set.  We felt that\n\
    \      Latin-1 represents a widely useful and reasonably portable\n      character\
    \ set.  Latin-1 is a direct subset of character sets\n      commonly used on popular\
    \ platforms such as Microsoft Windows and X\n      Windows.  It can also be handled\
    \ on Macintosh systems with a\n      simple remapping of characters.\n      There\
    \ is presently no provision for text employing character sets\n      other than\
    \ Latin-1. We recognize that the need for other character\n      sets will increase.\
    \  However, PNG already requires that\n      programmers implement a number of\
    \ new and unfamiliar features, and\n      text representation is not PNG's primary\
    \ purpose. Since PNG\n      provides for the creation and public registration\
    \ of new ancillary\n      chunks of general interest, we expect that text chunks\
    \ for other\n      character sets, such as Unicode, eventually will be registered\
    \ and\n      increase gradually in popularity.\n   12.11. PNG file signature\n\
    \      The first eight bytes of a PNG file always contain the following\n    \
    \  values:\n         (decimal)              137  80  78  71  13  10  26  10\n\
    \         (hexadecimal)           89  50  4e  47  0d  0a  1a  0a\n         (ASCII\
    \ C notation)    \\211   P   N   G  \\r  \\n \\032 \\n\n      This signature both\
    \ identifies the file as a PNG file and provides\n      for immediate detection\
    \ of common file-transfer problems.  The\n      first two bytes distinguish PNG\
    \ files on systems that expect the\n      first two bytes to identify the file\
    \ type uniquely.  The first\n      byte is chosen as a non-ASCII value to reduce\
    \ the probability that\n      a text file may be misrecognized as a PNG file;\
    \ also, it catches\n      bad file transfers that clear bit 7.  Bytes two through\
    \ four name\n      the format.  The CR-LF sequence catches bad file transfers\
    \ that\n      alter newline sequences.  The control-Z character stops file\n \
    \     display under MS-DOS.  The final line feed checks for the inverse\n    \
    \  of the CR-LF translation problem.\n      A decoder may further verify that\
    \ the next eight bytes contain an\n      IHDR chunk header with the correct chunk\
    \ length; this will catch\n      bad transfers that drop or alter null (zero)\
    \ bytes.\n      Note that there is no version number in the signature, nor indeed\n\
    \      anywhere in the file.  This is intentional: the chunk mechanism\n     \
    \ provides a better, more flexible way to handle format extensions,\n      as\
    \ explained in Chunk naming conventions (Section 12.13).\n   12.12. Chunk layout\n\
    \      The chunk design allows decoders to skip unrecognized or\n      uninteresting\
    \ chunks: it is simply necessary to skip the\n      appropriate number of bytes,\
    \ as determined from the length field.\n      Limiting chunk length to (2^31)-1\
    \ bytes avoids possible problems\n      for implementations that cannot conveniently\
    \ handle 4-byte\n      unsigned values.  In practice, chunks will usually be much\
    \ shorter\n      than that anyway.\n      A separate CRC is provided for each\
    \ chunk in order to detect\n      badly-transferred images as quickly as possible.\
    \  In particular,\n      critical data such as the image dimensions can be validated\
    \ before\n      being used.\n      The chunk length is excluded from the CRC so\
    \ that the CRC can be\n      calculated as the data is generated; this avoids\
    \ a second pass\n      over the data in cases where the chunk length is not known\
    \ in\n      advance.  Excluding the length from the CRC does not create any\n\
    \      extra risk of failing to discover file corruption, since if the\n     \
    \ length is wrong, the CRC check will fail: the CRC will be computed\n      on\
    \ the wrong set of bytes and then be tested against the wrong\n      value from\
    \ the file.\n   12.13. Chunk naming conventions\n      The chunk naming conventions\
    \ allow safe, flexible extension of the\n      PNG format.  This mechanism is\
    \ much better than a format version\n      number, because it works on a feature-by-feature\
    \ basis rather than\n      being an overall indicator.  Decoders can process newer\
    \ files if\n      and only if the files use no unknown critical features (as\n\
    \      indicated by finding unknown critical chunks).  Unknown ancillary\n   \
    \   chunks can be safely ignored.  We decided against having an\n      overall\
    \ format version number because experience has shown that\n      format version\
    \ numbers hurt portability as much as they help.\n      Version numbers tend to\
    \ be set unnecessarily high, leading to\n      older decoders rejecting files\
    \ that they could have processed\n      (this was a serious problem for several\
    \ years after the GIF89 spec\n      came out, for example).  Furthermore, private\
    \ extensions can be\n      made either critical or ancillary, and standard decoders\
    \ should\n      react appropriately; overall version numbers are no help for\n\
    \      private extensions.\n      A hypothetical chunk for vector graphics would\
    \ be a critical\n      chunk, since if ignored, important parts of the intended\
    \ image\n      would be missing.  A chunk carrying the Mandelbrot set coordinates\n\
    \      for a fractal image would be ancillary, since other applications\n    \
    \  could display the image without understanding what the image\n      represents.\
    \  In general, a chunk type should be made critical only\n      if it is impossible\
    \ to display a reasonable representation of the\n      intended image without\
    \ interpreting that chunk.\n      The public/private property bit ensures that\
    \ any newly defined\n      public chunk type name cannot conflict with proprietary\
    \ chunks\n      that could be in use somewhere.  However, this does not protect\n\
    \      users of private chunk names from the possibility that someone\n      else\
    \ may use the same chunk name for a different purpose.  It is a\n      good idea\
    \ to put additional identifying information at the start\n      of the data for\
    \ any private chunk type.\n      When a PNG file is modified, certain ancillary\
    \ chunks may need to\n      be changed to reflect changes in other chunks. For\
    \ example, a\n      histogram chunk needs to be changed if the image data changes.\
    \  If\n      the file editor does not recognize histogram chunks, copying them\n\
    \      blindly to a new output file is incorrect; such chunks should be\n    \
    \  dropped.  The safe/unsafe property bit allows ancillary chunks to\n      be\
    \ marked appropriately.\n      Not all possible modification scenarios are covered\
    \ by the\n      safe/unsafe semantics.  In particular, chunks that are dependent\n\
    \      on the total file contents are not supported.  (An example of such\n  \
    \    a chunk is an index of IDAT chunk locations within the file:\n      adding\
    \ a comment chunk would inadvertently break the index.)\n      Definition of such\
    \ chunks is discouraged.  If absolutely necessary\n      for a particular application,\
    \ such chunks can be made critical\n      chunks, with consequent loss of portability\
    \ to other applications.\n      In general, ancillary chunks can depend on critical\
    \ chunks but not\n      on other ancillary chunks.  It is expected that mutually\
    \ dependent\n      information should be put into a single chunk.\n      In some\
    \ situations it may be unavoidable to make one ancillary\n      chunk dependent\
    \ on another.  Although the chunk property bits are\n      insufficient to represent\
    \ this case, a simple solution is\n      available: in the dependent chunk, record\
    \ the CRC of the chunk\n      depended on.  It can then be determined whether\
    \ that chunk has\n      been changed by some other program.\n      The same technique\
    \ can be useful for other purposes.  For example,\n      if a program relies on\
    \ the palette being in a particular order, it\n      can store a private chunk\
    \ containing the CRC of the PLTE chunk.\n      If this value matches when the\
    \ file is again read in, then it\n      provides high confidence that the palette\
    \ has not been tampered\n      with.  Note that it is not necessary to mark the\
    \ private chunk\n      unsafe-to-copy when this technique is used; thus, such\
    \ a private\n      chunk can survive other editing of the file.\n   12.14. Palette\
    \ histograms\n      A viewer may not be able to provide as many colors as are\
    \ listed\n      in the image's palette.  (For example, some colors could be\n\
    \      reserved by a window system.)  To produce the best results in this\n  \
    \    situation, it is helpful to have information about the frequency\n      with\
    \ which each palette index actually appears, in order to choose\n      the best\
    \ palette for dithering or to drop the least-used colors.\n      Since images\
    \ are often created once and viewed many times, it\n      makes sense to calculate\
    \ this information in the encoder, although\n      it is not mandatory for the\
    \ encoder to provide it.\n      Other image formats have usually addressed this\
    \ problem by\n      specifying that the palette entries should appear in order\
    \ of\n      frequency of use.  That is an inferior solution, because it\n    \
    \  doesn't give the viewer nearly as much information: the viewer\n      can't\
    \ determine how much damage will be done by dropping the last\n      few colors.\
    \  Nor does a sorted palette give enough information to\n      choose a target\
    \ palette for dithering, in the case that the viewer\n      needs to reduce the\
    \ number of colors substantially.  A palette\n      histogram provides the information\
    \ needed to choose such a target\n      palette without making a pass over the\
    \ image data.\n"
- title: '13. Appendix: Gamma Tutorial'
  contents:
  - "13. Appendix: Gamma Tutorial\n   (This appendix is not part of the formal PNG\
    \ specification.)\n   It would be convenient for graphics programmers if all of\
    \ the\n   components of an imaging system were linear.  The voltage coming from\n\
    \   an electronic camera would be directly proportional to the intensity\n   (power)\
    \ of light in the scene, the light emitted by a CRT would be\n   directly proportional\
    \ to its input voltage, and so on.  However,\n   real-world devices do not behave\
    \ in this way.  All CRT displays,\n   almost all photographic film, and many electronic\
    \ cameras have\n   nonlinear signal-to-light-intensity or intensity-to-signal\n\
    \   characteristics.\n   Fortunately, all of these nonlinear devices have a transfer\
    \ function\n   that is approximated fairly well by a single type of mathematical\n\
    \   function: a power function.  This power function has the general\n   equation\n\
    \      output = input ^ gamma\n   where ^ denotes exponentiation, and \"gamma\"\
    \ (often printed using the\n   Greek letter gamma, thus the name) is simply the\
    \ exponent of the\n   power function.\n   By convention, \"input\" and \"output\"\
    \ are both scaled to the range\n   0..1, with 0 representing black and 1 representing\
    \ maximum white (or\n   red, etc).  Normalized in this way, the power function\
    \ is completely\n   described by a single number, the exponent \"gamma\".\n  \
    \ So, given a particular device, we can measure its output as a\n   function of\
    \ its input, fit a power function to this measured transfer\n   function, extract\
    \ the exponent, and call it gamma.  We often say\n   \"this device has a gamma\
    \ of 2.5\" as a shorthand for \"this device has\n   a power-law response with\
    \ an exponent of 2.5\".  We can also talk\n   about the gamma of a mathematical\
    \ transform, or of a lookup table in\n   a frame buffer, so long as the input\
    \ and output of the thing are\n   related by the power-law expression above.\n\
    \   How do gammas combine?\n      Real imaging systems will have several components,\
    \ and more than\n      one of these can be nonlinear.  If all of the components\
    \ have\n      transfer characteristics that are power functions, then the\n  \
    \    transfer function of the entire system is also a power function.\n      The\
    \ exponent (gamma) of the whole system's transfer function is\n      just the\
    \ product of all of the individual exponents (gammas) of\n      the separate stages\
    \ in the system.\n      Also, stages that are linear pose no problem, since a\
    \ power\n      function with an exponent of 1.0 is really a linear function. \
    \ So\n      a linear transfer function is just a special case of a power\n   \
    \   function, with a gamma of 1.0.\n      Thus, as long as our imaging system\
    \ contains only stages with\n      linear and power-law transfer functions, we\
    \ can meaningfully talk\n      about the gamma of the entire system.  This is\
    \ indeed the case\n      with most real imaging systems.\n   What should overall\
    \ gamma be?\n      If the overall gamma of an imaging system is 1.0, its output\
    \ is\n      linearly proportional to its input.  This means that the ratio\n \
    \     between the intensities of any two areas in the reproduced image\n     \
    \ will be the same as it was in the original scene.  It might seem\n      that\
    \ this should always be the goal of an imaging system: to\n      accurately reproduce\
    \ the tones of the original scene.  Alas, that\n      is not the case.\n     \
    \ When the reproduced image is to be viewed in \"bright surround\"\n      conditions,\
    \ where other white objects nearby in the room have\n      about the same brightness\
    \ as white in the image, then an overall\n      gamma of 1.0 does indeed give\
    \ real-looking reproduction of a\n      natural scene.  Photographic prints viewed\
    \ under room light and\n      computer displays in bright room light are typical\
    \ \"bright\n      surround\" viewing conditions.\n      However, sometimes images\
    \ are intended to be viewed in \"dark\n      surround\" conditions, where the\
    \ room is substantially black except\n      for the image.  This is typical of\
    \ the way movies and slides\n      (transparencies) are viewed by projection.\
    \  Under these\n      circumstances, an accurate reproduction of the original\
    \ scene\n      results in an image that human viewers judge as \"flat\" and lacking\n\
    \      in contrast.  It turns out that the projected image needs to have\n   \
    \   a gamma of about 1.5 relative to the original scene for viewers to\n     \
    \ judge it \"natural\".  Thus, slide film is designed to have a gamma\n      of\
    \ about 1.5, not 1.0.\n      There is also an intermediate condition called \"\
    dim surround\",\n      where the rest of the room is still visible to the viewer,\
    \ but is\n      noticeably darker than the reproduced image itself.  This is\n\
    \      typical of television viewing, at least in the evening, as well as\n  \
    \    subdued-light computer work areas.  In dim surround conditions,\n      the\
    \ reproduced image needs to have a gamma of about 1.25 relative\n      to the\
    \ original scene in order to look natural.\n      The requirement for boosted\
    \ contrast (gamma) in dark surround\n      conditions is due to the way the human\
    \ visual system works, and\n      applies equally well to computer monitors. \
    \ Thus, a PNG viewer\n      trying to achieve the maximum realism for the images\
    \ it displays\n      really needs to know what the room lighting conditions are,\
    \ and\n      adjust the gamma of the displayed image accordingly.\n      If asking\
    \ the user about room lighting conditions is inappropriate\n      or too difficult,\
    \ just assume that the overall gamma\n      (viewing_gamma as defined below) should\
    \ be 1.0 or 1.25.  That's\n      all that most systems that implement gamma correction\
    \ do.\n   What is a CRT's gamma?\n      All CRT displays have a power-law transfer\
    \ characteristic with a\n      gamma of about 2.5.  This is due to the physical\
    \ processes\n      involved in controlling the electron beam in the electron gun,\
    \ and\n      has nothing to do with the phosphor.\n      An exception to this\
    \ rule is fancy \"calibrated\" CRTs that have\n      internal electronics to alter\
    \ their transfer function.  If you\n      have one of these, you probably should\
    \ believe what the\n      manufacturer tells you its gamma is.  But in all other\
    \ cases,\n      assuming 2.5 is likely to be pretty accurate.\n      There are\
    \ various images around that purport to measure gamma,\n      usually by comparing\
    \ the intensity of an area containing\n      alternating white and black with\
    \ a series of areas of continuous\n      gray of different intensity.  These are\
    \ usually not reliable.\n      Test images that use a \"checkerboard\" pattern\
    \ of black and white\n      are the worst, because a single white pixel will be\
    \ reproduced\n      considerably darker than a large area of white.  An image\
    \ that\n      uses alternating black and white horizontal lines (such as the\n\
    \      \"gamma.png\" test image at\n      ftp://ftp.uu.net/graphics/png/images/suite/gamma.png)\
    \ is much\n      better, but even it may be inaccurate at high \"picture\" settings\n\
    \      on some CRTs.\n      If you have a good photometer, you can measure the\
    \ actual light\n      output of a CRT as a function of input voltage and fit a\
    \ power\n      function to the measurements.  However, note that this procedure\n\
    \      is very sensitive to the CRT's black level adjustment, somewhat\n     \
    \ sensitive to its picture adjustment, and also affected by ambient\n      light.\
    \  Furthermore, CRTs spread some light from bright areas of\n      an image into\
    \ nearby darker areas; a single bright spot against a\n      black background\
    \ may be seen to have a \"halo\".  Your measuring\n      technique will need to\
    \ minimize the effects of this.\n      Because of the difficulty of measuring\
    \ gamma, using either test\n      images or measuring equipment, you're usually\
    \ better off just\n      assuming gamma is 2.5 rather than trying to measure it.\n\
    \   What is gamma correction?\n      A CRT has a gamma of 2.5, and we can't change\
    \ that.  To get an\n      overall gamma of 1.0 (or somewhere near that) for an\
    \ imaging\n      system, we need to have at least one other component of the \"\
    image\n      pipeline\" that is nonlinear.  If, in fact, there is only one\n \
    \     nonlinear stage in addition to the CRT, then it's traditional to\n     \
    \ say that the CRT has a certain gamma, and that the other nonlinear\n      stage\
    \ provides \"gamma correction\" to compensate for the CRT.\n      However, exactly\
    \ where the \"correction\" is done depends on\n      circumstance.\n      In all\
    \ broadcast video systems, gamma correction is done in the\n      camera.  This\
    \ choice was made in the days when television\n      electronics were all analog,\
    \ and a good gamma-correction circuit\n      was expensive to build.  The original\
    \ NTSC video standard required\n      cameras to have a transfer function with\
    \ a gamma of 1/2.2, or\n      about 0.45.  Recently, a more complex two-part transfer\
    \ function\n      has been adopted [SMPTE-170M], but its behavior can be well\n\
    \      approximated by a power function with a gamma of 0.5.  When the\n     \
    \ resulting image is displayed on a CRT with a gamma of 2.5, the\n      image\
    \ on screen ends up with a gamma of about 1.25 relative to the\n      original\
    \ scene, which is appropriate for \"dim surround\" viewing.\n      These days,\
    \ video signals are often digitized and stored in\n      computer frame buffers.\
    \  This works fine, but remember that gamma\n      correction is \"built into\"\
    \ the video signal, and so the digitized\n      video has a gamma of about 0.5\
    \ relative to the original scene.\n      Computer rendering programs often produce\
    \ linear samples.  To\n      display these correctly, intensity on the CRT needs\
    \ to be directly\n      proportional to the sample values in the frame buffer.\
    \  This can\n      be done with a special hardware lookup table between the frame\n\
    \      buffer and the CRT hardware.  The lookup table (often called LUT)\n   \
    \   is loaded with a mapping that implements a power function with a\n      gamma\
    \ of 0.4, thus providing \"gamma correction\" for the CRT gamma.\n      Thus,\
    \ gamma correction sometimes happens before the frame buffer,\n      sometimes\
    \ after.  As long as images created in a particular\n      environment are always\
    \ displayed in that environment, everything\n      is fine.  But when people try\
    \ to exchange images, differences in\n      gamma correction conventions often\
    \ result in images that seem far\n      too bright and washed out, or far too\
    \ dark and contrasty.\n   Gamma-encoded samples are good\n      So, is it better\
    \ to do gamma correction before or after the frame\n      buffer?\n      In an\
    \ ideal world, sample values would be stored in floating\n      point, there would\
    \ be lots of precision, and it wouldn't really\n      matter much.  But in reality,\
    \ we're always trying to store images\n      in as few bits as we can.\n     \
    \ If we decide to use samples that are linearly proportional to\n      intensity,\
    \ and do the gamma correction in the frame buffer LUT, it\n      turns out that\
    \ we need to use at least 12 bits for each of red,\n      green, and blue to have\
    \ enough precision in intensity.  With any\n      less than that, we will sometimes\
    \ see \"contour bands\" or \"Mach\n      bands\" in the darker areas of the image,\
    \ where two adjacent sample\n      values are still far enough apart in intensity\
    \ for the difference\n      to be visible.\n      However, through an interesting\
    \ coincidence, the human eye's\n      subjective perception of brightness is related\
    \ to the physical\n      stimulation of light intensity in a manner that is very\
    \ much like\n      the power function used for gamma correction.  If we apply\
    \ gamma\n      correction to measured (or calculated) light intensity before\n\
    \      quantizing to an integer for storage in a frame buffer, we can get\n  \
    \    away with using many fewer bits to store the image.  In fact, 8\n      bits\
    \ per color is almost always sufficient to avoid contouring\n      artifacts.\
    \  This is because, since gamma correction is so closely\n      related to human\
    \ perception, we are assigning our 256 available\n      sample codes to intensity\
    \ values in a manner that approximates how\n      visible those intensity changes\
    \ are to the eye.  Compared to a\n      linear-sample image, we allocate fewer\
    \ sample values to brighter\n      parts of the tonal range and more sample values\
    \ to the darker\n      portions of the tonal range.\n      Thus, for the same\
    \ apparent image quality, images using gamma-\n      encoded sample values need\
    \ only about two-thirds as many bits of\n      storage as images using linear\
    \ samples.\n   General gamma handling\n      When more than two nonlinear transfer\
    \ functions are involved in\n      the image pipeline, the term \"gamma correction\"\
    \ becomes too vague.\n      If we consider a pipeline that involves capturing\
    \ (or calculating)\n      an image, storing it in an image file, reading the file,\
    \ and\n      displaying the image on some sort of display screen, there are at\n\
    \      least 5 places in the pipeline that could have nonlinear transfer\n   \
    \   functions.  Let's give each a specific name for their\n      characteristic\
    \ gamma:\n      camera_gamma\n         the characteristic of the image sensor\n\
    \      encoding_gamma\n         the gamma of any transformation performed by the\
    \ software\n         writing the image file\n      decoding_gamma\n         the\
    \ gamma of any transformation performed by the software\n         reading the\
    \ image file\n      LUT_gamma\n         the gamma of the frame buffer LUT, if\
    \ present\n      CRT_gamma\n         the gamma of the CRT, generally 2.5\n   \
    \   In addition, let's add a few other names:\n      file_gamma\n         the\
    \ gamma of the image in the file, relative to the original\n         scene.  This\
    \ is\n            file_gamma = camera_gamma * encoding_gamma\n      display_gamma\n\
    \         the gamma of the \"display system\" downstream of the frame\n      \
    \   buffer.  This is\n            display_gamma = LUT_gamma * CRT_gamma\n    \
    \  viewing_gamma\n         the overall gamma that we want to obtain to produce\
    \ pleasing\n         images --- generally 1.0 to 1.5.\n      The file_gamma value,\
    \ as defined above, is what goes in the gAMA\n      chunk in a PNG file.  If file_gamma\
    \ is not 1.0, we know that gamma\n      correction has been done on the sample\
    \ values in the file, and we\n      could call them \"gamma corrected\" samples.\
    \  However, since there\n      can be so many different values of gamma in the\
    \ image display\n      chain, and some of them are not known at the time the image\
    \ is\n      written, the samples are not really being \"corrected\" for a\n  \
    \    specific display condition.  We are really using a power function\n     \
    \ in the process of encoding an intensity range into a small integer\n      field,\
    \ and so it is more correct to say \"gamma encoded\" samples\n      instead of\
    \ \"gamma corrected\" samples.\n      When displaying an image file, the image\
    \ decoding program is\n      responsible for making the overall gamma of the system\
    \ equal to\n      the desired viewing_gamma, by selecting the decoding_gamma\n\
    \      appropriately.  When displaying a PNG file, the gAMA chunk\n      provides\
    \ the file_gamma value.  The display_gamma may be known for\n      this machine,\
    \ or it might be obtained from the system software, or\n      the user might have\
    \ to be asked what it is.  The correct\n      viewing_gamma depends on lighting\
    \ conditions, and that will\n      generally have to come from the user.\n   \
    \   Ultimately, you should have\n         file_gamma * decoding_gamma * display_gamma\
    \ = viewing_gamma\n   Some specific examples\n      In digital video systems,\
    \ camera_gamma is about 0.5 by declaration\n      of the various video standards\
    \ documents.  CRT_gamma is 2.5 as\n      usual, while encoding_gamma, decoding_gamma,\
    \ and LUT_gamma are all\n      1.0.  As a result, viewing_gamma ends up being\
    \ about 1.25.\n      On frame buffers that have hardware gamma correction tables,\
    \ and\n      that are calibrated to display linear samples correctly,\n      display_gamma\
    \ is 1.0.\n      Many workstations and X terminals and PC displays lack gamma\n\
    \      correction lookup tables.  Here, LUT_gamma is always 1.0, so\n      display_gamma\
    \ is 2.5.\n      On the Macintosh, there is a LUT.  By default, it is loaded with\
    \ a\n      table whose gamma is about 0.72, giving a display_gamma (LUT and\n\
    \      CRT combined) of about 1.8.  Some Macs have a \"Gamma\" control\n     \
    \ panel that allows gamma to be changed to 1.0, 1.2, 1.4, 1.8, or\n      2.2.\
    \  These settings load alternate LUTs that are designed to give\n      a display_gamma\
    \ that is equal to the label on the selected button.\n      Thus, the \"Gamma\"\
    \ control panel setting can be used directly as\n      display_gamma in decoder\
    \ calculations.\n      On recent SGI systems, there is a hardware gamma-correction\
    \ table\n      whose contents are controlled by the (privileged) \"gamma\" program.\n\
    \      The gamma of the table is actually the reciprocal of the number\n     \
    \ that \"gamma\" prints, and it does not include the CRT gamma. To\n      obtain\
    \ the display_gamma, you need to find the SGI system gamma\n      (either by looking\
    \ in a file, or asking the user) and then\n      calculating\n         display_gamma\
    \ = 2.5 / SGI_system_gamma\n      You will find SGI systems with the system gamma\
    \ set to 1.0 and 2.2\n      (or higher), but the default when machines are shipped\
    \ is 1.7.\n   A note about video gamma\n      The original NTSC video standards\
    \ specified a simple power-law\n      camera transfer function with a gamma of\
    \ 1/2.2 or 0.45.  This is\n      not possible to implement exactly in analog hardware\
    \ because the\n      function has infinite slope at x=0, so all cameras deviated\
    \ to\n      some degree from this ideal.  More recently, a new camera transfer\n\
    \      function that is physically realizable has been accepted as a\n      standard\
    \ [SMPTE-170M].  It is\n         Vout = 4.5 * Vin                    if Vin <\
    \ 0.018\n         Vout = 1.099 * (Vin^0.45) - 0.099   if Vin >= 0.018\n      where\
    \ Vin and Vout are measured on a scale of 0 to 1.  Although\n      the exponent\
    \ remains 0.45, the multiplication and subtraction\n      change the shape of\
    \ the transfer function, so it is no longer a\n      pure power function.  If\
    \ you want to perform extremely precise\n      calculations on video signals,\
    \ you should use the expression above\n      (or its inverse, as required).\n\
    \      However, PNG does not provide a way to specify that an image uses\n   \
    \   this exact transfer function; the gAMA chunk always assumes a pure\n     \
    \ power-law function. If we plot the two-part transfer function\n      above along\
    \ with the family of pure power functions, we find that\n      a power function\
    \ with a gamma of about 0.5 to 0.52 (not 0.45) most\n      closely approximates\
    \ the transfer function.  Thus, when writing a\n      PNG file with data obtained\
    \ from digitizing the output of a modern\n      video camera, the gAMA chunk should\
    \ contain 0.5 or 0.52, not 0.45.\n      The remaining difference between the true\
    \ transfer function and\n      the power function is insignificant for almost\
    \ all purposes.  (In\n      fact, the alignment errors in most cameras are likely\
    \ to be larger\n      than the difference between these functions.)  The designers\
    \ of\n      PNG deemed the simplicity and flexibility of a power-law\n      definition\
    \ of gAMA to be more important than being able to\n      describe the SMPTE-170M\
    \ transfer curve exactly.\n      The PAL and SECAM video standards specify a power-law\
    \ camera\n      transfer function with a gamma of 1/2.8 or 0.36 --- not the 1/2.2\n\
    \      of NTSC.  However, this is too low in practice, so real cameras\n     \
    \ are likely to have their gamma set close to NTSC practice.  Just\n      guessing\
    \ 0.45 or 0.5 is likely to give you viewable results, but\n      if you want precise\
    \ values you'll probably have to measure the\n      particular camera.\n   Further\
    \ reading\n      If you have access to the World Wide Web, read Charles Poynton's\n\
    \      excellent \"Gamma FAQ\" [GAMMA-FAQ] for more information about\n      gamma.\n"
- title: '14. Appendix: Color Tutorial'
  contents:
  - "14. Appendix: Color Tutorial\n   (This appendix is not part of the formal PNG\
    \ specification.)\n   About chromaticity\n      The cHRM chunk is used, together\
    \ with the gAMA chunk, to convey\n      precise color information so that a PNG\
    \ image can be displayed or\n      printed with better color fidelity than is\
    \ possible without this\n      information.  The preceding chapters state how\
    \ this information is\n      encoded in a PNG image.  This tutorial briefly outlines\
    \ the\n      underlying color theory for those who might not be familiar with\n\
    \      it.\n      Note that displaying an image with incorrect gamma will produce\n\
    \      much larger color errors than failing to use the chromaticity\n      data.\
    \  First be sure the monitor set-up and gamma correction are\n      right, then\
    \ worry about chromaticity.\n   The problem\n      The color of an object depends\
    \ not only on the precise spectrum of\n      light emitted or reflected from it,\
    \ but also on the observer ---\n      their species, what else they can see at\
    \ the same time, even what\n      they have recently looked at!  Furthermore,\
    \ two very different\n      spectra can produce exactly the same color sensation.\
    \  Color is\n      not an objective property of real-world objects; it is a\n\
    \      subjective, biological sensation.  However, by making some\n      simplifying\
    \ assumptions (such as: we are talking about human\n      vision) it is possible\
    \ to produce a mathematical model of color\n      and thereby obtain good color\
    \ accuracy.\n   Device-dependent color\n      Display the same RGB data on three\
    \ different monitors, side by\n      side, and you will get a noticeably different\
    \ color balance on\n      each display.  This is because each monitor emits a\
    \ slightly\n      different shade and intensity of red, green, and blue light.\
    \  RGB\n      is an example of a device-dependent color model --- the color you\n\
    \      get depends on the device.  This also means that a particular\n      color\
    \ --- represented as say RGB 87, 146, 116 on one monitor ---\n      might have\
    \ to be specified as RGB 98, 123, 104 on another to\n      produce the same color.\n\
    \   Device-independent color\n      A full physical description of a color would\
    \ require specifying\n      the exact spectral power distribution of the light\
    \ source.\n      Fortunately, the human eye and brain are not so sensitive as\
    \ to\n      require exact reproduction of a spectrum.  Mathematical, device-\n\
    \      independent color models exist that describe fairly well how a\n      particular\
    \ color will be seen by humans.  The most important\n      device-independent\
    \ color model, to which all others can be\n      related, was developed by the\
    \ International Lighting Committee\n      (CIE, in French) and is called XYZ.\n\
    \      In XYZ, X is the sum of a weighted power distribution over the\n      whole\
    \ visible spectrum.  So are Y and Z, each with different\n      weights.  Thus\
    \ any arbitrary spectral power distribution is\n      condensed down to just three\
    \ floating point numbers.  The weights\n      were derived from color matching\
    \ experiments done on human\n      subjects in the 1920s.  CIE XYZ has been an\
    \ International Standard\n      since 1931, and it has a number of useful properties:\n\
    \          * two colors with the same XYZ values will look the same to\n     \
    \       humans\n          * two colors with different XYZ values will not look\
    \ the same\n          * the Y value represents all the brightness information\n\
    \            (luminance)\n          * the XYZ color of any object can be objectively\
    \ measured\n      Color models based on XYZ have been used for many years by people\n\
    \      who need accurate control of color --- lighting engineers for film\n  \
    \    and TV, paint and dyestuffs manufacturers, and so on.  They are\n      thus\
    \ proven in industrial use.  Accurate, device-independent color\n      started\
    \ to spread from high-end, specialized areas into the\n      mainstream during\
    \ the late 1980s and early 1990s, and PNG takes\n      notice of that trend.\n\
    \   Calibrated, device-dependent color\n      Traditionally, image file formats\
    \ have used uncalibrated, device-\n      dependent color.  If the precise details\
    \ of the original display\n      device are known, it becomes possible to convert\
    \ the device-\n      dependent colors of a particular image to device-independent\
    \ ones.\n      Making simplifying assumptions, such as working with CRTs (which\n\
    \      are much easier than printers), all we need to know are the XYZ\n     \
    \ values of each primary color and the CRT_gamma.\n      So why does PNG not store\
    \ images in XYZ instead of RGB?  Well, two\n      reasons.  First, storing images\
    \ in XYZ would require more bits of\n      precision, which would make the files\
    \ bigger.  Second, all\n      programs would have to convert the image data before\
    \ viewing it.\n      Whether calibrated or not, all variants of RGB are close\
    \ enough\n      that undemanding viewers can get by with simply displaying the\n\
    \      data without color correction.  By storing calibrated RGB, PNG\n      retains\
    \ compatibility with existing programs that expect RGB data,\n      yet provides\
    \ enough information for conversion to XYZ in\n      applications that need precise\
    \ colors.  Thus, we get the best of\n      both worlds.\n   What are chromaticity\
    \ and luminance?\n      Chromaticity is an objective measurement of the color\
    \ of an\n      object, leaving aside the brightness information.  Chromaticity\n\
    \      uses two parameters x and y, which are readily calculated from\n      XYZ:\n\
    \         x = X / (X + Y + Z)\n         y = Y / (X + Y + Z)\n      XYZ colors\
    \ having the same chromaticity values will appear to have\n      the same hue\
    \ but can vary in absolute brightness.  Notice that x,y\n      are dimensionless\
    \ ratios, so they have the same values no matter\n      what units we've used\
    \ for X,Y,Z.\n      The Y value of an XYZ color is directly proportional to its\n\
    \      absolute brightness and is called the luminance of the color.  We\n   \
    \   can describe a color either by XYZ coordinates or by chromaticity\n      x,y\
    \ plus luminance Y.  The XYZ form has the advantage that it is\n      linearly\
    \ related to (linear, gamma=1.0) RGB color spaces.\n   How are computer monitor\
    \ colors described?\n      The \"white point\" of a monitor is the chromaticity\
    \ x,y of the\n      monitor's nominal white, that is, the color produced when\n\
    \      R=G=B=maximum.\n      It's customary to specify monitor colors by giving\
    \ the\n      chromaticities of the individual phosphors R, G, and B, plus the\n\
    \      white point.  The white point allows one to infer the relative\n      brightnesses\
    \ of the three phosphors, which isn't determined by\n      their chromaticities\
    \ alone.\n      Note that the absolute brightness of the monitor is not specified.\n\
    \      For computer graphics work, we generally don't care very much\n      about\
    \ absolute brightness levels.  Instead of dealing with\n      absolute XYZ values\
    \ (in which X,Y,Z are expressed in physical\n      units of radiated power, such\
    \ as candelas per square meter), it is\n      convenient to work in \"relative\
    \ XYZ\" units, where the monitor's\n      nominal white is taken to have a luminance\
    \ (Y) of 1.0.  Given this\n      assumption, it's simple to compute XYZ coordinates\
    \ for the\n      monitor's white, red, green, and blue from their chromaticity\n\
    \      values.\n      Why does cHRM use x,y rather than XYZ?  Simply because that\
    \ is how\n      manufacturers print the information in their spec sheets!\n  \
    \    Usually, the first thing a program will do is convert the cHRM\n      chromaticities\
    \ into relative XYZ space.\n   What can I do with it?\n      If a PNG file has\
    \ the gAMA and cHRM chunks, the source_RGB values\n      can be converted to XYZ.\
    \  This lets you:\n          * do accurate grayscale conversion (just use the\
    \ Y component)\n          * convert to RGB for your own monitor (to see the original\n\
    \            colors)\n          * print the image in Level 2 PostScript with better\
    \ color\n            fidelity than a simple RGB to CMYK conversion could provide\n\
    \          * calculate an optimal color palette\n          * pass the image data\
    \ to a color management system\n          * etc.\n   How do I convert from source_RGB\
    \ to XYZ?\n      Make a few simplifying assumptions first, like the monitor really\n\
    \      is jet black with no input and the guns don't interfere with one\n    \
    \  another.  Then, given that you know the CIE XYZ values for each of\n      red,\
    \ green, and blue for a particular monitor, you put them into a\n      matrix\
    \ m:\n                 Xr Xg Xb\n            m =  Yr Yg Yb\n                 Zr\
    \ Zg Zb\n      Here we assume we are working with linear RGB floating point data\n\
    \      in the range 0..1.  If the gamma is not 1.0, make it so on the\n      floating\
    \ point data.  Then convert source_RGB to XYZ by matrix\n      multiplication:\n\
    \            X     R\n            Y = m G\n            Z     B\n      In other\
    \ words, X = Xr*R + Xg*G + Xb*B, and similarly for Y and Z.\n      You can go\
    \ the other way too:\n            R      X\n            G = im Y\n           \
    \ B      Z\n      where im is the inverse of the matrix m.\n   What is a gamut?\n\
    \      The gamut of a device is the subset of visible colors which that\n    \
    \  device can display.  (It has nothing to do with gamma.)  The gamut\n      of\
    \ an RGB device can be visualized as a polyhedron in XYZ space;\n      the vertices\
    \ correspond to the device's black, blue, red, green,\n      magenta, cyan, yellow\
    \ and white.\n      Different devices have different gamuts, in other words one\
    \ device\n      will be able to display certain colors (usually highly saturated\n\
    \      ones) that another device cannot.  The gamut of a particular RGB\n    \
    \  device can be determined from its R, G, and B chromaticities and\n      white\
    \ point (the same values given in the cHRM chunk).  The gamut\n      of a color\
    \ printer is more complex and can only be determined by\n      measurement.  However,\
    \ printer gamuts are typically smaller than\n      monitor gamuts, meaning that\
    \ there can be many colors in a\n      displayable image that cannot physically\
    \ be printed.\n      Converting image data from one device to another generally\
    \ results\n      in gamut mismatches --- colors that cannot be represented exactly\n\
    \      on the destination device.  The process of making the colors fit,\n   \
    \   which can range from a simple clip to elaborate nonlinear scaling\n      transformations,\
    \ is termed gamut mapping.  The aim is to produce a\n      reasonable visual representation\
    \ of the original image.\n   Further reading\n      References [COLOR-1] through\
    \ [COLOR-5] provide more detail about\n      color theory.\n"
- title: '15. Appendix: Sample CRC Code'
  contents:
  - "15. Appendix: Sample CRC Code\n   The following sample code represents a practical\
    \ implementation of\n   the CRC (Cyclic Redundancy Check) employed in PNG chunks.\
    \  (See also\n   ISO 3309 [ISO-3309] or ITU-T V.42 [ITU-V42] for a formal\n  \
    \ specification.)\n   The sample code is in the ANSI C programming language. \
    \ Non C users\n   may find it easier to read with these hints:\n   &\n      Bitwise\
    \ AND operator.\n   ^\n      Bitwise exclusive-OR operator.  (Caution: elsewhere\
    \ in this\n      document, ^ represents exponentiation.)\n   >>\n      Bitwise\
    \ right shift operator.  When applied to an unsigned\n      quantity, as here,\
    \ right shift inserts zeroes at the left.\n   !\n      Logical NOT operator.\n\
    \   ++\n      \"n++\" increments the variable n.\n   0xNNN\n      0x introduces\
    \ a hexadecimal (base 16) constant.  Suffix L\n      indicates a long value (at\
    \ least 32 bits).\n      /* Table of CRCs of all 8-bit messages. */\n      unsigned\
    \ long crc_table[256];\n      /* Flag: has the table been computed? Initially\
    \ false. */\n      int crc_table_computed = 0;\n      /* Make the table for a\
    \ fast CRC. */\n      void make_crc_table(void)\n      {\n        unsigned long\
    \ c;\n        int n, k;\n        for (n = 0; n < 256; n++) {\n          c = (unsigned\
    \ long) n;\n          for (k = 0; k < 8; k++) {\n            if (c & 1)\n    \
    \          c = 0xedb88320L ^ (c >> 1);\n            else\n              c = c\
    \ >> 1;\n          }\n          crc_table[n] = c;\n        }\n        crc_table_computed\
    \ = 1;\n      }\n      /* Update a running CRC with the bytes buf[0..len-1]--the\
    \ CRC\n         should be initialized to all 1's, and the transmitted value\n\
    \         is the 1's complement of the final running CRC (see the\n         crc()\
    \ routine below)). */\n      unsigned long update_crc(unsigned long crc, unsigned\
    \ char *buf,\n                               int len)\n      {\n        unsigned\
    \ long c = crc;\n        int n;\n        if (!crc_table_computed)\n          make_crc_table();\n\
    \        for (n = 0; n < len; n++) {\n          c = crc_table[(c ^ buf[n]) & 0xff]\
    \ ^ (c >> 8);\n        }\n        return c;\n      }\n      /* Return the CRC\
    \ of the bytes buf[0..len-1]. */\n      unsigned long crc(unsigned char *buf,\
    \ int len)\n      {\n        return update_crc(0xffffffffL, buf, len) ^ 0xffffffffL;\n\
    \      }\n"
- title: '16. Appendix: Online Resources'
  contents:
  - "16. Appendix: Online Resources\n   (This appendix is not part of the formal PNG\
    \ specification.)\n   This appendix gives the locations of some Internet resources\
    \ for PNG\n   software developers.  By the nature of the Internet, the list is\n\
    \   incomplete and subject to change.\n   Archive sites\n      The latest released\
    \ versions of this document and related\n      information can always be found\
    \ at the PNG FTP archive site,\n      ftp://ftp.uu.net/graphics/png/.  The PNG\
    \ specification is\n      available in several formats, including HTML, plain\
    \ text, and\n      PostScript.\n   Reference implementation and test images\n\
    \      A reference implementation in portable C is available from the PNG\n  \
    \    FTP archive site, ftp://ftp.uu.net/graphics/png/src/.  The\n      reference\
    \ implementation is freely usable in all applications,\n      including commercial\
    \ applications.\n      Test images are available from\n      ftp://ftp.uu.net/graphics/png/images/.\n\
    \   Electronic mail\n      The maintainers of the PNG specification can be contacted\
    \ by e-\n      mail at png-info@uunet.uu.net or at png-group@w3.org.\n   PNG home\
    \ page\n      There is a World Wide Web home page for PNG at\n      http://quest.jpl.nasa.gov/PNG/.\
    \  This page is a central location\n      for current information about PNG and\
    \ PNG-related tools.\n"
- title: '17. Appendix: Revision History'
  contents:
  - "17. Appendix: Revision History\n   (This appendix is not part of the formal PNG\
    \ specification.)\n   The PNG format has been frozen since the Ninth Draft of\
    \ March 7,\n   1995, and all future changes are intended to be backwards compatible.\n\
    \   The revisions since the Ninth Draft are simply clarifications,\n   improvements\
    \ in presentation, and additions of supporting material.\n   On 1 October 1996,\
    \ the PNG specification was approved as a W3C (World\n   Wide Web Consortium)\
    \ Recommendation.\n   Changes since the Tenth Draft of 5 May, 1995\n         \
    \ * Clarified meaning of a suggested-palette PLTE chunk in a\n            truecolor\
    \ image that uses transparency\n          * Clarified exact semantics of sBIT\
    \ and allowed sample depth\n            scaling procedures\n          * Clarified\
    \ status of spaces in tEXt chunk keywords\n          * Distinguished private and\
    \ public extension values in type\n            and method fields\n          *\
    \ Added a \"Creation Time\" tEXt keyword\n          * Macintosh representation\
    \ of PNG specified\n          * Added discussion of security issues\n        \
    \  * Added more extensive discussion of gamma and chromaticity\n            handling,\
    \ including tutorial appendixes\n          * Clarified terminology, notably sample\
    \ depth vs. bit depth\n          * Added a glossary\n          * Editing and reformatting\n"
- title: 18. References
  contents:
  - "18. References\n   [COLOR-1]\n      Hall, Roy, Illumination and Color in Computer\
    \ Generated Imagery.\n      Springer-Verlag, New York, 1989.  ISBN 0-387-96774-5.\n\
    \   [COLOR-2]\n      Kasson, J., and W. Plouffe, \"An Analysis of Selected Computer\n\
    \      Interchange Color Spaces\", ACM Transactions on Graphics, vol 11 no\n \
    \     4 (1992), pp 373-405.\n   [COLOR-3]\n      Lilley, C., F. Lin, W.T. Hewitt,\
    \ and T.L.J. Howard, Colour in\n      Computer Graphics. CVCP, Sheffield, 1993.\
    \  ISBN 1-85889-022-5.\n      Also available from\n      <URL:http://info.mcc.ac.uk/CGU/ITTI/Col/colour_announce.html>\n\
    \   [COLOR-4]\n      Stone, M.C., W.B. Cowan, and J.C. Beatty, \"Color gamut mapping\
    \ and\n      the printing of digital images\", ACM Transactions on Graphics, vol\n\
    \      7 no 3 (1988), pp 249-292.\n   [COLOR-5]\n      Travis, David, Effective\
    \ Color Displays --- Theory and Practice.\n      Academic Press, London, 1991.\
    \  ISBN 0-12-697690-2.\n   [GAMMA-FAQ]\n      Poynton, C., \"Gamma FAQ\".\n  \
    \    <URL:http://www.inforamp.net/%7Epoynton/Poynton-colour.html>\n   [ISO-3309]\n\
    \      International Organization for Standardization, \"Information\n      Processing\
    \ Systems --- Data Communication High-Level Data Link\n      Control Procedure\
    \ --- Frame Structure\", IS 3309, October 1984, 3rd\n      Edition.\n   [ISO-8859]\n\
    \      International Organization for Standardization, \"Information\n      Processing\
    \ --- 8-bit Single-Byte Coded Graphic Character Sets ---\n      Part 1: Latin\
    \ Alphabet No. 1\", IS 8859-1, 1987.\n      Also see sample files at\n      ftp://ftp.uu.net/graphics/png/documents/iso_8859-1.*\n\
    \   [ITU-BT709]\n      International Telecommunications Union, \"Basic Parameter\
    \ Values\n      for the HDTV Standard for the Studio and for International\n \
    \     Programme Exchange\", ITU-R Recommendation BT.709 (formerly CCIR\n     \
    \ Rec. 709), 1990.\n   [ITU-V42]\n      International Telecommunications Union,\
    \ \"Error-correcting\n      Procedures for DCEs Using Asynchronous-to-Synchronous\
    \ Conversion\",\n      ITU-T Recommendation V.42, 1994, Rev. 1.\n   [PAETH]\n\
    \      Paeth, A.W., \"Image File Compression Made Easy\", in Graphics Gems\n \
    \     II, James Arvo, editor.  Academic Press, San Diego, 1991.  ISBN\n      0-12-064480-0.\n\
    \   [POSTSCRIPT]\n      Adobe Systems Incorporated, PostScript Language Reference\
    \ Manual,\n      2nd edition. Addison-Wesley, Reading, 1990.  ISBN 0-201-18127-4.\n\
    \   [PNG-EXTENSIONS]\n      PNG Group, \"PNG Special-Purpose Public Chunks\".\
    \  Available in\n      several formats from\n      ftp://ftp.uu.net/graphics/png/documents/pngextensions.*\n\
    \   [RFC-1123]\n      Braden, R., Editor, \"Requirements for Internet Hosts ---\n\
    \      Application and Support\", STD 3, RFC 1123, USC/Information\n      Sciences\
    \ Institute, October 1989.\n      <URL:ftp://ds.internic.net/rfc/rfc1123.txt>\n\
    \   [RFC-2045]\n      Freed, N., and N. Borenstein, \"Multipurpose Internet Mail\n\
    \      Extensions (MIME) Part One: Format of Internet Message Bodies\",\n    \
    \  RFC 2045, Innosoft, First Virtual, November 1996.\n      <URL:ftp://ds.internic.net/rfc/rfc2045.txt>\n\
    \   [RFC-2048]\n      Freed, N., Klensin, J., and J. Postel, \"Multipurpose Internet\
    \ Mail\n      Extensions (MIME) Part Four: Registration Procedures\", RFC 2048,\n\
    \      Innosoft, MCI, USC/Information Sciences Institute, November 1996.\n   \
    \   <URL:ftp://ds.internic.net/rfc/rfc2048.txt>\n   [RFC-1950]\n      Deutsch,\
    \ P. and J-L. Gailly, \"ZLIB Compressed Data Format\n      Specification version\
    \ 3.3\", RFC 1950, Aladdin Enterprises, May\n      1996.\n      <URL:ftp://ds.internic.net/rfc/rfc1950.txt>\n\
    \   [RFC-1951]\n      Deutsch, P., \"DEFLATE Compressed Data Format Specification\
    \ version\n      1.3\", RFC 1951, Aladdin Enterprises, May 1996.\n      <URL:ftp://ds.internic.net/rfc/rfc1951.txt>\n\
    \   [SMPTE-170M]\n      Society of Motion Picture and Television Engineers, \"\
    Television\n      --- Composite Analog Video Signal --- NTSC for Studio\n    \
    \  Applications\", SMPTE-170M, 1994.\n"
- title: 19. Credits
  contents:
  - "19. Credits\n   Editor\n      Thomas Boutell, boutell@boutell.com\n   Contributing\
    \ Editor\n      Tom Lane, tgl@sss.pgh.pa.us\n   Authors\n      Authors' names\
    \ are presented in alphabetical order.\n          * Mark Adler, madler@alumni.caltech.edu\n\
    \          * Thomas Boutell, boutell@boutell.com\n          * Christian Brunschen,\
    \ cb@df.lth.se\n          * Adam M. Costello, amc@cs.berkeley.edu\n          *\
    \ Lee Daniel Crocker, lee@piclab.com\n          * Andreas Dilger, adilger@enel.ucalgary.ca\n\
    \          * Oliver Fromme, fromme@rz.tu-clausthal.de\n          * Jean-loup Gailly,\
    \ gzip@prep.ai.mit.edu\n          * Chris Herborth, chrish@qnx.com\n         \
    \ * Alex Jakulin, Aleks.Jakulin@snet.fri.uni-lj.si\n          * Neal Kettler,\
    \ kettler@cs.colostate.edu\n          * Tom Lane, tgl@sss.pgh.pa.us\n        \
    \  * Alexander Lehmann, alex@hal.rhein-main.de\n          * Chris Lilley, chris@w3.org\n\
    \          * Dave Martindale, davem@cs.ubc.ca\n          * Owen Mortensen, 104707.650@compuserve.com\n\
    \          * Keith S. Pickens, ksp@swri.edu\n          * Robert P. Poole, lionboy@primenet.com\n\
    \          * Glenn Randers-Pehrson, glennrp@arl.mil or\n            randeg@alumni.rpi.edu\n\
    \          * Greg Roelofs, newt@pobox.com\n          * Willem van Schaik, willem@gintic.gov.sg\n\
    \          * Guy Schalnat\n          * Paul Schmidt, pschmidt@photodex.com\n \
    \         * Tim Wegner, 71320.675@compuserve.com\n          * Jeremy Wohl, jeremyw@anders.com\n\
    \      The authors wish to acknowledge the contributions of the Portable\n   \
    \   Network Graphics mailing list, the readers of comp.graphics, and\n      the\
    \ members of the World Wide Web Consortium (W3C).\n      The Adam7 interlacing\
    \ scheme is not patented and it is not the\n      intention of the originator\
    \ of that scheme to patent it. The\n      scheme may be freely used by all PNG\
    \ implementations. The name\n      \"Adam7\" may be freely used to describe interlace\
    \ method 1 of the\n      PNG specification.\n   Trademarks\n      GIF is a service\
    \ mark of CompuServe Incorporated.  IBM PC is a\n      trademark of International\
    \ Business Machines Corporation.\n      Macintosh is a trademark of Apple Computer,\
    \ Inc.  Microsoft and\n      MS-DOS are trademarks of Microsoft Corporation. \
    \ PhotoCD is a\n      trademark of Eastman Kodak Company.  PostScript and TIFF\
    \ are\n      trademarks of Adobe Systems Incorporated.  SGI is a trademark of\n\
    \      Silicon Graphics, Inc.  X Window System is a trademark of the\n      Massachusetts\
    \ Institute of Technology.\n"
- title: COPYRIGHT NOTICE
  contents:
  - "COPYRIGHT NOTICE\n   Copyright (c) 1996 by: Massachusetts Institute of Technology\
    \ (MIT)\n   This W3C specification is being provided by the copyright holders\n\
    \   under the following license. By obtaining, using and/or copying this\n   specification,\
    \ you agree that you have read, understood, and will\n   comply with the following\
    \ terms and conditions:\n   Permission to use, copy, and distribute this specification\
    \ for any\n   purpose and without fee or royalty is hereby granted, provided that\n\
    \   the full text of this NOTICE appears on ALL copies of the\n   specification\
    \ or portions thereof, including modifications, that you\n   make.\n   THIS SPECIFICATION\
    \ IS PROVIDED \"AS IS,\" AND COPYRIGHT HOLDERS MAKE NO\n   REPRESENTATIONS OR\
    \ WARRANTIES, EXPRESS OR IMPLIED.  BY WAY OF\n   EXAMPLE, BUT NOT LIMITATION,\
    \ COPYRIGHT HOLDERS MAKE NO\n   REPRESENTATIONS OR WARRANTIES OF MERCHANTABILITY\
    \ OR FITNESS FOR ANY\n   PARTICULAR PURPOSE OR THAT THE USE OF THE SPECIFICATION\
    \ WILL NOT\n   INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR OTHER\n\
    \   RIGHTS.  COPYRIGHT HOLDERS WILL BEAR NO LIABILITY FOR ANY USE OF THIS\n  \
    \ SPECIFICATION.\n   The name and trademarks of copyright holders may NOT be used\
    \ in\n   advertising or publicity pertaining to the specification without\n  \
    \ specific, written prior permission.  Title to copyright in this\n   specification\
    \ and any associated documentation will at all times\n   remain with copyright\
    \ holders.\n"
- title: Security Considerations
  contents:
  - "Security Considerations\n   Security issues are discussed in Security considerations\
    \ (Section\n   8.5).\n"
- title: Author's Address
  contents:
  - "Author's Address\n   Thomas Boutell\n   PO Box 20837\n   Seattle, WA  98102\n\
    \   Phone: (206) 329-4969\n   EMail: boutell@boutell.com\n"
