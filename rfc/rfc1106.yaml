- title: __initial_text__
  contents:
  - '                     TCP Big Window and Nak Options

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo discusses two extensions to the TCP protocol\
    \ to provide a\n   more efficient operation over a network with a high bandwidth*delay\n\
    \   product.  The extensions described in this document have been\n   implemented\
    \ and shown to work using resources at NASA.  This memo\n   describes an Experimental\
    \ Protocol, these extensions are not proposed\n   as an Internet standard, but\
    \ as a starting point for further\n   research.  Distribution of this memo is\
    \ unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   Two extensions to the TCP protocol are described in this RFC in\
    \ order\n   to provide a more efficient operation over a network with a high\n\
    \   bandwidth*delay product.  The main issue that still needs to be\n   solved\
    \ is congestion versus noise.  This issue is touched on in this\n   memo, but\
    \ further research is still needed on the applicability of\n   the extensions\
    \ in the Internet as a whole infrastructure and not just\n   high bandwidth*delay\
    \ product networks.  Even with this outstanding\n   issue, this document does\
    \ describe the use of these options in the\n   isolated satellite network environment\
    \ to help facilitate more\n   efficient use of this special medium to help off\
    \ load bulk data\n   transfers from links needed for interactive use.\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Recent work on TCP has shown great performance gains over\
    \ a variety\n   of network paths [1].  However, these changes still do not work\
    \ well\n   over network paths that have a large round trip delay (satellite with\n\
    \   a 600 ms round trip delay) or a very large bandwidth\n   (transcontinental\
    \ DS3 line).  These two networks exhibit a higher\n   bandwidth*delay product,\
    \ over 10**6 bits, than the 10**5 bits that\n   TCP is currently limited to. \
    \ This high bandwidth*delay product\n   refers to the amount of data that may\
    \ be unacknowledged so that all\n   of the networks bandwidth is being utilized\
    \ by TCP.  This may also be\n   referred to as \"filling the pipe\" [2] so that\
    \ the sender of data can\n   always put data onto the network and the receiver\
    \ will always have\n   something to read, and neither end of the connection will\
    \ be forced\n   to wait for the other end.\n   After the last batch of algorithm\
    \ improvements to TCP, performance\n   over high bandwidth*delay networks is still\
    \ very poor.  It appears\n   that no algorithm changes alone will make any significant\n\
    \   improvements over high bandwidth*delay networks, but will require an\n   extension\
    \ to the protocol itself.  This RFC discusses two possible\n   options to TCP\
    \ for this purpose.\n   The two options implemented and discussed in this RFC\
    \ are:\n   1.  NAKs\n      This extension allows the receiver of data to inform\
    \ the sender\n      that a packet of data was not received and needs to be resent.\n\
    \      This option proves to be useful over any network path (both high\n    \
    \  and low bandwidth*delay type networks) that experiences periodic\n      errors\
    \ such as lost packets, noisy links, or dropped packets due\n      to congestion.\
    \  The information conveyed by this option is\n      advisory and if ignored,\
    \ does not have any effect on TCP what so\n      ever.\n   2.  Big Windows\n \
    \     This option will give a method of expanding the current 16 bit (64\n   \
    \   Kbytes) TCP window to 32 bits of which 30 bits (over 1 gigabytes)\n      are\
    \ allowed for the receive window.  (The maximum window size\n      allowed in\
    \ TCP due to the requirement of TCP to detect old data\n      versus new data.\
    \  For a good explanation please see [2].)  No\n      changes are required to\
    \ the standard TCP header [6]. The 16 bit\n      field in the TCP header that\
    \ is used to convey the receive window\n      will remain unchanged.  The 32 bit\
    \ receive window is achieved\n      through the use of an option that contains\
    \ the upper half of the\n      window.  It is this option that is necessary to\
    \ fill large data\n      pipes such as a satellite link.\n   This RFC is broken\
    \ up into the following sections: section 2 will\n   discuss the operation of\
    \ the NAK option in greater detail, section 3\n   will discuss the big window\
    \ option in greater detail.  Section 4 will\n   discuss other effects of the big\
    \ windows and nak feature when used\n   together.  Included in this section will\
    \ be a brief discussion on the\n   effects of congestion versus noise to TCP and\
    \ possible options for\n   satellite networks.  Section 5 will be a conclusion\
    \ with some hints\n   as to what future development may be done at NASA, and then\
    \ an\n   appendix containing some test results is included.\n"
- title: 2.  NAK Option
  contents:
  - "2.  NAK Option\n   Any packet loss in a high bandwidth*delay network will have\
    \ a\n   catastrophic effect on throughput because of the simple\n   acknowledgement\
    \ of TCP.  TCP always acks the stream of data that has\n   successfully been received\
    \ and tells the sender the next byte of data\n   of the stream that is expected.\
    \  If a packet is lost and succeeding\n   packets arrive the current protocol\
    \ has no way of telling the sender\n   that it missed one packet but received\
    \ following packets.  TCP\n   currently resends all of the data over again, after\
    \ a timeout or the\n   sender suspects a lost packet due to a duplicate ack algorithm\
    \ [1],\n   until the receiver receives the lost packet and can then ack the lost\n\
    \   packet as well as succeeding packets received.  On a normal low\n   bandwidth*delay\
    \ network this effect is minimal if the timeout period\n   is set short enough.\
    \  However, on a long delay network such as a T1\n   satellite channel this is\
    \ catastrophic because by the time the lost\n   packet can be sent and the ack\
    \ returned the TCP window would have\n   been exhausted and both the sender and\
    \ receiver would be temporarily\n   stalled waiting for the packet and ack to\
    \ fully travel the data pipe.\n   This causes the pipe to become empty and requires\
    \ the sender to\n   refill the pipe after the ack is received.  This will cause\
    \ a minimum\n   of 3*X bandwidth loss, where X is the one way delay of the medium\
    \ and\n   may be much higher depending on the size of the timeout period and\n\
    \   bandwidth*delay product.  Its 1X for the packet to be resent, 1X for\n   the\
    \ ack to be received and 1X for the next packet being sent to reach\n   the destination.\
    \  This calculation assumes that the window size is\n   much smaller than the\
    \ pipe size (window = 1/2 data pipe or 1X), which\n   is the typical case with\
    \ the current TCP window limitation over long\n   delay networks such as a T1\
    \ satellite link.\n   An attempt to reduce this wasted bandwidth from 3*X was\
    \ introduced in\n   [1] by having the sender resend a packet after it notices\
    \ that a\n   number of consecutively received acks completely acknowledges already\n\
    \   acknowledged data.  On a typical network this will reduce the lost\n   bandwidth\
    \ to almost nil, since the packet will be resent before the\n   TCP window is\
    \ exhausted and with the data pipe being much smaller\n   than the TCP window,\
    \ the data pipe will not become empty and no\n   bandwidth will be lost.  On a\
    \ high delay network the reduction of\n   lost bandwidth is minimal such that\
    \ lost bandwidth is still\n   significant.  On a very noisy satellite, for instance,\
    \ the lost\n   bandwidth is very high (see appendix for some performance figures)\n\
    \   and performance is very poor.\n   There are two methods of informing the sender\
    \ of lost data.\n   Selective acknowledgements and NAKS.  Selective acknowledgements\
    \ have\n   been the object of research in a number of experimental protocols\n\
    \   including VMTP [3], NETBLT [4], and SatFTP [5].  The idea behind\n   selective\
    \ acks is that the receiver tells the sender which pieces it\n   received so that\
    \ the sender can resend the data not acked but already\n   sent once.  NAKs on\
    \ the other hand, tell the sender that a particular\n   packet of data needs to\
    \ be resent.\n   There are a couple of disadvantages of selective acks.  Namely,\
    \ in\n   some of the protocols mentioned above, the receiver waits a certain\n\
    \   time before sending the selective ack so that acks may be bundled up.\n  \
    \ This delay can cause some wasted bandwidth and requires more complex\n   state\
    \ information than the simple nak.  Even if the receiver doesn't\n   bundle up\
    \ the selective acks but sends them as it notices that\n   packets have been lost,\
    \ more complex state information is needed to\n   determine which packets have\
    \ been acked and which packets need to be\n   resent.  With naks, only the immediate\
    \ data needed to move the left\n   edge of the window is naked, thus almost completely\
    \ eliminating all\n   state information.\n   The selective ack has one advantage\
    \ over naks.  If the link is very\n   noisy and packets are being lost close together,\
    \ then the sender will\n   find out about all of the missing data at once and\
    \ can send all of\n   the missing data out immediately in an attempt to move the\
    \ left\n   window edge in the acknowledge number of the TCP header, thus keeping\n\
    \   the data pipe flowing.  Whereas with naks, the sender will be\n   notified\
    \ of lost packets one at a time and this will cause the sender\n   to process\
    \ extra packets compared to selective acks.  However,\n   empirical studies has\
    \ shown that most lost packets occur far enough\n   apart that the advantage of\
    \ selective acks over naks is rarely seen.\n   Also, if naks are sent out as soon\
    \ as a packet has been determined\n   lost, then the advantage of selective acks\
    \ becomes no more than\n   possibly a more aesthetic algorithm for handling lost\
    \ data, but\n   offers no gains over naks as described in this paper.  It is this\n\
    \   reason that the simplicity of naks was chosen over selective acks for\n  \
    \ the current implementation.\n"
- title: 2.1  Implementation details
  contents:
  - "2.1  Implementation details\n   When the receiver of data notices a gap between\
    \ the expected sequence\n   number and the actual sequence number of the packet\
    \ received, the\n   receiver can assume that the data between the two sequence\
    \ numbers is\n   either going to arrive late or is lost forever.  Since the receiver\n\
    \   can not distinguish between the two events a nak should be sent in\n   the\
    \ TCP option field.  Naking a packet still destined to arrive has\n   the effect\
    \ of causing the sender to resend the packet, wasting one\n   packets worth of\
    \ bandwidth.  Since this event is fairly rare, the\n   lost bandwidth is insignificant\
    \ as compared to that of not sending a\n   nak when the packet is not going to\
    \ arrive.  The option will take the\n   form as follows:\n      +========+=========+=========================+================+\n\
    \      +option= + length= + sequence number of      + number of      +\n     \
    \ +   A    +    7    +  first byte being naked + segments naked +\n      +========+=========+=========================+================+\n\
    \   This option contains the first sequence number not received and a\n   count\
    \ of how many segments of bytes needed to be resent, where\n   segments is the\
    \ size of the current TCP MSS being used for the\n   connection.  Since a nak\
    \ is an advisory piece of information, the\n   sending of a nak is unreliable\
    \ and no means for retransmitting a nak\n   is provided at this time.\n   When\
    \ the sender of data receives the option it may either choose to\n   do nothing\
    \ or it will resend the missing data immediately and then\n   continue sending\
    \ data where it left off before receiving the nak.\n   The receiver will keep\
    \ track of the last nak sent so that it will not\n   repeat the same nak.  If\
    \ it were to repeat the same nak the protocol\n   could get into the mode where\
    \ on every reception of data the receiver\n   would nak the first missing data\
    \ frame.  Since the data pipe may be\n   very large by the time the first nak\
    \ is read and responded to by the\n   sender, many naks would have been sent by\
    \ the receiver.  Since the\n   sender does not know that the naks are repetitious\
    \ it will resend the\n   data each time, thus wasting the network bandwidth with\
    \ useless\n   retransmissions of the same piece of data.  Having an unreliable\
    \ nak\n   may result in a nak being damaged and not being received by the\n  \
    \ sender, and in this case, we will let the tcp recover by its normal\n   means.\
    \  Empirical data has shown that the likelihood of the nak being\n   lost is quite\
    \ small and thus, this advisory nak option works quite\n   well.\n"
- title: 3.  Big Window Option
  contents:
  - "3.  Big Window Option\n   Currently TCP has a 16 bit window limitation built\
    \ into the protocol.\n   This limits the amount of outstanding unacknowledged\
    \ data to 64\n   Kbytes.  We have already seen that some networks have a pipe\
    \ larger\n   than 64 Kbytes.  A T1 satellite channel and a cross country DS3\n\
    \   network with a 30ms delay have data pipes much larger than 64 Kbytes.\n  \
    \ Thus, even on a perfectly conditioned link with no bandwidth wasted\n   due\
    \ to errors, the data pipe will not be filled and bandwidth will be\n   wasted.\
    \  What is needed is the ability to send more unacknowledged\n   data.  This is\
    \ achieved by having bigger windows, bigger than the\n   current limitation of\
    \ 16 bits.  This option to expands the window\n   size to 30 bits or over 1 gigabytes\
    \ by literally expanding the window\n   size mechanism currently used by TCP.\
    \  The added option contains the\n   upper 15 bits of the window while the lower\
    \ 16 bits will continue to\n   go where they normally go [6] in the TCP header.\n\
    \   A TCP session will use the big window options only if both sides\n   agree\
    \ to use them, otherwise the option is not used and the normal 16\n   bit windows\
    \ will be used.  Once the 2 sides agree to use the big\n   windows then every\
    \ packet thereafter will be expected to contain the\n   window option with the\
    \ current upper 15 bits of the window.  The\n   negotiation to decide whether\
    \ or not to use the bigger windows takes\n   place during the SYN and SYN ACK\
    \ segments of the TCP connection\n   startup process.  The originator of the connection\
    \ will include in\n   the SYN segment the following option:\n                \
    \    1 byte    1 byte      4 bytes\n              +=========+==========+===============+\n\
    \              +option=B + length=6 + 30 bit window +\n              +=========+==========+===============+\n\
    \   If the other end of the connection wants to use big windows it will\n   include\
    \ the same option back in the SYN ACK segment that it must\n   send.  At this\
    \ point, both sides have agreed to use big windows and\n   the specified windows\
    \ will be used.  It should be noted that the SYN\n   and SYN ACK segments will\
    \ use the small windows, and once the big\n   window option has been negotiated\
    \ then the bigger windows will be\n   used.\n   Once both sides have agreed to\
    \ use 32 bit windows the protocol will\n   function just as it did before with\
    \ no difference in operation, even\n   in the event of lost packets.  This claim\
    \ holds true since the\n   rcv_wnd and snd_wnd variables of tcp contain the 16\
    \ bit windows until\n   the big window option is negotiated and then they are\
    \ replaced with\n   the appropriate 32 bit values.  Thus, the use of big windows\
    \ becomes\n   part of the state information kept by TCP.\n   Other methods of\
    \ expanding the windows have been presented, including\n   a window multiple [2]\
    \ or streaming [5], but this solution is more\n   elegant in the sense that it\
    \ is a true extension of the window that\n   one day may easily become part of\
    \ the protocol and not just be an\n   option to the protocol.\n"
- title: 3.1  How does it work
  contents:
  - "3.1  How does it work\n   Once a connection has decided to use big windows every\
    \ succeeding\n   packet must contain the following option:\n        +=========+==========+==========================+\n\
    \        +option=C + length=4 + upper 15 bits of rcv_wnd +\n        +=========+==========+==========================+\n\
    \   With all segments sent, the sender supplies the size of its receive\n   window.\
    \  If the connection is only using 16 bits then this option is\n   not supplied,\
    \ otherwise the lower 16 bits of the receive window go\n   into the tcp header\
    \ where it currently resides [6] and the upper 15\n   bits of the window is put\
    \ into the data portion of the option C.\n   When the receiver processes the packet\
    \ it must first reform the\n   window and then process the packet as it would\
    \ in the absence of the\n   option.\n"
- title: 3.2  Impact of changes
  contents:
  - "3.2  Impact of changes\n   In implementing the first version of the big window\
    \ option there was\n   very little change required to the source.  State information\
    \ must be\n   added to the protocol to determine if the big window option is to\
    \ be\n   used and all 16 bit variables that dealt with window information must\n\
    \   now become 32 bit quantities.  A future document will describe in\n   more\
    \ detail the changes required to the 4.3 bsd tcp source code.\n   Test results\
    \ of the window change only are presented in the appendix.\n   When expanding\
    \ 16 bit quantities to 32 bit quantities in the TCP\n   control block in the source\
    \ (4.3 bsd source) may cause the structure\n   to become larger than the mbuf\
    \ used to hold the structure.  Care must\n   be taken to insure this doesn't occur\
    \ with your system or\n   undetermined events may take place.\n"
- title: 4.  Effects of Big Windows and Naks when used together
  contents:
  - "4.  Effects of Big Windows and Naks when used together\n   With big windows alone,\
    \ transfer times over a satellite were quite\n   impressive with the absence of\
    \ any introduced errors.  However, when\n   an error simulator was used to create\
    \ random errors during transfers,\n   performance went down extremely fast.  When\
    \ the nak option was added\n   to the big window option performance in the face\
    \ of errors went up\n   some but not to the level that was expected.  This section\
    \ will\n   discuss some issues that were overcome to produce the results given\n\
    \   in the appendix.\n"
- title: 4.1  Window Size and Nak benefits
  contents:
  - "4.1  Window Size and Nak benefits\n   With out errors, the window size required\
    \ to keep the data pipe full\n   is equal to the round trip delay * throughput\
    \ desired, or the data\n   pipe bandwidth (called Z from now on).  This and other\
    \ calculations\n   assume that processing time of the hosts is negligible.  In\
    \ the event\n   of an error (without NAKs), the window size needs to become larger\n\
    \   than Z in order to keep the data pipe full while the sender is\n   waiting\
    \ for the ack of the resent packet.  If the window size is\n   equaled to Z and\
    \ we assume that the retransmission timer is equaled\n   to Z, then when a packet\
    \ is lost, the retransmission timer will go\n   off as the last piece of data\
    \ in the window is sent.  In this case,\n   the lost piece of data can be resent\
    \ with no delay.  The data pipe\n   will empty out because it will take 1/2Z worth\
    \ of data to get the ack\n   back to the sender, an additional 1/2Z worth of data\
    \ to get the data\n   pipe refilled with new data.  This causes the required window\
    \ to be\n   2Z, 1Z to keep the data pipe full during normal operations and 1Z\
    \ to\n   keep the data pipe full while waiting for a lost packet to be resent\n\
    \   and acked.\n   If the same scenario in the last paragraph is used with the\
    \ addition\n   of NAKs, the required window size still needs to be 2Z to avoid\n\
    \   wasting any bandwidth in the event of a dropped packet.  This appears\n  \
    \ to mean that the nak option does not provide any benefits at all.\n   Testing\
    \ showed that the retransmission timer was larger than the data\n   pipe and in\
    \ the event of errors became much bigger than the data\n   pipe, because of the\
    \ retransmission backoff.  Thus, the nak option\n   bounds the required window\
    \ to 2Z such that in the event of an error\n   there is no lost bandwidth, even\
    \ with the retransmission timer\n   fluctuations.  The results in the appendix\
    \ shows that by using naks,\n   bandwidth waste associated with the retransmission\
    \ timer facility is\n   eliminated.\n"
- title: 4.2  Congestions vs Noise
  contents:
  - "4.2  Congestions vs Noise\n   An issue that must be looked at when implementing\
    \ both the NAKs and\n   big window scheme together is in the area of congestion\
    \ versus lost\n   packets due to the medium, or noise.  In the recent algorithm\n\
    \   enhancements [1], slow start was introduced so that whenever a data\n   transfer\
    \ is being started on a connection or right after a dropped\n   packet, the effective\
    \ send window would be set to a very small size\n   (typically would equal the\
    \ MSS being used).  This is done so that a\n   new connection would not cause\
    \ congestion by immediately overloading\n   the network, and so that an existing\
    \ connection would back off the\n   network if a packet was dropped due to congestion\
    \ and allow the\n   network to clear up.  If a connection using big windows loses\
    \ a\n   packet due to the medium (a packet corrupted by an error) the last\n \
    \  thing that should be done is to close the send window so that the\n   connection\
    \ can only send 1 packet and must use the slow start\n   algorithm to slowly work\
    \ itself back up to sending full windows worth\n   of data.  This algorithm would\
    \ quickly limit the usefulness of the\n   big window and nak options over lossy\
    \ links.\n   On the other hand, if a packet was dropped due to congestion and\
    \ the\n   sender assumes the packet was dropped because of noise the sender\n\
    \   will continue sending large amounts of data.  This action will cause\n   the\
    \ congestion to continue, more packets will be dropped, and that\n   part of the\
    \ network will collapse.  In this instance, the sender\n   would want to back\
    \ off from sending at the current window limit.\n   Using the current slow start\
    \ mechanism over a satellite builds up the\n   window too slowly [1].  Possibly\
    \ a better solution would be for the\n   window to be opened 2*Rlog2(W) instead\
    \ of R*log2(W) [1] (open window\n   by 2 packets instead of 1 for each acked packet).\
    \  This will reduce\n   the wasted bandwidth by opening the window much quicker\
    \ while giving\n   the network a chance to clear up.  More experimentation is\
    \ necessary\n   to find the optimal rate of opening the window, especially when\
    \ large\n   windows are being used.\n   The current recommendation for TCP is\
    \ to use the slow start mechanism\n   in the event of any lost packet.  If an\
    \ application knows that it\n   will be using a satellite with a high error rate,\
    \ it doesn't make\n   sense to force it to use the slow start mechanism for every\
    \ dropped\n   packet.  Instead, the application should be able to choose what\n\
    \   action should happen in the event of a lost packet.  In the BSD\n   environment,\
    \ a setsockopt call should be provided so that the\n   application may inform\
    \ TCP to handle lost packets in a special way\n   for this particular connection.\
    \  If the known error rate of a link is\n   known to be small, then by using slow\
    \ start with modified rate from\n   above, will cause the amount of bandwidth\
    \ loss to be very small in\n   respect to the amount of bandwidth actually utilized.\
    \  In this case,\n   the setsockopt call should not be used.  What is really needed\
    \ is a\n   way for a host to determine if a packet or packets are being dropped\n\
    \   due to congestion or noise.  Then, the host can choose to do the\n   right\
    \ thing.  This will require a mechanism like source quench to be\n   used.  For\
    \ this to happen more experimentation is necessary to\n   determine a solid definition\
    \ on the use of this mechanism.  Now it is\n   believed by some that using source\
    \ quench to avoid congestion only\n   adds to the problem, not help suppress it.\n\
    \   The TCP used to gather the results in the appendix for the big window\n  \
    \ with nak experiment, assumed that lost packets were the result of\n   noise\
    \ and not congestion.  This assumption was used to show how to\n   make the current\
    \ TCP work in such an environment.  The actual\n   satellite used in the experiment\
    \ (when the satellite simulator was\n   not used) only experienced an error rate\
    \ around 10e-10.  With this\n   error rate it is suggested that in practice when\
    \ big windows are used\n   over the link, TCP should use the slow start mechanism\
    \ for all lost\n   packets with the 2*Rlog2(W) rate discussed above.  Under most\n\
    \   situations when long delay networks are being used (transcontinental\n   DS3\
    \ networks using fiber with very low error rates, or satellite\n   links with\
    \ low error rates) big windows and naks should be used with\n   the assumption\
    \ that lost packets are the result of congestion until a\n   better algorithm\
    \ is devised [7].\n   Another problem noticed, while testing the affects of slow\
    \ start over\n   a satellite link, was at times, the retransmission timer was\
    \ set so\n   restrictive, that milliseconds before a naked packet's ack is\n \
    \  received the retransmission timer would go off due to a timed packet\n   within\
    \ the send window.  The timer was set at the round trip delay of\n   the network\
    \ allowing no time for packet processing.  If this timer\n   went off due to congestion\
    \ then backing off is the right thing to do,\n   otherwise to avoid the scenario\
    \ discovered by experimentation, the\n   transmit timer should be set a little\
    \ longer so that the\n   retransmission timer does not go off too early.  Care\
    \ must be taken\n   to make sure the right thing is done in the implementation\
    \ in\n   question so that a packet isn't retransmitted too soon, and blamed on\n\
    \   congestion when in fact, the ack is on its way.\n"
- title: 4.3  Duplicate Acks
  contents:
  - "4.3  Duplicate Acks\n   Another problem found with the 4.3bsd implementation\
    \ is in the area\n   of duplicate acks.  When the sender of data receives a certain\
    \ number\n   of acks (3 in the current Berkeley release) that acknowledge\n  \
    \ previously acked data before, it then assumes that a packet has been\n   lost\
    \ and will resend the one packet assumed lost, and close its send\n   window as\
    \ if the network is congested and the slow start algorithm\n   mention above will\
    \ be used to open the send window.  This facility is\n   no longer needed since\
    \ the sender can use the reception of a nak as\n   its indicator that a particular\
    \ packet was dropped.  If the nak\n   packet is lost then the retransmit timer\
    \ will go off and the packet\n   will be retransmitted by normal means.  If a\
    \ senders algorithm\n   continues to count duplicate acks the sender will find\
    \ itself\n   possibly receiving many duplicate acks after it has already resent\n\
    \   the packet due to a nak being received because of the large size of\n   the\
    \ data pipe.  By receiving all of these duplicate acks the sender\n   may find\
    \ itself doing nothing but resending the same packet of data\n   unnecessarily\
    \ while keeping the send window closed for absolutely no\n   reason.  By removing\
    \ this feature of the implementation a user can\n   expect to find a satellite\
    \ connection working much better in the face\n   of errors and other connections\
    \ should not see any performance loss,\n   but a slight improvement in performance\
    \ if anything at all.\n"
- title: 5.  Conclusion
  contents:
  - "5.  Conclusion\n   This paper has described two new options that if used will\
    \ make TCP a\n   more efficient protocol in the face of errors and a more efficient\n\
    \   protocol over networks that have a high bandwidth*delay product\n   without\
    \ decreasing performance over more common networks.  If a\n   system that implements\
    \ the options talks with one that does not, the\n   two systems should still be\
    \ able to communicate with no problems.\n   This assumes that the system doesn't\
    \ use the option numbers defined\n   in this paper in some other way or doesn't\
    \ panic when faced with an\n   option that the machine does not implement.  Currently\
    \ at NASA, there\n   are many machines that do not implement either option and\
    \ communicate\n   just fine with the systems that do implement them.\n   The drive\
    \ for implementing big windows has been the direct result of\n   trying to make\
    \ TCP more efficient over large delay networks [2,3,4,5]\n   such as a T1 satellite.\
    \  However, another practical use of large\n   windows is becoming more apparent\
    \ as the local area networks being\n   developed are becoming faster and supporting\
    \ much larger MTU's.\n   Hyperchannel, for instances, has been stated to be able\
    \ to support 1\n   Mega bit MTU's in their new line of products.  With the current\n\
    \   implementation of TCP, efficient use of hyperchannel is not utilized\n   as\
    \ it should because the physical mediums MTU is larger than the\n   maximum window\
    \ of the protocol being used.  By increasing the TCP\n   window size, better utilization\
    \ of networks like hyperchannel will be\n   gained instantly because the sender\
    \ can send 64 Kbyte packets (IP\n   limitation) but not have to operate in a stop\
    \ and wait fashion.\n   Future work is being started to increase the IP maximum\
    \ datagram size\n   so that even better utilization of fast local area networks\
    \ will be\n   seen by having the TCP/IP protocols being able to send large packets\n\
    \   over mediums with very large MTUs.  This will hopefully, eliminate\n   the\
    \ network protocol as the bottleneck in data transfers while\n   workstations\
    \ and workstation file system technology advances even\n   more so, than it already\
    \ has.\n   An area of concern when using the big window mechanism is the use of\n\
    \   machine resources.  When running over a satellite and a packet is\n   dropped\
    \ such that 2Z (where Z is the round trip delay) worth of data\n   is unacknowledged,\
    \ both ends of the connection need to be able to\n   buffer the data using machine\
    \ mbufs (or whatever mechanism the\n   machine uses), usually a valuable and scarce\
    \ commodity.  If the\n   window size is not chosen properly, some machines will\
    \ crash when the\n   memory is all used up, or it will keep other parts of the\
    \ system from\n   running.  Thus, setting the window to some fairly large arbitrary\n\
    \   number is not a good idea, especially on a general purpose machine\n   where\
    \ many users log on at any time.  What is currently being\n   engineered at NASA\
    \ is the ability for certain programs to use the\n   setsockopt feature or 4.3bsd\
    \ asking to use big windows such that the\n   average user may not have access\
    \ to the large windows, thus limiting\n   the use of big windows to applications\
    \ that absolutely need them and\n   to protect a valuable system resource.\n"
- title: 6.  References
  contents:
  - "6.  References\n  [1]  Jacobson, V., \"Congestion Avoidance and Control\", SIGCOMM\
    \ 88,\n       Stanford, Ca., August 1988.\n  [2]  Jacobson, V., and R. Braden,\
    \ \"TCP Extensions for Long-Delay\n       Paths\", LBL, USC/Information Sciences\
    \ Institute, RFC 1072,\n       October 1988.\n  [3]  Cheriton, D., \"VMTP: Versatile\
    \ Message Transaction Protocol\", RFC\n       1045, Stanford University, February\
    \ 1988.\n  [4]  Clark, D., M. Lambert, and L. Zhang, \"NETBLT: A Bulk Data\n \
    \      Transfer Protocol\", RFC 998, MIT, March 1987.\n  [5]  Fox, R., \"Draft\
    \ of Proposed Solution for High Delay Circuit File\n       Transfer\", GE/NAS\
    \ Internal Document, March 1988.\n  [6]  Postel, J., \"Transmission Control Protocol\
    \ -  DARPA Internet\n       Program Protocol Specification\",  RFC 793, DARPA,\
    \ September 1981.\n  [7]  Leiner, B., \"Critical Issues in High Bandwidth Networking\"\
    , RFC\n       1077, DARPA, November 1989.\n"
- title: 7.  Appendix
  contents:
  - "7.  Appendix\n   Both options have been implemented and tested.  Contained in\
    \ this\n   section is some performance gathered to support the use of these two\n\
    \   options.  The satellite channel used was a 1.544 Mbit link with a\n   580ms\
    \ round trip delay.  All values are given as units of bytes.\n   TCP with Big\
    \ Windows, No Naks:\n               |---------------transfer rates----------------------|\n\
    \   Window Size |  no error  |  10e-7 error rate | 10e-6 error rate |\n   -----------------------------------------------------------------\n\
    \     64K       |   94K      |      53K          |      14K         |\n   -----------------------------------------------------------------\n\
    \     72K       |   106K     |      51K          |      15K         |\n   -----------------------------------------------------------------\n\
    \     80K       |   115K     |      42K          |      14K         |\n   -----------------------------------------------------------------\n\
    \     92K       |   115K     |      43K          |      14K         |\n    -----------------------------------------------------------------\n\
    \     100K      |   135K     |      66K          |      15K         |\n   -----------------------------------------------------------------\n\
    \     112K      |   126K     |      53K          |      17K         |\n   -----------------------------------------------------------------\n\
    \     124K      |   154K     |      45K          |      14K         |\n   -----------------------------------------------------------------\n\
    \     136K      |   160K     |      66K          |      15K         |\n   -----------------------------------------------------------------\n\
    \     156K      |   167K     |      45K          |      14K         |\n   -----------------------------------------------------------------\n\
    \                                Figure 1.\n   TCP with Big Windows, and Naks:\n\
    \               |---------------transfer rates----------------------|\n   Window\
    \ Size |  no error  |  10e-7 error rate | 10e-6 error rate |\n   -----------------------------------------------------------------\n\
    \     64K       |   95K      |      83K          |      43K         |\n   -----------------------------------------------------------------\n\
    \     72K       |   104K     |      87K          |      49K         |\n   -----------------------------------------------------------------\n\
    \     80K       |   117K     |      96K          |      62K         |\n   -----------------------------------------------------------------\n\
    \     92K       |   124K     |      119K         |      39K         |\n   -----------------------------------------------------------------\n\
    \     100K      |   140K     |      124K         |      35K         |\n   -----------------------------------------------------------------\n\
    \     112K      |   151K     |      126K         |      53K         |\n   -----------------------------------------------------------------\n\
    \     124K      |   160K     |      140K         |      36K         |\n   -----------------------------------------------------------------\n\
    \     136K      |   167K     |      148K         |      38K         |\n   -----------------------------------------------------------------\n\
    \     156K      |   167K     |      160K         |      38K         |\n   -----------------------------------------------------------------\n\
    \                                Figure 2.\n   With a 10e-6 error rate, many naks\
    \ as well as data packets were\n   dropped, causing the wild swing in transfer\
    \ times.  Also, please note\n   that the machines used are SGI Iris 2500 Turbos\
    \ with the 3.6 OS with\n   the new TCP enhancements.  The performance associated\
    \ with the Irises\n   are slower than a Sun 3/260, but due to some source code\
    \ restrictions\n   the Iris was used.  Initial results on the Sun showed slightly\
    \ higher\n   performance and less variance.\n"
- title: Author's Address
  contents:
  - "Author's Address\n   Richard Fox\n   950 Linden #208\n   Sunnyvale, Cal, 94086\n\
    \   EMail: rfox@tandem.com\n"
