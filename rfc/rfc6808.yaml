- title: __initial_text__
  contents:
  - "            Test Plan and Results Supporting Advancement of\n               \
    \     RFC 2679 on the Standards Track\n"
- title: Abstract
  contents:
  - "Abstract\n   This memo provides the supporting test plan and results to advance\n\
    \   RFC 2679 on one-way delay metrics along the Standards Track,\n   following\
    \ the process in RFC 6576.  Observing that the metric\n   definitions themselves\
    \ should be the primary focus rather than the\n   implementations of metrics,\
    \ this memo describes the test procedures\n   to evaluate specific metric requirement\
    \ clauses to determine if the\n   requirement has been interpreted and implemented\
    \ as intended.  Two\n   completely independent implementations have been tested\
    \ against the\n   key specifications of RFC 2679.  This memo also provides direct\
    \ input\n   for development of a revision of RFC 2679.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc6808.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2012 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n   This document\
    \ may contain material from IETF Documents or IETF\n   Contributions published\
    \ or made publicly available before November\n   10, 2008.  The person(s) controlling\
    \ the copyright in some of this\n   material may not have granted the IETF Trust\
    \ the right to allow\n   modifications of such material outside the IETF Standards\
    \ Process.\n   Without obtaining an adequate license from the person(s) controlling\n\
    \   the copyright in such materials, this document may not be modified\n   outside\
    \ the IETF Standards Process, and derivative works of it may\n   not be created\
    \ outside the IETF Standards Process, except to format\n   it for publication\
    \ as an RFC or to translate it into languages other\n   than English.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \      1.1. Requirements Language ......................................5\n  \
    \ 2. A Definition-Centric Metric Advancement Process .................5\n   3.\
    \ Test Configuration ..............................................5\n   4. Error\
    \ Calibration, RFC 2679 .....................................9\n      4.1. NetProbe\
    \ Error and Type-P .................................10\n      4.2. Perfas+ Error\
    \ and Type-P ..................................12\n   5. Predetermined Limits\
    \ on Equivalence ............................12\n   6. Tests to Evaluate RFC 2679\
    \ Specifications ......................13\n      6.1. One-Way Delay, ADK Sample\
    \ Comparison: Same- and Cross-\n           Implementation ............................................13\n\
    \           6.1.1. NetProbe Same-Implementation Results ...............15\n  \
    \         6.1.2. Perfas+ Same-Implementation Results ................16\n    \
    \       6.1.3. One-Way Delay, Cross-Implementation ADK\n                  Comparison\
    \ .........................................16\n           6.1.4. Conclusions on\
    \ the ADK Results for One-Way Delay ...17\n           6.1.5. Additional Investigations\
    \ ..........................17\n      6.2. One-Way Delay, Loss Threshold, RFC\
    \ 2679 ...................20\n           6.2.1. NetProbe Results for Loss Threshold\
    \ ................21\n           6.2.2. Perfas+ Results for Loss Threshold .................21\n\
    \           6.2.3. Conclusions for Loss Threshold .....................21\n  \
    \    6.3. One-Way Delay, First Bit to Last Bit, RFC 2679 ............21\n    \
    \       6.3.1. NetProbe and Perfas+ Results for Serialization .....22\n      \
    \     6.3.2. Conclusions for Serialization ......................23\n      6.4.\
    \ One-Way Delay, Difference Sample Metric ...................24\n           6.4.1.\
    \ NetProbe Results for Differential Delay ............24\n           6.4.2. Perfas+\
    \ Results for Differential Delay .............25\n           6.4.3. Conclusions\
    \ for Differential Delay .................25\n      6.5. Implementation of Statistics\
    \ for One-Way Delay ............25\n   7. Conclusions and RFC 2679 Errata ................................26\n\
    \   8. Security Considerations ........................................26\n  \
    \ 9. Acknowledgements ...............................................27\n   10.\
    \ References ....................................................27\n      10.1.\
    \ Normative References .....................................27\n      10.2. Informative\
    \ References ...................................28\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The IETF IP Performance Metrics (IPPM) working group has\
    \ considered\n   how to advance their metrics along the Standards Track since\
    \ 2001,\n   with the initial publication of Bradner/Paxson/Mankin's memo\n   [METRICS-TEST].\
    \  The original proposal was to compare the performance\n   of metric implementations.\
    \  This was similar to the usual procedures\n   for advancing protocols, which\
    \ did not directly apply.  It was found\n   to be difficult to achieve consensus\
    \ on exactly how to compare\n   implementations, since there were many legitimate\
    \ sources of\n   variation that would emerge in the results despite the best attempts\n\
    \   to keep the network paths equal, and because considerable variation\n   was\
    \ allowed in the parameters (and therefore implementation) of each\n   metric.\
    \  Flexibility in metric definitions, essential for\n   customization and broad\
    \ appeal, made the comparison task quite\n   difficult.\n   A renewed work effort\
    \ investigated ways in which the measurement\n   variability could be reduced\
    \ and thereby simplify the problem of\n   comparison for equivalence.\n   The\
    \ consensus process documented in [RFC6576] is that metric\n   definitions rather\
    \ than the implementations of metrics should be the\n   primary focus of evaluation.\
    \  Equivalent test results are deemed to\n   be evidence that the metric specifications\
    \ are clear and unambiguous.\n   This is now the metric specification equivalent\
    \ of protocol\n   interoperability.  The [RFC6576] advancement process either\
    \ produces\n   confidence that the metric definitions and supporting material\
    \ are\n   clearly worded and unambiguous, or it identifies ways in which the\n\
    \   metric definitions should be revised to achieve clarity.\n   The metric RFC\
    \ advancement process requires documentation of the\n   testing and results. \
    \ [RFC6576] retains the testing requirement of\n   the original Standards Track\
    \ advancement process described in\n   [RFC2026] and [RFC5657], because widespread\
    \ deployment is\n   insufficient to determine whether RFCs that define performance\n\
    \   metrics result in consistent implementations.\n   The process also permits\
    \ identification of options that were not\n   implemented, so that they can be\
    \ removed from the advancing\n   specification (this is a similar aspect to protocol\
    \ advancement along\n   the Standards Track).  All errata must also be considered.\n\
    \   This memo's purpose is to implement the advancement process of\n   [RFC6576]\
    \ for [RFC2679].  It supplies the documentation that\n   accompanies the protocol\
    \ action request submitted to the Area\n   Director, including description of\
    \ the test setup, results for each\n   implementation, evaluation of each metric\
    \ specification, and\n   conclusions.\n   In particular, this memo documents the\
    \ consensus on the extent of\n   tolerable errors when assessing equivalence in\
    \ the results.  The IPPM\n   working group agreed that the test plan and procedures\
    \ should include\n   the threshold for determining equivalence, and that this\
    \ aspect\n   should be decided in advance of cross-implementation comparisons.\n\
    \   This memo includes procedures for same-implementation comparisons\n   that\
    \ may influence the equivalence threshold.\n   Although the conclusion reached\
    \ through testing is that [RFC2679]\n   should be advanced on the Standards Track\
    \ with modifications, the\n   revised text of RFC 2679 is not yet ready for review.\
    \  Therefore,\n   this memo documents the information to support [RFC2679] advancement,\n\
    \   and the approval of a revision of RFC 2769 is left for future action.\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in RFC 2119 [RFC2119].\n"
- title: 2.  A Definition-Centric Metric Advancement Process
  contents:
  - "2.  A Definition-Centric Metric Advancement Process\n   As a first principle,\
    \ the process described in Section 3.5 of\n   [RFC6576] takes the fact that the\
    \ metric definitions (embodied in the\n   text of the RFCs) are the objects that\
    \ require evaluation and\n   possible revision in order to advance to the next\
    \ step on the\n   Standards Track.  This memo follows that process.\n"
- title: 3.  Test Configuration
  contents:
  - "3.  Test Configuration\n   One metric implementation used was NetProbe version\
    \ 5.8.5 (an earlier\n   version is used in AT&T's IP network performance measurement\
    \ system\n   and deployed worldwide [WIPM]).  NetProbe uses UDP packets of\n \
    \  variable size, and it can produce test streams with Periodic\n   [RFC3432]\
    \ or Poisson [RFC2330] sample distributions.\n   The other metric implementation\
    \ used was Perfas+ version 3.1,\n   developed by Deutsche Telekom [Perfas].  Perfas+\
    \ uses UDP unicast\n   packets of variable size (but also supports TCP and multicast).\
    \  Test\n   streams with Periodic, Poisson, or uniform sample distributions may\n\
    \   be used.\n   Figure 1 shows a view of the test path as each implementation's\
    \ test\n   flows pass through the Internet and the Layer 2 Tunneling Protocol,\n\
    \   version 3 (L2TPv3) tunnel IDs (1 and 2), based on Figures 2 and 3 of\n   [RFC6576].\n\
    \           +----+  +----+                                +----+  +----+\n   \
    \        |Imp1|  |Imp1|           ,---.                |Imp2|  |Imp2|\n      \
    \     +----+  +----+          /     \\    +-------+  +----+  +----+\n        \
    \     | V100 | V200        /       \\   | Tunnel|   | V300  | V400\n         \
    \    |      |            (         )  | Head  |   |       |\n            +--------+\
    \  +------+ |         |__| Router|  +----------+\n            |Ethernet|  |Tunnel|\
    \ |Internet |  +---B---+  |Ethernet  |\n            |Switch  |--|Head  |-|   \
    \      |      |      |Switch    |\n            +-+--+---+  |Router| |        \
    \ |  +---+---+--+--+--+----+\n              |__|      +--A---+ (         )  |Network|\
    \     |__|\n                                  \\       /   |Emulat.|\n       \
    \     U-turn                 \\     /    |\"netem\"|     U-turn\n            V300\
    \ to V400            `-+-'     +-------+     V100 to V200\n           Implementations\
    \                  ,---.       +--------+\n                               +~~~~~~~~~~~/\
    \     \\~~~~~~| Remote |\n            +------->-----F2->-|          /       \\\
    \     |->---.  |\n            | +---------+      | Tunnel  (         )    |  \
    \   |  |\n            | | transmit|-F1->-|   ID 1  (         )    |->.  |  |\n\
    \            | | Imp 1   |      +~~~~~~~~~|         |~~~~|  |  |  |\n        \
    \    | | receive |-<--+           (         )    | F1  F2 |\n            | +---------+\
    \    |           |Internet |    |  |  |  |\n            *-------<-----+  F1  \
    \        |         |    |  |  |  |\n              +---------+ |  | +~~~~~~~~~|\
    \         |~~~~|  |  |  |\n              | transmit|-*  *-|         |        \
    \ |    |<-*  |  |\n              | Imp 2   |      | Tunnel  (         )    | \
    \    |  |\n              | receive |-<-F2-|   ID 2   \\       /     |<----*  |\n\
    \              +---------+      +~~~~~~~~~~~\\     /~~~~~~| Switch |\n       \
    \                                     `-+-'       +--------+\n   Illustrations\
    \ of a test setup with a bidirectional tunnel.  The upper\n   diagram emphasizes\
    \ the VLAN connectivity and geographical location.\n   The lower diagram shows\
    \ example flows traveling between two\n   measurement implementations (for simplicity,\
    \ only two flows are\n   shown).\n                                 Figure 1\n\
    \   The testing employs the Layer 2 Tunneling Protocol, version 3\n   (L2TPv3)\
    \ [RFC3931] tunnel between test sites on the Internet.  The\n   tunnel IP and\
    \ L2TPv3 headers are intended to conceal the test\n   equipment addresses and\
    \ ports from hash functions that would tend to\n   spread different test streams\
    \ across parallel network resources, with\n   likely variation in performance\
    \ as a result.\n   At each end of the tunnel, one pair of VLANs encapsulated in\
    \ the\n   tunnel are looped back so that test traffic is returned to each test\n\
    \   site.  Thus, test streams traverse the L2TP tunnel twice, but appear\n   to\
    \ be one-way tests from the test equipment point of view.\n   The network emulator\
    \ is a host running Fedora 14 Linux [Fedora14]\n   with IP forwarding enabled\
    \ and the \"netem\" Network emulator [netem]\n   loaded and operating as part\
    \ of the Fedora Kernel 2.6.35.11.\n   Connectivity across the netem/Fedora host\
    \ was accomplished by\n   bridging Ethernet VLAN interfaces together with \"brctl\"\
    \ commands\n   (e.g., eth1.100 <-> eth2.100).  The netem emulator was activated\
    \ on\n   one interface (eth1) and only operates on test streams traveling in\n\
    \   one direction.  In some tests, independent netem instances operated\n   separately\
    \ on each VLAN.\n   The links between the netem emulator host and router and switch\
    \ were\n   found to be 100baseTx-HD (100 Mbps half duplex) when the testing was\n\
    \   complete.  Use of half duplex was not intended, but probably added a\n   small\
    \ amount of delay variation that could have been avoided in full\n   duplex mode.\n\
    \   Each individual test was run with common packet rates (1 pps, 10 pps)\n  \
    \ Poisson/Periodic distributions, and IP packet sizes of 64, 340, and\n   500\
    \ Bytes.  These sizes cover a reasonable range while avoiding\n   fragmentation\
    \ and the complexities it causes, thus complying with the\n   notion of \"standard\
    \ formed packets\" described in Section 15 of\n   [RFC2330].\n   For these tests,\
    \ a stream of at least 300 packets were sent from\n   Source to Destination in\
    \ each implementation.  Periodic streams (as\n   per [RFC3432]) with 1 second\
    \ spacing were used, except as noted.\n   With the L2TPv3 tunnel in use, the metric\
    \ name for the testing\n   configured here (with respect to the IP header exposed\
    \ to Internet\n   processing) is:\n   Type-IP-protocol-115-One-way-Delay-<StreamType>-Stream\n\
    \   With (Section 4.2 of [RFC2679]) Metric Parameters:\n   + Src, the IP address\
    \ of a host (12.3.167.16 or 193.159.144.8)\n   + Dst, the IP address of a host\
    \ (193.159.144.8 or 12.3.167.16)\n   + T0, a time\n   + Tf, a time\n   + lambda,\
    \ a rate in reciprocal seconds\n   + Thresh, a maximum waiting time in seconds\
    \ (see Section 3.8.2 of\n   [RFC2679] and Section 4.3 of [RFC2679])\n   Metric\
    \ Units: A sequence of pairs; the elements of each pair are:\n   + T, a time,\
    \ and\n   + dT, either a real number or an undefined number of seconds.\n   The\
    \ values of T in the sequence are monotonic increasing.  Note that\n   T would\
    \ be a valid parameter to Type-P-One-way-Delay and that dT\n   would be a valid\
    \ value of Type-P-One-way-Delay.\n   Also, Section 3.8.4 of [RFC2679] recommends\
    \ that the path SHOULD be\n   reported.  In this test setup, most of the path\
    \ details will be\n   concealed from the implementations by the L2TPv3 tunnels;\
    \ thus, a\n   more informative path trace route can be conducted by the routers\
    \ at\n   each location.\n   When NetProbe is used in production, a traceroute\
    \ is conducted in\n   parallel with, and at the outset of, measurements.\n   Perfas+\
    \ does not support traceroute.\n IPLGW#traceroute 193.159.144.8\n Type escape\
    \ sequence to abort.\n Tracing the route to 193.159.144.8\n   1 12.126.218.245\
    \ [AS 7018] 0 msec 0 msec 4 msec\n   2 cr84.n54ny.ip.att.net (12.123.2.158) [AS\
    \ 7018] 4 msec 4 msec\n     cr83.n54ny.ip.att.net (12.123.2.26) [AS 7018] 4 msec\n\
    \   3 cr1.n54ny.ip.att.net (12.122.105.49) [AS 7018] 4 msec\n     cr2.n54ny.ip.att.net\
    \ (12.122.115.93) [AS 7018] 0 msec\n     cr1.n54ny.ip.att.net (12.122.105.49)\
    \ [AS 7018] 0 msec\n   4 n54ny02jt.ip.att.net (12.122.80.225) [AS 7018] 4 msec\
    \ 0 msec\n     n54ny02jt.ip.att.net (12.122.80.237) [AS 7018] 4 msec\n   5 192.205.34.182\
    \ [AS 7018] 0 msec\n     192.205.34.150 [AS 7018] 0 msec\n     192.205.34.182\
    \ [AS 7018] 4 msec\n   6 da-rg12-i.DA.DE.NET.DTAG.DE (62.154.1.30) [AS 3320] 88\
    \ msec 88 msec\n 88 msec\n   7 217.89.29.62 [AS 3320] 88 msec 88 msec 88 msec\n\
    \   8 217.89.29.55 [AS 3320] 88 msec 88 msec 88 msec\n   9  *  *  *\n   It was\
    \ only possible to conduct the traceroute for the measured path\n   on one of\
    \ the tunnel-head routers (the normal trace facilities of the\n   measurement\
    \ systems are confounded by the L2TPv3 tunnel\n   encapsulation).\n"
- title: 4.  Error Calibration, RFC 2679
  contents:
  - "4.  Error Calibration, RFC 2679\n   An implementation is required to report on\
    \ its error calibration in\n   Section 3.8 of [RFC2679] (also required in Section\
    \ 4.8 for sample\n   metrics).  Sections 3.6, 3.7, and 3.8 of [RFC2679] give the\
    \ detailed\n   formulation of the errors and uncertainties for calibration.  In\n\
    \   summary, Section 3.7.1 of [RFC2679] describes the total time-varying\n   uncertainty\
    \ as:\n   Esynch(t)+ Rsource + Rdest\n   where:\n   Esynch(t) denotes an upper\
    \ bound on the magnitude of clock\n   synchronization uncertainty.\n   Rsource\
    \ and Rdest denote the resolution of the source clock and the\n   destination\
    \ clock, respectively.\n   Further, Section 3.7.2 of [RFC2679] describes the total\
    \ wire-time\n   uncertainty as:\n   Hsource + Hdest\n   referring to the upper\
    \ bounds on host-time to wire-time for source\n   and destination, respectively.\n\
    \   Section 3.7.3 of [RFC2679] describes a test with small packets over\n   an\
    \ isolated minimal network where the results can be used to estimate\n   systematic\
    \ and random components of the sum of the above errors or\n   uncertainties. \
    \ In a test with hundreds of singletons, the median is\n   the systematic error\
    \ and when the median is subtracted from all\n   singletons, the remaining variability\
    \ is the random error.\n   The test context, or Type-P of the test packets, must\
    \ also be\n   reported, as required in Section 3.8 of [RFC2679] and all metrics\n\
    \   defined there.  Type-P is defined in Section 13 of [RFC2330] (as are\n   many\
    \ terms used below).\n"
- title: 4.1.  NetProbe Error and Type-P
  contents:
  - "4.1.  NetProbe Error and Type-P\n   Type-P for this test was IP-UDP with Best\
    \ Effort Differentiated\n   Services Code Point (DSCP).  These headers were encapsulated\n\
    \   according to the L2TPv3 specifications [RFC3931]; thus, they may not\n   influence\
    \ the treatment received as the packets traversed the\n   Internet.\n   In general,\
    \ NetProbe error is dependent on the specific version and\n   installation details.\n\
    \   NetProbe operates using host-time above the UDP layer, which is\n   different\
    \ from the wire-time preferred in [RFC2330], but it can be\n   identified as a\
    \ source of error according to Section 3.7.2 of\n   [RFC2679].\n   Accuracy of\
    \ NetProbe measurements is usually limited by NTP\n   synchronization performance\
    \ (which is typically taken as ~+/-1 ms\n   error or greater), although the installation\
    \ used in this testing\n   often exhibits errors much less than typical for NTP.\
    \  The primary\n   stratum 1 NTP server is closely located on a sparsely utilized\n\
    \   network management LAN; thus, it avoids many concerns raised in\n   Section\
    \ 10 of [RFC2330] (in fact, smooth adjustment, long-term drift\n   analysis and\
    \ compensation, and infrequent adjustment all lead to\n   stability during measurement\
    \ intervals, the main concern).\n   The resolution of the reported results is\
    \ 1 us (us = microsecond) in\n   the version of NetProbe tested here, which contributes\
    \ to at least\n   +/-1 us error.\n   NetProbe implements a timekeeping sanity\
    \ check on sending and\n   receiving time-stamping processes.  When a significant\
    \ process\n   interruption takes place, individual test packets are flagged as\n\
    \   possibly containing unusual time errors, and they are excluded from\n   the\
    \ sample used for all \"time\" metrics.\n   We performed a NetProbe calibration\
    \ of the type described in Section\n   3.7.3 of [RFC2679], using 64-Byte packets\
    \ over a cross-connect cable.\n   The results estimate systematic and random components\
    \ of the sum of\n   the Hsource + Hdest errors or uncertainties.  In a test with\
    \ 300\n   singletons conducted over 30 seconds (periodic sample with 100 ms\n\
    \   spacing), the median is the systematic error and the remaining\n   variability\
    \ is the random error.  One set of results is tabulated\n   below:\n   (Results\
    \ from the \"R\" software environment for statistical computing\n   and graphics\
    \ - http://www.r-project.org/ )\n   > summary(XD4CAL)\n         CAL1         \
    \   CAL2             CAL3\n    Min.   : 89.0   Min.   : 68.00   Min.   : 54.00\n\
    \    1st Qu.: 99.0   1st Qu.: 77.00   1st Qu.: 63.00\n    Median :110.0   Median\
    \ : 79.00   Median : 65.00\n    Mean   :116.8   Mean   : 83.74   Mean   : 69.65\n\
    \    3rd Qu.:127.0   3rd Qu.: 88.00   3rd Qu.: 74.00\n    Max.   :205.0   Max.\
    \   :177.00   Max.   :163.00\n   >\n   NetProbe Calibration with Cross-Connect\
    \ Cable, one-way delay values\n   in microseconds (us)\n   The median or systematic\
    \ error can be as high as 110 us, and the\n   range of the random error is also\
    \ on the order of 116 us for all\n   streams.\n   Also, anticipating the Anderson-Darling\
    \ K-sample (ADK) [ADK]\n   comparisons to follow, we corrected the CAL2 values\
    \ for the\n   difference between the means of CAL2 and CAL3 (as permitted in\n\
    \   Section 3.2 of [RFC6576]), and found strong support (for the Null\n   Hypothesis)\
    \ that the samples are from the same distribution\n   (resolution of 1 us and\
    \ alpha equal 0.05 and 0.01)\n   > XD4CVCAL2 <- XD4CAL$CAL2 - (mean(XD4CAL$CAL2)-mean(XD4CAL$CAL3))\n\
    \   > boxplot(XD4CVCAL2,XD4CAL$CAL3)\n   > XD4CV2_ADK <- adk.test(XD4CVCAL2, XD4CAL$CAL3)\n\
    \   > XD4CV2_ADK\n   Anderson-Darling k-sample test.\n   Number of samples:  2\n\
    \   Sample sizes: 300 300\n   Total number of values: 600\n   Number of unique\
    \ values: 97\n   Mean of Anderson Darling Criterion: 1\n   Standard deviation\
    \ of Anderson Darling Criterion: 0.75896\n   T = (Anderson-Darling Criterion -\
    \ mean)/sigma\n   Null Hypothesis: All samples come from a common population.\n\
    \                        t.obs P-value extrapolation\n   not adj. for ties  0.71734\
    \ 0.17042             0\n   adj. for ties     -0.39553 0.44589             1\n\
    \   >\n   using [Rtool] and [Radk].\n"
- title: 4.2.  Perfas+ Error and Type-P
  contents:
  - "4.2.  Perfas+ Error and Type-P\n   Perfas+ is configured to use GPS synchronization\
    \ and uses NTP\n   synchronization as a fall-back or default.  GPS synchronization\n\
    \   worked throughout this test with the exception of the calibration\n   stated\
    \ here (one implementation was NTP synchronized only).  The time\n   stamp accuracy\
    \ typically is 0.1 ms.\n   The resolution of the results reported by Perfas+ is\
    \ 1 us (us =\n   microsecond) in the version tested here, which contributes to\
    \ at\n   least +/-1 us error.\n   Port    5001 5002 5003\n   Min.    -227 -226\
    \  294\n   Median  -169 -167  323\n   Mean    -159 -157  335\n   Max.       6\
    \  -52  376\n   s        102  102   93\n   Perfas+ Calibration with Cross-Connect\
    \ Cable, one-way delay values in\n   microseconds (us)\n   The median or systematic\
    \ error can be as high as 323 us, and the\n   range of the random error is also\
    \ less than 232 us for all streams.\n"
- title: 5.  Predetermined Limits on Equivalence
  contents:
  - "5.  Predetermined Limits on Equivalence\n   This section provides the numerical\
    \ limits on comparisons between\n   implementations, in order to declare that\
    \ the results are equivalent\n   and therefore, the tested specification is clear.\
    \  These limits have\n   their basis in Section 3.1 of [RFC6576] and the Appendix\
    \ of\n   [RFC2330], with additional limits representing IP Performance Metrics\n\
    \   (IPPM) consensus prior to publication of results.\n   A key point is that\
    \ the allowable errors, corrections, and confidence\n   levels only need to be\
    \ sufficient to detect misinterpretation of the\n   tested specification resulting\
    \ in diverging implementations.\n   Also, the allowable error must be sufficient\
    \ to compensate for\n   measured path differences.  It was simply not possible\
    \ to measure\n   fully identical paths in the VLAN-loopback test configuration\
    \ used,\n   and this practical compromise must be taken into account.\n   For\
    \ Anderson-Darling K-sample (ADK) comparisons, the required\n   confidence factor\
    \ for the cross-implementation comparisons SHALL be\n   the smallest of:\n   o\
    \  0.95 confidence factor at 1 ms resolution, or\n   o  the smallest confidence\
    \ factor (in combination with resolution) of\n      the two same-implementation\
    \ comparisons for the same test\n      conditions.\n   A constant time accuracy\
    \ error of as much as +/-0.5 ms MAY be removed\n   from one implementation's distributions\
    \ (all singletons) before the\n   ADK comparison is conducted.\n   A constant\
    \ propagation delay error (due to use of different sub-nets\n   between the switch\
    \ and measurement devices at each location) of as\n   much as +2 ms MAY be removed\
    \ from one implementation's distributions\n   (all singletons) before the ADK\
    \ comparison is conducted.\n   For comparisons involving the mean of a sample\
    \ or other central\n   statistics, the limits on both the time accuracy error\
    \ and the\n   propagation delay error constants given above also apply.\n"
- title: 6.  Tests to Evaluate RFC 2679 Specifications
  contents:
  - "6.  Tests to Evaluate RFC 2679 Specifications\n   This section describes some\
    \ results from real-world (cross-Internet)\n   tests with measurement devices\
    \ implementing IPPM metrics and a\n   network emulator to create relevant conditions,\
    \ to determine whether\n   the metric definitions were interpreted consistently\
    \ by implementors.\n   The procedures are slightly modified from the original\
    \ procedures\n   contained in Appendix A.1 of [RFC6576].  The modifications include\n\
    \   the use of the mean statistic for comparisons.\n   Note that there are only\
    \ five instances of the requirement term\n   \"MUST\" in [RFC2679] outside of\
    \ the boilerplate and [RFC2119]\n   reference.\n"
- title: '6.1.  One-Way Delay, ADK Sample Comparison: Same- and Cross-'
  contents:
  - "6.1.  One-Way Delay, ADK Sample Comparison: Same- and Cross-\n      Implementation\n\
    \   This test determines if implementations produce results that appear\n   to\
    \ come from a common delay distribution, as an overall evaluation of\n   Section\
    \ 4 of [RFC2679], \"A Definition for Samples of One-way Delay\".\n   Same-implementation\
    \ comparison results help to set the threshold of\n   equivalence that will be\
    \ applied to cross-implementation comparisons.\n   This test is intended to evaluate\
    \ measurements in Sections 3 and 4 of\n   [RFC2679].\n   By testing the extent\
    \ to which the distributions of one-way delay\n   singletons from two implementations\
    \ of [RFC2679] appear to be from\n   the same distribution, we economize on comparisons,\
    \ because comparing\n   a set of individual summary statistics (as defined in\
    \ Section 5 of\n   [RFC2679]) would require another set of individual evaluations\
    \ of\n   equivalence.  Instead, we can simply check which statistics were\n  \
    \ implemented, and report on those facts.\n   1.  Configure an L2TPv3 path between\
    \ test sites, and each pair of\n       measurement devices to operate tests in\
    \ their designated pair of\n       VLANs.\n   2.  Measure a sample of one-way\
    \ delay singletons with two or more\n       implementations, using identical options\
    \ and network emulator\n       settings (if used).\n   3.  Measure a sample of\
    \ one-way delay singletons with *four*\n       instances of the *same* implementations,\
    \ using identical options,\n       noting that connectivity differences SHOULD\
    \ be the same as for\n       the cross-implementation testing.\n   4.  Apply the\
    \ ADK comparison procedures (see Appendices A and B of\n       [RFC6576]) and\
    \ determine the resolution and confidence factor for\n       distribution equivalence\
    \ of each same-implementation comparison\n       and each cross-implementation\
    \ comparison.\n   5.  Take the coarsest resolution and confidence factor for\n\
    \       distribution equivalence from the same-implementation pairs, or\n    \
    \   the limit defined in Section 5 above, as a limit on the\n       equivalence\
    \ threshold for these experimental conditions.\n   6.  Apply constant correction\
    \ factors to all singletons of the sample\n       distributions, as described\
    \ and limited in Section 5 above.\n   7.  Compare the cross-implementation ADK\
    \ performance with the\n       equivalence threshold determined in step 5 to determine\
    \ if\n       equivalence can be declared.\n   The common parameters used for tests\
    \ in this section are:\n   o  IP header + payload = 64 octets\n   o  Periodic\
    \ sampling at 1 packet per second\n   o  Test duration = 300 seconds (March 29,\
    \ 2011)\n   The netem emulator was set for 100 ms average delay, with uniform\n\
    \   delay variation of +/-50 ms.  In this experiment, the netem emulator\n   was\
    \ configured to operate independently on each VLAN; thus, the\n   emulator itself\
    \ is a potential source of error when comparing streams\n   that traverse the\
    \ test path in different directions.\n   In the result analysis of this section:\n\
    \   o  All comparisons used 1 microsecond resolution.\n   o  No correction factors\
    \ were applied.\n   o  The 0.95 confidence factor (1.960 for paired stream comparison)\n\
    \      was used.\n"
- title: 6.1.1.  NetProbe Same-Implementation Results
  contents:
  - "6.1.1.  NetProbe Same-Implementation Results\n   A single same-implementation\
    \ comparison fails the ADK criterion (s1\n   <-> sB).  We note that these streams\
    \ traversed the test path in\n   opposite directions, making the live network\
    \ factors a possibility to\n   explain the difference.\n   All other pair comparisons\
    \ pass the ADK criterion.\n          +------------------------------------------------------+\n\
    \          |            |             |             |             |\n        \
    \  | ti.obs (P) |     s1      |     s2      |     sA      |\n          |     \
    \       |             |             |             |\n          .............|.............|.............|.............|\n\
    \          |            |             |             |             |\n        \
    \  |    s2      | 0.25 (0.28) |             |             |\n          |     \
    \       |             |             |             |\n          ...........................|.............|.............|\n\
    \          |            |             |             |             |\n        \
    \  |    sA      | 0.60 (0.19) |-0.80 (0.57) |             |\n          |     \
    \       |             |             |             |\n          ...........................|.............|.............|\n\
    \          |            |             |             |             |\n        \
    \  |    sB      | 2.64 (0.03) | 0.07 (0.31) |-0.52 (0.48) |\n          |     \
    \       |             |             |             |\n          +------------+-------------+-------------+-------------+\n\
    \               NetProbe ADK results for same-implementation\n"
- title: 6.1.2.  Perfas+ Same-Implementation Results
  contents:
  - "6.1.2.  Perfas+ Same-Implementation Results\n   All pair comparisons pass the\
    \ ADK criterion.\n          +------------------------------------------------------+\n\
    \          |            |             |             |             |\n        \
    \  | ti.obs (P) |     p1      |     p2      |     p3      |\n          |     \
    \       |             |             |             |\n          .............|.............|.............|.............|\n\
    \          |            |             |             |             |\n        \
    \  |    p2      | 0.06 (0.32) |             |             |\n          |     \
    \       |             |             |             |\n          .........................................|.............|\n\
    \          |            |             |             |             |\n        \
    \  |    p3      | 1.09 (0.12) | 0.37 (0.24) |             |\n          |     \
    \       |             |             |             |\n          ...........................|.............|.............|\n\
    \          |            |             |             |             |\n        \
    \  |    p4      |-0.81 (0.57) |-0.13 (0.37) | 1.36 (0.09) |\n          |     \
    \       |             |             |             |\n          +------------+-------------+-------------+-------------+\n\
    \                Perfas+ ADK results for same-implementation\n"
- title: 6.1.3.  One-Way Delay, Cross-Implementation ADK Comparison
  contents:
  - "6.1.3.  One-Way Delay, Cross-Implementation ADK Comparison\n   The cross-implementation\
    \ results are compared using a combined ADK\n   analysis [Radk], where all NetProbe\
    \ results are compared with all\n   Perfas+ results after testing that the combined\
    \ same-implementation\n   results pass the ADK criterion.\n   When 4 (same) samples\
    \ are compared, the ADK criterion for 0.95\n   confidence is 1.915, and when all\
    \ 8 (cross) samples are compared it\n   is 1.85.\n   Combination of Anderson-Darling\
    \ K-Sample Tests.\n   Sample sizes within each data set:\n   Data set 1 :  299\
    \ 297 298 300 (NetProbe)\n   Data set 2 :  300 300 298 300 (Perfas+)\n   Total\
    \ sample size per data set: 1194 1198\n   Number of unique values per data set:\
    \ 1188 1192\n   ...\n   Null Hypothesis:\n   All samples within a data set come\
    \ from a common distribution.\n   The common distribution may change between data\
    \ sets.\n   NetProbe           ti.obs P-value extrapolation\n   not adj. for ties\
    \ 0.64999 0.21355             0\n   adj. for ties     0.64833 0.21392        \
    \     0\n   Perfas+\n   not adj. for ties 0.55968 0.23442             0\n   adj.\
    \ for ties     0.55840 0.23473             0\n   Combined Anderson-Darling Criterion:\n\
    \                      tc.obs P-value extrapolation\n   not adj. for ties 0.85537\
    \ 0.17967             0\n   adj. for ties     0.85329 0.18010             0\n\
    \   The combined same-implementation samples and the combined cross-\n   implementation\
    \ comparison all pass the ADK criterion at P>=0.18 and\n   support the Null Hypothesis\
    \ (both data sets come from a common\n   distribution).\n   We also see that the\
    \ paired ADK comparisons are rather critical.\n   Although the NetProbe s1-sB\
    \ comparison failed, the combined data set\n   from four streams passed the ADK\
    \ criterion easily.\n"
- title: 6.1.4.  Conclusions on the ADK Results for One-Way Delay
  contents:
  - "6.1.4.  Conclusions on the ADK Results for One-Way Delay\n   Similar testing\
    \ was repeated many times in the months of March and\n   April 2011.  There were\
    \ many experiments where a single test stream\n   from NetProbe or Perfas+ proved\
    \ to be different from the others in\n   paired comparisons (even same-implementation\
    \ comparisons).  When the\n   outlier stream was removed from the comparison,\
    \ the remaining streams\n   passed combined ADK criterion.  Also, the application\
    \ of correction\n   factors resulted in higher comparison success.\n   We conclude\
    \ that the two implementations are capable of producing\n   equivalent one-way\
    \ delay distributions based on their interpretation\n   of [RFC2679].\n"
- title: 6.1.5.  Additional Investigations
  contents:
  - "6.1.5.  Additional Investigations\n   On the final day of testing, we performed\
    \ a series of measurements to\n   evaluate the amount of emulated delay variation\
    \ necessary to achieve\n   successful ADK comparisons.  The need for correction\
    \ factors (as\n   permitted by Section 5) and the size of the measurement sample\n\
    \   (obtained as sub-sets of the complete measurement sample) were also\n   evaluated.\n\
    \   The common parameters used for tests in this section are:\n   o  IP header\
    \ + payload = 64 octets\n   o  Periodic sampling at 1 packet per second\n   o\
    \  Test duration = 300 seconds at each delay variation setting, for a\n      total\
    \ of 1200 seconds (May 2, 2011 at 1720 UTC)\n   The netem emulator was set for\
    \ 100 ms average delay, with (emulated)\n   uniform delay variation of:\n   o\
    \  +/-7.5 ms\n   o  +/-5.0 ms\n   o  +/-2.5 ms\n   o  0 ms\n   In this experiment,\
    \ the netem emulator was configured to operate\n   independently on each VLAN;\
    \ thus, the emulator itself is a potential\n   source of error when comparing\
    \ streams that traverse the test path in\n   different directions.\n   In the\
    \ result analysis of this section:\n   o  All comparisons used 1 microsecond resolution.\n\
    \   o  Correction factors *were* applied as noted (under column heading\n    \
    \  \"mean adj\").  The difference between each sample mean and the\n      lowest\
    \ mean of the NetProbe or Perfas+ stream samples was\n      subtracted from all\
    \ values in the sample. (\"raw\" indicates no\n      correction factors were used.)\
    \  All correction factors applied met\n      the limits described in Section 5.\n\
    \   o  The 0.95 confidence factor (1.960 for paired stream comparison)\n     \
    \ was used.\n   When 8 (cross) samples are compared, the ADK criterion for 0.95\n\
    \   confidence is 1.85.  The Combined ADK test statistic (\"TC observed\")\n \
    \  must be less than 1.85 to accept the Null Hypothesis (all samples in\n   the\
    \ data set are from a common distribution).\n   Emulated Delay               \
    \         Sub-Sample size\n   Variation     0ms\n   adk.combined (all)       \
    \    300 values             75 values\n   Adj. for ties           raw        \
    \ mean adj    raw        mean adj\n   TC observed             226.6563    67.51559\
    \    54.01359   21.56513\n   P-value                         0          0    \
    \       0          0\n   Mean std dev (all),us         719                   \
    \ 635\n   Mean diff of means,us         649          0         606          0\n\
    \   Variation +/- 2.5ms\n   adk.combined (all)           300 values          \
    \   75 values\n   Adj. for ties            raw        mean adj     raw       mean\
    \ adj\n   TC observed              14.50436   -1.60196     3.15935   -1.72104\n\
    \   P-value                         0     0.873      0.00799    0.89038\n   Mean\
    \ std dev (all),us        1655                   1702\n   Mean diff of means,us\
    \         471          0         513          0\n   Variation +/- 5ms\n   adk.combined\
    \ (all)           300 values             75 values\n   Adj. for ties         \
    \   raw        mean adj     raw       mean adj\n   TC observed               8.29921\
    \   -1.28927     0.37878   -1.81881\n   P-value                         0    0.81601\
    \     0.29984    0.90305\n   Mean std dev (all),us        3023               \
    \    2991\n   Mean diff of means,us         582          0         513       \
    \   0\n   Variation +/- 7.5ms\n   adk.combined (all)           300 values    \
    \         75 values\n   Adj. for ties            raw        mean adj     raw \
    \      mean adj\n   TC observed              2.53759    -0.72985     0.29241 \
    \  -1.15840\n   P-value                  0.01950     0.66942     0.32585    0.78686\n\
    \   Mean std dev (all),us        4449                   4506\n   Mean diff of\
    \ means,us         426          0         856          0\n   From the table above,\
    \ we conclude the following:\n   1.  None of the raw or mean adjusted results\
    \ pass the ADK criterion\n       with 0 ms emulated delay variation.  Use of the\
    \ 75 value sub-\n       sample yielded the same conclusion.  (We note the same\
    \ results\n       when comparing same-implementation samples for both NetProbe\
    \ and\n       Perfas+.)\n   2.  When the smallest emulated delay variation was\
    \ inserted (+/-2.5\n       ms), the mean adjusted samples pass the ADK criterion\
    \ and the\n       high P-value supports the result.  The raw results do not pass.\n\
    \   3.  At higher values of emulated delay variation (+/-5.0 ms and\n       +/-7.5\
    \ ms), again the mean adjusted values pass ADK.  We also see\n       that the\
    \ 75-value sub-sample passed the ADK in both raw and mean\n       adjusted cases.\
    \  This indicates that sample size may have played\n       a role in our results,\
    \ as noted in the Appendix of [RFC2330] for\n       Goodness-of-Fit testing.\n\
    \   We note that 150 value sub-samples were also evaluated, with ADK\n   conclusions\
    \ that followed the results for 300 values.  Also, same-\n   implementation analysis\
    \ was conducted with results similar to the\n   above, except that more of the\
    \ \"raw\" or uncorrected samples passed\n   the ADK criterion.\n"
- title: 6.2.  One-Way Delay, Loss Threshold, RFC 2679
  contents:
  - "6.2.  One-Way Delay, Loss Threshold, RFC 2679\n   This test determines if implementations\
    \ use the same configured\n   maximum waiting time delay from one measurement\
    \ to another under\n   different delay conditions, and correctly declare packets\
    \ arriving in\n   excess of the waiting time threshold as lost.\n   See the requirements\
    \ of Section 3.5 of [RFC2679], third bullet point,\n   and also Section 3.8.2\
    \ of [RFC2679].\n   1.  configure an L2TPv3 path between test sites, and each\
    \ pair of\n       measurement devices to operate tests in their designated pair\
    \ of\n       VLANs.\n   2.  configure the network emulator to add 1.0 sec. one-way\
    \ constant\n       delay in one direction of transmission.\n   3.  measure (average)\
    \ one-way delay with two or more implementations,\n       using identical waiting\
    \ time thresholds (Thresh) for loss set at\n       3 seconds.\n   4.  configure\
    \ the network emulator to add 3 sec. one-way constant\n       delay in one direction\
    \ of transmission equivalent to 2 seconds of\n       additional one-way delay\
    \ (or change the path delay while test is\n       in progress, when there are\
    \ sufficient packets at the first delay\n       setting).\n   5.  repeat/continue\
    \ measurements.\n   6.  observe that the increase measured in step 5 caused all\
    \ packets\n       with 2 sec. additional delay to be declared lost, and that all\n\
    \       packets that arrive successfully in step 3 are assigned a valid\n    \
    \   one-way delay.\n   The common parameters used for tests in this section are:\n\
    \   o  IP header + payload = 64 octets\n   o  Poisson sampling at lambda = 1 packet\
    \ per second\n   o  Test duration = 900 seconds total (March 21, 2011)\n   The\
    \ netem emulator was set to add constant delays as specified in the\n   procedure\
    \ above.\n"
- title: 6.2.1.  NetProbe Results for Loss Threshold
  contents:
  - "6.2.1.  NetProbe Results for Loss Threshold\n   In NetProbe, the Loss Threshold\
    \ is implemented uniformly over all\n   packets as a post-processing routine.\
    \  With the Loss Threshold set at\n   3 seconds, all packets with one-way delay\
    \ >3 seconds are marked\n   \"Lost\" and included in the Lost Packet list with\
    \ their transmission\n   time (as required in Section 3.3 of [RFC2680]).  This\
    \ resulted in 342\n   packets designated as lost in one of the test streams (with\
    \ average\n   delay = 3.091 sec.).\n"
- title: 6.2.2.  Perfas+ Results for Loss Threshold
  contents:
  - "6.2.2.  Perfas+ Results for Loss Threshold\n   Perfas+ uses a fixed Loss Threshold\
    \ that was not adjustable during\n   this study.  The Loss Threshold is approximately\
    \ one minute, and\n   emulation of a delay of this size was not attempted.  However,\
    \ it is\n   possible to implement any delay threshold desired with a post-\n \
    \  processing routine and subsequent analysis.  Using this method, 195\n   packets\
    \ would be declared lost (with average delay = 3.091 sec.).\n"
- title: 6.2.3.  Conclusions for Loss Threshold
  contents:
  - "6.2.3.  Conclusions for Loss Threshold\n   Both implementations assume that any\
    \ constant delay value desired can\n   be used as the Loss Threshold, since all\
    \ delays are stored as a pair\n   <Time, Delay> as required in [RFC2679].  This\
    \ is a simple way to\n   enforce the constant loss threshold envisioned in [RFC2679]\
    \ (see\n   specific section references above).  We take the position that the\n\
    \   assumption of post-processing is compliant and that the text of the\n   RFC\
    \ should be revised slightly to include this point.\n"
- title: 6.3.  One-Way Delay, First Bit to Last Bit, RFC 2679
  contents:
  - "6.3.  One-Way Delay, First Bit to Last Bit, RFC 2679\n   This test determines\
    \ if implementations register the same relative\n   change in delay from one packet\
    \ size to another, indicating that the\n   first-to-last time-stamping convention\
    \ has been followed.  This test\n   tends to cancel the sources of error that\
    \ may be present in an\n   implementation.\n   See the requirements of Section\
    \ 3.7.2 of [RFC2679], and Section 10.2\n   of [RFC2330].\n   1.  configure an\
    \ L2TPv3 path between test sites, and each pair of\n       measurement devices\
    \ to operate tests in their designated pair of\n       VLANs, and ideally including\
    \ a low-speed link (it was not\n       possible to change the link configuration\
    \ during testing, so the\n       lowest speed link present was the basis for serialization\
    \ time\n       comparisons).\n   2.  measure (average) one-way delay with two\
    \ or more implementations,\n       using identical options and equal size small\
    \ packets (64-octet IP\n       header and payload).\n   3.  maintain the same\
    \ path with additional emulated 100 ms one-way\n       delay.\n   4.  measure\
    \ (average) one-way delay with two or more implementations,\n       using identical\
    \ options and equal size large packets (500 octet\n       IP header and payload).\n\
    \   5.  observe that the increase measured between steps 2 and 4 is\n       equivalent\
    \ to the increase in ms expected due to the larger\n       serialization time\
    \ for each implementation.  Most of the\n       measurement errors in each system\
    \ should cancel, if they are\n       stationary.\n   The common parameters used\
    \ for tests in this section are:\n   o  IP header + payload = 64 octets\n   o\
    \  Periodic sampling at l packet per second\n   o  Test duration = 300 seconds\
    \ total (April 12)\n   The netem emulator was set to add constant 100 ms delay.\n"
- title: 6.3.1.  NetProbe and Perfas+ Results for Serialization
  contents:
  - "6.3.1.  NetProbe and Perfas+ Results for Serialization\n   When the IP header\
    \ + payload size was increased from 64 octets to 500\n   octets, there was a delay\
    \ increase observed.\n   Mean Delays in us\n   NetProbe\n   Payload    s1    \
    \  s2      sA      sB\n   500    190893  191179  190892  190971\n    64    189642\
    \  189785  189747  189467\n   Diff     1251    1394    1145    1505\n   Perfas\n\
    \   Payload    p1      p2      p3      p4\n   500    190908  190911  191126  190709\n\
    \    64    189706  189752  189763  190220\n   Diff     1202   1159    1363   \
    \   489\n   Serialization tests, all values in microseconds\n   The typical delay\
    \ increase when the larger packets were used was 1.1\n   to 1.5 ms (with one outlier).\
    \  The typical measurements indicate that\n   a link with approximately 3 Mbit/s\
    \ capacity is present on the path.\n   Through investigation of the facilities\
    \ involved, it was determined\n   that the lowest speed link was approximately\
    \ 45 Mbit/s, and therefore\n   the estimated difference should be about 0.077\
    \ ms.  The observed\n   differences are much higher.\n   The unexpected large\
    \ delay difference was also the outcome when\n   testing serialization times in\
    \ a lab environment, using the NIST Net\n   Emulator and NetProbe [ADV-METRICS].\n"
- title: 6.3.2.  Conclusions for Serialization
  contents:
  - "6.3.2.  Conclusions for Serialization\n   Since it was not possible to confirm\
    \ the estimated serialization time\n   increases in field tests, we resort to\
    \ examination of the\n   implementations to determine compliance.\n   NetProbe\
    \ performs all time stamping above the IP layer, accepting\n   that some compromises\
    \ must be made to achieve extreme portability and\n   measurement scale.  Therefore,\
    \ the first-to-last bit convention is\n   supported because the serialization\
    \ time is included in the one-way\n   delay measurement, enabling comparison with\
    \ other implementations.\n   Perfas+ is optimized for its purpose and performs\
    \ all time stamping\n   close to the interface hardware.  The first-to-last bit\
    \ convention is\n   supported because the serialization time is included in the\
    \ one-way\n   delay measurement, enabling comparison with other implementations.\n"
- title: 6.4.  One-Way Delay, Difference Sample Metric
  contents:
  - "6.4.  One-Way Delay, Difference Sample Metric\n   This test determines if implementations\
    \ register the same relative\n   increase in delay from one measurement to another\
    \ under different\n   delay conditions.  This test tends to cancel the sources\
    \ of error\n   that may be present in an implementation.\n   This test is intended\
    \ to evaluate measurements in Sections 3 and 4 of\n   [RFC2679].\n   1.  configure\
    \ an L2TPv3 path between test sites, and each pair of\n       measurement devices\
    \ to operate tests in their designated pair of\n       VLANs.\n   2.  measure\
    \ (average) one-way delay with two or more implementations,\n       using identical\
    \ options.\n   3.  configure the path with X+Y ms one-way delay.\n   4.  repeat\
    \ measurements.\n   5.  observe that the (average) increase measured in steps\
    \ 2 and 4 is\n       ~Y ms for each implementation.  Most of the measurement errors\
    \ in\n       each system should cancel, if they are stationary.\n   In this test,\
    \ X = 1000 ms and Y = 1000 ms.\n   The common parameters used for tests in this\
    \ section are:\n   o  IP header + payload = 64 octets\n   o  Poisson sampling\
    \ at lambda = 1 packet per second\n   o  Test duration = 900 seconds total (March\
    \ 21, 2011)\n   The netem emulator was set to add constant delays as specified\
    \ in the\n   procedure above.\n"
- title: 6.4.1.  NetProbe Results for Differential Delay
  contents:
  - "6.4.1.  NetProbe Results for Differential Delay\n         Average pre-increase\
    \ delay, microseconds        1089868.0\n         Average post 1 s additional,\
    \ microseconds        2089686.0\n         Difference (should be ~= Y = 1 s)  \
    \               999818.0\n               Average delays before/after 1 second\
    \ increase\n   The NetProbe implementation observed a 1 second increase with a\
    \ 182\n   microsecond error (assuming that the netem emulated delay difference\n\
    \   is exact).\n   We note that this differential delay test has been run under\
    \ lab\n   conditions and published in prior work [ADV-METRICS].  The error was\n\
    \   6 microseconds.\n"
- title: 6.4.2.  Perfas+ Results for Differential Delay
  contents:
  - "6.4.2.  Perfas+ Results for Differential Delay\n         Average pre-increase\
    \ delay, microseconds        1089794.0\n         Average post 1 s additional,\
    \ microseconds        2089801.0\n         Difference (should be ~= Y = 1 s)  \
    \              1000007.0\n               Average delays before/after 1 second\
    \ increase\n   The Perfas+ implementation observed a 1 second increase with a\
    \ 7\n   microsecond error.\n"
- title: 6.4.3.  Conclusions for Differential Delay
  contents:
  - "6.4.3.  Conclusions for Differential Delay\n   Again, the live network conditions\
    \ appear to have influenced the\n   results, but both implementations measured\
    \ the same delay increase\n   within their calibration accuracy.\n"
- title: 6.5.  Implementation of Statistics for One-Way Delay
  contents:
  - "6.5.  Implementation of Statistics for One-Way Delay\n   The ADK tests the extent\
    \ to which the sample distributions of one-way\n   delay singletons from two implementations\
    \ of [RFC2679] appear to be\n   from the same overall distribution.  By testing\
    \ this way, we\n   economize on the number of comparisons, because comparing a\
    \ set of\n   individual summary statistics (as defined in Section 5 of [RFC2679])\n\
    \   would require another set of individual evaluations of equivalence.\n   Instead,\
    \ we can simply check which statistics were implemented, and\n   report on those\
    \ facts, noting that Section 5 of [RFC2679] does not\n   specify the calculations\
    \ exactly, and gives only some illustrative\n   examples.\n                  \
    \                               NetProbe  Perfas+\n   5.1. Type-P-One-way-Delay-Percentile\
    \            yes       no\n   5.2. Type-P-One-way-Delay-Median               \
    \ yes       no\n   5.3. Type-P-One-way-Delay-Minimum               yes       yes\n\
    \   5.4. Type-P-One-way-Delay-Inverse-Percentile    no        no\n           \
    \       Implementation of Section 5 Statistics\n   Only the Type-P-One-way-Delay-Inverse-Percentile\
    \ has been ignored in\n   both implementations, so it is a candidate for removal\
    \ or deprecation\n   in a revision of RFC 2679 (this small discrepancy does not\
    \ affect\n   candidacy for advancement).\n"
- title: 7.  Conclusions and RFC 2679 Errata
  contents:
  - "7.  Conclusions and RFC 2679 Errata\n   The conclusions throughout Section 6\
    \ support the advancement of\n   [RFC2679] to the next step of the Standards Track,\
    \ because its\n   requirements are deemed to be clear and unambiguous based on\n\
    \   evaluation of the test results for two implementations.  The results\n   indicate\
    \ that these implementations produced statistically equivalent\n   results under\
    \ network conditions that were configured to be as close\n   to identical as possible.\n\
    \   Sections 6.2.3 and 6.5 indicate areas where minor revisions are\n   warranted\
    \ in RFC 2679.  The IETF has reached consensus on guidance\n   for reporting metrics\
    \ in [RFC6703], and this memo should be\n   referenced in the revision to RFC\
    \ 2679 to incorporate recent\n   experience where appropriate.\n   We note that\
    \ there is currently one erratum with status \"Held for\n   Document Update\"\
    \ for [RFC2679], and it appears this minor revision\n   and additional text should\
    \ be incorporated in a revision of RFC 2679.\n   The authors that revise [RFC2679]\
    \ should review all errata filed at\n   the time the document is being written.\
    \  They should not rely upon\n   this document to indicate all relevant errata\
    \ updates.\n"
- title: 8.  Security Considerations
  contents:
  - "8.  Security Considerations\n   The security considerations that apply to any\
    \ active measurement of\n   live networks are relevant here as well.  See [RFC4656]\
    \ and\n   [RFC5357].\n"
- title: 9.  Acknowledgements
  contents:
  - "9.  Acknowledgements\n   The authors thank Lars Eggert for his continued encouragement\
    \ to\n   advance the IPPM metrics during his tenure as AD Advisor.\n   Nicole\
    \ Kowalski supplied the needed CPE router for the NetProbe side\n   of the test\
    \ setup, and graciously managed her testing in spite of\n   issues caused by dual-use\
    \ of the router.  Thanks Nicole!\n   The \"NetProbe Team\" also acknowledges many\
    \ useful discussions with\n   Ganga Maguluri.\n"
- title: 10.  References
  contents:
  - '10.  References

    '
- title: 10.1.  Normative References
  contents:
  - "10.1.  Normative References\n   [RFC2026]  Bradner, S., \"The Internet Standards\
    \ Process -- Revision\n              3\", BCP 9, RFC 2026, October 1996.\n   [RFC2119]\
    \  Bradner, S., \"Key words for use in RFCs to Indicate\n              Requirement\
    \ Levels\", BCP 14, RFC 2119, March 1997.\n   [RFC2330]  Paxson, V., Almes, G.,\
    \ Mahdavi, J., and M. Mathis,\n              \"Framework for IP Performance Metrics\"\
    , RFC 2330,\n              May 1998.\n   [RFC2679]  Almes, G., Kalidindi, S.,\
    \ and M. Zekauskas, \"A One-way\n              Delay Metric for IPPM\", RFC 2679,\
    \ September 1999.\n   [RFC2680]  Almes, G., Kalidindi, S., and M. Zekauskas, \"\
    A One-way\n              Packet Loss Metric for IPPM\", RFC 2680, September 1999.\n\
    \   [RFC3432]  Raisanen, V., Grotefeld, G., and A. Morton, \"Network\n       \
    \       performance measurement with periodic streams\", RFC 3432,\n         \
    \     November 2002.\n   [RFC4656]  Shalunov, S., Teitelbaum, B., Karp, A., Boote,\
    \ J., and M.\n              Zekauskas, \"A One-way Active Measurement Protocol\n\
    \              (OWAMP)\", RFC 4656, September 2006.\n   [RFC5357]  Hedayat, K.,\
    \ Krzanowski, R., Morton, A., Yum, K., and J.\n              Babiarz, \"A Two-Way\
    \ Active Measurement Protocol (TWAMP)\",\n              RFC 5357, October 2008.\n\
    \   [RFC5657]  Dusseault, L. and R. Sparks, \"Guidance on Interoperation\n   \
    \           and Implementation Reports for Advancement to Draft\n            \
    \  Standard\", BCP 9, RFC 5657, September 2009.\n   [RFC6576]  Geib, R., Morton,\
    \ A., Fardid, R., and A. Steinmitz, \"IP\n              Performance Metrics (IPPM)\
    \ Standard Advancement Testing\",\n              BCP 176, RFC 6576, March 2012.\n\
    \   [RFC6703]  Morton, A., Ramachandran, G., and G. Maguluri, \"Reporting\n  \
    \            IP Network Performance Metrics: Different Points of View\",\n   \
    \           RFC 6703, August 2012.\n"
- title: 10.2.  Informative References
  contents:
  - "10.2.  Informative References\n   [ADK]      Scholz, F. and M. Stephens, \"K-sample\
    \ Anderson-Darling\n              Tests of fit, for continuous and discrete cases\"\
    ,\n              University of Washington, Technical Report No. 81,\n        \
    \      May 1986.\n   [ADV-METRICS]\n              Morton, A., \"Lab Test Results\
    \ for Advancing Metrics on the\n              Standards Track\", Work in Progress,\
    \ October 2010.\n   [Fedora14] Fedora Project, \"Fedora Project Home Page\", 2012,\n\
    \              <http://fedoraproject.org/>.\n   [METRICS-TEST]\n             \
    \ Bradner, S. and V. Paxson, \"Advancement of metrics\n              specifications\
    \ on the IETF Standards Track\", Work\n              in Progress, August 2007.\n\
    \   [Perfas]   Heidemann, C., \"Qualitaet in IP-Netzen Messverfahren\",\n    \
    \          published by ITG Fachgruppe, 2nd meeting 5.2.3 (NGN),\n           \
    \   November 2001, <http://www.itg523.de/oeffentlich/01nov/\n              Heidemann_QOS_Messverfahren.pdf>.\n\
    \   [RFC3931]  Lau, J., Townsley, M., and I. Goyret, \"Layer Two Tunneling\n \
    \             Protocol - Version 3 (L2TPv3)\", RFC 3931, March 2005.\n   [Radk]\
    \     Scholz, F., \"adk: Anderson-Darling K-Sample Test and\n              Combinations\
    \ of Such Tests. R package version 1.0.\", 2008.\n   [Rtool]    R Development\
    \ Core Team, \"R: A language and environment\n              for statistical computing.\
    \ R Foundation for Statistical\n              Computing, Vienna, Austria. ISBN\
    \ 3-900051-07-0\", 2011,\n              <http://www.R-project.org/>.\n   [WIPM]\
    \     AT&T, \"AT&T Global IP Network\", 2012,\n              <http://ipnetwork.bgtmo.ip.att.net/pws/index.html>.\n\
    \   [netem]    The Linux Foundation, \"netem\", 2009,\n              <http://www.linuxfoundation.org/collaborate/workgroups/\n\
    \              networking/netem>.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Len Ciavattone\n   AT&T Labs\n   200 Laurel Avenue South\n\
    \   Middletown, NJ  07748\n   USA\n   Phone: +1 732 420 1239\n   EMail: lencia@att.com\n\
    \   Ruediger Geib\n   Deutsche Telekom\n   Heinrich Hertz Str. 3-7\n   Darmstadt,\
    \   64295\n   Germany\n   Phone: +49 6151 58 12747\n   EMail: Ruediger.Geib@telekom.de\n\
    \   Al Morton\n   AT&T Labs\n   200 Laurel Avenue South\n   Middletown, NJ  07748\n\
    \   USA\n   Phone: +1 732 420 1571\n   Fax:   +1 732 368 1192\n   EMail: acmorton@att.com\n\
    \   URI:   http://home.comcast.net/~acmacm/\n   Matthias Wieser\n   Technical\
    \ University Darmstadt\n   Darmstadt,\n   Germany\n   EMail: matthias_michael.wieser@stud.tu-darmstadt.de\n"
