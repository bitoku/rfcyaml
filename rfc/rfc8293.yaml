- title: __initial_text__
  contents:
  - '    A Framework for Multicast in Network Virtualization over Layer 3

    '
- title: Abstract
  contents:
  - "Abstract\n   This document provides a framework for supporting multicast traffic\n\
    \   in a network that uses Network Virtualization over Layer 3 (NVO3).\n   Both\
    \ infrastructure multicast and application-specific multicast are\n   discussed.\
    \  It describes the various mechanisms that can be used for\n   delivering such\
    \ traffic as well as the data plane and control plane\n   considerations for each\
    \ of the mechanisms.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   https://www.rfc-editor.org/info/rfc8293.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2018 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (https://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n     1.1.  Infrastructure Multicast  . . . . . . . . . . . .\
    \ . . . .   3\n     1.2.  Application-Specific Multicast  . . . . . . . . . .\
    \ . . .   4\n   2.  Terminology and Abbreviations . . . . . . . . . . . . . .\
    \ . .   4\n   3.  Multicast Mechanisms in Networks That Use NVO3  . . . . . .\
    \ .   5\n     3.1.  No Multicast Support  . . . . . . . . . . . . . . . . . .\
    \   6\n     3.2.  Replication at the Source NVE . . . . . . . . . . . . . .  \
    \ 6\n     3.3.  Replication at a Multicast Service Node . . . . . . . . .   8\n\
    \     3.4.  IP Multicast in the Underlay  . . . . . . . . . . . . . .  10\n  \
    \   3.5.  Other Schemes . . . . . . . . . . . . . . . . . . . . . .  11\n   4.\
    \  Simultaneous Use of More Than One Mechanism . . . . . . . . .  12\n   5.  Other\
    \ Issues  . . . . . . . . . . . . . . . . . . . . . . . .  12\n     5.1.  Multicast-Agnostic\
    \ NVEs . . . . . . . . . . . . . . . . .  12\n     5.2.  Multicast Membership\
    \ Management for DC with VMs . . . . .  13\n   6.  Security Considerations . .\
    \ . . . . . . . . . . . . . . . . .  13\n   7.  IANA Considerations . . . . .\
    \ . . . . . . . . . . . . . . . .  13\n   8.  Summary . . . . . . . . . . . .\
    \ . . . . . . . . . . . . . . .  13\n   9.  References  . . . . . . . . . . .\
    \ . . . . . . . . . . . . . .  13\n     9.1.  Normative References  . . . . .\
    \ . . . . . . . . . . . . .  13\n     9.2.  Informative References  . . . . .\
    \ . . . . . . . . . . . .  14\n   Acknowledgments . . . . . . . . . . . . . .\
    \ . . . . . . . . . . .  17\n   Authors' Addresses  . . . . . . . . . . . . .\
    \ . . . . . . . . . .  17\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Network Virtualization over Layer 3 (NVO3) [RFC7365] is\
    \ a technology\n   that is used to address issues that arise in building large,\
    \ multi-\n   tenant data centers (DCs) that make extensive use of server\n   virtualization\
    \ [RFC7364].\n   This document provides a framework for supporting multicast traffic\n\
    \   in a network that uses NVO3.  Both infrastructure multicast and\n   application-specific\
    \ multicast are considered.  It describes various\n   mechanisms, and the considerations\
    \ of each of them, that can be used\n   for delivering such traffic in networks\
    \ that use NVO3.\n   The reader is assumed to be familiar with the terminology\
    \ and\n   concepts as defined in the NVO3 Framework [RFC7365] and NVO3\n   Architecture\
    \ [RFC8014] documents.\n"
- title: 1.1.  Infrastructure Multicast
  contents:
  - "1.1.  Infrastructure Multicast\n   Infrastructure multicast refers to networking\
    \ services that require\n   multicast or broadcast delivery, such as Address Resolution\
    \ Protocol\n   (ARP), Neighbor Discovery (ND), Dynamic Host Configuration Protocol\n\
    \   (DHCP), multicast Domain Name Server (mDNS), etc., some of which are\n   described\
    \ in Sections 5 and 6 of RFC 3819 [RFC3819].  It is possible\n   to provide solutions\
    \ for these services that do not involve multicast\n   in the underlay network.\
    \  For example, in the case of ARP/ND, a\n   Network Virtualization Authority\
    \ (NVA) can be used for distributing\n   the IP address to Media Access Control\
    \ (MAC) address mappings to all\n   of the Network Virtualization Edges (NVEs).\
    \  An NVE can then trap ARP\n   Request and/or ND Neighbor Solicitation messages\
    \ from the Tenant\n   Systems (TSs) that are attached to it and respond to them,\
    \ thereby\n   eliminating the need for the broadcast/multicast of such messages.\n\
    \   In the case of DHCP, the NVE can be configured to forward these\n   messages\
    \ using the DHCP relay function [RFC2131].\n   Of course, it is possible to support\
    \ all of these infrastructure\n   multicast protocols natively if the underlay\
    \ provides multicast\n   transport.  However, even in the presence of multicast\
    \ transport, it\n   may be beneficial to use the optimizations mentioned above\
    \ to reduce\n   the amount of such traffic in the network.\n"
- title: 1.2.  Application-Specific Multicast
  contents:
  - "1.2.  Application-Specific Multicast\n   Application-specific multicast traffic\
    \ refers to multicast traffic\n   that originates and is consumed by user applications.\
    \  Several such\n   applications are described elsewhere [DC-MC].  Application-specific\n\
    \   multicast may be either Source-Specific Multicast (SSM) or Any-Source\n  \
    \ Multicast (ASM) [RFC3569] and has the following characteristics:\n   1.  Receiver\
    \ hosts are expected to subscribe to multicast content\n       using protocols\
    \ such as IGMP [RFC3376] (IPv4) or Multicast\n       Listener Discovery (MLD)\
    \ [RFC2710] (IPv6).  Multicast sources and\n       listeners participate in these\
    \ protocols using addresses that are\n       in the TS address domain.\n   2.\
    \  The set of multicast listeners for each multicast group may not\n       be\
    \ known in advance.  Therefore, it may not be possible or\n       practical for\
    \ an NVA to get the list of participants for each\n       multicast group ahead\
    \ of time.\n"
- title: 2.  Terminology and Abbreviations
  contents:
  - "2.  Terminology and Abbreviations\n   In this document, the terms host, Tenant\
    \ System (TS), and Virtual\n   Machine (VM) are used interchangeably to represent\
    \ an end station\n   that originates or consumes data packets.\n   ASM:  Any-Source\
    \ Multicast\n   IGMP: Internet Group Management Protocol\n   LISP: Locator/ID\
    \ Separation Protocol\n   MSN:  Multicast Service Node\n   RLOC: Routing Locator\n\
    \   NVA:  Network Virtualization Authority\n   NVE:  Network Virtualization Edge\n\
    \   NVGRE:  Network Virtualization using GRE\n   PIM:  Protocol-Independent Multicast\n\
    \   SSM:  Source-Specific Multicast\n   TS:   Tenant System\n   VM:   Virtual\
    \ Machine\n   VN:   Virtual Network\n   VTEP: VXLAN Tunnel End Point\n   VXLAN:\
    \  Virtual eXtensible LAN\n"
- title: 3.  Multicast Mechanisms in Networks That Use NVO3
  contents:
  - "3.  Multicast Mechanisms in Networks That Use NVO3\n   In NVO3 environments,\
    \ traffic between NVEs is transported using an\n   encapsulation such as VXLAN\
    \ [RFC7348] [VXLAN-GPE], Network\n   Virtualization using Generic Routing Encapsulation\
    \ (NVGRE) [RFC7637],\n   Geneve [Geneve], Generic UDP Encapsulation [GUE], etc.\n\
    \   What makes networks using NVO3 different from other networks is that\n   some\
    \ NVEs, especially NVEs implemented in servers, might not support\n   regular\
    \ multicast protocols such as PIM.  Instead, the only\n   capability they may\
    \ support would be that of encapsulating data\n   packets from VMs with an outer\
    \ unicast header.  Therefore, it is\n   important for networks using NVO3 to have\
    \ mechanisms to support\n   multicast as a network capability for NVEs, to map\
    \ multicast traffic\n   from VMs (users/applications) to an equivalent multicast\
    \ capability\n   inside the NVE, or to figure out the outer destination address\
    \ if NVE\n   does not support native multicast (e.g., PIM) or IGMP.\n   With NVO3,\
    \ there are many possible ways that multicast may be handled\n   in such networks.\
    \  We discuss some of the attributes of the following\n   four methods:\n   1.\
    \  No multicast support\n   2.  Replication at the source NVE\n   3.  Replication\
    \ at a multicast service node\n   4.  IP multicast in the underlay\n   These methods\
    \ are briefly mentioned in the NVO3 Framework [RFC7365]\n   and NVO3 Architecture\
    \ [RFC8014] documents.  This document provides\n   more details about the basic\
    \ mechanisms underlying each of these\n   methods and discusses the issues and\
    \ trade-offs of each.\n   We note that other methods are also possible, such as\
    \ [EDGE-REP], but\n   we focus on the above four because they are the most common.\n\
    \   It is worth noting that when selecting a multicast mechanism, it is\n   useful\
    \ to consider the impact of these on any multicast congestion\n   control mechanisms\
    \ that applications may be using to obtain the\n   desired system dynamics.  In\
    \ addition, the same rules for Explicit\n   Congestion Notification (ECN) would\
    \ apply to multicast traffic being\n   encapsulated, as for unicast traffic [RFC6040].\n"
- title: 3.1.  No Multicast Support
  contents:
  - "3.1.  No Multicast Support\n   In this scenario, there is no support whatsoever\
    \ for multicast\n   traffic when using the overlay.  This method can only work\
    \ if the\n   following conditions are met:\n   1.  All of the application traffic\
    \ in the network is unicast traffic,\n       and the only multicast/broadcast\
    \ traffic is from ARP/ND\n       protocols.\n   2.  An NVA is used by all of the\
    \ NVEs to determine the mapping of a\n       given TS's MAC and IP address to\
    \ the NVE that it is attached to.\n       In other words, there is no data-plane\
    \ learning.  Address\n       resolution requests via ARP/ND that are issued by\
    \ the TSs must be\n       resolved by the NVE that they are attached to.\n   With\
    \ this approach, it is not possible to support application-\n   specific multicast.\
    \  However, certain multicast/broadcast\n   applications can be supported without\
    \ multicast; for example, DHCP,\n   which can be supported by use of DHCP relay\
    \ function [RFC2131].\n   The main drawback of this approach, even for unicast\
    \ traffic, is that\n   it is not possible to initiate communication with a TS\
    \ for which a\n   mapping to an NVE does not already exist at the NVA.  This is\
    \ a\n   problem in the case where the NVE is implemented in a physical switch\n\
    \   and the TS is a physical end station that has not registered with the\n  \
    \ NVA.\n"
- title: 3.2.  Replication at the Source NVE
  contents:
  - "3.2.  Replication at the Source NVE\n   With this method, the overlay attempts\
    \ to provide a multicast service\n   without requiring any specific support from\
    \ the underlay, other than\n   that of a unicast service.  A multicast or broadcast\
    \ transmission is\n   achieved by replicating the packet at the source NVE and\
    \ making\n   copies, one for each destination NVE that the multicast packet must\n\
    \   be sent to.\n   For this mechanism to work, the source NVE must know, a priori,\
    \ the\n   IP addresses of all destination NVEs that need to receive the packet.\n\
    \   For the purpose of ARP/ND, this would involve knowing the IP\n   addresses\
    \ of all the NVEs that have TSs in the VN of the TS that\n   generated the request.\n\
    \   For the support of application-specific multicast traffic, a method\n   similar\
    \ to that of receiver-sites registration for a particular\n   multicast group,\
    \ described in [LISP-Signal-Free], can be used.  The\n   registrations from different\
    \ receiver sites can be merged at the NVA,\n   which can construct a multicast\
    \ replication list inclusive of all\n   NVEs to which receivers for a particular\
    \ multicast group are\n   attached.  The replication list for each specific multicast\
    \ group is\n   maintained by the NVA.  Note that using receiver-sites registration\n\
    \   does not necessarily mean the source NVE must do replication.  If the\n  \
    \ NVA indicates that multicast packets are encapsulated to multicast\n   service\
    \ nodes, then there would be no replication at the NVE.\n   The receiver-sites\
    \ registration is achieved by egress NVEs performing\n   IGMP/MLD snooping to\
    \ maintain the state for which attached TSs have\n   subscribed to a given IP\
    \ multicast group.  When the members of a\n   multicast group are outside the\
    \ NVO3 domain, it is necessary for NVO3\n   gateways to keep track of the remote\
    \ members of each multicast group.\n   The NVEs and NVO3 gateways then communicate\
    \ with the multicast groups\n   that are of interest to the NVA.  If the membership\
    \ is not\n   communicated to the NVA, and if it is necessary to prevent TSs\n\
    \   attached to an NVE that have not subscribed to a multicast group from\n  \
    \ receiving the multicast traffic, the NVE would need to maintain\n   multicast\
    \ group membership information.\n   In the absence of IGMP/MLD snooping, the traffic\
    \ would be delivered\n   to all TSs that are part of the VN.\n   In multihoming\
    \ environments, i.e., in those where a TS is attached to\n   more than one NVE,\
    \ the NVA would be expected to provide information\n   to all of the NVEs under\
    \ its control about all of the NVEs to which\n   such a TS is attached.  The ingress\
    \ NVE can then choose any one of\n   those NVEs as the egress NVE for the data\
    \ frames destined towards the\n   multi-homed TS.\n   This method requires multiple\
    \ copies of the same packet to all NVEs\n   that participate in the VN.  If, for\
    \ example, a tenant subnet is\n   spread across 50 NVEs, the packet would have\
    \ to be replicated 50\n   times at the source NVE.  Obviously, this approach creates\
    \ more\n   traffic to the network that can cause congestion when the network\n\
    \   load is high.  This also creates an issue with the forwarding\n   performance\
    \ of the NVE.\n   Note that this method is similar to what was used in Virtual\
    \ Private\n   LAN Service (VPLS) [RFC4762] prior to support of Multiprotocol Label\n\
    \   Switching (MPLS) multicast [RFC7117].  While there are some\n   similarities\
    \ between MPLS Virtual Private Network (VPN) and NVO3,\n   there are some key\
    \ differences:\n   o  The attachment from Customer Edge (CE) to Provider Edge\
    \ (PE) in\n      VPNs is somewhat static, whereas in a DC that allows VMs to\n\
    \      migrate anywhere, the TS attachment to NVE is much more dynamic.\n   o\
    \  The number of PEs to which a single VPN customer is attached in an\n      MPLS\
    \ VPN environment is normally far less than the number of NVEs\n      to which\
    \ a VN's VMs are attached in a DC.\n   When a VPN customer has multiple multicast\
    \ groups, \"Multicast VPN\"\n   [RFC6513] combines all those multicast groups\
    \ within each VPN client\n   to one single multicast group in the MPLS (or VPN)\
    \ core.  The result\n   is that messages from any of the multicast groups belonging\
    \ to one\n   VPN customer will reach all the PE nodes of the client.  In other\n\
    \   words, any messages belonging to any multicast groups under customer\n   X\
    \ will reach all PEs of the customer X.  When the customer X is\n   attached to\
    \ only a handful of PEs, the use of this approach does not\n   result in an excessive\
    \ waste of bandwidth in the provider's network.\n   In a DC environment, a typical\
    \ hypervisor-based virtual switch may\n   only support on the order of 10's of\
    \ VMs (as of this writing).  A\n   subnet with N VMs may be, in the worst case,\
    \ spread across N virtual\n   switches (vSwitches).  Using an \"MPLS VPN multicast\"\
    \ approach in such\n   a scenario would require the creation of a multicast group\
    \ in the\n   core in order for the VN to reach all N NVEs.  If only a small\n\
    \   percentage of this client's VMs participate in application-specific\n   multicast,\
    \ a great number of NVEs will receive multicast traffic that\n   is not forwarded\
    \ to any of their attached VMs, resulting in a\n   considerable waste of bandwidth.\n\
    \   Therefore, the multicast VPN solution may not scale in a DC\n   environment\
    \ with the dynamic attachment of VNs to NVEs and a greater\n   number of NVEs\
    \ for each VN.\n"
- title: 3.3.  Replication at a Multicast Service Node
  contents:
  - "3.3.  Replication at a Multicast Service Node\n   With this method, all multicast\
    \ packets would be sent using a unicast\n   tunnel encapsulation from the ingress\
    \ NVE to a Multicast Service Node\n   (MSN).  The MSN, in turn, would create multiple\
    \ copies of the packet\n   and would deliver a copy, using a unicast tunnel encapsulation,\
    \ to\n   each of the NVEs that are part of the multicast group for which the\n\
    \   packet is intended.\n   This mechanism is similar to that used by the Asynchronous\
    \ Transfer\n   Mode (ATM) Forum's LAN Emulation (LANE) specification [LANE]. \
    \ The\n   MSN is similar to the Rendezvous Point (RP) in Protocol Independent\n\
    \   Multicast - Sparse Mode (PIM-SM), but different in that the user data\n  \
    \ traffic is carried by the NVO3 tunnels.\n   The following are possible ways\
    \ for the MSN to get the membership\n   information for each multicast group:\n\
    \   o  The MSN can obtain this membership information from the IGMP/MLD\n    \
    \  report messages sent by TSs in response to IGMP/MLD query messages\n      from\
    \ the MSN.  The IGMP/MLD query messages are sent from the MSN\n      to the NVEs,\
    \ which then forward the query messages to TSs attached\n      to them.  An IGMP/MLD\
    \ query message sent out by the MSN to an NVE\n      is encapsulated with the\
    \ MSN address in the outer IP source\n      address field and the address of the\
    \ NVE in the outer IP\n      destination address field.  An encapsulated IGMP/MLD\
    \ query message\n      also has a virtual network (VN) identifier (corresponding\
    \ to the\n      VN that the TSs belong to) in the outer header and a multicast\n\
    \      address in the inner IP destination address field.  Upon receiving\n  \
    \    the encapsulated IGMP/MLD query message, the NVE establishes a\n      mapping\
    \ for \"MSN address\" to \"multicast address\", decapsulates the\n      received\
    \ encapsulated IGMP/MLD message, and multicasts the\n      decapsulated query\
    \ message to the TSs that belong to the VN\n      attached to that NVE.  An IGMP/MLD\
    \ report message sent by a TS\n      includes the multicast address and the address\
    \ of the TS.  With\n      the proper \"MSN address\" to \"multicast address\"\
    \ mapping, the NVEs\n      can encapsulate all multicast data frames containing\
    \ the\n      \"multicast address\" with the address of the MSN in the outer IP\n\
    \      destination address field.\n   o  The MSN can obtain the membership information\
    \ from the NVEs that\n      have the capability to establish multicast groups\
    \ by snooping\n      native IGMP/MLD messages (note that the communication must\
    \ be\n      specific to the multicast addresses) or by having the NVA obtain\n\
    \      the information from the NVEs and in turn have MSN communicate\n      with\
    \ the NVA.  This approach requires additional protocol between\n      MSN and\
    \ NVEs.\n   Unlike the method described in Section 3.2, there is no performance\n\
    \   impact at the ingress NVE, nor are there any issues with multiple\n   copies\
    \ of the same packet from the source NVE to the MSN.  However,\n   there remain\
    \ issues with multiple copies of the same packet on links\n   that are common\
    \ to the paths from the MSN to each of the egress NVEs.\n   Additional issues\
    \ that are introduced with this method include the\n   availability of the MSN,\
    \ methods to scale the services offered by the\n   MSN, and the suboptimality\
    \ of the delivery paths.\n   Finally, the IP address of the source NVE must be\
    \ preserved in packet\n   copies created at the multicast service node if data-plane\
    \ learning\n   is in use.  This could create problems if IP source address Reverse\n\
    \   Path Forwarding (RPF) checks are in use.\n"
- title: 3.4.  IP Multicast in the Underlay
  contents:
  - "3.4.  IP Multicast in the Underlay\n   In this method, the underlay supports\
    \ IP multicast and the ingress\n   NVE encapsulates the packet with the appropriate\
    \ IP multicast address\n   in the tunnel encapsulation header for delivery to\
    \ the desired set of\n   NVEs.  The protocol in the underlay could be any variant\
    \ of PIM, or a\n   protocol-dependent multicast, such as [ISIS-Multicast].\n \
    \  If an NVE connects to its attached TSs via a Layer 2 network, there\n   are\
    \ multiple ways for NVEs to support the application-specific\n   multicast:\n\
    \   o  The NVE only supports the basic IGMP/MLD snooping function, while\n   \
    \   the \"TS routers\" handle the application-specific multicast.  This\n    \
    \  scheme doesn't utilize the underlay IP multicast protocols.\n      Instead\
    \ routers, which are themselves TSs attached to the NVE,\n      would handle multicast\
    \ protocols for the application-specific\n      multicast.  We refer to such routers\
    \ as TS routers.\n   o  The NVE can act as a pseudo multicast router for the directly\n\
    \      attached TSs and support the mapping of IGMP/MLD messages to the\n    \
    \  messages needed by the underlay IP multicast protocols.\n   With this method,\
    \ there are none of the issues with the methods\n   described in Sections 3.2\
    \ and 3.3 with respect to scaling and\n   congestion.  Instead, there are other\
    \ issues described below.\n   With PIM-SM, the number of flows required would\
    \ be (n*g), where n is\n   the number of source NVEs that source packets for the\
    \ group, and g is\n   the number of groups.  Bidirectional PIM (BIDIR-PIM) would\
    \ offer\n   better scalability with the number of flows required being g.\n  \
    \ Unfortunately, many vendors still do not fully support BIDIR or have\n   limitations\
    \ on its implementation.  [RFC6831] describes the use of\n   SSM as an alternative\
    \ to BIDIR, provided that the NVEs have a way to\n   learn of each other's IP\
    \ addresses so that they can join all of the\n   SSM Shortest Path Trees (SPTs)\
    \ to create/maintain an underlay SSM IP\n   multicast tunnel solution.\n   In\
    \ the absence of any additional mechanism (e.g., using an NVA for\n   address\
    \ resolution), for optimal delivery, there would have to be a\n   separate group\
    \ for each VN for infrastructure multicast plus a\n   separate group for each\
    \ application-specific multicast address within\n   a tenant.\n   An additional\
    \ consideration is that only the lower 23 bits of the IP\n   address (regardless\
    \ of whether IPv4 or IPv6 is in use) are mapped to\n   the outer MAC address,\
    \ and if there is equipment that prunes\n   multicasts at Layer 2, there will\
    \ be some aliasing.\n   Finally, a mechanism to efficiently provision such addresses\
    \ for each\n   group would be required.\n   There are additional optimizations\
    \ that are possible, but they come\n   with their own restrictions.  For example,\
    \ a set of tenants may be\n   restricted to some subset of NVEs, and they could\
    \ all share the same\n   outer IP multicast group address.  This, however, introduces\
    \ a\n   problem of suboptimal delivery (even if a particular tenant within\n \
    \  the group of tenants doesn't have a presence on one of the NVEs that\n   another\
    \ one does, the multicast packets would still be delivered to\n   that NVE). \
    \ It also introduces an additional network management\n   burden to optimize which\
    \ tenants should be part of the same tenant\n   group (based on the NVEs they\
    \ share), which somewhat dilutes the\n   value proposition of NVO3 (to completely\
    \ decouple the overlay and\n   physical network design allowing complete freedom\
    \ of placement of VMs\n   anywhere within the DC).\n   Multicast schemes such\
    \ as Bit Indexed Explicit Replication (BIER)\n   [RFC8279] may be able to provide\
    \ optimizations by allowing the\n   underlay network to provide optimum multicast\
    \ delivery without\n   requiring routers in the core of the network to maintain\
    \ per-\n   multicast group state.\n"
- title: 3.5.  Other Schemes
  contents:
  - "3.5.  Other Schemes\n   There are still other mechanisms that may be used that\
    \ attempt to\n   combine some of the advantages of the above methods by offering\n\
    \   multiple replication points, each with a limited degree of\n   replication\
    \ [EDGE-REP].  Such schemes offer a trade-off between the\n   amount of replication\
    \ at an intermediate node (e.g., router) versus\n   performing all of the replication\
    \ at the source NVE or all of the\n   replication at a multicast service node.\n"
- title: 4.  Simultaneous Use of More Than One Mechanism
  contents:
  - "4.  Simultaneous Use of More Than One Mechanism\n   While the mechanisms discussed\
    \ in the previous section have been\n   discussed individually, it is possible\
    \ for implementations to rely on\n   more than one of these.  For example, the\
    \ method of Section 3.1 could\n   be used for minimizing ARP/ND, while at the\
    \ same time, multicast\n   applications may be supported by one, or a combination,\
    \ of the other\n   methods.  For small multicast groups, the methods of source\
    \ NVE\n   replication or the use of a multicast service node may be attractive,\n\
    \   while for larger multicast groups, the use of multicast in the\n   underlay\
    \ may be preferable.\n"
- title: 5.  Other Issues
  contents:
  - '5.  Other Issues

    '
- title: 5.1.  Multicast-Agnostic NVEs
  contents:
  - "5.1.  Multicast-Agnostic NVEs\n   Some hypervisor-based NVEs do not process or\
    \ recognize IGMP/MLD\n   frames, i.e., those NVEs simply encapsulate the IGMP/MLD\
    \ messages in\n   the same way as they do for regular data frames.\n   By default,\
    \ a TS router periodically sends IGMP/MLD query messages to\n   all the hosts\
    \ in the subnet to trigger the hosts that are interested\n   in the multicast\
    \ stream to send back IGMP/MLD reports.  In order for\n   the MSN to get the updated\
    \ multicast group information, the MSN can\n   also send the IGMP/MLD query message\
    \ comprising a client-specific\n   multicast address encapsulated in an overlay\
    \ header to all of the\n   NVEs to which the TSs in the VN are attached.\n   However,\
    \ the MSN may not always be aware of the client-specific\n   multicast addresses.\
    \  In order to perform multicast filtering, the\n   MSN has to snoop the IGMP/MLD\
    \ messages between TSs and their\n   corresponding routers to maintain the multicast\
    \ membership.  In order\n   for the MSN to snoop the IGMP/MLD messages between\
    \ TSs and their\n   router, the NVA needs to configure the NVE to send copies\
    \ of the\n   IGMP/MLD messages to the MSN in addition to the default behavior\
    \ of\n   sending them to the TSs' routers; e.g., the NVA has to inform the\n \
    \  NVEs to encapsulate data frames with the Destination Address (DA)\n   being\
    \ 224.0.0.2 (DA of IGMP report) to the TSs' router and MSN.\n   This process is\
    \ similar to \"Source Replication\" described in\n   Section 3.2, except the NVEs\
    \ only replicate the message to the TSs'\n   router and MSN.\n"
- title: 5.2.  Multicast Membership Management for DC with VMs
  contents:
  - "5.2.  Multicast Membership Management for DC with VMs\n   For DCs with virtualized\
    \ servers, VMs can be added, deleted, or moved\n   very easily.  When VMs are\
    \ added, deleted, or moved, the NVEs to\n   which the VMs are attached are changed.\n\
    \   When a VM is deleted from an NVE or a new VM is added to an NVE, the\n   VM\
    \ management system should notify the MSN to send the IGMP/MLD query\n   messages\
    \ to the relevant NVEs (as described in Section 3.3) so that\n   the multicast\
    \ membership can be updated promptly.\n   Otherwise, if there are changes of VMs\
    \ attachment to NVEs (within the\n   duration of the configured default time interval\
    \ that the TSs routers\n   use for IGMP/MLD queries), multicast data may not reach\
    \ the VM(s)\n   that moved.\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   This document does not introduce any new security\
    \ considerations\n   beyond what is described in the NVO3 Architecture document\
    \ [RFC8014].\n"
- title: 7.  IANA Considerations
  contents:
  - "7.  IANA Considerations\n   This document does not require any IANA actions.\n"
- title: 8.  Summary
  contents:
  - "8.  Summary\n   This document has identified various mechanisms for supporting\n\
    \   application-specific multicast in networks that use NVO3.  It\n   highlights\
    \ the basics of each mechanism and some of the issues with\n   them.  As solutions\
    \ are developed, the protocols would need to\n   consider the use of these mechanisms,\
    \ and coexistence may be a\n   consideration.  It also highlights some of the\
    \ requirements for\n   supporting multicast applications in an NVO3 network.\n"
- title: 9.  References
  contents:
  - '9.  References

    '
- title: 9.1.  Normative References
  contents:
  - "9.1.  Normative References\n   [RFC3376]  Cain, B., Deering, S., Kouvelas, I.,\
    \ Fenner, B., and\n              A. Thyagarajan, \"Internet Group Management Protocol,\n\
    \              Version 3\", RFC 3376, DOI 10.17487/RFC3376, October 2002,\n  \
    \            <https://www.rfc-editor.org/info/rfc3376>.\n   [RFC6513]  Rosen,\
    \ E., Ed. and R. Aggarwal, Ed., \"Multicast in\n              MPLS/BGP IP VPNs\"\
    , RFC 6513, DOI 10.17487/RFC6513,\n              February 2012, <https://www.rfc-editor.org/info/rfc6513>.\n\
    \   [RFC7364]  Narten, T., Ed., Gray, E., Ed., Black, D., Fang, L.,\n        \
    \      Kreeger, L., and M. Napierala, \"Problem Statement:\n              Overlays\
    \ for Network Virtualization\", RFC 7364,\n              DOI 10.17487/RFC7364,\
    \ October 2014,\n              <https://www.rfc-editor.org/info/rfc7364>.\n  \
    \ [RFC7365]  Lasserre, M., Balus, F., Morin, T., Bitar, N., and\n            \
    \  Y. Rekhter, \"Framework for Data Center (DC) Network\n              Virtualization\"\
    , RFC 7365, DOI 10.17487/RFC7365, October\n              2014, <https://www.rfc-editor.org/info/rfc7365>.\n\
    \   [RFC8014]  Black, D., Hudson, J., Kreeger, L., Lasserre, M., and\n       \
    \       T. Narten, \"An Architecture for Data-Center Network\n              Virtualization\
    \ over Layer 3 (NVO3)\", RFC 8014,\n              DOI 10.17487/RFC8014, December\
    \ 2016,\n              <https://www.rfc-editor.org/info/rfc8014>.\n"
- title: 9.2.  Informative References
  contents:
  - "9.2.  Informative References\n   [RFC2131]  Droms, R., \"Dynamic Host Configuration\
    \ Protocol\",\n              RFC 2131, DOI 10.17487/RFC2131, March 1997,\n   \
    \           <https://www.rfc-editor.org/info/rfc2131>.\n   [RFC2710]  Deering,\
    \ S., Fenner, W., and B. Haberman, \"Multicast\n              Listener Discovery\
    \ (MLD) for IPv6\", RFC 2710,\n              DOI 10.17487/RFC2710, October 1999,\n\
    \              <https://www.rfc-editor.org/info/rfc2710>.\n   [RFC3569]  Bhattacharyya,\
    \ S., Ed., \"An Overview of Source-Specific\n              Multicast (SSM)\",\
    \ RFC 3569, DOI 10.17487/RFC3569, July\n              2003, <https://www.rfc-editor.org/info/rfc3569>.\n\
    \   [RFC3819]  Karn, P., Ed., Bormann, C., Fairhurst, G., Grossman, D.,\n    \
    \          Ludwig, R., Mahdavi, J., Montenegro, G., Touch, J., and\n         \
    \     L. Wood, \"Advice for Internet Subnetwork Designers\",\n              BCP\
    \ 89, RFC 3819, DOI 10.17487/RFC3819, July 2004,\n              <https://www.rfc-editor.org/info/rfc3819>.\n\
    \   [RFC4762]  Lasserre, M., Ed. and V. Kompella, Ed., \"Virtual Private\n   \
    \           LAN Service (VPLS) Using Label Distribution Protocol (LDP)\n     \
    \         Signaling\", RFC 4762, DOI 10.17487/RFC4762, January 2007,\n       \
    \       <https://www.rfc-editor.org/info/rfc4762>.\n   [RFC6040]  Briscoe, B.,\
    \ \"Tunnelling of Explicit Congestion\n              Notification\", RFC 6040,\
    \ DOI 10.17487/RFC6040, November\n              2010, <https://www.rfc-editor.org/info/rfc6040>.\n\
    \   [RFC6831]  Farinacci, D., Meyer, D., Zwiebel, J., and S. Venaas, \"The\n \
    \             Locator/ID Separation Protocol (LISP) for Multicast\n          \
    \    Environments\", RFC 6831, DOI 10.17487/RFC6831, January\n              2013,\
    \ <https://www.rfc-editor.org/info/rfc6831>.\n   [RFC7117]  Aggarwal, R., Ed.,\
    \ Kamite, Y., Fang, L., Rekhter, Y., and\n              C. Kodeboniya, \"Multicast\
    \ in Virtual Private LAN Service\n              (VPLS)\", RFC 7117, DOI 10.17487/RFC7117,\
    \ February 2014,\n              <https://www.rfc-editor.org/info/rfc7117>.\n \
    \  [RFC7348]  Mahalingam, M., Dutt, D., Duda, K., Agarwal, P., Kreeger,\n    \
    \          L., Sridhar, T., Bursell, M., and C. Wright, \"Virtual\n          \
    \    eXtensible Local Area Network (VXLAN): A Framework for\n              Overlaying\
    \ Virtualized Layer 2 Networks over Layer 3\n              Networks\", RFC 7348,\
    \ DOI 10.17487/RFC7348, August 2014,\n              <https://www.rfc-editor.org/info/rfc7348>.\n\
    \   [RFC7637]  Garg, P., Ed. and Y. Wang, Ed., \"NVGRE: Network\n            \
    \  Virtualization Using Generic Routing Encapsulation\",\n              RFC 7637,\
    \ DOI 10.17487/RFC7637, September 2015,\n              <https://www.rfc-editor.org/info/rfc7637>.\n\
    \   [RFC8279]  Wijnands, IJ., Ed., Rosen, E., Ed., Dolganow, A.,\n           \
    \   Przygienda, T., and S. Aldrin, \"Multicast Using Bit Index\n             \
    \ Explicit Replication (BIER)\", RFC 8279,\n              DOI 10.17487/RFC8279,\
    \ November 2017,\n              <https://www.rfc-editor.org/info/rfc8279>.\n \
    \  [DC-MC]    McBride, M. and H. Liu, \"Multicast in the Data Center\n       \
    \       Overview\", Work in Progress, draft-mcbride-armd-mcast-\n            \
    \  overview-02, July 2012.\n   [EDGE-REP] Marques, P., Fang, L., Winkworth, D.,\
    \ Cai, Y., and\n              P. Lapukhov, \"Edge multicast replication for BGP\
    \ IP\n              VPNs.\", Work in Progress, draft-marques-l3vpn-\n        \
    \      mcast-edge-01, June 2012.\n   [Geneve]   Gross, J., Ganga, I., and T. Sridhar,\
    \ \"Geneve: Generic\n              Network Virtualization Encapsulation\", Work\
    \ in Progress,\n              draft-ietf-nvo3-geneve-05, September 2017.\n   [GUE]\
    \      Herbert, T., Yong, L., and O. Zia, \"Generic UDP\n              Encapsulation\"\
    , Work in Progress,\n              draft-ietf-intarea-gue-05, December 2017.\n\
    \   [ISIS-Multicast]\n              Yong, L., Weiguo, H., Eastlake, D., Qu, A.,\
    \ Hudson, J.,\n              and U. Chunduri, \"IS-IS Protocol Extension For Building\n\
    \              Distribution Trees\", Work in Progress,\n              draft-yong-isis-ext-4-distribution-tree-03,\
    \ October 2014.\n   [LANE]     ATM Forum, \"LAN Emulation Over ATM: Version 1.0\"\
    , ATM\n              Forum Technical Committee, af-lane-0021.000, January 1995.\n\
    \   [LISP-Signal-Free]\n              Moreno, V. and D. Farinacci, \"Signal-Free\
    \ LISP Multicast\",\n              Work in Progress, draft-ietf-lisp-signal-free-\n\
    \              multicast-07, November 2017.\n   [VXLAN-GPE]\n              Maino,\
    \ F., Kreeger, L., and U. Elzur, \"Generic Protocol\n              Extension for\
    \ VXLAN\", Work in Progress,\n              draft-ietf-nvo3-vxlan-gpe-05, October\
    \ 2017.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   Many thanks are due to Dino Farinacci, Erik Nordmark, Lucy\
    \ Yong,\n   Nicolas Bouliane, Saumya Dikshit, Joe Touch, Olufemi Komolafe, and\n\
    \   Matthew Bocci for their valuable comments and suggestions.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Anoop Ghanwani\n   Dell\n   Email: anoop@alumni.duke.edu\n\
    \   Linda Dunbar\n   Huawei Technologies\n   5340 Legacy Drive, Suite 1750\n \
    \  Plano, TX  75024\n   United States of America\n   Phone: (469) 277 5840\n \
    \  Email: ldunbar@huawei.com\n   Mike McBride\n   Huawei Technologies\n   Email:\
    \ mmcbride7@gmail.com\n   Vinay Bannai\n   Google\n   Email: vbannai@gmail.com\n\
    \   Ram Krishnan\n   Dell\n   Email: ramkri123@gmail.com\n"
