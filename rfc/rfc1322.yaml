- title: __initial_text__
  contents:
  - '               A Unified Approach to Inter-Domain Routing

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard.  Distribution of this memo is\n\
    \   unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   This memo is an informational RFC which outlines one potential\n\
    \   approach for inter-domain routing in future global internets.  The\n   focus\
    \ is on scalability to very large networks and functionality, as\n   well as scalability,\
    \ to support routing in an environment of\n   heterogeneous services, requirements,\
    \ and route selection criteria.\n   Note: The work of D. Estrin and S. Hotz was\
    \ supported by the National\n   Science Foundation under contract number NCR-9011279,\
    \ with matching\n   funds from GTE Laboratories.  The work of Y. Rekhter was supported\
    \ by\n   the Defense Advanced Research Projects Agency, under contract\n   DABT63-91-C-0019.\
    \  Views and conclusions expressed in this paper are\n   not necessarily those\
    \ of the Defense Advanced Research Projects\n   Agency and National Science Foundation.\n"
- title: 1.0 Motivation
  contents:
  - "1.0 Motivation\n   The global internet can be modeled as a collection of hosts\n\
    \   interconnected via transmission and switching facilities.  Control\n   over\
    \ the collection of hosts and the transmission and switching\n   facilities that\
    \ compose the networking resources of the global\n   internet is not homogeneous,\
    \ but is distributed among multiple\n   administrative authorities.  Resources\
    \ under control of a single\n   administration form a domain.  In order to support\
    \ each domain's\n   autonomy and heterogeneity, routing consists of two distinct\n\
    \   components: intra-domain (interior) routing, and inter-domain\n   (exterior)\
    \ routing.  Intra-domain routing provides support for data\n   communication between\
    \ hosts where data traverses transmission and\n   switching facilities within\
    \ a single domain.  Inter-domain routing\n   provides support for data communication\
    \ between hosts where data\n   traverses transmission and switching facilities\
    \ spanning multiple\n   domains.  The entities that forward packets across domain\
    \ boundaries\n   are called border routers (BRs).  The entities responsible for\n\
    \   exchanging inter-domain routing information are called route servers\n   (RSs).\
    \  RSs and BRs may be colocated.\n   As the global internet grows, both in size\
    \ and in the diversity of\n   routing requirements, providing inter-domain routing\
    \ that can\n   accommodate both of these factors becomes more and more crucial.\
    \  The\n   number and diversity of routing requirements is increasing due to:\n\
    \   (a) transit restrictions imposed by source, destination, and transit\n   networks,\
    \ (b) different types of services offered and required, and\n   (c) the presence\
    \ of multiple carriers with different charging\n   schemes.  The combinatorial\
    \ explosion of mixing and matching these\n   different criteria weighs heavily\
    \ on the mechanisms provided by\n   conventional hop-by-hop routing architectures\
    \ ([ISIS10589, OSPF,\n   Hedrick88, EGP]).\n   Current work on inter-domain routing\
    \ within the Internet community\n   has diverged in two directions: one is best\
    \ represented by the Border\n   Gateway Protocol (BGP)/Inter-Domain Routeing Protocol\
    \ (IDRP)\n   architectures ([BGP91, Honig90, IDRP91]), and another is best\n \
    \  represented by the Inter-Domain Policy Routing (IDPR) architecture\n   ([IDPR90,\
    \ Clark90]).  In this paper we suggest that the two\n   architectures are quite\
    \ complementary and should not be considered\n   mutually exclusive.\n   We expect\
    \ that over the next 5 to 10 years, the types of services\n   available will continue\
    \ to evolve and that specialized facilities\n   will be employed to provide new\
    \ services.  While the number and\n   variety of routes provided by hop-by-hop\
    \ routing architectures with\n   type of service (TOS) support (i.e., multiple,\
    \ tagged routes) may be\n   sufficient for a large percentage of traffic, it is\
    \ important that\n   mechanisms be in place to support efficient routing of specialized\n\
    \   traffic types via special routes.  Examples of special routes are:\n   (1)\
    \ a route that travels through one or more transit domains that\n   discriminate\
    \ according to the source domain, (2) a route that travels\n   through transit\
    \ domains that support a service that is not widely or\n   regularly used.  We\
    \ refer to all other routes as generic.\n   Our desire to support special routes\
    \ efficiently led us to\n   investigate the dynamic installation of routes ([Breslau-Estrin91,\n\
    \   Clark90, IDPR90]).  In a previous paper ([Breslau-Estrin91]), we\n   evaluated\
    \ the algorithmic design choices for inter-domain policy\n   routing with specific\
    \ attention to accommodating source-specific and\n   other \"special\" routes.\
    \  The conclusion was that special routes are\n   best supported with source-routing\
    \ and extended link-state\n   algorithms; we refer to this approach as source-demand\
    \ routing\n   [Footnote:  The Inter-Domain Policy Routing (IDPR) architecture\
    \ uses\n   these techniques.].  However, a source-demand routing architecture,\n\
    \   used as the only means of inter-domain routing, has scaling problems\n   because\
    \ it does not lend itself to general hierarchical clustering\n   and aggregation\
    \ of routing and forwarding information.  For example,\n   even if a particular\
    \ route from an intermediate transit domain X, to\n   a destination domain Y is\
    \ shared by 1,000 source-domains, IDPR\n   requires that state for each of the\
    \ 1,000 routes be setup and\n   maintained in the transit border routers between\
    \ X and Y.  In\n   contrast, an alternative approach to inter-domain routing,\
    \ based on\n   hop-by-hop routing and a distributed route-computation algorithm\n\
    \   (described later), provides extensive support for aggregation and\n   abstraction\
    \ of reachability, topology, and forwarding information.\n   The Border Gateway\
    \ Protocol (BGP) and Inter-Domain Routeing Protocol\n   (IDRP) use these techniques\
    \ ([BGP91, IDRP91]).  While the BGP/IDRP\n   architecture is capable of accommodating\
    \ very large numbers of\n   datagram networks, it does not provide support for\
    \ specialized\n   routing requirements as flexibly and efficiently as IDPR-style\n\
    \   routing.\n"
- title: 1.1 Overview of the Unified Architecture
  contents:
  - "1.1 Overview of the Unified Architecture\n   We want to support special routes\
    \ and we want to exploit aggregation\n   when a special route is not needed. \
    \ Therefore, our scalable inter-\n   domain routing architecture consists of two\
    \ major components:\n   source-demand routing (SDR), and node routing (NR).  The\
    \ NR component\n   computes and installs routes that are shared by a significant\
    \ number\n   of sources.  These generic routes are commonly used and warrant wide\n\
    \   propagation, consequently, aggregation of routing information is\n   critical.\
    \  The SDR component computes and installs specialized routes\n   that are not\
    \ shared by enough sources to justify computation by NR\n   [Footnote: Routes\
    \ that are only needed sporadically (i.e., the demand\n   for them is not continuous\
    \ or otherwise predictable) are also\n   candidates for SDR.].  The potentially\
    \ large number of different\n   specialized routes, combined with their sparse\
    \ utilization, make them\n   too costly to support with the NR mechanism.\n  \
    \ A useful analogy to this approach is the manufacturing of consumer\n   products.\
    \  When predictable patterns of demand exist, firms produce\n   objects and sell\
    \ them as \"off the shelf\" consumer goods.  In our\n   architecture NR provides\
    \ off-the-shelf routes.  If demand is not\n   predictable, then firms accept special\
    \ orders and produce what is\n   demanded at the time it is needed.  In addition,\
    \ if a part is so\n   specialized that only a single or small number of consumers\
    \ need it,\n   the  consumer may repeatedly special order the part, even if it\
    \ is\n   needed in a predictable manner, because the consumer does not\n   represent\
    \ a big enough market for the producer to bother managing the\n   item as part\
    \ of its regular production.  SDR provides such special\n   order, on-demand routes.\n\
    \   By combining NR and SDR routing we propose to support inter-domain\n   routing\
    \ in internets of practically-unlimited size, while at the same\n   time providing\
    \ efficient support for specialized routing\n   requirements.\n   The development\
    \ of this architecture does assume that routing\n   requirements will be diverse\
    \ and that special routes will be needed.\n   On the other hand, the architecture\
    \ does not depend on assumptions\n   about the particular types of routes demanded\
    \ or on the distribution\n   of that demand.  Routing will adapt naturally over\
    \ time to changing\n   traffic patterns and new services by shifting computation\
    \ and\n   installation of particular types of routes between the two components\n\
    \   of the hybrid architecture [Footnote: Before continuing with our\n   explanation\
    \ of this architecture, we wish to state up front that\n   supporting highly specialized\
    \ routes for all source-destination pairs\n   in an internet, or even anything\
    \ close to that number, is not\n   feasible in any routing architecture that we\
    \ can foresee.  In other\n   words, we do not believe that any foreseeable routing\
    \ architecture\n   can support unconstrained proliferation of user requirements\
    \ and\n   network services.  At the same time, this is not necessarily a\n   problem.\
    \  The capabilities of the architecture may in fact exceed the\n   requirements\
    \ of the users.  Moreover, some of the requirements that\n   we regard as infeasible\
    \ from the inter-domain routing point of view,\n   may be supported by means completely\
    \ outside of routing.\n   Nevertheless, the caveat is stated here to preempt unrealistic\n\
    \   expectations.].\n   While the packet forwarding functions of the NR and SDR\
    \ components\n   have little or no coupling with each other, the connectivity\n\
    \   information exchange mechanism of the SDR component relies on\n   services\
    \ provided by the NR component.\n"
- title: 1.2 Outline
  contents:
  - "1.2 Outline\n   The remainder of this report is organized as follows.  Section\
    \ 2\n   outlines the requirements and priorities that guide the design of the\n\
    \   NR and SDR components.  Sections 3 and 4 describe the NR and SDR\n   design\
    \ choices, respectively, in light of these requirements.\n   Section 5 describes\
    \ protocol support for the unified architecture and\n   briefly discusses transition\
    \ issues.  We conclude with a brief\n   summary.\n"
- title: 2.0 Architectural Requirements and Priorities
  contents:
  - "2.0 Architectural Requirements and Priorities\n   In order to justify our design\
    \ choices for a scalable inter-domain\n   routing architecture, we must articulate\
    \ our evaluation criteria and\n   priorities.  This section defines complexity,\
    \ abstraction, policy,\n   and type of service requirements.\n"
- title: 2.1 Complexity
  contents:
  - "2.1 Complexity\n   Inter-domain routing complexity must be evaluated on the basis\
    \ of the\n   following performance metrics: (1) storage overhead, (2)\n   computational\
    \ overhead, and (3) message overhead.  This evaluation is\n   essential to determining\
    \ the scalability of any architecture.\n"
- title: 2.1.1 Storage Overhead
  contents:
  - "2.1.1 Storage Overhead\n   The storage overhead of an entity that participates\
    \ in inter-domain\n   routing comes from two sources: Routing Information Base\
    \ (RIB), and\n   Forwarding Information Base (FIB) overhead.  The RIB contains\
    \ the\n   routing information that entities exchange via the inter-domain\n  \
    \ routing protocol; the RIB is the input to the route computation.  The\n   FIB\
    \ contains the information that the entities use to forward the\n   inter-domain\
    \ traffic; the FIB is the output of the route computation.\n   For an acceptable\
    \ level of storage overhead, the amount of\n   information in both FIBs and RIBs\
    \ should grow significantly slower\n   than linearly (e.g., close to logarithmically)\
    \ with the total number\n   of domains in an internet.  To satisfy this requirement\
    \ with respect\n   to the RIB, the architecture must provide mechanisms for either\n\
    \   aggregation and abstraction of routing and forwarding information, or\n  \
    \ retrieval of a subset of this information on demand.  To satisfy this\n   requirement\
    \ with respect to the FIB, the architecture must provide\n   mechanisms for either\
    \ aggregation of the forwarding information (for\n   the NR computed routes),\
    \ or dynamic installation/tear down of this\n   information (for the SDR computed\
    \ routes).\n   Besides being an intrinsically important evaluation metric, storage\n\
    \   overhead has a direct impact on computational and bandwidth\n   complexity.\
    \  Unless the computational complexity is fixed (and\n   independent of the total\
    \ number of domains), the storage overhead has\n   direct impact on the computational\
    \ complexity of the architecture\n   since the routing information is used as\
    \ an input to route\n   computation. Moreover, unless the architecture employs\
    \ incremental\n   updates, where only changes to the routing information are\n\
    \   propagated, the storage overhead has direct impact on the bandwidth\n   overhead\
    \ of the architecture since the exchange of routing\n   information constitutes\
    \ most of the bandwidth overhead.\n"
- title: 2.1.2 Computational Overhead
  contents:
  - "2.1.2 Computational Overhead\n   The NR component will rely primarily on precomputation\
    \ of routes.  If\n   inter-domain routing is ubiquitous, then the precomputed\
    \ routes\n   include all reachable destinations.  Even if policy constraints make\n\
    \   fully ubiquitous routing impossible, the precomputed routes are\n   likely\
    \ to cover a very large percentage of all reachable\n   destinations.  Therefore\
    \ the complexity of this computation must be\n   as small as possible.  Specifically,\
    \ it is highly desirable that the\n   architecture would employ some form of partial\
    \ computation, where\n   changes in topology would require less than complete\
    \ recomputation.\n   Even if complete recomputation is necessary, its complexity\
    \ should be\n   less than linear with the total number of domains.\n   The SDR\
    \ component will use on-demand computation and caching.\n   Therefore the complexity\
    \ of this computation can be somewhat higher.\n   Another reason for relaxed complexity\
    \ requirements for SDR is that\n   SDR is expected to compute routes to a smaller\
    \ number of destinations\n   than is NR (although SDR route computation may be\
    \ invoked more\n   frequently).\n   Under no circumstances is computational complexity\
    \ allowed to become\n   exponential (for either the NR or SDR component).\n"
- title: 2.1.3 Bandwidth Overhead
  contents:
  - "2.1.3 Bandwidth Overhead\n   The bandwidth consumed by routing information distribution\
    \ should be\n   limited.  However, the possible use of data compression techniques\n\
    \   and the increasing speed of network links make this less important\n   than\
    \ route computation and storage overhead.  Bandwidth overhead may\n   be further\
    \ contained by using incremental (rather than complete)\n   exchange of routing\
    \ information.\n   While storage and bandwidth overhead may be interrelated, if\n\
    \   incremental updates are used then bandwidth overhead is negligible in\n  \
    \ the steady state (no changes in topology), and is independent of the\n   storage\
    \ overhead.  In other words, use of incremental updates\n   constrains the bandwidth\
    \ overhead to the dynamics of the internet.\n   Therefore, improvements in stability\
    \ of the physical links, combined\n   with techniques to dampen the effect of\
    \ topological instabilities,\n   will make the bandwidth overhead even less important.\n"
- title: 2.2 Aggregation
  contents:
  - "2.2 Aggregation\n   Aggregation and abstraction of routing and forwarding information\n\
    \   provides a very powerful mechanism for satisfying storage,\n   computational,\
    \ and bandwidth constraints.  The ability to aggregate,\n   and subsequently abstract,\
    \ routing and forwarding information is\n   essential to the scaling of the architecture\
    \ [Footnote: While we can\n   not prove that there are no other ways to achieve\
    \ scaling, we are not\n   aware of any mechanism other than clustering that allows\
    \ information\n   aggregation/abstraction.  Therefore, the rest of the paper assumes\n\
    \   that clustering is used for information aggregation/abstraction.].\n   This\
    \ is especially true with respect to the NR component, since the\n   NR component\
    \ must be capable of providing routes to all or almost all\n   reachable destinations.\n\
    \   At the same time, since preserving each domain's independence and\n   autonomy\
    \ is one of the crucial requirements of inter-domain routing,\n   the architecture\
    \ must strive for the maximum flexibility of its\n   aggregation scheme, i.e.,\
    \ impose as few preconditions, and as little\n   global coordination, as possible\
    \ on the participating domains.\n   The Routing Information Base (RIB) carries\
    \ three types of\n   information: (1) topology (i.e., the interconnections between\
    \ domains\n   or groups of domains), (2) network layer reachability, and (3)\n\
    \   transit constraint.  Aggregation of routing information should\n   provide\
    \ a reduction of all three components.  Aggregation of\n   forwarding information\
    \ will follow from reachability information\n   aggregation.\n   Clustering (by\
    \ forming routing domain confederations) serves the\n   following aggregation\
    \ functions: (1) to hide parts of the actual\n   physical topology, thus abstracting\
    \ topological information, (2) to\n   combine a set of reachable destination entities\
    \ into a single entity\n   and reduce storage overhead, and (3) to express transit\
    \ constraints\n   in terms of clusters, rather than individual domains.\n   As\
    \ argued in [Breslau-Estrin91], the architecture must allow\n   confederations\
    \ to be formed and changed without extensive\n   configuration and coordination;\
    \ in particular, forming a\n   confederation should not require global coordination\
    \ (such as that\n   required in ECMA ([ECMA89]).  In addition, aggregation should\
    \ not\n   require explicit designation of the relative placement of each domain\n\
    \   relative to another; i.e., domains or confederations of domains\n   should\
    \ not be required to agree on a partial ordering (i.e., who is\n   above whom,\
    \ etc.).\n   The architecture should allow different domains to use different\n\
    \   methods of aggregation and abstraction.  For example, a research\n   collaborator\
    \ at IBM might route to USC as a domain-level entity in\n   order to take advantage\
    \ of some special TOS connectivity to, or even\n   through, USC.  Whereas, someone\
    \ else at Digital Equipment Corporation\n   might see information at the level\
    \ of the California Educational\n   Institutions Confederation, and know only\
    \ that USC is a member.\n   Alternatively, USC might see part of the internal\
    \ structure within\n   the IBM Confederation (at the domain's level), whereas\
    \ UCLA may route\n   based on the confederation of IBM domains as a whole.\n \
    \  Support for confederations should be flexible.  Specifically, the\n   architecture\
    \ should allow confederations to overlap without being\n   nested, i.e., a single\
    \ domain, or a group of domains may be part of\n   more than one confederation.\
    \  For example, USC may be part of the\n   California Educational Institutions\
    \ Confederation and part of the US\n   R&D Institutions Confederation; one is\
    \ not a subset of the other.\n   Another example: T.J.  Watson Research Center\
    \ might be part of\n   NYSERNET Confederation and part of IBM-R&D-US Confederation.\
    \  While\n   the above examples describe cases where overlap consists of a single\n\
    \   domain, there may be other cases where multiple domains overlap.  As\n   an\
    \ example consider the set of domains that form the IBM\n   Confederation, and\
    \ another set of domains that form the DEC\n   Confederation.  Within IBM there\
    \ is a domain IBM-Research, and\n   similarly within DEC there is a domain DEC-Research.\
    \  Both of these\n   domains could be involved in some collaborative effort, and\
    \ thus have\n   established direct links.  The architecture should allow restricted\n\
    \   use of these direct links, so that other domains within the IBM\n   Confederation\
    \ would not be able to use it to talk to other domains\n   within the DEC Confederation.\
    \  A similar example exists when a\n   multinational corporation forms a confederation,\
    \ and the individual\n   branches within each country also belong to their respective\
    \ country\n   confederations.  The corporation may need to protect itself from\n\
    \   being used as an inter-country transit domain (due to internal, or\n   international,\
    \ policy).  All of the above examples illustrate a\n   situation where confederations\
    \ overlap, and it is necessary to\n   control the traffic traversing the overlapping\
    \ resources.\n   While flexible aggregation should be accommodated in any inter-domain\n\
    \   architecture, the extent to which this feature is exploited will have\n  \
    \ direct a effect on the scalability associated with aggregation.  At\n   the\
    \ same time, the exploitation of this feature depends on the way\n   addresses\
    \ are assigned.  Specifically, scaling associated with\n   forwarding information\
    \ depends heavily on the assumption that there\n   will be general correspondence\
    \ between the hierarchy of address\n   registration authorities, and the way routing\
    \ domains and routing\n   domain confederations are organized (see Section 2.6).\n"
- title: 2.3 Routing Policies
  contents:
  - "2.3 Routing Policies\n   Routing policies that the architecture must support\
    \ may be broadly\n   classified into transit policies and route selection policies\n\
    \   [Breslau-Estrin 91, Estrin89].\n   Restrictions imposed via transit policies\
    \ may be based on a variety\n   of factors.  The architecture should be able to\
    \ support restrictions\n   based on the source, destination, type of services\
    \ (TOS), user class\n   identification (UCI), charging, and path [Estrin89 , Little89].\
    \  The\n   architecture must allow expression of transit policies on all routes,\n\
    \   both NR and SDR.  Even if NR routes are widely used and have fewer\n   source\
    \ or destination restrictions, NR routes may have some transit\n   qualifiers\
    \ (e.g., TOS, charging, or user-class restriction).  If the\n   most widely-usable\
    \ route to a destination has policy qualifiers, it\n   should be advertiseable\
    \ by NR and the transit constraints should be\n   explicit.\n   Route selection\
    \ policies enable each domain to select a particular\n   route among multiple\
    \ routes to the same destination.  To maintain\n   maximum autonomy and independence\
    \ between domains, the architecture\n   must support heterogeneous route selection\
    \ policies, where each\n   domain can establish its own criteria for selecting\
    \ routes.  This\n   argument was made earlier with respect to source route selection\n\
    \   ([IDPR90, Clark90, Breslau-Estrin91]).  In addition, each\n   intermediate\
    \ transit domain must have the flexibility to apply its\n   own selection criteria\
    \ to the routes made available to it by its\n   neighbors.  This is really just\
    \ a corollary to the above; i.e., if we\n   allow route selection policy to be\
    \ expressed for NR routes, we can\n   not assume all domains will apply the same\
    \ policy.  The support for\n   dissimilar route selection policies is one of the\
    \ key factors that\n   distinguish inter-domain routing from intra-domain routing\
    \ ([ECMA89,\n   Estrin89]).  However, it is a non-goal of the architecture to\
    \ support\n   all possible route selection policies.  For more on unsupported\
    \ route\n   selection policies see Section 2.3.2 below.\n"
- title: 2.3.1 Routing Information Hiding
  contents:
  - "2.3.1 Routing Information Hiding\n   The architecture should not require all\
    \ domains within an internet to\n   reveal their connectivity and transit constraints\
    \ to each other.\n   Domains should be able to enforce their transit policies\
    \ while\n   limiting the advertisement of their policy and connectivity\n   information\
    \ as much as possible; such advertisement will be driven by\n   a \"need to know\"\
    \ criteria.  Moreover, advertising a transit policy to\n   domains that can not\
    \ use this policy will increase the amount of\n   routing information that must\
    \ be stored, processed, and propagated.\n   Not only may it be impractical for\
    \ a router to maintain full inter-\n   domain topology and policy information,\
    \ it may not be permitted to\n   obtain this information.\n"
- title: 2.3.2 Policies Not Supported
  contents:
  - "2.3.2 Policies Not Supported\n   In this and previous papers we have argued that\
    \ a global inter-domain\n   routing architecture should support a wide range of\
    \ policies.  In\n   this section we identify several types of policy that we explicitly\n\
    \   do not propose to support.  In general our reasoning is pragmatic; we\n  \
    \ think such policy types are either very expensive in terms of\n   overhead,\
    \ or may lead to routing instabilities.\n   1. Route selection policies contingent\
    \ on external behavior.\n      The route selection process may be modeled by a\
    \ function that\n      assigns a non-negative integer to a route, denoting the\
    \ degree\n      of preference.  Route selection applies this function to all\n\
    \      feasible routes to a given destination, and selects the route\n      with\
    \ the highest value.  To provide a stable environment, the\n      preference function\
    \ should not use as an input the status and\n      attributes of other routes\
    \ (either to the same or to a\n      different destination).\n   2. Transit policies\
    \ contingent on external behavior.\n      To provide a stable environment, the\
    \ domain's transit policies\n      can not be automatically affected by any information\
    \ external\n      to the domain.  Specifically, these policies can not be modified,\n\
    \      automatically, in response to information about other domains'\n      transit\
    \ policies, or routes selected by local or other domains.\n      Similarly, transit\
    \ policies can not be automatically modified\n      in response to information\
    \ about performance characteristics of\n      either local or external domains.\n\
    \   3. Policies contingent on external state (e.g., load).\n      It is a non-goal\
    \ of the architecture to support load-sensitive\n      routing for generic routes.\
    \  However, it is possible that some\n      types of service may employ load information\
    \ to select among\n      alternate SDR routes.\n   4. Very large number of simultaneous\
    \ SDR routes.\n      It is a non-goal of the architecture to support a very large\n\
    \      number of simultaneous SDR routes through any single router.\n      Specifically,\
    \ the FIB storage overhead associated with SDR\n      routes must be comparable\
    \ with that of NR routes, and should\n      always be bound by the complexity\
    \ requirements outlined earlier\n      [Foonote: As discussed earlier, theoretically\
    \ the state overhead\n      could grow O(N^2) with the number of domains in an\
    \ internet.\n      However, there is no evidence or intuition to suggest that\n\
    \      this will be a limiting factor on the wide utilization of SDR,\n      provided\
    \ that NR is available to handle generic routes.].\n"
- title: 2.4 Support for TOS Routing
  contents:
  - "2.4 Support for TOS Routing\n   Throughout this document we refer to support\
    \ for type of service\n   (TOS) routing.  There is a great deal of research and\
    \ development\n   activity currently underway to explore network architectures\
    \ and\n   protocols for high-bandwidth, multimedia traffic.  Some of this\n  \
    \ traffic is delay sensitive, while some requires high throughput.  It\n   is\
    \ unrealistic to assume that a single communication fabric will be\n   deployed\
    \ homogeneously across the internet (including all\n   metropolitan, regional,\
    \ and backbone networks) that will support all\n   types of traffic uniformly.\
    \  To support diverse traffic requirements\n   in a heterogeneous environment,\
    \ various resource management\n   mechanisms will be used in different parts of\
    \ the global internet\n   (e.g., resource reservation of various kinds) [ST2-90,\
    \ Zhang91].\n   In this context, it is the job of routing protocols to locate\
    \ routes\n   that can potentially support the particular TOS requested.  It is\n\
    \   explicitly not the job of the general routing protocols to locate\n   routes\
    \ that are guaranteed to have resources available at the\n   particular time of\
    \ the route request.  In other words, it is not\n   practical to assume that instantaneous\
    \ resource availability will be\n   known at all remote points in the global internet.\
    \  Rather, once a\n   TOS route has been identified, an application requiring\
    \ particular\n   service guarantees will attempt to use the route (e.g., using\
    \ an\n   explicit setup message if so required by the underlying networks).\n\
    \   In Section 4 we describe additional services that may be provided to\n   support\
    \ more adaptive route selection for special TOS [Footnote:\n   Adaptive route\
    \ selection implies adaptability only during the route\n   selection process.\
    \  Once a route is selected, it is not going to be a\n   subject to any adaptations,\
    \ except when it becomes infeasible.].\n"
- title: 2.5 Commonality between Routing Components
  contents:
  - "2.5 Commonality between Routing Components\n   While it is acceptable for the\
    \ NR and SDR components to be\n   dissimilar, we do recognize that such a solution\
    \ is less desirable --\n   all other things being equal.  In theory, there are\
    \ advantages in\n   having the NR and SDR components use similar algorithms and\n\
    \   mechanisms.  Code and databases could be shared and the architecture\n   would\
    \ be more manageable and comprehensible.  On the other hand,\n   there may be\
    \ some benefit (e.g., robustness) if the two parts of the\n   architecture are\
    \ heterogeneous, and not completely inter-dependent.\n   In Section 5 we list\
    \ several areas in which opportunities for\n   increased commonality (unification)\
    \ will be exploited.\n"
- title: 2.6 Interaction with Addressing
  contents:
  - "2.6 Interaction with Addressing\n   The proposed architecture should be applicable\
    \ to various addressing\n   schemes.  There are no specific assumptions about\
    \ a particular\n   address structure, except that this structure should facilitate\n\
    \   information aggregation, without forcing rigid hierarchical routing.\n   Beyond\
    \ this requirement, most of the proposals for extending the IP\n   address space,\
    \ for example, can be used in conjunction with our\n   architecture.  But our\
    \ architecture itself does not provide (or\n   impose) a particular solution to\
    \ the addressing problem.\n"
- title: 3.0 Design Choices for Node Routing (NR)
  contents:
  - "3.0 Design Choices for Node Routing (NR)\n   This section addresses the design\
    \ choices made for the NR component\n   in light of the above architectural requirements\
    \ and priorities.  All\n   of our discussion of NR assumes hop-by-hop routing.\
    \  Source routing\n   is subject to different constraints and is used for the\
    \ complementary\n   SDR component.\n"
- title: 3.1 Overview of NR
  contents:
  - "3.1 Overview of NR\n   The NR component is designed and optimized for an environment\
    \ where a\n   large percentage of packets will travel over routes that can be\n\
    \   shared by multiple sources and that have predictable traffic\n   patterns.\
    \  The efficiency of the NR component improves when a number\n   of source domains\
    \ share a particular route from some intermediate\n   point to a destination.\
    \  Moreover, NR is best suited to provide\n   routing for inter-domain data traffic\
    \ that is either steady enough to\n   justify the existence of a route, or predictable,\
    \ so that a route may\n   be installed when needed (based on the prediction, rather\
    \ than on the\n   actual traffic).  Such routes lend themselves to distributed\
    \ route\n   computation and installation procedures.\n   Routes that are installed\
    \ in routers, and information that was used\n   by the routers to compute these\
    \ routes, reflect the known operational\n   state of the routing facilities (as\
    \ well as the policy constraints)\n   at the time of route computation.  Route\
    \ computation is driven by\n   changes in either operational status of routing\
    \ facilities or policy\n   constraints.  The NR component supports route computation\
    \ that is\n   dynamically adaptable to both changes in topology and policy.  The\
    \ NR\n   component allows time-dependent selection or deletion of routes.\n  \
    \ However, time dependency must be predictable (e.g., advertising a\n   certain\
    \ route only after business hours) and routes should be used\n   widely enough\
    \ to warrant inclusion in NR.\n   The proposed architecture assumes that most\
    \ of the inter-domain\n   conversations will be forwarded via routes computed\
    \ and installed by\n   the NR component.\n   Moreover, the exchange of routing\
    \ information necessary for the SDR\n   component depends on facilities provided\
    \ by the NR component; i.e.,\n   NR policies must allow SDR reachability information\
    \ to travel.\n   Therefore, the architecture requires that all domains in an internet\n\
    \   implement and participate in NR.  Since scalability (with respect to\n   the\
    \ size of the internet) is one of the fundamental requirements for\n   the NR\
    \ component, it must provide multiple mechanisms with various\n   degrees of sophistication\
    \ for information aggregation and\n   abstraction.\n   The potential reduction\
    \ of routing and forwarding information depends\n   very heavily on the way addresses\
    \ are assigned and how domains and\n   their confederations are structured.  \"\
    If there is no correspondence\n   between the address registration hierarchy and\
    \ the organisation of\n   routeing domains, then ... each and every routeing domain\
    \ in the OSIE\n   would need a table entry potentially at every boundary IS of\
    \ every\n   other routeing domain\" ([Oran89]).  Indeed, scaling in the NR\n \
    \  component is almost entirely predicated on the assumption that there\n   will\
    \ be general correspondence between the hierarchy of address\n   assignment authorities\
    \ and the way routing domains are organised, so\n   that the efficient and frequent\
    \ aggregation of routing and forwarding\n   information will be possible.  Therefore,\
    \ given the nature of inter-\n   domain routing in general, and the NR component\
    \ in particular,\n   scalability of the architecture depends very heavily on the\n\
    \   flexibility of the scheme for information aggregation and\n   abstraction,\
    \ and on the preconditions that such a scheme imposes.\n   Moreover, given a flexible\
    \ architecture, the operational efficiency\n   (scalability) of the global internet,\
    \ or portions thereof, will\n   depend on tradeoffs made between flexibility and\
    \ aggregation.\n   While the NR component is optimized to satisfy the common case\n\
    \   routing requirements for an extremely large population of users, this\n  \
    \ does not imply that routes produced by the NR component would not be\n   used\
    \ for different types of service (TOS).  To the contrary, if a TOS\n   becomes\
    \ sufficiently widely used (i.e., by multiple domains and\n   predictably over\
    \ time), then it may warrant being computed by the NR\n   component.\n   To summarize,\
    \ the NR component is best suited to provide routes that\n   are used by more\
    \ than a single domain.  That is, the efficiency of\n   the NR component improves\
    \ when a number of source domains share a\n   particular route from some intermediate\
    \ point to a destination.\n   Moreover, NR is best suited to provide routing for\
    \ inter-domain data\n   traffic that is either steady enough to justify the existence\
    \ of a\n   route, or predictable, so that a route may be installed when needed,\n\
    \   (based on the prediction, rather than on the actual traffic).\n"
- title: 3.2 Routing Algorithm Choices for NR
  contents:
  - "3.2 Routing Algorithm Choices for NR\n   Given that a NR component based on hop-by-hop\
    \ routing is needed to\n   provide scalable, efficient inter-domain routing, the\
    \ remainder of\n   this section considers the fundamental design choices for the\
    \ NR\n   routing algorithm.\n   Typically the debate surrounding routing algorithms\
    \ focuses on link\n   state and distance vector protocols.  However, simple distance\
    \ vector\n   protocols (i.e., Routing Information Protocol [Hedrick88]), do not\n\
    \   scale because of convergence problems.  Improved distance vector\n   protocols,\
    \ such as those discussed in [Jaffee82, Zaumen91, Shin87],\n   have been developed\
    \ to address this issue using synchronization\n   mechanisms or additional path\
    \ information.  In the case of inter-\n   domain routing, having additional path\
    \ information is essential to\n   supporting policy.  Therefore, the algorithms\
    \ we consider for NR are\n   link state and one we call path vector (PV).  Whereas\
    \ the\n   characteristics of link state algorithms are generally understood\n\
    \   (for example, [Zaumen 91]), we must digress from our evaluation\n   discussion\
    \ to describe briefly the newer concept of the PV algorithm\n   [Footnote: We\
    \ assume that some form of SPF algorithm will be used to\n   compute paths over\
    \ the topology database when LS algorithms are used\n   [Dijkstra59, OSPF]].\n"
- title: 3.2.1 Path Vector Protocol Overview
  contents:
  - "3.2.1 Path Vector Protocol Overview\n   The Border Gateway Protocol (BGP) (see\
    \ [BGP91]) and the Inter Domain\n   Routing Protocol (IDRP) (see [IDRP91]) are\
    \ examples of path vector\n   (PV) protocols [Footnote: BGP is an inter-autonomous\
    \ system routing\n   protocol for TCP/IP internets.  IDRP is an OSI inter-domain\
    \ routing\n   protocol that is being progressed toward standardization within\
    \ ISO.\n   Since in terms of functionality BGP represents a proper subset of\n\
    \   IDRP, for the rest of the paper we will only consider IDRP.].\n   The routing\
    \ algorithm employed by PV bears a certain resemblance to\n   the traditional\
    \ Bellman-Ford routing algorithm in the sense that each\n   border router advertises\
    \ the destinations it can reach to its\n   neighboring BRs.  However,the PV routing\
    \ algorithm augments the\n   advertisement of reachable destinations with information\
    \ that\n   describes various properties of the paths to these destinations.\n\
    \   This information is expressed in terms of path attributes.  To\n   emphasize\
    \ the tight coupling between the reachable destinations and\n   properties of\
    \ the paths to these destinations, PV defines a route as\n   a pairing between\
    \ a destination and the attributes of the path to\n   that destination.  Thus\
    \ the name, path-vector protocol, where a BR\n   receives from its neighboring\
    \ BR a vector that contains paths to a\n   set of destinations [Footnote: The\
    \ term \"path-vector protocol\" bears\n   an intentional similarity to the term\
    \ \"distance-vector protocol\",\n   where a BR receives from each of its neighbors\
    \ a vector that contains\n   distances to a set of destinations.].  The path,\
    \ expressed in terms\n   of the domains (or confederations) traversed so far,\
    \ is carried in a\n   special path attribute which records the sequence of routing\
    \ domains\n   through which the reachability information has passed.  Suppression\n\
    \   of routing loops is implemented via this special path attribute, in\n   contrast\
    \ to LS and distance vector which use a globally-defined\n   monotonically-increasing\
    \ metric for route selection [Shin87].\n   Because PV does not require all routing\
    \ domains to have homogeneous\n   criteria (policies) for route selection, route\
    \ selection policies\n   used by one routing domain are not necessarily known\
    \ to other routing\n   domains.  To maintain the maximum degree of autonomy and\
    \ independence\n   between routing domains, each domain which participates in\
    \ PV may\n   have its own view of what constitutes an optimal route.  This view\
    \ is\n   based solely on local route selection policies and the information\n\
    \   carried in the path attributes of a route.  PV standardizes only the\n   results\
    \ of the route selection procedure, while allowing the\n   selection policies\
    \ that affect the route selection to be non-standard\n   [Footnote: This succinct\
    \ observation is attributed to Ross Callon\n   (Digital Equipment Corporation).].\n"
- title: 3.3 Complexity
  contents:
  - "3.3 Complexity\n   Given the above description of PV algorithms, we can compare\
    \ them to\n   LS algorithms in terms of the three complexity parameters defined\n\
    \   earlier.\n"
- title: 3.3.1 Storage Overhead
  contents:
  - "3.3.1 Storage Overhead\n   Without any aggregation of routing information, and\
    \ ignoring storage\n   overhead associated with transit constraints, it is possible\
    \ to show\n   that under some rather general assumptions the average case RIB\n\
    \   storage overhead of the PV scheme for a single TOS ranges between\n   O(N)\
    \ and O(Nlog(N)), where N is the total number of routing domains\n   ([Rekhter91]).\
    \  The LS RIB, with no aggregation of routing\n   information, no transit constraints,\
    \ a single homogeneous route\n   selection policy across all the domains, and\
    \ a single TOS, requires a\n   complete domain-level topology map whose size is\
    \ O(N).\n   Supporting heterogeneous route selection and transit policies with\n\
    \   hop-by-hop forwarding and LS requires each domain to know all other\n   domains\
    \ route selection and transit policies.  This may significantly\n   increase the\
    \ amount of routing information that must be stored by\n   each domain.  If the\
    \ number of policies advertised grows with the\n   number of domains, then the\
    \ storage could become unsupportable.  In\n   contrast, support for heterogeneous\
    \ route selection policies has no\n   effect on the storage complexity of the\
    \ PV scheme (aside from simply\n   storing the local policy information).  The\
    \ presence of transit\n   constraints in PV results in a restricted distribution\
    \ of routing\n   information, thus further reducing storage overhead.  In contrast,\n\
    \   with LS no such reduction is possible since each domain must know\n   every\
    \ other domain's transit policies.  Finally, some of the transit\n   constraints\
    \ (e.g., path sensitive constraints) can be expressed in a\n   more concise form\
    \ in PV (see aggregation discussion below).\n   The ability to further restrict\
    \ storage overhead is facilitated by\n   the PV routing algorithm, where route\
    \ computation precedes routing\n   information dissemination, and only routing\
    \ information associated\n   with the routes selected by a domain is distributed\
    \ to adjacent\n   domains.  In contrast, route selection with LS is decoupled\
    \ from the\n   distribution of routing information, and has no effect on such\n\
    \   distribution.\n   While theoretically routing information aggregation can\
    \ be used to\n   reduce storage complexity in both LS and PV, only aggregation\
    \ of\n   topological information would yield the same gain for both.\n   Aggregating\
    \ transit constraints with LS may result in either reduced\n   connectivity or\
    \ less information reduction, as compared with PV.\n   Aggregating heterogeneous\
    \ route selection policies in LS is highly\n   problematic, at best.  In PV, route\
    \ selection policies are not\n   distributed, thus making aggregation of route\
    \ selection policies a\n   non-issue [Footnote: Although a domain's selection\
    \ policies are not\n   explicitly distributed, they have an impact on the routes\
    \ available\n   to other domains.  A route that may be preferred by a particular\n\
    \   domain, and not prohibited by transit restrictions, may still be\n   unavailable\
    \ due to the selection policies of some intermediate\n   domain.  The ability\
    \ to compute and install alternative routes that\n   may be lost using hop-by-hop\
    \ routing (either LS of PV) is the\n   critical functionality provided by SDR.].\n\
    \   Support for multiple TOSs has the same impact on storage overhead for\n  \
    \ both LS and for PV.\n   Potentially the LS FIB may be smaller if routes are\
    \ computed at each\n   node on demand.  However, the gain of such a scheme depends\
    \ heavily\n   on the traffic patterns, memory size, and caching strategy.  If\
    \ there\n   is not a high degree of locality, severely degraded performance can\n\
    \   result due to excessive overall computation time and excessive\n   computation\
    \ delay when forwarding packets to a new destination.  If\n   demand driven route\
    \ computation is not used for LS, then the size of\n   the FIB would be the same\
    \ for both LS and PV.\n"
- title: 3.3.2 Route Computation Complexity
  contents:
  - "3.3.2 Route Computation Complexity\n   Even if all domains employ exactly the\
    \ same route selection policy,\n   computational complexity of PV is smaller than\
    \ that of LS.  The PV\n   computation consists of evaluating a newly arrived route\
    \ and\n   comparing it with the existing one [Footnote: Some additional checks\n\
    \   are required when an update is received to insure that a routing loop\n  \
    \ has not been created.].  Whereas, conventional LS computation\n   requires execution\
    \ of an SPF algorithm such as Dijkstra's.\n   With PV, topology changes only result\
    \ in the recomputation of routes\n   affected by these changes, which is more\
    \ efficient than complete\n   recomputation.  However, because of the inclusion\
    \ of full path\n   information with each distance vector, the effect of a topology\n\
    \   change may propagate farther than in traditional distance vector\n   algorithms.\
    \  On the other hand, the number of affected domains will\n   still be smaller\
    \ with PV than with conventional LS hop-by-hop\n   routing.  With PV, only those\
    \ domains whose routes are affected by\n   the changes have to recompute, while\
    \ with conventional LS hop-by-hop\n   routing all domains must recompute.  While\
    \ it is also possible to\n   employ partial recomputation with LS (i.e., when\
    \ topology changes,\n   only the affected routes are recomputed), \"studies suggest\
    \ that with\n   a very small number of link changes (perhaps 2) the expected\n\
    \   computational complexity of the incremental update exceeds the\n   complete\
    \ recalculation\" ([ANSI87-150R]).  However these checks are\n   much simpler\
    \ than executing a full SPF algorithm.\n   Support for heterogeneous route selection\
    \ policies has serious\n   implications for the computational complexity.  The\
    \ major problem\n   with supporting heterogeneous route selection policies with\
    \ LS is the\n   computational complexity of the route selection itself.\n   Specifically,\
    \ we are not aware of any algorithm with less than\n   exponential time complexity\
    \ that would be capable of computing routes\n   to all destinations, with LS hop-by-hop\
    \ routing and heterogeneous\n   route selection policies.  In contrast, PV allows\
    \ each domain to make\n   its route selection autonomously, based only on local\
    \ policies.\n   Therefore support for dissimilar route selection policies has\
    \ no\n   negative implications for the complexity of route computation in PV.\n\
    \   Similarly, providing support for path-sensitive transit policies in\n   LS\
    \ implies exponential computation, while in PV such support has no\n   impact\
    \ on the complexity of route computation.\n   In summary, because NR will rely\
    \ primarily on precomputation of\n   routes, aggregation is essential to the long-term\
    \ scalability of the\n   architecture.  To the extent aggregation is facilitated\
    \ with PV, so\n   is reduced computational complexity.  While similar arguments\
    \ may be\n   made for LS, the aggregation capabilities that can be achieved with\n\
    \   LS are more problematic because of LS' reliance on consistent\n   topology\
    \ maps at each node.  It is also not clear what additional\n   computational complexity\
    \ will be associated with aggregation of\n   transit constraints and heterogeneous\
    \ route selection policies in LS.\n"
- title: 3.3.3 Bandwidth Overhead
  contents:
  - "3.3.3 Bandwidth Overhead\n   PV routing updates include fully-expanded information.\
    \  A complete\n   route for each supported TOS is advertised.  In LS, TOS only\n\
    \   contributes a factor increase per link advertised, which is much less\n  \
    \ than the number of complete routes.  Although TOSs may be encoded\n   more efficiently\
    \ with LS than with PV, link state information is\n   flooded to all domains,\
    \ while with PV, routing updates are\n   distributed only to the domains that\
    \ actually use them.  Therefore,\n   it is difficult to make a general statement\
    \ about which scheme\n   imposes more bandwidth overhead, all other factors being\
    \ equal.\n   Moreover, this is perhaps really not an important difference, since\n\
    \   we are more concerned with the number of messages than with the\n   number\
    \ of bits (because of compression and greater link bandwidth, as\n   well as the\
    \ increased physical stability of links).\n"
- title: 3.4 Aggregation
  contents:
  - "3.4 Aggregation\n   Forming confederations of domains, for the purpose of consistent,\n\
    \   hop-by-hop, LS route computation, requires that domains within a\n   confederation\
    \ have consistent policies.  In addition, LS\n   confederation requires that any\
    \ lower level entity be a member of\n   only one higher level entity.  In general,\
    \ no intra-confederation\n   information can be made visible outside of a confederation,\
    \ or else\n   routing loops may occur as a result of using an inconsistent map\
    \ of\n   the network at different domains.  Therefore, the use of\n   confederations\
    \ with hop-by-hop LS is limited because each domain (or\n   confederation) can\
    \ only be a part of one higher level confederation\n   and only export policies\
    \ consistent with that confederation (see\n   examples in Section 2.2).  These\
    \ restrictions are likely to impact\n   the scaling capabilities of the architecture\
    \ quite severely.\n   In comparison, PV can accommodate different confederation\
    \ definitions\n   because looping is avoided by the use of full path information.\n\
    \   Consistent network maps are not needed at each route server, since\n   route\
    \ computation precedes routing information dissemination.\n   Consequently, PV\
    \ confederations can be nested, disjoint, or\n   overlapping.  A domain (or confederation)\
    \ can export different\n   policies or TOS as part of different confederations,\
    \ thus providing\n   the extreme flexibility that is crucial for the overall scaling\
    \ and\n   extensibility of the architecture.\n   In summary, aggregation is essential\
    \ to achieve acceptable complexity\n   bounds, and flexibility is essential to\
    \ achieve acceptable\n   aggregation across the global, decentralized internet.\
    \  PV's\n   strongest advantage stems from its flexibility.\n"
- title: 3.5 Policy
  contents:
  - "3.5 Policy\n   The need to allow expression of transit policy constraints on\
    \ any\n   route (i.e., NR routes as well as SDR routes), by itself, can be\n \
    \  supported by either LS or PV.  However, the associated complexities\n   of\
    \ supporting transit policy constraints are noticeably higher for LS\n   than\
    \ for PV.  This is due to the need to flood all transit policies\n   with LS,\
    \ where with PV transit policies are controlled via restricted\n   distribution\
    \ of routing information.  The latter always imposes less\n   overhead than flooding.\n\
    \   While all of the transit constraints that can be supported with LS\n   can\
    \ be supported with PV, the reverse is not true.  In other words,\n   there are\
    \ certain transit constraints (e.g., path-sensitive transit\n   constraints) that\
    \ are easily supported with PV, and are prohibitively\n   expensive (in terms\
    \ of complexity) to support in LS.  For example, it\n   is not clear how NR with\
    \ LS could support something like ECMA-style\n   policies that are based on hierarchical\
    \ relations between domains,\n   while support for such policies is straightforward\
    \ with PV.\n   As indicated above, support for heterogeneous route selection\n\
    \   policies, in view of its computational and storage complexity, is\n   impractical\
    \ with LS hop-by-hop routing.  In contrast, PV can\n   accommodate heterogeneous\
    \ route selection with little additional\n   overhead.\n"
- title: 3.6 Information Hiding
  contents:
  - "3.6 Information Hiding\n   PV has a clear advantage with respect to selective\
    \ information\n   hiding.  LS with hop-by-hop routing hinges on the ability of\
    \ all\n   domains to have exactly the same information; this clearly\n   contradicts\
    \ the notion of selective information hiding.  That is, the\n   requirement to\
    \ perform selective information hiding is unsatisfiable\n   with LS hop-by-hop\
    \ routing.\n"
- title: 3.7 Commonality between NR and SDR Components
  contents:
  - "3.7 Commonality between NR and SDR Components\n   In [Breslau-Estrin91] we argued\
    \ for the use of LS in conjunction with\n   SDR.  Therefore there is some preference\
    \ for using LS with NR.\n   However, there are several reasons why NR and SDR\
    \ would not use\n   exactly the same routing information, even if they did use\
    \ the same\n   algorithm.  Moreover, there are several opportunities for unifying\n\
    \   the management (distribution and storage) of routing and forwarding\n   information,\
    \ even if dissimilar algorithms are used.\n   In considering the differences between\
    \ NR and SDR we must address\n   several areas:\n     1. Routing information and\
    \ distribution protocol: LS for SDR is\n        quite different from the LS in\
    \ NR.  For example, SDR LS need\n        not aggregate domains; to the contrary\
    \ SDR LS  requires detailed\n        information to generate special routes.\n\
    \        In addition, consistency requirements (essential for NR) are\n      \
    \  unnecessary for the SDR component.  Therefore LS information for\n        the\
    \ SDR component can be retrieved on-demand, while the NR\n        component must\
    \ use flooding of topology information.\n     2. Route computation algorithm:\
    \ It is not clear whether route\n        computation algorithm(s)  can be shared\
    \ between the SDR and NR\n        components, given the difficulty of supporting\
    \  heterogeneous\n        route selection policies in NR.\n     3. Forwarding\
    \ information: The use of dissimilar route computation\n        algorithms does\
    \ not preclude common handling of packet\n        forwarding.  Even if LS were\
    \ used for NR, the requirement would\n        be the same, i.e., that the forwarding\
    \ agent can determine\n        whether to use a NR  precomputed route or an SDR\
    \ installed route\n        to forward a particular data packet.\n   In conclusion,\
    \ using similar algorithms and mechanisms for SDR and NR\n   components would\
    \ have benefits.  However, these benefits do not\n   dominate the other factors\
    \ as discussed before.\n"
- title: 3.8 Summary
  contents:
  - "3.8 Summary\n   Given the performance complexity issues associated with global\n\
    \   routing, aggregation of routing information is essential; at the same\n  \
    \ time we have argued that such aggregation must be flexible.  Given\n   the difficulties\
    \ of supporting LS hop-by-hop routing in the presence\n   of (a) flexible aggregation,\
    \ (b) heterogeneous route selection\n   policies, and (c) incomplete or inconsistent\
    \ routing information, we\n   see no alternative but to employ PV for the NR component\
    \ of our\n   architecture.\n   Based on the above tradeoffs, our NR component\
    \ employs a PV\n   architecture, where route computation and installation is done\
    \ in a\n   distributed fashion by the routers participating in the NR component\n\
    \   [Footnote: Packet forwarding along routes produced by the NR\n   component\
    \ can be accomplished by either source routing or hop-by-hop\n   routing.  The\
    \ latter is the primary choice because it reduces the\n   amount of state in routers\
    \ (if route setup is employed), or avoids\n   encoding an explicit source route\
    \ in network layer packets.  However,\n   the architecture does not preclude the\
    \ use of source routing (or\n   route setup) along the routes computed, but not\
    \ installed, by the NR\n   component.].\n   The distributed algorithm combines\
    \ some of the features of link state\n   with those of distance vector algorithms;\
    \ in addition to next hop\n   information, the NR component maintains path attributes\
    \ for each\n   route (e.g., the list of domains or routing domain confederations\n\
    \   that the routing information has traversed so far).  The path\n   attributes\
    \ that are carried along with a route express a variety of\n   routing policies,\
    \ and make explicit the entire route to the\n   destination.  With aggregation,\
    \ this is a superset of the domains\n   that form the path to the destination.\n"
- title: 4.0 Source-demand routing (SDR)
  contents:
  - "4.0 Source-demand routing (SDR)\n   Inter-domain routers participating in the\
    \ SDR component forward\n   packets according to routing information computed\
    \ and installed by\n   the domain that originates the traffic (source routing\
    \ domain).\n   It is important to realize that requiring route installation by\
    \ the\n   source routing domain is not a matter of choice, but rather a\n   necessity.\
    \  If a particular route is used by a small number of\n   domains (perhaps only\
    \ one) then it is more appropriate to have the\n   source compute and install\
    \ the special route instead of burdening the\n   intermediate nodes with the task\
    \ of looking for and selecting a route\n   with the specialized requirements.\
    \  In addition, if the demand for\n   the route is unpredictable, and thus can\
    \ be determined only by the\n   source, it should be up to the source to install\
    \ the route.\n   In general, information that is used by source routing domains\
    \ for\n   computing source-demand routes reflects administrative (but not\n  \
    \ operational) status of the routing facilities (i.e., configured\n   topology\
    \ and policy) [Footnote: If SDR uses NR information then\n   operational status\
    \ could be considered in some route selection.].\n   Consequently, it is possible\
    \ for a source routing domain to compute a\n   route that is not operational at\
    \ route installation time.  The SDR\n   component attempts to notify the source\
    \ domain of failures when route\n   installation is attempted.  Similarly, the\
    \ SDR component provides\n   mechanisms for the source routing domain to be notified\
    \ of failures\n   along previously-installed active routes.  In other words, the\
    \ SDR\n   component performs routing that is adaptive to topological changes;\n\
    \   however, the adaptability is achieved as a consequence of the route\n   installation\
    \ and route management mechanisms.  This is different from\n   the NR component,\
    \ where status changes are propagated and\n   incorporated by nodes as soon as\
    \ possible.  Therefore, to allow\n   faster adaptation to changes in the operational\
    \ status of routing\n   facilities, the SDR component allows the source domain\
    \ to switch to a\n   route computed by the NR component, if failure along the\
    \ source-\n   demand route is detected (either during the route installation phase,\n\
    \   or after the route is installed), and if policy permits use of the NR\n  \
    \ route.\n   The NR component will group domains into confederations to achieve\n\
    \   its scaling goals (see [IDRP91]).  In contrast, SDR will allow an\n   AD-level\
    \ route to be used by an individual domain without allowing\n   use by the entire\
    \ confederation to which the domain belongs.\n   Similarly, a single transit domain\
    \ may support a policy or special\n   TOS that is not supported by other domains\
    \ in its confederation(s).\n   In other words, the architecture uses SDR to support\
    \ non-\n   hierarchical, non-aggregated policies where and when needed.\n   Consequently,\
    \ SDR by itself does not have the scaling properties of\n   NR.  In compensation,\
    \ SDR does not require a complete, global domain\n   map if portions of the world\
    \ are never traversed or communicated\n   with.  As a result of the looser routing\
    \ structure, SDR does not\n   guarantee that a participating source routing domain\
    \ will always have\n   sufficient information to compute a route to a destination.\
    \  In\n   addition, if the domain does have sufficient information, it is\n  \
    \ possible that the quantity may be large enough to preclude storage\n   and/or\
    \ route computation in a timely fashion.  However, despite the\n   lack of guarantees,\
    \ it is a goal of the architecture to provide\n   efficient methods whereby sources\
    \ can obtain the information needed\n   to compute desired routes [Footnote: The\
    \ primary goal of policy or\n   TOS routing is to compute a route that satisfies\
    \ a set of specialized\n   requirements, and these requirements take precedence\
    \ over optimality.\n   In other words, even if a routing domain that participates\
    \ in SDR or\n   NR has sufficient information to compute a route, given a particular\n\
    \   set of requirements, the architecture does not guarantee that the\n   computed\
    \ route is optimal.].\n   Essential to SDR is the assumption that the routes installed\
    \ on\n   demand will be used sparingly.  The architecture assumes that at any\n\
    \   given moment the set of all source-demand routes installed in an\n   internet\
    \ forms a small fraction of the total number of source-demand\n   routes that\
    \ can potentially be installed by all the routing domains.\n   It is an assumption\
    \ of the architecture that the number of routes\n   installed in a BR by the SDR\
    \ component should be on the order of log\n   N (where N is the total number of\
    \ routing domains in the Internet),\n   so that the scaling properties of the\
    \ SDR component are comparable\n   with the scaling properties of the NR component.\
    \  The NR component\n   achieves this property as a result of hierarchy.\n   Note\
    \ that the above requirement does not imply that only a few\n   domains can participate\
    \ in SDR, or that routes installed by the SDR\n   component must have short life\
    \ times.  What the requirement does\n   imply, is that the product of the number\
    \ of routes specified by\n   domains that participate in SDR, times the average\
    \ SDR-route life\n   time, is bounded.  For example, the architecture allows either\
    \ a\n   small number of SDR routes with relatively long average life times,\n\
    \   or a large number of SDR routes with relatively short average life\n   times.\
    \  But the architecture clearly prohibits a large number of SDR\n   routes with\
    \ relatively long average life times.  The number of SDR\n   routes is a function\
    \ of the number of domains using SDR routes and\n   the number of routes used\
    \ per source domain.\n   In summary, SDR is well suited for traffic that (1) is\
    \ not widely-\n   used enough (or is not sufficiently predictable or steady) to\
    \ justify\n   computation and maintenance by the NR component, and (2) whose\n\
    \   duration is significantly longer than the time it takes to perform\n   the\
    \ route installation procedure.\n   The architecture does not require all domains\
    \ in the Internet to\n   participate in SDR.  Therefore, issues of scalability\
    \ (with respect\n   to the size of the Internet) become less crucial (though still\n\
    \   important) to the SDR component.  Instead, the primary focus of the\n   SDR\
    \ component is shifted towards the ability to compute routes that\n   satisfy\
    \ specialized requirements, where we assume that the total\n   number of domains\
    \ requiring special routes simultaneously through the\n   same part of the network\
    \ is small relative to the total population.\n"
- title: 4.1 Path Vector vs. Link State for SDR
  contents:
  - "4.1 Path Vector vs. Link State for SDR\n   It is feasible to use either a distance\
    \ vector or link state method\n   of route computation along with source routing.\
    \  One could imagine,\n   for instance, a protocol like BGP in which the source\
    \ uses the full\n   AD path information it receives in routing updates to create\
    \ a source\n   route. Such a protocol could address some of the deficiencies\n\
    \   identified with distance vector, hop-by-hop designs.  However, we opt\n  \
    \ against further discussion of such a protocol because there is less\n   to gain\
    \ by using source routing without also using a link state\n   scheme.  The power\
    \ of source routing, in the context of inter-AD\n   policy routing, is in giving\
    \ the source control over the entire\n   route.  This goal cannot be realized\
    \ fully when intermediate nodes\n   control which legal routes are advertised\
    \ to neighbors, and therefore\n   to a source.\n   In other words, intermediate\
    \ nodes should be able to preclude the use\n   of a route by expressing a transit\
    \ policy, but if a route is not\n   precluded (i.e.,  is legal according to all\
    \ ADs in the route), the\n   route should be made available to the source independent\
    \ of an\n   intermediate domain's preferences for how its own traffic flows.\n\
    \   Therefore, the SDR component employs an IDPR-like architecture in\n   which\
    \ link-state style updates are distributed with explicit policy\n   terms included\
    \ in each update along with the advertising node's\n   connectivity.\n"
- title: 4.2 Distribution of Routing Information
  contents:
  - "4.2 Distribution of Routing Information\n   By using a hop-by-hop NR component\
    \ based on PV to complement the\n   source-routing SDR component, we have alleviated\
    \ the pressure to\n   aggregate SDR forwarding information; the large percentage\
    \ of inter-\n   domain traffic carried, simultaneously, by any particular border\n\
    \   router will be forwarded using aggregated NR forwarding information.\n   However,\
    \ the use of NR does not address the other major scaling\n   problem associated\
    \ with SDR: that of distributing, storing, and\n   computing over a complete domain-level\
    \ topology map.  In this section\n   we describe promising opportunities for improving\
    \ the scaling\n   properties of SDR routing information distribution, storage,\
    \ and\n   computation.\n   Note that we do not propose to solve this problem in\
    \ the same way\n   that we solve it for NR.  A priori abstraction will not be\
    \ employed\n   since different domains may require different methods of abstracting\n\
    \   the same routing information.  For example, if we aggregate routing\n   information\
    \ of domains that do not share the same policy and TOS\n   characteristics (i.e.,\
    \ services), then outside of the aggregate, only\n   those services that are offered\
    \ by all domains in the aggregate will\n   be advertised.  In order to locate\
    \ special routes, SDR only uses\n   aggregates when the component domains (and\
    \ in turn the aggregate)\n   advertise the required TOS and policy descriptions.\
    \  When the\n   required TOS or policy characteristics are not offered by an\n\
    \   aggregate, full information about the component domains is used to\n   construct\
    \ a route through those domains that do support the\n   particular characteristics.\
    \  Consequently, we need some other, more\n   flexible, means of reducing the\
    \ amount of information distributed and\n   held.  We address two issues in turn:\
    \ distribution of configured\n   topology and policy information, and distribution\
    \ of dynamic status\n   information.\n"
- title: 4.2.1 Configured Information
  contents:
  - "4.2.1 Configured Information\n   Information about the existence of inter-domain\
    \ links, and policies\n   maintained by domains, changes slowly over time.  This\
    \ is referred to\n   as configured information.  In the current IDPR specification\n\
    \   complete, global, configuration information is kept by a route server\n  \
    \ in each domain.  Route servers (RS) are the entities that compute\n   source\
    \ routes.  On startup a RS can download the connectivity\n   database from a neighbor\
    \ RS; as domains, inter-domain links, or\n   policies change, the changes are\
    \ flooded to a RS in each domain.\n   We have not yet specified the exact mechanisms\
    \ for distributing\n   configured connectivity information for SDR.  However,\
    \ unlike the\n   current IDPR specification, the SDR component will not flood\
    \ all\n   configured information globally.  Several alternate methods for\n  \
    \ organizing and distributing information are under investigation.\n   Configured\
    \ information may be regularly distributed via an out-of-\n   band channel, e.g.,\
    \ CD/ROM.  In a similar vein, this information\n   could be posted in several\
    \ well-known locations for retrieval, e.g.,\n   via FTP.  Between these \"major\"\
    \ updates, aggregated collections of\n   changes may be flooded globally.  Moreover,\
    \ limited flooding (e.g.,\n   by hop-count) could be used as appropriate to the\
    \ \"importance\" of the\n   change; while a policy change in a major backbone\
    \ may still be\n   flooded globally, a new inter-regional link may be flooded\
    \ only\n   within those regions, and information about an additional link to a\n\
    \   non-transit domain may not be available until the next regularly-\n   scheduled\
    \ \"major\" distribution.\n   Changes that are not distributed as they occur will\
    \ not necessarily\n   be discovered.  However, a route server may learn pertinent\n\
    \   information by direct query of remote servers, or through error\n   messages\
    \ resulting from traffic sent along failed routes.  Complete\n   global flooding\
    \ may be avoided by using some combination of these\n   mechanisms.\n   Even if\
    \ an initial implementation uses a simple global flood, we must\n   study the\
    \ problem of structuring connectivity information such that\n   it can be retrieved\
    \ or distributed in a more selective manner, while\n   still allowing sources\
    \ to discover desired routes.  For example, we\n   imagine RSs requesting filtered\
    \ information from each other.  How the\n   RSs should define filters that will\
    \ get enough information to find\n   special routes, while also effectively limiting\
    \ the information, is\n   an open question.  Again, the question is how to effectively\n\
    \   anticipate and describe what information is needed in advance of\n   computing\
    \ the route.\n   The essential dilemma is that networks are not organized in a\
    \ nicely\n   geographical or topologically consistent manner (e.g., it is not\n\
    \   effective to ask for all networks going east-west that are within a\n   certain\
    \ north-south region of the target), hence a source domain does\n   not know what\
    \ information it needs (or should ask for) until it\n   searches for, and discovers,\
    \ the actual path.  Even with a central\n   database, techniques are needed to\
    \ structure configuration\n   information so that the potential paths that are\
    \ most likely to be\n   useful are explored first, thereby reducing the time required\
    \ for\n   route computation.\n   One promising approach organizes information\
    \ using route fragments\n   (partial paths) [Footnote: Route fragments were first\
    \ suggested by\n   Dave Clark and Noel Chiappa.].  Although the number of route\n\
    \   fragments grows faster than the number of domains (at least O(N^2)),\n   we\
    \ can selectively choose those that will be useful to compute\n   routes.  In\
    \ particular, for each stub domain, fragments would be\n   constructed to several\
    \ well-known backbones [Footnote: Route\n   fragments may be computed by a destination's\
    \ route server and either\n   made available via information service queries or\
    \ global flooding.\n   In addition, NR computed routes may be used as SDR route\
    \ fragments.].\n   Among its benefits, this approach aggregates domain information\
    \ in a\n   manner useful for computing source-routes, and provides an index,\n\
    \   namely the destination, which facilitates on-demand reference and\n   retrieval\
    \ of information pertinent to a particular route computation.\n   At this point,\
    \ it is not clear how route fragments will affect SDR's\n   ability to discover\
    \ non-hierarchical routes.\n"
- title: 4.2.2 Dynamic Status Information
  contents:
  - "4.2.2 Dynamic Status Information\n   Assuming a technique for global or partial\
    \ distribution of configured\n   information, a second issue is whether, and how,\
    \ to distribute\n   dynamic status information (i.e., whether an inter-domain\
    \ connection\n   is up or down).\n   In the current version of IDPR, dynamic status\
    \ information is flooded\n   globally in addition to configuration information.\
    \  We propose to\n   distribute status information based strictly on locality.\
    \  First,\n   dynamic information will be advertised within a small hop-count\n\
    \   radius.  This simple and low-overhead mechanism exploits topological\n   locality.\
    \  In addition to flooding status updates to nearby nodes, we\n   also want to\
    \ provide more accurate route information for long\n   distance communications\
    \ that entails more than a few network hops.\n   Reverse path update (RPU) is\
    \ a mechanism for sending dynamic status\n   information to nodes that are outside\
    \ the k-hop radius used for\n   updates, but that nevertheless would obtain better\
    \ service (fewer\n   failed setups) by having access to the dynamic information\
    \ [Estrin-\n   etal91].\n   RPU uses the existing active routes (represented by\
    \ installed setup\n   state or by a cache of the most recent source routes sent\
    \ via the\n   node in question) as a hint for distribution of event notifications.\n\
    \   Instead of reporting only the status of the route being used, RPU\n   reports\
    \ the status of the domain's other inter-domain connections.\n   If source routing\
    \ exhibits route locality, the source is more likely\n   to use other routes going\
    \ through the node in question; in any case\n   the overhead of the information\
    \ about other links will be minimal.\n   In this way, sources will receive status\
    \ information from regions of\n   the network through which they maintain active\
    \ routes, even if those\n   regions are more than k hops away.  Using such a scheme,\
    \ k could be\n   small to maximize efficiency, and RPU could be used to reduce\
    \ the\n   incidence of failed routes resulting from inaccurate status\n   information.\
    \  This will be useful if long-path communication exhibits\n   route locality\
    \ with respect to regions that are closer to the\n   destination (and therefore\
    \ outside the k hop radius of flooded\n   information).  In such situations, flooding\
    \ information to the source\n   of the long route would be inefficient because\
    \ k would have to be\n   equal to the length of the route, and in almost all cases,\
    \ the\n   percentage of nodes that would use the information decreases\n   significantly\
    \ with larger k.\n"
- title: 4.3 Source-Demand Route Management
  contents:
  - "4.3 Source-Demand Route Management\n   SDR may be built either on top of the\
    \ network layer supported by the\n   NR component, or in parallel with it.  SDR\
    \ forwarding will be\n   supported via two techniques: loose source-routing and\
    \ route setup.\n   The first technique, loose source-routing, would allow the\
    \ originator\n   of a packet to specify a sequence of domains that the packet\
    \ should\n   traverse on its path to a destination.  Forwarding such a packet\n\
    \   within a domain, or even between domains within a confederation,\n   would\
    \ be left to intra-domain routing.  This avoids per-connection\n   state and supports\
    \ transaction traffic.\n   The second technique, route setup, will be based on\
    \ mechanisms\n   developed for IDPR and described in [IDPR90].  It is well suited\
    \ to\n   conversations that persist significantly longer than a round-trip-\n\
    \   time.  The setup protocol defines packet formats and the processing\n   of\
    \ route installation request packets (i.e, setup packets).  When a\n   source\
    \ generates a setup packet, the first border router along the\n   specified source\
    \ route checks the setup request, and if accepted,\n   installs routing information;\
    \ this information includes a path ID,\n   the previous and next hops, and whatever\
    \ other accounting-related\n   information the particular domain requires.  The\
    \ setup packet is\n   passed on to the next BR in the domain-level source route,\
    \ and the\n   same procedure is carried out [Footnote: The setup packet may be\n\
    \   forwarded optimistically, i.e., before checks are completed, to\n   reduce\
    \ latency.].  When the setup packet reaches the destination, an\n   accept message\
    \ is propagated back hop by hop, and each BR en route\n   activates its routing\
    \ information.  Subsequent data packets traveling\n   along the same path to the\
    \ destination include a path ID in the\n   packet.  That path ID is used to locate\
    \ the appropriate next-hop\n   information for each packet.\n   Border routers\
    \ that support both the NR and the SDR components, must\n   be able to determine\
    \ what forwarding mechanism to use.  That is, when\n   presented with a network\
    \ layer PDU, such a BR should be able to make\n   an unambiguous decision about\
    \ whether forwarding of that PDU should\n   be handled by the NR or the SDR component.\
    \  Discrimination mechanisms\n   are dependent on whether the new network layer\
    \ introduced by the SDR\n   component is built on top of, or in parallel with,\
    \ the network layers\n   supported by the NR component.  Once the discrimination\
    \ is made,\n   packets that have to be forwarded via routes installed by the SDR\n\
    \   component are forwarded to the exit port associated with the\n   particular\
    \ Path ID in the packet header.  Packets that have to be\n   forwarded via routes\
    \ installed by the NR component are forwarded to\n   the exit port associated\
    \ with the particular destination and Type of\n   Service parameters (if present)\
    \ in their packet headers.\n   Next, we describe the primary differences between\
    \ the IDPR setup\n   procedure previously specified, and the procedure we propose\
    \ to\n   develop for this hybrid architecture.\n   During route installation,\
    \ if a BR on the path finds that the\n   remainder of the indicated route from\
    \ the BR to the destination is\n   identical to the NR route from the BR to the\
    \ destination, then the BR\n   can turn off the SDR route at that point and map\
    \ it onto the NR\n   route.  For this to occur, the specifications of the SDR\
    \ route must\n   completely match those of the NR route.  In addition, the entire\n\
    \   forward route must be equivalent (i.e., the remaining hops to the\n   destination).\n\
    \   Moreover, if the NR route changes during the course of an active SDR\n   route,\
    \ and the new NR route does not match the SDR route, then the\n   SDR route must\
    \ be installed for the remainder of the way to the\n   destination.  Consequently,\
    \ when an SDR route is mapped onto an NR\n   route, the original setup packet\
    \ must be saved.  A packet traveling\n   from a source to destination may therefore\
    \ traverse both an SDR and\n   an NR route segment; however, a packet will not\
    \ traverse another SDR\n   segment after traveling over an NR segment.  However,\
    \ during\n   transient periods packets could traverse the wrong route and\n  \
    \ therefore this must be an optional and controllable feature.\n   A source can\
    \ also request notification if a previously-down link or\n   node returns to operation\
    \ some time after a requested route setup\n   fails.  If a BR on the route discovers\
    \ that the requested next-hop BR\n   is not available, the BR can add the source\
    \ to a notification list\n   and when the next-hop BR becomes reachable, a notification\
    \ can be\n   sent back to the source.  This provides a means of flushing out bad\n\
    \   news when it is no longer true.  For example, a domain might decide\n   to\
    \ route through a secondary route when its preferred route fails;\n   the notification\
    \ mechanism would inform the source in a timely manner\n   when its preferred\
    \ route is available again.\n   A third option addresses adaptation after route\
    \ installation.  During\n   packet forwarding along an active SDR route, if a\
    \ BR finds that the\n   SDR route has failed, it may redirect the traffic along\
    \ an existing\n   NR route to the destination.  This adaptation is allowed only\
    \ if use\n   of the NR route does not violate policy; for example, it may provide\n\
    \   a less desirable type of service.  This is done only if the source\n   selects\
    \ the option at route setup time.  It is also up to the source\n   whether it\
    \ is to be notified of such actions.\n   When a SDR route does fail, the detecting\
    \ BR sends notification to\n   the source(s) of the active routes that are affected.\
    \  Optionally,\n   the detecting BR may include additional information about the\
    \ state\n   of other BRs in the same domain.  In particular, the BR can include\n\
    \   its domain's most recent \"update\" indicating that domain's inter-\n   domain\
    \ links and policy.  This can be helpful to the extent there is\n   communication\
    \ locality; i.e., if alternative routes might be used\n   that traverse the domain\
    \ in question, but avoid the failed BR.\n   In summary, when a route is first\
    \ installed, the source has several\n   options (which are represented by flags\
    \ in the route setup packet):\n     1. If an NR route is available that satisfies\
    \ all local policy\n        and TOS, then use it.  Otherwise...\n     2. Indicate\
    \ whether the source wants to allow the setup to\n        default to a NR route\
    \ if the SDR route setup fails.\n     3. Request notification of mapping to a\
    \ NR route.\n     4. Request additional configured information on failure.\n \
    \    5. Request addition to a notification list for resource\n        re-availability.\n\
    \     6. Allow data packets to be rerouted to a NR route when failure\n      \
    \  happens after setup (so long  as no policy is violated).\n     7. Request notification\
    \ of a reroute of data packets.\n     8. Request additional configured information\
    \ on failure notice\n        when the route is active.\n     9. Request addition\
    \ to a notification list if an active route\n        fails.\n"
- title: 5.0 The Unified Architecture
  contents:
  - "5.0 The Unified Architecture\n   In addition to further evaluation and implementation\
    \ of the proposed\n   architecture, future research must investigate opportunities\
    \ for\n   increased unification of the two components of our architecture.  We\n\
    \   are investigating several opportunities for additional commonality:\n    \
    \ 1. Routing Information Base:\n        Perhaps a single RIB could be shared by\
    \ both NR and SDR.\n        NR routes can be represented as a directed graph labeled\n\
    \        with flags (on the nodes or links) corresponding to the\n        generic\
    \ transit constraints.  SDR requires that this graph\n        be augmented by\
    \ links with non-generic policies that have\n        been discovered and maintained\
    \ for computing special routes;\n        in addition, special policy flags may\
    \ be added to links\n        already maintained by the NR component.\n     2.\
    \ Information Distribution:\n        The NR path vectors could include address(es)\
    \ of repositories\n        for SDR-update information for each AD (or confederation)\
    \ to\n        assist the SDR component in retrieving selective information\n \
    \       on demand.  For domains with minimal policies, where the space\n     \
    \   required for policy information is smaller than the space\n        required\
    \ for a repository address (e.g., if the policies for\n        the domain listed\
    \ are all wildcard), the NR path vectors could\n        include a flag to that\
    \ effect.\n     3. Packet Forwarding:\n        We should consider replacing the\
    \ current IDPR-style network\n        layer (which contains a global path identifier\
    \ used in\n        forwarding data packets to the next policy gateway on an\n\
    \        IDPR route)  with a standard header (e.g., IP or CLNP),\n        augmented\
    \ with some option fields.  This would  unify the\n        packet header parsing\
    \ and forwarding functions for SDR and NR,\n        and possibly eliminate some\
    \ encapsulation overhead.\n     4. Reachability Information:\n        Currently\
    \ IDRP distributes network reachability information\n        within updates, whereas\
    \ IDPR only distributes domain\n        reachability information.  IDPR uses a\
    \ domain name service\n        function to map network numbers to domain numbers;\
    \ the latter\n        is needed to make the routing decision.   We should consider\n\
    \        obtaining the network reachability and domain information in\n      \
    \  a unified manner.\n"
- title: 5.1 Applicability to Various Network Layer Protocols
  contents:
  - "5.1 Applicability to Various Network Layer Protocols\n   The proposed architecture\
    \ is designed to accommodate such existing\n   network layer protocols as IP ([Postel81]),\
    \ CLNP ([ISO-473-88]), and\n   ST-II ([ST2-90]).  In addition, we intend for this\
    \ architecture to\n   support future network layer mechanisms, e.g., Clark and\
    \ Jacobson's\n   proposal or Braden and Casner's Integrated Services IP.  However\
    \ on\n   principal we can not make sweeping guarantees in advance of the\n   mechanisms\
    \ themselves.  In any case, not all of the mentioned\n   protocols will be able\
    \ to utilize all of the capabilities provided by\n   the architecture.  For instance,\
    \ unless the increase in the number of\n   different types of services offered\
    \ is matched by the ability of a\n   particular network layer protocol to unambiguously\
    \ express requests\n   for such different types of services, the capability of\
    \ the\n   architecture to support routing in the presence of a large number of\n\
    \   different types of service is largely academic.  That is, not all\n   components\
    \ of the architecture will have equal importance for\n   different network layer\
    \ protocols.  On the other hand, this\n   architecture is designed to serve the\
    \ future global internetworking\n   environment.  The extensive research and development\
    \ currently\n   underway to implement and evaluate network mechanisms for different\n\
    \   types of service suggests that future networks will offer such\n   services.\n\
    \   One of the fundamental issues in the proposed architecture is the\n   issue\
    \ of single versus multiple protocols.  The architecture does not\n   make any\
    \ assumptions about whether each network layer is going to\n   have its own inter-domain\
    \ routing protocol, or a single inter-domain\n   routing protocol will be able\
    \ to cover multiple network layers\n   [Footnote: Similar issue already arose\
    \ with respect to the intra-\n   domain routing protocol, which generated sufficient\
    \ amount of\n   controversy within the Internet community.  It is our opinion,\
    \ that\n   the issue of single versus multiple protocols is more complex for the\n\
    \   inter-domain routing than for the intra-domain routing.].  That is,\n   the\
    \ proposed architecture can be realized either by a single inter-\n   domain routing\
    \ protocol covering multiple network layers, or by\n   multiple inter-domain routing\
    \ protocols (with the same architecture)\n   tailored to a specific network layer\
    \ [Footnote: If the single\n   protocol strategy is adopted, then it is likely\
    \ that IDRP will be\n   used as a base for the NR component.  Since presently\
    \ IDRP is\n   targeted towards CLNP, further work is needed to augment it to\n\
    \   support IP and ST-II.  If the multiple protocol strategy is adopted,\n   then\
    \ it is likely that BGP will be used as a base for the NR\n   component for IP,\
    \ and IDRP will be used as a base for the NR\n   component for CLNP.  Further\
    \ work is needed to specify protocol in\n   support for the NR component for ST-II.\
    \  Additional work may be\n   needed to specify new features that may be added\
    \ to BGP.].\n"
- title: 5.2 Transition
  contents:
  - "5.2 Transition\n   The proposed architecture is not intended for full deployment\
    \ in the\n   short term future.  We are proposing this architecture as a goal\n\
    \   towards which we can begin guiding our operational and research\n   investment\
    \ over the next 5 years.\n   At the same time, the architecture does not require\
    \ wholesale\n   overhaul of the existing Internet.  The NR component may be phased\
    \ in\n   gradually.  For example, the NR component for IP may be phased in by\n\
    \   replacing existing EGP-2 routing with BGP routing.  Once the NR\n   component\
    \ is in place, it can be augmented by the facilities provided\n   by the SDR component.\n\
    \   The most critical components of the architecture needed to support\n   SDR\
    \ include route installation and packet forwarding in the routers\n   that support\
    \ SDR.  Participation as a transit routing domain requires\n   that the domain\
    \ can distribute local configuration information (LCI)\n   and that some of its\
    \ routers implement the route installation and\n   route management protocols.\
    \  Participation as a source requires that\n   the domain have access to a RS\
    \ to compute routes, and that the source\n   domain has a router that implements\
    \ the route installation and route\n   management protocols.  In addition, a network\
    \ management entity must\n   describe local configuration information and send\
    \ it to the central\n   repository(ies).  A collection and distribution mechanism\
    \ must be put\n   in place, even if it is centralized.\n"
- title: 6.0 Conclusions and Future Work
  contents:
  - "6.0 Conclusions and Future Work\n   In summary, the proposed architecture combines\
    \ hop-by-hop path-\n   vector, and source-routed link-state, protocols, and uses\
    \ each for\n   that which it is best suited: NR uses PV and multiple, flexible,\n\
    \   levels of confederations to support efficient routing of generic\n   packets\
    \ over generic routes; SDR uses LS computation over a database\n   of configured\
    \ and dynamic information to route special traffic over\n   special routes.  In\
    \ the past, the community has viewed these two as\n   mutually exclusive; to the\
    \ contrary, they are quite complementary and\n   it is fortunate that we, as a\
    \ community, have pursued both paths in\n   parallel.  Together these two approaches\
    \ will flexibly and\n   efficiently support TOS and policy routing in very large\
    \ global\n   internets.\n   It is now time to consider the issues associated with\
    \ combining and\n   integrating the two.  We must go back and look at both architectures\n\
    \   and their constituent protocols, eliminate redundancies, fill in new\n   holes,\
    \ and provide seamless integration.\n"
- title: 7.0 Acknowledgments
  contents:
  - "7.0 Acknowledgments\n   We would like to thank Hans-Werner Braun (San Diego Supercomputer\n\
    \   Center), Lee Breslau (USC), Scott Brim (Cornell University), Tony Li\n   (cisco\
    \ Systems), Doug Montgomery (NIST), Tassos Nakassis (NIST),\n   Martha Steenstrup\
    \ (BBN), and Daniel Zappala (USC) for their comments\n   on a previous draft.\n"
- title: 8.0 References
  contents:
  - "8.0 References\n   [ANSI 87-150R]  \"Intermediate System to Intermediate System\
    \ Intra-\n   Domain Routing Exchange Protocol\", ANSI X3S3.3/87-150R.\n   [BGP\
    \ 91]  Lougheed, K., and Y. Rekhter, \"A Border Gateway Protocol 3\n   (BGP-3)\"\
    , RFC 1267, cisco Systems, T.J. Watson Research Center, IBM\n   Corp., October\
    \ 1991.\n   [Breslau-Estrin 91]  Breslau, L., and D. Estrin, \"Design and\n  \
    \ Evaluation of Inter-Domain Policy Routing Protocols\", To appear in\n   Journal\
    \  of Internetworking Research and Experience, 1991.  (Earlier\n   version appeared\
    \ in ACM Sigcomm 1990.)\n   [Clark 90]  Clark, D., \"Policy Routing in Internetworks\"\
    , Journal of\n   Internetworking Research and Experience, Vol.  1, pp. 35-52,\
    \ 1990.\n   [Dijkstra 59]  Dijkstra, E., \"A Note on Two Problems in Connection\n\
    \   with Graphs\", Numer. Math., Vol.  1, 1959, pp. 269-271.\n   [ECMA89]  \"\
    Inter-Domain Intermediate Systems Routing\", Draft\n   Technical Report ECMA TR/ISR,\
    \ ECMA/TC32-TG 10/89/56, May 1989.\n   [EGP]  Rosen, E., \"Exterior Gateway Protocol\
    \ (EGP)\", RFC 827, BBN,\n   October 1982.\n   [Estrin 89]  Estrin, D., \"Policy\
    \ Requirements for Inter\n   Administrative Domain Routing\", RFC 1125, USC Computer\
    \ Science\n   Department, November 1989.\n   [Estrin-etal91]  Estrin, D., Breslau,\
    \ L., and L. Zhang, \"Protocol\n   Mechanisms for Adaptive Routing in Global Multimedia\
    \ Internets\",\n   University of Southern California, Computer Science Department\n\
    \   Technical Report, CS-SYS-91-04, November 1991.\n   [Hedrick 88]  Hedrick,\
    \ C., \"Routing Information Protocol\", RFC 1058,\n   Rutgers University, June\
    \ 1988.\n   [Honig 90]  Honig, J., Katz, D., Mathis, M., Rekhter, Y., and J. Yu,\n\
    \   \"Application of the Border Gateway Protocol in the Internet\", RFC\n   1164,\
    \ Cornell Univ. Theory Center, Merit/NSFNET, Pittsburgh\n   Supercomputing Center,\
    \ T.J. Watson Research Center, IBM Corp., June\n   1990.\n   [IDPR90]  Steenstrup,\
    \ M., \"Inter-Domain Policy Routing Protocol\n   Specification and Usage: Version\
    \ 1\", Work in Progress, February 1991.\n   [IDRP91]  \"Intermediate System to\
    \ Intermediate System Inter-domain\n   Routeing Exchange Protocol\", ISO/IEC/\
    \ JTC1/SC6 CD10747.\n   [ISIS10589]  \"Information Processing Systems - Telecommunications\
    \ and\n   Information Exchange between Systems - Intermediate System to\n   Intermediate\
    \ System Intra-Domain Routing Exchange Protocol for use in\n   Conjunction with\
    \ the protocol for providing the Connectionless-mode\n   Network Service (ISO\
    \ 8473)\", ISO/IEC 10589.\n   [ISO-473 88]  \"Protocol for providing the connectionless-mode\
    \ network\n   service\", ISO 8473, 1988.\n   [Jaffee 82]  Jaffee, J., and F. Moss,\
    \ \"A Responsive Distributed\n   Routing Algorithm for Computer Networks\", IEEE\
    \ Transactions on\n   Communications, July 1982.\n   [Little 89]  Little, M.,\
    \ \"Goals and Functional Requirements for\n   Inter-Autonomous System Routing\"\
    , RFC 1126, SAIC, October 1989.\n   [Oran 89]  Oran, D., \"Expert's Paper: The\
    \ Relationship between\n   Addressing and Routeing\", ISO/JTC1/SC6/WG2, 1989.\n\
    \   [OSPF]  Moy, J., \"The Open Shortest Path First (OSPF) Specification\",\n\
    \   RFC 1131, Proteon, October 1989.\n   [Postel 81]  Postel, J., \"Internet Protocol\"\
    , RFC 791, DARPA,\n   September 1981.\n   [Rekhter 91]  Rekhter, Y., \"IDRP protocol\
    \ analysis: storage\n   complexity\", IBM Research Report RC17298(#76515), October\
    \ 1991.\n   [Shin87] Shin, K., and M. Chen, \"Performance Analysis of Distributed\n\
    \   Routing Strategies Free of Ping-Pong-Type Looping\", IEEE Transactions\n \
    \  on Computers, February 1987.\n   [ST2-90]  Topolcic, C., \"Experimental Internet\
    \ Stream Protocol,\n   version 2 (ST II)\", RFC 1190, CIP Working Group, October\
    \ 1990.\n   [Zaumen 91] Zaumen, W., and J. Garcia-Luna-Aceves, \"Dynamics of Link\n\
    \   State and Loop-free Distance-Vector Routing Algorithms\", ACM Sigcomm\n  \
    \ '91, Zurich, Switzerland, September 1991.\n   [Zhang 91] Zhang, L., \"Virtual\
    \ Clock: A New Traffic Control Algorithm\n   for Packet Switching Networks\".\n"
- title: Security Considerations
  contents:
  - "Security Considerations\n   Security issues are not discussed in this memo.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Deborah Estrin\n   University of Southern California\n\
    \   Computer Science Department, MC 0782\n   Los Angeles, California 90089-0782\n\
    \   Phone: (310) 740-4524\n   EMail: estrin@usc.edu\n   Yakov Rekhter\n   IBM\
    \ T.J. Watson Research Center\n   P.O. Box 218\n   Yorktown Heights, New York\
    \ 10598\n   Phone: (914) 945-3896\n   EMail: yakov@ibm.com\n   Steven Hotz\n \
    \  University of Southern California\n   Computer Science Department, MC 0782\n\
    \   Los Angeles, California 90089-0782\n   Phone: (310) 822-1511\n   EMail: hotz@usc.edu\n"
