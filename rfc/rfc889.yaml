- title: __initial_text__
  contents:
  - "                          Internet Delay Experiments\nThis memo reports on some\
    \ measurement experiments and suggests some possible\nimprovements to the TCP\
    \ retransmission timeout calculation.  This memo is\nboth a status report on the\
    \ measurements and advice to implementers of TCP.\n1.  Introduction\n     This\
    \ memorandum describes two series of experiments designed to explore\nthe transmission\
    \ characteristics of the Internet system.  One series of\nexperiments was designed\
    \ to determine the network delays with respect to\npacket length, while the other\
    \ was designed to assess the effectiveness of the\nTCP retransmission-timeout\
    \ algorithm specified in the standards documents.\nBoth sets of experiments were\
    \ conducted during the October - November 1983\ntime frame and used many hosts\
    \ distributed throughout the Internet system.\n     The objectives of these experiments\
    \ were first to accumulate experimental\ndata on actual network paths that could\
    \ be used as a benchmark of Internet\nsystem performance, and second to apply\
    \ these data to refine individual TCP\nimplementations and improve their performance.\n\
    \     The experiments were done using a specially instrumented measurement host\n\
    called a Fuzzball, which consists of an LSI-11 running IP/TCP and various\napplication-layer\
    \ protocols including TELNET, FTP and SMTP mail.  Among the\nvarious measurement\
    \ packages is the original PING (Packet InterNet Groper)\nprogram used over the\
    \ last six years for numerous tests and measurements of\nthe Internet system and\
    \ its client nets.  This program contains facilities to\nsend various kinds of\
    \ probe packets, including ICMP Echo messages, process the\nreply and record elapsed\
    \ times and other information in a data file, as well\nas produce real-time snapshot\
    \ histograms and traces.\n     Following an experiment run, the data collected\
    \ in the file were reduced\nby another set of programs and plotted on a Peritek\
    \ bit-map display with color\nmonitor.  The plots have been found invaluable in\
    \ the indentification and\nunderstanding of the causes of netword glitches and\
    \ other \"zoo\" phenomena.\nFinally, summary data were extracted and presented\
    \ in this memorandum.  The\nraw data files, including bit-map image files of the\
    \ various plots, are\navailable to other experimenters upon request.\n     The\
    \ Fuzzballs and their local-net architecture, called DCN, have about\ntwo-dozen\
    \ clones scattered worldwide, including one (DCN1) at the Linkabit\nCorporation\
    \ offices in McLean, Virginia, and another at the Norwegian\nTelecommunications\
    \ Adminstration (NTA) near Oslo, Norway.  The DCN1 Fuzzball\nis connected to the\
    \ ARPANET at the Mitre IMP by means of 1822 Error Control\nUnits operating over\
    \ a 56-Kbps line.  The NTA Fuzzball is connected to the\nNTARE Gateway by an 1822\
    \ interface and then via VDH/HAP operating over a\n9.6-Kbps line to SATNET at\
    \ the Tanum (Sweden) SIMP.  For most experiments\ndescribed below, these details\
    \ of the local connectivity can be ignored, since\nonly relatively small delays\
    \ are involved.\n     The remote test hosts were selected to represent canonical\
    \ paths in the\nInternet system and were scattered all over the world.  They included\
    \ some on\nthe ARPANET, MILNET, MINET, SATNET, TELENET and numerous local nets\
    \ reachable\nvia these long-haul nets.  As an example of the richness of the Internet\n\
    system connectivity and the experimental data base, data are included for\nthree\
    \ different paths from the ARPANET-based measurement host to London hosts,\ntwo\
    \ via different satellite links and one via an undersea cable.\n2.  Packet Length\
    \ Versus Delay\n     This set of experiments was designed to determine whether\
    \ delays across\nthe Internet are significantly influenced by packet length. \
    \ In cases where\nthe intrinsic propagation delays are high relative to the time\
    \ to transmit an\nindividual packet, one would expect that delays would not be\
    \ strongly affected\nby packet length.  This is the case with satellite nets,\
    \ including SATNET and\nWIDEBAND, but also with terrestrial nets where the degree\
    \ of traffic\naggregation is high, so that the measured traffic is a small proportion\
    \ of the\ntotal traffic on the path.  However, in cases where the intrinsic propagation\n\
    delays are low and the measured traffic represents the bulk of the traffic on\n\
    the path, quite the opposite would be expected.\n     The objective of the experiments\
    \ was to assess the degree to which TCP\nperformance could be improved by refining\
    \ the retransmission-timeout algorithm\nto include a dependency on packet length.\
    \  Another objective was to determine\nthe nature of the delay characteristic\
    \ versus packet length on tandem paths\nspanning networks of widely varying architectures,\
    \ including local-nets,\nterrestrial long-haul nets and satellite nets.\n2.1.\
    \  Experiment Design\n     There were two sets of experiments to measure delays\
    \ as a function of\npacket length.  One of these was based at DCN1, while the\
    \ other was based at\nNTA.  All experiments used ICMP Echo/Reply messages with\
    \ embedded timestamps.\nA cycle consisted of sending an ICMP Echo message of specified\
    \ length, waiting\nfor the corresponding ICMP Reply message to come back and recording\
    \ the\nelapsed time (normalized to one-way delay).  An experiment run, resulting\
    \ in\none line of the table below, consisted of 512 of these volleys.\n     The\
    \ length of each ICMP message was determined by a random-number\ngenerator uniformly\
    \ distributed between zero and 256.  Lengths less than 40\nwere rounded up to\
    \ 40, which is the minimum datagram size for an ICMP message\ncontaining timestamps\
    \ and just happens to also be the minimum TCP segment\nsize.  The maximum length\
    \ was chosen to avoid complications due to\nfragmentation and reassembly, since\
    \ ICMP messages are not ordinarily\nfragmented or reassembled by the gateways.\n\
    \     The data collected were first plotted as a scatter diagram on a color\n\
    bit-map display.  For all paths involving the ARPANET, this immediately\nrevealed\
    \ two distinct characteristics, one for short (single-packet) messages\nlonger\
    \ than this.  Linear regression lines were then fitted to each\ncharacteristic\
    \ with the results shown in the following table.  (Only one\ncharacteristic was\
    \ assumed for ARPANET-exclusive paths.) The table shows for\neach host the delays,\
    \ in milliseconds, for each type of message along with a\nrate computed on the\
    \ basis of these delays.  The \"Host ID\" column designates\nthe host at the remote\
    \ end of the path, with a letter suffix used when\nnecessary to identify a particular\
    \ run.\nHost    Single-packet   Rate    Multi-packet    Rate    Comments\nID \
    \     40      125     (bps)   125     256     (bps)\n---------------------------------------------------------------------------\n\
    DCN1 to nearby local-net hosts (calibration)\nDCN5    9                      \
    \         13      366422  DMA 1822\nDCN8    14                              20\
    \      268017  Ethernet\nIMP17   22                              60      45228\
    \   56K 1822/ECU\nFORD1   93                              274     9540    9600\
    \ DDCMP base\nUMD1    102                             473     4663    4800 synch\n\
    DCN6    188                             550     4782    4800 DDCMP\nFACC    243\
    \                             770     3282    9600/4800 DDCMP\nFOE     608   \
    \                          1917    1320    9600/14.4K stat mux\nDCN1 to ARPANET\
    \ hosts and local nets\nMILARP  61      105     15358   133     171     27769\
    \   MILNET gateway\nISID-L  166     263     6989    403     472     15029   low-traffic\
    \ period\nSCORE   184     318     5088    541     608     15745   low-traffic\
    \ period\nRVAX    231     398     4061    651     740     11781   Purdue local\
    \ net\nAJAX    322     578     2664    944     1081    7681    MIT local net\n\
    ISID-H  333     520     3643    715     889     6029    high-traffic period\n\
    BERK    336     967     1078    1188    1403    4879    UC Berkeley\nWASH    498\
    \     776     2441    1256    1348    11379   U Washington\nDCN1 to MILNET/MINET\
    \ hosts and local nets\nISIA-L  460     563     6633    1049    1140    11489\
    \   low-traffic period\nISIA-H  564     841     2447    1275    1635    2910 \
    \   high-traffic period\nBRL     560     973     1645    1605    1825    4768\
    \    BRL local net\nLON     585     835     2724    1775    1998    4696    MINET\
    \ host (London)\nHAWAII  679     980     2257    1817    1931    9238    a long\
    \ way off\nOFFICE3 762     1249    1396    2283    2414    8004    heavily loaded\
    \ host\nKOREA   897     1294    1712    2717    2770    19652   a long, long way\
    \ off\nDCN1 to TELENET hosts via ARPANET\nRICE    1456    2358    754     3086\
    \    3543    2297    via VAN gateway\nDCN1 to SATNET hosts and local nets via\
    \ ARPANET\nUCL     1089    1240    4514    1426    1548    8558    UCL zoo \n\
    NTA-L   1132    1417    2382    1524    1838    3339    low-traffic period\nNTA-H\
    \   1247    1504    2640    1681    1811    8078    high-traffic period\nNTA to\
    \ SATNET hosts\nTANUM   107                             368     6625    9600 bps\
    \ Tanum line\nETAM    964                             1274    5576    Etam channel\
    \ echo\nGOONY   972                             1256    6082    Goonhilly channel\
    \ echo\n2.2 Analysis of Results\n     The data clearly show a strong correlation\
    \ between delay and length, with\nthe longest packets showing delays two to three\
    \ times the shortest.  On paths\nvia ARPANET clones the delay characteristic shows\
    \ a stonger correlation with\nlength for single-packet messages than for multi-packet\
    \ messages, which is\nconsistent with a design which favors low delays for short\
    \ messages and high\nthroughputs for longer ones.\n     Most of the runs were\
    \ made during off-peak hours.  In the few cases where\nruns were made for a particular\
    \ host during both on-peak and off-peak hours,\ncomparison shows a greater dependency\
    \ on packet length than on traffic shift.\n     TCP implementors should be advised\
    \ that some dependency on packet length\nmay have to be built into the retransmission-timeout\
    \ estimation algorithm to\ninsure good performance over lossy nets like SATNET.\
    \  They should also be\nadvised that some Internet paths may require stupendous\
    \ timeout intervals\nranging to many seconds for the net alone, not to mention\
    \ additional delays on\nhost-system queues.\n     I call to your attention the\
    \ fact that the delays (at least for the\nlarger packets) from ARPANET hosts (e.g.\
    \  DCN1) to MILNET hosts (e.g.  ISIA)\nare in the same ballpark as the delays\
    \ to SATNET hosts (e.g.  UCL)!  I have\nalso observed that the packet-loss rates\
    \ on the MILNET path are at present not\nneglible (18 in 512 for ISIA-2).  Presumably,\
    \ the loss is in the gateways;\nhowever, there may well be a host or two out there\
    \ swamping the gateways with\nretransmitted data and which have a funny idea of\
    \ the \"normal\" timeout\ninterval.  The recent discovery of a bug in the TOPS-20\
    \ TCP implementation,\nwhere spurious ACKs were generated at an alarming rate,\
    \ would seem to confirm\nthat suspicion.\n3.  Retransmission-Timeout Algorithm\n\
    \     One of the basic features of TCP which allow it to be used on paths\nspanning\
    \ many nets of widely varying delay and packet-loss characteristics is\nthe retranansmission-timeout\
    \ algorithm, sometimes known as the \"RSRE\nAlgorithm\" for the original designers.\
    \  The algorithm operates by recording\nthe time and initial sequence number when\
    \ a segment is transmitted, then\ncomputing the elapsed time for that sequence\
    \ number to be acknowledged.  There\nare various degrees of sophistication in\
    \ the implementation of the algorithm,\nranging from allowing only one such computation\
    \ to be in progress at a time to\nallowing one for each segment outstanding at\
    \ a time on the connection.\n     The retransmission-timeout algorithm is basically\
    \ an estimation process.\nIt maintains an extimate of the current roundtrip delay\
    \ time and updates it as\nnew delay samples are computed.  The algorithm smooths\
    \ these samples and then\nestablishes a timeout, which if exceeded causes a retransmission.\
    \  The\nselection of the parameters of this algorithm are vitally important in\
    \ order\nto provide effective data transmission and avoid abuse of the Internet\
    \ system\nsuggested in the specification and used in some implementations, especially\
    \ in\ncases involving long-delay paths involving lossy nets.  The experiment was\n\
    designed to simulate the operation of the algorithm using data collected from\n\
    real paths involving some pretty leaky Internet plumbing.\n3.1.  Experiment Design\n\
    \     The experiment data base was constructed of well over a hundred runs\nusing\
    \ ICMP Echo/Reply messages bounced off hosts scattered all over the world.\nMost\
    \ runs, including all those summarized here, consisted of 512 echo/reply\ncycles\
    \ lasting from several seconds to twenty minutes or so.  Other runs\ndesigned\
    \ to detect network glitches lasted several hours.  Some runs used\npackets of\
    \ constant length, while others used different lengths distributed\nfrom 40 to\
    \ 256 octets.  The maximum length was chosen to avoid complications\nfragmented\
    \ or reassembled by the gateways.\n     The object of the experiment was to simulate\
    \ the packet delay\ndistribution seen by TCP over the paths measured.  Only the\
    \ network delay is\nof interest here, not the queueing delays within the hosts\
    \ themselves, which\ncan be considerable.  Also, only a single packet was allowed\
    \ in flight, so\nthat stress on the network itself was minimal.  Some tests were\
    \ conducted\nduring busy periods of network activity, while others were conducted\
    \ during\nquiet hours.\n     The 512 data points collected during each run were\
    \ processed by a program\nwhich plotted on a color bit-map display each data point\
    \ (x,y), where x\nrepresents the time since initiation of the experiment the and\
    \ y the measured\ndelay, normalized to the one-way delay.  Then, the simulated\n\
    retransmission-timeout algorithm was run on these data and its computed\ntimeout\
    \ plotted in the same way.  The display immediately reveals how the\nalgorithm\
    \ behaves in the face of varying traffic loads, network glitches, lost\npackets\
    \ and superfluous retransmissions.\n     Each experiment run also produced summary\
    \ statistics, which are\nsummarized in the table below.  Each line includes the\
    \ Host ID, which\nidentifies the run.  The suffix -1 indicates 40-octet packets,\
    \ -2 indicates\n256-octet packets and no suffix indicates uniformly distributed\
    \ lengths\nbetween 40 and 256.  The Lost Packets columns refer to instances when\
    \ no ICMP\nReply message was received for thirty seconds after transmission of\
    \ the ICMP\nEcho message, indicating probable loss of one or both messages.  The\
    \ RTX\nPackets columns refer to instances when the computed timeout is less than\
    \ the\nmeasured delay, which would result in a superfluous retransmission.  For\
    \ each\nof these two types of packets the  column indicates the number of instances\n\
    and the Time column indicates the total accumulated time required for the\nrecovery\
    \ action.\n     For reference purposes, the Mean column indicates the computed\
    \ mean delay\nof the echo/reply cycles, excluding those cycles involving packet\
    \ loss, while\nthe CoV column indicates the coefficient of variation.  Finally,\
    \ the Eff\ncolumn indicates the efficiency, computed as the ratio of the total\
    \ time\naccumulated while sending good data to this time plus the lost-packet\
    \ and\nrtx-packet time.\n     Complete sets of runs were made for each of the\
    \ hosts in the table below\nfor each of several selections of algorithm parameters.\
    \  The table itself\nreflects values, selected as described later, believed to\
    \ be a good compromise\nfor use on existing paths in the Internet system.\nHost\
    \    Total   Lost Packets    RTX Packets     Mean    CoV     Eff\nID      Time\
    \            Time            Time    \n-------------------------------------------------------------------\n\
    DCN1 to nearby local-net hosts (calibration)\nDCN5    5       0       0      \
    \ 0       0       11      .15     1\nDCN8    8       0       0       0       0\
    \       16      .13     1\nIMP17   19      0       0       0       0       38\
    \      .33     1\nFORD1   86      0       0       1       .2      167     .33\
    \     .99\nUMD1    135     0       0       2       .5      263     .45     .99\n\
    DCN6    177     0       0       0       0       347     .34     1\nFACC    368\
    \     196     222.1   6       9.2     267     1.1     .37\nFOE     670     3 \
    \      7.5     21      73.3    1150    .69     .87\nFOE-1   374     0       0\
    \       26      61.9    610     .75     .83\nFOE-2   1016    3       16.7    10\
    \      47.2    1859    .41     .93\nDCN1 to ARPANET hosts and local nets\nMILARP\
    \  59      0       0       2       .5      115     .39     .99\nISID    163  \
    \   0       0       1       1.8     316     .47     .98\nISID-1  84      0   \
    \    0       2       1       163     .18     .98\nISID-2  281     0       0  \
    \     3       17      516     .91     .93\nISID *  329     0       0       5 \
    \      12.9    619     .81     .96\nSCORE   208     0       0       1       .8\
    \      405     .46     .99\nRVAX    256     1       1.3     0       0       499\
    \     .42     .99\nAJAX    365     0       0       0       0       713     .44\
    \     1\nWASH    494     0       0       2       2.8     960     .39     .99\n\
    WASH-1  271     0       0       5       8       514     .34     .97\nWASH-2  749\
    \     1       9.8     2       17.5    1411    .4      .96\nBERK    528     20\
    \      50.1    4       35      865     1.13    .83\nDCN1 to MILNET/MINET hosts\
    \ and local nets\nISIA    436     4       7.4     2       15.7    807     .68\
    \     .94\nISIA-1  197     0       0       0       0       385     .27     1\n\
    ISIA-2  615     0       0       2       15      1172    .36     .97\nISIA *  595\
    \     18      54.1    6       33.3    992     .77     .85\nBRL     644     1 \
    \      3       1       1.9     1249    .43     .99\nBRL-1   318     0       0\
    \       4       13.6    596     .68     .95\nBRL-2   962     2       8.4     0\
    \       0       1864    .12     .99\nLON     677     0       0       3       11.7\
    \    1300    .51     .98\nLON-1   302     0       0       0       0       589\
    \     .06     1\nLON-2   1047    0       0       0       0       2044    .03 \
    \    1\nHAWAII  709     4       12.9    3       18.5    1325    .55     .95\n\
    OFFICE3 856     3       12.9    3       10.3    1627    .54     .97\nOFF3-1  432\
    \     2       4.2     2       6.9     823     .31     .97\nOFF3-2  1277    7 \
    \      39      3       41.5    2336    .44     .93\nKOREA   1048    3       14.5\
    \    2       18.7    1982    .48     .96\nKOREA-1 506     4       8.6     1  \
    \     2.2     967     .18     .97\nKOREA-2 1493    6       35.5    2       19.3\
    \    2810    .19     .96\nDCN1 to TELENET hosts via ARPANET\nRICE-1  368     1\
    \       .1      3       2.3     715     .11     .99\nRICE-2  1002    1       4.4\
    \     1       9.5     1930    .19     .98\nDCN1 to SATNET hosts and local nets\
    \ via ARPANET\nUCL     689     9       26.8    0       0       1294    .21   \
    \  .96\nUCL-1   623     39      92.8    2       5.3     1025    .32     .84\n\
    UCL-2   818     4       13.5    0       0       1571    .15     .98\nNTA     779\
    \     12      38.7    1       3.7     1438    .24     .94\nNTA-1   616     24\
    \      56.6    2       5.3     1083    .25     .89\nNTA-2   971     19      71.1\
    \    0       0       1757    .2      .92\nNTA to SATNET hosts and local nets\n\
    TANUM   110     3       1.6     0       0       213     .41     .98\nGOONY   587\
    \     19      44.2    1       2.9     1056    .23     .91\nETAM    608     32\
    \      76.3    1       3.1     1032    .29     .86\nUCL     612     5       12.6\
    \    2       8.5     1154    .24     .96\nNote:  * indicates randomly distributed\
    \ packets during periods of high ARPANET\nactivity.  The same entry without the\
    \ * indicates randomly distributed packets\nduring periods of low ARPANET activity.\n\
    3.2 Discussion of Results\n     It is immediately obvious from visual inspection\
    \ of the bit-map display\nthat the delay distribution is more-or-less Poissonly\
    \ distributed about a\nrelatively narrow range with important exceptions.  The\
    \ exceptions are\ncharacterized by occasional spasms where one or more packets\
    \ can be delayed\nmany times the typical value.  Such glitches have been commonly\
    \ noted before\non paths involving ARPANET and SATNET, but the true impact of\
    \ their occurance\non the timeout algorithm is much greater than I expected. \
    \ What commonly\nhappens is that the algorithm, when confronted with a short burst\
    \ of\nlong-delay packets after a relatively long interval of well-mannered behavior,\n\
    takes much too long to adapt to the spasm, thus inviting many superfluous\nretransmissions\
    \ and leading to congestion.\n     The incidence of long-delay bursts, or glitches,\
    \ varied widely during the\nexperiments.  Some of them were glitch-free, but most\
    \ had at least one glitch\nin 512 echo/reply volleys.  Glitches did not seem to\
    \ correlate well with\nincreases in baseline delay, which occurs as the result\
    \ of traffic surges, nor\ndid they correlate well with instances of packet loss.\
    \  I did not notice any\nparticular periodicity, such as might be expected with\
    \ regular pinging, for\nexample;  however, I did not process the data specially\
    \ for that.\n     There was no correction for packet length used in any of these\n\
    experiments, in spite of the results of the first set of experiments described\n\
    previously.  This may be done in a future set of experiments.  The algorithm\n\
    does cope well in the case of constant-length packets and in the case of\nrandomly\
    \ distributed packet lengths between 40 and 256 octets, as indicated in\nthe table.\
    \  Future experiments may involve bursts of short packets followed by\nbursts\
    \ of longer ones, so that the speed of adaptation of the algorithm can be\ndirectly\
    \ deterimend.\n     One particularily interesting experiment involved the FOE\
    \ host\n(FORD-FOE), which is located in London and reached via a 14.4-Kbps undersea\n\
    cable and statistical multiplexor.  The multiplexor introduces a moderate mean\n\
    delay, but with an extremely large delay dispersion.  The specified\nretransmission-timeout\
    \ algorithm had a hard time with this circuit, as might\nbe expected;  however,\
    \ with the improvments described below, TCP performance\nwas acceptable.  It is\
    \ unlikely that many instances of such ornery circuits\nwill occur in the Internet\
    \ system, but it is comforting to know that the\nalgorithm can deal effectively\
    \ with them.\n3.3.  Improvments to the Algorithm\n     The specified retransmission-timeout\
    \ algorithm, really a first-order\nlinear recursive filter, is characterized by\
    \ two parameters, a weighting\nfactor F and a threshold factor G.  For each measured\
    \ delay sample R the delay\nestimator E is updated:\n                        \
    \    \tE = F*E + (1 - F)*R .\nThen, if an interval equal to G*E expires after\
    \ transmitting a packet, the\npacket is retransmitted.  The current TCP specification\
    \ suggests values in the\nrange 0.8 to 0.9 for F and 1.5 to 2.0 for G.  These\
    \ values have been believed\nreasonable up to now over ARPANET and SATNET paths.\n\
    \     I found that a simple change to the algorithm made a worthwhile change in\n\
    the efficiency.  The change amounts to using two values of F, one (F1) when R\n\
    < E in the expression above and the other (F2) when R >= E, with F1 > F2.  The\n\
    effect is to make the algorithm more responsive to upward-going trends in\ndelay\
    \ and less respnsive to downward-going trends.  After a number of trials I\nconcluded\
    \ that values of F1 = 15/16 and F2 = 3/4 (with G = 2) gave the best\nall-around\
    \ performance.  The results on some paths (FOE, ISID, ISIA) were\nbetter by some\
    \ ten percent in efficiency, as compared to the values now used\nin typical implementations\
    \ where F = 7/8 and G = 2.  The results on most paths\nwere better by five percent,\
    \ while on a couple (FACC, UCL) the results were\nworse by a few percent.\n  \
    \   There was no clear-cut gain in fiddling with G.  The value G = 2 seemed\n\
    to represent the best overall compromise.  Note that increasing G makes\nsuperfluous\
    \ retransmissions less likely, but increases the total delay when\npackets are\
    \ lost.  Also, note that increasing F2 too much tends to cause\novershoot in the\
    \ case of network glitches and leads to the same result.  The\ntable above was\
    \ constructed using F1 = 15/16, F2 = 3/4 and G = 2.\n     Readers familiar with\
    \ signal-detection theory will recognize my\nsuggestion as analogous to an ordinary\
    \ peak-detector circuit.  F1 represents\nthe discharge time-constant, while F2\
    \ represents the charge time-constant.  G\nrepresents a \"squelch\" threshold,\
    \ as used in voice-operated switches, for\nexample.  Some wag may be even go on\
    \ to suggest a network glitch should be\ncalled a netspurt.\nAppendix.  Index\
    \ of Test Hosts\nName    Address         NIC Host Name\n-------------------------------------\n\
    DCN1 to nearby local-net hosts (calibration)\nDCN5    128.4.0.5       DCN5\nDCN8\
    \    128.4.0.8       DCN8\nIMP17   10.3.0.17       DCN-GATEWAY\nFORD1   128.5.0.1\
    \       FORD1\nUMD1    128.8.0.1       UMD1\nDCN6    128.4.0.6       DCN6\nFACC\
    \    128.5.32.1      FORD-WDL1\nFOE     128.5.0.15      FORD-FOE\nDCN1 to ARPANET\
    \ hosts and local nets\nMILARP  10.2.0.28       ARPA-MILNET-GW\nISID    10.0.0.27\
    \       USC-ISID\nSCORE   10.3.0.11       SU-SCORE\nRVAX    128.10.0.2      PURDUE-MORDRED\n\
    AJAX    18.10.0.64      MIT-AJAX\nWASH    10.0.0.91       WASHINGTON\nBERK   \
    \ 10.2.0.78       UCB-VAX\nDCN1 to MILNET/MINET hosts and local nets\nISIA   \
    \ 26.3.0.103      USC-ISIA\nBRL     192.5.21.6      BRL-VGR\nLON     24.0.0.7\
    \        MINET-LON-EM\nHAWAII  26.1.0.36       HAWAII-EMH\nOFFICE3 26.2.0.43 \
    \      OFFICE-3\nKOREA   26.0.0.117      KOREA-EMH\nDCN1 to TELENET hosts via\
    \ ARPANET\nRICE    14.0.0.12       RICE\nDCN1 to SATNET hosts and local nets via\
    \ ARPANET\nUCL     128.16.9.0      UCL-SAM\nNTA     128.39.0.2      NTARE1\nNTA\
    \ to SATNET hosts and local nets\nTANUM   4.0.0.64        TANUM-ECHO\nGOONY  \
    \ 4.0.0.63        GOONHILLY-ECHO\nETAM    4.0.0.62        ETAM-ECHO\n"
