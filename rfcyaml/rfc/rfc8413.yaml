- title: __initial_text__
  contents:
  - '                Framework for Scheduled Use of Resources

    '
- title: Abstract
  contents:
  - "Abstract\n   Time-Scheduled (TS) reservation of Traffic Engineering (TE) resources\n\
    \   can be used to provide resource booking for TE Label Switched Paths\n   so\
    \ as to better guarantee services for customers and to improve the\n   efficiency\
    \ of network resource usage at any moment in time, including\n   network usage\
    \ that is planned for the future.  This document provides\n   a framework that\
    \ describes and discusses the architecture for\n   supporting scheduled reservation\
    \ of TE resources.  This document does\n   not describe specific protocols or\
    \ protocol extensions needed to\n   realize this service.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are candidates for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   https://www.rfc-editor.org/info/rfc8413.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2018 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (https://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n   2.  Problem Statement . . . . . . . . . . . . . . . . . .\
    \ . . . .   4\n     2.1.  Provisioning TE-LSPs and TE Resources . . . . . . .\
    \ . . .   4\n     2.2.  Selecting the Path of an LSP  . . . . . . . . . . . .\
    \ . .   4\n     2.3.  Planning Future LSPs  . . . . . . . . . . . . . . . . .\
    \ .   5\n     2.4.  Looking at Future Demands on TE Resources . . . . . . . .\
    \   6\n       2.4.1.  Interaction between Time-Scheduled and Ad Hoc\n        \
    \       Reservations  . . . . . . . . . . . . . . . . . . . .   6\n     2.5. \
    \ Requisite State Information . . . . . . . . . . . . . . .   7\n   3.  Architectural\
    \ Concepts  . . . . . . . . . . . . . . . . . . .   8\n     3.1.  Where is Scheduling\
    \ State Held? . . . . . . . . . . . . .   8\n     3.2.  What State is Held? .\
    \ . . . . . . . . . . . . . . . . . .  10\n     3.3.  Enforcement of Operator\
    \ Policy  . . . . . . . . . . . . .  12\n   4.  Architecture Overview . . . .\
    \ . . . . . . . . . . . . . . . .  13\n     4.1.  Service Request . . . . . .\
    \ . . . . . . . . . . . . . . .  13\n       4.1.1.  Reoptimization After TED Updates\
    \  . . . . . . . . . .  14\n     4.2.  Initialization and Recovery . . . . . .\
    \ . . . . . . . . .  15\n     4.3.  Synchronization Between PCEs  . . . . . .\
    \ . . . . . . . .  15\n   5.  Multi-domain Considerations . . . . . . . . . .\
    \ . . . . . . .  16\n   6.  Security Considerations . . . . . . . . . . . . .\
    \ . . . . . .  18\n   7.  IANA Considerations . . . . . . . . . . . . . . . .\
    \ . . . . .  19\n   8.  Informative References  . . . . . . . . . . . . . . .\
    \ . . . .  19\n   Acknowledgements  . . . . . . . . . . . . . . . . . . . . .\
    \ . . .  21\n   Contributors  . . . . . . . . . . . . . . . . . . . . . . . .\
    \ . .  21\n   Authors' Addresses  . . . . . . . . . . . . . . . . . . . . . .\
    \ .  22\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Traffic Engineering Label Switched Paths (TE-LSPs) are connection-\n\
    \   oriented tunnels in packet and non-packet networks [RFC3209]\n   [RFC3945].\
    \  TE-LSPs may reserve network resources for use by the\n   traffic they carry,\
    \ thus providing some guarantees of service\n   delivery and allowing a network\
    \ operator to plan the use of the\n   resources across the whole network.\n  \
    \ In some technologies (such as wavelength switched optical networks)\n   the\
    \ resource is synonymous with the label that is switched on the\n   path of the\
    \ LSP so that it is not possible to establish an LSP that\n   can carry traffic\
    \ without assigning a physical resource to the LSP.\n   In other technologies\
    \ (such as packet switched networks), the\n   resources assigned to an LSP are\
    \ a measure of the capacity of a link\n   that is dedicated for use by the traffic\
    \ on the LSP.\n   In all cases, network planning consists of selecting paths for\
    \ LSPs\n   through the network so that there will be no contention for\n   resources.\
    \  LSP establishment is the act of setting up an LSP and\n   reserving resources\
    \ within the network.  Network optimization or\n   reoptimization is the process\
    \ of repositioning LSPs in the network to\n   make the unreserved network resources\
    \ more useful for potential\n   future LSPs while ensuring that the established\
    \ LSPs continue to\n   fulfill their objectives.\n   It is often the case that\
    \ it is known that an LSP will be needed at\n   some specific time in the future.\
    \  While a path for that LSP could be\n   computed using knowledge of the currently\
    \ established LSPs and the\n   currently available resources, this does not give\
    \ any degree of\n   certainty that the necessary resources will be available when\
    \ it is\n   time to set up the new LSP.  Yet, setting up the LSP ahead of the\n\
    \   time when it is needed (which would guarantee the availability of the\n  \
    \ resources) is wasteful since the network resources could be used for\n   some\
    \ other purpose in the meantime.\n   Similarly, it may be known that an LSP will\
    \ no longer be needed after\n   some future time and that it will be torn down,\
    \ which will release\n   the network resources that were assigned to it.  This\
    \ information can\n   be helpful in planning how a future LSP is placed in the\
    \ network.\n   Time-Scheduled (TS) reservation of TE resources can be used to\n\
    \   provide resource booking for TE-LSPs so as to better guarantee\n   services\
    \ for customers and to improve the efficiency of network\n   resource usage into\
    \ the future.  This document provides a framework\n   that describes the problem\
    \ and discusses the architecture for the\n   scheduled reservation of TE resources.\
    \  This document does not\n   describe specific protocols or protocol extensions\
    \ needed to realize\n   this service.\n"
- title: 2.  Problem Statement
  contents:
  - '2.  Problem Statement

    '
- title: 2.1.  Provisioning TE-LSPs and TE Resources
  contents:
  - "2.1.  Provisioning TE-LSPs and TE Resources\n   TE-LSPs in existing networks\
    \ are provisioned using a variety of\n   techniques.  They may be set up using\
    \ RSVP-TE as a signaling protocol\n   [RFC3209] [RFC3473].  Alternatively, they\
    \ could be established by\n   direct control of network elements such as in the\
    \ Software-Defined\n   Networking (SDN) paradigm.  They could also be provisioned\
    \ using the\n   PCE Communication Protocol (PCEP) [RFC5440] as a control protocol\
    \ to\n   communicate with the network elements.\n   TE resources are reserved\
    \ at the point of use.  That is, the\n   resources (wavelengths, timeslots, bandwidth,\
    \ etc.) are reserved for\n   use on a specific link and are tracked by the Label\
    \ Switching Routers\n   (LSRs) at the end points of the link.  Those LSRs learn\
    \ which\n   resources to reserve during the LSP setup process.\n   The use of\
    \ TE resources can be varied by changing the parameters of\n   the LSP that uses\
    \ them, and the resources can be released by tearing\n   down the LSP.\n   Resources\
    \ that have been reserved in the network for use by one LSP\n   may be preempted\
    \ for use by another LSP.  If RSVP-TE signaling is in\n   use, a holding priority\
    \ and a preemption priority are used to\n   determine which LSPs may preempt the\
    \ resources that are in use for\n   which other LSPs.  If direct (central) control\
    \ is in use, the\n   controller is able to make preemption decisions.  In either\
    \ case,\n   operator policy forms a key part of preemption since there is a trade\n\
    \   between disrupting existing LSPs and enabling new LSPs.\n"
- title: 2.2.  Selecting the Path of an LSP
  contents:
  - "2.2.  Selecting the Path of an LSP\n   Although TE-LSPs can determine their paths\
    \ hop by hop using the\n   shortest path toward the destination to route the signaling\
    \ protocol\n   messages [RFC3209], in practice this option is not applied because\
    \ it\n   does not look far enough ahead into the network to verify that the\n\
    \   desired resources are available.  Instead, the full length of the\n   path\
    \ of an LSP is usually computed ahead of time either by the head-\n   end LSR\
    \ of a signaled LSP or by Path Computation Element (PCE)\n   functionality that\
    \ is in a dedicated server or built into network\n   management software [RFC4655].\n\
    \   Such full-path computation is applied in order that an end-to-end\n   view\
    \ of the available resources in the network can be used to\n   determine the best\
    \ likelihood of establishing a viable LSP that meets\n   the service requirements.\
    \  Even in this situation, however, it is\n   possible that two LSPs being set\
    \ up at the same time will compete for\n   scarce network resources, which means\
    \ that one or both of them will\n   fail to be established.  This situation is\
    \ avoided by using a\n   centralized PCE that is aware of the LSP setup requests\
    \ that are in\n   progress.\n   Path selection may make allowance for preemption\
    \ as described in\n   Section 2.1.  That is, when selecting a path, the decision\
    \ may be\n   made to choose a path that will result in the preemption of an\n\
    \   existing LSP.  The trade-off between selecting a less optimal path,\n   failing\
    \ to select any path at all, and preempting an existing LSP\n   must be subject\
    \ to operator policy.\n   Path computation is subject to \"objective functions\"\
    \ that define what\n   criteria are to be met when the LSP is placed [RFC4655].\
    \  These can\n   be criteria that apply to the LSP itself (such as the shortest\
    \ path\n   to the destination) or to the network state after the LSP is set up\n\
    \   (such as the maximized residual link bandwidth).  The objective\n   functions\
    \ may be requested by the application requesting the LSP and\n   may be filtered\
    \ and enhanced by the computation engine according to\n   operator policy.\n"
- title: 2.3.  Planning Future LSPs
  contents:
  - "2.3.  Planning Future LSPs\n   LSPs may be established \"on demand\" when the\
    \ requester determines\n   that a new LSP is needed.  In this case, the path of\
    \ the LSP is\n   computed as described in Section 2.2.\n   However, in many situations,\
    \ the requester knows in advance that an\n   LSP will be needed at a particular\
    \ time in the future.  For example,\n   the requester may be aware of a large\
    \ traffic flow that will start at\n   a well-known time, perhaps for a database\
    \ synchronization or for the\n   exchange of content between streaming sites.\
    \  Furthermore, the\n   requester may also know for how long the LSP is required\
    \ before it\n   can be torn down.\n   The set of requests for future LSPs could\
    \ be collected and held in a\n   central database (such as at a Network Management\
    \ System (NMS)): when\n   the time comes for each LSP to be set up, the NMS can\
    \ ask the PCE to\n   compute a path and can then request the LSP to be provisioned.\
    \  This\n   approach has a number of drawbacks because it is not possible to\n\
    \   determine in advance whether it will be possible to deliver the LSP\n   since\
    \ the resources it needs might be used by other LSPs in the\n   network.  Thus,\
    \ at the time the requester asks for the future LSP,\n   the NMS can only make\
    \ a best-effort guarantee that the LSP will be\n   set up at the desired time.\n\
    \   A better solution, therefore, is for the requests for future LSPs to\n   be\
    \ serviced at once.  The paths of the LSPs can be computed ahead of\n   time and\
    \ converted into reservations of network resources during\n   specific windows\
    \ in the future.  That is, while the path of the LSP\n   is computed and the network\
    \ resources are reserved, the LSP is not\n   established in the network until\
    \ the time for which it is scheduled.\n   There is a need to take into account\
    \ items that need to be subject to\n   operator policy, such as 1) the amount\
    \ of capacity available for\n   scheduling future reservations, 2) the operator\
    \ preference for the\n   measures that are used with respect to the use of scheduled\
    \ resources\n   during rapid changes in traffic demand events, or 3) a complex\n\
    \   (multiple nodes/links) failure event so as to protect against network\n  \
    \ destabilization.  Operator policy is discussed further in\n   Section 3.3.\n"
- title: 2.4.  Looking at Future Demands on TE Resources
  contents:
  - "2.4.  Looking at Future Demands on TE Resources\n   While path computation, as\
    \ described in Section 2.2, takes account of\n   the currently available network\
    \ resources and can act to place LSPs\n   in the network so that there is the\
    \ best possibility of future LSPs\n   being accommodated, it cannot handle all\
    \ eventualities.  It is simple\n   to construct scenarios where LSPs that are\
    \ placed one at a time lead\n   to future LSPs being blocked, but where foreknowledge\
    \ of all of the\n   LSPs would have made it possible for them all to be set up.\n\
    \   If, therefore, we were able to know in advance what LSPs were going\n   to\
    \ be requested, we could plan for them and ensure resources were\n   available.\
    \  Furthermore, such an approach enables a commitment to be\n   made to a service\
    \ user that an LSP will be set up and available at a\n   specific time.\n   A\
    \ reservation service can be achieved by tracking the current use of\n   network\
    \ resources and also having a future view of the resource\n   usage.  We call\
    \ this Time-Scheduled TE (TS-TE) resource reservation.\n"
- title: 2.4.1.  Interaction between Time-Scheduled and Ad Hoc Reservations
  contents:
  - "2.4.1.  Interaction between Time-Scheduled and Ad Hoc Reservations\n   There\
    \ will, of course, be a mixture of resource uses in a network.\n   For example,\
    \ normal unplanned LSPs may be requested alongside TS-TE\n   LSPs.  When an unplanned\
    \ LSP is requested, no prior accommodation can\n   be made to arrange resource\
    \ availability, so the LSP can be placed no\n   better than would be the case\
    \ without TS-TE.  However, the new LSP\n   can be placed considering the future\
    \ demands of TS-TE LSPs that have\n   already been requested.  Of course, the\
    \ unplanned LSP has no known\n   end time and so any network planning must assume\
    \ that it will consume\n   resources forever.\n"
- title: 2.5.  Requisite State Information
  contents:
  - "2.5.  Requisite State Information\n   In order to achieve the TS-TE resource\
    \ reservation, the use of\n   resources on the path needs to be scheduled.  The\
    \ scheduling state is\n   used to indicate when resources are reserved and when\
    \ they are\n   available for use.\n   A simple information model for one piece\
    \ of the scheduling state is\n   as follows:\n      {\n        link id;\n    \
    \    resource id or reserved capacity;\n        reservation start time;\n    \
    \    reservation end time\n      }\n   The resource that is scheduled could be\
    \ link capacity, physical\n   resources on a link, buffers on an interface, etc.,\
    \ and could include\n   advanced considerations such as CPU utilization and the\
    \ availability\n   of memory at nodes within the network.  The resource-related\n\
    \   information might also include the maximal unreserved bandwidth of\n   the\
    \ link over a time interval.  That is, the intention is to book\n   (reserve)\
    \ a percentage of the residual (unreserved) bandwidth of the\n   link.  This could\
    \ be used, for example, to reserve bandwidth for a\n   particular class of traffic\
    \ (such as IP) that doesn't have a\n   provisioned LSP.\n   For any one resource,\
    \ there could be multiple pieces of the\n   scheduling state, and for any one\
    \ link, the timing windows might\n   overlap.\n   There are multiple ways to realize\
    \ this information model and\n   different ways to store the data.  The resource\
    \ state could be\n   expressed as a start time and an end time (as shown above),\
    \ or it\n   could be expressed as a start time and a duration.  Multiple\n   reservation\
    \ periods, possibly of different lengths, may need to be\n   recorded for each\
    \ resource.  Furthermore, the current state of\n   network reservation could be\
    \ kept separate from the scheduled usage,\n   or everything could be merged into\
    \ a single TS database.\n   An application may make a reservation request for\
    \ immediate resource\n   usage or to book resources for future use so as to maximize\
    \ the\n   chance of services being delivered and to avoid contention for\n   resources\
    \ in the future.  A single reservation request may book\n   resources for multiple\
    \ periods and might request a reservation that\n   repeats on a regular cycle.\n\
    \   A computation engine (that is, a PCE) may use the scheduling state\n   information\
    \ to help optimize the use of resources into the future and\n   reduce contention\
    \ or blocking when the resources are actually needed.\n   Note that it is also\
    \ necessary to store the information about future\n   LSPs as distinct from the\
    \ specific resource scheduling.  This\n   information is held to allow the LSPs\
    \ to be instantiated when they\n   are due, and use the paths/resources that have\
    \ been computed for\n   them, and also to provide correlation with the TS-TE resource\n\
    \   reservations so that it is clear why resources were reserved, thus\n   allowing\
    \ preemption and handling the release of reserved resources in\n   the event of\
    \ cancellation of future LSPs.  See Section 3.2 for\n   further discussion of\
    \ the distinction between scheduled resource\n   state and scheduled LSP state.\n\
    \   Network performance factors (such as maximum link utilization and the\n  \
    \ residual capacity of the network), with respect to supporting\n   scheduled\
    \ reservations, need to be supported and are subject to\n   operator policy.\n"
- title: 3.  Architectural Concepts
  contents:
  - "3.  Architectural Concepts\n   This section examines several important architectural\
    \ concepts to\n   understand the design decisions reached in this document to\
    \ achieve\n   TS-TE in a scalable and robust manner.\n"
- title: 3.1.  Where is Scheduling State Held?
  contents:
  - "3.1.  Where is Scheduling State Held?\n   The scheduling state information described\
    \ in Section 2.5 has to be\n   held somewhere.  There are two places where this\
    \ makes sense:\n   o  in the network nodes where the resources exist; or,\n  \
    \ o  in a central scheduling controller where decisions about resource\n     \
    \ allocation are made.\n   The first of these makes policing of resource allocation\
    \ easier.  It\n   means that many points in the network can request immediate\
    \ or\n   scheduled LSPs with the associated resource reservation, and that all\n\
    \   such requests can be correlated at the point where the resources are\n   allocated.\
    \  However, this approach has some scaling and technical\n   problems:\n   o \
    \ The most obvious issue is that each network node must retain the\n      full\
    \ time-based state for all of its resources.  In a busy network\n      with a\
    \ high arrival rate of new LSPs and a low hold time for each\n      LSP, this\
    \ could be a lot of state.  Network nodes are normally\n      implemented with\
    \ minimal spare memory.\n   o  In order that path computation can be performed,\
    \ the computing\n      entity normally known as a Path Computation Element (PCE)\n\
    \      [RFC4655] needs access to a database of available links and nodes\n   \
    \   in the network (as well as the TE properties of said links).  This\n     \
    \ database is known as the Traffic Engineering Database (TED) and is\n      usually\
    \ populated from information advertised in the IGP by each\n      of the network\
    \ nodes or exported using BGP Link State (BGP-LS)\n      [RFC7752].  To be able\
    \ to compute a path for a future LSP, the PCE\n      needs to populate the TED\
    \ with all of the future resource\n      availability: if this information is\
    \ held on the network nodes, it\n      must also be advertised in the IGP.  This\
    \ could be a significant\n      scaling issue for the IGP and the network nodes,\
    \ as all of the\n      advertised information is held at every network node and\
    \ must be\n      periodically refreshed by the IGP.\n   o  When a normal node\
    \ restarts, it can recover the resource\n      reservation state from the forwarding\
    \ hardware, from Non-Volatile\n      Random-Access Memory (NVRAM), or from adjacent\
    \ nodes through the\n      signaling protocol [RFC5063].  If the scheduling state\
    \ is held at\n      the network nodes, it must also be recovered after the restart\
    \ of\n      a network node.  This cannot be achieved from the forwarding\n   \
    \   hardware because the reservation will not have been made, could\n      require\
    \ additional expensive NVRAM, or might require that all\n      adjacent nodes\
    \ also have the scheduling state in order to\n      reinstall it on the restarting\
    \ node.  This is potentially complex\n      processing with scaling and cost implications.\n\
    \   Conversely, if the scheduling state is held centrally, it is easily\n   available\
    \ at the point of use.  That is, the PCE can utilize the\n   state to plan future\
    \ LSPs and can update that stored information with\n   the scheduled reservation\
    \ of resources for those future LSPs.  This\n   approach also has several issues:\n\
    \   o  If there are multiple controllers, then they must synchronize\n      their\
    \ stored scheduling state as they each plan future LSPs and\n      they must have\
    \ a mechanism to resolve resource contention.  This\n      is relatively simple\
    \ and is mitigated by the fact that there is\n      ample processing time to replan\
    \ future LSPs in the case of\n      resource contention.\n   o  If other sources\
    \ of immediate LSPs are allowed (for example, other\n      controllers or autonomous\
    \ action by head-end LSRs), then the\n      changes in resource availability caused\
    \ by the setup or tear down\n      of these LSPs must be reflected in the TED\
    \ (by use of the IGP as\n      is already normally done) and may have an impact\
    \ on planned future\n      LSPs.  This impact can be mitigated by replanning future\
    \ LSPs or\n      through LSP preemption.\n   o  If the scheduling state is held\
    \ centrally at a PCE, the state must\n      be held and restored after a system\
    \ restart.  This is relatively\n      easy to achieve on a central server that\
    \ can have access to non-\n      volatile storage.  The PCE could also synchronize\
    \ the scheduling\n      state with other PCEs after restart.  See Section 4.2\
    \ for details.\n   o  Of course, a centralized system must store information about\
    \ all\n      of the resources in the network.  In a busy network with a high\n\
    \      arrival rate of new LSPs and a low hold time for each LSP, this\n     \
    \ could be a lot of state.  This is multiplied by the size of the\n      network\
    \ measured both by the number of links and nodes and by the\n      number of trackable\
    \ resources on each link or at each node.  This\n      challenge may be mitigated\
    \ by the centralized server being\n      dedicated hardware, but there remains\
    \ the problem of collecting\n      the information from the network in a timely\
    \ way when there is\n      potentially a very large amount of information to be\
    \ collected and\n      when the rate of change of that information is high.  This\
    \ latter\n      challenge is only solved if the central server has full control\
    \ of\n      the booking of resources and the establishment of new LSPs so that\n\
    \      the information from the network only serves to confirm what the\n    \
    \  central server expected.\n   Thus, considering these trade-offs, the architectural\
    \ conclusion is\n   that the scheduling state should be held centrally at the\
    \ point of\n   use and not in the network devices.\n"
- title: 3.2.  What State is Held?
  contents:
  - "3.2.  What State is Held?\n   As already described, the PCE needs access to an\
    \ enhanced, time-based\n   TED.  It stores the Traffic Engineering (TE) information,\
    \ such as\n   bandwidth, for every link for a series of time intervals.  There\
    \ are\n   a few ways to store the TE information in the TED.  For example,\n \
    \  suppose that the amount of the unreserved bandwidth at a priority\n   level\
    \ for a link is Bj in a time interval from time Tj to Tk (k =\n   j+1), where\
    \ j = 0, 1, 2, ....\n        Bandwidth\n         ^\n         |               \
    \                     B3\n         |          B1                        ___________\n\
    \         |          __________\n         |B0                                \
    \             B4\n         |__________          B2                         _________\n\
    \         |                    ________________\n         |\n        -+------------------------------------------------------->\
    \ Time\n         |T0        T1        T2              T3         T4\n        \
    \     Figure 1: A Plot of Bandwidth Usage against Time\n   The unreserved bandwidth\
    \ for the link can be represented and stored\n   in the TED as [T0, B0], [T1,\
    \ B1], [T2, B2], [T3, B3], ... as shown in\n   Figure 1.\n   But it must be noted\
    \ that service requests for future LSPs are known\n   in terms of the LSPs whose\
    \ paths are computed and for which resources\n   are scheduled.  For example,\
    \ if the requester of a future LSP decides\n   to cancel the request or to modify\
    \ the request, the PCE must be able\n   to map this to the resources that were\
    \ reserved.  When the LSP (or\n   the request for the LSP with a number of time\
    \ intervals) is canceled,\n   the PCE must release the resources that were reserved\
    \ on each of the\n   links along the path of the LSP in every time interval from\
    \ the TED.\n   If the bandwidth that had been reserved for the LSP on a link was\
    \ B\n   from time T2 to T3 and the unreserved bandwidth on the link was B2\n \
    \  from T2 to T3, then B is added back to the link for the time interval\n   from\
    \ T2 to T3 and the unreserved bandwidth on the link from T2 to T3\n   will be\
    \ seen to be B2 + B.\n   This suggests that the PCE needs an LSP Database (LSP-DB)\
    \ [RFC8231]\n   that contains information not only about LSPs that are active\
    \ in the\n   network but also those that are planned.  For each time interval\
    \ that\n   applies to the LSP, the information for an LSP stored in the LSP-DB\n\
    \   includes: the time interval, the paths computed for the LSP\n   satisfying\
    \ the constraints in the time interval, and the resources\n   (such as bandwidth)\
    \ reserved for the LSP in the time interval.  See\n   also Section 2.3\n   It\
    \ is an implementation choice how the TED and LSP-DB are stored both\n   for dynamic\
    \ use and for recovery after failure or restart, but it may\n   be noted that\
    \ all of the information in the scheduled TED can be\n   recovered from the active\
    \ network state and from the scheduled LSP-\n   DB.\n"
- title: 3.3.  Enforcement of Operator Policy
  contents:
  - "3.3.  Enforcement of Operator Policy\n   Computation requests for LSPs are serviced\
    \ according to operator\n   policy.  For example, a PCE may refuse a computation\
    \ request because\n   the application making the request does not have sufficient\n\
    \   permissions or because servicing the request might take specific\n   resource\
    \ usage over a given threshold.\n   Furthermore, the preemption and holding priorities\
    \ of any particular\n   computation request may be subject to the operator's policies.\
    \  The\n   request could be rejected if it does not conform to the operator's\n\
    \   policies, or (possibly more likely) the priorities could be set/\n   overwritten\
    \ according to the operator's policies.\n   Additionally, the Objective Functions\
    \ (OFs) of computation request\n   (such as maximizing residual bandwidth) are\
    \ also subject to operator\n   policies.  It is highly likely that the choice\
    \ of OFs is not\n   available to an application and is selected by the PCE or\
    \ management\n   system subject to operator policies and knowledge of the application.\n\
    \   None of these statements is new to scheduled resources.  They apply\n   to\
    \ stateless, stateful, passive, and active PCEs, and they continue\n   to apply\
    \ to scheduling of resources.\n   An operator may choose to configure special\
    \ behavior for a PCE that\n   handles resource scheduling.  For example, an operator\
    \ might want\n   only a certain percentage of any resource to be bookable.  And\
    \ an\n   operator might want the preemption of booked resources to be an\n   inverse\
    \ function of how far in the future the resources are needed\n   for the first\
    \ time.\n   It is a general assumption about the architecture described in\n \
    \  Section 4 that a PCE is under the operational control of the operator\n   that\
    \ owns the resources that the PCE manipulates.  Thus, the operator\n   may configure\
    \ any amount of (potentially complex) policy at the PCE.\n   This configuration\
    \ would also include policy points surrounding\n   reoptimization of existing\
    \ and planned LSPs in the event of changes\n   in the current and future (planned)\
    \ resource availability.\n   The granularity of the timing window offered to an\
    \ application will\n   depend on an operator's policy as well as the implementation\
    \ in the\n   PCE and goes to define the operator' service offerings.  Different\n\
    \   granularities and different lengths of prebooking may be offered to\n   different\
    \ applications.\n"
- title: 4.  Architecture Overview
  contents:
  - "4.  Architecture Overview\n   The architectural considerations and conclusions\
    \ described in the\n   previous section lead to the architecture described in\
    \ this section\n   and illustrated in Figure 2.  The interfaces and interactions\
    \ shown\n   in the figure and labeled (a) through (f) are described in\n   Section\
    \ 4.1.\n          -------------------\n         | Service Requester |\n      \
    \    -------------------\n                     ^\n                    a|\n   \
    \                  v\n                  -------   b   --------\n             \
    \    |       |<--->| LSP-DB |\n                 |       |      --------\n    \
    \             |  PCE  |\n                 |       |  c    -----\n            \
    \     |       |<---->| TED |\n                  -------        -----\n       \
    \           ^     ^\n                  |     |\n                 d|     |e\n \
    \                 |     |\n            ------+-----+--------------------\n   \
    \               |     |          Network\n                  |     --------\n \
    \                 |    | Router |\n                  v     --------\n        \
    \        -----          -----\n               | LSR |<------>| LSR |\n       \
    \         -----     f    -----\n      Figure 2: Reference Architecture for Scheduled\
    \ Use of Resources\n"
- title: 4.1.  Service Request
  contents:
  - "4.1.  Service Request\n   As shown in Figure 2, some component in the network\
    \ requests a\n   service.  This may be an application, an NMS, an LSR, or any\n\
    \   component that qualifies as a Path Computation Client (PCC).  We show\n  \
    \ this on the figure as the \"Service Requester\", and it sends a request\n  \
    \ to the PCE for an LSP to be set up at some time (either now or in the\n   future).\
    \  The request, indicated on Figure 2 by the arrow (a),\n   includes all of the\
    \ parameters of the LSP that the requester wishes\n   to supply, such as priority,\
    \ bandwidth, start time, and end time.\n   Note that the requester in this case\
    \ may be the LSR shown in the\n   figure or may be a distinct system.\n   The\
    \ PCE enters the LSP request in its LSP-DB (b) and uses information\n   from its\
    \ TED (c) to compute a path that satisfies the constraints\n   (such as bandwidth)\
    \ for the LSP in the time interval from the start\n   time to the end time.  It\
    \ updates the future resource availability in\n   the TED so that further path\
    \ computations can take account of the\n   scheduled resource usage.  It stores\
    \ the path for the LSP into the\n   LSP-DB (b).\n   When it is time (i.e., at\
    \ the start time) for the LSP to be set up,\n   the PCE sends a PCEP Initiate\
    \ request to the head-end LSR (d), which\n   provides the path to be signaled\
    \ as well as other parameters, such as\n   the bandwidth of the LSP.\n   As the\
    \ LSP is signaled between LSRs (f), the use of resources in the\n   network is\
    \ updated and distributed using the IGP.  This information\n   is shared with\
    \ the PCE either through the IGP or using BGP-LS (e),\n   and the PCE updates\
    \ the information stored in its TED (c).\n   After the LSP is set up, the head-end\
    \ LSR sends a PCEP LSP State\n   Report (PCRpt) message to the PCE (d).  The report\
    \ contains the\n   resources, such as bandwidth usage, for the LSP.  The PCE updates\
    \ the\n   status of the LSP in the LSP-DB according to the report.\n   When an\
    \ LSP is no longer required (either because the Service\n   Requester has canceled\
    \ the request or because the LSP's scheduled\n   lifetime has expired), the PCE\
    \ can remove it.  If the LSP is\n   currently active, the PCE instructs the head-end\
    \ LSR to tear it down\n   (d), and the network resource usage will be updated\
    \ by the IGP and\n   advertised back to the PCE through the IGP or BGP-LS (e).\
    \  Once the\n   LSP is no longer active, the PCE can remove it from the LSP-DB\
    \ (b).\n"
- title: 4.1.1.  Reoptimization After TED Updates
  contents:
  - "4.1.1.  Reoptimization After TED Updates\n   When the TED is updated as indicated\
    \ in Section 4.1, depending on\n   operator policy (so as to minimize network\
    \ perturbations), the PCE\n   may perform reoptimization of the LSPs for which\
    \ it has computed\n   paths.  These LSPs may be already provisioned, in which\
    \ case the PCE\n   issues PCEP Update request messages for the LSPs that should\
    \ be\n   adjusted.  Additionally, the LSPs being reoptimized may be scheduled\n\
    \   LSPs that have not yet been provisioned, in which case reoptimization\n  \
    \ involves updating the store of scheduled LSPs and resources.\n   In all cases,\
    \ the purpose of reoptimization is to take account of the\n   resource usage and\
    \ availability in the network and to compute paths\n   for the current and future\
    \ LSPs that best satisfy the objectives of\n   those LSPs while keeping the network\
    \ as clear as possible to support\n   further LSPs.  Since reoptimization may\
    \ perturb established LSPs, it\n   is subject to operator oversight and policy.\
    \  As the stability of the\n   network will be impacted by frequent changes, the\
    \ extent and impact\n   of any reoptimization needs to be subject to operator\
    \ policy.\n   Additionally, the status of the reserved resources (alarms) can\n\
    \   enhance the computation and planning for future LSPs and may\n   influence\
    \ repair and reoptimization.  Control of recalculations based\n   on failures\
    \ and notifications to the operator is also subject to\n   policy.\n   See Section\
    \ 3.3 for further discussion of operator policy.\n"
- title: 4.2.  Initialization and Recovery
  contents:
  - "4.2.  Initialization and Recovery\n   When a PCE in the architecture shown in\
    \ Figure 2 is initialized, it\n   must learn the state from the network, from\
    \ its stored databases, and\n   potentially from other PCEs in the network.\n\
    \   The first step is to get an accurate view of the topology and\n   resource\
    \ availability in the network.  This would normally involve\n   reading the state\
    \ directly from the network via the IGP or BGP-LS\n   (e), but it might include\
    \ receiving a copy of the TED from another\n   PCE.  Note that a TED stored from\
    \ a previous instantiation of the PCE\n   is unlikely to be valid.\n   Next, the\
    \ PCE must construct a time-based TED to show scheduled\n   resource usage.  How\
    \ it does this is implementation specific, and\n   this document does not dictate\
    \ any particular mechanism: it may\n   recover a time-based TED previously saved\
    \ to non-volatile storage, or\n   it may reconstruct the time-based TED from information\
    \ retrieved from\n   the LSP-DB previously saved to non-volatile storage.  If\
    \ there is\n   more than one PCE active in the network, the recovering PCE will\
    \ need\n   to synchronize the LSP-DB and time-based TED with other PCEs (see\n\
    \   Section 4.3).\n   Note that the stored LSP-DB needs to include the intended\
    \ state and\n   actual state of the LSPs so that when a PCE recovers, it is able\
    \ to\n   determine what actions are necessary.\n"
- title: 4.3.  Synchronization Between PCEs
  contents:
  - "4.3.  Synchronization Between PCEs\n   If there is active in the network more\
    \ than one PCE that supports\n   scheduling, it is important to achieve some consistency\
    \ between the\n   scheduled TED and scheduled LSP-DB held by the PCEs.\n   [RFC7399]\
    \ answers various questions around synchronization between\n   the PCEs.  It should\
    \ be noted that the time-based \"scheduled\"\n   information adds another dimension\
    \ to the issue of synchronization\n   between PCEs.  It should also be noted that\
    \ a deployment may use a\n   primary PCE and then have other PCEs as backup, where\
    \ a backup PCE\n   can take over only in the event of a failure of the primary\
    \ PCE.\n   Alternatively, the PCEs may share the load at all times.  The choice\n\
    \   of the synchronization technique is largely dependent on the\n   deployment\
    \ of PCEs in the network.\n   One option for ensuring that multiple PCEs use the\
    \ same scheduled\n   information is simply to have the PCEs driven from the same\
    \ shared\n   database, but it is likely to be inefficient, and interoperation\n\
    \   between multiple implementations will be harder.\n   Another option is for\
    \ each PCE to be responsible for its own\n   scheduled database and to utilize\
    \ some distributed database\n   synchronization mechanism to have consistent information.\
    \  Depending\n   on the implementation, this could be efficient, but interoperation\n\
    \   between heterogeneous implementations is still hard.\n   A further approach\
    \ is to utilize PCEP messages to synchronize the\n   scheduled state between PCEs.\
    \  This approach would work well if the\n   number of PCEs that support scheduling\
    \ is small, but as the number\n   increases, considerable message exchange needs\
    \ to happen to keep the\n   scheduled databases synchronized.  Future solutions\
    \ could also\n   utilize some synchronization optimization techniques for efficiency.\n\
    \   Another variation would be to request information from other PCEs for\n  \
    \ a particular time slice, but this might have an impact on the\n   optimization\
    \ algorithm.\n"
- title: 5.  Multi-domain Considerations
  contents:
  - "5.  Multi-domain Considerations\n   Multi-domain path computation usually requires\
    \ some form of\n   cooperation between PCEs, each of which has responsibility\
    \ for\n   determining a segment of the end-to-end path in the domain for which\n\
    \   it has computational responsibility.  When computing a scheduled\n   path,\
    \ resources need to be booked in all of the domains that the path\n   will cross\
    \ so that they are available when the LSP is finally\n   signaled.\n   Per-domain\
    \ path computation [RFC5152] is not an appropriate mechanism\n   when a scheduled\
    \ LSP is being computed because the computation\n   requests at downstream PCEs\
    \ are only triggered by signaling.\n   However, a similar mechanism could be used\
    \ where cooperating PCEs\n   exchange Path Computation Request (PCReq) messages\
    \ for a scheduled\n   LSP, as shown in Figure 3.  In this case, the service requester\
    \ asks\n   for a scheduled LSP that will span two domains (a).  PCE1 computes\
    \ a\n   path across Domain 1 and reserves the resources and also asks PCE2 to\n\
    \   compute and reserve in Domain 2 (b).  PCE2 may return a full path or\n   could\
    \ return a path key [RFC5520].  When it is time for LSP setup,\n   PCE1 triggers\
    \ the head-end LSR (c), and the LSP is signaled (d).  If\n   a path key is used,\
    \ the entry LSR in Domain 2 will consult PCE2 for\n   the path expansion (e) before\
    \ completing signaling (f).\n          -------------------\n         | Service\
    \ Requester |\n          -------------------\n             ^\n            a|\n\
    \             v\n          ------         b          ------\n         |      |<---------------->|\
    \      |\n         | PCE1 |                  | PCE2 |\n         |      |     \
    \             |      |\n          ------                    ------\n         \
    \   ^                         ^\n            |                         |\n   \
    \        c|                        e|\n            |                         |\n\
    \        ----+-----------------    ----+-----------------\n       |    |     \
    \   Domain 1 |  |    |        Domain 2 |\n       |    v                 |  | \
    \   v                 |\n       |  -----   d   -----   |  |   -----   f   -----\
    \  |\n       | | LSR |<--->| LSR |<-+--+->| LSR |<--->| LSR | |\n       |  -----\
    \       -----   |  |   -----       -----  |\n        ----------------------  \
    \  ----------------------\n         Figure 3: Per-Domain Path Computation for\
    \ Scheduled LSPs\n   Another mechanism for PCE cooperation in multi-domain LSP\
    \ setup is\n   Backward Recursive PCE-Based Computation (BRPC) [RFC5441].  This\n\
    \   approach relies on the downstream domain to supply a variety of\n   potential\
    \ paths to the upstream domain.  Although BRPC can arrive at\n   a more optimal\
    \ end-to-end path than per-domain path computation, it\n   is not well suited\
    \ to LSP scheduling because the downstream PCE would\n   need to reserve resources\
    \ on all of the potential paths and then\n   release those that the upstream PCE\
    \ announced it did not plan to use.\n   Finally, we should consider hierarchical\
    \ PCE (H-PCE) [RFC6805].  This\n   mode of operation is similar to that shown\
    \ in Figure 3, but a parent\n   PCE is used to coordinate the requests to the\
    \ child PCEs, which then\n   results in better visibility of the end-to-end path\
    \ and better\n   coordination of the resource booking.  The sequenced flow of\
    \ control\n   is shown in Figure 4.\n          -------------------\n         |\
    \ Service Requester |\n          -------------------\n             ^\n       \
    \     a|\n             v\n          --------\n         |        |\n         |\
    \ Parent |\n         |  PCE   |\n         |        |\n          --------\n   \
    \          ^ ^         b\n            b| |_______________________\n          \
    \   |                         |\n             v                         v\n  \
    \        ------                    ------\n         |      |                 \
    \ |      |\n         | PCE1 |                  | PCE2 |\n         |      |   \
    \               |      |\n          ------                    ------\n       \
    \     ^                         ^\n            |                         |\n \
    \          c|                        e|\n            |                       \
    \  |\n        ----+-----------------    ----+-----------------\n       |    |\
    \        Domain 1 |  |    |        Domain 2 |\n       |    v                 |\
    \  |    v                 |\n       |  -----   d   -----   |  |   -----   f  \
    \ -----  |\n       | | LSR |<--->| LSR |<-+--+->| LSR |<--->| LSR | |\n      \
    \ |  -----       -----   |  |   -----       -----  |\n        ----------------------\
    \    ----------------------\n    Figure 4: Hierarchical PCE for Path Computation\
    \ for Scheduled LSPs\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   The protocol implications of scheduled resources\
    \ are unchanged from\n   \"on demand\" LSP computation and setup.  A discussion\
    \ of securing PCEP\n   is found in [RFC5440], and work to extend that security\
    \ is provided\n   in [RFC8253].  Furthermore, the path key mechanism described\
    \ in\n   [RFC5520] can be used to enhance privacy and security.\n   Similarly,\
    \ there is no change to the security implications for the\n   signaling of scheduled\
    \ LSPs.  A discussion of the security of the\n   signaling protocols that would\
    \ be used is found in [RFC5920].\n   However, the use of scheduled LSPs extends\
    \ the attack surface for a\n   PCE-enabled TE system by providing a larger (logically\
    \ infinite)\n   window during which an attack can be initiated or planned.  That\
    \ is,\n   if bogus scheduled LSPs can be requested and entered into the LSP-DB,\n\
    \   then a large number of LSPs could be launched and significant network\n  \
    \ resources could be blocked.  Control of scheduling requests needs to\n   be\
    \ subject to operator policy, and additional authorization needs to\n   be applied\
    \ for access to LSP scheduling.  Diagnostic tools need to be\n   provided to inspect\
    \ the LSP-DB to spot attacks.\n"
- title: 7.  IANA Considerations
  contents:
  - "7.  IANA Considerations\n   This document has no IANA actions.\n"
- title: 8.  Informative References
  contents:
  - "8.  Informative References\n   [AUTOBW]   Yong, L. and Y. Lee, \"ASON/GMPLS Extension\
    \ for Reservation\n              and Time Based Automatic Bandwidth Service\"\
    , Work in\n              Progress, draft-yong-ccamp-ason-gmpls-autobw-service-00,\n\
    \              October 2006.\n   [DRAGON]   National Science Foundation, \"The\
    \ DRAGON Project: Dynamic\n              Resource Allocation via GMPLS Optical\
    \ Networks\", Overview\n              and Status Presentation at ONT3, September\
    \ 2006,\n              <http://www.maxgigapop.net/wp-content/uploads/\n      \
    \        The-DRAGON-Project.pdf>.\n   [FRAMEWORK-TTS]\n              Chen, H.,\
    \ Toy, M., Liu, L., and K. Pithewan, \"Framework\n              for Temporal Tunnel\
    \ Services\", Work In Progress, draft-\n              chen-teas-frmwk-tts-01,\
    \ March 2016.\n   [RFC3209]  Awduche, D., Berger, L., Gan, D., Li, T., Srinivasan,\
    \ V.,\n              and G. Swallow, \"RSVP-TE: Extensions to RSVP for LSP\n \
    \             Tunnels\", RFC 3209, DOI 10.17487/RFC3209, December 2001,\n    \
    \          <https://www.rfc-editor.org/info/rfc3209>.\n   [RFC3473]  Berger, L.,\
    \ Ed., \"Generalized Multi-Protocol Label\n              Switching (GMPLS) Signaling\
    \ Resource ReserVation Protocol-\n              Traffic Engineering (RSVP-TE)\
    \ Extensions\", RFC 3473,\n              DOI 10.17487/RFC3473, January 2003,\n\
    \              <https://www.rfc-editor.org/info/rfc3473>.\n   [RFC3945]  Mannie,\
    \ E., Ed., \"Generalized Multi-Protocol Label\n              Switching (GMPLS)\
    \ Architecture\", RFC 3945,\n              DOI 10.17487/RFC3945, October 2004,\n\
    \              <https://www.rfc-editor.org/info/rfc3945>.\n   [RFC4655]  Farrel,\
    \ A., Vasseur, J., and J. Ash, \"A Path Computation\n              Element (PCE)-Based\
    \ Architecture\", RFC 4655,\n              DOI 10.17487/RFC4655, August 2006,\n\
    \              <https://www.rfc-editor.org/info/rfc4655>.\n   [RFC5063]  Satyanarayana,\
    \ A., Ed. and R. Rahman, Ed., \"Extensions to\n              GMPLS Resource Reservation\
    \ Protocol (RSVP) Graceful\n              Restart\", RFC 5063, DOI 10.17487/RFC5063,\
    \ October 2007,\n              <https://www.rfc-editor.org/info/rfc5063>.\n  \
    \ [RFC5152]  Vasseur, JP., Ed., Ayyangar, A., Ed., and R. Zhang, \"A\n       \
    \       Per-Domain Path Computation Method for Establishing Inter-\n         \
    \     Domain Traffic Engineering (TE) Label Switched Paths\n              (LSPs)\"\
    , RFC 5152, DOI 10.17487/RFC5152, February 2008,\n              <https://www.rfc-editor.org/info/rfc5152>.\n\
    \   [RFC5440]  Vasseur, JP., Ed. and JL. Le Roux, Ed., \"Path Computation\n  \
    \            Element (PCE) Communication Protocol (PCEP)\", RFC 5440,\n      \
    \        DOI 10.17487/RFC5440, March 2009,\n              <https://www.rfc-editor.org/info/rfc5440>.\n\
    \   [RFC5441]  Vasseur, JP., Ed., Zhang, R., Bitar, N., and JL. Le Roux,\n   \
    \           \"A Backward-Recursive PCE-Based Computation (BRPC)\n            \
    \  Procedure to Compute Shortest Constrained Inter-Domain\n              Traffic\
    \ Engineering Label Switched Paths\", RFC 5441,\n              DOI 10.17487/RFC5441,\
    \ April 2009,\n              <https://www.rfc-editor.org/info/rfc5441>.\n   [RFC5520]\
    \  Bradford, R., Ed., Vasseur, JP., and A. Farrel,\n              \"Preserving\
    \ Topology Confidentiality in Inter-Domain Path\n              Computation Using\
    \ a Path-Key-Based Mechanism\", RFC 5520,\n              DOI 10.17487/RFC5520,\
    \ April 2009,\n              <https://www.rfc-editor.org/info/rfc5520>.\n   [RFC5920]\
    \  Fang, L., Ed., \"Security Framework for MPLS and GMPLS\n              Networks\"\
    , RFC 5920, DOI 10.17487/RFC5920, July 2010,\n              <https://www.rfc-editor.org/info/rfc5920>.\n\
    \   [RFC6805]  King, D., Ed. and A. Farrel, Ed., \"The Application of the\n  \
    \            Path Computation Element Architecture to the Determination\n    \
    \          of a Sequence of Domains in MPLS and GMPLS\", RFC 6805,\n         \
    \     DOI 10.17487/RFC6805, November 2012,\n              <https://www.rfc-editor.org/info/rfc6805>.\n\
    \   [RFC7399]  Farrel, A. and D. King, \"Unanswered Questions in the Path\n  \
    \            Computation Element Architecture\", RFC 7399,\n              DOI\
    \ 10.17487/RFC7399, October 2014,\n              <https://www.rfc-editor.org/info/rfc7399>.\n\
    \   [RFC7752]  Gredler, H., Ed., Medved, J., Previdi, S., Farrel, A., and\n  \
    \            S. Ray, \"North-Bound Distribution of Link-State and\n          \
    \    Traffic Engineering (TE) Information Using BGP\", RFC 7752,\n           \
    \   DOI 10.17487/RFC7752, March 2016,\n              <https://www.rfc-editor.org/info/rfc7752>.\n\
    \   [RFC8231]  Crabbe, E., Minei, I., Medved, J., and R. Varga, \"Path\n     \
    \         Computation Element Communication Protocol (PCEP)\n              Extensions\
    \ for Stateful PCE\", RFC 8231,\n              DOI 10.17487/RFC8231, September\
    \ 2017,\n              <https://www.rfc-editor.org/info/rfc8231>.\n   [RFC8253]\
    \  Lopez, D., Gonzalez de Dios, O., Wu, Q., and D. Dhody,\n              \"PCEPS:\
    \ Usage of TLS to Provide a Secure Transport for the\n              Path Computation\
    \ Element Communication Protocol (PCEP)\",\n              RFC 8253, DOI 10.17487/RFC8253,\
    \ October 2017,\n              <https://www.rfc-editor.org/info/rfc8253>.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   This work has benefited from the discussions of resource\
    \ scheduling\n   over the years.  In particular, the DRAGON project [DRAGON] and\n\
    \   [AUTOBW], both of which provide approaches to auto-bandwidth services\n  \
    \ in GMPLS networks.\n   Mehmet Toy, Lei Liu, and Khuzema Pithewan contributed\
    \ to an earlier\n   version of [FRAMEWORK-TTS].  We would like to thank the authors\
    \ of\n   that document on Temporal Tunnel Services for material that assisted\n\
    \   in thinking about this document.\n   Thanks to Michael Scharf and Daniele\
    \ Ceccarelli for useful comments\n   on this work.\n   Jonathan Hardwick provided\
    \ a helpful Routing Directorate review.\n   Deborah Brungard, Mirja Kuehlewind,\
    \ and Benjamin Kaduk suggested many\n   changes during their Area Director reviews.\n"
- title: Contributors
  contents:
  - "Contributors\n   The following person contributed to discussions that led to\
    \ the\n   development of this document:\n   Dhruv Dhody\n   Email: dhruv.dhody@huawei.com\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Yan Zhuang\n   Huawei\n   101 Software Avenue, Yuhua District\n\
    \   Nanjing, Jiangsu  210012\n   China\n   Email: zhuangyan.zhuang@huawei.com\n\
    \   Qin Wu\n   Huawei\n   101 Software Avenue, Yuhua District\n   Nanjing, Jiangsu\
    \  210012\n   China\n   Email: bill.wu@huawei.com\n   Huaimo Chen\n   Huawei\n\
    \   Boston, MA\n   United States of America\n   Email: huaimo.chen@huawei.com\n\
    \   Adrian Farrel\n   Juniper Networks\n   Email: afarrel@juniper.net\n"
