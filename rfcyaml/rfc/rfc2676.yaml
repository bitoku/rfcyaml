- title: __initial_text__
  contents:
  - '               QoS Routing Mechanisms and OSPF Extensions

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo defines an Experimental Protocol for the Internet\n\
    \   community.  It does not specify an Internet standard of any kind.\n   Discussion\
    \ and suggestions for improvement are requested.\n   Distribution of this memo\
    \ is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (1999).  All Rights Reserved.\n"
- title: Abstract
  contents:
  - "Abstract\n   This memo describes extensions to the OSPF [Moy98] protocol to\n\
    \   support QoS routes.  The focus of this document is on the algorithms\n   used\
    \ to compute QoS routes and on the necessary modifications to OSPF\n   to support\
    \ this function, e.g., the information needed, its format,\n   how it is distributed,\
    \ and how it is used by the QoS path selection\n   process.  Aspects related to\
    \ how QoS routes are established and\n   managed are also briefly discussed. \
    \ The goal of this document is to\n   identify a framework and possible approaches\
    \ to allow deployment of\n   QoS routing capabilities with the minimum possible\
    \ impact to the\n   existing routing infrastructure.\n   In addition, experience\
    \ from an implementation of the proposed\n   extensions in the GateD environment\
    \ [Con], along with performance\n   measurements is presented.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction                                       \
    \             3\n       1.1. Overall Framework . . . . . . . . . . . . . . . .\
    \ . . . . 3\n       1.2. Simplifying Assumptions . . . . . . . . . . . . . . .\
    \ . . 5\n   2. Path Selection Information and Algorithms                     \
    \  7\n       2.1. Metrics . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\
    \       2.2. Advertisement of Link State Information . . . . . . . . . 8\n   \
    \    2.3. Path Selection  . . . . . . . . . . . . . . . . . . . . .10\n      \
    \       2.3.1. Path Computation Algorithm  . . . . . . . . . . .11\n   3. OSPF\
    \ Protocol Extensions                                       16\n       3.1. QoS\
    \ -- Optional Capabilities  . . . . . . . . . . . . . .17\n       3.2. Encoding\
    \ Resources as Extended TOS  . . . . . . . . . . .17\n             3.2.1. Encoding\
    \ bandwidth resource . . . . . . . . . . .19\n             3.2.2. Encoding Delay\
    \  . . . . . . . . . . . . . . . . .21\n       3.3. Packet Formats  . . . . .\
    \ . . . . . . . . . . . . . . . .21\n       3.4. Calculating the Inter-area Routes\
    \ . . . . . . . . . . . .22\n       3.5. Open Issues . . . . . . . . . . . . .\
    \ . . . . . . . . . .22\n   4. A Reference Implementation based on GateD     \
    \                 22\n       4.1. The Gate Daemon (GateD) Program . . . . . .\
    \ . . . . . . .22\n       4.2. Implementing the QoS Extensions of OSPF . . . .\
    \ . . . . .23\n             4.2.1. Design Objectives and Scope . . . . . . . .\
    \ . . .23\n             4.2.2. Architecture  . . . . . . . . . . . . . . . . .\
    \ .24\n       4.3. Major Implementation Issues . . . . . . . . . . . . . . .25\n\
    \       4.4. Bandwidth and Processing Overhead of QoS Routing  . . . .29\n   5.\
    \ Security Considerations                                        32\n   A. Pseudocode\
    \ for the BF Based Pre-Computation Algorithm          33\n   B. On-Demand Dijkstra\
    \ Algorithm for QoS Path Computation          36\n   C. Precomputation Using Dijkstra\
    \ Algorithm                        39\n   D. Explicit Routing Support        \
    \                               43\n   Endnotes                              \
    \                            45\n   References                               \
    \                         46\n   Authors' Addresses                          \
    \                      48\n   Full Copyright Statement                       \
    \                   50\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   In this document, we describe a set of proposed additions\
    \ to the OSPF\n   routing protocol (these additions have been implemented on top\
    \ of the\n   GateD [Con] implementation of OSPF V2 [Moy98]) to support Quality-\n\
    \   of-Service (QoS) routing in IP networks.  Support for QoS routing can\n  \
    \ be viewed as consisting of three major components:\n   1. Obtain the information\
    \ needed to compute QoS paths and select a\n      path capable of meeting the\
    \ QoS requirements of a given request,\n   2. Establish the path selected to accommodate\
    \ a new request,\n   3. Maintain the path assigned for use by a given request.\n\
    \   Although we touch upon aspects related to the last two components,\n   the\
    \ focus of this document is on the first one.  In particular, we\n   discuss the\
    \ metrics required to support QoS, the extension to the\n   OSPF link state advertisement\
    \ mechanism to propagate updates of QoS\n   metrics, and the modifications to\
    \ the path selection to accommodate\n   QoS requests.  The goal of the extensions\
    \ described in this document\n   is to improve performance for QoS flows (likelihood\
    \ to be routed on a\n   path capable of providing the requested QoS), with minimal\
    \ impact on\n   the existing OSPF protocol and its current implementation.  Given\
    \ the\n   inherent complexity of QoS routing, achieving this goal obviously\n\
    \   implies trading-off \"optimality\" for \"simplicity\", but we believe\n  \
    \ this to be required in order to facilitate deployment of QoS routing\n   capabilities.\n\
    \   In addition to describing the proposed extensions to the OSPF\n   protocol,\
    \ this document also reports experimental data based on\n   performance measurements\
    \ of an implementation done on the GateD\n   platform (see Section 4).\n"
- title: 1.1. Overall Framework
  contents:
  - "1.1. Overall Framework\n   We consider a network (1) that supports both best-effort\
    \ packets and\n   packets with QoS guarantees.  The way in which the network resources\n\
    \   are split between the two classes is irrelevant, except for the\n   assumption\
    \ that each QoS capable router in the network is able to\n   dedicate some of\
    \ its resources to satisfy the requirements of QoS\n   packets.  QoS capable routers\
    \ are also assumed capable of identifying\n   and advertising resources that remain\
    \ available to new QoS flows.  In\n   addition, we limit ourselves to the case\
    \ where all the routers\n   involved support the QoS extensions described in this\
    \ document, i.e.,\n   we do not consider the problem of establishing a route in\
    \ a\n   heterogeneous environment where some routers are QoS-capable and\n   others\
    \ are not.  Furthermore, in this document, we focus on the case\n   of unicast\
    \ flows, although many of the additions we define are\n   applicable to multicast\
    \ flows as well.\n   We assume that a flow with QoS requirements specifies them\
    \ in some\n   fashion that is accessible to the routing protocol.  For example,\n\
    \   this could correspond to the arrival of an RSVP [RZB+97] PATH\n   message,\
    \ whose TSpec is passed to routing together with the\n   destination address.\
    \  After processing such a request, the routing\n   protocol returns the path\
    \ that it deems the most suitable given the\n   flow's requirements.  Depending\
    \ on the scope of the path selection\n   process, this returned path could range\
    \ from simply identifying the\n   best next hop, i.e., a hop-by-hop path selection\
    \ model, to specifying\n   all intermediate nodes to the destination, i.e., an\
    \ explicit route\n   model.  The nature of the path being returned impacts the\
    \ operation\n   of the path selection algorithm as it translates into different\n\
    \   requirements for constructing and returning the appropriate path\n   information.\
    \  However, it does not affect the basic operation of the\n   path selection algorithm\
    \ (2).\n   For simplicity and also because it is the model currently supported\n\
    \   in the implementation (see Section 4 for details), in the rest of\n   this\
    \ document we focus on the hop-by-hop path selection model.  The\n   additional\
    \ modifications required to support an explicit routing\n   model are discussed\
    \ in appendix D, but are peripheral to the main\n   focus of this document which\
    \ concentrates on the specific extensions\n   to the OPSF protocol to support\
    \ computation of QoS routes.\n   In addition to the problem of selecting a QoS\
    \ path and possibly\n   reserving the corresponding resources, one should note\
    \ that the\n   successful delivery of QoS guarantees requires that the packets\
    \ of\n   the associated \"QoS flow\" be forwarded on the selected path.  This\n\
    \   typically requires the installation of corresponding forwarding state\n  \
    \ in the router.  For example, with RSVP [RZB+97] flows a classifier\n   entry\
    \ is created based on the filter specs contained in the RESV\n   message.  In\
    \ the case of a Differentiated Service [KNB98] setting,\n   the classifier entry\
    \ may be based on the destination address (or\n   prefix) and the corresponding\
    \ value of the DS byte.  The mechanisms\n   described in this document are at\
    \ the control path level and are,\n   therefore, independent of data path mechanisms\
    \ such as the packet\n   classification method used.  Nevertheless, it is important\
    \ to notice\n   that consistent delivery of QoS guarantees implies stability of\
    \ the\n   data path.  In particular, while it is possible that after a path is\n\
    \   first selected, network conditions change and result in the\n   appearance\
    \ of \"better\" paths, such changes should be prevented from\n   unnecessarily\
    \ affecting existing paths.  In particular, switching\n   over to a new (and better)\
    \ path should be limited to specific\n   conditions, e.g., when the initial selection\
    \ turns out to be\n   inadequate or extremely \"expensive\".  This aspect is beyond\
    \ the scope\n   of QoS routing and belongs to the realm of path management, which\
    \ is\n   outside the main focus of this document.  However, because of its\n \
    \  potentially significant impact on the usefulness of QoS routing, we\n   briefly\
    \ outline a possible approach to path management.\n   Avoiding unnecessary changes\
    \ to QoS paths requires that state\n   information be maintained for each QoS\
    \ path after it has been\n   selected.  This state information is used to track\
    \ the validity of\n   the path, i.e., is the current path adequate or should QoS\
    \ routing be\n   queried again to generate a new and potentially better path.\
    \  We say\n   that a path is \"pinned\" when its state specifies that QoS routing\n\
    \   need not be queried anew, while a path is considered \"un-pinned\"\n   otherwise.\
    \  The main issue is then to define how, when, and where\n   path pinning and\
    \ un-pinning is to take place, and this will typically\n   depend on the mechanism\
    \ used to request QoS routes.  For example,\n   when the RSVP protocol is the\
    \ mechanism being used, it is desirable\n   that path management be kept as synergetic\
    \ as possible with the\n   existing RSVP state management.  In other words, pinning\
    \ and un-\n   pinning of paths should be coordinated with RSVP soft states, and\n\
    \   structured so as to require minimal changes to RSVP processing rules.\n  \
    \ A broad RSVP-routing interface that enables this is described in\n   [GKR97].\
    \  Use of such an interface in the context of reserving\n   resources along an\
    \ explicit path with RSVP is discussed in [GLG+97].\n   Details of path management\
    \ and a means for avoiding loops in case of\n   hop-by-hop path setup can be found\
    \ in [GKH97], and are not addressed\n   further in this document.\n"
- title: 1.2. Simplifying Assumptions
  contents:
  - "1.2. Simplifying Assumptions\n   In order to achieve our goal of minimizing impact\
    \ to the existing\n   protocol and implementation, we impose certain restrictions\
    \ on the\n   range of extensions we initially consider to support QoS. The first\n\
    \   restriction is on the type of additional (QoS) metrics that will be\n   added\
    \ to Link State Advertisements (LSAs) for the purpose of\n   distributing metrics\
    \ updates.  Specifically, the extensions to LSAs\n   that we initially consider,\
    \ include only available bandwidth and\n   delay.  In addition, path selection\
    \ is itself limited to considering\n   only bandwidth requirements.  In particular,\
    \ the path selection\n   algorithm selects paths capable of satisfying the bandwidth\n\
    \   requirement of flows, while at the same time trying to minimize the\n   amount\
    \ of network resources that need to be allocated, i.e., minimize\n   the number\
    \ of hops used.\n   This focus on bandwidth is adequate in most instances, and\
    \ meant to\n   keep initial complexity at an acceptable level.  However, it does\
    \ not\n   fully capture the complete range of potential QoS requirements.  For\n\
    \   example, a delay-sensitive flow of an interactive application could\n   be\
    \ put on a path using a satellite link, if that link provided a\n   direct path\
    \ and had plenty of unused bandwidth.  This would clearly\n   be an undesirable\
    \ choice.  Our approach to preventing such poor\n   choices, is to assign delay-sensitive\
    \ flows to a \"policy\" that would\n   eliminate from the network all links with\
    \ high propagation delay,\n   e.g., satellite links, before invoking the path\
    \ selection algorithm.\n   In general, multiple policies could be used to capture\
    \ different\n   requirements, each presenting to the path selection algorithm\
    \ a\n   correspondingly pruned network topology, on which the same algorithm\n\
    \   would be used to generate an appropriate path.  Alternatively,\n   different\
    \ algorithms could be used depending on the QoS requirements\n   expressed by\
    \ an incoming request.  Such extensions are beyond the\n   scope of this document,\
    \ which limits itself to describing the case of\n   a single metric, bandwidth.\
    \  However, it is worth pointing out that a\n   simple extension to the path selection\
    \ algorithm proposed in this\n   document allows us to directly account for delay,\
    \ under certain\n   conditions, when rate-based schedulers are employed, as in\
    \ the\n   Guaranteed Service proposal [SPG97]; details can be found in [GOW97].\n\
    \   Another important aspect to ensure that introducing support for QoS\n   routing\
    \ has the minimal possible impact, is to develop a solution\n   that has the smallest\
    \ possible computing overhead.  Additional\n   computations are unavoidable, but\
    \ it is desirable to keep the\n   computational cost of QoS routing at a level\
    \ comparable to that of\n   traditional routing algorithms.  One possible approach\
    \ to achieve\n   this goal, is to allow pre-computation of QoS routes.  This is\
    \ the\n   method that was chosen for the implementation of the QoS extensions\n\
    \   to OSPF and is, therefore, the one described in detail in this\n   document.\
    \  Alternative approaches are briefly reviewed in appendices.\n   However, it\
    \ should be noted that although several alternative path\n   selection algorithms\
    \ are possible, the same algorithm should be used\n   consistently within a given\
    \ routing domain.  This requirement may be\n   relaxed when explicit routing is\
    \ used, as the responsibility for\n   selecting a QoS path lies with a single\
    \ entity, the origin of the\n   request, which then ensures consistency even if\
    \ each router uses a\n   different path selection algorithm.  Nevertheless, the\
    \ use of a\n   common path selection algorithm within an AS is recommended, if\
    \ not\n   necessary, for proper operation.\n   A last aspect of concern regarding\
    \ the introduction of QoS routing,\n   is to control the overhead associated with\
    \ the additional link state\n   updates caused by more frequent changes to link\
    \ metrics.  The goal is\n   to minimize the amount of additional update traffic\
    \ without adversely\n   affecting the performance of path selection.  In Section\
    \ 2.2, we\n   present a brief discussion of various alternatives that trade\n\
    \   accuracy of link state information for protocol overhead.  Potential\n   enhancements\
    \ to the path selection algorithm, which seek to\n   (directly) account for the\
    \ inaccuracies in link metrics, are\n   described in [GOW97], while a comprehensive\
    \ treatment of the subject\n   can be found in [LO98, GO99].  In Section 4, we\
    \ also describe the\n   design choices made in a reference implementation, to\
    \ allow future\n   extensions and experimentation with different link state update\n\
    \   mechanisms.\n   The rest of this document is structured as follows.  In Section\
    \ 2, we\n   describe the general design choices and mechanisms we rely on to\n\
    \   support QoS request.  This includes details on the path selection\n   metrics,\
    \ link state update extensions, and the path selection\n   algorithm itself. \
    \ Section 3 focuses on the specific extensions that\n   the OSPF protocol requires,\
    \ while Section 4 describes their\n   implementation in the GateD platform and\
    \ also presents some\n   experimental results.  Section 5 briefly addresses security\
    \ issues\n   that the proposed schemes may raise.  Finally, several appendices\n\
    \   provide additional material of interest, e.g., alternative path\n   selection\
    \ algorithms and support for explicit routes, but somewhat\n   outside the main\
    \ focus of this document.\n"
- title: 2. Path Selection Information and Algorithms
  contents:
  - "2. Path Selection Information and Algorithms\n   This section reviews the basic\
    \ building blocks of QoS path selection,\n   namely the metrics on the which the\
    \ routing algorithm operates, the\n   mechanisms used to propagate updates for\
    \ these metrics, and finally\n   the path selection algorithm itself.\n"
- title: 2.1. Metrics
  contents:
  - "2.1. Metrics\n   The process of selecting a path that can satisfy the QoS requirements\n\
    \   of a new flow relies on both the knowledge of the flow's requirements\n  \
    \ and characteristics, and information about the availability of\n   resources\
    \ in the network.  In addition, for purposes of efficiency,\n   it is also important\
    \ for the algorithm to account for the amount of\n   resources the network has\
    \ to allocate to support a new flow.  In\n   general, the network prefers to select\
    \ the \"cheapest\" path among all\n   paths suitable for a new flow, and it may\
    \ even decide not to accept a\n   new flow for which a feasible path exists, if\
    \ the cost of the path is\n   deemed too high.  Accounting for these aspects involves\
    \ several\n   metrics on which the path selection process is based.  They include:\n\
    \   -  Link available bandwidth:  As mentioned earlier, we currently\n      assume\
    \ that most QoS requirements are derivable from a rate-\n      related quantity,\
    \ termed \"bandwidth.\"  We further assume that\n      associated with each link\
    \ is a maximal bandwidth value, e.g., the\n      link physical bandwidth or some\
    \ fraction thereof that has been set\n      aside for QoS flows.  Since for a\
    \ link to be capable of accepting\n      a new flow with given bandwidth requirements,\
    \ at least that much\n      bandwidth must be still available on the link, the\
    \ relevant link\n      metric is, therefore, the (current) amount of available\
    \ (i.e.,\n      unallocated) bandwidth.  Changes in this metric need to be\n \
    \     advertised as part of extended LSAs, so that accurate information\n    \
    \  is available to the path selection algorithm.\n   -  Link propagation delay:\
    \  This quantity is meant to identify high\n      latency links, e.g., satellite\
    \ links, which may be unsuitable for\n      real-time requests.  This quantity\
    \ also needs to be advertised as\n      part of extended LSAs, although timely\
    \ dissemination of this\n      information is not critical as this parameter is\
    \ unlikely to\n      change (significantly) over time.  As mentioned earlier,\
    \ link\n      propagation delay can be used to decide on the pruning of specific\n\
    \      links, when selecting a path for a delay sensitive request; also,\n   \
    \   it can be used to support a related extension, as described in\n      [GOW97].\n\
    \   -  Hop-count:  This quantity is used as a measure of the path cost to\n  \
    \    the network.  A path with a smaller number of hops (that can\n      support\
    \ a requested connection) is typically preferable, since it\n      consumes fewer\
    \ network resources.  As a result, the path selection\n      algorithm will attempt\
    \ to find the minimum hop path capable of\n      satisfying the requirements of\
    \ a given request.  Note that\n      contrary to bandwidth and propagation delay,\
    \ hop count is a metric\n      that does not affect LSAs, and it is only used\
    \ implicitly as part\n      of the path selection algorithm.\n"
- title: 2.2. Advertisement of Link State Information
  contents:
  - "2.2. Advertisement of Link State Information\n   The new link metrics identified\
    \ in the previous section need to be\n   advertised across the network, so that\
    \ each router can compute\n   accurate and consistent QoS routes.  It is assumed\
    \ that each router\n   maintains an updated database of the network topology,\
    \ including the\n   current state (available bandwidth and propagation delay)\
    \ of each\n   link.  As mentioned before, the distribution of link state (metrics)\n\
    \   information is based on extending OSPF mechanisms.  The detailed\n   format\
    \ of those extensions is described in Section 3, but in addition\n   to how link\
    \ state information is distributed, another important\n   aspect is when such\
    \ distribution is to take place.\n   One option is to mandate periodic updates,\
    \ where the period of\n   updates is determined based on a tolerable corresponding\
    \ load on the\n   network and the routers.  The main disadvantage of such an approach\n\
    \   is that major changes in the bandwidth available on a link could\n   remain\
    \ unknown for a full period and, therefore, result in many\n   incorrect routing\
    \ decisions.  Ideally, routers should have the most\n   current view of the bandwidth\
    \ available on all links in the network,\n   so that they can make the most accurate\
    \ decision of which path to\n   select.  Unfortunately, this then calls for very\
    \ frequent updates,\n   e.g., each time the available bandwidth of a link changes,\
    \ which is\n   neither scalable nor practical.  In general, there is a trade-off\n\
    \   between the protocol overhead of frequent updates and the accuracy of\n  \
    \ the network state information that the path selection algorithm\n   depends\
    \ on.  We outline next a few possible link state update\n   policies, which strike\
    \ a practical compromise.\n   The basic idea is to trigger link state advertisements\
    \ only when\n   there is a significant change in the value of metrics since the\
    \ last\n   advertisement.  The notion of significance of a change can be based\n\
    \   on an \"absolute\" scale or a \"relative\" one.  An absolute scale means\n\
    \   partitioning the range of values that a metric can take into\n   equivalence\
    \ classes and triggering an update whenever the metric\n   changes sufficiently\
    \ to cross a class boundary (3).  A relative\n   scale, on the other hand, triggers\
    \ updates when the percentage change\n   in the metric value exceeds a predefined\
    \ threshold.  Independent of\n   whether a relative or an absolute change trigger\
    \ mechanism is used, a\n   periodic trigger constraint can also be added.  This\
    \ constraint can\n   be in the form of a hold-down timer, which is used to force\
    \ a minimum\n   spacing between consecutive updates.  Alternatively, a transmit\
    \ timer\n   can also be used to ensure the transmission of an update after a\n\
    \   certain time has expired.  Such a feature can be useful if link state\n  \
    \ updates advertising bandwidth changes are sent unreliably.  The\n   current\
    \ protocol extensions described in Section 3 as well as the\n   implementation\
    \ of Section 4 do not consider such an option as metric\n   updates are sent using\
    \ the standard, and reliable, OSPF flooding\n   mechanism.  However, this is clearly\
    \ an extension worth considering\n   as it can help lower substantially the protocol\
    \ overhead associated\n   with metrics updates.\n   In both the relative and absolute\
    \ change approaches, the metric value\n   advertised in an LSA can be either the\
    \ actual or a quantized value.\n   Advertising the actual metric value is more\
    \ accurate and, therefore,\n   preferable when metrics are frequently updated.\
    \  On the other hand,\n   when updates are less frequent, e.g., because of a low\
    \ sensitivity\n   trigger or the use of hold-down timers, advertising quantized\
    \ values\n   can be of benefit.  This is because it can help increase the number\n\
    \   of equal cost paths and, therefore, improve robustness to metrics\n   inaccuracies.\
    \  In general, there is a broad space of possible trade-\n   offs between accuracy\
    \ and overhead and selecting an appropriate\n   design point is difficult and\
    \ depends on many parameters (see\n   [AGKT98] for a more detailed discussion\
    \ of these issues).  As a\n   result, in order to help acquire a better understanding\
    \ of these\n   issues, the implementation described in Section 4 supports a range\
    \ of\n   options that allow exploration of the available design space.  In\n \
    \  addition, Section 4 also reports experimental data on the traffic\n   load\
    \ and processing overhead generated by links state updates for\n   different configurations.\n"
- title: 2.3. Path Selection
  contents:
  - "2.3. Path Selection\n   There are two major aspects to computing paths for QoS\
    \ requests.  The\n   first is the actual path selection algorithm itself, i.e.,\
    \ which\n   metrics and criteria it relies on.  The second is when the algorithm\n\
    \   is actually invoked.\n   The topology on which the algorithm is run is, as\
    \ with the standard\n   OSPF path selection, a directed graph where vertices (4)\
    \ consist of\n   routers and networks (transit vertices) as well as stub networks\n\
    \   (non-transit vertices).  When computing a path, stub networks are\n   added\
    \ as a post-processing step, which is essentially similar to what\n   is done\
    \ with the current OSPF routing protocol.  The optimization\n   criteria used\
    \ by the path selection are reflected in the costs\n   associated with each interface\
    \ in the topology and how those costs\n   are accounted for in the algorithm itself.\
    \  As mentioned before, the\n   cost of a path is a function of both its hop count\
    \ and the amount of\n   available bandwidth.  As a result, each interface has\
    \ associated with\n   it a metric, which corresponds to the amount of bandwidth\
    \ that\n   remains available on this interface.  This metric is combined with\n\
    \   hop count information to provide a cost value, whose goal is to pick\n   a\
    \ path with the minimum possible number of hops among those that can\n   support\
    \ the requested bandwidth.  When several such paths are\n   available, the preference\
    \ is for the path whose available bandwidth\n   (i.e., the smallest value on any\
    \ of the links in the path) is\n   maximal.  The rationale for the above rule\
    \ is the following:  we\n   focus on feasible paths (as accounted by the available\
    \ bandwidth\n   metric) that consume a minimal amount of network resources (as\n\
    \   accounted by the hop-count metric); and the rule for selecting among\n   these\
    \ paths is meant to balance load as well as maximize the\n   likelihood that the\
    \ required bandwidth is indeed available.\n   It should be noted that standard\
    \ routing algorithms are typically\n   single objective optimizations, i.e., they\
    \ may minimize the hop-\n   count, or maximize the path bandwidth, but not both.\
    \  Double\n   objective path optimization is a more complex task, and, in general,\n\
    \   it is an intractable problem [GJ79].  Nevertheless, because of the\n   specific\
    \ nature of the two objectives being optimized (bandwidth and\n   hop count),\
    \ the complexity of the above algorithm is competitive with\n   even that of standard\
    \ single-objective algorithms.  For readers\n   interested in a thorough treatment\
    \ of the topic, with insights into\n   the connection between the different algorithms,\
    \ linear algebra and\n   modification of metrics, [Car79] is recommended.\n  \
    \ Before proceeding with a more detailed description of the path\n   selection\
    \ algorithm itself, we briefly review the available options\n   when it comes\
    \ to deciding when to invoke the algorithm.  The two main\n   options are:  1)\
    \ to perform on-demand computations, that is, trigger\n   a computation for each\
    \ new request, and 2) to use some form of pre-\n   computation.  The on-demand\
    \ case involves no additional issues in\n   terms of when computations should\
    \ be triggered, but running the path\n   selection algorithm for each new request\
    \ can be computationally\n   expensive (see [AT98] for a discussion on this issue).\
    \  On the other\n   hand, pre-computing paths amortizes the computational cost\
    \ over\n   multiple requests, but each computation instance is usually more\n\
    \   expensive than in the on-demand case (paths are computed to all\n   destinations\
    \ and for all possible bandwidth requests rather than for\n   a single destination\
    \ and a given bandwidth request).  Furthermore,\n   depending on how often paths\
    \ are recomputed, the accuracy of the\n   selected paths may be lower.  In this\
    \ document, we primarily focus on\n   the case of pre-computed paths, which is\
    \ also the only method\n   currently supported in the reference implementation\
    \ described in\n   Section 4.  In this case, clearly, an important issue is when\
    \ such\n   pre-computation should take place.  The two main options we consider\n\
    \   are periodic pre-computations and pre-computations after a given (N)\n   number\
    \ of updates have been received.  The former has the benefit of\n   ensuring a\
    \ strict bound on the computational load associated with\n   pre-computations,\
    \ while the latter can provide for a more responsive\n   solution (5).  Section\
    \ 4 provides some experimental results comparing\n   the performance and cost\
    \ of periodic pre-computations for different\n   period values.\n"
- title: 2.3.1. Path Computation Algorithm
  contents:
  - "2.3.1. Path Computation Algorithm\n   This section describes a path selection\
    \ algorithm, which for a given\n   network topology and link metrics (available\
    \ bandwidth), pre-computes\n   all possible QoS paths, while maintaining a reasonably\
    \ low\n   computational complexity.  Specifically, the algorithm pre-computes\n\
    \   for any destination a minimum hop count path with maximum bandwidth,\n   and\
    \ has a computational complexity comparable to that of a standard\n   Bellman-Ford\
    \ shortest path algorithm.  The Bellman-Ford (BF) shortest\n   path algorithm\
    \ is adapted to compute paths of maximum available\n   bandwidth for all hop counts.\
    \  It is a property of the BF algorithm\n   that, at its h-th iteration, it identifies\
    \ the optimal (in our\n   context:  maximal bandwidth) path between the source\
    \ and each\n   destination, among paths of at most h hops.  In other words, the\
    \ cost\n   of a path is a function of its available bandwidth, i.e., the\n   smallest\
    \ available bandwidth on all links of the path, and finding a\n   minimum cost\
    \ path amounts to finding a maximum bandwidth path.\n   However, because the BF\
    \ algorithm progresses by increasing hop count,\n   it essentially provides for\
    \ free the hop count of a path as a second\n   optimization criteria.\n   Specifically,\
    \ at the kth (hop count) iteration of the algorithm, the\n   maximum bandwidth\
    \ available to all destinations on a path of no more\n   than k hops is recorded\
    \ (together with the corresponding routing\n   information).  After the algorithm\
    \ terminates, this information\n   provides for all destinations and bandwidth\
    \ requirements, the path\n   with the smallest possible number of hops and sufficient\
    \ bandwidth to\n   accommodate the new request.  Furthermore, this path is also\
    \ the one\n   with the maximal available bandwidth among all the feasible paths\n\
    \   with at most these many hops.  This is because for any hop count, the\n  \
    \ algorithm always selects the one with maximum available bandwidth.\n   We now\
    \ proceed with a more detailed description of the algorithm and\n   the data structure\
    \ used to record routing information, i.e., the QoS\n   routing table that gets\
    \ built as the algorithm progresses (the\n   pseudo-code for the algorithm can\
    \ be found in Appendix A).  As\n   mentioned before, the algorithm operates on\
    \ a directed graph\n   consisting only of transit vertices (routers and networks),\
    \ with\n   stub-networks subsequently added to the path(s) generated by the\n\
    \   algorithm.  The metric associated with each edge in the graph is the\n   bandwidth\
    \ available on the corresponding interface.  Let us denote by\n   b(n;m) the available\
    \ bandwidth on the link from node n to m.  The\n   vertex corresponding to the\
    \ router where the algorithm is being run,\n   i.e., the computing router, is\
    \ denoted as the \"source node\" for the\n   purpose of path selection.  The algorithm\
    \ proceeds to pre-compute\n   paths from this source node to all possible destination\
    \ networks and\n   for all possible bandwidth values.  At each (hop count) iteration,\n\
    \   intermediate results are recorded in a QoS routing table, which has\n   the\
    \ following structure:\n"
- title: 'The QoS routing table:'
  contents:
  - "The QoS routing table:\n   -  a KxH matrix, where K is the number of destinations\
    \ (vertices in\n      the graph) and H is the maximal allowed (or possible) number\
    \ of\n      hops for a path.\n   -  The (n;h) entry is built during the hth iteration\
    \ (hop count\n      value) of the algorithm, and consists of two fields:\n   \
    \      *  bw:  the maximum available bandwidth, on a path of at most h\n     \
    \       hops between the source node (router) and destination node\n         \
    \   n;\n         *  neighbor:  this is the routing information associated with\n\
    \            the h (or less) hops path to destination node n, whose\n        \
    \    available bandwidth is bw.  In the context of hop-by-hop\n            path\
    \ selection (6), the neighbor information is simply the\n            identity\
    \ of the node adjacent to the source node on that\n            path.  As a rule,\
    \ the \"neighbor\" node must be a router and\n            not a network, the only\
    \ exception being the case where the\n            network is the destination node\
    \ (and the selected path is\n            the single edge interconnecting the source\
    \ to it).\n   Next, we provide additional details on the operation of the algorithm\n\
    \   and how the entries in the routing table are updated as the algorithm\n  \
    \ proceeds.  For simplicity, we first describe the simpler case where\n   all\
    \ edges count as \"hops,\" and later explain how zero-hop edges are\n   handled.\
    \  Zero-hop edges arise in the case of transit networks\n   vertices, where only\
    \ one of the two incoming and outgoing edges\n   should be counted in the hop\
    \ count computation, as they both\n   correspond to the same physical hop.  Accounting\
    \ for this aspect\n   requires distinguishing between network and router nodes,\
    \ and the\n   steps involved are detailed later in this section as well as in\
    \ the\n   pseudo-code of Appendix A.\n   When the algorithm is invoked, the routing\
    \ table is first initialized\n   with all bw fields set to 0 and neighbor fields\
    \ cleared.  Next, the\n   entries in the first column (which corresponds to one-hop\
    \ paths) of\n   the neighbors of the computing router are modified in the following\n\
    \   way:  the bw field is set to the value of the available bandwidth on\n   the\
    \ direct edge from the source.  The neighbor field is set to the\n   identity\
    \ of the neighbor of the computing router, i.e., the next\n   router on the selected\
    \ path.\n   Afterwards, the algorithm iterates for at most H iterations\n   (considering\
    \ the above initial iteration as the first).  The value of\n   H could be implicit,\
    \ i.e., the diameter of the network or, in order\n   to better control the worst\
    \ case complexity, it can be set explicitly\n   thereby limiting path lengths\
    \ to at most H hops.  In the latter case,\n   H must be assigned a value larger\
    \ than the length of the minimum\n   hop-count path to any node in the graph.\n\
    \   At iteration h, we first copy column h-1  into column h.  In\n   addition,\
    \ the algorithm keeps a list of nodes that changed their bw\n   value in the previous\
    \ iteration, i.e., during the (h-1)-th iteration.\n   The algorithm then looks\
    \ at each link (n;m) where n is a node whose\n   bw value changed in the previous\
    \ iteration, and checks the maximal\n   available bandwidth on an (at most) h-hop\
    \ path to node m whose final\n   hop is that link.  This amounts to taking the\
    \ minimum between the bw\n   field in entry (n;h-1) and the link metric value\
    \ b(n;m) kept in the\n   topology database.  If this value is higher than the\
    \ present value of\n   the bw field in entry (m;h), then a better (larger bw value)\
    \ path has\n   been found for destination m and with at most h hops.  The bw field\n\
    \   of entry (m;h) is then updated to reflect this new value.  In the\n   case\
    \ of hop-by-hop routing, the neighbor field of entry (m;h) is set\n   to the same\
    \ value as in entry (n;h-1).  This records the identity of\n   the first hop (next\
    \ hop from the source) on the best path identified\n   thus far for destination\
    \ m and with h (or less) hops.\n   As mentioned earlier, extending the above algorithm\
    \ to handle zero-\n   hop edges is needed due to the possible use of multi-access\
    \ networks,\n   e.g., T/R, E/N, etc., to interconnect routers.  Such entities\
    \ are\n   also represented by means of a vertex in the OSPF topology, but a\n\
    \   network connecting two routers should clearly be considered as a\n   single\
    \ hop path rather than a two hop path.  For example, consider\n   three routers\
    \ A, B, and C connected over an Ethernet network N, which\n   the OSPF topology\
    \ represents as in Figure 1.\n                           A----N----B\n       \
    \                         |\n                                |\n             \
    \                   C\n                        Figure 1: Zero-Hop Edges\n   In\
    \ the example of Figure 1, although there are directed edges in both\n   directions,\
    \ an edge from the network to any of the three routers must\n   have zero \"cost\"\
    , so that it is not counted twice.  It should be\n   noted that when considering\
    \ such environments in the context of QoS\n   routing, it is assumed that some\
    \ entity is responsible for\n   determining the \"available bandwidth\" on the\
    \ network, e.g., a subnet\n   bandwidth manager.  The specification and operation\
    \ of such an entity\n   is beyond the scope of this document.\n   Accommodating\
    \ zero-hop edges in the context of the path selection\n   algorithm described\
    \ above is done as follows:  At each iteration h\n   (starting with the first),\
    \ whenever an entry (m;h) is modified, it is\n   checked whether there are zero-cost\
    \ edges (m;k) emerging from node m.\n   This is the case when m is a transit network.\
    \  In that case, we\n   attempt to further improve the entry of node k within\
    \ the current\n   iteration, i.e., entry (k;h) (rather than entry (k;h+1)), since\
    \ the\n   edge (m;k) should not count as an additional hop.  As with the\n   regular\
    \ operation of the algorithm, this amounts to taking the\n   minimum between the\
    \ bw field in entry (m;h) and the link metric value\n   b(m;k) kept in the topology\
    \ database (7).  If this value is higher\n   than the present value of the bw\
    \ field in entry (k;h), then the bw\n   field of entry (k;h) is updated to this\
    \ new value.  In the case of\n   hop-by-hop routing, the neighbor field of entry\
    \ (k;h) is set, as\n   usual, to the same value as in entry (m;h) (which is also\
    \ the value\n   in entry (n;h-1)).\n   Note that while for simplicity of the exposition,\
    \ the issue of equal\n   cost, i.e., same hop count and available bandwidth, is\
    \ not detailed\n   in the above description, it can be easily supported.  It only\n\
    \   requires that the neighbor field be expanded to record the list of\n   next\
    \ (previous) hops, when multiple equal cost paths are present.\n"
- title: Addition of Stub Networks
  contents:
  - "Addition of Stub Networks\n   As was mentioned earlier, the path selection algorithm\
    \ is run on a\n   graph whose vertices consist only of routers and transit networks\
    \ and\n   not stub networks.  This is intended to keep the computational\n   complexity\
    \ as low as possible as stub networks can be added\n   relatively easily through\
    \ a post-processing step.  This second\n   processing step is similar to the one\
    \ used in the current OSPF\n   routing table calculation [Moy98], with some differences\
    \ to account\n   for the QoS nature of routes.\n   Specifically, after the QoS\
    \ routing table has been constructed, all\n   the router vertices are again considered.\
    \  For each router, stub\n   networks whose links appear in the router's link\
    \ advertisements will\n   be processed to determine QoS routes available to them.\
    \  The QoS\n   routing information for a stub network is similar to that of routers\n\
    \   and transit networks and consists of an extension to the QoS routing\n   table\
    \ in the form of an additional row.  The columns in that new row\n   again correspond\
    \ to paths of different hop counts, and contain both\n   bandwidth and next hop\
    \ information.  We also assume that an available\n   bandwidth value has been\
    \ advertised for the stub network.  As before,\n   how this value is determined\
    \ is beyond the scope of this document.\n   The QoS routes for a stub network\
    \ S are constructed as follows:\n   Each entry in the row corresponding to stub\
    \ network S has its bw(s)\n   field initialized to zero and its neighbor set to\
    \ null.  When a stub\n   network S is found in the link advertisement of router\
    \ V, the value\n   bw(S,h) in the hth column of the row corresponding to stub\
    \ network S\n   is updated as follows:\n      bw(S,h) = max ( bw(S,h) ; min (\
    \ bw(V,h) , b(V,S) ) ),\n   where bw(V,h) is the bandwidth value of the corresponding\
    \ column for\n   the QoS routing table row associated with router V, i.e., the\n\
    \   bandwidth available on an h hop path to V, and b(V,S) is the\n   advertised\
    \ available bandwidth on the link from V to S.  The above\n   expression essentially\
    \ states that the bandwidth of a h hop path to\n   stub network S is updated using\
    \ a path through router V, only if the\n   minimum of the bandwidth of the h hop\
    \ path to V and the bandwidth on\n   the link between V and S is larger than the\
    \ current value.\n   Update of the neighbor field proceeds similarly whenever\
    \ the\n   bandwidth of a path through V is found to be larger than or equal to\n\
    \   the current value.  If it is larger, then the neighbor field of V in\n   the\
    \ corresponding column replaces the current neighbor field of S.\n   If it is\
    \ equal, then the neighbor field of V in the corresponding\n   column is concatenated\
    \ with the existing field for S, i.e., the\n   current set of neighbors for V\
    \ is added to the current set of\n   neighbors for S.\n"
- title: Extracting Forwarding Information from Routing Table
  contents:
  - "Extracting Forwarding Information from Routing Table\n   When the QoS paths are\
    \ precomputed, the forwarding information for a\n   flow with given destination\
    \ and bandwidth requirement needs to be\n   extracted from the routing table.\
    \  The case of hop-by-hop routing is\n   simpler than that of explicit routing.\
    \  This is because, only the\n   next hop needs to be returned instead of an explicit\
    \ route.\n   Specifically, assume a new request to destination, say, d, and with\n\
    \   bandwidth requirements B.  The index of the destination vertex\n   identifies\
    \ the row in the QoS routing table that needs to be checked\n   to generate a\
    \ path.  Assuming that the QoS routing table was\n   constructed using the Bellman-Ford\
    \ algorithm presented later in this\n   section, the search then proceeds by increasing\
    \ index (hop) count\n   until an entry is found, say at hop count or column index\
    \ of h, with\n   a value of the bw field which is equal to or larger than B. \
    \ This\n   entry points to the initial information identifying the selected\n\
    \   path.\n   If the path computation algorithm stores multiple equal cost paths,\n\
    \   then some degree of load balancing can be achieved at the time of\n   path\
    \ selection.  A next hop from the list of equivalent next hops can\n   be chosen\
    \ in a round robin manner, or randomly with a probability\n   that is weighted\
    \ by the actual available bandwidth on the local\n   interface.  The latter is\
    \ the method used in the implementation\n   described in Section 4.\n   The case\
    \ of explicit routing is discussed in Appendix D.\n"
- title: 3. OSPF Protocol Extensions
  contents:
  - "3. OSPF Protocol Extensions\n   As stated earlier, one of our goals is to limit\
    \ the additions to the\n   existing OSPF V2 protocol, while still providing the\
    \ required level\n   of support for QoS based routing.  To this end, all of the\
    \ existing\n   OSPF mechanisms, data structures, advertisements, and data formats\n\
    \   remain in place.  The purpose of this section of the document is to\n   describe\
    \ the extensions to the OSPF protocol needed to support QoS as\n   outlined in\
    \ the previous sections.\n"
- title: 3.1. QoS -- Optional Capabilities
  contents:
  - "3.1. QoS -- Optional Capabilities\n   The OSPF Options field is present in OSPF\
    \ Hello packets, Database\n   Description packets and all LSAs.  The Options field\
    \ enables OSPF\n   routers to support (or not support) optional capabilities,\
    \ and to\n   communicate their capability level to other OSPF routers.  Through\n\
    \   this mechanism, routers of differing capabilities can be mixed within\n  \
    \ an OSPF routing domain.  Currently, the OSPF standard [Moy98]\n   specifies\
    \ the following 5 bits in the options octet:\n           +-----------------------------------------------+\n\
    \           |  *  |  *  | DC  |  EA | N/P |  MC |  E  |  *  |\n           +-----------------------------------------------+\n\
    \   Note that the least significant bit (`T' bit) that was used to\n   indicate\
    \ TOS routing capability in the older OSPF specification\n   [Moy94] has been\
    \ removed.  However, for backward compatibility with\n   previous versions of\
    \ the OSPF specification, TOS-specific information\n   can be included in router-LSAs,\
    \ summary-LSAs and AS-external-LSAs.\n   We propose to reclaim the `T' bit as\
    \ an indicator of router's QoS\n   routing capability and refer to it as the `Q'\
    \ bit.  In fact, QoS\n   capability can be viewed as an extension of the TOS-capabilities\
    \ and\n   QoS routing as a form of TOS-based routing.  A router sets this bit\n\
    \   in its hello packets to indicate that it is capable of supporting\n   such\
    \ routing.  When this bit is set in a router or summary links link\n   state advertisement,\
    \ it means that there are QoS fields to process in\n   the packet.  When this\
    \ bit is set in a network link state\n   advertisement it means that the network\
    \ described in the\n   advertisement is QoS capable.\n   We need to be careful\
    \ in this approach so as to avoid confusing any\n   old style (i.e., RFC 1583\
    \ based) TOS routing implementations.  The\n   TOS metric encoding rules of QoS\
    \ fields introduced further in this\n   section will show how this is achieved.\
    \  Additionally, unlike the RFC\n   1583 specification that unadvertised TOS metrics\
    \ be treated to have\n   same cost as TOS 0, for the purpose of computing QOS\
    \ routes,\n   unadvertised TOS metrics (on a hop) indicate lack of connectivity\
    \ for\n   the specific TOS metrics (for that hop).\n"
- title: 3.2. Encoding Resources as Extended TOS
  contents:
  - "3.2. Encoding Resources as Extended TOS\n   Introduction of QoS should ideally\
    \ not influence the compatibility\n   with existing OSPFv2 routers.  To achieve\
    \ this goal, necessary\n   extensions in packet formats must be defined in a way\
    \ that either is\n   understood by OSPFv2 routers, ignored, or in the worst case\n\
    \   \"gracefully\" misinterpreted.  Encoding of QoS metrics in the TOS\n   field\
    \ which fortunately enough is longer in OSPF packets than\n   officially defined\
    \ in [Alm92], allows us to mimic the new facility as\n   extended TOS capability.\
    \  OSPFv2 routers will either disregard these\n   definitions or consider those\
    \ unspecified.  Specific precautions are\n   taken to prevent careless OSPF implementations\
    \ from influencing\n   traditional TOS routers (if any) when misinterpreting the\
    \ QoS\n   extensions.\n   For QoS resources, 32 combinations are available through\
    \ the use of\n   the fifth bit in TOS fields contained in different LSAs.  Since\n\
    \   [Alm92] defines TOS as being four bits long, this definition never\n   conflicts\
    \ with existing values.  Additionally, to prevent naive\n   implementations that\
    \ do not take all bits of the TOS field in OSPF\n   packets into considerations,\
    \ the definitions of the `QoS encodings'\n   is aligned in their semantics with\
    \ the TOS encoding.  Only bandwidth\n   and delay are specified as of today and\
    \ their values map onto\n   `maximize throughput' and `minimize delay' if the\
    \ most significant\n   bit is not taken into account.  Accordingly, link reliability\
    \ and\n   jitter could be defined later if necessary.\n        OSPF encoding \
    \  RFC 1349 TOS values\n        ___________________________________________\n\
    \        0               0000 normal service\n        2               0001 minimize\
    \ monetary cost\n        4               0010 maximize reliability\n        6\
    \               0011\n        8               0100 maximize throughput\n     \
    \   10              0101\n        12              0110\n        14           \
    \   0111\n        16              1000 minimize delay\n        18            \
    \  1001\n        20              1010\n        22              1011\n        24\
    \              1100\n        26              1101\n        28              1110\n\
    \        30              1111\n        OSPF encoding   `QoS encoding values'\n\
    \        -------------------------------------------\n        32             10000\n\
    \        34             10001\n        36             10010\n        38      \
    \       10011\n        40             10100 bandwidth\n        42            \
    \ 10101\n        44             10110\n        46             10111\n        48\
    \             11000 delay\n        50             11001\n        52          \
    \   11010\n        54             11011\n        56             11100\n      \
    \  58             11101\n        60             11110\n        62            \
    \ 11111\n        Representing TOS and QoS in OSPF.\n"
- title: 3.2.1. Encoding bandwidth resource
  contents:
  - "3.2.1. Encoding bandwidth resource\n   Given the fact that the actual metric\
    \ field in OSPF packets only\n   provides 16 bits to encode the value used and\
    \ that links supporting\n   bandwidth ranging into Gbits/s are becoming reality,\
    \ linear\n   representation of the available resource metric is not feasible.\
    \  The\n   solution is exponential encoding using appropriately chosen implicit\n\
    \   base value and number bits for encoding mantissa and the exponent.\n   Detailed\
    \ considerations leading to the solution described are not\n   presented here\
    \ but can be found in [Prz95].\n   Given a base of 8, the 3 most significant bits\
    \ should be reserved for\n   the exponent part and the remaining 13 for the mantissa.\
    \  This allows\n   a simple comparison for two numbers encoded in this form, which\
    \ is\n   often useful during implementation.\n   The following table shows bandwidth\
    \ ranges covered when using\n   different exponents and the granularity of possible\
    \ reservations.\n        exponent\n        value x         range (2^13-1)*8^x\
    \      step 8^x\n        -------------------------------------------------\n \
    \       0               8,191                   1\n        1               65,528\
    \                  8\n        2               524,224                 64\n   \
    \     3               4,193,792               512\n        4               33,550,336\
    \              4,096\n        5               268,402,688             32,768\n\
    \        6               2,147,221,504           262,144\n        7          \
    \     17,177,772,032          2,097,152\n          Ranges of Exponent Values for\
    \ 13 bits,\n               base 8 Encoding, in Bytes/s\n   The bandwidth encoding\
    \ rule may be summarized as: \"represent\n   available bandwidth in 16 bit field\
    \ as a 3 bit exponent (with assumed\n   base of 8) followed by a 13 bit mantissa\
    \ as shown below and advertise\n   2's complement of the above representation.\"\
    \n        0       8       16\n        |       |       |\n        -----------------\n\
    \       |EXP| MANT        |\n        -----------------\n   Thus, the above encoding\
    \ advertises a numeric value that is\n      2^16 -1 -(exponential encoding of\
    \ the available bandwidth):\n   This has the property of advertising a higher\
    \ numeric value for lower\n   available bandwidth, a notion that is consistent\
    \ with that of cost.\n   Although it may seem slightly pedantic to insist on the\
    \ property that\n   less bandwidth is expressed higher values, it has, besides\n\
    \   consistency, a robustness aspect in it.  A router with a poor OSPF\n   implementation\
    \ could misuse or misunderstand bandwidth metric as\n   normal administrative\
    \ cost provided to it and compute spanning trees\n   with a \"normal\" Dijkstra.\
    \  The effect of a heavily congested link\n   advertising numerically very low\
    \ cost could be disastrous in such a\n   scenario.  It would raise the link's\
    \ attractiveness for future\n   traffic instead of lowering it.  Evidence that\
    \ such considerations\n   are not speculative, but similar scenarios have been\
    \ encountered, can\n   be found in [Tan89].\n   Concluding with an example, assume\
    \ a link with bandwidth of 8 Gbits/s\n   = 1024^3 Bytes/s, its encoding would\
    \ consist of an exponent value of\n   6 since 1024^3= 4,096*8^6, which would then\
    \ have a granularity of 8^6\n   or approx. 260 kBytes/s.  The associated binary\
    \ representation would\n   then be %(110) 0 1000 0000 0000% or 53,248 (8).  The\
    \ bandwidth cost\n   (advertised value) of this link when it is idle, is then\
    \ the 2's\n   complement of the above binary representation, i.e., %(001) 1 0111\n\
    \   1111 1111% which corresponds to a decimal value of (2^16 - 1) -\n   53,248\
    \ = 12,287.  Assuming now a current reservation level of 6;400\n   Mbits/s = 200\
    \ * 1024^2, there remains 1;600 Mbits/s of available\n   bandwidth on the link.\
    \  The encoding of this available bandwidth of\n   1'600 Mbits/s is 6,400 * 8^5,\
    \ which corresponds to a granularity of\n   8^5 or approx. 30 kBytes/s, and has\
    \ a binary representation of %(101)\n   1 1001 0000 0000% or decimal value of\
    \ 47,360.  The advertised cost of\n   the link with this load level, is then %(010)\
    \ 0 0110 1111 1111%, or\n   (2^16-1) -47,360 = 18,175.\n   Note that the cost\
    \ function behaves as it should, i.e., the less\n   bandwidth is available on\
    \ a link, the higher the cost and the less\n   attractive the link becomes.  Furthermore,\
    \ the targeted property of\n   better granularity for links with less bandwidth\
    \ available is also\n   achieved.  It should, however, be pointed out that the\
    \ numbers given\n   in the above examples match exactly the resolution of the\
    \ proposed\n   encoding, which is of course not always the case in practice. \
    \ This\n   leaves open the question of how to encode available bandwidth values\n\
    \   when they do not exactly match the encoding.  The standard practice\n   is\
    \ to round it to the closest number.  Because we are ultimately\n   interested\
    \ in the cost value for which it may be better to be\n   pessimistic than optimistic,\
    \ we choose to round costs up and,\n   therefore, bandwidth down.\n"
- title: 3.2.2. Encoding Delay
  contents:
  - "3.2.2. Encoding Delay\n   Delay is encoded in microseconds using the same exponential\
    \ method as\n   described for bandwidth except that the base is defined to be\
    \ 4\n   instead of 8.  Therefore, the maximum delay that can be expressed is\n\
    \   (2^13-1) *4^7 i.e., approx. 134 seconds.\n"
- title: 3.3. Packet Formats
  contents:
  - "3.3. Packet Formats\n   Given the extended TOS notation to account for QoS metrics,\
    \ no\n   changes in packet formats are necessary except for the\n   (re)introduction\
    \ of T-bit as the Q-bit in the options field.  Routers\n   not understanding the\
    \ Q-bit should either not consider the QoS\n   metrics distributed or consider\
    \ those as `unknown' TOS.\n   To support QoS, there are additions to two Link\
    \ State Advertisements,\n   the Router Links Advertisement and the Summary Links\
    \ Advertisement.\n   As stated above, a router identifies itself as supporting\
    \ QoS by\n   setting the Q-bit in the options field of the Link State Header.\n\
    \   When a router that supports QoS receives either the Router Links or\n   Summary\
    \ Links Advertisement, it should parse the QoS metrics encoded\n   in the received\
    \ Advertisement.\n"
- title: 3.4. Calculating the Inter-area Routes
  contents:
  - "3.4. Calculating the Inter-area Routes\n   This document proposes a very limited\
    \ use of OSPF areas, that is, it\n   is assumed that summary links advertisements\
    \ exist for all networks\n   in the area.  This document does not discuss the\
    \ problem of providing\n   support for area address ranges and QoS metric aggregation.\
    \  This is\n   left for further studies.\n"
- title: 3.5. Open Issues
  contents:
  - "3.5. Open Issues\n   Support for AS External Links, Virtual Links, and incremental\
    \ updates\n   for summary link advertisements are not addressed in this document\n\
    \   and are left for further study.  For Virtual Links that do exist, it\n   is\
    \ assumed for path selection that these links are non-QoS capable\n   even if\
    \ the router advertises QoS capability.  Also, as stated\n   earlier, this document\
    \ does not address the issue of non-QoS routers\n   within a QoS domain.\n"
- title: 4. A Reference Implementation based on GateD
  contents:
  - "4. A Reference Implementation based on GateD\n   In this section we report on\
    \ the experience gained from implementing\n   the pre-computation based approach\
    \ of Section 2.3.1 in the GateD\n   [Con] environment.  First, we briefly introduce\
    \ the GateD\n   environment, and then present some details on how the QoS extensions\n\
    \   were implemented in this environment.  Finally, we discuss issues\n   that\
    \ arose during the implementation effort and present some\n   measurement based\
    \ results on the overhead that the QoS extensions\n   impose on a QoS capable\
    \ router and a network of QoS routers.  For\n   further details on the implementation\
    \ study, the reader is referred\n   to [AGK99].  Additional performance evaluation\
    \ based on simulations\n   can be found in [AGKT98].\n"
- title: 4.1. The Gate Daemon (GateD) Program
  contents:
  - "4.1. The Gate Daemon (GateD) Program\n   GateD [Con] is a popular, public domain\
    \ (9) program that provides a\n   platform for implementing routing protocols\
    \ on hosts running the Unix\n   operating system.  The distribution of the GateD\
    \ software also\n   includes implementations of many popular routing protocols,\
    \ including\n   the OSPF protocol.  The GateD environment offers a variety of\n\
    \   services useful for implementing a routing protocol.  These services\n   include\
    \ a) support for creation and management of timers, b) memory\n   management,\
    \ c) a simple scheduling mechanism, d) interfaces for\n   manipulating the host's\
    \ routing table and accessing the network, and\n   e) route management (e.g.,\
    \ route prioritization and route exchange\n   between protocols).\n   All GateD\
    \ processing is done within a single Unix process, and\n   routing protocols are\
    \ implemented as one or several tasks.  A GateD\n   task is a collection of code\
    \ associated with a Unix socket.  The\n   socket is used for the input and output\
    \ requirements of the task.\n   The main loop of GateD contains, among other operations,\
    \ a select()\n   call over all task sockets to determine if any read/write or\
    \ error\n   conditions occurred in any of them.  GateD implements the OSPF link\n\
    \   state database using a radix tree for fast access to individual link\n   state\
    \ records.  In addition, link state records for neighboring\n   network elements\
    \ (such as adjacent routers) are linked together at\n   the database level with\
    \ pointers.  GateD maintains a single routing\n   table that contains routes discovered\
    \ by all the active routing\n   protocols.  Multiple routes to the same destination\
    \ are prioritized\n   according to a set of rules and administrative preferences\
    \ and only a\n   single route is active per destination.  These routes are\n \
    \  periodically downloaded in the host's kernel forwarding table.\n"
- title: 4.2. Implementing the QoS Extensions of OSPF
  contents:
  - '4.2. Implementing the QoS Extensions of OSPF

    '
- title: 4.2.1. Design Objectives and Scope
  contents:
  - "4.2.1. Design Objectives and Scope\n   One of our major design objectives was\
    \ to gain substantial experience\n   with a functionally complete QoS routing\
    \ implementation while\n   containing the overall implementation complexity. \
    \ Thus, our\n   architecture was modular and aimed at reusing the existing OSPF\
    \ code\n   with only minimal changes.  QoS extensions were localized to specific\n\
    \   modules and their interaction with existing OSPF code was kept to a\n   minimum.\
    \  Besides reducing the development and testing effort, this\n   approach also\
    \ facilitated experimentation with different alternatives\n   for implementing\
    \ the QoS specific features such as triggering\n   policies for link state updates\
    \ and QoS route table computation.\n   Several of the design choices were also\
    \ influenced by our assumptions\n   regarding the core functionalities that an\
    \ early prototype\n   implementation of QoS routing must demonstrate.  Some of\
    \ the\n   important assumptions/requirements are:\n   -  Support for only hop-by-hop\
    \ routing.  This affected the path\n      structure in the QoS routing table as\
    \ it only needs to store next\n      hop information.  As mentioned earlier, the\
    \ structure can be\n      easily extended to allow construction of explicit routes.\n\
    \   -  Support for path pre-computation.  This required the creation of a\n  \
    \    separate QoS routing table and its associated path structure, and\n     \
    \ was motivated by the need to minimize processing overhead.\n   -  Full integration\
    \ of the QoS extensions into the GateD framework,\n      including configuration\
    \ support, error logging, etc.  This was\n      required to ensure a fully functional\
    \ implementation that could be\n      used by others.\n   -  Ability to allow\
    \ experimentation with different approaches, e.g.,\n      use of different update\
    \ and pre-computation triggering policies\n      with support for selection and\
    \ parameterization of these policies\n      from the GateD configuration file.\n\
    \   -  Decoupling from local traffic and resource management components,\n   \
    \   i.e., packet classifiers and schedulers and local call admission.\n      This\
    \ is supported by providing an API between QoS routing and the\n      local traffic\
    \ management module, which hides all internal details\n      or mechanisms.  Future\
    \ implementations will be able to specify\n      their own mechanisms for this\
    \ module.\n   -  Interface to RSVP. The implementation assumes that RSVP [RZB+97]\n\
    \      is the mechanism used to request routes with specific QoS\n      requirements.\
    \  Such requests are communicated through an interface\n      based on [GKR97],\
    \ and used the RSVP code developed at ISI, version\n      4.2a2 [RZB+97].\n  \
    \ In addition, our implementation also relies on several of the\n   simplifying\
    \ assumptions made earlier in this document, namely:\n   -  The scope of QoS route\
    \ computation is currently limited to a\n      single area.\n   -  All routers\
    \ within the area are assumed to run a QoS enabled\n      version of OSPF, i.e.,\
    \ inter-operability with non-QoS aware\n      versions of the OSPF protocol is\
    \ not considered.\n   -  All interfaces on a router are assumed to be QoS capable.\n"
- title: 4.2.2. Architecture
  contents:
  - "4.2.2. Architecture\n   The above design decisions and assumptions resulted in\
    \ the\n   architecture shown in Figure 2.  It consists of three major\n   components:\
    \  the signaling component (RSVP in our case); the QoS\n   routing component;\
    \ and the traffic manager.  In the rest of this\n   section we concentrate on\
    \ the structure and operation of the QoS\n   routing component.  As can be seen\
    \ in Figure 2, the QoS routing\n   extensions are further divided into the following\
    \ modules:\n   -  Update trigger module determines when to advertise local link\n\
    \      state updates.  This module implements a variety of triggering\n      policies:\
    \  periodic, threshold based triggering, and class based\n      triggering.  This\
    \ module also implements a hold-down timer that\n      enforces minimum spacing\
    \ between two consecutive update\n      triggerings from the same node.\n   -\
    \  Pre-computation trigger module determines when to perform QoS path\n      pre-computation.\
    \  So far, this module implements only periodic\n      pre-computation triggering.\n\
    \   -  Path pre-computation module computes the QoS routing table based\n    \
    \  on the QoS specific link state information as described in Section\n      2.3.1.\n\
    \   -  Path selection and management module selects a path for a request\n   \
    \   with particular QoS requirements, and manages it once selected,\n      i.e.,\
    \ reacts to link or reservation failures.  Path selection is\n      performed\
    \ as described in Section 2.3.1.  Path management\n      functionality is not\
    \ currently supported.\n   -  QoS routing table module implements the QoS specific\
    \ routing\n      table, which is maintained independently of the other GateD\n\
    \      routing tables.\n   -  Tspec mapping module maps request requirements expressed\
    \ in the\n      form of RSVP Tspecs and Rspecs into the bandwidth requirements\n\
    \      that QoS routing uses.\n"
- title: 4.3. Major Implementation Issues
  contents:
  - "4.3. Major Implementation Issues\n   Mapping the above design to the framework\
    \ of the GateD implementation\n   of OSPF led to a number of issues and design\
    \ decisions.  These issues\n   mainly fell under two categories:  a) interoperation\
    \ of the QoS\n   extensions with pre-existing similar OSPF mechanisms, and b)\n\
    \   structure, placement, and organization of the QoS routing table.\n   Next,\
    \ we briefly discuss these issues and justify the resulting\n   design decisions.\n\
    \                    +--------------------------------------------------+\n  \
    \                  |              +-----------------------------+     |\n    \
    \                |              | QoS Route Table Computation |     |\n      \
    \              |              +-----------------------------+     |\n        \
    \            |                 |                    |           |\n          \
    \          |                 V                    |           |\n            \
    \        |  +-----------------+                 |           |\n       +-------------->|\
    \ QoS Route Table |                 |           |\n       |            |  +-----------------+\
    \                 |           |\n       |            |                       \
    \               |           |\n       |            |  +----------------------+\
    \     +---------------+  |\n       |            |  | Core OSPF Functions  |  \
    \   | Precomputation|  |\n       |            |  |        +             |    \
    \ | Trigger       |  |\n       |            |  | (Enhanced) Topology  |     +---------------+\
    \  |\n       |            |  | Data Base            |             |          |\n\
    \       |            |  +----------------------+             |          |\n  \
    \     |            |         |           |                 |          |\n    \
    \   |            |         |       +----------------------------+   |\n      \
    \ |            |         |       | Receive and update QoS-LSA |   |\n       |\
    \            |         |       +----------------------------+   |\n       |  \
    \          |         |                             |          |\n       |    \
    \        |         |                    +----------------+  |\n       |      \
    \      |         |                    | Local Interface|  |\n       |        \
    \    |         |                    | Status Monitor |  |\n       |          \
    \  |         |                    +----------------+  |\n"
- title: +----------------+  |         |                            |           |
  contents:
  - '+----------------+  |         |                            |           |

    '
- title: '| Path Selection |  |    +--------------+          +----------------+  |'
  contents:
  - '| Path Selection |  |    +--------------+          +----------------+  |

    '
- title: '| & Management   |  |    | Build and    |          | Link State     |  |'
  contents:
  - '| & Management   |  |    | Build and    |          | Link State     |  |

    '
- title: +----------------+  |    | Send QoS-LSA |----------| Update Trigger |  |
  contents:
  - "+----------------+  |    | Send QoS-LSA |----------| Update Trigger |  |\n  \
    \     |            |    +--------------+          +----------------+  |\n"
- title: +----------------+  |                                           |      |
  contents:
  - '+----------------+  |                                           |      |

    '
- title: '| QoS Parameter  |  |                                           |      |'
  contents:
  - '| QoS Parameter  |  |                                           |      |

    '
- title: '| Mapping        |  |        OSPF with QoS Routing Extensions   |      |'
  contents:
  - '| Mapping        |  |        OSPF with QoS Routing Extensions   |      |

    '
- title: '|----------------+  +-------------------------------------------|------+'
  contents:
  - "|----------------+  +-------------------------------------------|------+\n  \
    \     |                                                        |\n"
- title: +----------------+                                          +----------+
  contents:
  - '+----------------+                                          +----------+

    '
- title: '| QoS Route      |                                          | Local    |'
  contents:
  - '| QoS Route      |                                          | Local    |

    '
- title: '| Request Client |<---------------------------------------->| Resource |'
  contents:
  - '| Request Client |<---------------------------------------->| Resource |

    '
- title: '| (e.g. RSVP)    |                                          | Manager  |'
  contents:
  - '| (e.g. RSVP)    |                                          | Manager  |

    '
- title: +----------------+                                          +----------+
  contents:
  - "+----------------+                                          +----------+\n  \
    \                Figure 2: The software architecture\n   The ability to trigger\
    \ link state updates in response to changes in\n   bandwidth availability on interfaces\
    \ is an essential component of the\n   QoS extensions.  Mechanisms for triggering\
    \ these updates and\n   controlling their rate have been mentioned in Section\
    \ 2.2.  In\n   addition, OSPF implements its own mechanism for triggering link\
    \ state\n   updates as well as its own hold down timer, which may be incompatible\n\
    \   with what is used for the QoS link state updates.  We handle such\n   potential\
    \ conflicts as follows.  First, since OSPF triggers updates\n   on a periodic\
    \ basis with low frequency, we expect these updates to be\n   only a small part\
    \ of the total volume of updates generated.  As a\n   result, we chose to maintain\
    \ the periodic update triggering of OSPF.\n   Resolving conflicts in the settings\
    \ of the different hold down timer\n   settings requires more care.  In particular,\
    \ it is important to\n   ensure that the existing OSPF hold down timer does not\
    \ interfere with\n   QoS updates.  One option is to disable the existing OSPF\
    \ timer, but\n   protection against transient overloads calls for some hold down\n\
    \   timer, albeit with a small value.  As a result, the existing OSPF\n   hold\
    \ down timer was kept, but reduced its value to 1 second.  This\n   value is low\
    \ enough (actually is the lowest possible, since GateD\n   timers have a maximum\
    \ resolution of 1 second) so that it does not\n   interfere with the generation\
    \ of the QoS link state updates, which\n   will actually often have hold down\
    \ timers of their own with higher\n   values.  An additional complexity is that\
    \ the triggering of QoS link\n   state updates needs to be made aware of updates\
    \ performed by OSPF\n   itself.  This is necessary, as regular OSPF updates also\
    \ carry\n   bandwidth information, and this needs to be considered by QoS updates\n\
    \   to properly determine when to trigger a new link state update.\n   Another\
    \ existing OSPF mechanism that has the potential to interfere\n   with the extensions\
    \ needed for QoS routing, is the support for\n   delayed acknowledgments that\
    \ allows aggregation of acknowledgments\n   for multiple LSAs.  Since link state\
    \ updates are maintained in\n   retransmission queues until acknowledged, excessive\
    \ delay in the\n   generation of the acknowledgement combined with the increased\
    \ rates\n   of QoS updates may result in overflows of the retransmission queues.\n\
    \   To avoid these potential overflows, this mechanism was bypassed\n   altogether\
    \ and LSAs received from neighboring routers were\n   immediately acknowledged.\
    \  Another approach which was considered but\n   not implemented, was to make\
    \ QoS LSAs unreliable, i.e., eliminate\n   their acknowledgments, so as to avoid\
    \ any potential interference.\n   Making QoS LSAs unreliable would be a reasonable\
    \ design choice\n   because of their higher frequency compared to the regular\
    \ LSAs and\n   the reduced impact that the loss of a QoS LSA has on the protocol\n\
    \   operation.  Note that the loss of a QoS LSA does not interfere with\n   the\
    \ base operation of OSPF, and only transiently reduces the quality\n   of paths\
    \ discovered by QoS routing.\n   The structure and placement of the QoS routing\
    \ table also raises some\n   interesting implementation issues.  Pre-computed\
    \ paths are placed\n   into a QoS routing table.  This table is implemented as\
    \ a set of path\n   structures, one for each destination, which contain all the\
    \ available\n   paths to this destination.  In order to be able to efficiently\
    \ locate\n   individual path structures, an access structure is needed.  In order\n\
    \   to minimize the develpement effort, the radix tree structure used for\n  \
    \ the regular GateD routing tables was reused.  In addition, the QoS\n   routing\
    \ table was kept independent of the GateD routing tables to\n   conform to the\
    \ design goal of localizing changes and minimizing the\n   impact on the existing\
    \ OSPF code.  An additional reason for\n   maintaining the QoS routing separate\
    \ and self-contained is that it is\n   re-computed under conditions that are different\
    \ from those used for\n   the regular routing tables.\n   Furthermore, since the\
    \ QoS routing table is re-built frequently, it\n   must be organized so that its\
    \ computation is efficient.  A common\n   operation during the computation of\
    \ the QoS routing table is mapping\n   a link state database entry to the corresponding\
    \ path structure.  In\n   order to make this operation efficient, the link state\
    \ database\n   entries were extended to contain a pointer to the corresponding\
    \ path\n   structure.  In addition, when a new QoS routing table is to be\n  \
    \ computed, the previous one must be de-allocated.  This is\n   accomplished by\
    \ traversing the radix tree in-order, and de-allocating\n   each node in the tree.\
    \  This full de-allocation of the QoS routing\n   table is potentially wasteful,\
    \ especially since memory allocation and\n   de-allocation is an expensive operation.\
    \  Furthermore, because path\n   pre-computations are typically not triggered\
    \ by changes in topology,\n   the set of destinations will usually remain the\
    \ same and correspond\n   to an unchanged radix tree.  A natural optimization\
    \ would then be to\n   de-allocate only the path structures and maintain the radix\
    \ tree.  A\n   further enhancement would be to maintain the path structures as\
    \ well,\n   and attempt to incrementally update them only when required.\n   However,\
    \ despite the potential gains, these optimizations have not\n   been included\
    \ in the initial implementation.  The main reason is that\n   they involve subtle\
    \ and numerous checks to ensure the integrity of\n   the overall data structure\
    \ at all times, e.g., correctly remove\n   failed destinations from the radix\
    \ tree and update the tree\n   accordingly.\n"
- title: 4.4. Bandwidth and Processing Overhead of QoS Routing
  contents:
  - "4.4. Bandwidth and Processing Overhead of QoS Routing\n   After completing the\
    \ implementation outlined in the previous\n   sections, it was possible to perform\
    \ an experimental study of the\n   cost and nature of the overhead of the QoS\
    \ routing extensions\n   proposed in this document.  In particular, using a simple\
    \ setup\n   consisting of two interconnected routers, it is possible to measure\n\
    \   the cost of individual QoS routing related operations.  These\n   operations\
    \ are:  a) computation of the QoS routing table, b)\n   selection of a path from\
    \ the QoS routing table, c) generation of a\n   link state update, and d) reception\
    \ of a link state update.  Note\n   that the last two operations are not really\
    \ specific to QoS routing\n   since regular OSPF also performs them.  Nevertheless,\
    \ we expect the\n   more sensitive update triggering mechanisms required for effective\n\
    \   QoS routing to result in increased number of updates, making the cost\n  \
    \ of processing updates an important component of the QoS routing\n   overhead.\
    \  An additional cost dimension is the memory required for\n   storing the QoS\
    \ routing table.  Scaling of the above costs with\n   increasing sizes of the\
    \ topology database was investigated by\n   artificially populating the topology\
    \ databases of the routers under\n   measurement.\n   Table 1 shows how the measured\
    \ costs depend on the size of the\n   topology.  The topology used in the measurements\
    \ was built by\n   replicating a basic building block consisting of four routers\n\
    \   connected with transit networks in a rectangular arrangement.  The\n   details\
    \ of the topology and the measurements can be found in [AGK99].\n   The system\
    \ running the GateD software was an IBM IntelliStation Z Pro\n   with a Pentium\
    \ Pro processor at 200 MHz, 64 MBytes or real memory,\n   running FreeBSD 2.2.5-RELEASE\
    \ and GateD 4.  From the results of Table\n   1, one can observe that the cost\
    \ of path pre-computation is not much\n   higher than that of the regular SPF\
    \ computation.  However, path pre-\n   computation may need to be performed much\
    \ more often than the SPF\n   computation, and this can potentially lead to higher\
    \ processing\n   costs.  This issue was investigated in a set of subsequent\n\
    \   experiments, that are described later in this section.  The other\n   cost\
    \ components reported in Table 1 include memory, and it can be\n   seen that the\
    \ QoS routing table requires roughly 80% more memory than\n   the regular routing\
    \ table.  Finally, the cost of selecting a path is\n   found to be very small\
    \ compared to the path pre-computation times.\n   As expected, all the measured\
    \ quantities increase as the size of the\n   topology increases.  In particular,\
    \ the storage requirements and the\n   processing costs for both SPF computation\
    \ and QoS path pre-\n   computation scale almost linearly with the network size.\n"
- title: ________________________________________________________________________
  contents:
  - '________________________________________________________________________

    '
- title: '|Link_state_database_size_______|_25_|__49_|__81__|__121_|__169_|__225_|'
  contents:
  - '|Link_state_database_size_______|_25_|__49_|__81__|__121_|__169_|__225_|

    '
- title: '|Regular_SPF_time_(microsec)____|215_|_440_|_747__|_1158_|_1621_|_2187_|'
  contents:
  - '|Regular_SPF_time_(microsec)____|215_|_440_|_747__|_1158_|_1621_|_2187_|

    '
- title: '|Pre-computation_time_(microsec)|736_|_1622|_2883_|_4602_|_6617_|_9265_|'
  contents:
  - '|Pre-computation_time_(microsec)|736_|_1622|_2883_|_4602_|_6617_|_9265_|

    '
- title: '|SPF_routing_table_size_(bytes)_|2608|_4984|_8152_|_12112|_16864|_22408|'
  contents:
  - '|SPF_routing_table_size_(bytes)_|2608|_4984|_8152_|_12112|_16864|_22408|

    '
- title: '|QoS_routing_table_size_(bytes)_|3924|_7952|_13148|_19736|_27676|_36796|'
  contents:
  - '|QoS_routing_table_size_(bytes)_|3924|_7952|_13148|_19736|_27676|_36796|

    '
- title: '|Path_Selection_time_(microsec)_|_.7_|_1.6_|__2.8_|__4.6_|__6.6_|__9.2_|'
  contents:
  - "|Path_Selection_time_(microsec)_|_.7_|_1.6_|__2.8_|__4.6_|__6.6_|__9.2_|\n  \
    \               Table 1: Stand alone QoS routing costs\n   In addition to the\
    \ stand alone costs reported in Table 1, it is\n   important to assess the actual\
    \ operational load induced by QoS\n   routing in the context of a large network.\
    \  Since it is not practical\n   to reproduce a large scale network in a lab setting,\
    \ the approach\n   used was to combine simulation and measurements.  Specifically,\
    \ a\n   simulation was used to obtain a time stamped trace of QoS routing\n  \
    \ related events that occur in a given router in a large scale network.\n   The\
    \ trace was then used to artificially induce similar load\n   conditions on a\
    \ real router and its adjacent links.  In particular,\n   it was used to measure\
    \ the processing load at the router and\n   bandwidth usage that could be attributed\
    \ to QoS updates.  A more\n   complete discussion of the measurement method and\
    \ related\n   considerations can be found in [AGK99].\n   The use of a simulation\
    \ further allows the use of different\n   configurations, where network topology\
    \ is varied together with other\n   QoS parameters such as a) period of pre-computation,\
    \ and b) threshold\n   for triggering link state updates.  The results reported\
    \ here were\n   derived using two types of topologies.  One based on a regular\
    \ but\n   artificial 8x8 mesh network, and another (isp) which has been used in\n\
    \   several previous studies [AGKT98, AT98] and that approximates the\n   network\
    \ of a nation-wide ISP. As far as pre-computation periods are\n   concerned, three\
    \ values of 1, 5 and 50 seconds were chosen, and for\n   the triggering of link\
    \ state update thresholds of 10% and 80% were\n   used.  These values were selected\
    \ as they cover a wide range in terms\n   of precision of pre-computed paths and\
    \ accuracy of the link state\n   information available at the routers.  Also note\
    \ that 1 second is the\n   smallest pre-computation period allowed by GateD.\n\
    \   Table 2 provides results on the processing load at the router driven\n   by\
    \ the simulation trace, for the two topologies and different\n   combinations\
    \ of QoS parameters, i.e., pre-computation period and\n   threshold for triggering\
    \ link state updates.  Table 3 gives the\n   bandwidth consumption of QoS updates\
    \ on the links adjacent to the\n   router.\n    ________________________________________________________________\n\
    \    |_____________________|_________Pre-computation_Period_________|\n    |Link_state_threshold_|___1_sec____|____5_sec____|____50_sec___|\n\
    \    |_________10%_________|.45%_(1.6%)_|__.29%_(2%)__|__.17%_(3%)__|\n    |_________80%_________|.16%_(2.4%)_|__.04%_(3%)__|_.02%_(3.8%)_|\n\
    \                                  isp\n    ________________________________________________________________\n\
    \    |_________10%_________|3.37%_(2.1%)|_2.23%_(3.3%)|_1.78%_(7.7%)|\n    |_________80%_________|1.54%_(5.4%)|_.42%_(6.6%)_|_.14%_(10.4%)|\n\
    \                               8x8 mesh\n       Table 2: Router processing load\
    \ and (bandwidth blocking).\n   In Table 2, processing load is expressed as the\
    \ percentage of the\n   total CPU resources that are consumed by GateD processing.\
    \  The same\n   table also shows the routing performance that is achieved for\
    \ each\n   combination of QoS parameters, so that comparison of the different\n\
    \   processing cost/routing performance trade-offs can be made.  Routing\n   performance\
    \ is measured using the bandwidth blocking ratio, defined\n   as the sum of requested\
    \ bandwidth of the requests that were rejected\n   over the total offered bandwidth.\
    \  As can be seen from Table 2,\n   processing load is low even when the QoS routing\
    \ table is recomputed\n   every second, and LSAs are generated every time the\
    \ available\n   bandwidth on a link changes by more than 10% of the last advertised\n\
    \   value.  This seems to indicate that given today's processor\n   technology,\
    \ QoS routing should not be viewed as a costly enhancement,\n   at least not in\
    \ terms of its processing requirements.  Another\n   general observation is that\
    \ while network size has obviously an\n   impact, it does not seem to drastically\
    \ affect the relative influence\n   of the different parameters.  In particular,\
    \ despite the differences\n   that exist between the isp and mesh topologies,\
    \ changing the pre-\n   computation period or the update threshold translates\
    \ into\n   essentially similar relative changes.\n   Similar conclusions can be\
    \ drawn for the update traffic shown in\n   Table 3.  In all cases, this traffic\
    \ is only a small fraction of the\n   link's capacity.  Clearly, both the router\
    \ load and the link\n   bandwidth consumption depend on the router and link that\
    \ was the\n   target of the measurements and will vary for different choices.\
    \  The\n   results shown here are meant to be indicative, and a more complete\n\
    \   discussion can be found in [AGK99].\n                _______________________________________\n\
    \                |_Link_state_threshold_|_______________|\n                |_________10%__________|3112_bytes/sec_|\n\
    \                |_________80%__________|177_bytes/sec__|\n                  \
    \                isp\n                ________________________________________\n\
    \                |_________10%__________|15438_bytes/sec_|\n                |_________80%__________|1053_bytes/sec__|\n\
    \                               8x8 mesh\n                   Table 3: Link state\
    \ update traffic\n   Summarizing, by carrying out the implementation of the proposed\
    \ QoS\n   routing extensions to OSPF we demonstrated that such extensions are\n\
    \   fairly straightforward to implement.  Furthermore, by measuring the\n   performance\
    \ of the real system we were able to demonstrate that the\n   overheads associated\
    \ with QoS routing are not excessive, and are\n   definitely within the capabilities\
    \ of modern processor and\n   workstation technology.\n"
- title: 5. Security Considerations
  contents:
  - "5. Security Considerations\n   The QoS extensions proposed in this document do\
    \ not raise any\n   security considerations that are in addition to the ones associated\n\
    \   with regular OSPF. The security considerations of OSPF are presented\n   in\
    \ [Moy98].  However, it should be noted that this document assumes\n   the availability\
    \ of some entity responsible for assessing the\n   legitimacy of QoS requests.\
    \  For example, when the protocol used for\n   initiating QoS requests is the\
    \ RSVP protocol, this capability can be\n   provided through the use of RSVP Policy\
    \ Decision Points and Policy\n   Enforcement Points as described in [YPG97]. \
    \ Similarly, a policy\n   server enforcing the acceptability of QoS requests by\
    \ implementing\n   decisions based on the rules and languages of [RMK+98], would\
    \ also be\n   capable of providing the desired functionality.\n"
- title: APPENDICES
  contents:
  - 'APPENDICES

    '
- title: A. Pseudocode for the BF Based Pre-Computation Algorithm
  contents:
  - "A. Pseudocode for the BF Based Pre-Computation Algorithm\n   Note:  The pseudocode\
    \ below assumes a hop-by-hop forwarding approach\n   in updating the neighbor\
    \ field.  The modifications needed to support\n   explicit route construction\
    \ are straightforward.  The pseudocode also\n   does not handle equal cost multi-paths\
    \ for simplicity, but the\n   modification needed to add this support are straightforward.\n"
- title: 'Input:'
  contents:
  - "Input:\n  V = set of vertices, labeled by integers 1 to N.\n  L = set of edges,\
    \ labeled by ordered pairs (n,m) of vertex labels.\n  s = source vertex (at which\
    \ the algorithm is executed).\n  For all edges (n,m) in L:\n    * b(n,m) = available\
    \ bandwidth (according to last received update)\n    on interface associated with\
    \ the edge between vertices n and m.\n    * If(n,m) outgoing interface corresponding\
    \ to edge (n,m) when n is\n      a router.\n  H = maximum hop-count (at most the\
    \ graph diameter).\n"
- title: 'Type:'
  contents:
  - "Type:\n  tab_entry: record\n                 bw = integer,\n                \
    \ neighbor = integer 1..N.\n"
- title: 'Variables:'
  contents:
  - "Variables:\n  TT[1..N, 1..H]: topology table, whose (n,h) entry is a tab_entry\n\
    \                  record, such that:\n                    TT[n,h].bw is the maximum\
    \ available bandwidth (as\n                      known thus far) on a path of\
    \ at most h hops\n                      between vertices s and n,\n          \
    \          TT[n,h].neighbor is the first hop on that path (a\n               \
    \       neighbor of s). It is either a router or the\n                      destination\
    \ n.\n  S_prev: list of vertices that changed a bw value in the TT table\n   \
    \       in the previous iteration.\n  S_new: list of vertices that changed a bw\
    \ value (in the TT table\n          etc.) in the current iteration.\n"
- title: 'The Algorithm:'
  contents:
  - 'The Algorithm:

    '
- title: begin;
  contents:
  - "begin;\n  for n:=1 to N do  /* initialization */\n  begin;\n    TT[n,0].bw :=\
    \ 0;\n    TT[n,0].neighbor := null\n    TT[n,1].bw := 0;\n    TT[n,1].neighbor\
    \ := null\n  end;\n  TT[s,0].bw := infinity;\n  reset S_prev;\n  for all neighbors\
    \ n of s do\n  begin;\n    TT[n,1].bw := max( TT[n,1].bw, b[s,n]);\n    if (TT[n,1].bw\
    \ = b[s,n]) then TT[n,1].neighbor := If(s,n);\n             /* need to make sure\
    \ we are picking the maximum */\n             /* bandwidth path for routers that\
    \ can be reached */\n             /* through both networks and point-to-point\
    \ links */\n       if (n is a router) then\n           S_prev :=  S_prev union\
    \ {n}\n             /* only a router is added to S_prev, */\n             /* if\
    \ it is not already included in S_prev */\n       else     /* n is a network:\
    \ */\n             /* proceed with network--router edges, without */\n       \
    \      /* counting another hop */\n          for all (n,k) in L, k <> s, do\n\
    \             /* i.e., for all other neighboring routers of n */\n          begin;\n\
    \          TT[k,1].bw := max( min( TT[n,1].bw, b[n,k]), TT[k,1].bw );\n      \
    \       /* In case k could be reached through another path */\n             /*\
    \ (a point-to-point link or another network) with */\n             /* more bandwidth,\
    \ we do not want to update TT[k,1].bw */\n          if (min( TT[n,1].bw, b[n,k])\
    \ = TT[k,1].bw )\n             /* If we have updated TT[k,1].bw by going through\
    \ */\n             /* network n  */\n          then TT[k,1].neighbor := If(s,n);\n\
    \             /* neighbor is interface to network n */\n          if ( {k} not\
    \ in S_prev) then S_prev :=  S_prev union {k}\n             /* only routers are\
    \ added to S_prev, but we again need */\n             /* to check they are not\
    \ already included in S_prev */\n          end\n  end;\n  for h:=2 to H do   /*\
    \ consider all possible number of hops */\n  begin;\n    reset S_new;\n    for\
    \ all vertices m in V do\n    begin;\n      TT[m,h].bw := TT[m,h-1].bw;\n    \
    \  TT[m,h].neighbor := TT[m,h-1].neighbor\n    end;\n    for all vertices n in\
    \ S_prev do\n             /* as shall become evident, S_prev contains only routers\
    \ */\n    begin;\n      for all edges (n,m) in L do\n      if min( TT[n,h-1].bw,\
    \ b[n,m]) > TT[m,h].bw then\n      begin;\n        TT[m,h].bw := min( TT[n,h-1].bw,\
    \ b[n,m]);\n        TT[m,h].neighbor := TT[n,h-1].neighbor;\n        if m is a\
    \ router then S_new :=  S_new union {m}\n             /* only routers are added\
    \ to S_new */\n        else /* m is a network: */\n             /* proceed with\
    \ network--router edges, without counting */\n             /* them as another\
    \ hop */\n        for all (m,k) in L, k <> n,\n             /* i.e., for all other\
    \ neighboring routers of m */\n        if min( TT[m,h].bw, b[m,k]) > TT[k,h].bw\
    \ then\n        begin;\n             /* Note: still counting it as the h-th hop,\
    \ as (m,k) is */\n             /* a network--router edge */\n          TT[k,h].bw\
    \ := min( TT[m,h].bw, b[m,k]);\n          TT[k,h].neighbor := TT[m,h].neighbor;\n\
    \          S_new :=  S_new union {k}\n             /* only routers are added to\
    \ S_new */\n        end\n      end\n    end;\n    S_prev := S_new;\n         \
    \   /* the two lists can be handled by a toggle bit */\n    if S_prev=null then\
    \ h=H+1   /* if no changes then exit */\n  end;\n"
- title: end.
  contents:
  - 'end.

    '
- title: B. On-Demand Dijkstra Algorithm for QoS Path Computation
  contents:
  - "B. On-Demand Dijkstra Algorithm for QoS Path Computation\n   In the main text,\
    \ we described an algorithm that allows pre-\n   computation of QoS routes.  However,\
    \ it may be feasible in some\n   instances, e.g., limited number of requests for\
    \ QoS routes, to\n   instead perform such computations on-demand, i.e., upon receipt\
    \ of a\n   request for a QoS route.  The benefit of such an approach is that\n\
    \   depending on how often recomputation of pre-computed routes is\n   triggered,\
    \ on-demand route computation can yield better routes by\n   using the most recent\
    \ link metrics available.  Another benefit of\n   on-demand path computation is\
    \ the associated storage saving, i.e.,\n   there is no need for a QoS routing\
    \ table.  This is essentially the\n   standard trade-off between memory and processing\
    \ cycles.\n   In this section, we briefly describe how a standard Dijkstra\n \
    \  algorithm can, for a given destination and bandwidth requirement,\n   generate\
    \ a minimum hop path that can accommodate the required\n   bandwidth and also\
    \ has maximum bandwidth.  Because the Dijkstra\n   algorithm is already used in\
    \ the current OSPF route computation, only\n   differences from the standard algorithm\
    \ are described.  Also, while\n   for simplicity we do not consider here zero-hop\
    \ edges, the\n   modification required for supporting them is straightforward.\n\
    \   The algorithm essentially performs a minimum hop path computation, on\n  \
    \ a graph from which all edges, whose available bandwidth is less than\n   that\
    \ requested by the flow triggering the computation, have been\n   removed.  This\
    \ can be performed either through a pre-processing step,\n   or while running\
    \ the algorithm by checking the available bandwidth\n   value for any edge that\
    \ is being considered (see the pseudocode that\n   follows).  Another modification\
    \ to a standard Dijkstra based minimum\n   hop count path computation, is that\
    \ the list of equal cost next\n   (previous) hops which is maintained as the algorithm\
    \ proceeds, needs\n   to be sorted according to available bandwidth.  This is\
    \ to allow\n   selection of the minimum hop path with maximum available bandwidth.\n\
    \   Alternatively, the algorithm could also be modified to, at each step,\n  \
    \ only keep among equal hop count paths the one with maximum available\n   bandwidth.\
    \  This would essentially amount to considering a cost that\n   is function of\
    \ both hop count and available bandwidth.\n   Note:  The pseudocode below assumes\
    \ a hop-by-hop forwarding approach\n   in updating the neighbor field.  Addition\
    \ of routes to stub networks\n   is done in a second phase as usual.  The modifications\
    \ needed to\n   support explicit route construction are straightforward.  The\n\
    \   pseudocode also does not handle equal cost multi-paths for\n   simplicity,\
    \ but the modifications needed to add this support are also\n   easily done.\n"
- title: 'Input:'
  contents:
  - "Input:\n  V = set of vertices, labeled by integers 1 to N.\n  L = set of edges,\
    \ labeled by ordered pairs (n,m) of vertex labels.\n  s = source vertex (at which\
    \ the algorithm is executed).\n  For all edges (n,m) in L:\n    * b(n,m) = available\
    \ bandwidth (according to last received update)\n    on interface associated with\
    \ the edge between vertices n and m.\n    * If(n,m) = outgoing interface corresponding\
    \ to edge (n,m) when n is\n      a router.\n  d = destination vertex.\n  B = requested\
    \ bandwidth for the flow served.\n"
- title: 'Type:'
  contents:
  - "Type:\n  tab_entry: record\n                 hops = integer,\n              \
    \   neighbor = integer 1..N,\n                 ontree = boolean.\n"
- title: 'Variables:'
  contents:
  - "Variables:\n  TT[1..N]: topology table, whose (n) entry is a tab_entry\n    \
    \              record, such that:\n                    TT[n].bw is the available\
    \ bandwidth (as known\n                        thus far) on a shortest-path between\n\
    \                        vertices s and n,\n                    TT[n].neighbor\
    \ is the first hop on that path (a\n                        neighbor of s). It\
    \ is either a router or the\n                        destination n.\n  S: list\
    \ of candidate vertices;\n  v: vertex under consideration;\n"
- title: 'The Algorithm:'
  contents:
  - 'The Algorithm:

    '
- title: begin;
  contents:
  - "begin;\n  for n:=1 to N do  /* initialization */\n  begin;\n    TT[n].hops :=\
    \ infinity;\n    TT[n].neighbor := null;\n    TT[n].ontree := FALSE;\n  end;\n\
    \  TT[s].hops := 0;\n  reset S;\n  v:= s;\n  while v <> d do\n  begin;\n    TT[v].ontree\
    \ := TRUE;\n    for all edges (v,m) in L and b(v,m) >= B do\n    begin;\n    \
    \  if m is a router\n      begin;\n        if not TT[m].ontree then\n        begin;\n\
    \          /* bandwidth must be fulfilled since all links >= B */\n          if\
    \ TT[m].hops > TT[v].hops + 1 then\n          begin\n            S := S union\
    \ { m };\n            TT[m].hops := TT[v].hops + 1;\n            TT[m].neighbor\
    \ := v;\n          end;\n        end;\n      end;\n      else /* must be a network,\
    \ iterate over all attached routers */\n      begin; /* each network -- router\
    \ edge treated as zero hop edge */\n        for all (m,k) in L, k <> v,\n    \
    \         /* i.e., for all other neighboring routers of m */\n        if not TT[k].ontree\
    \ and b(m,k) >= B then\n        begin;\n          if TT[k].hops > TT[v].hops \
    \ then\n          begin;\n            S := S union { k };\n            TT[k].hops\
    \ := TT[v].hops;\n            TT[k].neighbor := v;\n          end;\n        end;\n\
    \      end;\n    end; /* of all edges from the vertex under consideration */\n\
    \    if S is empty then\n    begin;\n      v=d; /* which will end the algorithm\
    \ */\n    end;\n    else\n    begin;\n      v := first element of S;\n      S\
    \ := S - {v}; /* remove and store the candidate to consider */\n    end;\n  end;\
    \ /* from processing of the candidate list */\n"
- title: end.
  contents:
  - 'end.

    '
- title: C. Precomputation Using Dijkstra Algorithm
  contents:
  - "C. Precomputation Using Dijkstra Algorithm\n   This appendix outlines a Dijkstra-based\
    \ algorithm that allows pre-\n   computation of QoS routes for all destinations\
    \ and bandwidth values.\n   The benefit of using a Dijkstra-based algorithm is\
    \ a greater synergy\n   with existing OSPF implementations.  The solution to compute\
    \ all\n   \"best\" paths is to consecutively compute shortest path spanning trees\n\
    \   starting from a complete graph and removing links with less bandwidth\n  \
    \ than the threshold used in the previous computation.  This yields\n   paths\
    \ with possibly better bandwidth but of course more hops.\n   Despite the large\
    \ number of Dijkstra computations involved, several\n   optimizations such as\
    \ incremental spanning tree computation can be\n   used and allow for efficient\
    \ implementations in terms of complexity\n   as well as storage since the structure\
    \ of computed paths leans itself\n   towards path compression [ST83].  Details\
    \ including measurements and\n   applicability studies can be found in [Prz95]\
    \ and [BP95].\n   A variation of this theme is to trade the \"accuracy\" of the\
    \ pre-\n   computed paths, (i.e., the paths being generated may be of a larger\n\
    \   hop count than needed) for the benefit of using a modified version of\n  \
    \ Dijkstra shortest path algorithm and also saving on some\n   computations. \
    \ This loss in accuracy comes from the need to rely on\n   quantized bandwidth\
    \ values, which are used when computing a minimum\n   hop count path.  In other\
    \ words, the range of possible bandwidth\n   values that can be requested by a\
    \ new flow is mapped into a fixed\n   number of quantized values, and minimum\
    \ hop count paths are generated\n   for each quantized value.  For example, one\
    \ could assume that\n   bandwidth values are quantized as low, medium, and high,\
    \ and minimum\n   hop count paths are computed for each of these three values.\
    \  A new\n   flow is then assigned to the minimum hop path that can carry the\n\
    \   smallest quantized value, i.e., low, medium, or high, larger than or\n   equal\
    \ to what it requested.  We restrict our discussion here to this\n   \"quantized\"\
    \ version of the algorithm.\n   Here too, we discuss the elementary case where\
    \ all edges count as\n   \"hops\", and note that the modification required for\
    \ supporting zero-\n   hop edges is straightforward.\n   As with the BF algorithm,\
    \ the algorithm relies on a routing table\n   that gets built as the algorithm\
    \ progresses.  The structure of the\n   routing table is as follows:\n"
- title: 'The QoS routing table:'
  contents:
  - "The QoS routing table:\n   -  a K x Q matrix, where K is the number of vertices\
    \ and Q is the\n      number of quantized bandwidth values.\n   -  The (n;q) entry\
    \ contains information that identifies the minimum\n      hop count path to destination\
    \ n, which is capable of accommodating\n      a bandwidth request of at least\
    \ bw[q] (the qth quantized bandwidth\n      value).  It consists of two fields:\n\
    \      *  hops:  the minimal number of hops on a path between the source\n   \
    \      node and destination n, which can accommodate a request of at\n       \
    \  least bw[q] units of bandwidth.\n      *  neighbor:  this is the routing information\
    \ associated with the\n         minimum hop count path to destination node n,\
    \ whose available\n         bandwidth is at least bw[q].  With a hop-by-hop routing\n\
    \         approach, the neighbor information is simply the identity of\n     \
    \    the node adjacent to the source node on that path.\n   The algorithm operates\
    \ again on a directed graph where vertices\n   correspond to routers and transit\
    \ networks.  The metric associated\n   with each edge in the graph is as before\
    \ the bandwidth available on\n   the corresponding interface, where b(n;m) is\
    \ the available bandwidth\n   on the edge between vertices n and m.  The vertex\
    \ corresponding to\n   the router where the algorithm is being run is selected\
    \ as the source\n   node for the purpose of path selection, and the algorithm\
    \ proceeds to\n   compute paths to all other nodes (destinations).\n   Starting\
    \ with the highest quantization index, Q, the algorithm\n   considers the indices\
    \ consecutively, in decreasing order.  For each\n   index q, the algorithm deletes\
    \ from the original network topology all\n   links (n;m) for which b(n;m) < bw[q],\
    \ and then runs on the remaining\n   topology a Dijkstra-based minimum hop count\
    \ algorithm  (10) between\n   the source node and all other nodes (vertices) in\
    \ the graph.  Note\n   that as with the Dijkstra used for on-demand path computation,\
    \ the\n   elimination of links such that b(n;m) < bw[q] could also be performed\n\
    \   while running the algorithm.\n   After the algorithm terminates, the q-th\
    \ column in the routing table\n   is updated.  This amounts to recording in the\
    \ hops field the hop\n   count value of the path that was generated by the algorithm,\
    \ and by\n   updating the neighbor field.  As before, the update of the neighbor\n\
    \   field depends on the scope of the path computation.  In the case of a\n  \
    \ hop-by-hop routing decision, the neighbor field is set to the\n   identity of\
    \ the node adjacent to the source node (next hop) on the\n   path returned by\
    \ the algorithm.  However, note that in order to\n   ensure that the path with\
    \ the maximal available bandwidth is always\n   chosen among all minimum hop paths\
    \ that can accommodate a given\n   quantized bandwidth, a slightly different update\
    \ mechanism of the\n   neighbor field needs to be used in some instances.  Specifically,\n\
    \   when for a given row, i.e., destination node n, the value of the hops\n  \
    \ field in column q is found equal to the value in column q+1 (here we\n   assume\
    \ q<Q), i.e., paths that can accommodate bw[q] and bw[q+ 1] have\n   the same\
    \ hop count, then the algorithm copies the value of the\n   neighbor field from\
    \ entry (n;q+1) into that of entry (n;q).\n   Note:  The pseudocode below assumes\
    \ a hop-by-hop forwarding approach\n   in updating the neighbor field.  The modifications\
    \ needed to support\n   explicit route construction are straightforward.  The\
    \ pseudocode also\n   does not handle equal cost multi-paths for simplicity, but\
    \ the\n   modification needed to add this support have been described above.\n\
    \   Details of the post-processing step of adding stub networks are\n   omitted.\n"
- title: 'Input:'
  contents:
  - "Input:\n  V = set of vertices, labeled by integers 1 to N.\n  L = set of edges,\
    \ labeled by ordered pairs (n,m) of vertex labels.\n  s = source vertex (at which\
    \ the algorithm is executed).\n  bw[1..Q] = array of bandwidth values to \"quantize\"\
    \ flow requests to.\n  For all edges (n,m) in L:\n    * b(n,m) = available bandwidth\
    \ (according to last received update)\n    on interface associated with the edge\
    \ between vertices n and m.\n    * If(n,m) = outgoing interface corresponding\
    \ to edge (n,m) when n is\n      a router.\n"
- title: 'Type:'
  contents:
  - "Type:\n  tab_entry: record\n                 hops = integer,\n              \
    \   neighbor = integer 1..N,\n                 ontree = boolean.\n"
- title: 'Variables:'
  contents:
  - "Variables:\n  TT[1..N, 1..Q]: topology table, whose (n,q) entry is a tab_entry\n\
    \                  record, such that:\n                    TT[n,q].bw is the available\
    \ bandwidth (as known\n                        thus far) on a shortest-path between\n\
    \                        vertices s and n accommodating bandwidth b(q),\n    \
    \                TT[n,q].neighbor is the first hop on that path (a\n         \
    \               neighbor of s). It is either a router or the\n               \
    \         destination n.\n  S: list of candidate vertices;\n  v: vertex under\
    \ consideration;\n  q: \"quantize\" step\n"
- title: 'The Algorithm:'
  contents:
  - 'The Algorithm:

    '
- title: begin;
  contents:
  - "begin;\n  for r:=1 to Q do\n  begin;\n    for n:=1 to N do  /* initialization\
    \ */\n    begin;\n      TT[n,r].hops     := infinity;\n      TT[n,r].neighbor\
    \ := null;\n      TT[n,r].ontree   := FALSE;\n    end;\n    TT[s,r].hops := 0;\n\
    \  end;\n  for r:=1 to Q do\n  begin;\n    S = {s};\n    while S not empty do\n\
    \    begin;\n      v := first element of S;\n      S := S - {v}; /* remove and\
    \ store the candidate to consider */\n      TT[v,r].ontree := TRUE;\n      for\
    \ all edges (v,m) in L and b(v,m) >= bw[r] do\n      begin;\n        if m is a\
    \ router\n        begin;\n          if not TT[m,r].ontree then\n          begin;\n\
    \            /* bandwidth must be fulfilled since all links >= bw[r] */\n    \
    \        if TT[m,r].hops > TT[v,r].hops + 1 then\n            begin\n        \
    \      S := S union { m };\n              TT[m,r].hops := TT[v,r].hops + 1;\n\
    \              TT[m,r].neighbor := v;\n            end;\n          end;\n    \
    \    end;\n        else /* must be a network, iterate over all attached\n    \
    \            routers */\n        begin;\n          for all (m,k) in L, k <> v,\n\
    \               /* i.e., for all other neighboring routers of m */\n         \
    \ if not TT[k,r].ontree and b(m,k) >= bw[r] then\n          begin;\n         \
    \   if TT[k,r].hops > TT[v,r].hops + 2 then\n            begin;\n            \
    \  S := S union { k };\n              TT[k,r].hops := TT[v,r].hops + 2;\n    \
    \          TT[k,r].neighbor := v;\n            end;\n          end;\n        end;\n\
    \      end; /* of all edges from the vertex under consideration */\n    end; /*\
    \ from processing of the candidate list */\n  end; /* of \"quantize\" steps */\n"
- title: end.
  contents:
  - 'end.

    '
- title: D. Explicit Routing Support
  contents:
  - "D. Explicit Routing Support\n   As mentioned before, the scope of the path selection\
    \ process can\n   range from simply returning the next hop on the QoS path selected\
    \ for\n   the flow, to specifying the complete path that was computed, i.e., an\n\
    \   explicit route.  Obviously, the information being returned by the\n   path\
    \ selection algorithm differs in these two cases, and constructing\n   it imposes\
    \ different requirements on the path computation algorithm\n   and the data structures\
    \ it relies on.  While the presentation of the\n   path computation algorithms\
    \ focused on the hop-by-hop routing\n   approach, the same algorithms can be applied\
    \ to generate explicit\n   routes with minor modifications.  These modifications\
    \ and how they\n   facilitate constructing explicit routes are discussed next.\n\
    \   The general approach to facilitate construction of explicit routes is\n  \
    \ to update the neighbor field differently from the way it is done for\n   hop-by-hop\
    \ routing as described in Section 2.3.  Recall that in the\n   path computation\
    \ algorithms the neighbor field is updated to reflect\n   the identity of the\
    \ router adjacent to the source node on the partial\n   path computed.  This facilitates\
    \ returning the next hop at the source\n   for the specific path.  In the context\
    \ of explicit routing, the\n   neighbor information is updated to reflect the\
    \ identity of the\n   previous router on the path.\n   In general, there can be\
    \ multiple equivalent paths for a given hop\n   count.  Thus, the neighbor information\
    \ is stored as a list rather\n   than single value.  Associated with each neighbor,\
    \ additional\n   information is stored to facilitate load balancing among these\n\
    \   multiple paths at the time of path selection.  Specifically, we store\n  \
    \ the advertised available bandwidth on the link from the neighbor to\n   the\
    \ destination in the entry.\n   With this change, the basic approach used to extract\
    \ the complete\n   list of vertices on a path from the neighbor information in\
    \ the QoS\n   routing table is to proceed `recursively' from the destination to\
    \ the\n   origin vertex.  The path is extracted by stepping through the\n   precomputed\
    \ QoS routing table from vertex to vertex, and identifying\n   at each step the\
    \ corresponding neighbor (precursor) information.  The\n   process is described\
    \ as recursive since the neighbor node identified\n   in one step becomes the\
    \ destination node for table look up in the\n   next step.  Once the source router\
    \ is reached, the concatenation of\n   all the neighbor fields that have been\
    \ extracted forms the desired\n   explicit route.  This applies to algorithms\
    \ of Section 2.3.1 and\n   Appendix C.  If at a particular stage there are multiple\
    \ neighbor\n   choices (due to equal cost multi-paths), one of them can be chosen\
    \ at\n   random with a probability that is weighted, for example, by the\n   associated\
    \ bandwidth on the link from the neighbor to the (current)\n   destination.\n\
    \   Specifically, assume a new request to destination, say, d, and with\n   bandwidth\
    \ requirements B.  The index of the destination vertex\n   identifies the row\
    \ in the QoS routing table that needs to be checked\n   to generate a path.  The\
    \ row is then searched to identify a suitable\n   path.  If the Bellman-Ford algorithm\
    \ of Section 2.3.1 was used, the\n   search proceeds by increasing index (hop)\
    \ count until an entry is\n   found, say at hop count or column index of h, with\
    \ a value of the bw\n   field that is equal to or greater than B.  This entry\
    \ points to the\n   initial information identifying the selected path.  If the\
    \ Dijkstra\n   algorithm of Appendix C is used, the first quantized value qB such\n\
    \   that qB  >=   B is first identified, and the associated column then\n   determines\
    \ the first entry in the QoS routing table that identifies\n   the selected path.\n\
    \   Once this first entry has been identified, reconstruction of the\n   complete\
    \ list of vertices on the path proceeds similarly, whether the\n   table was built\
    \ using the algorithm of Section 2.3.1 or Appendix C.\n   Specifically, in both\
    \ cases, the neighbor field in each entry points\n   to the previous node on the\
    \ path from the source node and with the\n   same bandwidth capabilities as those\
    \ associated with the current\n   entry.  The complete path is, therefore, reconstructed\
    \ by following\n   the pointers provided by the neighbor field of successive entries.\n\
    \   In the case of the Bellman-Ford algorithm of Section 2.3.1, this\n   means\
    \ moving backwards in the table from column to column, using at\n   each step\
    \ the row index pointed to by the neighbor field of the entry\n   in the previous\
    \ column.  Each time, the corresponding vertex index\n   specified in the neighbor\
    \ field is pre-pended to the list of vertices\n   constructed so far.  Since we\
    \ start at column h, the process ends\n   when the first column is reached, i.e.,\
    \ after h steps, at which point\n   the list of vertices making up the path has\
    \ been reconstructed.\n   In the case of the Dijkstra algorithm of Appendix C,\
    \ the backtracking\n   process is similar although slightly different because\
    \ of the\n   different relation between paths and columns in the routing table,\n\
    \   i.e., a column now corresponds to a quantized bandwidth value instead\n  \
    \ of a hop count.  The backtracking now proceeds along the column\n   corresponding\
    \ to the quantized bandwidth value needed to satisfy the\n   bandwidth requirements\
    \ of the flow.  At each step, the vertex index\n   specified in the neighbor field\
    \ is pre-pended to the list of vertices\n   constructed so far, and is used to\
    \ identify the next row index to\n   move to.  The process ends when an entry\
    \ is reached whose neighbor\n   field specifies the origin vertex of the flow.\
    \  Note that since there\n   are as many rows in the table as there are vertices\
    \ in the graph,\n   i.e., N, it could take up to N steps before the process terminates.\n\
    \   Note that the identification of the first entry in the routing table\n   is\
    \ identical to what was described for the hop-by-hop routing case.\n   However,\
    \ as described in this section, the update of the neighbor\n   fields while constructing\
    \ the QoS routing tables, is being performed\n   differently in the explicit and\
    \ hop-by-hop routing cases.  Clearly,\n   two different neighbor fields can be\
    \ kept in each entry and updates\n   to both could certainly be performed jointly,\
    \ if support for both\n   xplicit routing and hop-by-hop routing is needed.\n"
- title: Endnotes
  contents:
  - "Endnotes\n   1. In this document we commit the abuse of notation of calling a\n\
    \      \"network\" the interconnection of routers and networks through\n     \
    \ which we attempt to compute a QoS path.\n   2. This is true for uni-cast flows,\
    \ but in the case of multi-cast\n      flows, hop-by-hop and an explicit routing\
    \ clearly have different\n      implications.\n   3. Some hysteresis mechanism\
    \ should be added to suppress updates when\n      the metric value oscillates\
    \ around a class boundary.\n   4. In this document, we use the terms node and\
    \ vertex\n      interchangeably.\n   5. Various hybrid methods can also be envisioned,\
    \ e.g., periodic\n      computations except if more than a given number of updates\
    \ are\n      received within a shorter interval, or periodic updates except if\n\
    \      the change in metrics corresponding to a given update exceeds a\n     \
    \ certain threshold.  Such variations are, however, not considered\n      in this\
    \ document.\n   6. Modifications to support explicit routing are discussed in\n\
    \      Appendix D.\n   7. Note, that this does not say anything on whether to\
    \ differentiate\n      between outgoing and incoming bandwidth on a shared media\
    \ network.\n      As a matter of fact, a reasonable option is to set the incoming\n\
    \      bandwidth (from network to router) to infinity, and only use the\n    \
    \  outgoing bandwidth value to characterize bandwidth availability on\n      the\
    \ shared network.\n   8. exponent in parenthesis\n   9. Access to some of the\
    \ more recent versions of the GateD software\n      is restricted to the GateD\
    \ consortium members.\n   10. Note that a Breadth-First-Search (BFS) algorithm\
    \ [CLR90] could\n      also be used.  It has a lower complexity, but would not\
    \ allow\n      reuse of existing code in an OSPF implementation.\n"
- title: References
  contents:
  - "References\n   [AGK99]  G. Apostolopoulos, R. Guerin, and S. Kamat. Implementation\n\
    \            and performance meassurements of QoS routing extensions to\n    \
    \        OSPF.  In Proceedings of INFOCOM'99, pages 680--688, New\n          \
    \  York, NY, March 1999.\n   [AGKT98] G. Apostolopoulos, R. Guerin, S. Kamat,\
    \ and S. K. Tripathi.\n            QoS routing:  A performance perspective.  In\
    \ Proceedings of\n            ACM SIGCOMM'98, pages 17--28, Vancouver, Canada,\
    \ October\n   [Alm92]  Almquist, P., \"Type of Service in the Internet Protocol\n\
    \            Suite\", RFC 1349, July 1992.\n   [AT98]   G. Apostolopoulos and\
    \ S. K. Tripathi.  On reducing the\n            processing cost of on-demand QoS\
    \ path computation.  In\n            Proceedings of ICNP'98, pages 80--89, Austin,\
    \ TX, October\n            1998.\n   [BP95]   J.-Y. Le Boudec and T. Przygienda.\
    \  A Route Pre-Computation\n            Algorithm for Integrated Services Networks.\
    \  Journal of\n            Network and Systems Management, 3(4), 1995.\n   [Car79]\
    \  B. Carre.  Graphs and Networks.  Oxford University Press,\n            ISBN\
    \ 0-19-859622-7, Oxford, UK, 1979.\n   [CLR90]  T. H. Cormen, C. E. Leiserson,\
    \ and R. L. Rivest.\n            Introduction to Algorithms.  MIT Press, Cambridge,\
    \ MA, 1990.\n   [Con]    Merit GateD Consortium.  The Gate Daemon (GateD) project.\n\
    \   [GJ79]   M.R. Garey and D.S. Johnson.  Computers and Intractability.\n   \
    \         Freeman, San Francisco, 1979.\n   [GKH97]  R. Guerin, S. Kamat, and\
    \ S. Herzog.  QoS Path Management\n            with RSVP.  In Proceedings of the\
    \ 2nd IEEE Global Internet\n            Mini-Conference, pages 1914-1918, Phoenix,\
    \ AZ, November\n   [GKR97]  Guerin, R., Kamat, S. and E. Rosen, \"An Extended\
    \ RSVP\n            Routing Interface, Work in Progress.\n   [GLG+97] Der-Hwa\
    \ G., Li, T., Guerin, R., Rosen, E. and S. Kamat,\n            \"Setting Up Reservations\
    \ on Explicit Paths using RSVP\", Work\n            in Progress.\n   [GO99]  \
    \ R. Guerin and A. Orda.  QoS-Based Routing in Networks with\n            Inaccurate\
    \ Information: Theory and Algorithms.  IEEE/ACM\n            Transactions on Networking,\
    \ 7(3):350--364, June 1999.\n   [GOW97]  R. Guerin, A. Orda, and D. Williams.\
    \  QoS Routing Mechanisms\n            and OSPF Extensions.  In Proceedings of\
    \ the 2nd IEEE Global\n            Internet Mini-Conference, pages 1903-1908,\
    \ Phoenix, AZ,\n            November 1997.\n   [KNB98]  Nichols, K., Blake, S.,\
    \ Baker F. and D. Black, \"Definition\n            of the Differentiated Services\
    \ Field (DS Field) in the IPv4\n            and IPv6 Headers\", RFC 2474, December\
    \ 1998.\n   [LO98]   D. H. Lorenz and A. Orda.  QoS Routing in Networks with\n\
    \            Uncertain Parameters.  IEEE/ACM Transactions on Networking,\n   \
    \         6(6):768--778, December 1998.\n   [Moy94]  Moy, J., \"OSPF Version 2\"\
    , RFC 1583, March 1994.\n   [Moy98]  Moy, J., \"OSPF Version 2\", STD 54, RFC\
    \ 2328, April 1998.\n   [Prz95]  A. Przygienda.  Link State Routing with QoS in\
    \ ATM LANs.\n            Ph.D. Thesis Nr. 11051, Swiss Federal Institute of\n\
    \            Technology, April 1995.\n   [RMK+98] R. Rajan, J. C. Martin, S. Kamat,\
    \ M. See, R. Chaudhury, D.\n            Verma, G. Powers, and R. Yavatkar.  Schema\
    \ for\n            differentiated services and integrated services in networks.\n\
    \            INTERNET-DRAFT, October 1998.  work in progress.\n   [RZB+97] Braden,\
    \ R., Editor, Zhang, L., Berson, S., Herzog, S. and S.\n            Jamin, \"\
    Resource reSerVation Protocol (RSVP) Version 1,\n            Functional Specification\"\
    , RFC 2205, September 1997.\n   [SPG97]  Shenker, S., Partridge, C. and R. Guerin,\
    \ \"Specification of\n            Guaranteed Quality of Service\", RFC 2212, November\
    \ 1997.\n   [ST83]   D.D. Sleator and R.E. Tarjan.  A Data Structure for Dynamic\n\
    \            Trees.  Journal of Computer Systems, 26, 1983.\n   [Tan89]  A. Tannenbaum.\
    \  Computer Networks.  Addisson Wesley, 1989.\n   [YPG97]  Yavatkar, R., Pendarakis,\
    \ D. and R. Guerin, \"A Framework for\n            Policy-based Admission Control\"\
    , INTERNET-DRAFT, April 1999.\n            Work in Progress.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   George Apostolopoulos\n   IBM T.J. Watson Research Center\n\
    \   P.O. Box 704\n   Yorktown Heights, NY 10598\n   Phone: +1 914 784-6204\n \
    \  Fax:   +1 914 784-6205\n   EMail: georgeap@watson.ibm.com\n   Roch Guerin\n\
    \   University Of Pennsylvania\n   Department of Electrical Engineering, Rm 367\
    \ GRW\n   200 South 33rd Street\n   Philadelphia, PA 19104--6390\n   Phone: +1\
    \ 215-898-9351\n   EMail: guerin@ee.upenn.edu\n   Sanjay Kamat\n   Bell Laboratories\n\
    \   Lucent Technologies\n   Room 4C-510\n   101 Crawfords Corner Road\n   Holmdel,\
    \ NJ 07733\n   Phone: (732) 949-5936\n   email: sanjayk@dnrc.bell-labs.com\n \
    \  Ariel Orda\n   Dept. Electrical Engineering\n   Technion - I.I.T\n   Haifa,\
    \ 32000 - ISRAEL\n   Phone: +011 972-4-8294646\n   Fax:   +011 972-4-8323041\n\
    \   EMail: ariel@ee.technion.ac.il\n   Tony Przygienda\n   Siara Systems\n   300\
    \ Ferguson Drive\n   Moutain View\n   California 94043\n   Phone: +1 732 949-5936\n\
    \   Email: prz@siara.com\n   Doug Williams\n   IBM T.J. Watson Research Center\n\
    \   P.O. Box 704\n   Yorktown Heights, NY 10598\n   Phone: +1 914 784-5047\n \
    \  Fax:   +1 914 784-6318\n   EMail: dougw@watson.ibm.com\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The Internet Society (1999).  All\
    \ Rights Reserved.\n   This document and translations of it may be copied and\
    \ furnished to\n   others, and derivative works that comment on or otherwise explain\
    \ it\n   or assist in its implementation may be prepared, copied, published\n\
    \   and distributed, in whole or in part, without restriction of any\n   kind,\
    \ provided that the above copyright notice and this paragraph are\n   included\
    \ on all such copies and derivative works.  However, this\n   document itself\
    \ may not be modified in any way, such as by removing\n   the copyright notice\
    \ or references to the Internet Society or other\n   Internet organizations, except\
    \ as needed for the purpose of\n   developing Internet standards in which case\
    \ the procedures for\n   copyrights defined in the Internet Standards process\
    \ must be\n   followed, or as required to translate it into languages other than\n\
    \   English.\n   The limited permissions granted above are perpetual and will\
    \ not be\n   revoked by the Internet Society or its successors or assigns.\n \
    \  This document and the information contained herein is provided on an\n   \"\
    AS IS\" basis and THE INTERNET SOCIETY AND THE INTERNET ENGINEERING\n   TASK FORCE\
    \ DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING\n   BUT NOT LIMITED\
    \ TO ANY WARRANTY THAT THE USE OF THE INFORMATION\n   HEREIN WILL NOT INFRINGE\
    \ ANY RIGHTS OR ANY IMPLIED WARRANTIES OF\n   MERCHANTABILITY OR FITNESS FOR A\
    \ PARTICULAR PURPOSE.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
