- title: __initial_text__
  contents:
  - '                Byte and Packet Congestion Notification

    '
- title: Abstract
  contents:
  - "Abstract\n   This document provides recommendations of best current practice\
    \ for\n   dropping or marking packets using any active queue management (AQM)\n\
    \   algorithm, including Random Early Detection (RED), BLUE, Pre-\n   Congestion\
    \ Notification (PCN), and newer schemes such as CoDel\n   (Controlled Delay) and\
    \ PIE (Proportional Integral controller\n   Enhanced).  We give three strong recommendations:\
    \ (1) packet size\n   should be taken into account when transports detect and\
    \ respond to\n   congestion indications, (2) packet size should not be taken into\n\
    \   account when network equipment creates congestion signals (marking,\n   dropping),\
    \ and therefore (3) in the specific case of RED, the byte-\n   mode packet drop\
    \ variant that drops fewer small packets should not be\n   used.  This memo updates\
    \ RFC 2309 to deprecate deliberate\n   preferential treatment of small packets\
    \ in AQM algorithms.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo documents an Internet Best Current Practice.\n\
    \   This document is a product of the Internet Engineering Task Force\n   (IETF).\
    \  It represents the consensus of the IETF community.  It has\n   received public\
    \ review and has been approved for publication by the\n   Internet Engineering\
    \ Steering Group (IESG).  Further information on\n   BCPs is available in Section\
    \ 2 of RFC 5741.\n   Information about the current status of this document, any\
    \ errata,\n   and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc7141.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2014 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   4\n     1.1.  Terminology and Scoping . . . . . . . . . . . . .\
    \ . . . .   6\n     1.2.  Example Comparing Packet-Mode Drop and Byte-Mode Drop\
    \ . .   7\n   2.  Recommendations . . . . . . . . . . . . . . . . . . . . . .\
    \ .   9\n     2.1.  Recommendation on Queue Measurement . . . . . . . . . . .\
    \   9\n     2.2.  Recommendation on Encoding Congestion Notification  . . .  10\n\
    \     2.3.  Recommendation on Responding to Congestion  . . . . . . .  11\n  \
    \   2.4.  Recommendation on Handling Congestion Indications When\n           Splitting\
    \ or Merging Packets  . . . . . . . . . . . . . .  12\n   3.  Motivating Arguments\
    \  . . . . . . . . . . . . . . . . . . . .  13\n     3.1.  Avoiding Perverse Incentives\
    \ to (Ab)use Smaller Packets .  13\n     3.2.  Small != Control  . . . . . . .\
    \ . . . . . . . . . . . . .  14\n     3.3.  Transport-Independent Network . .\
    \ . . . . . . . . . . . .  14\n     3.4.  Partial Deployment of AQM . . . . .\
    \ . . . . . . . . . . .  16\n     3.5.  Implementation Efficiency . . . . . .\
    \ . . . . . . . . . .  17\n   4.  A Survey and Critique of Past Advice  . . .\
    \ . . . . . . . . .  17\n     4.1.  Congestion Measurement Advice . . . . . .\
    \ . . . . . . . .  18\n       4.1.1.  Fixed-Size Packet Buffers . . . . . . .\
    \ . . . . . . .  18\n       4.1.2.  Congestion Measurement without a Queue  .\
    \ . . . . . .  19\n     4.2.  Congestion Notification Advice  . . . . . . . .\
    \ . . . . .  20\n       4.2.1.  Network Bias When Encoding  . . . . . . . . .\
    \ . . . .  20\n       4.2.2.  Transport Bias When Decoding  . . . . . . . . .\
    \ . . .  22\n       4.2.3.  Making Transports Robust against Control Packet\n\
    \               Losses  . . . . . . . . . . . . . . . . . . . . . . .  23\n  \
    \     4.2.4.  Congestion Notification: Summary of Conflicting\n              \
    \ Advice  . . . . . . . . . . . . . . . . . . . . . . .  24\n   5.  Outstanding\
    \ Issues and Next Steps . . . . . . . . . . . . . .  25\n     5.1.  Bit-congestible\
    \ Network . . . . . . . . . . . . . . . . .  25\n     5.2.  Bit- and Packet-Congestible\
    \ Network . . . . . . . . . . .  26\n   6.  Security Considerations . . . . .\
    \ . . . . . . . . . . . . . .  26\n   7.  Conclusions . . . . . . . . . . . .\
    \ . . . . . . . . . . . . .  27\n   8.  Acknowledgements  . . . . . . . . . .\
    \ . . . . . . . . . . . .  28\n   9.  References  . . . . . . . . . . . . . .\
    \ . . . . . . . . . . .  28\n     9.1.  Normative References  . . . . . . . .\
    \ . . . . . . . . . .  28\n     9.2.  Informative References  . . . . . . . .\
    \ . . . . . . . . .  29\n   Appendix A.  Survey of RED Implementation Status \
    \ . . . . . . . .  33\n   Appendix B.  Sufficiency of Packet-Mode Drop  . . .\
    \ . . . . . . .  34\n     B.1.  Packet-Size (In)Dependence in Transports  . .\
    \ . . . . . .  35\n     B.2.  Bit-Congestible and Packet-Congestible Indications\
    \  . . .  38\n   Appendix C.  Byte-Mode Drop Complicates Policing Congestion\n\
    \                Response . . . . . . . . . . . . . . . . . . . . . .  39\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document provides recommendations of best current practice\
    \ for\n   how we should correctly scale congestion control functions with\n  \
    \ respect to packet size for the long term.  It also recognises that\n   expediency\
    \ may be necessary to deal with existing widely deployed\n   protocols that don't\
    \ live up to the long-term goal.\n   When signalling congestion, the problem of\
    \ how (and whether) to take\n   packet sizes into account has exercised the minds\
    \ of researchers and\n   practitioners for as long as active queue management\
    \ (AQM) has been\n   discussed.  Indeed, one reason AQM was originally introduced\
    \ was to\n   reduce the lock-out effects that small packets can have on large\n\
    \   packets in tail-drop queues.  This memo aims to state the principles\n   we\
    \ should be using and to outline how these principles will affect\n   future protocol\
    \ design, taking into account pre-existing deployments.\n   The question of whether\
    \ to take into account packet size arises at\n   three stages in the congestion\
    \ notification process:\n   Measuring congestion:  When a congested resource measures\
    \ locally how\n      congested it is, should it measure its queue length in time,\n\
    \      bytes, or packets?\n   Encoding congestion notification into the wire protocol:\
    \  When a\n      congested network resource signals its level of congestion, should\n\
    \      the probability that it drops/marks each packet depend on the size\n  \
    \    of the particular packet in question?\n   Decoding congestion notification\
    \ from the wire protocol:  When a\n      transport interprets the notification\
    \ in order to decide how much\n      to respond to congestion, should it take\
    \ into account the size of\n      each missing or marked packet?\n   Consensus\
    \ has emerged over the years concerning the first stage,\n   which Section 2.1\
    \ records in the RFC Series.  In summary: If\n   possible, it is best to measure\
    \ congestion by time in the queue;\n   otherwise, the choice between bytes and\
    \ packets solely depends on\n   whether the resource is congested by bytes or\
    \ packets.\n   The controversy is mainly around the last two stages: whether to\n\
    \   allow for the size of the specific packet notifying congestion i)\n   when\
    \ the network encodes or ii) when the transport decodes the\n   congestion notification.\n\
    \   Currently, the RFC series is silent on this matter other than a paper\n  \
    \ trail of advice referenced from [RFC2309], which conditionally\n   recommends\
    \ byte-mode (packet-size dependent) drop [pktByteEmail].\n   Reducing the number\
    \ of small packets dropped certainly has some\n   tempting advantages: i) it drops\
    \ fewer control packets, which tend to\n   be small and ii) it makes TCP's bit\
    \ rate less dependent on packet\n   size.  However, there are ways of addressing\
    \ these issues at the\n   transport layer, rather than reverse engineering network\
    \ forwarding\n   to fix the problems.\n   This memo updates [RFC2309] to deprecate\
    \ deliberate preferential\n   treatment of packets in AQM algorithms solely because\
    \ of their size.\n   It recommends that (1) packet size should be taken into account\
    \ when\n   transports detect and respond to congestion indications, (2) not when\n\
    \   network equipment creates them.  This memo also adds to the\n   congestion\
    \ control principles enumerated in BCP 41 [RFC2914].\n   In the particular case\
    \ of Random Early Detection (RED), this means\n   that the byte-mode packet drop\
    \ variant should not be used to drop\n   fewer small packets, because that creates\
    \ a perverse incentive for\n   transports to use tiny segments, consequently also\
    \ opening up a DoS\n   vulnerability.  Fortunately, all the RED implementers who\
    \ responded\n   to our admittedly limited survey (Section 4.2.4) have not followed\n\
    \   the earlier advice to use byte-mode drop, so the position this memo\n   argues\
    \ for seems to already exist in implementations.\n   However, at the transport\
    \ layer, TCP congestion control is a widely\n   deployed protocol that doesn't\
    \ scale with packet size (i.e., its\n   reduction in rate does not take into account\
    \ the size of a lost\n   packet).  To date, this hasn't been a significant problem\
    \ because\n   most TCP implementations have been used with similar packet sizes.\n\
    \   But, as we design new congestion control mechanisms, this memo\n   recommends\
    \ that we build in scaling with packet size rather than\n   assuming that we should\
    \ follow TCP's example.\n   This memo continues as follows.  First, it discusses\
    \ terminology and\n   scoping.  Section 2 gives concrete formal recommendations,\
    \ followed\n   by motivating arguments in Section 3.  We then critically survey\
    \ the\n   advice given previously in the RFC Series and the research literature\n\
    \   (Section 4), referring to an assessment of whether or not this advice\n  \
    \ has been followed in production networks (Appendix A).  To wrap up,\n   outstanding\
    \ issues are discussed that will need resolution both to\n   inform future protocol\
    \ designs and to handle legacy AQM deployments\n   (Section 5).  Then security\
    \ issues are collected together in\n   Section 6 before conclusions are drawn\
    \ in Section 7.  The interested\n   reader can find discussion of more detailed\
    \ issues on the theme of\n   byte vs. packet in the appendices.\n   This memo\
    \ intentionally includes a non-negligible amount of material\n   on the subject.\
    \  For the busy reader, Section 2 summarises the\n   recommendations for the Internet\
    \ community.\n"
- title: 1.1.  Terminology and Scoping
  contents:
  - "1.1.  Terminology and Scoping\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in [RFC2119].\n   This memo applies to the design of all AQM algorithms, for\
    \ example,\n   Random Early Detection (RED) [RFC2309], BLUE [BLUE02], Pre-Congestion\n\
    \   Notification (PCN) [RFC5670], Controlled Delay (CoDel) [CoDel], and\n   the\
    \ Proportional Integral controller Enhanced (PIE) [PIE].\n   Throughout, RED is\
    \ used as a concrete example because it is a widely\n   known and deployed AQM\
    \ algorithm.  There is no intention to imply\n   that the advice is any less applicable\
    \ to the other algorithms, nor\n   that RED is preferred.\n   Congestion Notification:\
    \  Congestion notification is a changing\n      signal that aims to communicate\
    \ the probability that the network\n      resource(s) will not be able to forward\
    \ the level of traffic load\n      offered (or that there is an impending risk\
    \ that they will not be\n      able to).\n      The 'impending risk' qualifier\
    \ is added, because AQM systems set a\n      virtual limit smaller than the actual\
    \ limit to the resource, then\n      notify the transport when this virtual limit\
    \ is exceeded in order\n      to avoid uncontrolled congestion of the actual capacity.\n\
    \      Congestion notification communicates a real number bounded by the\n   \
    \   range [ 0 , 1 ].  This ties in with the most well-understood\n      measure\
    \ of congestion notification: drop probability.\n   Explicit and Implicit Notification:\
    \  The byte vs. packet dilemma\n      concerns congestion notification irrespective\
    \ of whether it is\n      signalled implicitly by drop or explicitly using ECN\
    \ [RFC3168] or\n      PCN [RFC5670].  Throughout this document, unless clear from\
    \ the\n      context, the term 'marking' will be used to mean notifying\n    \
    \  congestion explicitly, while 'congestion notification' will be\n      used\
    \ to mean notifying congestion either implicitly by drop or\n      explicitly\
    \ by marking.\n   Bit-congestible vs. Packet-congestible:  If the load on a resource\n\
    \      depends on the rate at which packets arrive, it is called 'packet-\n  \
    \    congestible'.  If the load depends on the rate at which bits\n      arrive,\
    \ it is called 'bit-congestible'.\n      Examples of packet-congestible resources\
    \ are route look-up engines\n      and firewalls, because load depends on how\
    \ many packet headers\n      they have to process.  Examples of bit-congestible\
    \ resources are\n      transmission links, radio power, and most buffer memory,\
    \ because\n      the load depends on how many bits they have to transmit or store.\n\
    \      Some machine architectures use fixed-size packet buffers, so\n      buffer\
    \ memory in these cases is packet-congestible (see\n      Section 4.1.1).\n  \
    \    The path through a machine will typically encounter both packet-\n      congestible\
    \ and bit-congestible resources.  However, currently, a\n      design goal of\
    \ network processing equipment such as routers and\n      firewalls is to size\
    \ the packet-processing engine(s) relative to\n      the lines in order to keep\
    \ packet processing uncongested, even\n      under worst-case packet rates with\
    \ runs of minimum-size packets.\n      Therefore, packet congestion is currently\
    \ rare (see Section 3.3 of\n      [RFC6077]), but there is no guarantee that it\
    \ will not become more\n      common in the future.\n      Note that information\
    \ is generally processed or transmitted with a\n      minimum granularity greater\
    \ than a bit (e.g., octets).  The\n      appropriate granularity for the resource\
    \ in question should be\n      used, but for the sake of brevity we will talk\
    \ in terms of bytes\n      in this memo.\n   Coarser Granularity:  Resources may\
    \ be congestible at higher levels\n      of granularity than bits or packets,\
    \ for instance stateful\n      firewalls are flow-congestible and call-servers\
    \ are session-\n      congestible.  This memo focuses on congestion of connectionless\n\
    \      resources, but the same principles may be applicable for\n      congestion\
    \ notification protocols controlling per-flow and per-\n      session processing\
    \ or state.\n   RED Terminology:  In RED, whether to use packets or bytes when\n\
    \      measuring queues is called, respectively, 'packet-mode queue\n      measurement'\
    \ or 'byte-mode queue measurement'.  And whether the\n      probability of dropping\
    \ a particular packet is independent or\n      dependent on its size is called,\
    \ respectively, 'packet-mode drop'\n      or 'byte-mode drop'.  The terms 'byte-mode'\
    \ and 'packet-mode'\n      should not be used without specifying whether they\
    \ apply to queue\n      measurement or to drop.\n"
- title: 1.2.  Example Comparing Packet-Mode Drop and Byte-Mode Drop
  contents:
  - "1.2.  Example Comparing Packet-Mode Drop and Byte-Mode Drop\n   Taking RED as\
    \ a well-known example algorithm, a central question\n   addressed by this document\
    \ is whether to recommend RED's packet-mode\n   drop variant and to deprecate\
    \ byte-mode drop.  Table 1 compares how\n   packet-mode and byte-mode drop affect\
    \ two flows of different size\n   packets.  For each it gives the expected number\
    \ of packets and of\n   bits dropped in one second.  Each example flow runs at\
    \ the same bit\n   rate of 48 Mbps, but one is broken up into small 60 byte packets\
    \ and\n   the other into large 1,500 byte packets.\n   To keep up the same bit\
    \ rate, in one second there are about 25 times\n   more small packets because\
    \ they are 25 times smaller.  As can be seen\n   from the table, the packet rate\
    \ is 100,000 small packets versus 4,000\n   large packets per second (pps).\n\
    \     Parameter            Formula         Small packets Large packets\n     --------------------\
    \ --------------- ------------- -------------\n     Packet size          s/8 \
    \                     60 B       1,500 B\n     Packet size          s        \
    \               480 b      12,000 b\n     Bit rate             x             \
    \        48 Mbps       48 Mbps\n     Packet rate          u = x/s            \
    \  100 kpps        4 kpps\n     Packet-mode Drop\n     Pkt-loss probability p\
    \                        0.1%          0.1%\n     Pkt-loss rate        p*u   \
    \                100 pps         4 pps\n     Bit-loss rate        p*u*s      \
    \           48 kbps       48 kbps\n     Byte-mode Drop       MTU, M=12,000 b\n\
    \     Pkt-loss probability b = p*s/M              0.004%          0.1%\n     Pkt-loss\
    \ rate        b*u                     4 pps         4 pps\n     Bit-loss rate\
    \        b*u*s               1.92 kbps       48 kbps\n         Table 1: Example\
    \ Comparing Packet-Mode and Byte-Mode Drop\n   For packet-mode drop, we illustrate\
    \ the effect of a drop probability\n   of 0.1%, which the algorithm applies to\
    \ all packets irrespective of\n   size.  Because there are 25 times more small\
    \ packets in one second,\n   it naturally drops 25 times more small packets, that\
    \ is, 100 small\n   packets but only 4 large packets.  But if we count how many\
    \ bits it\n   drops, there are 48,000 bits in 100 small packets and 48,000 bits\
    \ in\n   4 large packets -- the same number of bits of small packets as large.\n\
    \      The packet-mode drop algorithm drops any bit with the same\n      probability\
    \ whether the bit is in a small or a large packet.\n   For byte-mode drop, again\
    \ we use an example drop probability of 0.1%,\n   but only for maximum size packets\
    \ (assuming the link maximum\n   transmission unit (MTU) is 1,500 B or 12,000\
    \ b).  The byte-mode\n   algorithm reduces the drop probability of smaller packets\n\
    \   proportional to their size, making the probability that it drops a\n   small\
    \ packet 25 times smaller at 0.004%.  But there are 25 times more\n   small packets,\
    \ so dropping them with 25 times lower probability\n   results in dropping the\
    \ same number of packets: 4 drops in both\n   cases.  The 4 small dropped packets\
    \ contain 25 times less bits than\n   the 4 large dropped packets: 1,920 compared\
    \ to 48,000.\n      The byte-mode drop algorithm drops any bit with a probability\n\
    \      proportionate to the size of the packet it is in.\n"
- title: 2.  Recommendations
  contents:
  - "2.  Recommendations\n   This section gives recommendations related to network\
    \ equipment in\n   Sections 2.1 and 2.2, and we discuss the implications on transport\n\
    \   protocols in Sections 2.3 and 2.4.\n"
- title: 2.1.  Recommendation on Queue Measurement
  contents:
  - "2.1.  Recommendation on Queue Measurement\n   Ideally, an AQM would measure the\
    \ service time of the queue to\n   measure congestion of a resource.  However\
    \ service time can only be\n   measured as packets leave the queue, where it is\
    \ not always expedient\n   to implement a full AQM algorithm.  To predict the\
    \ service time as\n   packets join the queue, an AQM algorithm needs to measure\
    \ the length\n   of the queue.\n   In this case, if the resource is bit-congestible,\
    \ the AQM\n   implementation SHOULD measure the length of the queue in bytes and,\n\
    \   if the resource is packet-congestible, the implementation SHOULD\n   measure\
    \ the length of the queue in packets.  Subject to the\n   exceptions below, no\
    \ other choice makes sense, because the number of\n   packets waiting in the queue\
    \ isn't relevant if the resource gets\n   congested by bytes and vice versa. \
    \ For example, the length of the\n   queue into a transmission line would be measured\
    \ in bytes, while the\n   length of the queue into a firewall would be measured\
    \ in packets.\n   To avoid the pathological effects of tail drop, the AQM can\
    \ then\n   transform this service time or queue length into the probability of\n\
    \   dropping or marking a packet (e.g., RED's piecewise linear function\n   between\
    \ thresholds).\n   What this advice means for RED as a specific example:\n   1.\
    \  A RED implementation SHOULD use byte-mode queue measurement for\n       measuring\
    \ the congestion of bit-congestible resources and packet-\n       mode queue measurement\
    \ for packet-congestible resources.\n   2.  An implementation SHOULD NOT make\
    \ it possible to configure the\n       way a queue measures itself, because whether\
    \ a queue is bit-\n       congestible or packet-congestible is an inherent property\
    \ of the\n       queue.\n   Exceptions to these recommendations might be necessary,\
    \ for instance\n   where a packet-congestible resource has to be configured as\
    \ a proxy\n   bottleneck for a bit-congestible resource in an adjacent box that\n\
    \   does not support AQM.\n   The recommended approach in less straightforward\
    \ scenarios, such as\n   fixed-size packet buffers, resources without a queue,\
    \ and buffers\n   comprising a mix of packet and bit-congestible resources, is\n\
    \   discussed in Section 4.1.  For instance, Section 4.1.1 explains that\n   the\
    \ queue into a line should be measured in bytes even if the queue\n   consists\
    \ of fixed-size packet buffers, because the root cause of any\n   congestion is\
    \ bytes arriving too fast for the line -- packets filling\n   buffers are merely\
    \ a symptom of the underlying congestion of the\n   line.\n"
- title: 2.2.  Recommendation on Encoding Congestion Notification
  contents:
  - "2.2.  Recommendation on Encoding Congestion Notification\n   When encoding congestion\
    \ notification (e.g., by drop, ECN, or PCN),\n   the probability that network\
    \ equipment drops or marks a particular\n   packet to notify congestion SHOULD\
    \ NOT depend on the size of the\n   packet in question.  As the example in Section\
    \ 1.2 illustrates, to\n   drop any bit with probability 0.1%, it is only necessary\
    \ to drop\n   every packet with probability 0.1% without regard to the size of\
    \ each\n   packet.\n   This approach ensures the network layer offers sufficient\
    \ congestion\n   information for all known and future transport protocols and\
    \ also\n   ensures no perverse incentives are created that would encourage\n \
    \  transports to use inappropriately small packet sizes.\n   What this advice\
    \ means for RED as a specific example:\n   1.  The RED AQM algorithm SHOULD NOT\
    \ use byte-mode drop, i.e., it\n       ought to use packet-mode drop.  Byte-mode\
    \ drop is more complex,\n       it creates the perverse incentive to fragment\
    \ segments into tiny\n       pieces and it is vulnerable to floods of small packets.\n\
    \   2.  If a vendor has implemented byte-mode drop, and an operator has\n    \
    \   turned it on, it is RECOMMENDED that the operator use packet-mode\n      \
    \ drop instead, after establishing if there are any implications on\n       the\
    \ relative performance of applications using different packet\n       sizes. \
    \ The unlikely possibility of some application-specific\n       legacy use of\
    \ byte-mode drop is the only reason that all the\n       above recommendations\
    \ on encoding congestion notification are not\n       phrased more strongly.\n\
    \       RED as a whole SHOULD NOT be switched off.  Without RED, a tail-\n   \
    \    drop queue biases against large packets and is vulnerable to\n       floods\
    \ of small packets.\n   Note well that RED's byte-mode queue drop is completely\
    \ orthogonal to\n   byte-mode queue measurement and should not be confused with\
    \ it.  If a\n   RED implementation has a byte-mode but does not specify what sort\
    \ of\n   byte-mode, it is most probably byte-mode queue measurement, which is\n\
    \   fine.  However, if in doubt, the vendor should be consulted.\n   A survey\
    \ (Appendix A) showed that there appears to be little, if any,\n   installed base\
    \ of the byte-mode drop variant of RED.  This suggests\n   that deprecating byte-mode\
    \ drop will have little, if any, incremental\n   deployment impact.\n"
- title: 2.3.  Recommendation on Responding to Congestion
  contents:
  - "2.3.  Recommendation on Responding to Congestion\n   When a transport detects\
    \ that a packet has been lost or congestion\n   marked, it SHOULD consider the\
    \ strength of the congestion indication\n   as proportionate to the size in octets\
    \ (bytes) of the missing or\n   marked packet.\n   In other words, when a packet\
    \ indicates congestion (by being lost or\n   marked), it can be considered conceptually\
    \ as if there is a\n   congestion indication on every octet of the packet, not\
    \ just one\n   indication per packet.\n   To be clear, the above recommendation\
    \ solely describes how a\n   transport should interpret the meaning of a congestion\
    \ indication, as\n   a long term goal.  It makes no recommendation on whether\
    \ a transport\n   should act differently based on this interpretation.  It merely\
    \ aids\n   interoperability between transports, if they choose to make their\n\
    \   actions depend on the strength of congestion indications.\n   This definition\
    \ will be useful as the IETF transport area continues\n   its programme of:\n\
    \   o  updating host-based congestion control protocols to take packet\n     \
    \ size into account, and\n   o  making transports less sensitive to losing control\
    \ packets like\n      SYNs and pure ACKs.\n   What this advice means for the case\
    \ of TCP:\n   1.  If two TCP flows with different packet sizes are required to\
    \ run\n       at equal bit rates under the same path conditions, this SHOULD be\n\
    \       done by altering TCP (Section 4.2.2), not network equipment (the\n   \
    \    latter affects other transports besides TCP).\n   2.  If it is desired to\
    \ improve TCP performance by reducing the\n       chance that a SYN or a pure\
    \ ACK will be dropped, this SHOULD be\n       done by modifying TCP (Section 4.2.3),\
    \ not network equipment.\n   To be clear, we are not recommending at all that\
    \ TCPs under\n   equivalent conditions should aim for equal bit rates.  We are\
    \ merely\n   saying that anyone trying to do such a thing should modify their\
    \ TCP\n   algorithm, not the network.\n   These recommendations are phrased as\
    \ 'SHOULD' rather than 'MUST',\n   because there may be cases where expediency\
    \ dictates that\n   compatibility with pre-existing versions of a transport protocol\
    \ make\n   the recommendations impractical.\n"
- title: 2.4.  Recommendation on Handling Congestion Indications When Splitting
  contents:
  - "2.4.  Recommendation on Handling Congestion Indications When Splitting\n    \
    \  or Merging Packets\n   Packets carrying congestion indications may be split\
    \ or merged in\n   some circumstances (e.g., at an RTP / RTP Control Protocol\
    \ (RTCP)\n   transcoder or during IP fragment reassembly).  Splitting and merging\n\
    \   only make sense in the context of ECN, not loss.\n   The general rule to follow\
    \ is that the number of octets in packets\n   with congestion indications SHOULD\
    \ be equivalent before and after\n   merging or splitting.  This is based on the\
    \ principle used above;\n   that an indication of congestion on a packet can be\
    \ considered as an\n   indication of congestion on each octet of the packet.\n\
    \   The above rule is not phrased with the word 'MUST' to allow the\n   following\
    \ exception.  There are cases in which pre-existing protocols\n   were not designed\
    \ to conserve congestion-marked octets (e.g., IP\n   fragment reassembly [RFC3168]\
    \ or loss statistics in RTCP receiver\n   reports [RFC3550] before ECN was added\
    \ [RFC6679]).  When any such\n   protocol is updated, it SHOULD comply with the\
    \ above rule to conserve\n   marked octets.  However, the rule may be relaxed\
    \ if it would\n   otherwise become too complex to interoperate with pre-existing\n\
    \   implementations of the protocol.\n   One can think of a splitting or merging\
    \ process as if all the\n   incoming congestion-marked octets increment a counter\
    \ and all the\n   outgoing marked octets decrement the same counter.  In order\
    \ to\n   ensure that congestion indications remain timely, even the smallest\n\
    \   positive remainder in the conceptual counter should trigger the next\n   outgoing\
    \ packet to be marked (causing the counter to go negative).\n"
- title: 3.  Motivating Arguments
  contents:
  - "3.  Motivating Arguments\n   This section is informative.  It justifies the recommendations\
    \ made\n   in the previous section.\n"
- title: 3.1.  Avoiding Perverse Incentives to (Ab)use Smaller Packets
  contents:
  - "3.1.  Avoiding Perverse Incentives to (Ab)use Smaller Packets\n   Increasingly,\
    \ it is being recognised that a protocol design must take\n   care not to cause\
    \ unintended consequences by giving the parties in\n   the protocol exchange perverse\
    \ incentives [Evol_cc] [RFC3426].  Given\n   there are many good reasons why larger\
    \ path maximum transmission\n   units (PMTUs) would help solve a number of scaling\
    \ issues, we do not\n   want to create any bias against large packets that is\
    \ greater than\n   their true cost.\n   Imagine a scenario where the same bit\
    \ rate of packets will contribute\n   the same to bit congestion of a link irrespective\
    \ of whether it is\n   sent as fewer larger packets or more smaller packets. \
    \ A protocol\n   design that caused larger packets to be more likely to be dropped\n\
    \   than smaller ones would be dangerous in both of the following cases:\n   Malicious\
    \ transports:  A queue that gives an advantage to small\n      packets can be\
    \ used to amplify the force of a flooding attack.  By\n      sending a flood of\
    \ small packets, the attacker can get the queue\n      to discard more large-packet\
    \ traffic, allowing more attack traffic\n      to get through to cause further\
    \ damage.  Such a queue allows\n      attack traffic to have a disproportionately\
    \ large effect on\n      regular traffic without the attacker having to do much\
    \ work.\n   Non-malicious transports:  Even if an application designer is not\n\
    \      actually malicious, if over time it is noticed that small packets\n   \
    \   tend to go faster, designers will act in their own interest and\n      use\
    \ smaller packets.  Queues that give advantage to small packets\n      create\
    \ an evolutionary pressure for applications or transports to\n      send at the\
    \ same bit rate but break their data stream down into\n      tiny segments to\
    \ reduce their drop rate.  Encouraging a high\n      volume of tiny packets might\
    \ in turn unnecessarily overload a\n      completely unrelated part of the system,\
    \ perhaps more limited by\n      header processing than bandwidth.\n   Imagine\
    \ that two unresponsive flows arrive at a bit-congestible\n   transmission link\
    \ each with the same bit rate, say 1 Mbps, but one\n   consists of 1,500 B and\
    \ the other 60 B packets, which are 25x\n   smaller.  Consider a scenario where\
    \ gentle RED [gentle_RED] is used,\n   along with the variant of RED we advise\
    \ against, i.e., where the RED\n   algorithm is configured to adjust the drop\
    \ probability of packets in\n   proportion to each packet's size (byte-mode packet\
    \ drop).  In this\n   case, RED aims to drop 25x more of the larger packets than\
    \ the\n   smaller ones.  Thus, for example, if RED drops 25% of the larger\n \
    \  packets, it will aim to drop 1% of the smaller packets (but, in\n   practice,\
    \ it may drop more as congestion increases; see Appendix B.4\n   of [RFC4828]).\
    \  Even though both flows arrive with the same bit rate,\n   the bit rate the\
    \ RED queue aims to pass to the line will be 750 kbps\n   for the flow of larger\
    \ packets but 990 kbps for the smaller packets\n   (because of rate variations,\
    \ it will actually be a little less than\n   this target).\n   Note that, although\
    \ the byte-mode drop variant of RED amplifies\n   small-packet attacks, tail-drop\
    \ queues amplify small-packet attacks\n   even more (see Security Considerations\
    \ in Section 6).  Wherever\n   possible, neither should be used.\n"
- title: 3.2.  Small != Control
  contents:
  - "3.2.  Small != Control\n   Dropping fewer control packets considerably improves\
    \ performance.  It\n   is tempting to drop small packets with lower probability\
    \ in order to\n   improve performance, because many control packets tend to be\
    \ smaller\n   (TCP SYNs and ACKs, DNS queries and responses, SIP messages, HTTP\n\
    \   GETs, etc).  However, we must not give control packets preference\n   purely\
    \ by virtue of their smallness, otherwise it is too easy for any\n   data source\
    \ to get the same preferential treatment simply by sending\n   data in smaller\
    \ packets.  Again, we should not create perverse\n   incentives to favour small\
    \ packets rather than to favour control\n   packets, which is what we intend.\n\
    \   Just because many control packets are small does not mean all small\n   packets\
    \ are control packets.\n   So, rather than fix these problems in the network,\
    \ we argue that the\n   transport should be made more robust against losses of\
    \ control\n   packets (see Section 4.2.3).\n"
- title: 3.3.  Transport-Independent Network
  contents:
  - "3.3.  Transport-Independent Network\n   TCP congestion control ensures that flows\
    \ competing for the same\n   resource each maintain the same number of segments\
    \ in flight,\n   irrespective of segment size.  So under similar conditions, flows\n\
    \   with different segment sizes will get different bit rates.\n   To counter\
    \ this effect, it seems tempting not to follow our\n   recommendation, and instead\
    \ for the network to bias congestion\n   notification by packet size in order\
    \ to equalise the bit rates of\n   flows with different packet sizes.  However,\
    \ in order to do this, the\n   queuing algorithm has to make assumptions about\
    \ the transport, which\n   become embedded in the network.  Specifically:\n  \
    \ o  The queuing algorithm has to assume how aggressively the transport\n    \
    \  will respond to congestion (see Section 4.2.4).  If the network\n      assumes\
    \ the transport responds as aggressively as TCP NewReno, it\n      will be wrong\
    \ for Compound TCP and differently wrong for Cubic\n      TCP, etc.  To achieve\
    \ equal bit rates, each transport then has to\n      guess what assumption the\
    \ network made, and work out how to\n      replace this assumed aggressiveness\
    \ with its own aggressiveness.\n   o  Also, if the network biases congestion notification\
    \ by packet\n      size, it has to assume a baseline packet size -- all proposed\n\
    \      algorithms use the local MTU (for example, see the byte-mode loss\n   \
    \   probability formula in Table 1).  Then if the non-Reno transports\n      mentioned\
    \ above are trying to reverse engineer what the network\n      assumed, they also\
    \ have to guess the MTU of the congested link.\n   Even though reducing the drop\
    \ probability of small packets (e.g.,\n   RED's byte-mode drop) helps ensure TCP\
    \ flows with different packet\n   sizes will achieve similar bit rates, we argue\
    \ that this correction\n   should be made to any future transport protocols based\
    \ on TCP, not to\n   the network in order to fix one transport, no matter how\
    \ predominant\n   it is.  Effectively, favouring small packets is reverse engineering\n\
    \   of network equipment around one particular transport protocol (TCP),\n   contrary\
    \ to the excellent advice in [RFC3426], which asks designers\n   to question \"\
    Why are you proposing a solution at this layer of the\n   protocol stack, rather\
    \ than at another layer?\"\n   In contrast, if the network never takes packet\
    \ size into account, the\n   transport can be certain it will never need to guess\
    \ any assumptions\n   that the network has made.  And the network passes two pieces\
    \ of\n   information to the transport that are sufficient in all cases: i)\n \
    \  congestion notification on the packet and ii) the size of the packet.\n   Both\
    \ are available for the transport to combine (by taking packet\n   size into account\
    \ when responding to congestion) or not.  Appendix B\n   checks that these two\
    \ pieces of information are sufficient for all\n   relevant scenarios.\n   When\
    \ the network does not take packet size into account, it allows\n   transport\
    \ protocols to choose whether or not to take packet size into\n   account.  However,\
    \ if the network were to bias congestion\n   notification by packet size, transport\
    \ protocols would have no\n   choice; those that did not take into account packet\
    \ size themselves\n   would unwittingly become dependent on packet size, and those\
    \ that\n   already took packet size into account would end up taking it into\n\
    \   account twice.\n"
- title: 3.4.  Partial Deployment of AQM
  contents:
  - "3.4.  Partial Deployment of AQM\n   In overview, the argument in this section\
    \ runs as follows:\n   o  Because the network does not and cannot always drop\
    \ packets in\n      proportion to their size, it shouldn't be given the task of\
    \ making\n      drop signals depend on packet size at all.\n   o  Transports on\
    \ the other hand don't always want to make their rate\n      response proportional\
    \ to the size of dropped packets, but if they\n      want to, they always can.\n\
    \   The argument is similar to the end-to-end argument that says \"Don't\n   do\
    \ X in the network if end systems can do X by themselves, and they\n   want to\
    \ be able to choose whether to do X anyway\".  Actually the\n   following argument\
    \ is stronger; in addition it says \"Don't give the\n   network task X that could\
    \ be done by the end systems, if X is not\n   deployed on all network nodes, and\
    \ end systems won't be able to tell\n   whether their network is doing X, or whether\
    \ they need to do X\n   themselves.\"  In this case, the X in question is \"making\
    \ the response\n   to congestion depend on packet size\".\n   We will now re-run\
    \ this argument reviewing each step in more depth.\n   The argument applies solely\
    \ to drop, not to ECN marking.\n   A queue drops packets for either of two reasons:\
    \ a) to signal to host\n   congestion controls that they should reduce the load\
    \ and b) because\n   there is no buffer left to store the packets.  Active queue\n\
    \   management tries to use drops as a signal for hosts to slow down\n   (case\
    \ a) so that drops due to buffer exhaustion (case b) should not\n   be necessary.\n\
    \   AQM is not universally deployed in every queue in the Internet; many\n   cheap\
    \ Ethernet bridges, software firewalls, NATs on consumer devices,\n   etc implement\
    \ simple tail-drop buffers.  Even if AQM were universal,\n   it has to be able\
    \ to cope with buffer exhaustion (by switching to a\n   behaviour like tail drop),\
    \ in order to cope with unresponsive or\n   excessive transports.  For these reasons\
    \ networks will sometimes be\n   dropping packets as a last resort (case b) rather\
    \ than under AQM\n   control (case a).\n   When buffers are exhausted (case b),\
    \ they don't naturally drop\n   packets in proportion to their size.  The network\
    \ can only reduce the\n   probability of dropping smaller packets if it has enough\
    \ space to\n   store them somewhere while it waits for a larger packet that it\
    \ can\n   drop.  If the buffer is exhausted, it does not have this choice.\n \
    \  Admittedly tail drop does naturally drop somewhat fewer small\n   packets,\
    \ but exactly how few depends more on the mix of sizes than\n   the size of the\
    \ packet in question.  Nonetheless, in general, if we\n   wanted networks to do\
    \ size-dependent drop, we would need universal\n   deployment of (packet-size\
    \ dependent) AQM code, which is currently\n   unrealistic.\n   A host transport\
    \ cannot know whether any particular drop was a\n   deliberate signal from an\
    \ AQM or a sign of a queue shedding packets\n   due to buffer exhaustion.  Therefore,\
    \ because the network cannot\n   universally do size-dependent drop, it should\
    \ not do it all.\n   Whereas universality is desirable in the network, diversity\
    \ is\n   desirable between different transport-layer protocols -- some, like\n\
    \   standards track TCP congestion control [RFC5681], may not choose to\n   make\
    \ their rate response proportionate to the size of each dropped\n   packet, while\
    \ others will (e.g., TCP-Friendly Rate Control for Small\n   Packets (TFRC-SP)\
    \ [RFC4828]).\n"
- title: 3.5.  Implementation Efficiency
  contents:
  - "3.5.  Implementation Efficiency\n   Biasing against large packets typically requires\
    \ an extra multiply\n   and divide in the network (see the example byte-mode drop\
    \ formula in\n   Table 1).  Taking packet size into account at the transport rather\n\
    \   than in the network ensures that neither the network nor the\n   transport\
    \ needs to do a multiply operation -- multiplication by\n   packet size is effectively\
    \ achieved as a repeated add when the\n   transport adds to its count of marked\
    \ bytes as each congestion event\n   is fed to it.  Also, the work to do the biasing\
    \ is spread over many\n   hosts, rather than concentrated in just the congested\
    \ network\n   element.  These aren't principled reasons in themselves, but they\
    \ are\n   a happy consequence of the other principled reasons.\n"
- title: 4.  A Survey and Critique of Past Advice
  contents:
  - "4.  A Survey and Critique of Past Advice\n   This section is informative, not\
    \ normative.\n   The original 1993 paper on RED [RED93] proposed two options for\
    \ the\n   RED active queue management algorithm: packet mode and byte mode.\n\
    \   Packet mode measured the queue length in packets and dropped (or\n   marked)\
    \ individual packets with a probability independent of their\n   size.  Byte mode\
    \ measured the queue length in bytes and marked an\n   individual packet with\
    \ probability in proportion to its size\n   (relative to the maximum packet size).\
    \  In the paper's outline of\n   further work, it was stated that no recommendation\
    \ had been made on\n   whether the queue size should be measured in bytes or packets,\
    \ but\n   noted that the difference could be significant.\n   When RED was recommended\
    \ for general deployment in 1998 [RFC2309],\n   the two modes were mentioned implying\
    \ the choice between them was a\n   question of performance, referring to a 1997\
    \ email [pktByteEmail] for\n   advice on tuning.  A later addendum to this email\
    \ introduced the\n   insight that there are in fact two orthogonal choices:\n\
    \   o  whether to measure queue length in bytes or packets (Section 4.1),\n  \
    \    and\n   o  whether the drop probability of an individual packet should depend\n\
    \      on its own size (Section 4.2).\n   The rest of this section is structured\
    \ accordingly.\n"
- title: 4.1.  Congestion Measurement Advice
  contents:
  - "4.1.  Congestion Measurement Advice\n   The choice of which metric to use to\
    \ measure queue length was left\n   open in RFC 2309.  It is now well understood\
    \ that queues for bit-\n   congestible resources should be measured in bytes,\
    \ and queues for\n   packet-congestible resources should be measured in packets\n\
    \   [pktByteEmail].\n   Congestion in some legacy bit-congestible buffers is only\
    \ measured in\n   packets not bytes.  In such cases, the operator has to take\
    \ into\n   account a typical mix of packet sizes when setting the thresholds.\n\
    \   Any AQM algorithm on such a buffer will be oversensitive to high\n   proportions\
    \ of small packets, e.g., a DoS attack, and under-sensitive\n   to high proportions\
    \ of large packets.  However, there is no need to\n   make allowances for the\
    \ possibility of such a legacy in future\n   protocol design.  This is safe because\
    \ any under-sensitivity during\n   unusual traffic mixes cannot lead to congestion\
    \ collapse given that\n   the buffer will eventually revert to tail drop, which\
    \ discards\n   proportionately more large packets.\n"
- title: 4.1.1.  Fixed-Size Packet Buffers
  contents:
  - "4.1.1.  Fixed-Size Packet Buffers\n   The question of whether to measure queues\
    \ in bytes or packets seems\n   to be well understood.  However, measuring congestion\
    \ is confusing\n   when the resource is bit-congestible but the queue into the\
    \ resource\n   is packet-congestible.  This section outlines the approach to take.\n\
    \   Some, mostly older, queuing hardware allocates fixed-size buffers in\n   which\
    \ to store each packet in the queue.  This hardware forwards\n   packets to the\
    \ line in one of two ways:\n   o  With some hardware, any fixed-size buffers not\
    \ completely filled\n      by a packet are padded when transmitted to the wire.\
    \  This case\n      should clearly be treated as packet-congestible, because both\n\
    \      queuing and transmission are in fixed MTU-size units.  Therefore,\n   \
    \   the queue length in packets is a good model of congestion of the\n      link.\n\
    \   o  More commonly, hardware with fixed-size packet buffers transmits\n    \
    \  packets to the line without padding.  This implies a hybrid\n      forwarding\
    \ system with transmission congestion dependent on the\n      size of packets\
    \ but queue congestion dependent on the number of\n      packets, irrespective\
    \ of their size.\n      Nonetheless, there would be no queue at all unless the\
    \ line had\n      become congested -- the root cause of any congestion is too\
    \ many\n      bytes arriving for the line.  Therefore, the AQM should measure\n\
    \      the queue length as the sum of all the packet sizes in bytes that\n   \
    \   are queued up waiting to be serviced by the line, irrespective of\n      whether\
    \ each packet is held in a fixed-size buffer.\n   In the (unlikely) first case\
    \ where use of padding means the queue\n   should be measured in packets, further\
    \ confusion is likely because\n   the fixed buffers are rarely all one size. \
    \ Typically, pools of\n   different-sized buffers are provided (Cisco uses the\
    \ term 'buffer\n   carving' for the process of dividing up memory into these pools\n\
    \   [IOSArch]).  Usually, if the pool of small buffers is exhausted,\n   arriving\
    \ small packets can borrow space in the pool of large buffers,\n   but not vice\
    \ versa.  However, there is no need to consider all this\n   complexity, because\
    \ the root cause of any congestion is still line\n   overload -- buffer consumption\
    \ is only the symptom.  Therefore, the\n   length of the queue should be measured\
    \ as the sum of the bytes in the\n   queue that will be transmitted to the line,\
    \ including any padding.\n   In the (unusual) case of transmission with padding,\
    \ this means the\n   sum of the sizes of the small buffers queued plus the sum\
    \ of the\n   sizes of the large buffers queued.\n   We will return to borrowing\
    \ of fixed-size buffers when we discuss\n   biasing the drop/marking probability\
    \ of a specific packet because of\n   its size in Section 4.2.1.  But here, we\
    \ can repeat the simple rule\n   for how to measure the length of queues of fixed\
    \ buffers: no matter\n   how complicated the buffering scheme is, ultimately a\
    \ transmission\n   line is nearly always bit-congestible so the number of bytes\
    \ queued\n   up waiting for the line measures how congested the line is, and it\
    \ is\n   rarely important to measure how congested the buffering system is.\n"
- title: 4.1.2.  Congestion Measurement without a Queue
  contents:
  - "4.1.2.  Congestion Measurement without a Queue\n   AQM algorithms are nearly\
    \ always described assuming there is a queue\n   for a congested resource and\
    \ the algorithm can use the queue length\n   to determine the probability that\
    \ it will drop or mark each packet.\n   But not all congested resources lead to\
    \ queues.  For instance, power-\n   limited resources are usually bit-congestible\
    \ if energy is primarily\n   required for transmission rather than header processing,\
    \ but it is\n   rare for a link protocol to build a queue as it approaches maximum\n\
    \   power.\n   Nonetheless, AQM algorithms do not require a queue in order to\
    \ work.\n   For instance, spectrum congestion can be modelled by signal quality\n\
    \   using the target bit-energy-to-noise-density ratio.  And, to model\n   radio\
    \ power exhaustion, transmission-power levels can be measured and\n   compared\
    \ to the maximum power available.  [ECNFixedWireless] proposes\n   a practical\
    \ and theoretically sound way to combine congestion\n   notification for different\
    \ bit-congestible resources at different\n   layers along an end-to-end path,\
    \ whether wireless or wired, and\n   whether with or without queues.\n   In wireless\
    \ protocols that use request to send / clear to send\n   (RTS / CTS) control,\
    \ such as some variants of IEEE802.11, it is\n   reasonable to base an AQM on\
    \ the time spent waiting for transmission\n   opportunities (TXOPs) even though\
    \ the wireless spectrum is usually\n   regarded as congested by bits (for a given\
    \ coding scheme).  This is\n   because requests for TXOPs queue up as the spectrum\
    \ gets congested by\n   all the bits being transferred.  So the time that TXOPs\
    \ are queued\n   directly reflects bit congestion of the spectrum.\n"
- title: 4.2.  Congestion Notification Advice
  contents:
  - '4.2.  Congestion Notification Advice

    '
- title: 4.2.1.  Network Bias When Encoding
  contents:
  - '4.2.1.  Network Bias When Encoding

    '
- title: 4.2.1.1.  Advice on Packet-Size Bias in RED
  contents:
  - "4.2.1.1.  Advice on Packet-Size Bias in RED\n   The previously mentioned email\
    \ [pktByteEmail] referred to by\n   [RFC2309] advised that most scarce resources\
    \ in the Internet were\n   bit-congestible, which is still believed to be true\
    \ (Section 1.1).\n   But it went on to offer advice that is updated by this memo.\
    \  It said\n   that drop probability should depend on the size of the packet being\n\
    \   considered for drop if the resource is bit-congestible, but not if it\n  \
    \ is packet-congestible.  The argument continued that if packet drops\n   were\
    \ inflated by packet size (byte-mode dropping), \"a flow's fraction\n   of the\
    \ packet drops is then a good indication of that flow's fraction\n   of the link\
    \ bandwidth in bits per second\".  This was consistent with\n   a referenced policing\
    \ mechanism being worked on at the time for\n   detecting unusually high bandwidth\
    \ flows, eventually published in\n   1999 [pBox].  However, the problem could\
    \ and should have been solved\n   by making the policing mechanism count the volume\
    \ of bytes randomly\n   dropped, not the number of packets.\n   A few months before\
    \ RFC 2309 was published, an addendum was added to\n   the above archived email\
    \ referenced from the RFC, in which the final\n   paragraph seemed to partially\
    \ retract what had previously been said.\n   It clarified that the question of\
    \ whether the probability of\n   dropping/marking a packet should depend on its\
    \ size was not related\n   to whether the resource itself was bit-congestible,\
    \ but a completely\n   orthogonal question.  However, the only example given had\
    \ the queue\n   measured in packets but packet drop depended on the size of the\n\
    \   packet in question.  No example was given the other way round.\n   In 2000,\
    \ Cnodder et al. [REDbyte] pointed out that there was an error\n   in the part\
    \ of the original 1993 RED algorithm that aimed to\n   distribute drops uniformly,\
    \ because it didn't correctly take into\n   account the adjustment for packet\
    \ size.  They recommended an\n   algorithm called RED_4 to fix this.  But they\
    \ also recommended a\n   further change, RED_5, to adjust the drop rate dependent\
    \ on the\n   square of the relative packet size.  This was indeed consistent with\n\
    \   one implied motivation behind RED's byte-mode drop -- that we should\n   reverse\
    \ engineer the network to improve the performance of dominant\n   end-to-end congestion\
    \ control mechanisms.  This memo makes a\n   different recommendations in Section\
    \ 2.\n   By 2003, a further change had been made to the adjustment for packet\n\
    \   size, this time in the RED algorithm of the ns2 simulator.  Instead\n   of\
    \ taking each packet's size relative to a 'maximum packet size', it\n   was taken\
    \ relative to a 'mean packet size', intended to be a static\n   value representative\
    \ of the 'typical' packet size on the link.  We\n   have not been able to find\
    \ a justification in the literature for this\n   change; however, Eddy and Allman\
    \ conducted experiments [REDbias] that\n   assessed how sensitive RED was to this\
    \ parameter, amongst other\n   things.  This changed algorithm can often lead\
    \ to drop probabilities\n   of greater than 1 (which gives a hint that there is\
    \ probably a\n   mistake in the theory somewhere).\n   On 10-Nov-2004, this variant\
    \ of byte-mode packet drop was made the\n   default in the ns2 simulator.  It\
    \ seems unlikely that byte-mode drop\n   has ever been implemented in production\
    \ networks (Appendix A);\n   therefore, any conclusions based on ns2 simulations\
    \ that use RED\n   without disabling byte-mode drop are likely to behave very\n\
    \   differently from RED in production networks.\n"
- title: 4.2.1.2.  Packet-Size Bias Regardless of AQM
  contents:
  - "4.2.1.2.  Packet-Size Bias Regardless of AQM\n   The byte-mode drop variant of\
    \ RED (or a similar variant of other AQM\n   algorithms) is not the only possible\
    \ bias towards small packets in\n   queuing systems.  We have already mentioned\
    \ that tail-drop queues\n   naturally tend to lock out large packets once they\
    \ are full.\n   But also, queues with fixed-size buffers reduce the probability\
    \ that\n   small packets will be dropped if (and only if) they allow small\n \
    \  packets to borrow buffers from the pools for larger packets (see\n   Section\
    \ 4.1.1).  Borrowing effectively makes the maximum queue size\n   for small packets\
    \ greater than that for large packets, because more\n   buffers can be used by\
    \ small packets while less will fit large\n   packets.  Incidentally, the bias\
    \ towards small packets from buffer\n   borrowing is nothing like as large as\
    \ that of RED's byte-mode drop.\n   Nonetheless, fixed-buffer memory with tail\
    \ drop is still prone to\n   lock out large packets, purely because of the tail-drop\
    \ aspect.  So,\n   fixed-size packet buffers should be augmented with a good AQM\n\
    \   algorithm and packet-mode drop.  If an AQM is too complicated to\n   implement\
    \ with multiple fixed buffer pools, the minimum necessary to\n   prevent large-packet\
    \ lockout is to ensure that smaller packets never\n   use the last available buffer\
    \ in any of the pools for larger packets.\n"
- title: 4.2.2.  Transport Bias When Decoding
  contents:
  - "4.2.2.  Transport Bias When Decoding\n   The above proposals to alter the network\
    \ equipment to bias towards\n   smaller packets have largely carried on outside\
    \ the IETF process.\n   Whereas, within the IETF, there are many different proposals\
    \ to alter\n   transport protocols to achieve the same goals, i.e., either to\
    \ make\n   the flow bit rate take into account packet size, or to protect\n  \
    \ control packets from loss.  This memo argues that altering transport\n   protocols\
    \ is the more principled approach.\n   A recently approved experimental RFC adapts\
    \ its transport-layer\n   protocol to take into account packet sizes relative\
    \ to typical TCP\n   packet sizes.  This proposes a new small-packet variant of\
    \ TCP-\n   friendly rate control (TFRC [RFC5348]), which is called TFRC-SP\n \
    \  [RFC4828].  Essentially, it proposes a rate equation that inflates\n   the\
    \ flow rate by the ratio of a typical TCP segment size (1,500 B\n   including\
    \ TCP header) over the actual segment size [PktSizeEquCC].\n   (There are also\
    \ other important differences of detail relative to\n   TFRC, such as using virtual\
    \ packets [CCvarPktSize] to avoid\n   responding to multiple losses per round\
    \ trip and using a minimum\n   inter-packet interval.)\n   Section 4.5.1 of the\
    \ TFRC-SP specification discusses the implications\n   of operating in an environment\
    \ where queues have been configured to\n   drop smaller packets with proportionately\
    \ lower probability than\n   larger ones.  But it only discusses TCP operating\
    \ in such an\n   environment, only mentioning TFRC-SP briefly when discussing\
    \ how to\n   define fairness with TCP.  And it only discusses the byte-mode\n\
    \   dropping version of RED as it was before Cnodder et al. pointed out\n   that\
    \ it didn't sufficiently bias towards small packets to make TCP\n   independent\
    \ of packet size.\n   So the TFRC-SP specification doesn't address the issue of\
    \ whether the\n   network or the transport _should_ handle fairness between different\n\
    \   packet sizes.  In Appendix B.4 of RFC 4828, it discusses the\n   possibility\
    \ of both TFRC-SP and some network buffers duplicating each\n   other's attempts\
    \ to deliberately bias towards small packets.  But the\n   discussion is not conclusive,\
    \ instead reporting simulations of many\n   of the possibilities in order to assess\
    \ performance but not\n   recommending any particular course of action.\n   The\
    \ paper originally proposing TFRC with virtual packets (VP-TFRC)\n   [CCvarPktSize]\
    \ proposed that there should perhaps be two variants to\n   cater for the different\
    \ variants of RED.  However, as the TFRC-SP\n   authors point out, there is no\
    \ way for a transport to know whether\n   some queues on its path have deployed\
    \ RED with byte-mode packet drop\n   (except if an exhaustive survey found that\
    \ no one has deployed it! --\n   see Appendix A).  Incidentally, VP-TFRC also\
    \ proposed that byte-mode\n   RED dropping should really square the packet-size\
    \ compensation factor\n   (like that of Cnodder's RED_5, but apparently unaware\
    \ of it).\n   Pre-congestion notification [RFC5670] is an IETF technology to use\
    \ a\n   virtual queue for AQM marking for packets within one Diffserv class\n\
    \   in order to give early warning prior to any real queuing.  The PCN-\n   marking\
    \ algorithms have been designed not to take into account packet\n   size when\
    \ forwarding through queues.  Instead, the general principle\n   has been to take\
    \ the sizes of marked packets into account when\n   monitoring the fraction of\
    \ marking at the edge of the network, as\n   recommended here.\n"
- title: 4.2.3.  Making Transports Robust against Control Packet Losses
  contents:
  - "4.2.3.  Making Transports Robust against Control Packet Losses\n   Recently,\
    \ two RFCs have defined changes to TCP that make it more\n   robust against losing\
    \ small control packets [RFC5562] [RFC5690].  In\n   both cases, they note that\
    \ the case for these two TCP changes would\n   be weaker if RED were biased against\
    \ dropping small packets.  We\n   argue here that these two proposals are a safer\
    \ and more principled\n   way to achieve TCP performance improvements than reverse\
    \ engineering\n   RED to benefit TCP.\n   Although there are no known proposals,\
    \ it would also be possible and\n   perfectly valid to make control packets robust\
    \ against drop by\n   requesting a scheduling class with lower drop probability,\
    \ which\n   would be achieved by re-marking to a Diffserv code point [RFC2474]\n\
    \   within the same behaviour aggregate.\n   Although not brought to the IETF,\
    \ a simple proposal from Wischik\n   [DupTCP] suggests that the first three packets\
    \ of every TCP flow\n   should be routinely duplicated after a short delay.  It\
    \ shows that\n   this would greatly improve the chances of short flows completing\n\
    \   quickly, but it would hardly increase traffic levels on the Internet,\n  \
    \ because Internet bytes have always been concentrated in the large\n   flows.\
    \  It further shows that the performance of many typical\n   applications depends\
    \ on completion of long serial chains of short\n   messages.  It argues that,\
    \ given most of the value people get from\n   the Internet is concentrated within\
    \ short flows, this simple\n   expedient would greatly increase the value of the\
    \ best-effort\n   Internet at minimal cost.  A similar but more extensive approach\
    \ has\n   been evaluated on Google servers [GentleAggro].\n   The proposals discussed\
    \ in this sub-section are experimental\n   approaches that are not yet in wide\
    \ operational use, but they are\n   existence proofs that transports can make\
    \ themselves robust against\n   loss of control packets.  The examples are all\
    \ TCP-based, but\n   applications over non-TCP transports could mitigate loss\
    \ of control\n   packets by making similar use of Diffserv, data duplication,\
    \ FEC,\n   etc.\n"
- title: '4.2.4.  Congestion Notification: Summary of Conflicting Advice'
  contents:
  - "4.2.4.  Congestion Notification: Summary of Conflicting Advice\n   +-----------+-----------------+-----------------+-------------------+\n\
    \   | transport |  RED_1 (packet- |  RED_4 (linear  |   RED_5 (square   |\n  \
    \ |        cc |    mode drop)   | byte-mode drop) |  byte-mode drop)  |\n   +-----------+-----------------+-----------------+-------------------+\n\
    \   |    TCP or |    s/sqrt(p)    |    sqrt(s/p)    |     1/sqrt(p)     |\n  \
    \ |      TFRC |                 |                 |                   |\n   |\
    \   TFRC-SP |    1/sqrt(p)    |   1/sqrt(s*p)   |   1/(s*sqrt(p))   |\n   +-----------+-----------------+-----------------+-------------------+\n\
    \    Table 2: Dependence of flow bit rate per RTT on packet size, s, and\n   \
    \  drop probability, p, when there is network and/or transport bias\n        \
    \         towards small packets to varying degrees\n   Table 2 aims to summarise\
    \ the potential effects of all the advice\n   from different sources.  Each column\
    \ shows a different possible AQM\n   behaviour in different queues in the network,\
    \ using the terminology\n   of Cnodder et al. outlined earlier (RED_1 is basic\
    \ RED with packet-\n   mode drop).  Each row shows a different transport behaviour:\
    \ TCP\n   [RFC5681] and TFRC [RFC5348] on the top row with TFRC-SP [RFC4828]\n\
    \   below.  Each cell shows how the bits per round trip of a flow depends\n  \
    \ on packet size, s, and drop probability, p.  In order to declutter\n   the formulae\
    \ to focus on packet-size dependence, they are all given\n   per round trip, which\
    \ removes any RTT term.\n   Let us assume that the goal is for the bit rate of\
    \ a flow to be\n   independent of packet size.  Suppressing all inessential details,\
    \ the\n   table shows that this should either be achievable by not altering the\n\
    \   TCP transport in a RED_5 network, or using the small packet TFRC-SP\n   transport\
    \ (or similar) in a network without any byte-mode dropping\n   RED (top right\
    \ and bottom left).  Top left is the 'do nothing'\n   scenario, while bottom right\
    \ is the 'do both' scenario in which the\n   bit rate would become far too biased\
    \ towards small packets.  Of\n   course, if any form of byte-mode dropping RED\
    \ has been deployed on a\n   subset of queues that congest, each path through\
    \ the network will\n   present a different hybrid scenario to its transport.\n\
    \   Whatever the case, we can see that the linear byte-mode drop column\n   in\
    \ the middle would considerably complicate the Internet.  Even if\n   one believes\
    \ the network should be doing the biasing, linear byte-\n   mode drop is a half-way\
    \ house that doesn't bias enough towards small\n   packets.  Section 2 recommends\
    \ that _all_ bias in network equipment\n   towards small packets should be turned\
    \ off -- if indeed any equipment\n   vendors have implemented it -- leaving packet-size\
    \ bias solely as the\n   preserve of the transport layer (solely the leftmost,\
    \ packet-mode\n   drop column).\n   In practice, it seems that no deliberate bias\
    \ towards small packets\n   has been implemented for production networks.  Of\
    \ the 19% of vendors\n   who responded to a survey of 84 equipment vendors, none\
    \ had\n   implemented byte-mode drop in RED (see Appendix A for details).\n"
- title: 5.  Outstanding Issues and Next Steps
  contents:
  - '5.  Outstanding Issues and Next Steps

    '
- title: 5.1.  Bit-congestible Network
  contents:
  - "5.1.  Bit-congestible Network\n   For a connectionless network with nearly all\
    \ resources being bit-\n   congestible, the recommended position is clear -- the\
    \ network should\n   not make allowance for packet sizes and the transport should.\
    \  This\n   leaves two outstanding issues:\n   o  The question of how to handle\
    \ any legacy AQM deployments using\n      byte-mode drop;\n   o  The need to start\
    \ a programme to update transport congestion\n      control protocol standards\
    \ to take packet size into account.\n   A survey of equipment vendors (Section\
    \ 4.2.4) found no evidence that\n   byte-mode packet drop had been implemented,\
    \ so deployment will be\n   sparse at best.  A migration strategy is not really\
    \ needed to remove\n   an algorithm that may not even be deployed.\n   A programme\
    \ of experimental updates to take packet size into account\n   in transport congestion\
    \ control protocols has already started with\n   TFRC-SP [RFC4828].\n"
- title: 5.2.  Bit- and Packet-Congestible Network
  contents:
  - "5.2.  Bit- and Packet-Congestible Network\n   The position is much less clear-cut\
    \ if the Internet becomes populated\n   by a more even mix of both packet-congestible\
    \ and bit-congestible\n   resources (see Appendix B.2).  This problem is not pressing,\
    \ because\n   most Internet resources are designed to be bit-congestible before\n\
    \   packet processing starts to congest (see Section 1.1).\n   The IRTF's Internet\
    \ Congestion Control Research Group (ICCRG) has set\n   itself the task of reaching\
    \ consensus on generic forwarding\n   mechanisms that are necessary and sufficient\
    \ to support the\n   Internet's future congestion control requirements (the first\n\
    \   challenge in [RFC6077]).  The research question of whether packet\n   congestion\
    \ might become common and what to do if it does may in the\n   future be explored\
    \ in the IRTF (the \"Challenge 3: Packet Size\" in\n   [RFC6077]).\n   Note that\
    \ sometimes it seems that resources might be congested by\n   neither bits nor\
    \ packets, e.g., where the queue for access to a\n   wireless medium is in units\
    \ of transmission opportunities.  However,\n   the root cause of congestion of\
    \ the underlying spectrum is overload\n   of bits (see Section 4.1.2).\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   This memo recommends that queues do not bias\
    \ drop probability due to\n   packets size.  For instance, dropping small packets\
    \ less often than\n   large ones creates a perverse incentive for transports to\
    \ break down\n   their flows into tiny segments.  One of the benefits of implementing\n\
    \   AQM was meant to be to remove this perverse incentive that tail-drop\n   queues\
    \ gave to small packets.\n   In practice, transports cannot all be trusted to\
    \ respond to\n   congestion.  So another reason for recommending that queues not\
    \ bias\n   drop probability towards small packets is to avoid the vulnerability\n\
    \   to small-packet DDoS attacks that would otherwise result.  One of the\n  \
    \ benefits of implementing AQM was meant to be to remove tail drop's\n   DoS vulnerability\
    \ to small packets, so we shouldn't add it back\n   again.\n   If most queues\
    \ implemented AQM with byte-mode drop, the resulting\n   network would amplify\
    \ the potency of a small-packet DDoS attack.  At\n   the first queue, the stream\
    \ of packets would push aside a greater\n   proportion of large packets, so more\
    \ of the small packets would\n   survive to attack the next queue.  Thus a flood\
    \ of small packets\n   would continue on towards the destination, pushing regular\
    \ traffic\n   with large packets out of the way in one queue after the next, but\n\
    \   suffering much less drop itself.\n   Appendix C explains why the ability of\
    \ networks to police the\n   response of _any_ transport to congestion depends\
    \ on bit-congestible\n   network resources only doing packet-mode drop, not byte-mode\
    \ drop.\n   In summary, it says that making drop probability depend on the size\n\
    \   of the packets that bits happen to be divided into simply encourages\n   the\
    \ bits to be divided into smaller packets.  Byte-mode drop would\n   therefore\
    \ irreversibly complicate any attempt to fix the Internet's\n   incentive structures.\n"
- title: 7.  Conclusions
  contents:
  - "7.  Conclusions\n   This memo identifies the three distinct stages of the congestion\n\
    \   notification process where implementations need to decide whether to\n   take\
    \ packet size into account.  The recommendations provided in\n   Section 2 of\
    \ this memo are different in each case:\n   o  When network equipment measures\
    \ the length of a queue, if it is\n      not feasible to use time; it is recommended\
    \ to count in bytes if\n      the network resource is congested by bytes, or to\
    \ count in packets\n      if is congested by packets.\n   o  When network equipment\
    \ decides whether to drop (or mark) a packet,\n      it is recommended that the\
    \ size of the particular packet should\n      not be taken into account.\n   o\
    \  However, when a transport algorithm responds to a dropped or\n      marked\
    \ packet, the size of the rate reduction should be\n      proportionate to the\
    \ size of the packet.\n   In summary, the answers are 'it depends', 'no', and\
    \ 'yes',\n   respectively.\n   For the specific case of RED, this means that byte-mode\
    \ queue\n   measurement will often be appropriate, but the use of byte-mode drop\n\
    \   is very strongly discouraged.\n   At the transport layer, the IETF should\
    \ continue updating congestion\n   control protocols to take into account the\
    \ size of each packet that\n   indicates congestion.  Also, the IETF should continue\
    \ to make\n   protocols less sensitive to losing control packets like SYNs, pure\n\
    \   ACKs, and DNS exchanges.  Although many control packets happen to be\n   small,\
    \ the alternative of network equipment favouring all small\n   packets would be\
    \ dangerous.  That would create perverse incentives to\n   split data transfers\
    \ into smaller packets.\n   The memo develops these recommendations from principled\
    \ arguments\n   concerning scaling, layering, incentives, inherent efficiency,\n\
    \   security, and 'policeability'.  It also addresses practical issues\n   such\
    \ as specific buffer architectures and incremental deployment.\n   Indeed, a limited\
    \ survey of RED implementations is discussed, which\n   shows there appears to\
    \ be little, if any, installed base of RED's\n   byte-mode drop.  Therefore, it\
    \ can be deprecated with little, if any,\n   incremental deployment complications.\n\
    \   The recommendations have been developed on the well-founded basis\n   that\
    \ most Internet resources are bit-congestible, not packet-\n   congestible.  We\
    \ need to know the likelihood that this assumption\n   will prevail in the longer\
    \ term and, if it might not, what protocol\n   changes will be needed to cater\
    \ for a mix of the two.  The IRTF\n   Internet Congestion Control Research Group\
    \ (ICCRG) is currently\n   working on these problems [RFC6077].\n"
- title: 8.  Acknowledgements
  contents:
  - "8.  Acknowledgements\n   Thank you to Sally Floyd, who gave extensive and useful\
    \ review\n   comments.  Also thanks for the reviews from Philip Eardley, David\n\
    \   Black, Fred Baker, David Taht, Toby Moncaster, Arnaud Jacquet, and\n   Mirja\
    \ Kuehlewind, as well as helpful explanations of different\n   hardware approaches\
    \ from Larry Dunn and Fred Baker.  We are grateful\n   to Bruce Davie and his\
    \ colleagues for providing a timely and\n   efficient survey of RED implementation\
    \ in Cisco's product range.\n   Also, grateful thanks to Toby Moncaster, Will\
    \ Dormann, John Regnault,\n   Simon Carter, and Stefaan De Cnodder who further\
    \ helped survey the\n   current status of RED implementation and deployment, and,\
    \ finally,\n   thanks to the anonymous individuals who responded.\n   Bob Briscoe\
    \ and Jukka Manner were partly funded by Trilogy and\n   Trilogy 2, research projects\
    \ (ICT-216372, ICT-317756) supported by\n   the European Community under its Seventh\
    \ Framework Programme.  The\n   views expressed here are those of the authors\
    \ only.\n"
- title: 9.  References
  contents:
  - '9.  References

    '
- title: 9.1.  Normative References
  contents:
  - "9.1.  Normative References\n   [RFC2119]  Bradner, S., \"Key words for use in\
    \ RFCs to Indicate\n              Requirement Levels\", BCP 14, RFC 2119, March\
    \ 1997.\n   [RFC2309]  Braden, B., Clark, D., Crowcroft, J., Davie, B., Deering,\n\
    \              S., Estrin, D., Floyd, S., Jacobson, V., Minshall, G.,\n      \
    \        Partridge, C., Peterson, L., Ramakrishnan, K., Shenker,\n           \
    \   S., Wroclawski, J., and L. Zhang, \"Recommendations on\n              Queue\
    \ Management and Congestion Avoidance in the\n              Internet\", RFC 2309,\
    \ April 1998.\n   [RFC2914]  Floyd, S., \"Congestion Control Principles\", BCP\
    \ 41, RFC\n              2914, September 2000.\n   [RFC3168]  Ramakrishnan, K.,\
    \ Floyd, S., and D. Black, \"The Addition\n              of Explicit Congestion\
    \ Notification (ECN) to IP\", RFC\n              3168, September 2001.\n"
- title: 9.2.  Informative References
  contents:
  - "9.2.  Informative References\n   [BLUE02]   Feng, W-c., Shin, K., Kandlur, D.,\
    \ and D. Saha, \"The BLUE\n              active queue management algorithms\"\
    , IEEE/ACM Transactions\n              on Networking 10(4) 513-528, August 2002,\n\
    \              <http://dx.doi.org/10.1109/TNET.2002.801399>.\n   [CCvarPktSize]\n\
    \              Widmer, J., Boutremans, C., and J-Y. Le Boudec, \"End-to-\n   \
    \           end congestion control for TCP-friendly flows with\n             \
    \ variable packet size\", ACM CCR 34(2) 137-151, April 2004,\n              <http://doi.acm.org/10.1145/997150.997162>.\n\
    \   [CHOKe_Var_Pkt]\n              Psounis, K., Pan, R., and B. Prabhaker, \"\
    Approximate Fair\n              Dropping for Variable-Length Packets\", IEEE Micro\n\
    \              21(1):48-56, January-February 2001,\n              <http://ieeexplore.ieee.org/xpl/\n\
    \              articleDetails.jsp?arnumber=903061>.\n   [CoDel]    Nichols, K.\
    \ and V. Jacobson, \"Controlled Delay Active\n              Queue Management\"\
    , Work in Progress, February 2013.\n   [DRQ]      Shin, M., Chong, S., and I.\
    \ Rhee, \"Dual-Resource TCP/AQM\n              for Processing-Constrained Networks\"\
    , IEEE/ACM\n              Transactions on Networking Vol 16, issue 2, April 2008,\n\
    \              <http://dx.doi.org/10.1109/TNET.2007.900415>.\n   [DupTCP]   Wischik,\
    \ D., \"Short messages\", Philosophical Transactions\n              of the Royal\
    \ Society A 366(1872):1941-1953, June 2008,\n              <http://rsta.royalsocietypublishing.org/content/366/1872/\n\
    \              1941.full.pdf+html>.\n   [ECNFixedWireless]\n              Siris,\
    \ V., \"Resource Control for Elastic Traffic in CDMA\n              Networks\"\
    , Proc. ACM MOBICOM'02 , September 2002,\n              <http://www.ics.forth.gr/netlab/publications/\n\
    \              resource_control_elastic_cdma.html>.\n   [Evol_cc]  Gibbens, R.\
    \ and F. Kelly, \"Resource pricing and the\n              evolution of congestion\
    \ control\", Automatica\n              35(12)1969-1985, December 1999,\n     \
    \         <http://www.sciencedirect.com/science/article/pii/\n              S0005109899001351>.\n\
    \   [GentleAggro]\n              Flach, T., Dukkipati, N., Terzis, A., Raghavan,\
    \ B.,\n              Cardwell, N., Cheng, Y., Jain, A., Hao, S., Katz-Bassett,\n\
    \              E., and R. Govindan, \"Reducing web latency: the virtue of\n  \
    \            gentle aggression\", ACM SIGCOMM CCR 43(4)159-170, August\n     \
    \         2013, <http://doi.acm.org/10.1145/2486001.2486014>.\n   [IOSArch]  Bollapragada,\
    \ V., White, R., and C. Murphy, \"Inside Cisco\n              IOS Software Architecture\"\
    , Cisco Press: CCIE Professional\n              Development ISBN13: 978-1-57870-181-0,\
    \ July 2000.\n   [PIE]      Pan, R., Natarajan, P., Piglione, C., Prabhu, M.,\n\
    \              Subramanian, V., Baker, F., and B. Steeg, \"PIE: A\n          \
    \    Lightweight Control Scheme To Address the Bufferbloat\n              Problem\"\
    , Work in Progress, February 2014.\n   [PktSizeEquCC]\n              Vasallo,\
    \ P., \"Variable Packet Size Equation-Based\n              Congestion Control\"\
    , ICSI Technical Report tr-00-008,\n              2000, <http://http.icsi.berkeley.edu/ftp/global/pub/\n\
    \              techreports/2000/tr-00-008.pdf>.\n   [RED93]    Floyd, S. and V.\
    \ Jacobson, \"Random Early Detection (RED)\n              gateways for Congestion\
    \ Avoidance\", IEEE/ACM Transactions\n              on Networking 1(4) 397--413,\
    \ August 1993,\n              <http://ieeexplore.ieee.org/xpls/\n            \
    \  abs_all.jsp?arnumber=251892>.\n   [REDbias]  Eddy, W. and M. Allman, \"A Comparison\
    \ of RED's Byte and\n              Packet Modes\", Computer Networks 42(3) 261--280,\
    \ June\n              2003,\n              <http://www.ir.bbn.com/documents/articles/redbias.ps>.\n\
    \   [REDbyte]  De Cnodder, S., Elloumi, O., and K. Pauwels, \"Effect of\n    \
    \          different packet sizes on RED performance\", Proc. 5th IEEE\n     \
    \         Symposium on Computers and Communications (ISCC) 793-799,\n        \
    \      July 2000, <http://ieeexplore.ieee.org/xpls/\n              abs_all.jsp?arnumber=860741>.\n\
    \   [RFC2474]  Nichols, K., Blake, S., Baker, F., and D. Black,\n            \
    \  \"Definition of the Differentiated Services Field (DS\n              Field)\
    \ in the IPv4 and IPv6 Headers\", RFC 2474, December\n              1998.\n  \
    \ [RFC3426]  Floyd, S., \"General Architectural and Policy\n              Considerations\"\
    , RFC 3426, November 2002.\n   [RFC3550]  Schulzrinne, H., Casner, S., Frederick,\
    \ R., and V.\n              Jacobson, \"RTP: A Transport Protocol for Real-Time\n\
    \              Applications\", STD 64, RFC 3550, July 2003.\n   [RFC3714]  Floyd,\
    \ S. and J. Kempf, \"IAB Concerns Regarding Congestion\n              Control\
    \ for Voice Traffic in the Internet\", RFC 3714,\n              March 2004.\n\
    \   [RFC4828]  Floyd, S. and E. Kohler, \"TCP Friendly Rate Control\n        \
    \      (TFRC): The Small-Packet (SP) Variant\", RFC 4828, April\n            \
    \  2007.\n   [RFC5348]  Floyd, S., Handley, M., Padhye, J., and J. Widmer, \"\
    TCP\n              Friendly Rate Control (TFRC): Protocol Specification\", RFC\n\
    \              5348, September 2008.\n   [RFC5562]  Kuzmanovic, A., Mondal, A.,\
    \ Floyd, S., and K.\n              Ramakrishnan, \"Adding Explicit Congestion\
    \ Notification\n              (ECN) Capability to TCP's SYN/ACK Packets\", RFC\
    \ 5562, June\n              2009.\n   [RFC5670]  Eardley, P., \"Metering and Marking\
    \ Behaviour of PCN-\n              Nodes\", RFC 5670, November 2009.\n   [RFC5681]\
    \  Allman, M., Paxson, V., and E. Blanton, \"TCP Congestion\n              Control\"\
    , RFC 5681, September 2009.\n   [RFC5690]  Floyd, S., Arcia, A., Ros, D., and\
    \ J. Iyengar, \"Adding\n              Acknowledgement Congestion Control to TCP\"\
    , RFC 5690,\n              February 2010.\n   [RFC6077]  Papadimitriou, D., Welzl,\
    \ M., Scharf, M., and B. Briscoe,\n              \"Open Research Issues in Internet\
    \ Congestion Control\", RFC\n              6077, February 2011.\n   [RFC6679]\
    \  Westerlund, M., Johansson, I., Perkins, C., O'Hanlon, P.,\n              and\
    \ K. Carlberg, \"Explicit Congestion Notification (ECN)\n              for RTP\
    \ over UDP\", RFC 6679, August 2012.\n   [RFC6789]  Briscoe, B., Woundy, R., and\
    \ A. Cooper, \"Congestion\n              Exposure (ConEx) Concepts and Use Cases\"\
    , RFC 6789,\n              December 2012.\n   [Rate_fair_Dis]\n              Briscoe,\
    \ B., \"Flow Rate Fairness: Dismantling a Religion\",\n              ACM CCR 37(2)63-74,\
    \ April 2007,\n              <http://portal.acm.org/citation.cfm?id=1232926>.\n\
    \   [gentle_RED]\n              Floyd, S., \"Recommendation on using the \"gentle_\"\
    \ variant\n              of RED\", Web page , March 2000,\n              <http://www.icir.org/floyd/red/gentle.html>.\n\
    \   [pBox]     Floyd, S. and K. Fall, \"Promoting the Use of End-to-End\n    \
    \          Congestion Control\", IEEE/ACM Transactions on Networking\n       \
    \       7(4) 458--472, August 1999, <http://ieeexplore.ieee.org/\n           \
    \   xpls/abs_all.jsp?arnumber=793002>.\n   [pktByteEmail]\n              Floyd,\
    \ S., \"RED: Discussions of Byte and Packet Modes\",\n              email, March\
    \ 1997,\n              <http://ee.lbl.gov/floyd/REDaveraging.txt>.\n"
- title: Appendix A.  Survey of RED Implementation Status
  contents:
  - "Appendix A.  Survey of RED Implementation Status\n   This Appendix is informative,\
    \ not normative.\n   In May 2007 a survey was conducted of 84 vendors to assess\
    \ how widely\n   drop probability based on packet size has been implemented in\
    \ RED\n   Table 3.  About 19% of those surveyed replied, giving a sample size\n\
    \   of 16.  Although in most cases we do not have permission to identify\n   the\
    \ respondents, we can say that those that have responded include\n   most of the\
    \ larger equipment vendors, covering a large fraction of\n   the market.  The\
    \ two who gave permission to be identified were Cisco\n   and Alcatel-Lucent.\
    \  The others range across the large network\n   equipment vendors at L3 & L2,\
    \ firewall vendors, wireless equipment\n   vendors, as well as large software\
    \ businesses with a small selection\n   of networking products.  All those who\
    \ responded confirmed that they\n   have not implemented the variant of RED with\
    \ drop dependent on packet\n   size (2 were fairly sure they had not but needed\
    \ to check more\n   thoroughly).  At the time the survey was conducted, Linux\
    \ did not\n   implement RED with packet-size bias of drop, although we have not\n\
    \   investigated a wider range of open source code.\n     +-------------------------------+----------------+--------------+\n\
    \     |                      Response | No. of vendors | % of vendors |\n    \
    \ +-------------------------------+----------------+--------------+\n     |  \
    \             Not implemented |             14 |          17% |\n     |    Not\
    \ implemented (probably) |              2 |           2% |\n     |           \
    \        Implemented |              0 |           0% |\n     |               \
    \    No response |             68 |          81% |\n     | Total companies/orgs\
    \ surveyed |             84 |         100% |\n     +-------------------------------+----------------+--------------+\n\
    \    Table 3: Vendor Survey on byte-mode drop variant of RED (lower drop\n   \
    \                   probability for small packets)\n   Where reasons were given\
    \ for why the byte-mode drop variant had not\n   been implemented, the extra complexity\
    \ of packet-bias code was most\n   prevalent, though one vendor had a more principled\
    \ reason for\n   avoiding it -- similar to the argument of this document.\n  \
    \ Our survey was of vendor implementations, so we cannot be certain\n   about\
    \ operator deployment.  But we believe many queues in the\n   Internet are still\
    \ tail drop.  The company of one of the co-authors\n   (BT) has widely deployed\
    \ RED; however, many tail-drop queues are\n   bound to still exist, particularly\
    \ in access network equipment and on\n   middleboxes like firewalls, where RED\
    \ is not always available.\n   Routers using a memory architecture based on fixed-size\
    \ buffers with\n   borrowing may also still be prevalent in the Internet.  As\
    \ explained\n   in Section 4.2.1, these also provide a marginal (but legitimate)\
    \ bias\n   towards small packets.  So even though RED byte-mode drop is not\n\
    \   prevalent, it is likely there is still some bias towards small\n   packets\
    \ in the Internet due to tail-drop and fixed-buffer borrowing.\n"
- title: Appendix B.  Sufficiency of Packet-Mode Drop
  contents:
  - "Appendix B.  Sufficiency of Packet-Mode Drop\n   This Appendix is informative,\
    \ not normative.\n   Here we check that packet-mode drop (or marking) in the network\
    \ gives\n   sufficiently generic information for the transport layer to use. \
    \ We\n   check against a 2x2 matrix of four scenarios that may occur now or in\n\
    \   the future (Table 4).  Checking the two scenarios in each of the\n   horizontal\
    \ and vertical dimensions tests the extremes of sensitivity\n   to packet size\
    \ in the transport and in the network respectively.\n   Note that this section\
    \ does not consider byte-mode drop at all.\n   Having deprecated byte-mode drop,\
    \ the goal here is to check that\n   packet-mode drop will be sufficient in all\
    \ cases.\n   +-------------------------------+-----------------+-----------------+\n\
    \   |                  Transport -> |  a) Independent | b) Dependent on |\n  \
    \ | ----------------------------- |  of packet size |  packet size of |\n   |\
    \ Network                       |  of congestion  |    congestion   |\n   |  \
    \                             |  notifications  |  notifications  |\n   +-------------------------------+-----------------+-----------------+\n\
    \   | 1) Predominantly bit-         |   Scenario a1)  |   Scenario b1)  |\n  \
    \ | congestible network           |                 |                 |\n   |\
    \ 2) Mix of bit-congestible and |   Scenario a2)  |   Scenario b2)  |\n   | pkt-congestible\
    \ network       |                 |                 |\n   +-------------------------------+-----------------+-----------------+\n\
    \                Table 4: Four Possible Congestion Scenarios\n   Appendix B.1\
    \ focuses on the horizontal dimension of Table 4 checking\n   that packet-mode\
    \ drop (or marking) gives sufficient information,\n   whether or not the transport\
    \ uses it -- scenarios b) and a)\n   respectively.\n   Appendix B.2 focuses on\
    \ the vertical dimension of Table 4, checking\n   that packet-mode drop gives\
    \ sufficient information to the transport\n   whether resources in the network\
    \ are bit-congestible or packet-\n   congestible (these terms are defined in Section\
    \ 1.1).\n   Notation:  To be concrete, we will compare two flows with different\n\
    \      packet sizes, s_1 and s_2.  As an example, we will take\n      s_1 = 60\
    \ B = 480 b and s_2 = 1,500 B = 12,000 b.\n      A flow's bit rate, x [bps], is\
    \ related to its packet rate, u\n      [pps], by\n         x(t) = s*u(t).\n  \
    \    In the bit-congestible case, path congestion will be denoted by\n      p_b,\
    \ and in the packet-congestible case by p_p.  When either case\n      is implied,\
    \ the letter p alone will denote path congestion.\n"
- title: B.1.  Packet-Size (In)Dependence in Transports
  contents:
  - "B.1.  Packet-Size (In)Dependence in Transports\n   In all cases, we consider\
    \ a packet-mode drop queue that indicates\n   congestion by dropping (or marking)\
    \ packets with probability p\n   irrespective of packet size.  We use an example\
    \ value of loss\n   (marking) probability, p=0.1%.\n   A transport like TCP as\
    \ specified in RFC 5681 treats a congestion\n   notification on any packet whatever\
    \ its size as one event.  However,\n   a network with just the packet-mode drop\
    \ algorithm gives more\n   information if the transport chooses to use it.  We\
    \ will use Table 5\n   to illustrate this.\n   We will set aside the last column\
    \ until later.  The columns labelled\n   'Flow 1' and 'Flow 2' compare two flows\
    \ consisting of 60 B and\n   1,500 B packets respectively.  The body of the table\
    \ considers two\n   separate cases, one where the flows have an equal bit rate\
    \ and the\n   other with equal packet rates.  In both cases, the two flows fill\
    \ a\n   96 Mbps link.  Therefore, in the equal bit rate case, they each have\n\
    \   half the bit rate (48Mbps).  Whereas, with equal packet rates, Flow 1\n  \
    \ uses 25 times smaller packets so it gets 25 times less bit rate -- it\n   only\
    \ gets 1/(1+25) of the link capacity (96 Mbps / 26 = 4 Mbps after\n   rounding).\
    \  In contrast Flow 2 gets 25 times more bit rate (92 Mbps)\n   in the equal packet\
    \ rate case because its packets are 25 times\n   larger.  The packet rate shown\
    \ for each flow could easily be derived\n   once the bit rate was known by dividing\
    \ the bit rate by packet size,\n   as shown in the column labelled 'Formula'.\n\
    \      Parameter               Formula       Flow 1   Flow 2 Combined\n      -----------------------\
    \ ----------- -------- -------- --------\n      Packet size             s/8  \
    \           60 B  1,500 B    (Mix)\n      Packet size             s          \
    \    480 b 12,000 b    (Mix)\n      Pkt loss probability    p               0.1%\
    \     0.1%     0.1%\n      EQUAL BIT RATE CASE\n      Bit rate               \
    \ x            48 Mbps  48 Mbps  96 Mbps\n      Packet rate             u = x/s\
    \     100 kpps   4 kpps 104 kpps\n      Absolute pkt-loss rate  p*u          100\
    \ pps    4 pps  104 pps\n      Absolute bit-loss rate  p*u*s        48 kbps  48\
    \ kbps  96 kbps\n      Ratio of lost/sent pkts p*u/u           0.1%     0.1% \
    \    0.1%\n      Ratio of lost/sent bits p*u*s/(u*s)     0.1%     0.1%     0.1%\n\
    \      EQUAL PACKET RATE CASE\n      Bit rate                x             4 Mbps\
    \  92 Mbps  96 Mbps\n      Packet rate             u = x/s       8 kpps   8 kpps\
    \  15 kpps\n      Absolute pkt-loss rate  p*u            8 pps    8 pps   15 pps\n\
    \      Absolute bit-loss rate  p*u*s         4 kbps  92 kbps  96 kbps\n      Ratio\
    \ of lost/sent pkts p*u/u           0.1%     0.1%     0.1%\n      Ratio of lost/sent\
    \ bits p*u*s/(u*s)     0.1%     0.1%     0.1%\n    Table 5: Absolute Loss Rates\
    \ and Loss Ratios for Flows of Small and\n                      Large Packets\
    \ and Both Combined\n   So far, we have merely set up the scenarios.  We now consider\n\
    \   congestion notification in the scenario.  Two TCP flows with the same\n  \
    \ round-trip time aim to equalise their packet-loss rates over time;\n   that\
    \ is, the number of packets lost in a second, which is the packets\n   per second\
    \ (u) multiplied by the probability that each one is dropped\n   (p).  Thus, TCP\
    \ converges on the case labelled 'Equal packet rate' in\n   the table, where both\
    \ flows aim for the same absolute packet-loss\n   rate (both 8 pps in the table).\n\
    \   Packet-mode drop actually gives flows sufficient information to\n   measure\
    \ their loss rate in bits per second, if they choose, not just\n   packets per\
    \ second.  Each flow can count the size of a lost or marked\n   packet and scale\
    \ its rate response in proportion (as TFRC-SP does).\n   The result is shown in\
    \ the row entitled 'Absolute bit-loss rate',\n   where the bits lost in a second\
    \ is the packets per second (u)\n   multiplied by the probability of losing a\
    \ packet (p) multiplied by\n   the packet size (s).  Such an algorithm would try\
    \ to remove any\n   imbalance in the bit-loss rate such as the wide disparity\
    \ in the case\n   labelled 'Equal packet rate' (4k bps vs. 92 kbps).  Instead,\
    \ a\n   packet-size-dependent algorithm would aim for equal bit-loss rates,\n\
    \   which would drive both flows towards the case labelled 'Equal bit\n   rate',\
    \ by driving them to equal bit-loss rates (both 48 kbps in this\n   example).\n\
    \   The explanation so far has assumed that each flow consists of packets\n  \
    \ of only one constant size.  Nonetheless, it extends naturally to\n   flows with\
    \ mixed packet sizes.  In the right-most column of Table 5,\n   a flow of mixed-size\
    \ packets is created simply by considering Flow 1\n   and Flow 2 as a single aggregated\
    \ flow.  There is no need for a flow\n   to maintain an average packet size. \
    \ It is only necessary for the\n   transport to scale its response to each congestion\
    \ indication by the\n   size of each individual lost (or marked) packet.  Taking,\
    \ for\n   example, the case labelled 'Equal packet rate', in one second about\
    \ 8\n   small packets and 8 large packets are lost (making closer to 15 than\n\
    \   16 losses per second due to rounding).  If the transport multiplies\n   each\
    \ loss by its size, in one second it responds to 8*480 and\n   8*12,000 lost bits,\
    \ adding up to 96,000 lost bits in a second.  This\n   double checks correctly,\
    \ being the same as 0.1% of the total bit rate\n   of 96 Mbps.  For completeness,\
    \ the formula for absolute bit-loss rate\n   is p(u1*s1+u2*s2).\n   Incidentally,\
    \ a transport will always measure the loss probability\n   the same, irrespective\
    \ of whether it measures in packets or in bytes.\n   In other words, the ratio\
    \ of lost packets to sent packets will be the\n   same as the ratio of lost bytes\
    \ to sent bytes.  (This is why TCP's\n   bit rate is still proportional to packet\
    \ size, even when byte\n   counting is used, as recommended for TCP in [RFC5681],\
    \ mainly for\n   orthogonal security reasons.)  This is intuitively obvious by\n\
    \   comparing two example flows; one with 60 B packets, the other with\n   1,500\
    \ B packets.  If both flows pass through a queue with drop\n   probability 0.1%,\
    \ each flow will lose 1 in 1,000 packets.  In the\n   stream of 60 B packets,\
    \ the ratio of lost bytes to sent bytes will be\n   60 B in every 60,000 B; and\
    \ in the stream of 1,500 B packets, the\n   loss ratio will be 1,500 B out of\
    \ 1,500,000 B.  When the transport\n   responds to the ratio of lost to sent packets,\
    \ it will measure the\n   same ratio whether it measures in packets or bytes:\
    \ 0.1% in both\n   cases.  The fact that this ratio is the same whether measured\
    \ in\n   packets or bytes can be seen in Table 5, where the ratio of lost\n  \
    \ packets to sent packets and the ratio of lost bytes to sent bytes is\n   always\
    \ 0.1% in all cases (recall that the scenario was set up with\n   p=0.1%).\n \
    \  This discussion of how the ratio can be measured in packets or bytes\n   is\
    \ only raised here to highlight that it is irrelevant to this memo!\n   Whether\
    \ or not a transport depends on packet size depends on how this\n   ratio is used\
    \ within the congestion control algorithm.\n   So far, we have shown that packet-mode\
    \ drop passes sufficient\n   information to the transport layer so that the transport\
    \ can take bit\n   congestion into account, by using the sizes of the packets\
    \ that\n   indicate congestion.  We have also shown that the transport can\n \
    \  choose not to take packet size into account if it wishes.  We will\n   now\
    \ consider whether the transport can know which to do.\n"
- title: B.2.  Bit-Congestible and Packet-Congestible Indications
  contents:
  - "B.2.  Bit-Congestible and Packet-Congestible Indications\n   As a thought-experiment,\
    \ imagine an idealised congestion notification\n   protocol that supports both\
    \ bit-congestible and packet-congestible\n   resources.  It would require at least\
    \ two ECN flags, one for each of\n   the bit-congestible and packet-congestible\
    \ resources.\n   1.  A packet-congestible resource trying to code congestion level\
    \ p_p\n       into a packet stream should mark the idealised 'packet\n       congestion'\
    \ field in each packet with probability p_p\n       irrespective of the packet's\
    \ size.  The transport should then\n       take a packet with the packet congestion\
    \ field marked to mean\n       just one mark, irrespective of the packet size.\n\
    \   2.  A bit-congestible resource trying to code time-varying byte-\n       congestion\
    \ level p_b into a packet stream should mark the 'byte\n       congestion' field\
    \ in each packet with probability p_b, again\n       irrespective of the packet's\
    \ size.  Unlike before, the transport\n       should take a packet with the byte\
    \ congestion field marked to\n       count as a mark on each byte in the packet.\n\
    \   This hides a fundamental problem -- much more fundamental than\n   whether\
    \ we can magically create header space for yet another ECN\n   flag, or whether\
    \ it would work while being deployed incrementally.\n   Distinguishing drop from\
    \ delivery naturally provides just one\n   implicit bit of congestion indication\
    \ information -- the packet is\n   either dropped or not.  It is hard to drop\
    \ a packet in two ways that\n   are distinguishable remotely.  This is a similar\
    \ problem to that of\n   distinguishing wireless transmission losses from congestive\
    \ losses.\n   This problem would not be solved, even if ECN were universally\n\
    \   deployed.  A congestion notification protocol must survive a\n   transition\
    \ from low levels of congestion to high.  Marking two states\n   is feasible with\
    \ explicit marking, but it is much harder if packets\n   are dropped.  Also, it\
    \ will not always be cost-effective to implement\n   AQM at every low-level resource,\
    \ so drop will often have to suffice.\n   We are not saying two ECN fields will\
    \ be needed (and we are not\n   saying that somehow a resource should be able\
    \ to drop a packet in one\n   of two different ways so that the transport can\
    \ distinguish which\n   sort of drop it was!).  These two congestion notification\
    \ channels\n   are a conceptual device to illustrate a dilemma we could face in\
    \ the\n   future.  Section 3 gives four good reasons why it would be a bad idea\n\
    \   to allow for packet size by biasing drop probability in favour of\n   small\
    \ packets within the network.  The impracticality of our thought\n   experiment\
    \ shows that it will be hard to give transports a practical\n   way to know whether\
    \ or not to take into account the size of\n   congestion indication packets.\n\
    \   Fortunately, this dilemma is not pressing because by design most\n   equipment\
    \ becomes bit-congested before its packet processing becomes\n   congested (as\
    \ already outlined in Section 1.1).  Therefore,\n   transports can be designed\
    \ on the relatively sound assumption that a\n   congestion indication will usually\
    \ imply bit congestion.\n   Nonetheless, although the above idealised protocol\
    \ isn't intended for\n   implementation, we do want to emphasise that research\
    \ is needed to\n   predict whether there are good reasons to believe that packet\n\
    \   congestion might become more common, and if so, to find a way to\n   somehow\
    \ distinguish between bit and packet congestion [RFC3714].\n   Recently, the dual\
    \ resource queue (DRQ) proposal [DRQ] has been made\n   on the premise that, as\
    \ network processors become more cost-\n   effective, per-packet operations will\
    \ become more complex\n   (irrespective of whether more function in the network\
    \ is desirable).\n   Consequently the premise is that CPU congestion will become\
    \ more\n   common.  DRQ is a proposed modification to the RED algorithm that\n\
    \   folds both bit congestion and packet congestion into one signal\n   (either\
    \ loss or ECN).\n   Finally, we note one further complication.  Strictly, packet-\n\
    \   congestible resources are often cycle-congestible.  For instance, for\n  \
    \ routing lookups, load depends on the complexity of each lookup and\n   whether\
    \ or not the pattern of arrivals is amenable to caching.  This\n   also reminds\
    \ us that any solution must not require a forwarding\n   engine to use excessive\
    \ processor cycles in order to decide how to\n   say it has no spare processor\
    \ cycles.\n"
- title: Appendix C.  Byte-Mode Drop Complicates Policing Congestion Response
  contents:
  - "Appendix C.  Byte-Mode Drop Complicates Policing Congestion Response\n   This\
    \ section is informative, not normative.\n   There are two main classes of approach\
    \ to policing congestion\n   response: (i) policing at each bottleneck link or\
    \ (ii) policing at\n   the edges of networks.  Packet-mode drop in RED is compatible\
    \ with\n   either, while byte-mode drop precludes edge policing.\n   The simplicity\
    \ of an edge policer relies on one dropped or marked\n   packet being equivalent\
    \ to another of the same size without having to\n   know which link the drop or\
    \ mark occurred at.  However, the byte-mode\n   drop algorithm has to depend on\
    \ the local MTU of the line -- it needs\n   to use some concept of a 'normal'\
    \ packet size.  Therefore, one\n   dropped or marked packet from a byte-mode drop\
    \ algorithm is not\n   necessarily equivalent to another from a different link.\
    \  A policing\n   function local to the link can know the local MTU where the\n\
    \   congestion occurred.  However, a policer at the edge of the network\n   cannot,\
    \ at least not without a lot of complexity.\n   The early research proposals for\
    \ type (i) policing at a bottleneck\n   link [pBox] used byte-mode drop, then\
    \ detected flows that contributed\n   disproportionately to the number of packets\
    \ dropped.  However, with\n   no extra complexity, later proposals used packet-mode\
    \ drop and looked\n   for flows that contributed a disproportionate amount of\
    \ dropped bytes\n   [CHOKe_Var_Pkt].\n   Work is progressing on the Congestion\
    \ Exposure (ConEx) protocol\n   [RFC6789], which enables a type (ii) edge policer\
    \ located at a user's\n   attachment point.  The idea is to be able to take an\
    \ integrated view\n   of the effect of all a user's traffic on any link in the\n\
    \   internetwork.  However, byte-mode drop would effectively preclude\n   such\
    \ edge policing because of the MTU issue above.\n   Indeed, making drop probability\
    \ depend on the size of the packets\n   that bits happen to be divided into would\
    \ simply encourage the bits\n   to be divided into smaller packets in order to\
    \ confuse policing.  In\n   contrast, as long as a dropped/marked packet is taken\
    \ to mean that\n   all the bytes in the packet are dropped/marked, a policer can\
    \ remain\n   robust against sequences of bits being re-divided into different\
    \ size\n   packets or across different size flows [Rate_fair_Dis].\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Bob Briscoe\n   BT\n   B54/77, Adastral Park\n   Martlesham\
    \ Heath\n   Ipswich  IP5 3RE\n   UK\n   Phone: +44 1473 645196\n   EMail: bob.briscoe@bt.com\n\
    \   URI:   http://bobbriscoe.net/\n   Jukka Manner\n   Aalto University\n   Department\
    \ of Communications and Networking (Comnet)\n   P.O. Box 13000\n   FIN-00076 Aalto\n\
    \   Finland\n   Phone: +358 9 470 22481\n   EMail: jukka.manner@aalto.fi\n   URI:\
    \   http://www.netlab.tkk.fi/~jmanner/\n"
