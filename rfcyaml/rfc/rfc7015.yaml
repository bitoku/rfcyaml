- contents:
  - '  Flow Aggregation for the IP Flow Information Export (IPFIX) Protocol

    '
  title: __initial_text__
- contents:
  - "Abstract\n   This document provides a common implementation-independent basis
    for\n   the interoperable application of the IP Flow Information Export\n   (IPFIX)
    protocol to the handling of Aggregated Flows, which are IPFIX\n   Flows representing
    packets from multiple Original Flows sharing some\n   set of common properties.
    \ It does this through a detailed\n   terminology and a descriptive Intermediate
    Aggregation Process\n   architecture, including a specification of methods for
    Original Flow\n   counting and counter distribution across intervals.\n"
  title: Abstract
- contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This
    document is a product of the Internet Engineering Task Force\n   (IETF).  It represents
    the consensus of the IETF community.  It has\n   received public review and has
    been approved for publication by the\n   Internet Engineering Steering Group (IESG).
    \ Further information on\n   Internet Standards is available in Section 2 of RFC
    5741.\n   Information about the current status of this document, any errata,\n
    \  and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc7015.\n"
  title: Status of This Memo
- contents:
  - "Copyright Notice\n   Copyright (c) 2013 IETF Trust and the persons identified
    as the\n   document authors.  All rights reserved.\n   This document is subject
    to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n
    \  (http://trustee.ietf.org/license-info) in effect on the date of\n   publication
    of this document.  Please review these documents\n   carefully, as they describe
    your rights and restrictions with respect\n   to this document.  Code Components
    extracted from this document must\n   include Simplified BSD License text as described
    in Section 4.e of\n   the Trust Legal Provisions and are provided without warranty
    as\n   described in the Simplified BSD License.\n"
  title: Copyright Notice
- contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n
    \     1.1. IPFIX Protocol Overview ....................................4\n      1.2.
    IPFIX Documents Overview ...................................5\n   2. Terminology
    .....................................................5\n   3. Use Cases for IPFIX
    Aggregation .................................7\n   4. Architecture for Flow Aggregation
    ...............................8\n      4.1. Aggregation within the IPFIX Architecture
    ..................8\n      4.2. Intermediate Aggregation Process Architecture
    .............12\n           4.2.1. Correlation and Normalization ......................14\n
    \  5. IP Flow Aggregation Operations .................................15\n      5.1.
    Temporal Aggregation through Interval Distribution ........15\n           5.1.1.
    Distributing Values across Intervals ...............16\n           5.1.2. Time
    Composition ...................................18\n           5.1.3. External
    Interval Distribution .....................19\n      5.2. Spatial Aggregation
    of Flow Keys ..........................19\n           5.2.1. Counting Original
    Flows ............................21\n           5.2.1. Counting Distinct Key
    Values .......................22\n      5.3. Spatial Aggregation of Non-key Fields
    .....................22\n           5.3.1. Counter Statistics .................................22\n
    \          5.3.2. Derivation of New Values from Flow Keys and\n                  Non-key
    fields .....................................23\n      5.4. Aggregation Combination
    ...................................23\n   6. Additional Considerations and Special
    Cases in Flow\n      Aggregation ....................................................24\n
    \     6.1. Exact versus Approximate Counting during Aggregation ......24\n      6.2.
    Delay and Loss Introduced by the IAP ......................24\n      6.3. Considerations
    for Aggregation of Sampled Flows ...........24\n      6.4. Considerations for
    Aggregation of Heterogeneous Flows .....25\n   7. Export of Aggregated IP Flows
    Using IPFIX ......................25\n      7.1. Time Interval Export ......................................25\n
    \     7.2. Flow Count Export .........................................25\n           7.2.1.
    originalFlowsPresent ...............................26\n           7.2.2. originalFlowsInitiated
    .............................26\n           7.2.3. originalFlowsCompleted .............................26\n
    \          7.2.4. deltaFlowCount .....................................26\n      7.3.
    Distinct Host Export ......................................27\n           7.3.1.
    distinctCountOfSourceIPAddress .....................27\n           7.3.2. distinctCountOfDestinationIPAddress
    ................27\n           7.3.3. distinctCountOfSourceIPv4Address ...................27\n
    \          7.3.4. distinctCountOfDestinationIPv4Address ..............28\n           7.3.5.
    distinctCountOfSourceIPv6Address ...................28\n           7.3.6. distinctCountOfDestinationIPv6Address
    ..............28\n      7.4. Aggregate Counter Distribution Export .....................28\n
    \          7.4.1. Aggregate Counter Distribution Options Template ....29\n           7.4.2.
    valueDistributionMethod Information Element ........29\n   8. Examples .......................................................31\n
    \     8.1. Traffic Time Series per Source ............................32\n      8.2.
    Core Traffic Matrix .......................................37\n      8.3. Distinct
    Source Count per Destination Endpoint ............42\n      8.4. Traffic Time
    Series per Source with Counter Distribution ..44\n   9. Security Considerations
    ........................................46\n   10. IANA Considerations ...........................................46\n
    \  11. Acknowledgments ...............................................46\n   12.
    References ....................................................47\n      12.1.
    Normative References .....................................47\n      12.2. Informative
    References ...................................47\n"
  title: Table of Contents
- contents:
  - "1.  Introduction\n   The assembly of packet data into Flows serves a variety
    of different\n   purposes, as noted in the requirements [RFC3917] and applicability\n
    \  statement [RFC5472] for the IP Flow Information Export (IPFIX)\n   protocol
    [RFC7011].  Aggregation beyond the Flow level, into records\n   representing multiple
    Flows, is a common analysis and data reduction\n   technique as well, with applicability
    to large-scale network data\n   analysis, archiving, and inter-organization exchange.
    \ This\n   applicability in large-scale situations, in particular, led to the\n
    \  inclusion of aggregation as part of the IPFIX Mediation Problem\n   Statement
    [RFC5982], and the definition of an Intermediate\n   Aggregation Process in the
    Mediator framework [RFC6183].\n   Aggregation is used for analysis and data reduction
    in a wide variety\n   of applications, for example, in traffic matrix calculation,\n
    \  generation of time series data for visualizations or anomaly\n   detection,
    or data reduction for long-term trending and storage.\n   Depending on the keys
    used for aggregation, it may additionally have\n   an anonymizing effect on the
    data: for example, aggregation\n   operations that eliminate IP addresses make
    it impossible to later\n   directly identify nodes using those addresses.\n   Aggregation,
    as defined and described in this document, covers the\n   applications defined
    in [RFC5982], including Sections 5.1 \"Adjusting\n   Flow Granularity\", 5.4 \"Time
    Composition\", and 5.5 \"Spatial\n   Composition\".  However, Section 4.2 of this
    document specifies a more\n   flexible architecture for an Intermediate Aggregation
    Process than\n   that envisioned by the original Mediator work [RFC5982].  Instead
    of\n   a focus on these specific limited use cases, the Intermediate\n   Aggregation
    Process is specified to cover any activity commonly\n   described as \"Flow aggregation\".
    \ This architecture is intended to\n   describe any such activity without reference
    to the specific\n   implementation of aggregation.\n   An Intermediate Aggregation
    Process may be applied to data collected\n   from multiple Observation Points,
    as it is natural to use aggregation\n   for data reduction when concentrating
    measurement data.  This\n   document specifically does not address the protocol
    issues that arise\n   when combining IPFIX data from multiple Observation Points
    and\n   exporting from a single Mediator, as these issues are general to\n   IPFIX
    Mediation; they are therefore treated in detail in the\n   Mediation Protocol
    document [IPFIX-MED-PROTO].\n   Since Aggregated Flows as defined in the following
    section are\n   essentially Flows, the IPFIX protocol [RFC7011] can be used to\n
    \  export, and the IPFIX File Format [RFC5655] can be used to store,\n   aggregated
    data \"as is\"; there are no changes necessary to the\n   protocol.  This document
    provides a common basis for the application\n   of IPFIX to the handling of aggregated
    data, through a detailed\n   terminology, Intermediate Aggregation Process architecture,
    and\n   methods for Original Flow counting and counter distribution across\n   intervals.
    \ Note that Sections 5, 6, and 7 of this document are\n   normative.\n"
  - contents:
    - "1.1.  IPFIX Protocol Overview\n   In the IPFIX protocol, { type, length, value
      } tuples are expressed\n   in Templates containing { type, length } pairs, specifying
      which\n   { value } fields are present in data records conforming to the\n   Template,
      giving great flexibility as to what data is transmitted.\n   Since Templates
      are sent very infrequently compared with Data\n   Records, this results in significant
      bandwidth savings.  Various\n   different data formats may be transmitted simply
      by sending new\n   Templates specifying the { type, length } pairs for the new
      data\n   format.  See [RFC7011] for more information.\n   The IPFIX Information
      Element Registry [IANA-IPFIX] defines a large\n   number of standard Information
      Elements that provide the necessary {\n   type } information for Templates.
      \ The use of standard elements\n   enables interoperability among different
      vendors' implementations.\n   Additionally, non-standard enterprise-specific
      elements may be\n   defined for private use.\n"
    title: 1.1.  IPFIX Protocol Overview
  - contents:
    - "1.2.  IPFIX Documents Overview\n   \"Specification of the IP Flow Information
      Export (IPFIX) Protocol for\n   the Exchange of Flow Information\" [RFC7011]
      and its associated\n   documents define the IPFIX protocol, which provides network
      engineers\n   and administrators with access to IP traffic Flow information.\n
      \  IPFIX has a formal description of IPFIX Information Elements, their\n   names,
      types, and additional semantic information, as specified in\n   the IPFIX Information
      Model [RFC7012].  The IPFIX Information Element\n   registry [IANA-IPFIX] is
      maintained by IANA.  New Information Element\n   definitions can be added to
      this registry subject to an Expert Review\n   [RFC5226], with additional process
      considerations described in\n   [RFC7013].\n   \"Architecture for IP Flow Information
      Export\" [RFC5470] defines the\n   architecture for the export of measured IP
      Flow information out of an\n   IPFIX Exporting Process to an IPFIX Collecting
      Process and the basic\n   terminology used to describe the elements of this
      architecture, per\n   the requirements defined in \"Requirements for IP Flow
      Information\n   Export\" [RFC3917].  The IPFIX protocol document [RFC7011] covers
      the\n   details of the method for transporting IPFIX Data Records and\n   Templates
      via a congestion-aware transport protocol from an IPFIX\n   Exporting Process
      to an IPFIX Collecting Process.\n   \"IP Flow Information Export (IPFIX) Mediation:
      Problem Statement\"\n   [RFC5982] introduces the concept of IPFIX Mediators,
      and defines the\n   use cases for which they were designed; \"IP Flow Information
      Export\n   (IPFIX) Mediation: Framework\" [RFC6183] then provides an\n   architectural
      framework for Mediators.  Protocol-level issues (e.g.,\n   Template and Observation
      Domain handling across Mediators) are\n   covered by \"Operation of the IP Flow
      Information Export (IPFIX)\n   Protocol on IPFIX Mediators\" [IPFIX-MED-PROTO].\n
      \  This document specifies an Intermediate Process for Flow aggregation\n   that
      may be applied at an IPFIX Mediator, as well as at an original\n   Observation
      Point prior to export, or for analysis and data reduction\n   purposes after
      receipt at a Collecting Process.\n"
    title: 1.2.  IPFIX Documents Overview
  title: 1.  Introduction
- contents:
  - "2.  Terminology\n   Terms used in this document that are defined in the Terminology\n
    \  section of the IPFIX protocol document [RFC7011] are to be\n   interpreted
    as defined there.\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\",
    \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\"
    in this\n   document are to be interpreted as described in [RFC2119].\n   In addition,
    this document defines the following terms:\n   Aggregated Flow:  A Flow, as defined
    by [RFC7011], derived from a set\n      of zero or more Original Flows within
    a defined Aggregation\n      Interval.  Note that an Aggregated Flow is defined
    in the context\n      of an Intermediate Aggregation Process only.  Once an Aggregated\n
    \     Flow is exported, it is essentially a Flow as in [RFC7011] and can\n      be
    treated as such.\n   Intermediate Aggregation Process:  an Intermediate Aggregation\n
    \     Process (IAP), as in [RFC6183], that aggregates records, based\n      upon
    a set of Flow Keys or functions applied to fields from the\n      record.\n   Aggregation
    Interval:  A time interval imposed upon an Aggregated\n      Flow.  Intermediate
    Aggregation Processes may use a regular\n      Aggregation Interval (e.g., \"every
    five minutes\", \"every calendar\n      month\"), though regularity is not necessary.
    \ Aggregation\n      intervals may also be derived from the time intervals of
    the\n      Original Flows being aggregated.\n   Partially Aggregated Flow:  A
    Flow during processing within an\n      Intermediate Aggregation Process; refers
    to an intermediate data\n      structure during aggregation within the Intermediate
    Aggregation\n      Process architecture detailed in Section 4.2.\n   Original
    Flow:  A Flow given as input to an Intermediate Aggregation\n      Process in
    order to generate Aggregated Flows.\n   Contributing Flow:  An Original Flow that
    is partially or completely\n      represented within an Aggregated Flow.  Each
    Aggregated Flow is\n      made up of zero or more Contributing Flows, and an Original
    Flow\n      may contribute to zero or more Aggregated Flows.\n   Original Exporter:
    \ The Exporter from which the Original Flows are\n      received; meaningful only
    when an IAP is deployed at a Mediator.\n   The terminology presented herein improves
    the precision of, but does\n   not supersede or contradict the terms related to,
    Mediation and\n   aggregation defined in the Mediation Problem Statement [RFC5982]
    and\n   the Mediation Framework [RFC6183] documents.  Within this document,\n
    \  the terminology defined in this section is to be considered\n   normative.\n"
  title: 2.  Terminology
- contents:
  - "3.  Use Cases for IPFIX Aggregation\n   Aggregation, as a common data reduction
    method used in traffic data\n   analysis, has many applications.  When used with
    a regular\n   Aggregation Interval and Original Flows containing timing\n   information,
    it generates time series data from a collection of Flows\n   with discrete intervals,
    as in the example in Section 8.1.  This time\n   series data is itself useful
    for a wide variety of analysis tasks,\n   such as generating input for network
    anomaly detection systems or\n   driving visualizations of volume per time for
    traffic with specific\n   characteristics.  As a second example, traffic matrix
    calculation\n   from Flow data, as shown in Section 8.2 is inherently an aggregation\n
    \  action, by spatially aggregating the Flow Key down to input or output\n   interface,
    address prefix, or autonomous system (AS).\n   Irregular or data-dependent Aggregation
    Intervals and key aggregation\n   operations can also be used to provide adaptive
    aggregation of\n   network Flow data.  Here, full Flow Records can be kept for
    Flows of\n   interest, while Flows deemed \"less interesting\" to a given\n   application
    can be aggregated.  For example, in an IPFIX Mediator\n   equipped with traffic
    classification capabilities for security\n   purposes, potentially malicious Flows
    could be exported directly,\n   while known-good or probably-good Flows (e.g.,
    normal web browsing)\n   could be exported simply as time series volumes per web
    server.\n   Aggregation can also be applied to final analysis of stored Flow\n
    \  data, as shown in the example in Section 8.3.  All such aggregation\n   applications
    in which timing information is not available or not\n   important can be treated
    as if an infinite Aggregation Interval\n   applies.\n   Note that an Intermediate
    Aggregation Process that removes\n   potentially sensitive information as identified
    in [RFC6235] may tend\n   to have an anonymizing effect on the Aggregated Flows
    as well;\n   however, any application of aggregation as part of a data protection\n
    \  scheme should ensure that all the issues raised in [RFC6235] are\n   addressed,
    specifically Sections 4 (\"Anonymization of IP Flow Data\"),\n   7.2 (\"IPFIX-Specific
    Anonymization Guidelines\"), and 9 (\"Security\n   Considerations\").\n   While
    much of the discussion in this document, and all of the\n   examples, apply to
    the common case that the Original Flows to be\n   aggregated are all of the same
    underlying type (i.e., are represented\n   with identical Templates or compatible
    Templates containing a core\n   set Information Elements that can be freely converted
    to one\n   another), and that each packet observed by the Metering Process\n   associated
    with the Original Exporter is represented, this is not a\n   necessary assumption.
    \ Aggregation can also be applied as part of a\n   technique using both aggregation
    and correlation to pull together\n   multiple views of the same traffic from different
    Observation Points\n   using different Templates.  For example, consider a set
    of\n   applications running at different Observation Points for different\n   purposes
    -- one generating Flows with round-trip times for passive\n   performance measurement,
    and one generating billing records.  Once\n   correlated, these Flows could be
    used to produce Aggregated Flows\n   containing both volume and performance information
    together.  The\n   correlation and normalization operation described in Section
    4.2.1\n   handles this specific case of correlation.  Flow correlation in the\n
    \  general case is outside the scope of this document.\n"
  title: 3.  Use Cases for IPFIX Aggregation
- contents:
  - "4.  Architecture for Flow Aggregation\n   This section specifies the architecture
    of the Intermediate\n   Aggregation Process and how it fits into the IPFIX architecture.\n"
  - contents:
    - "4.1.  Aggregation within the IPFIX Architecture\n   An Intermediate Aggregation
      Process could be deployed at any of three\n   places within the IPFIX architecture.
      \ While aggregation is most\n   commonly done within a Mediator that collects
      Original Flows from an\n   Original Exporter and exports Aggregated Flows, aggregation
      can also\n   occur before initial export, or after final collection, as shown
      in\n   Figure 1.  The presence of an IAP at any of these points is, of\n   course,
      optional.\n   +===========================================+\n   |  IPFIX Exporter
      \       +----------------+ |\n   |                        | Metering Proc. |
      |\n   | +-----------------+    +----------------+ |\n   | | Metering Proc.  |
      or |      IAP       | |\n   | +-----------------+----+----------------+ |\n
      \  | |           Exporting Process           | |\n   | +-|----------------------------------|--+
      |\n   +===|==================================|====+\n       |                                  |\n
      \  +===|===========================+      |\n   |   |  Aggregating Mediator
      \    |      |\n   + +-V-------------------+       |      |\n   | | Collecting
      Process  |       |      |\n   + +---------------------+       |      |\n   |
      |         IAP         |       |      |\n   + +---------------------+       |
      \     |\n   | |  Exporting Process  |       |      |\n   + +-|-------------------+
      \      |      |\n   +===|===========================+      |\n       |                                  |\n
      \  +===|==================================|=====+\n   |   | Collector                        |
      \    |\n   | +-V----------------------------------V-+   |\n   | |         Collecting
      Process           |   |\n   | +------------------+-------------------+   |\n
      \  |                    |        IAP        |   |\n   |                    +-------------------+
      \  |\n   |  (Aggregation      |   File Writer     |   |\n       for Storage)
      \    +-----------|-------+   |\n   +================================|===========+\n
      \                                   |\n                             +------V-----------+\n
      \                            |    IPFIX File    |\n                             +------------------+\n
      \                Figure 1: Potential Aggregation Locations\n   The Mediator
      use case is further shown in Figures A and B in\n   [RFC6183].\n   Aggregation
      can be applied for either intermediate or final analytic\n   purposes.  In certain
      circumstances, it may make sense to export\n   Aggregated Flows directly after
      metering, for example, if the\n   Exporting Process is applied to drive a time
      series visualization, or\n   when Flow data export bandwidth is restricted and
      Flow or packet\n   sampling is not an option.  Note that this case, where the\n
      \  Aggregation Process is essentially integrated into the Metering\n   Process,
      is basically covered by the IPFIX architecture [RFC5470]:\n   the Flow Keys
      used are simply a subset of those that would normally\n   be used, and time
      intervals may be chosen other than those available\n   from the cache policies
      customarily offered by the Metering Process.\n   A Metering Process in this
      arrangement MAY choose to simulate the\n   generation of larger Flows in order
      to generate Original Flow counts,\n   if the application calls for compatibility
      with an Intermediate\n   Aggregation Process deployed in a separate location.\n
      \  In the specific case that an Intermediate Aggregation Process is\n   employed
      for data reduction for storage purposes, it can take\n   Original Flows from
      a Collecting Process or File Reader and pass\n   Aggregated Flows to a File
      Writer for storage.\n   Deployment of an Intermediate Aggregation Process within
      a Mediator\n   [RFC5982] is a much more flexible arrangement.  Here, the Mediator\n
      \  consumes Original Flows and produces Aggregated Flows; this\n   arrangement
      is suited to any of the use cases detailed in Section 3.\n   In a Mediator,
      Original Flows from multiple sources can also be\n   aggregated into a single
      stream of Aggregated Flows.  The\n   architectural specifics of this arrangement
      are not addressed in this\n   document, which is concerned only with the aggregation
      operation\n   itself.  See [IPFIX-MED-PROTO] for details.\n   The data paths
      into and out of an Intermediate Aggregation Process\n   are shown in Figure
      2.\n   packets --+               IPFIX Messages      IPFIX Files\n             |
      \                    |                  |\n             V                     V
      \                 V\n   +==================+ +====================+ +=============+\n
      \  | Metering Process | | Collecting Process | | File Reader |\n   |                  |
      +====================+ +=============+\n   | (Original Flows  |            |
      \                 |\n   |    or direct     |            |  Original Flows  |\n
      \  |   aggregation)   |            V                  V\n   + - - - - - - -
      - -+======================================+\n   |           Intermediate Aggregation
      Process (IAP)        |\n   +=========================================================+\n
      \            | Aggregated                  Aggregated |\n             | Flows
      \                           Flows |\n             V                                        V\n
      \  +===================+                       +=============+\n   | Exporting
      Process |                       | File Writer |\n   +===================+                       +=============+\n
      \            |                                        |\n             V                                        V\n
      \      IPFIX Messages                            IPFIX Files\n           Figure
      2: Data Paths through the Aggregation Process\n   Note that as Aggregated Flows
      are IPFIX Flows, an Intermediate\n   Aggregation Process may aggregate already
      Aggregated Flows from an\n   upstream IAP as well as Original Flows from an
      upstream Original\n   Exporter or Metering Process.\n   Aggregation may also
      need to correlate Original Flows from multiple\n   Metering Processes, each
      according to a different Template with\n   different Flow Keys and values.  This
      arrangement is shown in Figure\n   3; in this case, the correlation and normalization
      operation\n   described in Section 4.2.1 handles merging the Original Flows
      before\n   aggregation.\n   packets --+---------------------+------------------+\n
      \            |                     |                  |\n             V                     V
      \                 V\n   +====================+ +====================+ +====================+\n
      \  | Metering Process 1 | | Metering Process 2 | | Metering Process n |\n   +====================+
      +====================+ +====================+\n             |                     |
      \ Original Flows  |\n             V                     V                  V\n
      \  +==================================================================+\n   |
      Intermediate Aggregation Process  +  correlation / normalization |\n   +==================================================================+\n
      \            | Aggregated                  Aggregated |\n             | Flows
      \                           Flows |\n             V                                        V\n
      \  +===================+                       +=============+\n   | Exporting
      Process |                       | File Writer |\n   +===================+                       +=============+\n
      \            |                                        |\n             +------------>
      IPFIX Messages <----------+\n   Figure 3: Aggregating Original Flows from Multiple
      Metering Processes\n"
    title: 4.1.  Aggregation within the IPFIX Architecture
  - contents:
    - "4.2.  Intermediate Aggregation Process Architecture\n   Within this document,
      an Intermediate Aggregation Process can be seen\n   as hosting a function composed
      of four types of operations on\n   Partially Aggregated Flows, as illustrated
      in Figure 4: interval\n   distribution (temporal), key aggregation (spatial),
      value aggregation\n   (spatial), and aggregate combination.  \"Partially Aggregated
      Flows\",\n   as defined in Section 2, are essentially the intermediate results
      of\n   aggregation, internal to the Intermediate Aggregation Process.\n           Original
      Flows  /   Original Flows requiring correlation\n   +=============|===================|===================|=============+\n
      \  |             |   Intermediate    |    Aggregation    |   Process   |\n   |
      \            |                   V                   V             |\n   |             |
      \  +-----------------------------------------------+ |\n   |             |   |
      \  (optional) correlation and normalization    | |\n   |             |   +-----------------------------------------------+
      |\n   |             |                          |                          |\n
      \  |             V                          V                          |\n   |
      \ +--------------------------------------------------------------+ |\n   |  |
      \               interval distribution (temporal)              | |\n   |  +--------------------------------------------------------------+
      |\n   |           | ^                         | ^                |        |\n
      \  |           | |  Partially Aggregated   | |                |        |\n   |
      \          V |         Flows           V |                |        |\n   |  +-------------------+
      \      +--------------------+      |        |\n   |  |  key aggregation  |<------|
      \ value aggregation |      |        |\n   |  |     (spatial)     |------>|      (spatial)
      \    |      |        |\n   |  +-------------------+       +--------------------+
      \     |        |\n   |            |                          |                  |
      \       |\n   |            |   Partially Aggregated   |                  |        |\n
      \  |            V          Flows           V                  V        |\n   |
      \ +--------------------------------------------------------------+ |\n   |  |
      \                    aggregate combination                    | |\n   |  +--------------------------------------------------------------+
      |\n   |                                       |                           |\n
      \  +=======================================|===========================+\n                                           V\n
      \                                  Aggregated Flows\n    Figure 4: Conceptual
      Model of Aggregation Operations within an IAP\n   Interval distribution:  a
      temporal aggregation operation that imposes\n      an Aggregation Interval on
      the Partially Aggregated Flow.  This\n      Aggregation Interval may be regular,
      irregular, or derived from\n      the timing of the Original Flows themselves.
      \ Interval\n      distribution is discussed in detail in Section 5.1.\n   Key
      aggregation:  a spatial aggregation operation that results in the\n      addition,
      modification, or deletion of Flow Key fields in the\n      Partially Aggregated
      Flows.  New Flow Keys may be derived from\n      existing Flow Keys (e.g., looking
      up an AS number (ASN) for an IP\n      address), or \"promoted\" from specific
      non-key fields (e.g., when\n      aggregating Flows by packet count per Flow).
      \ Key aggregation can\n      also add new non-key fields derived from Flow Keys
      that are\n      deleted during key aggregation: mainly counters of unique reduced\n
      \     keys.  Key aggregation is discussed in detail in Section 5.2.\n   Value
      aggregation:  a spatial aggregation operation that results in\n      the addition,
      modification, or deletion of non-key fields in the\n      Partially Aggregated
      Flows.  These non-key fields may be \"demoted\"\n      from existing key fields,
      or derived from existing key or non-key\n      fields.  Value aggregation is
      discussed in detail in Section 5.3.\n   Aggregate combination:  an operation
      combining multiple Partially\n      Aggregated Flows having undergone interval
      distribution, key\n      aggregation, and value aggregation that share Flow
      Keys and\n      Aggregation Intervals into a single Aggregated Flow per set
      of\n      Flow Key values and Aggregation Interval.  Aggregate combination\n
      \     is discussed in detail in Section 5.4.\n   Correlation and normalization:
      \ an optional operation that applies\n      when accepting Original Flows from
      Metering Processes that export\n      different views of essentially the same
      Flows before aggregation.\n      The details of correlation and normalization
      are specified in\n      Section 4.2.1, below.\n   The first three of these operations
      may be carried out any number of\n   times in any order, either on Original
      Flows or on the results of one\n   of the operations above, with one caveat:
      since Flows carry their own\n   interval data, any spatial aggregation operation
      implies a temporal\n   aggregation operation, so at least one interval distribution
      step,\n   even if implicit, is required by this architecture.  This is shown
      as\n   the first step for the sake of simplicity in the diagram above.  Once\n
      \  all aggregation operations are complete, aggregate combination\n   ensures
      that for a given Aggregation Interval, set of Flow Key\n   values, and Observation
      Domain, only one Flow is produced by the\n   Intermediate Aggregation Process.\n
      \  This model describes the operations within a single Intermediate\n   Aggregation
      Process, and it is anticipated that most aggregation will\n   be applied within
      a single process.  However, as the steps in the\n   model may be applied in
      any order and aggregate combination is\n   idempotent, any number of Intermediate
      Aggregation Processes\n   operating in series can be modeled as a single process.
      \ This allows\n   aggregation operations to be flexibly distributed across any
      number\n   of processes, should application or deployment considerations so\n
      \  dictate.\n"
    - contents:
      - "4.2.1.  Correlation and Normalization\n   When accepting Original Flows from
        multiple Metering Processes, each\n   of which provides a different view of
        the Original Flow as seen from\n   the point of view of the IAP, an optional
        correlation and\n   normalization operation combines each of these single
        Flow Records\n   into a set of unified Partially Aggregated Flows before applying\n
        \  interval distribution.  These unified Flows appear as if they had\n   been
        measured at a single Metering Process that used the union of the\n   set of
        Flow Keys and non-key fields of all Metering Processes sending\n   Original
        Flows to the IAP.\n   Since, due to export errors or other slight irregularities
        in Flow\n   metering, the multiple views may not be completely consistent;\n
        \  normalization involves applying a set of corrections that are\n   specific
        to the aggregation application in order to ensure\n   consistency in the unified
        Flows.\n   In general, correlation and normalization should take multiple
        views\n   of essentially the same Flow, as determined by the configuration
        of\n   the operation itself, and render them into a single unified Flow.\n
        \  Flows that are essentially different should not be unified by the\n   correlation
        and normalization operation.  This operation therefore\n   requires enough
        information about the configuration and deployment of\n   Metering Processes
        from which it correlates Original Flows in order\n   to make this distinction
        correctly and consistently.\n   The exact steps performed to correlate and
        normalize Flows in this\n   step are application, implementation, and deployment
        specific, and\n   will not be further specified in this document.\n"
      title: 4.2.1.  Correlation and Normalization
    title: 4.2.  Intermediate Aggregation Process Architecture
  title: 4.  Architecture for Flow Aggregation
- contents:
  - "5.  IP Flow Aggregation Operations\n   As stated in Section 2, an Aggregated
    Flow is simply an IPFIX Flow\n   generated from Original Flows by an Intermediate
    Aggregation Process.\n   Here, we detail the operations by which this is achieved
    within an\n   Intermediate Aggregation Process.\n"
  - contents:
    - "5.1.  Temporal Aggregation through Interval Distribution\n   Interval distribution
      imposes a time interval on the resulting\n   Aggregated Flows.  The selection
      of an interval is specific to the\n   given aggregation application.  Intervals
      may be derived from the\n   Original Flows themselves (e.g., an interval may
      be selected to cover\n   the entire time containing the set of all Flows sharing
      a given Key,\n   as in Time Composition, described in Section 5.1.2) or externally\n
      \  imposed; in the latter case the externally imposed interval may be\n   regular
      (e.g., every five minutes) or irregular (e.g., to allow for\n   different time
      resolutions at different times of day, under different\n   network conditions,
      or indeed for different sets of Original Flows).\n   The length of the imposed
      interval itself has trade-offs.  Shorter\n   intervals allow higher-resolution
      aggregated data and, in streaming\n   applications, faster reaction time.  Longer
      intervals generally lead\n   to greater data reduction and simplified counter
      distribution.\n   Specifically, counter distribution is greatly simplified by
      the\n   choice of an interval longer than the duration of longest Original\n
      \  Flow, itself generally determined by the Original Flow's Metering\n   Process
      active timeout; in this case, an Original Flow can contribute\n   to at most
      two Aggregated Flows, and the more complex value\n   distribution methods become
      inapplicable.\n   |                |                |                |\n   |
      |<--Flow A-->| |                |                |\n   |        |<--Flow B-->|
      \          |                |\n   |          |<-------------Flow C-------------->|
      \  |\n   |                |                |                |\n   |   interval
      0   |   interval 1   |   interval 2   |\n              Figure 5: Illustration
      of Interval Distribution\n   In Figure 5, we illustrate three common possibilities
      for interval\n   distribution as applies with regular intervals to a set of
      three\n   Original Flows.  For Flow A, the start and end times lie within the\n
      \  boundaries of a single interval 0; therefore, Flow A contributes to\n   only
      one Aggregated Flow.  Flow B, by contrast, has the same duration\n   but crosses
      the boundary between intervals 0 and 1; therefore, it\n   will contribute to
      two Aggregated Flows, and its counters must be\n   distributed among these Flows;
      though, in the two-interval case, this\n   can be simplified somewhat simply
      by picking one of the two intervals\n   or proportionally distributing between
      them.  Only Flows like Flow A\n   and Flow B will be produced when the interval
      is chosen to be longer\n   than the duration of longest Original Flow, as above.
      \ More\n   complicated is the case of Flow C, which contributes to more than
      two\n   Aggregated Flows and must have its counters distributed according to\n
      \  some policy as in Section 5.1.1.\n"
    - contents:
      - "5.1.1.  Distributing Values across Intervals\n   In general, counters in
        Aggregated Flows are treated the same as in\n   any Flow.  Each counter is
        independently calculated as if it were\n   derived from the set of packets
        in the Original Flow.  For example,\n   delta counters are summed, the most
        recent total count for each\n   Original Flow taken then summed across Flows,
        and so on.\n   When the Aggregation Interval is guaranteed to be longer than
        the\n   longest Original Flow, a Flow can cross at most one Interval\n   boundary,
        and will therefore contribute to at most two Aggregated\n   Flows.  Most common
        in this case is to arbitrarily but consistently\n   choose to account the
        Original Flow's counters either to the first or\n   to the last Aggregated
        Flow to which it could contribute.\n   However, this becomes more complicated
        when the Aggregation Interval\n   is shorter than the longest Original Flow
        in the source data.  In\n   such cases, each Original Flow can incompletely
        cover one or more\n   time intervals, and apply to one or more Aggregated
        Flows.  In this\n   case, the Intermediate Aggregation Process must distribute
        the\n   counters in the Original Flows across one or more resulting\n   Aggregated
        Flows.  There are several methods for doing this, listed\n   here in roughly
        increasing order of complexity and accuracy; most of\n   these are necessary
        only in specialized cases.\n   End Interval:  The counters for an Original
        Flow are added to the\n      counters of the appropriate Aggregated Flow containing
        the end\n      time of the Original Flow.\n   Start Interval:  The counters
        for an Original Flow are added to the\n      counters of the appropriate Aggregated
        Flow containing the start\n      time of the Original Flow.\n   Mid Interval:
        \ The counters for an Original Flow are added to the\n      counters of a
        single appropriate Aggregated Flow containing some\n      timestamp between
        start and end time of the Original Flow.\n   Simple Uniform Distribution:
        \ Each counter for an Original Flow is\n      divided by the number of time
        intervals the Original Flow covers\n      (i.e., of appropriate Aggregated
        Flows sharing the same Flow\n      Keys), and this number is added to each
        corresponding counter in\n      each Aggregated Flow.\n   Proportional Uniform
        Distribution:  This is like simple uniform\n      distribution, but accounts
        for the fractional portions of a time\n      interval covered by an Original
        Flow in the first and last time\n      interval.  Each counter for an Original
        Flow is divided by the\n      number of time _units_ the Original Flow covers,
        to derive a mean\n      count rate.  This rate is then multiplied by the number
        of time\n      units in the intersection of the duration of the Original Flow
        and\n      the time interval of each Aggregated Flow.\n   Simulated Process:
        \ Each counter of the Original Flow is distributed\n      among the intervals
        of the Aggregated Flows according to some\n      function the Intermediate
        Aggregation Process uses based upon\n      properties of Flows presumed to
        be like the Original Flow.  For\n      example, Flow Records representing
        bulk transfer might follow a\n      more or less proportional uniform distribution,
        while interactive\n      processes are far more bursty.\n   Direct:  The Intermediate
        Aggregation Process has access to the\n      original packet timings from
        the packets making up the Original\n      Flow, and uses these to distribute
        or recalculate the counters.\n   A method for exporting the distribution of
        counters across multiple\n   Aggregated Flows is detailed in Section 7.4.
        \ In any case, counters\n   MUST be distributed across the multiple Aggregated
        Flows in such a\n   way that the total count is preserved, within the limits
        of accuracy\n   of the implementation.  This property allows data to be aggregated\n
        \  and re-aggregated with negligible loss of original count information.\n
        \  To avoid confusion in interpretation of the aggregated data, all the\n
        \  counters in a given Aggregated Flow MUST be distributed via the same\n
        \  method.\n   More complex counter distribution methods generally require
        that the\n   interval distribution process track multiple \"current\" time
        intervals\n   at once.  This may introduce some delay into the aggregation\n
        \  operation, as an interval should only expire and be available for\n   export
        when no additional Original Flows applying to the interval are\n   expected
        to arrive at the Intermediate Aggregation Process.\n   Note, however, that
        since there is no guarantee that Flows from the\n   Original Exporter will
        arrive in any given order, whether for\n   transport-specific reasons (i.e.,
        UDP reordering) or reasons specific\n   to the implementation of the Metering
        Process or Exporting Process,\n   even simpler distribution methods may need
        to deal with Flows\n   arriving in an order other than start time or end time.
        \ Therefore,\n   the use of larger intervals does not obviate the need to
        buffer\n   Partially Aggregated Flows within \"current\" time intervals, to
        ensure\n   the IAP can accept Flow time intervals in any arrival order.  More\n
        \  generally, the interval distribution process SHOULD accept Flow start\n
        \  and end times in the Original Flows in any reasonable order.  The\n   expiration
        of intervals in interval distribution operations is\n   dependent on implementation
        and deployment requirements, and it MUST\n   be made configurable in contexts
        in which \"reasonable order\" is not\n   obvious at implementation time.  This
        operation may lead to delay and\n   loss introduced by the IAP, as detailed
        in Section 6.2.\n"
      title: 5.1.1.  Distributing Values across Intervals
    - contents:
      - "5.1.2.  Time Composition\n   Time Composition, as in Section 5.4 of [RFC5982]
        (or interval\n   combination), is a special case of aggregation, where interval\n
        \  distribution imposes longer intervals on Flows with matching keys and\n
        \  \"chained\" start and end times, without any key reduction, in order to\n
        \  join long-lived Flows that may have been split (e.g., due to an\n   active
        timeout shorter than the actual duration of the Flow).  Here,\n   no Key aggregation
        is applied, and the Aggregation Interval is chosen\n   on a per-Flow basis
        to cover the interval spanned by the set of\n   Aggregated Flows.  This may
        be applied alone in order to normalize\n   split Flows, or it may be applied
        in combination with other\n   aggregation functions in order to obtain more
        accurate Original Flow\n   counts.\n"
      title: 5.1.2.  Time Composition
    - contents:
      - "5.1.3.  External Interval Distribution\n   Note that much of the difficulty
        of interval distribution at an IAP\n   can be avoided simply by configuring
        the original Exporters to\n   synchronize the time intervals in the Original
        Flows with the desired\n   aggregation interval.  The resulting Original Flows
        would then be\n   split to align perfectly with the time intervals imposed
        during\n   interval imposition, as shown in Figure 6, though this may reduce\n
        \  their usefulness for non-aggregation purposes.  This approach allows\n
        \  the Intermediate Aggregation Process to use Start Interval or End\n   Interval
        distribution, while having equivalent information to that\n   available to
        direct interval distribution.\n   |                |                |                |\n
        \  |<----Flow D---->|<----Flow E---->|<----Flow F---->|\n   |                |
        \               |                |\n   |   interval 0   |   interval 1   |
        \  interval 2   |\n         Figure 6: Illustration of External Interval Distribution\n"
      title: 5.1.3.  External Interval Distribution
    title: 5.1.  Temporal Aggregation through Interval Distribution
  - contents:
    - "5.2.  Spatial Aggregation of Flow Keys\n   Key aggregation generates a new
      set of Flow Key values for the\n   Aggregated Flows from the Original Flow Key
      and non-key fields in the\n   Original Flows or from correlation of the Original
      Flow information\n   with some external source.  There are two basic operations
      here.\n   First, Aggregated Flow Keys may be derived directly from Original\n
      \  Flow Keys through reduction, or they may be derived by the dropping\n   of
      fields or precision in the Original Flow Keys.  Second, Aggregated\n   Flow
      Keys may be derived through replacement, e.g., by removing one\n   or more fields
      from the Original Flow and replacing them with fields\n   derived from the removed
      fields.  Replacement may refer to external\n   information (e.g., IP to AS number
      mappings).  Replacement may apply\n   to Flow Keys as well as non-key fields.
      \ For example, consider an\n   application that aggregates Original Flows by
      packet count (i.e.,\n   generating an Aggregated Flow for all one-packet Flows,
      one for all\n   two-packet Flows, and so on).  This application would promote
      the\n   packet count to a Flow Key.\n   Key aggregation may also result in the
      addition of new non-key fields\n   to the Aggregated Flows, namely, Original
      Flow counters and unique\n   reduced key counters.  These are treated in more
      detail in Sections\n   5.2.1 and 5.2.2, respectively.\n   In any key aggregation
      operation, reduction and/or replacement may be\n   applied any number of times
      in any order.  Which of these operations\n   are supported by a given implementation
      is implementation and\n   application dependent.\n   Original Flow Keys\n   +---------+---------+----------+----------+-------+-----+\n
      \  | src ip4 | dst ip4 | src port | dst port | proto | tos |\n   +---------+---------+----------+----------+-------+-----+\n
      \       |         |         |          |         |      |\n     retain   mask
      /24      X          X         X      X\n        |         |\n        V         V\n
      \  +---------+-------------+\n   | src ip4 | dst ip4 /24 |\n   +---------+-------------+\n
      \  Aggregated Flow Keys (by source address and destination /24 network)\n          Figure
      7: Illustration of Key Aggregation by Reduction\n   Figure 7 illustrates an
      example reduction operation, aggregation by\n   source address and destination
      /24 network.  Here, the port,\n   protocol, and type-of-service information
      is removed from the Flow\n   Key, the source address is retained, and the destination
      address is\n   masked by dropping the lower 8 bits.\n   Original Flow Keys\n
      \  +---------+---------+----------+----------+-------+-----+\n   | src ip4 |
      dst ip4 | src port | dst port | proto | tos |\n   +---------+---------+----------+----------+-------+-----+\n
      \       |         |         |          |         |      |\n        V         V
      \        |          |         |      |\n   +-------------------+    X          X
      \        X      X\n   | ASN lookup table  |\n   +-------------------+\n        |
      \        |\n        V         V\n   +---------+---------+\n   | src asn | dst
      asn |\n   +---------+---------+\n   Aggregated Flow Keys (by source and destination
      ASN)\n                 Figure 8: Illustration of Key Aggregation\n                       by
      Reduction and Replacement\n   Figure 8 illustrates an example reduction and
      replacement operation,\n   aggregation by source and destination Border Gateway
      Protocol (BGP)\n   Autonomous System Number (ASN) without ASN information available
      in\n   the Original Flow.  Here, the port, protocol, and type-of-service\n   information
      is removed from the Flow Keys, while the source and\n   destination addresses
      are run though an IP address to ASN lookup\n   table, and the Aggregated Flow
      Keys are made up of the resulting\n   source and destination ASNs.\n"
    - contents:
      - "5.2.1.  Counting Original Flows\n   When aggregating multiple Original Flows
        into an Aggregated Flow, it\n   is often useful to know how many Original
        Flows are present in the\n   Aggregated Flow. Section 7.2 introduces four
        new Information Elements\n   to export these counters.\n   There are two possible
        ways to count Original Flows, which we call\n   conservative and non-conservative.
        \ Conservative Flow counting has\n   the property that each Original Flow
        contributes exactly one to the\n   total Flow count within a set of Aggregated
        Flows.  In other words,\n   conservative Flow counters are distributed just
        as any other counter\n   during interval distribution, except each Original
        Flow is assumed to\n   have a Flow count of one.  When a count for an Original
        Flow must be\n   distributed across a set of Aggregated Flows, and a distribution\n
        \  method is used that does not account for that Original Flow\n   completely
        within a single Aggregated Flow, conservative Flow\n   counting requires a
        fractional representation.\n   By contrast, non-conservative Flow counting
        is used to count how many\n   Contributing Flows are represented in an Aggregated
        Flow.  Flow\n   counters are not distributed in this case.  An Original Flow
        that is\n   present within N Aggregated Flows would add N to the sum of non-\n
        \  conservative Flow counts, one to each Aggregated Flow.  In other\n   words,
        the sum of conservative Flow counts over a set of Aggregated\n   Flows is
        always equal to the number of Original Flows, while the sum\n   of non-conservative
        Flow counts is strictly greater than or equal to\n   the number of Original
        Flows.\n   For example, consider Flows A, B, and C as illustrated in Figure
        5.\n   Assume that the key aggregation step aggregates the keys of these\n
        \  three Flows to the same aggregated Flow Key, and that start interval\n
        \  counter distribution is in effect.  The conservative Flow count for\n   interval
        0 is 3 (since Flows A, B, and C all begin in this interval),\n   and for the
        other two intervals is 0.  The non-conservative Flow\n   count for interval
        0 is also 3 (due to the presence of Flows A, B,\n   and C), for interval 1
        is 2 (Flows B and C), and for interval 2 is 1\n   (Flow C).  The sum of the
        conservative counts 3 + 0 + 0 = 3, the\n   number of Original Flows; while
        the sum of the non-conservative\n   counts 3 + 2 + 1 = 6.\n   Note that the
        active and inactive timeouts used to generate Original\n   Flows, as well
        as the cache policy used to generate those Flows, have\n   an effect on how
        meaningful either the conservative or non-\n   conservative Flow count will
        be during aggregation.  In general,\n   Original Exporters using the IPFIX
        Configuration Model SHOULD be\n   configured to export Flows with equal or
        similar activeTimeout and\n   inactiveTimeout configuration values, and the
        same cacheMode, as\n   defined in [RFC6728].  Original Exporters not using
        the IPFIX\n   Configuration Model SHOULD be configured equivalently.\n"
      title: 5.2.1.  Counting Original Flows
    - contents:
      - "5.2.2.  Counting Distinct Key Values\n   One common case in aggregation is
        counting distinct key values that\n   were reduced away during key aggregation.
        \ The most common use case\n   for this is counting distinct hosts per Flow
        Key; for example, in\n   host characterization or anomaly detection, distinct
        sources per\n   destination or distinct destinations per source are common
        metrics.\n   These new non-key fields are added during key aggregation.\n
        \  For such applications, Information Elements for distinct counts of\n   IPv4
        and IPv6 addresses are defined in Section 7.3.  These are named\n   distinctCountOf(KeyName).
        \ Additional such Information Elements\n   should be registered with IANA
        on an as-needed basis.\n"
      title: 5.2.2.  Counting Distinct Key Values
    title: 5.2.  Spatial Aggregation of Flow Keys
  - contents:
    - "5.3.  Spatial Aggregation of Non-key Fields\n   Aggregation operations may
      also lead to the addition of value fields\n   that are demoted from key fields
      or are derived from other value\n   fields in the Original Flows.  Specific
      cases of this are treated in\n   the subsections below.\n"
    - contents:
      - "5.3.1.  Counter Statistics\n   Some applications of aggregation may benefit
        from computing different\n   statistics than those native to each non-key
        field (e.g., flags are\n   natively combined via union and delta counters
        by summing).  For\n   example, minimum and maximum packet counts per Flow,
        mean bytes per\n   packet per Contributing Flow, and so on.  Certain Information\n
        \  Elements for these applications are already provided in the IANA\n   IPFIX
        Information Elements registry [IANA-IPFIX] (e.g.,\n   minimumIpTotalLength).\n
        \  A complete specification of additional aggregate counter statistics\n   is
        outside the scope of this document, and should be added in the\n   future
        to the IANA IPFIX Information Elements registry on a per-\n   application,
        as-needed basis.\n"
      title: 5.3.1.  Counter Statistics
    - contents:
      - "5.3.2.  Derivation of New Values from Flow Keys and Non-key fields\n   More
        complex operations may lead to other derived fields being\n   generated from
        the set of values or Flow Keys reduced away during\n   aggregation.  A prime
        example of this is sample entropy calculation.\n   This counts distinct values
        and frequency, so it is similar to\n   distinct key counting as in Section
        5.2.2; however, it may be applied\n   to the distribution of values for any
        Flow field.\n   Sample entropy calculation provides a one-number normalized\n
        \  representation of the value spread and is useful for anomaly\n   detection.
        \ The behavior of entropy statistics is such that a small\n   number of keys
        showing up very often drives the entropy value down\n   towards zero, while
        a large number of keys, each showing up with\n   lower frequency, drives the
        entropy value up.\n   Entropy statistics are generally useful for identifier
        keys, such as\n   IP addresses, port numbers, AS numbers, etc.  They can also
        be\n   calculated on Flow length, Flow duration fields, and the like, even\n
        \  if this generally yields less distinct value shifts when the traffic\n
        \  mix changes.\n   As a practical example, one host scanning a lot of other
        hosts will\n   drive source IP entropy down and target IP entropy up.  A similar\n
        \  effect can be observed for ports.  This pattern can also be caused by\n
        \  the scan-traffic of a fast Internet worm.  A second example would be\n
        \  a Distributed Denial of Service (DDoS) flooding attack against a\n   single
        target (or small number of targets) that drives source IP\n   entropy up and
        target IP entropy down.\n   A complete specification of additional derived
        values or entropy\n   Information Elements is outside the scope of this document.
        \ Any such\n   Information Elements should be added in the future to the IANA
        IPFIX\n   Information Elements registry on a per-application, as-needed basis.\n"
      title: 5.3.2.  Derivation of New Values from Flow Keys and Non-key fields
    title: 5.3.  Spatial Aggregation of Non-key Fields
  - contents:
    - "5.4.  Aggregation Combination\n   Interval distribution and key aggregation
      together may generate\n   multiple Partially Aggregated Flows covering the same
      time interval\n   with the same set of Flow Key values.  The process of combining
      these\n   Partially Aggregated Flows into a single Aggregated Flow is called\n
      \  aggregation combination.  In general, non-Key values from multiple\n   Contributing
      Flows are combined using the same operation by which\n   values are combined
      from packets to form Flows for each Information\n   Element.  Delta counters
      are summed, flags are unioned, and so on.\n"
    title: 5.4.  Aggregation Combination
  title: 5.  IP Flow Aggregation Operations
- contents:
  - '6.  Additional Considerations and Special Cases in Flow Aggregation

    '
  - contents:
    - "6.1.  Exact versus Approximate Counting during Aggregation\n   In certain circumstances,
      particularly involving aggregation by\n   devices with limited resources, and
      in situations where exact\n   aggregated counts are less important than relative
      magnitudes (e.g.,\n   driving graphical displays), counter distribution during
      key\n   aggregation may be performed by approximate counting means (e.g.,\n
      \  Bloom filters).  The choice to use approximate counting is\n   implementation
      and application dependent.\n"
    title: 6.1.  Exact versus Approximate Counting during Aggregation
  - contents:
    - "6.2.  Delay and Loss Introduced by the IAP\n   When accepting Original Flows
      in export order from traffic captured\n   live, the Intermediate Aggregation
      Process waits for all Original\n   Flows that may contribute to a given interval
      during interval\n   distribution.  This is generally dominated by the active
      timeout of\n   the Metering Process measuring the Original Flows.  For example,
      with\n   Metering Processes configured with a five-minute active timeout, the\n
      \  Intermediate Aggregation Process introduces a delay of at least five\n   minutes
      to all exported Aggregated Flows to ensure it has received\n   all Original
      Flows.  Note that when aggregating Flows from multiple\n   Metering Processes
      with different active timeouts, the delay is\n   determined by the maximum active
      timeout.\n   In certain circumstances, additional delay at the original Exporter\n
      \  may cause an IAP to close an interval before the last Original\n   Flow(s)
      accountable to the interval arrives.  In this case, the IAP\n   MAY drop the
      late Original Flow(s).  Accounting of Flows lost at an\n   Intermediate Process
      due to such issues is covered in\n   [IPFIX-MED-PROTO].\n"
    title: 6.2.  Delay and Loss Introduced by the IAP
  - contents:
    - "6.3.  Considerations for Aggregation of Sampled Flows\n   The accuracy of Aggregated
      Flows may also be affected by sampling of\n   the Original Flows, or sampling
      of packets making up the Original\n   Flows.  At the time of writing, the effect
      of sampling on Flow\n   aggregation is still an open research question.  However,
      to maximize\n   the comparability of Aggregated Flows, aggregation of sampled
      Flows\n   should only be applied to Original Flows sampled using the same\n
      \  sampling rate and sampling algorithm, Flows created from packets\n   sampled
      using the same sampling rate and sampling algorithm, or\n   Original Flows that
      have been normalized as if they had the same\n   sampling rate and algorithm
      before aggregation.  For more on packet\n   sampling within IPFIX, see [RFC5476].
      \ For more on Flow sampling\n   within the IPFIX Mediator framework, see [RFC7014].\n"
    title: 6.3.  Considerations for Aggregation of Sampled Flows
  - contents:
    - "6.4.  Considerations for Aggregation of Heterogeneous Flows\n   Aggregation
      may be applied to Original Flows from different sources\n   and of different
      types (i.e., represented using different, perhaps\n   wildly different Templates).
      \ When the goal is to separate the\n   heterogeneous Original Flows and aggregate
      them into heterogeneous\n   Aggregated Flows, each aggregation should be done
      at its own\n   Intermediate Aggregation Process.  The Observation Domain ID
      on the\n   Messages containing the output Aggregated Flows can be used to\n
      \  identify the different Processes and to segregate the output.\n   However,
      when the goal is to aggregate these Flows into a single\n   stream of Aggregated
      Flows representing one type of data, and if the\n   Original Flows may represent
      the same original packet at two\n   different Observation Points, the Original
      Flows should be correlated\n   by the correlation and normalization operation
      within the IAP to\n   ensure that each packet is only represented in a single
      Aggregated\n   Flow or set of Aggregated Flows differing only by aggregation\n
      \  interval.\n"
    title: 6.4.  Considerations for Aggregation of Heterogeneous Flows
  title: 6.  Additional Considerations and Special Cases in Flow Aggregation
- contents:
  - "7.  Export of Aggregated IP Flows Using IPFIX\n   In general, Aggregated Flows
    are exported in IPFIX as any other Flow.\n   However, certain aspects of Aggregated
    Flow export benefit from\n   additional guidelines or new Information Elements
    to represent\n   aggregation metadata or information generated during aggregation.\n
    \  These are detailed in the following subsections.\n"
  - contents:
    - "7.1.  Time Interval Export\n   Since an Aggregated Flow is simply a Flow, the
      existing timestamp\n   Information Elements in the IPFIX Information Model (e.g.,\n
      \  flowStartMilliseconds, flowEndNanoseconds) are sufficient to specify\n   the
      time interval for aggregation.  Therefore, no new aggregation-\n   specific
      Information Elements for exporting time interval information\n   are necessary.\n
      \  Each Aggregated Flow carrying timing information SHOULD contain both\n   an
      interval start and interval end timestamp.\n"
    title: 7.1.  Time Interval Export
  - contents:
    - "7.2.  Flow Count Export\n   The following four Information Elements are defined
      to count Original\n   Flows as discussed in Section 5.2.1.\n"
    - contents:
      - "7.2.1.  originalFlowsPresent\n   Description:  The non-conservative count
        of Original Flows\n      contributing to this Aggregated Flow.  Non-conservative
        counts\n      need not sum to the original count on re-aggregation.\n   Abstract
        Data Type:  unsigned64\n   Data Type Semantics:  deltaCounter\n   ElementID:
        \ 375\n"
      title: 7.2.1.  originalFlowsPresent
    - contents:
      - "7.2.2.  originalFlowsInitiated\n   Description:  The conservative count of
        Original Flows whose first\n      packet is represented within this Aggregated
        Flow.  Conservative\n      counts must sum to the original count on re-aggregation.\n
        \  Abstract Data Type:  unsigned64\n   Data Type Semantics:  deltaCounter\n
        \  ElementID:  376\n"
      title: 7.2.2.  originalFlowsInitiated
    - contents:
      - "7.2.3.  originalFlowsCompleted\n   Description:  The conservative count of
        Original Flows whose last\n      packet is represented within this Aggregated
        Flow.  Conservative\n      counts must sum to the original count on re-aggregation.\n
        \  Abstract Data Type:  unsigned64\n   Data Type Semantics:  deltaCounter\n
        \  ElementID:  377\n"
      title: 7.2.3.  originalFlowsCompleted
    - contents:
      - "7.2.4.  deltaFlowCount\n   Description:  The conservative count of Original
        Flows contributing\n      to this Aggregated Flow; may be distributed via
        any of the methods\n      expressed by the valueDistributionMethod Information
        Element.\n   Abstract Data Type:  unsigned64\n   Data Type Semantics:  deltaCounter\n
        \  ElementID:  3\n"
      title: 7.2.4.  deltaFlowCount
    title: 7.2.  Flow Count Export
  - contents:
    - "7.3.  Distinct Host Export\n   The following six Information Elements represent
      the distinct counts\n   of source and destination network-layer addresses used
      to export\n   distinct host counts reduced away during key aggregation.\n"
    - contents:
      - "7.3.1.  distinctCountOfSourceIPAddress\n   Description:  The count of distinct
        source IP address values for\n      Original Flows contributing to this Aggregated
        Flow, without\n      regard to IP version.  This Information Element is preferred
        to\n      the IP-version-specific counters, unless it is important to\n      separate
        the counts by version.\n   Abstract Data Type:  unsigned64\n   Data Type Semantics:
        \ totalCounter\n   ElementID:  378\n"
      title: 7.3.1.  distinctCountOfSourceIPAddress
    - contents:
      - "7.3.2.  distinctCountOfDestinationIPAddress\n   Description:  The count of
        distinct destination IP address values for\n      Original Flows contributing
        to this Aggregated Flow, without\n      regard to IP version.  This Information
        Element is preferred to\n      the version-specific counters below, unless
        it is important to\n      separate the counts by version.\n   Abstract Data
        Type:  unsigned64\n   Data Type Semantics:  totalCounter\n   ElementID:  379\n"
      title: 7.3.2.  distinctCountOfDestinationIPAddress
    - contents:
      - "7.3.3.  distinctCountOfSourceIPv4Address\n   Description:  The count of distinct
        source IPv4 address values for\n      Original Flows contributing to this
        Aggregated Flow.\n   Abstract Data Type:  unsigned32\n   Data Type Semantics:
        \ totalCounter\n   ElementID:  380\n"
      title: 7.3.3.  distinctCountOfSourceIPv4Address
    - contents:
      - "7.3.4.  distinctCountOfDestinationIPv4Address\n   Description:  The count
        of distinct destination IPv4 address values\n      for Original Flows contributing
        to this Aggregated Flow.\n   Abstract Data Type:  unsigned32\n   Data Type
        Semantics:  totalCounter\n   ElementID:  381\n"
      title: 7.3.4.  distinctCountOfDestinationIPv4Address
    - contents:
      - "7.3.5.  distinctCountOfSourceIPv6Address\n   Description:  The count of distinct
        source IPv6 address values for\n      Original Flows contributing to this
        Aggregated Flow.\n   Abstract Data Type:  unsigned64\n   Data Type Semantics:
        \ totalCounter\n   ElementID:  382\n"
      title: 7.3.5.  distinctCountOfSourceIPv6Address
    - contents:
      - "7.3.6.  distinctCountOfDestinationIPv6Address\n   Description:  The count
        of distinct destination IPv6 address values\n      for Original Flows contributing
        to this Aggregated Flow.\n   Abstract Data Type:  unsigned64\n   Data Type
        Semantics:  totalCounter\n   ElementID:  383\n"
      title: 7.3.6.  distinctCountOfDestinationIPv6Address
    title: 7.3.  Distinct Host Export
  - contents:
    - "7.4.  Aggregate Counter Distribution Export\n   When exporting counters distributed
      among Aggregated Flows, as\n   described in Section 5.1.1, the Exporting Process
      MAY export an\n   Aggregate Counter Distribution Option Record for each Template\n
      \  describing Aggregated Flow records; this Options Template is\n   described
      below.  It uses the valueDistributionMethod Information\n   Element, also defined
      below.  Since, in many cases, distribution is\n   simple, accounting the counters
      from Contributing Flows to the first\n   Interval to which they contribute,
      this is the default situation, for\n   which no Aggregate Counter Distribution
      Record is necessary;\n   Aggregate Counter Distribution Records are only applicable
      in more\n   exotic situations, such as using an Aggregation Interval smaller
      than\n   the durations of Original Flows.\n"
    - contents:
      - "7.4.1.  Aggregate Counter Distribution Options Template\n   This Options
        Template defines the Aggregate Counter Distribution\n   Record, which allows
        the binding of a value distribution method to a\n   Template ID.  The scope
        is the Template ID, whose uniqueness, per\n   [RFC7011], is local to the Transport
        Session and Observation Domain\n   that generated the Template ID.  This is
        used to signal to the\n   Collecting Process how the counters were distributed.
        \ The fields are\n   as below:\n   +-----------------------------+-------------------------------------+\n
        \  | IE                          | Description                         |\n
        \  +-----------------------------+-------------------------------------+\n
        \  | templateId [scope]          | The Template ID of the Template     |\n
        \  |                             | defining the Aggregated Flows to    |\n
        \  |                             | which this distribution option      |\n
        \  |                             | applies.  This Information Element |\n
        \  |                             | MUST be defined as a Scope field.   |\n
        \  | valueDistributionMethod     | The method used to distribute the   |\n
        \  |                             | counters for the Aggregated Flows   |\n
        \  |                             | defined by the associated Template. |\n
        \  +-----------------------------+-------------------------------------+\n"
      title: 7.4.1.  Aggregate Counter Distribution Options Template
    - contents:
      - "7.4.2.  valueDistributionMethod Information Element\n   Description:  A description
        of the method used to distribute the\n      counters from Contributing Flows
        into the Aggregated Flow records\n      described by an associated scope,
        generally a Template.  The\n      method is deemed to apply to all the non-Key
        Information Elements\n      in the referenced scope for which value distribution
        is a valid\n      operation; if the originalFlowsInitiated and/or\n      originalFlowsCompleted
        Information Elements appear in the\n      Template, they are not subject to
        this distribution method, as\n      they each infer their own distribution
        method.  This is intended\n      to be a complete set of possible value distribution
        methods; it is\n      encoded as follows:\n   +-------+-----------------------------------------------------------+\n
        \  | Value | Description                                               |\n
        \  +-------+-----------------------------------------------------------+\n
        \  | 0     | Unspecified: The counters for an Original Flow are        |\n
        \  |       | explicitly not distributed according to any other method  |\n
        \  |       | defined for this Information Element; use for arbitrary   |\n
        \  |       | distribution, or distribution algorithms not described by |\n
        \  |       | any other codepoint.                                      |\n
        \  |       | --------------------------------------------------------- |\n
        \  |       |                                                           |\n
        \  | 1     | Start Interval: The counters for an Original Flow are     |\n
        \  |       | added to the counters of the appropriate Aggregated Flow  |\n
        \  |       | containing the start time of the Original Flow.  This     |\n
        \  |       | should be assumed the default if value distribution       |\n
        \  |       | information is not available at a Collecting Process for  |\n
        \  |       | an Aggregated Flow.                                       |\n
        \  |       | --------------------------------------------------------- |\n
        \  |       |                                                           |\n
        \  | 2     | End Interval: The counters for an Original Flow are added |\n
        \  |       | to the counters of the appropriate Aggregated Flow        |\n
        \  |       | containing the end time of the Original Flow.             |\n
        \  |       | --------------------------------------------------------- |\n
        \  |       |                                                           |\n
        \  | 3     | Mid Interval: The counters for an Original Flow are added |\n
        \  |       | to the counters of a single appropriate Aggregated Flow   |\n
        \  |       | containing some timestamp between start and end time of   |\n
        \  |       | the Original Flow.                                        |\n
        \  |       | --------------------------------------------------------- |\n
        \  |       |                                                           |\n
        \  | 4     | Simple Uniform Distribution: Each counter for an Original |\n
        \  |       | Flow is divided by the number of time intervals the       |\n
        \  |       | Original Flow covers (i.e., of appropriate Aggregated     |\n
        \  |       | Flows sharing the same Flow Key), and this number is      |\n
        \  |       | added to each corresponding counter in each Aggregated    |\n
        \  |       | Flow.                                                     |\n
        \  |       | --------------------------------------------------------- |\n
        \  |       |                                                           |\n
        \  | 5     | Proportional Uniform Distribution: Each counter for an    |\n
        \  |       | Original Flow is divided by the number of time units the  |\n
        \  |       | Original Flow covers, to derive a mean count rate.  This  |\n
        \  |       | mean count rate is then multiplied by the number of time  |\n
        \  |       | units in the intersection of the duration of the Original |\n
        \  |       | Flow and the time interval of each Aggregated Flow.       |\n
        \  |       |  This is like simple uniform distribution, but accounts   |\n
        \  |       | for the fractional portions of a time interval covered by |\n
        \  |       | an Original Flow in the first and last time interval.     |\n
        \  |       | --------------------------------------------------------- |\n
        \  |       | --------------------------------------------------------- |\n
        \  | 6     | Simulated Process: Each counter of the Original Flow is   |\n
        \  |       | distributed among the intervals of the Aggregated Flows   |\n
        \  |       | according to some function the Intermediate Aggregation   |\n
        \  |       | Process uses based upon properties of Flows presumed to   |\n
        \  |       | be like the Original Flow.  This is essentially an        |\n
        \  |       | assertion that the Intermediate Aggregation Process has   |\n
        \  |       | no direct packet timing information but is nevertheless   |\n
        \  |       | not using one of the other simpler distribution methods.  |\n
        \  |       | The Intermediate Aggregation Process specifically makes   |\n
        \  |       | no assertion as to the correctness of the simulation.     |\n
        \  |       | --------------------------------------------------------- |\n
        \  |       |                                                           |\n
        \  | 7     | Direct: The Intermediate Aggregation Process has access   |\n
        \  |       | to the original packet timings from the packets making up |\n
        \  |       | the Original Flow, and uses these to distribute or        |\n
        \  |       | recalculate the counters.                                 |\n
        \  +-------+-----------------------------------------------------------+\n
        \  Abstract Data Type:  unsigned8\n   ElementID:  384\n"
      title: 7.4.2.  valueDistributionMethod Information Element
    title: 7.4.  Aggregate Counter Distribution Export
  title: 7.  Export of Aggregated IP Flows Using IPFIX
- contents:
  - "8.  Examples\n   In these examples, the same data, described by the same Template,\n
    \  will be aggregated multiple different ways; this illustrates the\n   various
    different functions that could be implemented by Intermediate\n   Aggregation
    Processes.  Templates are shown in IESpec format as\n   introduced in [RFC7013].
    \ The source data format is a simplified\n   Flow: timestamps, traditional 5-tuple,
    and octet count; the Flow Key\n   fields are the 5-tuple.  The Template is shown
    in Figure 9.\n   flowStartMilliseconds(152)[8]\n   flowEndMilliseconds(153)[8]\n
    \  sourceIPv4Address(8)[4]{key}\n   destinationIPv4Address(12)[4]{key}\n   sourceTransportPort(7)[2]{key}\n
    \  destinationTransportPort(11)[2]{key}\n   protocolIdentifier(4)[1]{key}\n   octetDeltaCount(1)[8]\n
    \                  Figure 9: Input Template for Examples\n   The data records
    given as input to the examples in this section are\n   shown below; timestamps
    are given in H:MM:SS.sss format.  In this and\n   subsequent figures, flowStartMilliseconds
    is shown in H:MM:SS.sss\n   format as 'start time', flowEndMilliseconds is shown
    in H:MM:SS.sss\n   format as 'end time', sourceIPv4Address is shown as 'source
    ip4' with\n   the following 'port' representing sourceTransportPort,\n   destinationIPv4Address
    is shown as 'dest ip4' with the following\n   'port' representing destinationTransportPort,
    protocolIdentifier is\n   shown as 'pt', and octetDeltaCount as 'oct'.\n  start
    time |end time   |source ip4 |port |dest ip4      |port|pt|  oct\n  9:00:00.138
    9:00:00.138 192.0.2.2   47113 192.0.2.131    53   17   119\n  9:00:03.246 9:00:03.246
    192.0.2.2   22153 192.0.2.131    53   17    83\n  9:00:00.478 9:00:03.486 192.0.2.2
    \  52420 198.51.100.2   443  6   1637\n  9:00:07.172 9:00:07.172 192.0.2.3   56047
    192.0.2.131    53   17   111\n  9:00:07.309 9:00:14.861 192.0.2.3   41183 198.51.100.67
    \ 80   6  16838\n  9:00:03.556 9:00:19.876 192.0.2.2   17606 198.51.100.68  80
    \  6  11538\n  9:00:25.210 9:00:25.210 192.0.2.3   47113 192.0.2.131    53   17
    \  119\n  9:00:26.358 9:00:30.198 192.0.2.3   48458 198.51.100.133 80   6   2973\n
    \ 9:00:29.213 9:01:00.061 192.0.2.4   61295 198.51.100.2   443  6   8350\n  9:04:00.207
    9:04:04.431 203.0.113.3 41256 198.51.100.133 80   6    778\n  9:03:59.624 9:04:06.984
    203.0.113.3 51662 198.51.100.3   80   6    883\n  9:00:30.532 9:06:15.402 192.0.2.2
    \  37581 198.51.100.2   80   6  15420\n  9:06:56.813 9:06:59.821 203.0.113.3 52572
    198.51.100.2   443  6   1637\n  9:06:30.565 9:07:00.261 203.0.113.3 49914 198.51.100.133
    80   6    561\n  9:06:55.160 9:07:05.208 192.0.2.2   50824 198.51.100.2   443
    \ 6   1899\n  9:06:49.322 9:07:05.322 192.0.2.3   34597 198.51.100.3   80   6
    \  1284\n  9:07:05.849 9:07:09.625 203.0.113.3 58907 198.51.100.4   80   6   2670\n
    \ 9:10:45.161 9:10:45.161 192.0.2.4   22478 192.0.2.131    53   17    75\n  9:10:45.209
    9:11:01.465 192.0.2.4   49513 198.51.100.68  80   6   3374\n  9:10:57.094 9:11:00.614
    192.0.2.4   64832 198.51.100.67  80   6    138\n  9:10:59.770 9:11:02.842 192.0.2.3
    \  60833 198.51.100.69  443  6   2325\n  9:02:18.390 9:13:46.598 203.0.113.3 39586
    198.51.100.17  80   6  11200\n  9:13:53.933 9:14:06.605 192.0.2.2   19638 198.51.100.3
    \  80   6   2869\n  9:13:02.864 9:14:08.720 192.0.2.3   40429 198.51.100.4   80
    \  6  18289\n                    Figure 10: Input Data for Examples\n"
  - contents:
    - "8.1.  Traffic Time Series per Source\n   Aggregating Flows by source IP address
      in time series (i.e., with a\n   regular interval) can be used in subsequent
      heavy-hitter analysis and\n   as a source parameter for statistical anomaly
      detection techniques.\n   Here, the Intermediate Aggregation Process imposes
      an interval,\n   aggregates the key to remove all key fields other than the
      source IP\n   address, then combines the result into a stream of Aggregated
      Flows.\n   The imposed interval of five minutes is longer than the majority
      of\n   Flows; for those Flows crossing interval boundaries, the entire Flow\n
      \  is accounted to the interval containing the start time of the Flow.\n   In
      this example, the Partially Aggregated Flows after each conceptual\n   operation
      in the Intermediate Aggregation Process are shown.  These\n   are meant to be
      illustrative of the conceptual operations only, and\n   not to suggest an implementation
      (indeed, the example shown here\n   would not necessarily be the most efficient
      method for performing\n   these operations).  Subsequent examples will omit
      the Partially\n   Aggregated Flows for brevity.\n   The input to this process
      could be any Flow Record containing a\n   source IP address and octet counter;
      consider for this example the\n   Template and data from the introduction.  The
      Intermediate\n   Aggregation Process would then output records containing just\n
      \  timestamps, source IP, and octetDeltaCount, as in Figure 11.\n   flowStartMilliseconds(152)[8]\n
      \  flowEndMilliseconds(153)[8]\n   sourceIPv4Address(8)[4]\n   octetDeltaCount(1)[8]\n
      \          Figure 11: Output Template for Time Series per Source\n   Assume
      the goal is to get 5-minute (300 s) time series of octet\n   counts per source
      IP address.  The aggregation operations would then\n   be arranged as in Figure
      12.\n                    Original Flows\n                          |\n                          V\n
      \             +-----------------------+\n              | interval distribution
      |\n              |  * impose uniform     |\n              |    300s time interval
      |\n              +-----------------------+\n                  |\n                  |
      Partially Aggregated Flows\n                  V\n   +------------------------+\n
      \  |  key aggregation       |\n   |   * reduce key to only |\n   |     sourceIPv4Address
      \ |\n   +------------------------+\n                  |\n                  |
      Partially Aggregated Flows\n                  V\n             +-------------------------+\n
      \            |  aggregate combination  |\n             |   * sum octetDeltaCount
      |\n             +-------------------------+\n                          |\n                          V\n
      \                 Aggregated Flows\n       Figure 12: Aggregation Operations
      for Time Series per Source\n   After applying the interval distribution step
      to the source data in\n   Figure 10, only the time intervals have changed; the
      Partially\n   Aggregated Flows are shown in Figure 13.  Note that interval\n
      \  distribution follows the default Start Interval policy; that is, the\n   entire
      Flow is accounted to the interval containing the Flow's start\n   time.\n  start
      time |end time   |source ip4 |port |dest ip4      |port|pt|  oct\n  9:00:00.000
      9:05:00.000 192.0.2.2   47113 192.0.2.131    53   17   119\n  9:00:00.000 9:05:00.000
      192.0.2.2   22153 192.0.2.131    53   17    83\n  9:00:00.000 9:05:00.000 192.0.2.2
      \  52420 198.51.100.2   443  6   1637\n  9:00:00.000 9:05:00.000 192.0.2.3   56047
      192.0.2.131    53   17   111\n  9:00:00.000 9:05:00.000 192.0.2.3   41183 198.51.100.67
      \ 80   6  16838\n  9:00:00.000 9:05:00.000 192.0.2.2   17606 198.51.100.68  80
      \  6  11538\n  9:00:00.000 9:05:00.000 192.0.2.3   47113 192.0.2.131    53   17
      \  119\n  9:00:00.000 9:05:00.000 192.0.2.3   48458 198.51.100.133 80   6   2973\n
      \ 9:00:00.000 9:05:00.000 192.0.2.4   61295 198.51.100.2   443  6   8350\n  9:00:00.000
      9:05:00.000 203.0.113.3 41256 198.51.100.133 80   6    778\n  9:00:00.000 9:05:00.000
      203.0.113.3 51662 198.51.100.3   80   6    883\n  9:00:00.000 9:05:00.000 192.0.2.2
      \  37581 198.51.100.2   80   6  15420\n  9:00:00.000 9:05:00.000 203.0.113.3
      39586 198.51.100.17  80   6  11200\n  9:05:00.000 9:10:00.000 203.0.113.3 52572
      198.51.100.2   443  6   1637\n  9:05:00.000 9:10:00.000 203.0.113.3 49914 197.51.100.133
      80   6    561\n  9:05:00.000 9:10:00.000 192.0.2.2   50824 198.51.100.2   443
      \ 6   1899\n  9:05:00.000 9:10:00.000 192.0.2.3   34597 198.51.100.3   80   6
      \  1284\n  9:05:00.000 9:10:00.000 203.0.113.3 58907 198.51.100.4   80   6   2670\n
      \ 9:10:00.000 9:15:00.000 192.0.2.4   22478 192.0.2.131    53   17    75\n  9:10:00.000
      9:15:00.000 192.0.2.4   49513 198.51.100.68  80   6   3374\n  9:10:00.000 9:15:00.000
      192.0.2.4   64832 198.51.100.67  80   6    138\n  9:10:00.000 9:15:00.000 192.0.2.3
      \  60833 198.51.100.69  443  6   2325\n  9:10:00.000 9:15:00.000 192.0.2.2   19638
      198.51.100.3   80   6   2869\n  9:10:00.000 9:15:00.000 192.0.2.3   40429 198.51.100.4
      \  80   6  18289\n         Figure 13: Interval Imposition for Time Series per
      Source\n   After the key aggregation step, all Flow Keys except the source IP\n
      \  address have been discarded, as shown in Figure 14.  This leaves\n   duplicate
      Partially Aggregated Flows to be combined in the final\n   operation.\n   start
      time |end time   |source ip4 |octets\n   9:00:00.000 9:05:00.000 192.0.2.2      119\n
      \  9:00:00.000 9:05:00.000 192.0.2.2       83\n   9:00:00.000 9:05:00.000 192.0.2.2
      \    1637\n   9:00:00.000 9:05:00.000 192.0.2.3      111\n   9:00:00.000 9:05:00.000
      192.0.2.3    16838\n   9:00:00.000 9:05:00.000 192.0.2.2    11538\n   9:00:00.000
      9:05:00.000 192.0.2.3      119\n   9:00:00.000 9:05:00.000 192.0.2.3     2973\n
      \  9:00:00.000 9:05:00.000 192.0.2.4     8350\n   9:00:00.000 9:05:00.000 203.0.113.3
      \   778\n   9:00:00.000 9:05:00.000 203.0.113.3    883\n   9:00:00.000 9:05:00.000
      192.0.2.2    15420\n   9:00:00.000 9:05:00.000 203.0.113.3  11200\n   9:05:00.000
      9:10:00.000 203.0.113.3   1637\n   9:05:00.000 9:10:00.000 203.0.113.3    561\n
      \  9:05:00.000 9:10:00.000 192.0.2.2     1899\n   9:05:00.000 9:10:00.000 192.0.2.3
      \    1284\n   9:05:00.000 9:10:00.000 203.0.113.3   2670\n   9:10:00.000 9:15:00.000
      192.0.2.4       75\n   9:10:00.000 9:15:00.000 192.0.2.4     3374\n   9:10:00.000
      9:15:00.000 192.0.2.4      138\n   9:10:00.000 9:15:00.000 192.0.2.3     2325\n
      \  9:10:00.000 9:15:00.000 192.0.2.2     2869\n   9:10:00.000 9:15:00.000 192.0.2.3
      \   18289\n           Figure 14: Key Aggregation for Time Series per Source\n
      \  Aggregate combination sums the counters per key and interval; the\n   summations
      of the first two keys and intervals are shown in detail in\n   Figure 15.\n
      \    start time |end time   |source ip4 |octets\n     9:00:00.000 9:05:00.000
      192.0.2.2      119\n     9:00:00.000 9:05:00.000 192.0.2.2       83\n     9:00:00.000
      9:05:00.000 192.0.2.2     1637\n     9:00:00.000 9:05:00.000 192.0.2.2    11538\n
      \  + 9:00:00.000 9:05:00.000 192.0.2.2    15420\n                                          -----\n
      \  = 9:00:00.000 9:05:00.000 192.0.2.2    28797\n     9:00:00.000 9:05:00.000
      192.0.2.3      111\n     9:00:00.000 9:05:00.000 192.0.2.3    16838\n     9:00:00.000
      9:05:00.000 192.0.2.3      119\n   + 9:00:00.000 9:05:00.000 192.0.2.3     2973\n
      \                                         -----\n   = 9:00:00.000 9:05:00.000
      192.0.2.3    20041\n             Figure 15: Summation during Aggregate Combination\n
      \  This can be applied to each set of Partially Aggregated Flows to\n   produce
      the final Aggregated Flows that are shown in Figure 16, as\n   exported by the
      Template in Figure 11.\n   start time |end time   |source ip4 |octets\n   9:00:00.000
      9:05:00.000 192.0.2.2    28797\n   9:00:00.000 9:05:00.000 192.0.2.3    20041\n
      \  9:00:00.000 9:05:00.000 192.0.2.4     8350\n   9:00:00.000 9:05:00.000 203.0.113.3
      \ 12861\n   9:05:00.000 9:10:00.000 192.0.2.2     1899\n   9:05:00.000 9:10:00.000
      192.0.2.3     1284\n   9:05:00.000 9:10:00.000 203.0.113.3   4868\n   9:10:00.000
      9:15:00.000 192.0.2.2     2869\n   9:10:00.000 9:15:00.000 192.0.2.3    20614\n
      \  9:10:00.000 9:15:00.000 192.0.2.4     3587\n          Figure 16: Aggregated
      Flows for Time Series per Source\n"
    title: 8.1.  Traffic Time Series per Source
  - contents:
    - "8.2.  Core Traffic Matrix\n   Aggregating Flows by source and destination ASN
      in time series is\n   used to generate core traffic matrices.  The core traffic
      matrix\n   provides a view of the state of the routes within a network, and
      it\n   can be used for long-term planning of changes to network design based\n
      \  on traffic demand.  Here, imposed time intervals are generally much\n   longer
      than active Flow timeouts.  The traffic matrix is reported in\n   terms of octets,
      packets, and flows, as each of these values may have\n   a subtly different
      effect on capacity planning.\n   This example demonstrates key aggregation using
      derived keys and\n   Original Flow counting.  While some Original Flows may
      be generated\n   by Exporting Processes on forwarding devices, and therefore
      contain\n   the bgpSourceAsNumber and bgpDestinationAsNumber Information\n   Elements,
      Original Flows from Exporting Processes on dedicated\n   measurement devices
      without routing data contain only a\n   destinationIPv[46]Address.  For these
      Flows, the Mediator must look\n   up a next-hop AS from an IP-to-AS table, replacing
      source and\n   destination addresses with ASNs.  The table used in this example
      is\n   shown in Figure 17.  (Note that due to limited example address space,\n
      \  in this example we ignore the common practice of routing only blocks\n   of
      /24 or larger.)\n   prefix           |ASN\n   192.0.2.0/25      64496\n   192.0.2.128/25
      \   64497\n   198.51.100/24     64498\n   203.0.113.0/24    64499\n                        Figure
      17: Example ASN Map\n   The Template for Aggregated Flows produced by this example
      is shown\n   in Figure 18.\n   flowStartMilliseconds(152)[8]\n   flowEndMilliseconds(153)[8]\n
      \  bgpSourceAsNumber(16)[4]\n   bgpDestinationAsNumber(17)[4]\n   octetDeltaCount(1)[8]\n
      \              Figure 18: Output Template for Traffic Matrix\n   Assume the
      goal is to get 60-minute time series of octet counts per\n   source/destination
      ASN pair.  The aggregation operations would then\n   be arranged as in Figure
      19.\n                    Original Flows\n                          |\n                          V\n
      \             +-----------------------+\n              | interval distribution
      |\n              |  * impose uniform     |\n              |    3600s time interval|\n
      \             +-----------------------+\n                  |\n                  |
      Partially Aggregated Flows\n                  V\n   +------------------------+\n
      \  |  key aggregation       |\n   |  * reduce key to only  |\n   |    sourceIPv4Address
      + |\n   |    destIPv4Address     |\n   +------------------------+\n                  |\n
      \                 V\n   +------------------------+\n   |  key aggregation       |\n
      \  |  * replace addresses   |\n   |    with ASN from map   |\n   +------------------------+\n
      \                 |\n                  | Partially Aggregated Flows\n                  V\n
      \            +-------------------------+\n             |  aggregate combination
      \ |\n             |   * sum octetDeltaCount |\n             +-------------------------+\n
      \                         |\n                          V\n                  Aggregated
      Flows\n           Figure 19: Aggregation Operations for Traffic Matrix\n   After
      applying the interval distribution step to the source data in\n   Figure 10,
      the Partially Aggregated Flows are shown in Figure 20.\n   Note that the Flows
      are identical to those in the interval\n   distribution step in the previous
      example, except the chosen interval\n   (1 hour, 3600 seconds) is different;
      therefore, all the Flows fit\n   into a single interval.\n   start time |end
      time |source ip4 |port |dest ip4      |port|pt|  oct\n   9:00:00     10:00:00
      \ 192.0.2.2   47113 192.0.2.131    53   17   119\n   9:00:00     10:00:00  192.0.2.2
      \  22153 192.0.2.131    53   17    83\n   9:00:00     10:00:00  192.0.2.2   52420
      198.51.100.2   443  6   1637\n   9:00:00     10:00:00  192.0.2.3   56047 192.0.2.131
      \   53   17   111\n   9:00:00     10:00:00  192.0.2.3   41183 198.51.100.67
      \ 80   6  16838\n   9:00:00     10:00:00  192.0.2.2   17606 198.51.100.68  80
      \  6  11538\n   9:00:00     10:00:00  192.0.2.3   47113 192.0.2.131    53   17
      \  119\n   9:00:00     10:00:00  192.0.2.3   48458 198.51.100.133 80   6   2973\n
      \  9:00:00     10:00:00  192.0.2.4   61295 198.51.100.2   443  6   8350\n   9:00:00
      \    10:00:00  203.0.113.3 41256 198.51.100.133 80   6    778\n   9:00:00     10:00:00
      \ 203.0.113.3 51662 198.51.100.3   80   6    883\n   9:00:00     10:00:00  192.0.2.2
      \  37581 198.51.100.2   80   6  15420\n   9:00:00     10:00:00  203.0.113.3
      52572 198.51.100.2   443  6   1637\n   9:00:00     10:00:00  203.0.113.3 49914
      197.51.100.133 80   6    561\n   9:00:00     10:00:00  192.0.2.2   50824 198.51.100.2
      \  443  6   1899\n   9:00:00     10:00:00  192.0.2.3   34597 198.51.100.3   80
      \  6   1284\n   9:00:00     10:00:00  203.0.113.3 58907 198.51.100.4   80   6
      \  2670\n   9:00:00     10:00:00  192.0.2.4   22478 192.0.2.131    53   17    75\n
      \  9:00:00     10:00:00  192.0.2.4   49513 198.51.100.68  80   6   3374\n   9:00:00
      \    10:00:00  192.0.2.4   64832 198.51.100.67  80   6    138\n   9:00:00     10:00:00
      \ 192.0.2.3   60833 198.51.100.69  443  6   2325\n   9:00:00     10:00:00  203.0.113.3
      39586 198.51.100.17  80   6  11200\n   9:00:00     10:00:00  192.0.2.2   19638
      198.51.100.3   80   6   2869\n   9:00:00     10:00:00  192.0.2.3   40429 198.51.100.4
      \  80   6  18289\n             Figure 20: Interval Imposition for Traffic Matrix\n
      \  The next steps are to discard irrelevant key fields and to replace\n   the
      source and destination addresses with source and destination ASNs\n   in the
      map; the results of these key aggregation steps are shown in\n   Figure 21.\n
      \  start time |end time |source ASN |dest ASN |octets\n   9:00:00     10:00:00
      \ AS64496     AS64497      119\n   9:00:00     10:00:00  AS64496     AS64497
      \      83\n   9:00:00     10:00:00  AS64496     AS64498     1637\n   9:00:00
      \    10:00:00  AS64496     AS64497      111\n   9:00:00     10:00:00  AS64496
      \    AS64498    16838\n   9:00:00     10:00:00  AS64496     AS64498    11538\n
      \  9:00:00     10:00:00  AS64496     AS64497      119\n   9:00:00     10:00:00
      \ AS64496     AS64498     2973\n   9:00:00     10:00:00  AS64496     AS64498
      \    8350\n   9:00:00     10:00:00  AS64499     AS64498      778\n   9:00:00
      \    10:00:00  AS64499     AS64498      883\n   9:00:00     10:00:00  AS64496
      \    AS64498    15420\n   9:00:00     10:00:00  AS64499     AS64498     1637\n
      \  9:00:00     10:00:00  AS64499     AS64498      561\n   9:00:00     10:00:00
      \ AS64496     AS64498     1899\n   9:00:00     10:00:00  AS64496     AS64498
      \    1284\n   9:00:00     10:00:00  AS64499     AS64498     2670\n   9:00:00
      \    10:00:00  AS64496     AS64497       75\n   9:00:00     10:00:00  AS64496
      \    AS64498     3374\n   9:00:00     10:00:00  AS64496     AS64498      138\n
      \  9:00:00     10:00:00  AS64496     AS64498     2325\n   9:00:00     10:00:00
      \ AS64499     AS64498    11200\n   9:00:00     10:00:00  AS64496     AS64498
      \    2869\n   9:00:00     10:00:00  AS64496     AS64498    18289\n              Figure
      21: Key Aggregation for Traffic Matrix:\n                         Reduction
      and Replacement\n   Finally, aggregate combination sums the counters per key
      and\n   interval.  The resulting Aggregated Flows containing the traffic\n   matrix,
      shown in Figure 22, are then exported using the Template in\n   Figure 18.  Note
      that these Aggregated Flows represent a sparse\n   matrix: AS pairs for which
      no traffic was received have no\n   corresponding record in the output.\n   start
      time  end time  source ASN  dest ASN  octets\n   9:00:00     10:00:00  AS64496
      \    AS64497      507\n   9:00:00     10:00:00  AS64496     AS64498    86934\n
      \  9:00:00     10:00:00  AS64499     AS64498    17729\n              Figure
      22: Aggregated Flows for Traffic Matrix\n   The output of this operation is
      suitable for re-aggregation: that is,\n   traffic matrices from single links
      or Observation Points can be\n   aggregated through the same interval imposition
      and aggregate\n   combination steps in order to build a traffic matrix for an
      entire\n   network.\n"
    title: 8.2.  Core Traffic Matrix
  - contents:
    - "8.3.  Distinct Source Count per Destination Endpoint\n   Aggregating Flows
      by destination address and port, and counting\n   distinct sources aggregated
      away, can be used as part of passive\n   service inventory and host characterization.
      \ This example shows\n   aggregation as an analysis technique, performed on
      source data stored\n   in an IPFIX File.  As the Transport Session in this File
      is bounded,\n   removal of all timestamp information allows summarization of
      the\n   entire time interval contained within the interval.  Removal of\n   timing
      information during interval imposition is equivalent to an\n   infinitely long
      imposed time interval.  This demonstrates both how\n   infinite intervals work,
      and how unique counters work.  The\n   aggregation operations are summarized
      in Figure 23.\n                    Original Flows\n                          |\n
      \                         V\n              +-----------------------+\n              |
      interval distribution |\n              |  * discard timestamps |\n              +-----------------------+\n
      \                 |\n                  | Partially Aggregated Flows\n                  V\n
      \  +----------------------------+\n   |  value aggregation         |\n   |  *
      discard octetDeltaCount |\n   +----------------------------+\n                  |\n
      \                 | Partially Aggregated Flows\n                  V\n   +----------------------------+\n
      \  |  key aggregation           |\n   |   * reduce key to only     |\n   |     destIPv4Address
      +      |\n   |     destTransportPort,     |\n   |   * count distinct sources
      |\n   +----------------------------+\n                  |\n                  |
      Partially Aggregated Flows\n                  V\n       +----------------------------------------------+\n
      \      |  aggregate combination                       |\n       |   * no-op
      (distinct sources already counted) |\n       +----------------------------------------------+\n
      \                         |\n                          V\n                  Aggregated
      Flows\n            Figure 23: Aggregation Operations for Source Count\n   The
      Template for Aggregated Flows produced by this example is shown\n   in Figure
      24.\n   destinationIPv4Address(12)[4]\n   destinationTransportPort(11)[2]\n
      \  distinctCountOfSourceIPAddress(378)[8]\n                Figure 24: Output
      Template for Source Count\n   Interval distribution, in this case, merely discards
      the timestamp\n   information from the Original Flows in Figure 10, and as such
      is not\n   shown.  Likewise, the value aggregation step simply discards the\n
      \  octetDeltaCount value field.  The key aggregation step reduces the\n   key
      to the destinationIPv4Address and destinationTransportPort,\n   counting the
      distinct source addresses.  Since this is essentially\n   the output of this
      aggregation function, the aggregate combination\n   operation is a no-op; the
      resulting Aggregated Flows are shown in\n   Figure 25.\n   dest ip4      |port
      |dist src\n   192.0.2.131    53           3\n   198.51.100.2   80           1\n
      \  198.51.100.2   443          3\n   198.51.100.67  80           2\n   198.51.100.68
      \ 80           2\n   198.51.100.133 80           2\n   198.51.100.3   80           3\n
      \  198.51.100.4   80           2\n   198.51.100.17  80           1\n   198.51.100.69
      \ 443          1\n               Figure 25: Aggregated Flows for Source Count\n"
    title: 8.3.  Distinct Source Count per Destination Endpoint
  - contents:
    - "8.4.  Traffic Time Series per Source with Counter Distribution\n   Returning
      to the example in Section 8.1, note that our source data\n   contains some Flows
      with durations longer than the imposed interval\n   of five minutes.  The default
      method for dealing with such Flows is\n   to account them to the interval containing
      the Flow's start time.\n   In this example, the same data is aggregated using
      the same\n   arrangement of operations and the same output Template as in\n
      \  Section 8.1, but using a different counter distribution policy,\n   Simple
      Uniform Distribution, as described in Section 5.1.1.  In order\n   to do this,
      the Exporting Process first exports the Aggregate Counter\n   Distribution Options
      Template, as in Figure 26.\n   templateId(12)[2]{scope}\n   valueDistributionMethod(384)[1]\n
      \       Figure 26: Aggregate Counter Distribution Options Template\n   This
      Template is followed by an Aggregate Counter Distribution Record\n   described
      by this Template; assuming the output Template in Figure 11\n   has ID 257,
      this record would appear as in Figure 27.\n   template ID | value distribution
      method\n           257   4 (simple uniform)\n             Figure 27: Aggregate
      Counter Distribution Record\n   Following metadata export, the aggregation steps
      follow as before.\n   However, two long Flows are distributed across multiple
      intervals in\n   the interval imposition step, as indicated with \"*\" in Figure
      28.\n   Note the uneven distribution of the three-interval, 11200-octet Flow\n
      \  into three Partially Aggregated Flows of 3733, 3733, and 3734 octets;\n   this
      ensures no cumulative error is injected by the interval\n   distribution step.\n
      start time |end time   |source ip4 |port |dest ip4      |port|pt|  oct\n 9:00:00.000
      9:05:00.000 192.0.2.2   47113 192.0.2.131    53   17   119\n 9:00:00.000 9:05:00.000
      192.0.2.2   22153 192.0.2.131    53   17    83\n 9:00:00.000 9:05:00.000 192.0.2.2
      \  52420 198.51.100.2   443  6   1637\n 9:00:00.000 9:05:00.000 192.0.2.3   56047
      192.0.2.131    53   17   111\n 9:00:00.000 9:05:00.000 192.0.2.3   41183 198.51.100.67
      \ 80   6  16838\n 9:00:00.000 9:05:00.000 192.0.2.2   17606 198.51.100.68  80
      \  6  11538\n 9:00:00.000 9:05:00.000 192.0.2.3   47113 192.0.2.131    53   17
      \  119\n 9:00:00.000 9:05:00.000 192.0.2.3   48458 198.51.100.133 80   6   2973\n
      9:00:00.000 9:05:00.000 192.0.2.4   61295 198.51.100.2   443  6   8350\n 9:00:00.000
      9:05:00.000 203.0.113.3 41256 198.51.100.133 80   6    778\n 9:00:00.000 9:05:00.000
      203.0.113.3 51662 198.51.100.3   80   6    883\n 9:00:00.000 9:05:00.000 192.0.2.2
      \  37581 198.51.100.2   80   6   7710*\n 9:00:00.000 9:05:00.000 203.0.113.3
      39586 198.51.100.17  80   6   3733*\n 9:05:00.000 9:10:00.000 203.0.113.3 52572
      198.51.100.2   443  6   1637\n 9:05:00.000 9:10:00.000 203.0.113.3 49914 197.51.100.133
      80   6    561\n 9:05:00.000 9:10:00.000 192.0.2.2   50824 198.51.100.2   443
      \ 6   1899\n 9:05:00.000 9:10:00.000 192.0.2.3   34597 198.51.100.3   80   6
      \  1284\n 9:05:00.000 9:10:00.000 203.0.113.3 58907 198.51.100.4   80   6   2670\n
      9:05:00.000 9:10:00.000 192.0.2.2   37581 198.51.100.2   80   6   7710*\n 9:05:00.000
      9:10:00.000 203.0.113.3 39586 198.51.100.17  80   6   3733*\n 9:10:00.000 9:15:00.000
      192.0.2.4   22478 192.0.2.131    53   17    75\n 9:10:00.000 9:15:00.000 192.0.2.4
      \  49513 198.51.100.68  80   6   3374\n 9:10:00.000 9:15:00.000 192.0.2.4   64832
      198.51.100.67  80   6    138\n 9:10:00.000 9:15:00.000 192.0.2.3   60833 198.51.100.69
      \ 443  6   2325\n 9:10:00.000 9:15:00.000 192.0.2.2   19638 198.51.100.3   80
      \  6   2869\n 9:10:00.000 9:15:00.000 192.0.2.3   40429 198.51.100.4   80   6
      \ 18289\n 9:10:00.000 9:15:00.000 203.0.113.3 39586 198.51.100.17  80   6   3734*\n
      \ Figure 28: Distributed Interval Imposition for Time Series per Source\n   Subsequent
      steps are as in Section 8.1; the results, to be exported\n   using the Template
      shown in Figure 11, are shown in Figure 29, with\n   Aggregated Flows differing
      from the example in Section 8.1 indicated\n   by \"*\".\n   start time |end
      time   |source ip4 |octets\n   9:00:00.000 9:05:00.000 192.0.2.2    21087*\n
      \  9:00:00.000 9:05:00.000 192.0.2.3    20041\n   9:00:00.000 9:05:00.000 192.0.2.4
      \    8350\n   9:00:00.000 9:05:00.000 203.0.113.3   5394*\n   9:05:00.000 9:10:00.000
      192.0.2.2     9609*\n   9:05:00.000 9:10:00.000 192.0.2.3     1284\n   9:05:00.000
      9:10:00.000 203.0.113.3   8601*\n   9:10:00.000 9:15:00.000 192.0.2.2     2869\n
      \  9:10:00.000 9:15:00.000 192.0.2.3    20614\n   9:10:00.000 9:15:00.000 192.0.2.4
      \    3587\n   9:10:00.000 9:15:00.000 203.0.113.3   3734*\n          Figure
      29: Aggregated Flows for Time Series per Source\n                         with
      Counter Distribution\n"
    title: 8.4.  Traffic Time Series per Source with Counter Distribution
  title: 8.  Examples
- contents:
  - "9.  Security Considerations\n   This document specifies the operation of an Intermediate
    Aggregation\n   Process with the IPFIX protocol; the Security Considerations for
    the\n   protocol itself in Section 11 of [RFC7011] therefore apply.  In the\n
    \  common case that aggregation is performed on a Mediator, the Security\n   Considerations
    for Mediators in Section 9 of [RFC6183] apply as well.\n   As mentioned in Section
    3, certain aggregation operations may tend to\n   have an anonymizing effect on
    Flow data by obliterating sensitive\n   identifiers.  Aggregation may also be
    combined with anonymization\n   within a Mediator, or as part of a chain of Mediators,
    to further\n   leverage this effect.  In any case in which an Intermediate\n   Aggregation
    Process is applied as part of a data anonymization or\n   protection scheme, or
    is used together with anonymization as\n   described in [RFC6235], the Security
    Considerations in Section 9 of\n   [RFC6235] apply.\n"
  title: 9.  Security Considerations
- contents:
  - "10.  IANA Considerations\n   This document specifies the creation of new IPFIX
    Information\n   Elements in the IPFIX Information Element registry [IANA-IPFIX],
    as\n   defined in Section 7 above.  IANA has assigned Information Element\n   numbers
    to these Information Elements, and entered them into the\n   registry.\n"
  title: 10.  IANA Considerations
- contents:
  - "11.  Acknowledgments\n   Special thanks to Elisa Boschi for early work on the
    concepts laid\n   out in this document.  Thanks to Lothar Braun, Christian Henke,
    and\n   Rahul Patel for their reviews and valuable feedback, with special\n   thanks
    to Paul Aitken for his multiple detailed reviews.  This work\n   is materially
    supported by the European Union Seventh Framework\n   Programme under grant agreement
    257315 (DEMONS).\n"
  title: 11.  Acknowledgments
- contents:
  - '12.  References

    '
  - contents:
    - "12.1.  Normative References\n   [RFC2119]  Bradner, S., \"Key words for use
      in RFCs to Indicate\n              Requirement Levels\", BCP 14, RFC 2119, March
      1997.\n   [RFC5226]  Narten, T. and H. Alvestrand, \"Guidelines for Writing
      an\n              IANA Considerations Section in RFCs\", BCP 26, RFC 5226,\n
      \             May 2008.\n   [RFC7011]  Claise, B., Ed., Trammell, B., Ed., and
      P. Aitken,\n              \"Specification of the IP Flow Information Export
      (IPFIX)\n              Protocol for the Exchange of Flow Information\", STD
      77,\n              RFC 7011, September 2013.\n"
    title: 12.1.  Normative References
  - contents:
    - "12.2.  Informative References\n   [RFC3917]  Quittek, J., Zseby, T., Claise,
      B., and S. Zander,\n              \"Requirements for IP Flow Information Export
      (IPFIX)\", RFC\n              3917, October 2004.\n   [RFC5470]  Sadasivan,
      G., Brownlee, N., Claise, B., and J. Quittek,\n              \"Architecture
      for IP Flow Information Export\", RFC 5470,\n              March 2009.\n   [RFC5472]
      \ Zseby, T., Boschi, E., Brownlee, N., and B. Claise, \"IP\n              Flow
      Information Export (IPFIX) Applicability\", RFC 5472,\n              March 2009.\n
      \  [RFC5476]  Claise, B., Johnson, A., and J. Quittek, \"Packet Sampling\n              (PSAMP)
      Protocol Specifications\", RFC 5476, March 2009.\n   [RFC5655]  Trammell, B.,
      Boschi, E., Mark, L., Zseby, T., and A.\n              Wagner, \"Specification
      of the IP Flow Information Export\n              (IPFIX) File Format\", RFC
      5655, October 2009.\n   [RFC5982]  Kobayashi, A. and B. Claise, \"IP Flow Information
      Export\n              (IPFIX) Mediation: Problem Statement\", RFC 5982, August\n
      \             2010.\n   [RFC6183]  Kobayashi, A., Claise, B., Muenz, G., and
      K. Ishibashi,\n              \"IP Flow Information Export (IPFIX) Mediation:
      Framework\",\n              RFC 6183, April 2011.\n   [RFC6235]  Boschi, E.
      and B. Trammell, \"IP Flow Anonymization\n              Support\", RFC 6235,
      May 2011.\n   [RFC6728]  Muenz, G., Claise, B., and P. Aitken, \"Configuration
      Data\n              Model for the IP Flow Information Export (IPFIX) and\n              Packet
      Sampling (PSAMP) Protocols\", RFC 6728, October\n              2012.\n   [RFC7012]
      \ Claise, B., Ed. and B. Trammell, Ed., \"Information Model\n              for
      IP Flow Information Export (IPFIX)\", RFC 7012,\n              September 2013.\n
      \  [RFC7013]  Trammell, B. and B. Claise, \"Guidelines for Authors and\n              Reviewers
      of IP Flow Information Export (IPFIX)\n              Information Elements\",
      BCP 184, RFC 7013, September 2013.\n   [RFC7014]  D'Antonio, S., Zseby, T.,
      Henke, C., and L. Peluso, \"Flow\n              Selection Techniques\", RFC
      7014, September 2013.\n   [IANA-IPFIX]\n              IANA, \"IP Flow Information
      Export (IPFIX) Entities\",\n              <http://www.iana.org/assignments/ipfix>.\n
      \  [IPFIX-MED-PROTO]\n              Claise, B., Kobayashi, A., and B. Trammell,
      \"Operation of\n              the IP Flow Information Export (IPFIX) Protocol
      on IPFIX\n              Mediators\", Work in Progress, July 2013.\n"
    title: 12.2.  Informative References
  title: 12.  References
- contents:
  - "Authors' Addresses\n   Brian Trammell\n   Swiss Federal Institute of Technology
    Zurich\n   Gloriastrasse 35\n   8092 Zurich\n   Switzerland\n   Phone: +41 44
    632 70 13\n   EMail: trammell@tik.ee.ethz.ch\n   Arno Wagner\n   Consecom AG\n
    \  Bleicherweg 64a\n   8002 Zurich\n   Switzerland\n   EMail: arno@wagner.name\n
    \  Benoit Claise\n   Cisco Systems, Inc.\n   De Kleetlaan 6a b1\n   1831 Diegem\n
    \  Belgium\n   Phone: +32 2 704 5622\n   EMail: bclaise@cisco.com\n"
  title: Authors' Addresses
