- title: __initial_text__
  contents:
  - ''
- title: Independent Submission                                      L. Song, Ed.
  contents:
  - "Independent Submission                                      L. Song, Ed.\n  \
    \                          Yeti DNS Testbed\n"
- title: Abstract
  contents:
  - "Abstract\n   Yeti DNS is an experimental, non-production root server testbed\
    \ that\n   provides an environment where technical and operational experiments\n\
    \   can safely be performed without risk to production root server\n   infrastructure.\
    \  This document aims solely to document the technical\n   and operational experience\
    \ of deploying a system that is similar to\n   but different from the Root Server\
    \ system (on which the Internet's\n   Domain Name System is designed and built).\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This is a contribution to\
    \ the RFC Series, independently of any other\n   RFC stream.  The RFC Editor has\
    \ chosen to publish this document at\n   its discretion and makes no statement\
    \ about its value for\n   implementation or deployment.  Documents approved for\
    \ publication by\n   the RFC Editor are not candidates for any level of Internet\
    \ Standard;\n   see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   https://www.rfc-editor.org/info/rfc8483.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2018 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (https://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n   2.  Requirements Notation and Conventions . . . . . . . .\
    \ . . . .   5\n   3.  Areas of Study  . . . . . . . . . . . . . . . . . . . .\
    \ . . .   5\n     3.1.  Implementation of a Testbed like the Root Server System\
    \ .   5\n     3.2.  Yeti-Root Zone Distribution . . . . . . . . . . . . . . .\
    \   5\n     3.3.  Yeti-Root Server Names and Addressing . . . . . . . . . .  \
    \ 5\n     3.4.  IPv6-Only Yeti-Root Servers . . . . . . . . . . . . . . .   6\n\
    \     3.5.  DNSSEC in the Yeti-Root Zone  . . . . . . . . . . . . . .   6\n  \
    \ 4.  Yeti DNS Testbed Infrastructure . . . . . . . . . . . . . . .   7\n    \
    \ 4.1.  Root Zone Retrieval . . . . . . . . . . . . . . . . . . .   8\n     4.2.\
    \  Transformation of Root Zone to Yeti-Root Zone . . . . . .   9\n       4.2.1.\
    \  ZSK and KSK Key Sets Shared between DMs . . . . . . .  10\n       4.2.2.  Unique\
    \ ZSK per DM; No Shared KSK  . . . . . . . . . .  10\n       4.2.3.  Preserving\
    \ Root Zone NSEC Chain and ZSK RRSIGs  . . .  11\n     4.3.  Yeti-Root Zone Distribution\
    \ . . . . . . . . . . . . . . .  12\n     4.4.  Synchronization of Service Metadata\
    \ . . . . . . . . . . .  12\n     4.5.  Yeti-Root Server Naming Scheme  . . .\
    \ . . . . . . . . . .  13\n     4.6.  Yeti-Root Servers . . . . . . . . . . .\
    \ . . . . . . . . .  14\n     4.7.  Experimental Traffic  . . . . . . . . . .\
    \ . . . . . . . .  16\n     4.8.  Traffic Capture and Analysis  . . . . . . .\
    \ . . . . . . .  16\n   5.  Operational Experience with the Yeti DNS Testbed \
    \ . . . . . .  17\n     5.1.  Viability of IPv6-Only Operation  . . . . . . .\
    \ . . . . .  17\n       5.1.1.  IPv6 Fragmentation  . . . . . . . . . . . . .\
    \ . . . .  18\n       5.1.2.  Serving IPv4-Only End-Users . . . . . . . . . .\
    \ . . .  19\n     5.2.  Zone Distribution . . . . . . . . . . . . . . . . . .\
    \ . .  19\n       5.2.1.  Zone Transfers  . . . . . . . . . . . . . . . . . .\
    \ .  19\n       5.2.2.  Delays in Yeti-Root Zone Distribution . . . . . . . .\
    \  20\n       5.2.3.  Mixed RRSIGs from Different DM ZSKs . . . . . . . . .  21\n\
    \     5.3.  DNSSEC KSK Rollover . . . . . . . . . . . . . . . . . . .  22\n  \
    \     5.3.1.  Failure-Case KSK Rollover . . . . . . . . . . . . . .  22\n    \
    \   5.3.2.  KSK Rollover vs. BIND9 Views  . . . . . . . . . . . .  22\n      \
    \ 5.3.3.  Large Responses during KSK Rollover . . . . . . . . .  23\n     5.4.\
    \  Capture of Large DNS Response . . . . . . . . . . . . . .  24\n     5.5.  Automated\
    \ Maintenance of the Hints File . . . . . . . . .  24\n     5.6.  Root Label Compression\
    \ in Knot DNS Server . . . . . . . .  25\n   6.  Conclusions . . . . . . . . .\
    \ . . . . . . . . . . . . . . . .  26\n   7.  Security Considerations . . . .\
    \ . . . . . . . . . . . . . . .  28\n   8.  IANA Considerations . . . . . . .\
    \ . . . . . . . . . . . . . .  28\n   9.  References  . . . . . . . . . . . .\
    \ . . . . . . . . . . . . .  29\n     9.1.  Normative References  . . . . . .\
    \ . . . . . . . . . . . .  29\n     9.2.  Informative References  . . . . . .\
    \ . . . . . . . . . . .  29\n   Appendix A.  Yeti-Root Hints File . . . . . .\
    \ . . . . . . . . . .  33\n   Appendix B.  Yeti-Root Server Priming Response \
    \ . . . . . . . . .  34\n   Appendix C.  Active IPv6 Prefixes in Yeti DNS Testbed\
    \ . . . . . .  36\n   Appendix D.  Tools Developed for Yeti DNS Testbed . . .\
    \ . . . . .  36\n   Appendix E.  Controversy  . . . . . . . . . . . . . . . .\
    \ . . . .  37\n   Acknowledgments . . . . . . . . . . . . . . . . . . . . . .\
    \ . . .  38\n   Authors' Addresses  . . . . . . . . . . . . . . . . . . . . .\
    \ . .  39\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The Domain Name System (DNS), as originally specified in\
    \ [RFC1034]\n   and [RFC1035], has proved to be an enduring and important platform\n\
    \   upon which almost every end-user of the Internet relies.  Despite its\n  \
    \ longevity, extensions to the protocol, new implementations, and\n   refinements\
    \ to DNS operations continue to emerge both inside and\n   outside the IETF.\n\
    \   The Root Server system in particular has seen technical innovation\n   and\
    \ development, for example, in the form of wide-scale anycast\n   deployment,\
    \ the mitigation of unwanted traffic on a global scale, the\n   widespread deployment\
    \ of Response Rate Limiting [RRL], the\n   introduction of IPv6 transport, the\
    \ deployment of DNSSEC, changes in\n   DNSSEC key sizes, and preparations to roll\
    \ the root zone's Key\n   Signing Key (KSK) and corresponding trust anchor.  These\
    \ projects\n   created tremendous qualitative operational change and required\n\
    \   impressive caution and study prior to implementation.  They took\n   place\
    \ in parallel with the quantitative expansion or delegations for\n   new TLDs\
    \ (see <https://newgtlds.icann.org/>).\n   Aspects of the operational structure\
    \ of the Root Server system have\n   been described in such documents as [TNO2009],\
    \ [ISC-TN-2003-1],\n   [RSSAC001], and [RFC7720].  Such references, considered\
    \ together,\n   provide sufficient insight into the operations of the system as\
    \ a\n   whole that it is straightforward to imagine structural changes to the\n\
    \   Root Server system's infrastructure and to wonder what the\n   operational\
    \ implications of such changes might be.\n   The Yeti DNS Project was conceived\
    \ in May 2015 with the aim of\n   providing a non-production testbed that would\
    \ be open for use by\n   anyone from the technical community to propose or run\
    \ experiments\n   designed to answer these kinds of questions.  Coordination for\
    \ the\n   project was provided by BII, TISF, and the WIDE Project.  Thus, Yeti\n\
    \   DNS is an independently coordinated project and is not affiliated\n   with\
    \ the IETF, ICANN, IANA, or any Root Server Operator.  The\n   objectives of the\
    \ Yeti Project were set by the participants in the\n   project based on experiments\
    \ that they considered would provide\n   valuable information.\n   Many volunteers\
    \ collaborated to build a distributed testbed that at\n   the time of writing\
    \ includes 25 Yeti root servers with 16 operators\n   and handles experimental\
    \ traffic from individual volunteers,\n   universities, DNS vendors, and distributed\
    \ measurement networks.\n   By design, the Yeti testbed system serves the root\
    \ zone published by\n   the IANA with only those structural modifications necessary\
    \ to ensure\n   that it is able to function usefully in the Yeti testbed system\n\
    \   instead of the production Root Server system.  In particular, no\n   delegation\
    \ for any top-level zone is changed, added, or removed from\n   the IANA-published\
    \ root zone to construct the root zone served by the\n   Yeti testbed system,\
    \ and changes in the root zone are reflected in\n   the testbed in near real-time.\
    \  In this document, for clarity, we\n   refer to the zone derived from the IANA-published\
    \ root zone as the\n   Yeti-Root zone.\n   The Yeti DNS testbed serves a similar\
    \ function to the Root Server\n   system in the sense that they both serve similar\
    \ zones: the Yeti-Root\n   zone and the IANA-published root zone.  However, the\
    \ Yeti DNS testbed\n   only serves clients that are explicitly configured to participate\
    \ in\n   the experiment, whereas the Root Server system serves the whole\n   Internet.\
    \  Since the dependent end-users and systems of the Yeti DNS\n   testbed are known\
    \ and their operations well-coordinated with those of\n   the Yeti project, it\
    \ has been possible to deploy structural changes\n   in the Yeti DNS testbed with\
    \ effective measurement and analysis,\n   something that is difficult or simply\
    \ impractical in the production\n   Root Server system.\n   This document describes\
    \ the motivation for the Yeti project,\n   describes the Yeti testbed infrastructure,\
    \ and provides the technical\n   and operational experiences of some users of\
    \ the Yeti testbed.  This\n   document neither addresses the relevant policies\
    \ under which the Root\n   Server system is operated nor makes any proposal for\
    \ changing any\n   aspect of its implementation or operation.\n"
- title: 2.  Requirements Notation and Conventions
  contents:
  - "2.  Requirements Notation and Conventions\n   Through the document, any mention\
    \ of \"Root\" with an uppercase \"R\" and\n   without other prefix, refers to\
    \ the \"IANA Root\" systems used in the\n   production Internet.  Proper mentions\
    \ of the Yeti infrastructure will\n   be prefixed with \"Yeti\", like \"Yeti-Root\
    \ zone\", \"Yeti DNS\", and so\n   on.\n"
- title: 3.  Areas of Study
  contents:
  - "3.  Areas of Study\n   This section provides some examples of the topics that\
    \ the developers\n   of the Yeti DNS testbed considered important to address.\
    \  As noted in\n   Section 1, the Yeti DNS is an independently coordinated project\
    \ and\n   is not affiliated with the IETF, ICANN, IANA, or any Root Server\n \
    \  Operator.  Thus, the topics and areas for study were selected by (and\n   for)\
    \ the proponents of the Yeti project to address their own concerns\n   and in\
    \ the hope that the information and tools provided would be of\n   wider interest.\n\
    \   Each example included below is illustrated with indicative questions.\n"
- title: 3.1.  Implementation of a Testbed like the Root Server System
  contents:
  - "3.1.  Implementation of a Testbed like the Root Server System\n   o  How can\
    \ a testbed be constructed and deployed on the Internet,\n      allowing useful\
    \ public participation without any risk of\n      disruption of the Root Server\
    \ system?\n   o  How can representative traffic be introduced into such a testbed\n\
    \      such that insights into the impact of specific differences between\n  \
    \    the testbed and the Root Server system can be observed?\n"
- title: 3.2.  Yeti-Root Zone Distribution
  contents:
  - "3.2.  Yeti-Root Zone Distribution\n   o  What are the scaling properties of Yeti-Root\
    \ zone distribution as\n      the number of Yeti-Root servers, Yeti-Root server\
    \ instances, or\n      intermediate distribution points increases?\n"
- title: 3.3.  Yeti-Root Server Names and Addressing
  contents:
  - "3.3.  Yeti-Root Server Names and Addressing\n   o  What naming schemes other\
    \ than those closely analogous to the use\n      of ROOT-SERVERS.NET in the production\
    \ root zone are practical, and\n      what are their respective advantages and\
    \ disadvantages?\n   o  What are the risks and benefits of signing the zone that\
    \ contains\n      the names of the Yeti-Root servers?\n   o  What automatic mechanisms\
    \ might be useful to improve the rate at\n      which clients of Yeti-Root servers\
    \ are able to react to a Yeti-\n      Root server renumbering event?\n"
- title: 3.4.  IPv6-Only Yeti-Root Servers
  contents:
  - "3.4.  IPv6-Only Yeti-Root Servers\n   o  Are there negative operational effects\
    \ in the use of IPv6-only\n      Yeti-Root servers, compared to the use of servers\
    \ that are dual-\n      stack?\n   o  What effect does the IPv6 fragmentation\
    \ model have on the\n      operation of Yeti-Root servers, compared with that\
    \ of IPv4?\n"
- title: 3.5.  DNSSEC in the Yeti-Root Zone
  contents:
  - "3.5.  DNSSEC in the Yeti-Root Zone\n   o  Is it practical to sign the Yeti-Root\
    \ zone using multiple,\n      independently operated DNSSEC signers and multiple\
    \ corresponding\n      Zone Signing Keys (ZSKs)?\n   o  To what extent is [RFC5011]\
    \ (\"Automated Updates of DNS Security\n      (DNSSEC) Trust Anchors\") supported\
    \ by resolvers?\n   o  Does the KSK Rollover plan designed and in the process\
    \ of being\n      implemented by ICANN work as expected on the Yeti testbed?\n\
    \   o  What is the operational impact of using much larger RSA key sizes\n   \
    \   in the ZSKs used in a root?\n   o  What are the operational consequences of\
    \ choosing DNSSEC\n      algorithms other than RSA to sign a root?\n"
- title: 4.  Yeti DNS Testbed Infrastructure
  contents:
  - "4.  Yeti DNS Testbed Infrastructure\n   The purpose of the testbed is to allow\
    \ DNS queries from stub\n   resolvers, mediated by recursive resolvers, to be\
    \ delivered to Yeti-\n   Root servers, and for corresponding responses generated\
    \ on the Yeti-\n   Root servers to be returned, as illustrated in Figure 1.\n\
    \       ,----------.        ,-----------.        ,------------.\n       |   stub\
    \   +------> | recursive +------> | Yeti-Root  |\n       | resolver | <------+\
    \ resolver  | <------+ nameserver |\n       `----------'        `-----------'\
    \        `------------'\n          ^                   ^                    ^\n\
    \          |  appropriate      |  Yeti-Root hints;  |  Yeti-Root zone\n      \
    \    `- resolver         `- Yeti-Root trust   `- with DNSKEY RRset\n         \
    \    configured          anchor               signed by\n                    \
    \                                  Yeti-Root KSK\n                  Figure 1:\
    \ High-Level Testbed Components\n   To use the Yeti DNS testbed, a recursive resolver\
    \ must be configured\n   to use the Yeti-Root servers.  That configuration consists\
    \ of a list\n   of names and addresses for the Yeti-Root servers (often referred\
    \ to\n   as a \"hints file\") that replaces the corresponding hints used for the\n\
    \   production Root Server system (Appendix A).  If resolvers are\n   configured\
    \ to validate DNSSEC, then they also need to be configured\n   with a DNSSEC trust\
    \ anchor that corresponds to a KSK used in the Yeti\n   DNS Project, in place\
    \ of the normal trust anchor set used for the\n   Root Zone.\n   Since the Yeti\
    \ root(s) are signed with Yeti keys, rather than those\n   used by the IANA Root,\
    \ corresponding changes are needed in the\n   resolver trust anchors.  Corresponding\
    \ changes are required in the\n   Yeti-Root hints file Appendix A.  Those changes\
    \ would be properly\n   rejected as bogus by any validator using the production\
    \ Root Server\n   system's root zone trust anchor set.\n   Stub resolvers become\
    \ part of the Yeti DNS testbed by their use of\n   recursive resolvers that are\
    \ configured as described above.\n   The data flow from IANA to stub resolvers\
    \ through the Yeti testbed is\n   illustrated in Figure 2 and is described in\
    \ more detail in the\n   sections that follow.\n                             \
    \ ,----------------.\n                         ,-- / IANA Root Zone / ---.\n \
    \                        |  `----------------'     |\n                       \
    \  |            |            |\n                         |            |      \
    \      |       Root Zone\n ,--------------.    ,---V---.    ,---V---.    ,---V---.\n\
    \ | Yeti Traffic |    | BII   |    | WIDE  |    | TISF  |\n | Collection   | \
    \   |  DM   |    |  DM   |    |  DM   |\n `----+----+----'    `---+---'    `---+---'\
    \    `---+---'\n      |    |       ,-----'    ,-------'            `----.\n  \
    \    |    |       |          |                         |  Yeti-Root\n      ^ \
    \   ^       |          |                         |     Zone\n      |    |   ,---V---.\
    \  ,---V---.                 ,---V---.\n      |    `---+ Yeti  |  | Yeti  |  .\
    \ . . . . . .  | Yeti  |\n      |        | Root  |  | Root  |                \
    \ | Root  |\n      |        `---+---'  `---+---'                 `---+---'\n \
    \     |            |          |                         |    DNS\n      |    \
    \        |          |                         |  Response\n      |         ,--V----------V-------------------------V--.\n\
    \      `---------+              Yeti Resolvers              |\n              \
    \  `--------------------+---------------------'\n                            \
    \         |                       DNS\n                                     |\
    \                     Response\n                ,--------------------V---------------------.\n\
    \                |            Yeti Stub Resolvers           |\n              \
    \  `------------------------------------------'\n The three coordinators of the\
    \ Yeti DNS testbed:\n    BII : Beijing Internet Institute\n    WIDE: Widely Integrated\
    \ Distributed Environment Project\n    TISF: A collaborative engineering and security\
    \ project by Paul Vixie\n                        Figure 2: Testbed Data Flow\n\
    \   Note that the roots are not bound to Distribution Masters (DMs).  DMs\n  \
    \ update their zone on a schedule described in Section 4.1.  Each DM\n   that\
    \ updates the latest zone can notify all roots, so the zone\n   transfer can happen\
    \ between any DM and any root.\n"
- title: 4.1.  Root Zone Retrieval
  contents:
  - "4.1.  Root Zone Retrieval\n   The Yeti-Root zone is distributed within the Yeti\
    \ DNS testbed through\n   a set of internal master servers that are referred to\
    \ as Distribution\n   Masters (DMs).  These server elements distribute the Yeti-Root\
    \ zone\n   to all Yeti-Root servers.  The means by which the Yeti DMs construct\n\
    \   the Yeti-Root zone for distribution is described below.\n   Since Yeti DNS\
    \ DMs do not receive DNS NOTIFY [RFC1996] messages from\n   the Root Server system,\
    \ a polling approach is used to determine when\n   new revisions of the root zone\
    \ are available from the production Root\n   Server system.  Each Yeti DM requests\
    \ the Root Zone SOA record from a\n   Root server that permits unauthenticated\
    \ zone transfers of the root\n   zone, and performs a zone transfer from that\
    \ server if the retrieved\n   value of SOA.SERIAL is greater than that of the\
    \ last retrieved zone.\n   At the time of writing, unauthenticated zone transfers\
    \ of the Root\n   Zone are available directly from B-Root, C-Root, F-Root, G-Root,\n\
    \   K-Root, and L-Root; two servers XFR.CJR.DNS.ICANN.ORG and\n   XFR.LAX.DNS.ICANN.ORG;\
    \ and via FTP from sites maintained by the Root\n   Zone Maintainer and the IANA\
    \ Functions Operator.  The Yeti DNS\n   testbed retrieves the Root Zone using\
    \ zone transfers from F-Root.\n   The schedule on which F-Root is polled by each\
    \ Yeti DM is as follows:\n                  +-------------+-----------------------+\n\
    \                  | DM Operator | Time                  |\n                 \
    \ +-------------+-----------------------+\n                  | BII         | UTC\
    \ hour + 00 minutes |\n                  | WIDE        | UTC hour + 20 minutes\
    \ |\n                  | TISF        | UTC hour + 40 minutes |\n             \
    \     +-------------+-----------------------+\n   The Yeti DNS testbed uses multiple\
    \ DMs, each of which acts\n   autonomously and equivalently to its siblings. \
    \ Any single DM can act\n   to distribute new revisions of the Yeti-Root zone\
    \ and is also\n   responsible for signing the RRsets that are changed as part\
    \ of the\n   transformation of the Root Zone into the Yeti-Root zone described\
    \ in\n   Section 4.2.  This multiple DM model intends to provide a basic\n   structure\
    \ to implement the idea of shared zone control as proposed in\n   [ITI2014].\n"
- title: 4.2.  Transformation of Root Zone to Yeti-Root Zone
  contents:
  - "4.2.  Transformation of Root Zone to Yeti-Root Zone\n   Two distinct approaches\
    \ have been deployed in the Yeti DNS testbed,\n   separately, to transform the\
    \ Root Zone into the Yeti-Root zone.  At a\n   high level, the approaches are\
    \ equivalent in the sense that they\n   replace a minimal set of information in\
    \ the root zone with\n   corresponding data for the Yeti DNS testbed; the mechanisms\
    \ by which\n   the transforms are executed are different, however.  The approaches\n\
    \   are discussed in Sections 4.2.1 and 4.2.2.\n   A third approach has also been\
    \ proposed, but not yet implemented.\n   The motivations and changes implied by\
    \ that approach are described in\n   Section 4.2.3.\n"
- title: 4.2.1.  ZSK and KSK Key Sets Shared between DMs
  contents:
  - "4.2.1.  ZSK and KSK Key Sets Shared between DMs\n   The approach described here\
    \ was the first to be implemented.  It\n   features entirely autonomous operation\
    \ of each DM, but also requires\n   secret key material (the private key in each\
    \ of the Yeti-Root KSK and\n   ZSK key pairs) to be distributed and maintained\
    \ on each DM in a\n   coordinated way.\n   The Root Zone is transformed as follows\
    \ to produce the Yeti-Root\n   zone.  This transformation is carried out autonomously\
    \ on each Yeti\n   DNS Project DM.  Each DM carries an authentic copy of the current\
    \ set\n   of Yeti KSK and ZSK key pairs, synchronized between all DMs (see\n \
    \  Section 4.4).\n   1.  SOA.MNAME is set to www.yeti-dns.org.\n   2.  SOA.RNAME\
    \ is set to <dm-operator>.yeti-dns.org, where\n       <dm-operator> is currently\
    \ one of \"wide\", \"bii\", or \"tisf\".\n   3.  All DNSKEY, RRSIG, and NSEC records\
    \ are removed.\n   4.  The apex Name Server (NS) RRset is removed, with the\n\
    \       corresponding root server glue (A and AAAA) RRsets.\n   5.  A Yeti DNSKEY\
    \ RRset is added to the apex, comprising the public\n       parts of all Yeti\
    \ KSK and ZSKs.\n   6.  A Yeti NS RRset is added to the apex that includes all\
    \ Yeti-Root\n       servers.\n   7.  Glue records (AAAA only, since Yeti-Root\
    \ servers are v6-only) for\n       all Yeti-Root servers are added.\n   8.  The\
    \ Yeti-Root zone is signed: the NSEC chain is regenerated; the\n       Yeti KSK\
    \ is used to sign the DNSKEY RRset; and the shared ZSK is\n       used to sign\
    \ every other RRset.\n   Note that the SOA.SERIAL value published in the Yeti-Root\
    \ zone is\n   identical to that found in the root zone.\n"
- title: 4.2.2.  Unique ZSK per DM; No Shared KSK
  contents:
  - "4.2.2.  Unique ZSK per DM; No Shared KSK\n   The approach described here was\
    \ the second to be implemented and\n   maintained as stable state.  Each DM is\
    \ provisioned with its own,\n   dedicated ZSK key pairs that are not shared with\
    \ other DMs.  A Yeti-\n   Root DNSKEY RRset is constructed and signed upstream\
    \ of all DMs as\n   the union of the set of active Yeti-Root KSKs and the set\
    \ of active\n   ZSKs for every individual DM.  Each DM now only requires the secret\n\
    \   part of its own dedicated ZSK key pairs to be available locally, and\n   no\
    \ other secret key material is shared.  The high-level approach is\n   illustrated\
    \ in Figure 3.\n                            ,----------.         ,-----------.\n\
    \                   .--------> BII ZSK  +---------> Yeti-Root |\n            \
    \       | signs  `----------'  signs  `-----------'\n                   |\n  \
    \   ,-----------. |        ,----------.         ,-----------.\n     | Yeti KSK\
    \  +-+--------> TISF ZSK +---------> Yeti-Root |\n     `-----------' | signs \
    \ `----------'  signs  `-----------'\n                   |\n                 \
    \  |        ,----------.         ,-----------.\n                   `-------->\
    \ WIDE ZSK +---------> Yeti-Root |\n                     signs  `----------' \
    \ signs  `-----------'\n                        Figure 3: Unique ZSK per DM\n\
    \   The process of retrieving the Root Zone from the Root Server system\n   and\
    \ replacing and signing the apex DNSKEY RRset no longer takes place\n   on the\
    \ DMs; instead, it takes place on a central Hidden Master.  The\n   production\
    \ of signed DNSKEY RRsets is analogous to the use of Signed\n   Key Responses\
    \ (SKRs) produced during ICANN KSK key ceremonies\n   [ICANN2010].\n   Each DM\
    \ now retrieves source data (with a premodified and Yeti-signed\n   DNSKEY RRset,\
    \ but otherwise unchanged) from the Yeti DNS Hidden\n   Master instead of from\
    \ the Root Server system.\n   Each DM carries out a similar transformation to\
    \ that described in\n   Section 4.2.1, except that DMs no longer need to modify\
    \ or sign the\n   DNSKEY RRset, and the DM's unique local ZSK is used to sign\
    \ every\n   other RRset.\n"
- title: 4.2.3.  Preserving Root Zone NSEC Chain and ZSK RRSIGs
  contents:
  - "4.2.3.  Preserving Root Zone NSEC Chain and ZSK RRSIGs\n   A change to the transformation\
    \ described in Section 4.2.2 has been\n   proposed as a Yeti experiment called\
    \ PINZ [PINZ], which would\n   preserve the NSEC chain from the Root Zone and\
    \ all RRSIG RRs\n   generated using the Root Zone's ZSKs.  The DNSKEY RRset would\n\
    \   continue to be modified to replace the Root Zone KSKs, but Root Zone\n   ZSKs\
    \ would be kept intact, and the Yeti KSK would be used to generate\n   replacement\
    \ signatures over the apex DNSKEY and NS RRsets.  Source\n   data would continue\
    \ to flow from the Root Server system through the\n   Hidden Master to the set\
    \ of DMs, but no DNSSEC operations would be\n   required on the DMs, and the source\
    \ NSEC and most RRSIGs would remain\n   intact.\n   This approach has been suggested\
    \ in order to keep minimal changes\n   from the IANA Root zone and provide cryptographically\
    \ verifiable\n   confidence that no owner name in the root zone had been changed\
    \ in\n   the process of producing the Yeti-Root zone from the Root Zone,\n   thereby\
    \ addressing one of the concerns described in Appendix E in a\n   way that can\
    \ be verified automatically.\n"
- title: 4.3.  Yeti-Root Zone Distribution
  contents:
  - "4.3.  Yeti-Root Zone Distribution\n   Each Yeti DM is configured with a full\
    \ list of Yeti-Root server\n   addresses to send NOTIFY [RFC1996] messages to.\
    \  This also forms the\n   basis for an address-based access-control list for\
    \ zone transfers.\n   Authentication by address could be replaced with more rigorous\n\
    \   mechanisms (e.g., using Transaction Signatures (TSIGs) [RFC2845]).\n   This\
    \ has not been done at the time of writing since the use of\n   address-based\
    \ controls avoids the need for the distribution of shared\n   secrets amongst\
    \ the Yeti-Root server operators.\n   Individual Yeti-Root servers are configured\
    \ with a full set of Yeti\n   DM addresses to which SOA and AXFR queries may be\
    \ sent in the\n   conventional manner.\n"
- title: 4.4.  Synchronization of Service Metadata
  contents:
  - "4.4.  Synchronization of Service Metadata\n   Changes in the Yeti DNS testbed\
    \ infrastructure such as the addition\n   or removal of Yeti-Root servers, renumbering\
    \ Yeti-Root servers, or\n   DNSSEC key rollovers require coordinated changes to\
    \ take place on all\n   DMs.  The Yeti DNS testbed is subject to more frequent\
    \ changes than\n   are observed in the Root Server system and includes substantially\n\
    \   more Yeti-Root servers than there are IANA Root Servers, and hence a\n   manual\
    \ change process in the Yeti testbed would be more likely to\n   suffer from human\
    \ error.  An automated and cooperative process was\n   consequently implemented.\n\
    \   The theory of this operation is that each DM operator runs a Git\n   repository\
    \ locally, containing all service metadata involved in the\n   operation of each\
    \ DM.  When a change is desired and approved among\n   all Yeti coordinators,\
    \ one DM operator (usually BII) updates the\n   local Git repository.  A serial\
    \ number in the future (in two days) is\n   chosen for when the changes become\
    \ active.  The DM operator then\n   pushes the changes to the Git repositories\
    \ of the other two DM\n   operators who have a chance to check and edit the changes.\
    \  When the\n   serial number of the root zone passes the number chosen, the changes\n\
    \   are pulled automatically to individual DMs and promoted to\n   production.\n\
    \   The three Git repositories are synchronized by configuring them as\n   remote\
    \ servers.  For example, at BII we push to all three DMs'\n   repositories as\
    \ follows:\n             $ git remote -v\n             origin yeticonf@yeti-conf.dns-lab.net:dm\
    \ (fetch)\n             origin yeticonf@yeti-conf.dns-lab.net:dm (push)\n    \
    \         origin yeticonf@yeti-dns.tisf.net:dm (push)\n             origin yeticonf@yeti-repository.wide.ad.jp:dm\
    \ (push)\n   For more detailed information on DM synchronization, please see this\n\
    \   document in Yeti's GitHub repository: <https://github.com/BII-Lab/\n   Yeti-Project/blob/master/doc/Yeti-DM-Sync.md>.\n"
- title: 4.5.  Yeti-Root Server Naming Scheme
  contents:
  - "4.5.  Yeti-Root Server Naming Scheme\n   The current naming scheme for Root Servers\
    \ was normalized to use\n   single-character host names (\"A\" through \"M\")\
    \ under the domain ROOT-\n   SERVERS.NET, as described in [RSSAC023].  The principal\
    \ benefit of\n   this naming scheme was that DNS label compression could be used\
    \ to\n   produce a priming response that would fit within 512 bytes at the\n \
    \  time it was introduced, where 512 bytes is the maximum DNS message\n   size\
    \ using UDP transport without EDNS(0) [RFC6891].\n   Yeti-Root servers do not\
    \ use this optimization, but rather use free-\n   form nameserver names chosen\
    \ by their respective operators -- in\n   other words, no attempt is made to minimize\
    \ the size of the priming\n   response through the use of label compression. \
    \ This approach aims to\n   challenge the need to minimize the priming response\
    \ in a modern DNS\n   ecosystem where EDNS(0) is prevalent.\n   Priming responses\
    \ from Yeti-Root servers (unlike those from Root\n   Servers) do not always include\
    \ server addresses in the additional\n   section.  In particular, Yeti-Root servers\
    \ running BIND9 return an\n   empty additional section if the configuration parameter\
    \ \"minimum-\n   responses\" is set, forcing resolvers to complete the priming\
    \ process\n   with a set of conventional recursive lookups in order to resolve\n\
    \   addresses for each Yeti-Root server.  The Yeti-Root servers running\n   NSD\
    \ were observed to return a fully populated additional section\n   (depending,\
    \ of course, on the EDNS buffer size in use).\n   Various approaches to normalize\
    \ the composition of the priming\n   response were considered, including:\n  \
    \ o  Require use of DNS implementations that exhibit a desired behavior\n    \
    \  in the priming response.\n   o  Modify nameserver software or configuration\
    \ as used by Yeti-Root\n      servers.\n   o  Isolate the names of Yeti-Root servers\
    \ in one or more zones that\n      could be slaved on each Yeti-Root server, renaming\
    \ servers as\n      necessary, giving each a source of authoritative data with\
    \ which\n      the authority section of a priming response could be fully\n  \
    \    populated.  This is the approach used in the Root Server system\n      with\
    \ the ROOT-SERVERS.NET zone.\n   The potential mitigation of renaming all Yeti-Root\
    \ servers using a\n   scheme that would allow their names to exist directly in\
    \ the root\n   zone was not considered because that approach implies the invention\n\
    \   of new top-level labels not present in the Root Zone.\n   Given the relative\
    \ infrequency of priming queries by individual\n   resolvers and the additional\
    \ complexity or other compromises implied\n   by each of those mitigations, the\
    \ decision was made to make no effort\n   to ensure that the composition of priming\
    \ responses was identical\n   across servers.  Even the empty additional sections\
    \ generated by\n   Yeti-Root servers running BIND9 seem to be sufficient for all\n\
    \   resolver software tested; resolvers simply perform a new recursive\n   lookup\
    \ for each authoritative server name they need to resolve.\n"
- title: 4.6.  Yeti-Root Servers
  contents:
  - "4.6.  Yeti-Root Servers\n   Various volunteers have donated authoritative servers\
    \ to act as Yeti-\n   Root servers.  At the time of writing, there are 25 Yeti-Root\
    \ servers\n   distributed globally, one of which is named using a label as\n \
    \  specified in IDNA2008 [RFC5890] (it is shown in the following list in\n   punycode).\n\
    \   +-------------------------------------+---------------+-------------+\n  \
    \ | Name                                | Operator      | Location    |\n   +-------------------------------------+---------------+-------------+\n\
    \   | bii.dns-lab.net                     | BII           | CHINA       |\n  \
    \ | yeti-ns.tsif.net                    | TSIF          | USA         |\n   |\
    \ yeti-ns.wide.ad.jp                  | WIDE Project  | Japan       |\n   | yeti-ns.as59715.net\
    \                 | as59715       | Italy       |\n   | dahu1.yeti.eu.org    \
    \               | Dahu Group    | France      |\n   | ns-yeti.bondis.org     \
    \             | Bond Internet | Spain       |\n   |                          \
    \           | Systems       |             |\n   | yeti-ns.ix.ru              \
    \         | Russia        | MSK-IX      |\n   | yeti.bofh.priv.at            \
    \       | CERT Austria  | Austria     |\n   | yeti.ipv6.ernet.in             \
    \     | ERNET India   | India       |\n   | yeti-dns01.dnsworkshop.org       \
    \   | dnsworkshop   | Germany     |\n   |                                    \
    \ | /informnis    |             |\n   | dahu2.yeti.eu.org                   |\
    \ Dahu Group    | France      |\n   | yeti.aquaray.com                    | Aqua\
    \ Ray SAS  | France      |\n   | yeti-ns.switch.ch                   | SWITCH\
    \        | Switzerland |\n   | yeti-ns.lab.nic.cl                  | NIC Chile\
    \     | Chile       |\n   | yeti-ns1.dns-lab.net                | BII        \
    \   | China       |\n   | yeti-ns2.dns-lab.net                | BII          \
    \ | China       |\n   | yeti-ns3.dns-lab.net                | BII           |\
    \ China       |\n   | ca...a23dc.yeti-dns.net             | Yeti-ZA       | South\
    \       |\n   |                                     |               | Africa \
    \     |\n   | 3f...374cd.yeti-dns.net             | Yeti-AU       | Australia\
    \   |\n   | yeti1.ipv6.ernet.in                 | ERNET India   | India      \
    \ |\n   | xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c | ERNET India   | India       |\n\
    \   | yeti-dns02.dnsworkshop.org          | dnsworkshop   | USA         |\n  \
    \ |                                     | /informnis    |             |\n   |\
    \ yeti.mind-dns.nl                    | Monshouwer    | Netherlands |\n   |  \
    \                                   | Internet      |             |\n   |    \
    \                                 | Diensten      |             |\n   | yeti-ns.datev.net\
    \                   | DATEV         | Germany     |\n   | yeti.jhcloos.net.  \
    \                 | jhcloos       | USA         |\n   +-------------------------------------+---------------+-------------+\n\
    \   The current list of Yeti-Root servers is made available to a\n   participating\
    \ resolver first using a substitute hints file Appendix A\n   and subsequently\
    \ by the usual resolver priming process [RFC8109].\n   All Yeti-Root servers are\
    \ IPv6-only, because of the IPv6-only\n   Internet of the foreseeable future,\
    \ and hence the Yeti-Root hints\n   file contains no IPv4 addresses and the Yeti-Root\
    \ zone contains no\n   IPv4 glue records.  Note that the rationale of an IPv6-only\
    \ testbed\n   is to test whether an IPv6-only root can survive any problem or\n\
    \   impact when IPv4 is turned off, much like the context of the IETF\n   SUNSET4\
    \ WG [SUNSET4].\n   At the time of writing, all root servers within the Root Server\n\
    \   system serve the ROOT-SERVERS.NET zone in addition to the root zone,\n   and\
    \ all but one also serve the ARPA zone.  Yeti-Root servers serve\n   the Yeti-Root\
    \ zone only.\n   Significant software diversity exists across the set of Yeti-Root\n\
    \   servers, as reported by their volunteer operators at the time of\n   writing:\n\
    \   o  Platform: 18 of 25 Yeti-Root servers are implemented on a Virtual\n   \
    \   Private Server (VPS) rather than bare metal.\n   o  Operating System: 15 Yeti-Root\
    \ servers run on Linux (Ubuntu,\n      Debian, CentOS, Red Hat, and ArchLinux);\
    \ 4 run on FreeBSD; 1 on\n      NetBSD; and 1 on Windows Server 2016.\n   o  DNS\
    \ software: 16 of 25 Yeti-Root servers use BIND9 (versions\n      varying between\
    \ 9.9.7 and 9.10.3); 4 use NSD (4.10 and 4.15); 2\n      use Knot (2.0.1 and 2.1.0);\
    \ 1 uses Bundy (1.2.0); 1 uses PowerDNS\n      (4.1.3); and 1 uses MS DNS (10.0.14300.1000).\n"
- title: 4.7.  Experimental Traffic
  contents:
  - "4.7.  Experimental Traffic\n   For the Yeti DNS testbed to be useful as a platform\
    \ for\n   experimentation, it needs to carry statistically representative\n  \
    \ traffic.  Several approaches have been taken to load the system with\n   traffic,\
    \ including both real-world traffic triggered by end-users and\n   synthetic traffic.\n\
    \   Resolvers that have been explicitly configured to participate in the\n   testbed,\
    \ as described in Section 4, are a source of real-world, end-\n   user traffic.\
    \  Due to an efficient cache mechanism, the mean query\n   rate is less than 100\
    \ qps in the Yeti testbed, but a variety of\n   sources were observed as active\
    \ during 2017, as summarized in\n   Appendix C.\n   Synthetic traffic has been\
    \ introduced to the system from time to time\n   in order to increase traffic\
    \ loads.  Approaches include the use of\n   distributed measurement platforms\
    \ such as RIPE ATLAS to send DNS\n   queries to Yeti-Root servers and the capture\
    \ of traffic (sent from\n   non-Yeti resolvers to the Root Server system) that\
    \ was subsequently\n   modified and replayed towards Yeti-Root servers.\n"
- title: 4.8.  Traffic Capture and Analysis
  contents:
  - "4.8.  Traffic Capture and Analysis\n   Traffic capture of queries and responses\
    \ is available in the testbed\n   in both Yeti resolvers and Yeti-Root servers\
    \ in anticipation of\n   experiments that require packet-level visibility into\
    \ DNS traffic.\n   Traffic capture is performed on Yeti-Root servers using either\n\
    \   o  dnscap <https://www.dns-oarc.net/tools/dnscap> or\n   o  pcapdump, part\
    \ of the pcaputils Debian package\n      <https://packages.debian.org/sid/pcaputils>,\
    \ with a patch to\n      facilitate triggered file upload (see <https://bugs.debian.org/\n\
    \      cgi-bin/bugreport.cgi?bug=545985>).\n   PCAP-format files containing packet\
    \ captures are uploaded using rsync\n   to central storage.\n"
- title: 5.  Operational Experience with the Yeti DNS Testbed
  contents:
  - "5.  Operational Experience with the Yeti DNS Testbed\n   The following sections\
    \ provide commentary on the operation and impact\n   analyses of the Yeti DNS\
    \ testbed described in Section 4.  More\n   detailed descriptions of observed\
    \ phenomena are available in the Yeti\n   DNS mailing list archives <http://lists.yeti-dns.org/pipermail/\n\
    \   discuss/> and on the Yeti DNS blog <https://yeti-dns.org/blog.html>.\n"
- title: 5.1.  Viability of IPv6-Only Operation
  contents:
  - "5.1.  Viability of IPv6-Only Operation\n   All Yeti-Root servers were deployed\
    \ with IPv6 connectivity, and no\n   IPv4 addresses for any Yeti-Root server were\
    \ made available (e.g., in\n   the Yeti hints file or in the DNS itself).  This\
    \ implementation\n   decision constrained the Yeti-Root system to be v6 only.\n\
    \   DNS implementations are generally adept at using both IPv4 and IPv6\n   when\
    \ both are available.  Servers that cannot be reliably reached\n   over one protocol\
    \ might be better queried over the other, to the\n   benefit of end-users in the\
    \ common case where DNS resolution is on\n   the critical path for end-users'\
    \ perception of performance.  However,\n   this optimization also means that systemic\
    \ problems with one protocol\n   can be masked by the other.  By forcing all traffic\
    \ to be carried\n   over IPv6, the Yeti DNS testbed aimed to expose any such problems\
    \ and\n   make them easier to identify and understand.  Several examples of\n\
    \   IPv6-specific phenomena observed during the operation of the testbed\n   are\
    \ described in the sections that follow.\n   Although the Yeti-Root servers themselves\
    \ were only reachable using\n   IPv6, real-world end-users often have no IPv6\
    \ connectivity.  The\n   testbed was also able to explore the degree to which\
    \ IPv6-only Yeti-\n   Root servers were able to serve single-stack, IPv4-only\
    \ end-user\n   populations through the use of dual-stack Yeti resolvers.\n"
- title: 5.1.1.  IPv6 Fragmentation
  contents:
  - "5.1.1.  IPv6 Fragmentation\n   In the Root Server system, structural changes\
    \ with the potential to\n   increase response sizes (and hence fragmentation,\
    \ fallback to TCP\n   transport, or both) have been exercised with great care,\
    \ since the\n   impact on clients has been difficult to predict or measure.  The\
    \ Yeti\n   DNS testbed is experimental and has the luxury of a known client\n\
    \   base, making it far easier to make such changes and measure their\n   impact.\n\
    \   Many of the experimental design choices described in this document\n   were\
    \ expected to trigger larger responses.  For example, the choice\n   of naming\
    \ scheme for Yeti-Root servers described in Section 4.5\n   defeats label compression.\
    \  It makes a large priming response (up to\n   1754 octets with 25 NS records\
    \ and their corresponding glue records);\n   the Yeti-Root zone transformation\
    \ approach described in Section 4.2.2\n   greatly enlarges the apex DNSKEY RRset\
    \ especially during the KSK\n   rollover (up to 1975 octets with 3 ZSKs and 2\
    \ KSKs).  Therefore, an\n   increased incidence of fragmentation was expected.\n\
    \   The Yeti DNS testbed provides service on IPv6 only.  However,\n   middleboxes\
    \ (such as firewalls and some routers) are not friendly on\n   IPv6 fragments.\
    \  There are reports of a notable packet drop rate due\n   to the mistreatment\
    \ of middleboxes on IPv6 fragments [FRAGDROP]\n   [RFC7872].  One APNIC study\
    \ [IPv6-frag-DNS] reported that 37% of\n   endpoints using IPv6-capable DNS resolvers\
    \ cannot receive a\n   fragmented IPv6 response over UDP.\n   To study the impact,\
    \ RIPE Atlas probes were used.  For each Yeti-Root\n   server, an Atlas measurement\
    \ was set up using 100 IPv6-enabled probes\n   from five regions, sending a DNS\
    \ query for \"./IN/DNSKEY\" using UDP\n   transport with DO=1.  This measurement,\
    \ when carried out concurrently\n   with a Yeti KSK rollover, further exacerbating\
    \ the potential for\n   fragmentation, identified a 7% failure rate compared with\
    \ a non-\n   fragmented control.  A failure rate of 2% was observed with response\n\
    \   sizes of 1414 octets, which was surprising given the expected\n   prevalence\
    \ of 1500-octet (Ethernet-framed) MTUs.\n   The consequences of fragmentation\
    \ were not limited to failures in\n   delivering DNS responses over UDP transport.\
    \  There were two cases\n   where a Yeti-Root server failed when using TCP to\
    \ transfer the Yeti-\n   Root zone from a DM.  DM log files revealed \"socket\
    \ is not connected\"\n   errors corresponding to zone transfer requests.  Further\n\
    \   experimentation revealed that combinations of NetBSD 6.1, NetBSD\n   7.0RC1,\
    \ FreeBSD 10.0, Debian 3.2, and VMWare ESXI 5.5 resulted in a\n   high TCP Maximum\
    \ Segment Size (MSS) value of 1440 octets being\n   negotiated between client\
    \ and server despite the presence of the\n   IPV6_USE_MIN_MTU socket option, as\
    \ described in [USE_MIN_MTU].  The\n   mismatch appears to cause outbound segments\
    \ of a size greater than\n   1280 octets to be dropped before sending.  Setting\
    \ the local TCP MSS\n   to 1220 octets (chosen as 1280 - 60, the size of the IPv6\
    \ TCP header\n   with no other extension headers) was observed to be a pragmatic\n\
    \   mitigation.\n"
- title: 5.1.2.  Serving IPv4-Only End-Users
  contents:
  - "5.1.2.  Serving IPv4-Only End-Users\n   Yeti resolvers have been successfully\
    \ used by real-world end-users\n   for general name resolution within a number\
    \ of participant\n   organizations, including resolution of names to IPv4 addresses\
    \ and\n   resolution by IPv4-only end-user devices.\n   Some participants, recognizing\
    \ the operational importance of\n   reliability in resolver infrastructure and\
    \ concerned about the\n   stability of their IPv6 connectivity, chose to deploy\
    \ Yeti resolvers\n   in parallel to conventional resolvers, making both available\
    \ to end-\n   users.  While the viability of this approach provides a useful data\n\
    \   point, end-users using Yeti resolvers exclusively provided a better\n   opportunity\
    \ to identify and understand any failures in the Yeti DNS\n   testbed infrastructure.\n\
    \   Resolvers deployed in IPv4-only environments were able to join the\n   Yeti\
    \ DNS testbed by way of upstream, dual-stack Yeti resolvers.  In\n   one case\
    \ (CERNET2), this was done by assigning IPv4 addresses to\n   Yeti-Root servers\
    \ and mapping them in dual-stack IVI translation\n   devices [RFC6219].\n"
- title: 5.2.  Zone Distribution
  contents:
  - "5.2.  Zone Distribution\n   The Yeti DNS testbed makes use of multiple DMs to\
    \ distribute the\n   Yeti-Root zone, an approach that would allow the number of\
    \ Yeti-Root\n   servers to scale to a higher number than could be supported by\
    \ a\n   single distribution source and that provided redundancy.  The use of\n\
    \   multiple DMs introduced some operational challenges, however, which\n   are\
    \ described in the following sections.\n"
- title: 5.2.1.  Zone Transfers
  contents:
  - "5.2.1.  Zone Transfers\n   Yeti-Root servers were configured to serve the Yeti-Root\
    \ zone as\n   slaves.  Each slave had all DMs configured as masters, providing\n\
    \   redundancy in zone synchronization.\n   Each DM in the Yeti testbed served\
    \ a Yeti-Root zone that was\n   functionally equivalent but not congruent to that\
    \ served by every\n   other DM (see Section 4.3).  The differences included variations\
    \ in\n   the SOA.MNAME field and, more critically, in the RRSIGs for\n   everything\
    \ other than the apex DNSKEY RRset, since signatures for all\n   other RRsets\
    \ are generated using a private key that is only available\n   to the DM serving\
    \ its particular variant of the zone (see Sections\n   4.2.1 and 4.2.2).\n   Incremental\
    \ Zone Transfer (IXFR), as described in [RFC1995], is a\n   viable mechanism to\
    \ use for zone synchronization between any Yeti-\n   Root server and a consistent,\
    \ single DM.  However, if that Yeti-Root\n   server ever selected a different\
    \ DM, IXFR would no longer be a safe\n   mechanism; structural changes between\
    \ the incongruent zones on\n   different DMs would not be included in any transferred\
    \ delta, and the\n   result would be a zone that was not internally self-consistent.\
    \  For\n   this reason, the first transfer after a change of DM would require\n\
    \   AXFR not IXFR.\n   None of the DNS software in use on Yeti-Root servers supports\
    \ this\n   mixture of IXFR/AXFR according to the master server in use.  This is\n\
    \   unsurprising, given that the environment described above in the Yeti-\n  \
    \ Root system is idiosyncratic; conventional zone transfer graphs\n   involve\
    \ zones that are congruent between all nodes.  For this reason,\n   all Yeti-Root\
    \ servers are configured to use AXFR at all times, and\n   never IXFR, to ensure\
    \ that zones being served are internally self-\n   consistent.\n"
- title: 5.2.2.  Delays in Yeti-Root Zone Distribution
  contents:
  - "5.2.2.  Delays in Yeti-Root Zone Distribution\n   Each Yeti DM polled the Root\
    \ Server system for a new revision of the\n   root zone on an interleaved schedule,\
    \ as described in Section 4.1.\n   Consequently, different DMs were expected to\
    \ retrieve each revision\n   of the root zone, and make a corresponding revision\
    \ of the Yeti-Root\n   zone available, at different times.  The availability of\
    \ a new\n   revision of the Yeti-Root zone on the first DM would typically\n \
    \  precede that of the last by 40 minutes.\n   Given this distribution mechanism,\
    \ it might be expected that the\n   maximum latency between the publication of\
    \ a new revision of the root\n   zone and the availability of the corresponding\
    \ Yeti-Root zone on any\n   Yeti-Root server would be 20 minutes, since in normal\
    \ operation at\n   least one DM should serve that Yeti-Zone within 20 minutes\
    \ of root\n   zone publication.  In practice, this was not observed.\n   In one\
    \ case, a Yeti-Root server running Bundy 1.2.0 on FreeBSD\n   10.2-RELEASE was\
    \ found to lag root zone publication by as much as ten\n   hours.  Upon investigation,\
    \ this was found to be due to software\n   defects that were subsequently corrected.\n\
    \   More generally, Yeti-Root servers were observed routinely to lag root\n  \
    \ zone publication by more than 20 minutes, and relatively often by\n   more than\
    \ 40 minutes.  Whilst in some cases this might be assumed to\n   be a result of\
    \ connectivity problems, perhaps suppressing the\n   delivery of NOTIFY messages,\
    \ it was also observed that Yeti-Root\n   servers receiving a NOTIFY from one\
    \ DM would often send SOA queries\n   and AXFR requests to a different DM.  If\
    \ that DM were not yet serving\n   the new revision of the Yeti-Root zone, a delay\
    \ in updating the Yeti-\n   Root server would naturally result.\n"
- title: 5.2.3.  Mixed RRSIGs from Different DM ZSKs
  contents:
  - "5.2.3.  Mixed RRSIGs from Different DM ZSKs\n   The second approach for doing\
    \ the transformation of Root Zone to\n   Yeti-Root zone (Section 4.2.2) introduces\
    \ a situation where mixed\n   RRSIGs from different DM ZSKs are cached in one\
    \ resolver.\n   It is observed that the Yeti-Root zone served by any particular\
    \ Yeti-\n   Root server will include signatures generated using the ZSK from the\n\
    \   DM that served the Yeti-Root zone to that Yeti-Root server.\n   Signatures\
    \ cached at resolvers might be retrieved from any Yeti-Root\n   server, and hence\
    \ are expected to be a mixture of signatures\n   generated by different ZSKs.\
    \  Since all ZSKs can be trusted through\n   the signature by the Yeti KSK over\
    \ the DNSKEY RRset, which includes\n   all ZSKs, the mixture of signatures was\
    \ predicted not to be a threat\n   to reliable validation.\n   It was first tested\
    \ in BII's lab environment as a proof of concept.\n   It was observed in the resolver's\
    \ DNSSEC log that the process of\n   verifying an RDATA set shows \"success\"\
    \ with a key (keyid) in the\n   DNSKEY RRset.  It was implemented later in three\
    \ DMs that were\n   carefully coordinated and made public to all Yeti resolver\
    \ operators\n   and participants in Yeti's mailing list.  At least 45 Yeti resolvers\n\
    \   (deployed by Yeti operators) were being monitored and had set a\n   reporting\
    \ trigger if anything was wrong.  In addition, the Yeti\n   mailing list is open\
    \ for error reports from other participants.  So\n   far, the Yeti testbed has\
    \ been operated in this configuration (with\n   multiple ZSKs) for 2 years.  This\
    \ configuration has proven workable\n   and reliable, even when rollovers of individual\
    \ ZSKs are on different\n   schedules.\n   Another consequence of this approach\
    \ is that the apex DNSKEY RRset in\n   the Yeti-Root zone is much larger than\
    \ the corresponding DNSKEY RRset\n   in the Root Zone.  This requires more space\
    \ and produces a larger\n   response to the query for the DNSKEY RRset especially\
    \ during the KSK\n   rollover.\n"
- title: 5.3.  DNSSEC KSK Rollover
  contents:
  - "5.3.  DNSSEC KSK Rollover\n   At the time of writing, the Root Zone KSK is expected\
    \ to undergo a\n   carefully orchestrated rollover as described in [ICANN2016].\
    \  ICANN\n   has commissioned various tests and has published an external test\n\
    \   plan [ICANN2017].\n   Three related DNSSEC KSK rollover exercises were carried\
    \ out on the\n   Yeti DNS testbed, somewhat concurrent with the planning and execution\n\
    \   of the rollover in the root zone.  Brief descriptions of these\n   exercises\
    \ are included below.\n"
- title: 5.3.1.  Failure-Case KSK Rollover
  contents:
  - "5.3.1.  Failure-Case KSK Rollover\n   The first KSK rollover that was executed\
    \ on the Yeti DNS testbed\n   deliberately ignored the 30-day hold-down timer\
    \ specified in\n   [RFC5011] before retiring the outgoing KSK.\n   It was confirmed\
    \ that clients of some (but not all) validating Yeti\n   resolvers experienced\
    \ resolution failures (received SERVFAIL\n   responses) following this change.\
    \  Those resolvers required\n   administrator intervention to install a functional\
    \ trust anchor\n   before resolution was restored.\n"
- title: 5.3.2.  KSK Rollover vs. BIND9 Views
  contents:
  - "5.3.2.  KSK Rollover vs. BIND9 Views\n   The second Yeti KSK rollover was designed\
    \ with similar phases to the\n   ICANN's KSK rollover, although with modified\
    \ timings to reduce the\n   time required to complete the process.  The \"slot\"\
    \ used in this\n   rollover was ten days long, as follows:\n              +-----------------+----------------+----------+\n\
    \              |                 | Old Key: 19444 | New Key  |\n             \
    \ +-----------------+----------------+----------+\n              | slot 1    \
    \      | pub+sign       |          |\n              | slot 2, 3, 4, 5 | pub+sign\
    \       | pub      |\n              | slot 6, 7       | pub            | pub+sign\
    \ |\n              | slot 8          | revoke         | pub+sign |\n         \
    \     | slot 9          |                | pub+sign |\n              +-----------------+----------------+----------+\n\
    \   During this rollover exercise, a problem was observed on one Yeti\n   resolver\
    \ that was running BIND 9.10.4-p2 [KROLL-ISSUE].  That\n   resolver was configured\
    \ with multiple views serving clients in\n   different subnets at the time that\
    \ the KSK rollover began.  DNSSEC\n   validation failures were observed following\
    \ the completion of the KSK\n   rollover, triggered by the addition of a new view\
    \ that was intended\n   to serve clients from a new subnet.\n   BIND 9.10 requires\
    \ \"managed-keys\" configuration to be specified in\n   every view, a detail that\
    \ was apparently not obvious to the operator\n   in this case and that was subsequently\
    \ highlighted by the Internet\n   Systems Consortium (ISC) in their general advice\
    \ relating to KSK\n   rollover in the root zone to users of BIND 9 [ISC-BIND].\
    \  When the\n   \"managed-keys\" configuration is present in every view that is\n\
    \   configured to perform validation, trust anchors for all views are\n   updated\
    \ during a KSK rollover.\n"
- title: 5.3.3.  Large Responses during KSK Rollover
  contents:
  - "5.3.3.  Large Responses during KSK Rollover\n   Since a KSK rollover necessarily\
    \ involves the publication of outgoing\n   and incoming public keys simultaneously,\
    \ an increase in the size of\n   DNSKEY responses is expected.  The third KSK\
    \ rollover carried out on\n   the Yeti DNS testbed was accompanied by a concerted\
    \ effort to observe\n   response sizes and their impact on end-users.\n   As described\
    \ in Section 4.2.2, in the Yeti DNS testbed each DM can\n   maintain control of\
    \ its own set of ZSKs, which can undergo rollover\n   independently.  During a\
    \ KSK rollover where concurrent ZSK rollovers\n   are executed by each of three\
    \ DMs, the maximum number of apex DNSKEY\n   RRs present is eight (incoming and\
    \ outgoing KSK, plus incoming and\n   outgoing of each of three ZSKs).  In practice,\
    \ however, such\n   concurrency did not occur; only the BII ZSK was rolled during\
    \ the KSK\n   rollover, and hence only three DNSKEY RRset configurations were\n\
    \   observed:\n   o  3 ZSKs and 2 KSKs, DNSKEY response of 1975 octets;\n   o\
    \  3 ZSKs and 1 KSK, DNSKEY response of 1414 octets; and\n   o  2 ZSKs and 1 KSK,\
    \ DNSKEY response of 1139 octets.\n   RIPE Atlas probes were used as described\
    \ in Section 5.1.1 to send\n   DNSKEY queries directly to Yeti-Root servers. \
    \ The numbers of queries\n   and failures were recorded and categorized according\
    \ to the response\n   sizes at the time the queries were sent.  A summary of the\
    \ results\n   ([YetiLR]) is as follows:\n        +---------------+----------+---------------+--------------+\n\
    \        | Response Size | Failures | Total Queries | Failure Rate |\n       \
    \ +---------------+----------+---------------+--------------+\n        | 1139\
    \          | 274      | 64252         | 0.0042       |\n        | 1414       \
    \   | 3141     | 126951        | 0.0247       |\n        | 1975          | 2920\
    \     | 42529         | 0.0687       |\n        +---------------+----------+---------------+--------------+\n\
    \   The general approach illustrated briefly here provides a useful\n   example\
    \ of how the design of the Yeti DNS testbed, separate from the\n   Root Server\
    \ system but constructed as a live testbed on the Internet,\n   facilitates the\
    \ use of general-purpose active measurement facilities\n   (such as RIPE Atlas\
    \ probes) as well as internal passive measurement\n   (such as packet capture).\n"
- title: 5.4.  Capture of Large DNS Response
  contents:
  - "5.4.  Capture of Large DNS Response\n   Packet capture is a common approach in\
    \ production DNS systems where\n   operators require fine-grained insight into\
    \ traffic in order to\n   understand production traffic.  For authoritative servers,\
    \ capture of\n   inbound query traffic is often sufficient, since responses can\
    \ be\n   synthesized with knowledge of the zones being served at the time the\n\
    \   query was received.  Queries are generally small enough not to be\n   fragmented,\
    \ and even with TCP transport are generally packed within a\n   single segment.\n\
    \   The Yeti DNS testbed has different requirements; in particular, there\n  \
    \ is a desire to compare responses obtained from the Yeti\n   infrastructure with\
    \ those received from the Root Server system in\n   response to a single query\
    \ stream (e.g., using the \"Yeti Many Mirror\n   Verifier\" (YmmV) as described\
    \ in Appendix D).  Some Yeti-Root servers\n   were capable of recovering complete\
    \ DNS messages from within\n   nameservers, e.g., using dnstap; however, not all\
    \ servers provided\n   that functionality, and a consistent approach was desirable.\n\
    \   The requirement to perform passive capture of responses from the wire\n  \
    \ together with experiments that were expected (and in some cases\n   designed)\
    \ to trigger fragmentation and use of TCP transport led to\n   the development\
    \ of a new tool, PcapParser, to perform fragment and\n   TCP stream reassembly\
    \ from raw packet capture data.  A brief\n   description of PcapParser is included\
    \ in Appendix D.\n"
- title: 5.5.  Automated Maintenance of the Hints File
  contents:
  - "5.5.  Automated Maintenance of the Hints File\n   Renumbering events in the Root\
    \ Server system are relatively rare.\n   Although each such event is accompanied\
    \ by the publication of an\n   updated hints file in standard locations, the task\
    \ of updating local\n   copies of that file used by DNS resolvers is manual, and\
    \ the process\n   has an observably long tail.  For example, in 2015 J-Root was\
    \ still\n   receiving traffic at its old address some thirteen years after\n \
    \  renumbering [Wessels2015].\n   The observed impact of these old, deployed hints\
    \ files is minimal,\n   likely due to the very low frequency of such renumbering\
    \ events.\n   Even the oldest of hints files would still contain some accurate\
    \ root\n   server addresses from which priming responses could be obtained.\n\
    \   By contrast, due to the experimental nature of the system and the\n   fact\
    \ that it is operated mainly by volunteers, Yeti-Root servers are\n   added, removed,\
    \ and renumbered with much greater frequency.  A tool\n   to facilitate automatic\
    \ maintenance of hints files was therefore\n   created: [hintUpdate].\n   The\
    \ automated procedure followed by the hintUpdate tool is as\n   follows.\n   1.\
    \  Use the local resolver to obtain a response to the query\n       \"./IN/NS\"\
    .\n   2.  Use the local resolver to obtain a set of IPv4 and IPv6 addresses\n\
    \       for each name server.\n   3.  Validate all signatures obtained from the\
    \ local resolvers and\n       confirm that all data is signed.\n   4.  Compare\
    \ the data obtained to that contained within the currently\n       active hints\
    \ file; if there are differences, rotate the old one\n       away and replace\
    \ it with a new one.\n   This tool would not function unmodified when used in\
    \ the Root Server\n   system, since the names of individual Root Servers (e.g.,\
    \ A.ROOT-\n   SERVERS.NET) are not DNSSEC signed.  All Yeti-Root server names\
    \ are\n   DNSSEC signed, however, and hence this tool functions as expected in\n\
    \   that environment.\n"
- title: 5.6.  Root Label Compression in Knot DNS Server
  contents:
  - "5.6.  Root Label Compression in Knot DNS Server\n   [RFC1035] specifies that\
    \ domain names can be compressed when encoded\n   in DNS messages, and can be\
    \ represented as one of\n   1.  a sequence of labels ending in a zero octet;\n\
    \   2.  a pointer; or\n   3.  a sequence of labels ending with a pointer.\n  \
    \ The purpose of this flexibility is to reduce the size of domain names\n   encoded\
    \ in DNS messages.\n   It was observed that Yeti-Root servers running Knot 2.0\
    \ would\n   compress the zero-length label (the root domain, often represented\
    \ as\n   \".\") using a pointer to an earlier example.  Although legal, this\n\
    \   encoding increases the encoded size of the root label from one octet\n   to\
    \ two; it was also found to break some client software -- in\n   particular, the\
    \ Go DNS library.  Bug reports were filed against both\n   Knot and the Go DNS\
    \ library, and both were resolved in subsequent\n   releases.\n"
- title: 6.  Conclusions
  contents:
  - "6.  Conclusions\n   Yeti DNS was designed and implemented as a live DNS root\
    \ system\n   testbed.  It serves a root zone (\"Yeti-Root\" in this document)\n\
    \   derived from the root zone published by the IANA with only those\n   structural\
    \ modifications necessary to ensure its function in the\n   testbed system.  The\
    \ Yeti DNS testbed has proven to be a useful\n   platform to address many questions\
    \ that would be challenging to\n   answer using the production Root Server system,\
    \ such as those\n   included in Section 3.\n   Indicative findings following from\
    \ the construction and operation of\n   the Yeti DNS testbed include:\n   o  Operation\
    \ in a pure IPv6-only environment; confirmation of a\n      significant failure\
    \ rate in the transmission of large responses\n      (~7%), but no other persistent\
    \ failures observed.  Two cases in\n      which Yeti-Root servers failed to retrieve\
    \ the Yeti-Root zone due\n      to fragmentation of TCP segments; mitigated by\
    \ setting a TCP MSS\n      of 1220 octets (see Section 5.1.1).\n   o  Successful\
    \ operation with three autonomous Yeti-Root zone signers\n      and 25 Yeti-Root\
    \ servers, and confirmation that IXFR is not an\n      appropriate transfer mechanism\
    \ of zones that are structurally\n      incongruent across different transfer\
    \ paths (see Section 5.2).\n   o  ZSK size increased to 2048 bits and multiple\
    \ KSK rollovers\n      executed to exercise support of RFC 5011 in validating\
    \ resolvers;\n      identification of pitfalls relating to views in BIND9 when\n\
    \      configured with \"managed-keys\" (see Section 5.3).\n   o  Use of natural\
    \ (non-normalized) names for Yeti-Root servers\n      exposed some differences\
    \ between implementations in the inclusion\n      of additional-section glue in\
    \ responses to priming queries;\n      however, despite this inefficiency, Yeti\
    \ resolvers were observed\n      to function adequately (see Section 4.5).\n \
    \  o  It was observed that Knot 2.0 performed label compression on the\n     \
    \ root (empty) label.  This resulted in an increased encoding size\n      for\
    \ references to the root label, since a pointer is encoded as\n      two octets\
    \ whilst the root label itself only requires one (see\n      Section 5.6).\n \
    \  o  Some tools were developed in response to the operational\n      experience\
    \ of running and using the Yeti DNS testbed: DNS fragment\n      and DNS Additional\
    \ Truncated Response (ATR) for large DNS\n      responses, a BIND9 patch for additional-section\
    \ glue, YmmV, and\n      IPv6 defrag for capturing and mirroring traffic.  In\
    \ addition, a\n      tool to facilitate automatic maintenance of hints files was\n\
    \      created (see Appendix D).\n   The Yeti DNS testbed was used only by end-users\
    \ whose local\n   infrastructure providers had made the conscious decision to\
    \ do so, as\n   is appropriate for an experimental, non-production system.  So\
    \ far,\n   no serious user complaints have reached Yeti's mailing list during\n\
    \   Yeti normal operation.  Adding more instances into the Yeti root\n   system\
    \ may help to enhance the quality of service, but it is\n   generally accepted\
    \ that Yeti DNS performance is good enough to serve\n   the purpose of DNS Root\
    \ testbed.\n   The experience gained during the operation of the Yeti DNS testbed\n\
    \   suggested several topics worthy of further study:\n   o  Priming truncation\
    \ and TCP-only Yeti-Root servers: observe and\n      measure the worst-possible\
    \ case for priming truncation by\n      responding with TC=1 to all priming queries\
    \ received over UDP\n      transport, forcing clients to retry using TCP.  This\
    \ should also\n      give some insight into the usefulness of TCP-only DNS in\
    \ general.\n   o  KSK ECDSA Rollover: one possible way to reduce DNSKEY response\n\
    \      sizes is to change to an elliptic curve signing algorithm.  While\n   \
    \   in principle this can be done separately for the KSK and the ZSK,\n      the\
    \ RIPE NCC has done research recently and discovered that some\n      resolvers\
    \ require that both KSK and ZSK use the same algorithm.\n      This means that\
    \ an algorithm roll also involves a KSK roll.\n      Performing an algorithm roll\
    \ at the root would be an interesting\n      challenge.\n   o  Sticky Notify for\
    \ zone transfer: the non-applicability of IXFR as\n      a zone transfer mechanism\
    \ in the Yeti DNS testbed could be\n      mitigated by the implementation of a\
    \ sticky preference for master\n      server for each slave.  This would be so\
    \ that an initial AXFR\n      response could be followed up with IXFR requests\
    \ without\n      compromising zone integrity in the case (as with Yeti) that\n\
    \      equivalent but incongruent versions of a zone are served by\n      different\
    \ masters.\n   o  Key distribution for zone transfer credentials: the use of a\n\
    \      shared secret between slave and master requires key distribution\n    \
    \  and management whose scaling properties are not ideally suited to\n      systems\
    \ with large numbers of transfer clients.  Other approaches\n      for key distribution\
    \ and authentication could be considered.\n   o  DNS is a tree-based hierarchical\
    \ database.  Mathematically, it has\n      a root node and dependency between\
    \ parent and child nodes.  So,\n      any failures and instability of parent nodes\
    \ (Root in Yeti's case)\n      may impact their child nodes if there is a human\
    \ mistake, a\n      malicious attack, or even an earthquake.  It is proposed to\
    \ define\n      technology and practices to allow any organization, from the\n\
    \      smallest company to nations, to be self-sufficient in their DNS.\n   o\
    \  In Section 3.12 of [RFC8324], a \"Centrally Controlled Root\" is\n      viewed\
    \ as an issue of DNS.  In future work, it would be\n      interesting to test\
    \ some technical tools like blockchain [BC] to\n      either remove the technical\
    \ requirement for a central authority\n      over the root or enhance the security\
    \ and stability of the\n      existing Root.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   As introduced in Section 4.4, service metadata\
    \ is synchronized among\n   3 DMs using Git tool.  Any security issue around Git\
    \ may affect Yeti\n   DM operation.  For example, a hacker may compromise one\
    \ DM's Git\n   repository and push unwanted changes to the Yeti DM system; this\
    \ may\n   introduce a bad root server or bad key for a period of time.\n   The\
    \ Yeti resolver needs the bootstrapping files to join the testbed,\n   like the\
    \ hints file and trust anchor of Yeti.  All required\n   information is published\
    \ on <yeti-dns.org> and <github.com>.  If a\n   hacker tampers with those websites\
    \ by creating a fake page, a new\n   resolver may lose its way and be configured\
    \ with a bad root.\n   DNSSEC is an important research goal in the Yeti DNS testbed.\
    \  To\n   reduce the central function of DNSSEC for Root zone, we sign the\n \
    \  Yeti-Root zone using multiple, independently operated DNSSEC signers\n   and\
    \ multiple corresponding ZSKs (see Section 4.2).  To verify ICANN's\n   KSK rollover,\
    \ we rolled the Yeti KSK three times according to RFC\n   5011, and we do have\
    \ some observations (see Section 5.3).  In\n   addition, larger RSA key sizes\
    \ were used in the testbed before\n   2048-bit keys were used in the ZSK signing\
    \ process of the IANA Root\n   zone.\n"
- title: 8.  IANA Considerations
  contents:
  - "8.  IANA Considerations\n   This document has no IANA actions.\n"
- title: 9.  References
  contents:
  - '9.  References

    '
- title: 9.1.  Normative References
  contents:
  - "9.1.  Normative References\n   [RFC1034]  Mockapetris, P., \"Domain names - concepts\
    \ and facilities\",\n              STD 13, RFC 1034, DOI 10.17487/RFC1034, November\
    \ 1987,\n              <https://www.rfc-editor.org/info/rfc1034>.\n   [RFC1035]\
    \  Mockapetris, P., \"Domain names - implementation and\n              specification\"\
    , STD 13, RFC 1035, DOI 10.17487/RFC1035,\n              November 1987, <https://www.rfc-editor.org/info/rfc1035>.\n\
    \   [RFC1995]  Ohta, M., \"Incremental Zone Transfer in DNS\", RFC 1995,\n   \
    \           DOI 10.17487/RFC1995, August 1996,\n              <https://www.rfc-editor.org/info/rfc1995>.\n\
    \   [RFC1996]  Vixie, P., \"A Mechanism for Prompt Notification of Zone\n    \
    \          Changes (DNS NOTIFY)\", RFC 1996, DOI 10.17487/RFC1996,\n         \
    \     August 1996, <https://www.rfc-editor.org/info/rfc1996>.\n   [RFC5011]  StJohns,\
    \ M., \"Automated Updates of DNS Security (DNSSEC)\n              Trust Anchors\"\
    , STD 74, RFC 5011, DOI 10.17487/RFC5011,\n              September 2007, <https://www.rfc-editor.org/info/rfc5011>.\n\
    \   [RFC5890]  Klensin, J., \"Internationalized Domain Names for\n           \
    \   Applications (IDNA): Definitions and Document Framework\",\n             \
    \ RFC 5890, DOI 10.17487/RFC5890, August 2010,\n              <https://www.rfc-editor.org/info/rfc5890>.\n"
- title: 9.2.  Informative References
  contents:
  - "9.2.  Informative References\n   [ATR]      Song, L., \"ATR: Additional Truncation\
    \ Response for Large\n              DNS Response\", Work in Progress, draft-song-atr-large-\n\
    \              resp-02, August 2018.\n   [BC]       Wikipedia, \"Blockchain\"\
    , September 2018,\n              <https://en.wikipedia.org/w/\n              index.php?title=Blockchain&oldid=861681529>.\n\
    \   [FRAGDROP] Jaeggli, J., Colitti, L., Kumari, W., Vyncke, E., Kaeo,\n     \
    \         M., and T. Taylor, \"Why Operators Filter Fragments and\n          \
    \    What It Implies\", Work in Progress, draft-taylor-v6ops-\n              fragdrop-02,\
    \ December 2013.\n   [FRAGMENTS]\n              Sivaraman, M., Kerr, S., and D.\
    \ Song, \"DNS message\n              fragments\", Work in Progress, draft-muks-dns-message-\n\
    \              fragments-00, July 2015.\n   [hintUpdate]\n              \"Hintfile\
    \ Auto Update\", commit de428c0, October 2015,\n              <https://github.com/BII-Lab/Hintfile-Auto-Update>.\n\
    \   [HOW_ATR_WORKS]\n              Huston, G., \"How well does ATR actually work?\"\
    ,\n              APNIC blog, April 2018,\n              <https://blog.apnic.net/2018/04/16/\n\
    \              how-well-does-atr-actually-work/>.\n   [ICANN2010]\n          \
    \    Schlyter, J., Lamb, R., and R. Balasubramanian, \"DNSSEC\n              Key\
    \ Management Implementation for the Root Zone (DRAFT)\",\n              May 2010,\
    \ <http://www.root-dnssec.org/wp-content/\n              uploads/2010/05/draft-icann-dnssec-keymgmt-01.txt>.\n\
    \   [ICANN2016]\n              Design Team, \"Root Zone KSK Rollover Plan\", March\
    \ 2016,\n              <https://www.iana.org/reports/2016/\n              root-ksk-rollover-design-20160307.pdf>.\n\
    \   [ICANN2017]\n              ICANN, \"2017 KSK Rollover External Test Plan\"\
    , July 2016,\n              <https://www.icann.org/en/system/files/files/\n  \
    \            ksk-rollover-external-test-plan-22jul16-en.pdf>.\n   [IPv6-frag-DNS]\n\
    \              Huston, G., \"Dealing with IPv6 fragmentation in the DNS\",\n \
    \             APNIC blog, August 2017,\n              <https://blog.apnic.net/2017/08/22/\n\
    \              dealing-ipv6-fragmentation-dns>.\n   [ISC-BIND] Risk, V., \"2017\
    \ Root Key Rollover - What Does it Mean for\n              BIND Users?\", Internet\
    \ Systems Consortium, December 2016,\n              <https://www.isc.org/blogs/2017-root-key-rollover-what-\n\
    \              does-it-mean-for-bind-users/>.\n   [ISC-TN-2003-1]\n          \
    \    Abley, J., \"Hierarchical Anycast for Global Service\n              Distribution\"\
    , March 2003,\n              <http://ftp.isc.org/isc/pubs/tn/isc-tn-2003-1.txt>.\n\
    \   [ITI2014]  ICANN, \"Identifier Technology Innovation Report\", May\n     \
    \         2014, <https://www.icann.org/en/system/files/files/\n              iti-report-15may14-en.pdf>.\n\
    \   [KROLL-ISSUE]\n              Song, D., \"A DNSSEC issue during Yeti KSK rollover\"\
    , Yeti\n              DNS blog, October 2016, <http://yeti-dns.org/yeti/blog/\n\
    \              2016/10/26/A-DNSSEC-issue-during-Yeti-KSK-rollover.html>.\n   [PINZ]\
    \     Song, D., \"Yeti experiment plan for PINZ\", Yeti DNS blog,\n          \
    \    May 2018, <http://yeti-dns.org/yeti/blog/2018/05/01/\n              Experiment-plan-for-PINZ.html>.\n\
    \   [RFC2826]  Internet Architecture Board, \"IAB Technical Comment on the\n \
    \             Unique DNS Root\", RFC 2826, DOI 10.17487/RFC2826, May\n       \
    \       2000, <https://www.rfc-editor.org/info/rfc2826>.\n   [RFC2845]  Vixie,\
    \ P., Gudmundsson, O., Eastlake 3rd, D., and B.\n              Wellington, \"\
    Secret Key Transaction Authentication for DNS\n              (TSIG)\", RFC 2845,\
    \ DOI 10.17487/RFC2845, May 2000,\n              <https://www.rfc-editor.org/info/rfc2845>.\n\
    \   [RFC6219]  Li, X., Bao, C., Chen, M., Zhang, H., and J. Wu, \"The\n      \
    \        China Education and Research Network (CERNET) IVI\n              Translation\
    \ Design and Deployment for the IPv4/IPv6\n              Coexistence and Transition\"\
    , RFC 6219,\n              DOI 10.17487/RFC6219, May 2011,\n              <https://www.rfc-editor.org/info/rfc6219>.\n\
    \   [RFC6891]  Damas, J., Graff, M., and P. Vixie, \"Extension Mechanisms\n  \
    \            for DNS (EDNS(0))\", STD 75, RFC 6891,\n              DOI 10.17487/RFC6891,\
    \ April 2013,\n              <https://www.rfc-editor.org/info/rfc6891>.\n   [RFC7720]\
    \  Blanchet, M. and L-J. Liman, \"DNS Root Name Service\n              Protocol\
    \ and Deployment Requirements\", BCP 40, RFC 7720,\n              DOI 10.17487/RFC7720,\
    \ December 2015,\n              <https://www.rfc-editor.org/info/rfc7720>.\n \
    \  [RFC7872]  Gont, F., Linkova, J., Chown, T., and W. Liu,\n              \"\
    Observations on the Dropping of Packets with IPv6\n              Extension Headers\
    \ in the Real World\", RFC 7872,\n              DOI 10.17487/RFC7872, June 2016,\n\
    \              <https://www.rfc-editor.org/info/rfc7872>.\n   [RFC8109]  Koch,\
    \ P., Larson, M., and P. Hoffman, \"Initializing a DNS\n              Resolver\
    \ with Priming Queries\", BCP 209, RFC 8109,\n              DOI 10.17487/RFC8109,\
    \ March 2017,\n              <https://www.rfc-editor.org/info/rfc8109>.\n   [RFC8324]\
    \  Klensin, J., \"DNS Privacy, Authorization, Special Uses,\n              Encoding,\
    \ Characters, Matching, and Root Structure: Time\n              for Another Look?\"\
    , RFC 8324, DOI 10.17487/RFC8324,\n              February 2018, <https://www.rfc-editor.org/info/rfc8324>.\n\
    \   [RRL]      Vixie, P. and V. Schryver, \"Response Rate Limiting in the\n  \
    \            Domain Name System (DNS RRL)\", June 2012,\n              <http://www.redbarn.org/dns/ratelimits>.\n\
    \   [RSSAC001] Root Server System Advisory Committee (RSSAC), \"Service\n    \
    \          Expectations of Root Servers\", RSSAC001 Version 1,\n             \
    \ December 2015,\n              <https://www.icann.org/en/system/files/files/\n\
    \              rssac-001-root-service-expectations-04dec15-en.pdf>.\n   [RSSAC023]\
    \ Root Server System Advisory Committee (RSSAC), \"History of\n              the\
    \ Root Server System\", November 2016,\n              <https://www.icann.org/en/system/files/files/\n\
    \              rssac-023-04nov16-en.pdf>.\n   [SUNSET4]  IETF, \"Sunsetting IPv4\
    \ (sunset4) Concluded WG\",\n              <https://datatracker.ietf.org/wg/sunset4/about/>.\n\
    \   [TNO2009]  Gijsen, B., Jamakovic, A., and F. Roijers, \"Root Scaling\n   \
    \           Study: Description of the DNS Root Scaling Model\",\n            \
    \  TNO report, September 2009,\n              <https://www.icann.org/en/system/files/files/\n\
    \              root-scaling-model-description-29sep09-en.pdf>.\n   [USE_MIN_MTU]\n\
    \              Andrews, M., \"TCP Fails To Respect IPV6_USE_MIN_MTU\", Work\n\
    \              in Progress, draft-andrews-tcp-and-ipv6-use-minmtu-04,\n      \
    \        October 2015.\n   [Wessels2015]\n              Wessels, D., Castonguay,\
    \ J., and P. Barber, \"Thirteen\n              Years of 'Old J-Root'\", DNS-OARC\
    \ Fall 2015 Workshop,\n              October 2015, <https://indico.dns-oarc.net/event/24/\n\
    \              session/10/contribution/10/material/slides/0.pdf>.\n   [YetiLR]\
    \   \"Observation on Large response issue during Yeti KSK\n              rollover\"\
    , Yeti DNS blog, August 2017,\n              <https://yeti-dns.org/yeti/blog/2017/08/02/\n\
    \              large-packet-impact-during-yeti-ksk-rollover.html>.\n"
- title: Appendix A.  Yeti-Root Hints File
  contents:
  - "Appendix A.  Yeti-Root Hints File\n   The following hints file (complete and\
    \ accurate at the time of\n   writing) causes a DNS resolver to use the Yeti DNS\
    \ testbed in place\n   of the production Root Server system and hence participate\
    \ in\n   experiments running on the testbed.\n   Note that some lines have been\
    \ wrapped in the text that follows in\n   order to fit within the production constraints\
    \ of this document.\n   Wrapped lines are indicated with a blackslash character\
    \ (\"\\\"),\n   following common convention.\n   .                     3600000\
    \  IN   NS     bii.dns-lab.net\n   bii.dns-lab.net       3600000  IN   AAAA  \
    \ 240c:f:1:22::6\n   .                     3600000  IN   NS     yeti-ns.tisf.net\n\
    \   yeti-ns.tisf.net      3600000  IN   AAAA   2001:559:8000::6\n   .        \
    \             3600000  IN   NS     yeti-ns.wide.ad.jp\n   yeti-ns.wide.ad.jp \
    \   3600000  IN   AAAA   2001:200:1d9::35\n   .                     3600000  IN\
    \   NS     yeti-ns.as59715.net\n   yeti-ns.as59715.net   3600000  IN   AAAA  \
    \ \\\n                              2a02:cdc5:9715:0:185:5:203:53\n   .      \
    \               3600000  IN   NS     dahu1.yeti.eu.org\n   dahu1.yeti.eu.org \
    \    3600000  IN   AAAA   \\\n                              2001:4b98:dc2:45:216:3eff:fe4b:8c5b\n\
    \   .                     3600000  IN   NS     ns-yeti.bondis.org\n   ns-yeti.bondis.org\
    \    3600000  IN   AAAA   2a02:2810:0:405::250\n   .                     3600000\
    \  IN   NS     yeti-ns.ix.ru\n   yeti-ns.ix.ru         3600000  IN   AAAA   2001:6d0:6d06::53\n\
    \   .                     3600000  IN   NS     yeti.bofh.priv.at\n   yeti.bofh.priv.at\
    \     3600000  IN   AAAA   2a01:4f8:161:6106:1::10\n   .                     3600000\
    \  IN   NS     yeti.ipv6.ernet.in\n   yeti.ipv6.ernet.in    3600000  IN   AAAA\
    \   2001:e30:1c1e:1::333\n   .                     3600000  IN   NS     yeti-dns01.dnsworkshop.org\n\
    \   yeti-dns01.dnsworkshop.org \\\n                         3600000  IN   AAAA\
    \   2001:1608:10:167:32e::53\n   .                     3600000  IN   NS     yeti-ns.conit.co\n\
    \   yeti-ns.conit.co      3600000  IN   AAAA   \\\n                          \
    \   2604:6600:2000:11::4854:a010\n   .                     3600000  IN   NS  \
    \   dahu2.yeti.eu.org\n   dahu2.yeti.eu.org     3600000  IN   AAAA   2001:67c:217c:6::2\n\
    \   .                     3600000  IN   NS     yeti.aquaray.com\n   yeti.aquaray.com\
    \      3600000  IN   AAAA   2a02:ec0:200::1\n   .                     3600000\
    \  IN   NS     yeti-ns.switch.ch\n   yeti-ns.switch.ch     3600000  IN   AAAA\
    \   2001:620:0:ff::29\n   .                     3600000  IN   NS     yeti-ns.lab.nic.cl\n\
    \   yeti-ns.lab.nic.cl    3600000  IN   AAAA   2001:1398:1:21::8001\n   .    \
    \                 3600000  IN   NS     yeti-ns1.dns-lab.net\n   yeti-ns1.dns-lab.net\
    \  3600000  IN   AAAA   2001:da8:a3:a027::6\n   .                     3600000\
    \  IN   NS     yeti-ns2.dns-lab.net\n   yeti-ns2.dns-lab.net  3600000  IN   AAAA\
    \   2001:da8:268:4200::6\n   .                     3600000  IN   NS     yeti-ns3.dns-lab.net\n\
    \   yeti-ns3.dns-lab.net  3600000  IN   AAAA   2400:a980:30ff::6\n   .       \
    \              3600000  IN   NS     \\\n                           ca978112ca1bbdcafac231b39a23dc.yeti-dns.net\n\
    \   ca978112ca1bbdcafac231b39a23dc.yeti-dns.net \\\n                         3600000\
    \  IN   AAAA   2c0f:f530::6\n   .                     3600000  IN   NS     \\\n\
    \                           3e23e8160039594a33894f6564e1b1.yeti-dns.net\n   3e23e8160039594a33894f6564e1b1.yeti-dns.net\
    \ \\\n                         3600000  IN   AAAA   2803:80:1004:63::1\n   . \
    \                    3600000  IN   NS     \\\n                           3f79bb7b435b05321651daefd374cd.yeti-dns.net\n\
    \   3f79bb7b435b05321651daefd374cd.yeti-dns.net \\\n                         3600000\
    \  IN   AAAA   2401:c900:1401:3b:c::6\n   .                     3600000  IN  \
    \ NS     \\\n                           xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c\n\
    \   xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c \\\n                         3600000 \
    \ IN   AAAA   2001:e30:1c1e:10::333\n   .                     3600000  IN   NS\
    \     yeti1.ipv6.ernet.in\n   yeti1.ipv6.ernet.in   3600000  IN   AAAA   2001:e30:187d::333\n\
    \   .                     3600000  IN   NS     yeti-dns02.dnsworkshop.org\n  \
    \ yeti-dns02.dnsworkshop.org \\\n                         3600000  IN   AAAA \
    \  2001:19f0:0:1133::53\n   .                     3600000  IN   NS     yeti.mind-dns.nl\n\
    \   yeti.mind-dns.nl      3600000  IN   AAAA   2a02:990:100:b01::53:0\n"
- title: Appendix B.  Yeti-Root Server Priming Response
  contents:
  - "Appendix B.  Yeti-Root Server Priming Response\n   Here is the reply of a Yeti\
    \ root name server to a priming request.\n   The authoritative server runs NSD.\n\
    \   ...\n   ;; Got answer:\n   ;; ->>HEADER<<- opcode: QUERY, status: NOERROR,\
    \ id: 62391\n   ;; flags: qr aa rd; QUERY: 1, ANSWER: 26, AUTHORITY: 0, ADDITIONAL:\
    \ 7\n   ;; WARNING: recursion requested but not available\n   ;; OPT PSEUDOSECTION:\n\
    \   ; EDNS: version: 0, flags: do; udp: 1460\n   ;; QUESTION SECTION:\n   ;. \
    \                     IN NS\n   ;; ANSWER SECTION:\n   .            86400 IN NS\
    \ bii.dns-lab.net.\n   .            86400 IN NS yeti.bofh.priv.at.\n   .     \
    \       86400 IN NS yeti.ipv6.ernet.in.\n   .            86400 IN NS yeti.aquaray.com.\n\
    \   .            86400 IN NS yeti.jhcloos.net.\n   .            86400 IN NS yeti.mind-dns.nl.\n\
    \   .            86400 IN NS dahu1.yeti.eu.org.\n   .            86400 IN NS dahu2.yeti.eu.org.\n\
    \   .            86400 IN NS yeti1.ipv6.ernet.in.\n   .            86400 IN NS\
    \ ns-yeti.bondis.org.\n   .            86400 IN NS yeti-ns.ix.ru.\n   .      \
    \      86400 IN NS yeti-ns.lab.nic.cl.\n   .            86400 IN NS yeti-ns.tisf.net.\n\
    \   .            86400 IN NS yeti-ns.wide.ad.jp.\n   .            86400 IN NS\
    \ yeti-ns.datev.net.\n   .            86400 IN NS yeti-ns.switch.ch.\n   .   \
    \         86400 IN NS yeti-ns.as59715.net.\n   .            86400 IN NS yeti-ns1.dns-lab.net.\n\
    \   .            86400 IN NS yeti-ns2.dns-lab.net.\n   .            86400 IN NS\
    \ yeti-ns3.dns-lab.net.\n   .            86400 IN NS xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c.\n\
    \   .            86400 IN NS yeti-dns01.dnsworkshop.org.\n   .            86400\
    \ IN NS yeti-dns02.dnsworkshop.org.\n   .            86400 IN NS 3f79bb7b435b05321651daefd374cd.yeti-dns.net.\n\
    \   .            86400 IN NS ca978112ca1bbdcafac231b39a23dc.yeti-dns.net.\n  \
    \ .            86400 IN RRSIG NS 8 0 86400 (\n                            20171121050105\
    \ 20171114050105 26253 .\n                            FUvezvZgKtlLzQx2WKyg+D6dw/pITcbuZhzStZfg+LNa\n\
    \                            DjLJ9oGIBTU1BuqTujKHdxQn0DcdFh9QE68EPs+93bZr\n  \
    \                          VlplkmObj8f0B7zTQgGWBkI/K4Tn6bZ1I7QJ0Zwnk1mS\n    \
    \                        BmEPkWmvo0kkaTQbcID+tMTodL6wPAgW1AdwQUInfy21\n      \
    \                      p+31GGm3+SU6SJsgeHOzPUQW+dUVWmdj6uvWCnUkzW9p\n        \
    \                    +5en4+85jBfEOf+qiyvaQwUUe98xZ1TOiSwYvk5s/qiv\n          \
    \                  AMjG6nY+xndwJUwhcJAXBVmGgrtbiR8GiGZfGqt748VX\n            \
    \                4esLNtD8vdypucffem6n0T0eV1c+7j/eIA== )\n   ;; ADDITIONAL SECTION:\n\
    \   bii.dns-lab.net.        86400 IN AAAA 240c:f:1:22::6\n   yeti.bofh.priv.at.\
    \      86400 IN AAAA 2a01:4f8:161:6106:1::10\n   yeti.ipv6.ernet.in.     86400\
    \ IN AAAA 2001:e30:1c1e:1::333\n   yeti.aquaray.com.       86400 IN AAAA 2a02:ec0:200::1\n\
    \   yeti.jhcloos.net.       86400 IN AAAA 2001:19f0:5401:1c3::53\n   yeti.mind-dns.nl.\
    \       86400 IN AAAA 2a02:990:100:b01::53:0\n   ;; Query time: 163 msec\n   ;;\
    \ SERVER: 2001:4b98:dc2:45:216:3eff:fe4b:8c5b#53\n   ;; WHEN: Tue Nov 14 16:45:37\
    \ +08 2017\n   ;; MSG SIZE  rcvd: 1222\n"
- title: Appendix C.  Active IPv6 Prefixes in Yeti DNS Testbed
  contents:
  - "Appendix C.  Active IPv6 Prefixes in Yeti DNS Testbed\n   The following table\
    \ shows the prefixes that were active during 2017.\n   +----------------------+---------------------------------+----------+\n\
    \   | Prefix               | Originator                      | Location |\n  \
    \ +----------------------+---------------------------------+----------+\n   |\
    \ 240c::/28            | BII                             | CN       |\n   | 2001:6d0:6d06::/48\
    \   | MSK-IX                          | RU       |\n   | 2001:1488::/32      \
    \ | CZ.NIC                          | CZ       |\n   | 2001:620::/32        |\
    \ SWITCH                          | CH       |\n   | 2001:470::/32        | Hurricane\
    \ Electric, Inc.        | US       |\n   | 2001:0DA8:0202::/48  | BUPT6-CERNET2\
    \                   | CN       |\n   | 2001:19f0:6c00::/38  | Choopa, LLC    \
    \                 | US       |\n   | 2001:da8:205::/48    | BJTU6-CERNET2    \
    \               | CN       |\n   | 2001:62a::/31        | Vienna University Computer\
    \      | AT       |\n   |                      | Center                      \
    \    |          |\n   | 2001:67c:217c::/48   | AFNIC                         \
    \  | FR       |\n   | 2a02:2478::/32       | Profitbricks GmbH               |\
    \ DE       |\n   | 2001:1398:1::/48     | NIC Chile                       | CL\
    \       |\n   | 2001:4490:dc4c::/46  | NIB (National Internet          | IN  \
    \     |\n   |                      | Backbone)                       |       \
    \   |\n   | 2001:4b98::/32       | Gandi                           | FR      \
    \ |\n   | 2a02:aa8:0:2000::/52 | T-Systems-Eltec                 | ES       |\n\
    \   | 2a03:b240::/32       | Netskin GmbH                    | CH       |\n  \
    \ | 2801:1a0::/42        | Universidad de Ibague           | CO       |\n   |\
    \ 2a00:1cc8::/40       | ICT Valle Umbra s.r.l.          | IT       |\n   | 2a02:cdc0::/29\
    \       | ORG-CdSB1-RIPE                  | IT       |\n   +----------------------+---------------------------------+----------+\n"
- title: Appendix D.  Tools Developed for Yeti DNS Testbed
  contents:
  - "Appendix D.  Tools Developed for Yeti DNS Testbed\n   Various tools were developed\
    \ to support the Yeti DNS testbed, a\n   selection of which are described briefly\
    \ below.\n   YmmV (\"Yeti Many Mirror Verifier\") is designed to make it easy\
    \ and\n   safe for a DNS administrator to capture traffic sent from a resolver\n\
    \   to the Root Server system and to replay it towards Yeti-Root servers.\n  \
    \ Responses from both systems are recorded and compared, and\n   differences are\
    \ logged.  See <https://github.com/BII-Lab/ymmv>.\n   PcapParser is a module used\
    \ by YmmV which reassembles fragmented IPv6\n   datagrams and TCP segments from\
    \ a PCAP archive and extracts DNS\n   messages contained within them.  See <https://github.com/RunxiaWan/\n\
    \   PcapParser>.\n   DNS-layer-fragmentation implements DNS proxies that perform\n\
    \   application-level fragmentation of DNS messages, based on\n   [FRAGMENTS].\
    \  The idea with these proxies is to explore splitting DNS\n   messages in the\
    \ protocol itself, so they will not by fragmented by\n   the IP layer.  See <https://github.com/BII-Lab/DNS-layer-\n\
    \   Fragmentation>.\n   DNS_ATR is an implementation of DNS Additional Truncated\
    \ Response\n   (ATR), as described in [ATR] and [HOW_ATR_WORKS].  DNS_ATR acts\
    \ as a\n   proxy between resolver and authoritative servers, forwarding queries\n\
    \   and responses as a silent and transparent listener.  Responses that\n   are\
    \ larger than a nominated threshold (1280 octets by default)\n   trigger additional\
    \ truncated responses to be sent immediately\n   following the large response.\
    \  See <https://github.com/songlinjian/\n   DNS_ATR>.\n"
- title: Appendix E.  Controversy
  contents:
  - "Appendix E.  Controversy\n   The Yeti DNS Project, its infrastructure and the\
    \ various experiments\n   that have been carried out using that infrastructure,\
    \ have been\n   described by people involved in the project in many public meetings\n\
    \   at technical venues since its inception.  The mailing lists using\n   which\
    \ the operation of the infrastructure has been coordinated are\n   open to join,\
    \ and their archives are public.  The project as a whole\n   has been the subject\
    \ of robust public discussion.\n   Some commentators have expressed concern that\
    \ the Yeti DNS Project\n   is, in effect, operating an alternate root, challenging\
    \ the IAB's\n   comments published in [RFC2826].  Other such alternate roots are\n\
    \   considered to have caused end-user confusion and instability in the\n   namespace\
    \ of the DNS by the introduction of new top-level labels or\n   the different\
    \ use of top-level labels present in the Root Server\n   system.  The coordinators\
    \ of the Yeti DNS Project do not consider the\n   Yeti DNS Project to be an alternate\
    \ root in this sense, since by\n   design the namespace enabled by the Yeti-Root\
    \ zone is identical to\n   that of the Root Zone.\n   Some commentators have expressed\
    \ concern that the Yeti DNS Project\n   seeks to influence or subvert administrative\
    \ policy relating to the\n   Root Server system, in particular in the use of DNSSEC\
    \ trust anchors\n   not published by the IANA and the use of Yeti-Root servers\
    \ in regions\n   where governments or other organizations have expressed interest\
    \ in\n   operating a Root Server.  The coordinators of the Yeti-Root project\n\
    \   observe that their mandate is entirely technical and has no ambition\n   to\
    \ influence policy directly; they do hope, however, that technical\n   findings\
    \ from the Yeti DNS Project might act as a useful resource for\n   the wider technical\
    \ community.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   Firstly, the authors would like to acknowledge the contributions\
    \ from\n   the people who were involved in the implementation and operation of\n\
    \   the Yeti DNS by donating their time and resources.  They are:\n      Tomohiro\
    \ Ishihara, Antonio Prado, Stephane Bortzmeyer, Mickael\n      Jouanne, Pierre\
    \ Beyssac, Joao Damas, Pavel Khramtsov, Dmitry\n      Burkov, Dima Burkov, Kovalenko\
    \ Dmitry, Otmar Lendl, Praveen Misra,\n      Carsten Strotmann, Edwin Gomez, Daniel\
    \ Stirnimann, Andreas\n      Schulze, Remi Gacogne, Guillaume de Lafond, Yves\
    \ Bovard, Hugo\n      Salgado, Kees Monshouwer, Li Zhen, Daobiao Gong, Andreas\
    \ Schulze,\n      James Cloos, and Runxia Wan.\n   Thanks to all people who gave\
    \ important advice and comments to Yeti,\n   either in face-to-face meetings or\
    \ virtually via phone or mailing\n   list.  Some of the individuals are as follows:\n\
    \      Wu Hequan, Zhou Hongren, Cheng Yunqing, Xia Chongfeng, Tang\n      Xiongyan,\
    \ Li Yuxiao, Feng Ming, Zhang Tongxu, Duan Xiaodong, Wang\n      Yang, Wang JiYe,\
    \ Wang Lei, Zhao Zhifeng, Chen Wei, Wang Wei, Wang\n      Jilong, Du Yuejing,\
    \ Tan XiaoSheng, Chen Shangyi, Huang Chenqing,\n      Ma Yan, Li Xing, Cui Yong,\
    \ Bi Jun, Duan Haixing, Marc Blanchet,\n      Andrew Sullivan, Suzanne Wolf, Terry\
    \ Manderson, Geoff Huston, Jaap\n      Akkerhuis, Kaveh Ranjbar, Jun Murai, Paul\
    \ Wilson, and Kilnam\n      Chonm.\n   The authors also acknowledge the assistance\
    \ of the Independent\n   Submissions Editorial Board, and of the following reviewers\
    \ whose\n   opinions helped improve the clarity of this document:\n      Joe Abley,\
    \ Paul Mockapetris, and Subramanian Moonesamy.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Linjian Song (editor)\n   Beijing Internet Institute\n\
    \   2nd Floor, Building 5, No.58 Jing Hai Wu Lu, BDA\n   Beijing  100176\n   China\n\
    \   Email: songlinjian@gmail.com\n   URI:   http://www.biigroup.com/\n   Dong\
    \ Liu\n   Beijing Internet Institute\n   2nd Floor, Building 5, No.58 Jing Hai\
    \ Wu Lu, BDA\n   Beijing  100176\n   China\n   Email: dliu@biigroup.com\n   URI:\
    \   http://www.biigroup.com/\n   Paul Vixie\n   TISF\n   11400 La Honda Road\n\
    \   Woodside, California  94062\n   United States of America\n   Email: vixie@tisf.net\n\
    \   URI:   http://www.redbarn.org/\n   Akira Kato\n   Keio University/WIDE Project\n\
    \   Graduate School of Media Design, 4-1-1 Hiyoshi, Kohoku\n   Yokohama  223-8526\n\
    \   Japan\n   Email: kato@wide.ad.jp\n   URI:   http://www.kmd.keio.ac.jp/\n \
    \  Shane Kerr\n   Antoon Coolenlaan 41\n   Uithoorn  1422 GN\n   The Netherlands\n\
    \   Email: shane@time-travellers.org\n"
