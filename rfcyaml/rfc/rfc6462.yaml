- title: __initial_text__
  contents:
  - ''
- title: Internet Architecture Board (IAB)                              A. Cooper
  contents:
  - "Internet Architecture Board (IAB)                              A. Cooper\n  \
    \             Report from the Internet Privacy Workshop\n"
- title: Abstract
  contents:
  - "Abstract\n   On December 8-9, 2010, the IAB co-hosted an Internet privacy workshop\n\
    \   with the World Wide Web Consortium (W3C), the Internet Society\n   (ISOC),\
    \ and MIT's Computer Science and Artificial Intelligence\n   Laboratory (CSAIL).\
    \  The workshop revealed some of the fundamental\n   challenges in designing,\
    \ deploying, and analyzing privacy-protective\n   Internet protocols and systems.\
    \  Although workshop participants and\n   the community as a whole are still far\
    \ from understanding how best to\n   systematically address privacy within Internet\
    \ standards development,\n   workshop participants identified a number of potential\
    \ next steps.\n   For the IETF, these included the creation of a privacy directorate\
    \ to\n   review Internet-Drafts, further work on documenting privacy\n   considerations\
    \ for protocol developers, and a number of exploratory\n   efforts concerning\
    \ fingerprinting and anonymized routing.  Potential\n   action items for the W3C\
    \ included investigating the formation of a\n   privacy interest group and formulating\
    \ guidance about fingerprinting,\n   referrer headers, data minimization in APIs,\
    \ usability, and general\n   considerations for non-browser-based protocols.\n\
    \   Note that this document is a report on the proceedings of the\n   workshop.\
    \  The views and positions documented in this report are\n   those of the workshop\
    \ participants and do not necessarily reflect the\n   views of the IAB, W3C, ISOC,\
    \ or MIT CSAIL.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Architecture Board (IAB)\n   and represents information that\
    \ the IAB has deemed valuable to\n   provide for permanent record.  Documents\
    \ approved for publication by\n   the IAB are not a candidate for any level of\
    \ Internet Standard; see\n   Section 2 of RFC 5741.\n   Information about the\
    \ current status of this document, any errata,\n   and how to provide feedback\
    \ on it may be obtained at\n   http://www.rfc-editor.org/info/rfc6462.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2012 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n\
    \   2. Workshop Overview ...............................................3\n  \
    \    2.1. Technical Discussion .......................................4\n    \
    \  2.2. SDO Discussion .............................................5\n   3. Design\
    \ Challenges ...............................................6\n      3.1. Ease\
    \ of Fingerprinting .....................................6\n      3.2. Information\
    \ Leakage ........................................7\n      3.3. Differentiating\
    \ between First and Third Parties ............8\n      3.4. Lack of Transparency\
    \ and User Awareness ....................9\n   4. Deployment and Analysis Challenges\
    \ ..............................9\n      4.1. Generative Protocols vs. Contextual\
    \ Threats ................9\n      4.2. Tension between Privacy Protection and\
    \ Usability ..........11\n      4.3. Interaction between Business, Legal, and\
    \ Technical\n           Incentives ................................................12\n\
    \           4.3.1. Role of Regulation .................................12\n  \
    \         4.3.2. P3P: A Case Study of the Importance of Incentives ..13\n   5.\
    \ Conclusions and Next Steps .....................................14\n      5.1.\
    \ IETF Outlook ..............................................14\n      5.2. W3C\
    \ Outlook ...............................................15\n      5.3. Other\
    \ Future Work .........................................15\n   6. Acknowledgements\
    \ ...............................................15\n   7. Security Considerations\
    \ ........................................15\n   8. Informative References .........................................16\n\
    \   Appendix A. Workshop Materials ....................................19\n  \
    \ Appendix B. Workshop Participants .................................19\n   Appendix\
    \ C. Accepted Position Papers ..............................21\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   On December 8-9, 2010, the IAB co-hosted a workshop with\
    \ the W3C,\n   ISOC, and MIT's Computer Science and Artificial Intelligence\n\
    \   Laboratory (CSAIL) about Internet privacy [Workshop].  The workshop\n   was\
    \ organized to help the Internet community gain some understanding\n   of what\
    \ it means for Internet-based systems to respect privacy, how\n   such systems\
    \ have been or could be designed, how the relationship\n   between the web and\
    \ the broader Internet impacts privacy, and what\n   specific work the IETF and/or\
    \ the W3C might pursue to address\n   Internet privacy.  An overview of topics\
    \ discussed at the workshop is\n   provided in Section 2.\n   The workshop discussions\
    \ revealed the complexity and broad-based\n   nature of privacy on the Internet.\
    \  Across numerous different\n   applications, a number of fundamental design\
    \ challenges appear again\n   and again: the increasing ease of user/device/application\n\
    \   fingerprinting, unforeseen information leakage, difficulties in\n   distinguishing\
    \ first parties from third parties, complications\n   arising from system dependencies,\
    \ and the lack of transparency and\n   user awareness of privacy risks and tradeoffs\
    \ (see Section 3).\n   Workshop participants also identified a number of barriers\
    \ to\n   successful deployment and analysis of privacy-minded protocols and\n\
    \   systems, including the difficulty of using generic protocols and\n   tools\
    \ to defend against context-specific threats; the tension between\n   privacy\
    \ protection and usability; and the difficulty of navigating\n   between business,\
    \ legal, and individual incentives (see Section 4).\n   Privacy challenges far\
    \ outnumber solutions, but the workshop\n   identified a number of concrete preliminary\
    \ steps that standards\n   organizations can take to help ensure respect for user\
    \ privacy in the\n   design of future standards and systems.  For the IETF, these\
    \ included\n   the creation of a privacy directorate to review Internet-Drafts,\n\
    \   further work on documenting privacy considerations for protocol\n   developers,\
    \ and initiating a number of exploratory efforts concerning\n   fingerprinting\
    \ and anonymized routing.  Potential action items for\n   the W3C included investigating\
    \ the formation of a privacy interest\n   group and formulating guidance about\
    \ fingerprinting, referrer\n   headers, data minimization in APIs, usability,\
    \ and general\n   considerations for non-browser-based protocols.  These next\
    \ steps and\n   workshop outcomes are discussed in Section 5.\n"
- title: 2.  Workshop Overview
  contents:
  - "2.  Workshop Overview\n   The workshop explored both current technical challenges\
    \ to protecting\n   privacy and the ways in which standards organizations can\
    \ help to\n   address those challenges.  Links to workshop materials are listed\
    \ in\n   Appendix A.\n"
- title: 2.1.  Technical Discussion
  contents:
  - "2.1.  Technical Discussion\n   The workshop explored privacy challenges in three\
    \ different technical\n   domains: at the network level, at the browser level,\
    \ and with respect\n   to cross-site data exchanges.  Example technologies were\
    \ highlighted\n   in each area to motivate the discussion.\n   At the network\
    \ level, participants discussed IP address hiding in\n   mobility protocols, privacy\
    \ extensions for IPv6 addressing [RFC4941],\n   and onion routing.  Discussion\
    \ about the Tor project [Tor] was\n   particularly insightful.  Tor is a circuit-based,\
    \ low-latency\n   communication service designed to anonymize protocols that run\
    \ over\n   TCP.  End hosts participating in a Tor exchange choose a path through\n\
    \   the network and build a circuit in which each \"onion router\" in the\n  \
    \ path knows its predecessor and successor, but no other nodes in the\n   circuit.\
    \  Each onion router in the path unwraps and decrypts received\n   information\
    \ before relaying it downstream.\n   For Tor to provide anonymity guarantees,\
    \ Tor nodes need to be able to\n   strip out information elements that can be\
    \ used to re-identify users\n   over time.  For example, web technologies such\
    \ as cookies, large\n   portions of JavaScript, and almost all browser plug-ins\
    \ (including\n   Flash) need to be disabled in order to maintain Tor's privacy\n\
    \   properties during web use, significantly hampering usability.\n   At the browser\
    \ level, the discussion focused first on experiences\n   with \"private browsing\"\
    \ modes.  Private browsing puts a browser into\n   a temporary session where no\
    \ information about the user's browsing\n   session is stored locally after the\
    \ session ends.  The goal is to\n   protect the user's browsing behavior from\
    \ others who may make use of\n   the same browser on the same machine.  Private\
    \ browsing is not\n   designed to protect the user from being tracked by malware\
    \ (e.g.,\n   keyloggers), remote servers, employers, or governments, but there\
    \ is\n   some evidence that users fail to understand the distinction between\n\
    \   protection from snooping among users who share a device and these\n   other\
    \ forms of tracking.  The specific protections offered by private\n   browsing\
    \ modes also vary from browser to browser, creating privacy\n   loopholes in some\
    \ cases.\n   The browser discussion also addressed proposals for \"Do Not Track\"\
    \n   (DNT) technologies to be built into browsers to provide users with a\n  \
    \ simple way to opt out of web tracking.  At the time of the workshop,\n   various\
    \ different technical proposals had been designed to offer\n   users the ability\
    \ to indicate their preference to opt out or to block\n   communication to certain\
    \ web sites altogether.  The discussions at\n   the workshop illustrated a lack\
    \ of agreement about what type of\n   tracking is acceptable, which technical\
    \ mechanisms would be best\n   suited for different scenarios, and how the mechanisms\
    \ would interact\n   with other aspects of privacy protection (such as notices\
    \ to users).\n   The cross-site data-sharing discussion focused on current uses\
    \ of\n   Open Authorization (OAuth) (with Facebook Connect, for example).\n  \
    \ While improvements have been made in obtaining user consent to\n   sharing data\
    \ between sites, challenges remain with regard to data\n   minimization, ease\
    \ of use, hidden sharing of data, and centralization\n   of identity information.\n"
- title: 2.2.  SDO Discussion
  contents:
  - "2.2.  SDO Discussion\n   Participants discussed past experiences in approaching\
    \ privacy within\n   the IETF and the W3C.  Individual protocol efforts within\
    \ the IETF\n   have sought to address certain privacy threats over the years.\n\
    \   Protocol designers have taken steps to reduce the potential for\n   identifiability\
    \ associated with protocol usage, such as in the IPv6\n   privacy extensions case\
    \ [RFC4941].  Protocols architected to rely on\n   intermediaries have sought\
    \ to minimize the user data exposed in\n   transit, most notably in SIP [RFC3323].\
    \  Protocol architectures used\n   in interpersonal exchange have sought to give\
    \ users granular control\n   over their information, including presence [RFC2778]\
    \ and geolocation\n   information [RFC3693].  Efforts to square privacy with usability\
    \ are\n   ongoing; the ALTO working group [ALTO], for example, is working out\n\
    \   how to balance the needs of users and network operators to share data\n  \
    \ with each other about content preferences and network topologies\n   against\
    \ legitimate concerns about revealing too much of either kind\n   of information.\n\
    \   The IETF also has experience to draw on in building a culture of\n   security\
    \ awareness.  Beginning with [RFC1543], RFCs were required to\n   contain a Security\
    \ Considerations section.  But that simple mandate\n   did not immediately translate\
    \ into the extensive security\n   consciousness that permeates the IETF today.\
    \  Over many years and\n   with much effort invested, a more systematic approach\
    \ to security has\n   evolved that makes use of a variety of tools and resources:\
    \ the\n   security area itself, guidelines to RFC authors about security\n   considerations\
    \ [RFC3552], the security directorate, security advisors\n   assigned to individual\
    \ working groups, security tutorials at IETF\n   meetings, and so on.\n   The\
    \ W3C likewise has a number of past efforts to draw on.  One of the\n   earliest\
    \ large-scale standards efforts aimed at improving web privacy\n   was the Platform\
    \ for Privacy Preferences [P3P].  The idea behind P3P\n   was to have web sites\
    \ provide machine-readable privacy policies that\n   browsers could vet and possibly\
    \ override according to the user's\n   preference.  The P3P policy expression\
    \ language was robust enough to\n   allow sites to make complex assertions about\
    \ how they intended to\n   make use of data related to users, but market developments\
    \ have\n   created a number of challenges with deployed policies.\n   More recent\
    \ work at the W3C centered around the appropriateness of\n   various privacy features\
    \ to be included in the Geolocation API\n   [Geolocation], which gives web sites\
    \ a way to access the user's\n   precise location.  The API requires that implementations\
    \ obtain user\n   consent before accessing location information and allow users\
    \ to\n   revoke that consent, but decisions about retention, secondary use,\n\
    \   and data minimization are left up to individual web sites and\n   applications.\
    \  The geolocation effort and the P3P experience both\n   raise questions about\
    \ how to navigate usability, regulation, business\n   incentives, and other aspects\
    \ that normally lie outside the scope of\n   standards development organization\
    \ (SDO) work.\n"
- title: 3.  Design Challenges
  contents:
  - "3.  Design Challenges\n   Workshop discussions surfaced a number of key issues\
    \ that can make\n   designing privacy-sensitive protocols and systems difficult:\
    \ the\n   increasing ease of user/device/application fingerprinting, unforeseen\n\
    \   information leakage, difficulties in distinguishing first parties\n   from\
    \ third parties, complications arising from system dependencies,\n   and the lack\
    \ of transparency and user awareness of privacy risks and\n   tradeoffs.\n"
- title: 3.1.  Ease of Fingerprinting
  contents:
  - "3.1.  Ease of Fingerprinting\n   Internet applications and protocols now share\
    \ so many unique\n   identifiers and other bits of information as part of their\
    \ ordinary\n   operation that it is becoming increasingly easy for remote nodes\
    \ to\n   create unique device or application fingerprints and re-identify the\n\
    \   same devices or applications over time [Panopticlick].  Hardware\n   identifiers,\
    \ IP addresses, transport protocol parameters, cookies,\n   other forms of web\
    \ storage, and a vast array of browser-based\n   information may be routinely\
    \ shared as users browse the web.  The\n   ease of fingerprinting presents a significant\
    \ challenge for any\n   application that seeks to guarantee anonymity or unlinkability\
    \ (such\n   as [Tor], which uses onion routing to strip out data that identifies\n\
    \   communications endpoints).\n   In many cases, the information that can be\
    \ used to fingerprint a\n   device was not originally shared for that purpose;\
    \ identifiers and\n   other information are provided to support some other functionality\n\
    \   (like IP addresses being shared in order to route packets), and may\n   incidentally\
    \ be used to fingerprint.  This complicates the task of\n   preventing fingerprinting,\
    \ because each application or protocol\n   likely needs its own identifiers and\
    \ information to function.\n   Furthermore, some services are increasingly coming\
    \ to rely on\n   fingerprinting in order to detect fraud or provide customized\n\
    \   content, for example.  Finding privacy-friendly substitutes for\n   fingerprinting\
    \ will only become more difficult as these services\n   become more entrenched\
    \ (see Section 4.3).\n   The space of fingerprinting mitigations requires further\
    \ exploration.\n   For example, workshop participants discussed the use of JavaScript\n\
    \   queries to obtain a browser's (often highly unique) font list, and\n   the\
    \ tradeoffs associated with browsers instead (or additionally)\n   supporting\
    \ some small subset of fonts in order to reduce browser\n   identifiability. \
    \ As with many other privacy features, such a\n   restriction presents a tradeoff\
    \ between privacy and usability, and in\n   the case of fingerprinting writ large,\
    \ it may be difficult to find\n   consensus about which mitigations appropriately\
    \ balance both values.\n   As a first step, the IETF may consider documenting\
    \ the fingerprinting\n   implications for widely used IETF protocols (TCP, HTTP,\
    \ SIP, etc.).\n"
- title: 3.2.  Information Leakage
  contents:
  - "3.2.  Information Leakage\n   Internet protocols and services tend to leak information\
    \ in ways that\n   were not foreseen at design time, as explored during the IETF\
    \ 77\n   technical plenary [IETF77] and in recent research [PrivLoss]\n   [PrivDiffus].\
    \  For example, the HTTP referrer header [RFC2616]\n   (misspelled in the original\
    \ specification as \"Referer\") provides a\n   way for a web site to obtain the\
    \ URI of the resource that referred\n   the user to the site.  Referrer headers\
    \ provide valuable insights to\n   web sites about where their users come from,\
    \ but they can also leak\n   sensitive information (search terms or user IDs,\
    \ for example),\n   because URI strings on the web often contain this information.\
    \  The\n   infrastructure of an individual web site is often designed solely\n\
    \   with a view to making the site itself function properly, and\n   embedding\
    \ search terms or other user-specific information in URIs may\n   serve that goal,\
    \ but when those URIs leak out to other sites via a\n   referrer header, it creates\
    \ the potential for third parties to use\n   and abuse the data contained therein.\n\
    \   The use of URIs for authentication of identity or capabilities can be\n  \
    \ susceptible to the same kinds of problems.  Relying on a \"possession\n   model\"\
    \ where any user in possession of an authentication or\n   capability URI can\
    \ gain access to a resource is only suitable in\n   situations with some means\
    \ of control over URI distribution, and can\n   lead to wide leakage when used\
    \ on the open web.\n"
- title: 3.3.  Differentiating between First and Third Parties
  contents:
  - "3.3.  Differentiating between First and Third Parties\n   Distinguishing between\
    \ \"first-party\" interactions and \"third-party\"\n   interactions is important\
    \ for understanding the implications of data\n   collection, sharing, and use\
    \ that take place during the normal course\n   of web use.  Unfortunately, the\
    \ traditional meanings of these\n   concepts do not always clearly match up with\
    \ user expectations or\n   evolving web technologies.  Traditionally, the term\
    \ \"first party\" has\n   been used to refer to the domain of a web site to which\
    \ a user agent\n   directs an explicit request on behalf of a user.  The term\
    \ \"third\n   party\" has been used to refer to the domain of a web resource that\
    \ a\n   user agent requests as a result of a first-party request, with the\n \
    \  third-party resource hosted at a different domain from the first-\n   party\
    \ domain.\n   This distinction between first-party and third-party domains is\
    \ in\n   part a result of long-standing user agent practices for handling HTTP\n\
    \   cookies.  Typically, HTTP cookies are returned only to the origin\n   server\
    \ that set them [RFC6265].  Cookies set from first-party domains\n   may not be\
    \ read by third-party domains and vice versa.  In some\n   cases, cookies set\
    \ from first-party domains that contain subdomains\n   are accessible by all subdomains\
    \ of the first-party domain.  The\n   distinction between first-party domains\
    \ and third-party domains is\n   reflected in browser-based cookie controls: major\
    \ web browsers all\n   offer distinct first-party cookie settings and third-party\
    \ cookie\n   settings.\n   However, a user's perception or expectation of the\
    \ difference between\n   a \"first party\" and a \"third party\" may not fall\
    \ neatly within these\n   distinctions.  Users may expect that content hosted\
    \ on a first-party\n   subdomain, but provided or used by a third party, would\
    \ be treated as\n   third-party content, but browsers often treat it as first-party\n\
    \   content.  Conversely, when third-party content appears from a source\n   with\
    \ which the user has an established relationship -- such as the\n   Facebook \"\
    Like\" button or other social widgets -- users may consider\n   their interaction\
    \ with that content to be a desirable first-party\n   interaction, even though\
    \ the content is hosted on a third-party\n   domain.\n   Handling these expectations\
    \ programmatically is difficult, since the\n   same identifier structures (domains,\
    \ subdomains) can correlate to\n   different user expectations in different contexts.\
    \  On the other\n   hand, prompting users to express a preference about what kinds\
    \ of\n   data collection and use should be allowable by each party encountered\n\
    \   on the web is not practical.  Web and browser developers are actively\n  \
    \ seeking novel ways to address this challenge, but there are few\n   clear-cut\
    \ solutions.\n"
- title: 3.4.  Lack of Transparency and User Awareness
  contents:
  - "3.4.  Lack of Transparency and User Awareness\n   There is no question that users\
    \ lack a full understanding of how\n   their information is being used and what\
    \ the tradeoffs are between\n   having their data collected and accessing services\
    \ at little or no\n   cost.  Much of the tracking that takes place on the web\
    \ is passive\n   and invisible to users.  Most companies disclose their data usage\n\
    \   practices in written privacy policies, but these policies are rarely\n   read,\
    \ difficult to understand, and often fail to disclose salient\n   details (such\
    \ as data retention lifetimes).  Even when web tracking\n   is associated with\
    \ some visual indication -- a highly targeted Gmail\n   ad or the Facebook \"\
    Like\" button, for example -- users often do not\n   realize that it is occurring.\n\
    \   Efforts abound to attempt to present information about data\n   collection\
    \ and usage in a more digestible way.  P3P was one early\n   effort, but because\
    \ it sought to support the expression of the vast\n   expanse of potential policies\
    \ that companies may have, it developed\n   more complexity than the average user\
    \ (or user interface) could\n   sustain.  More recent efforts have focused on\
    \ using a limited set of\n   icons to represent policies or provide an indication\
    \ that tracking is\n   taking place.\n"
- title: 4.  Deployment and Analysis Challenges
  contents:
  - "4.  Deployment and Analysis Challenges\n   Workshop participants identified a\
    \ number of barriers to both\n   deployment of privacy-protecting technologies\
    \ and the analysis of the\n   privacy properties of technological systems.  These\
    \ included the\n   difficulty of using generic protocols and tools to defend against\n\
    \   context-specific threats; the tension between privacy protection and\n   usability;\
    \ and the difficulty of navigating between business, legal,\n   and individual\
    \ incentives.\n"
- title: 4.1.  Generative Protocols vs. Contextual Threats
  contents:
  - "4.1.  Generative Protocols vs. Contextual Threats\n   Privacy is not a binary\
    \ state.  Rather than operating either entirely\n   in private or entirely in\
    \ public, individuals experience privacy\n   contextually, resulting in differing\
    \ requirements for privacy\n   protection, depending on the circumstance and the\
    \ individual.  On the\n   Internet, the contextual nature of privacy means that\
    \ threats against\n   it can vary, depending on the deployment scenario, the usage\n\
    \   scenario, the capabilities of different attackers, and the level of\n   concern\
    \ that different kinds of attackers generate among different\n   users.\n   Addressing\
    \ the full waterfront of privacy threats within generic\n   protocols and tools\
    \ is largely intractable.  As a result, existing\n   privacy features developed\
    \ at the network and application layers have\n   taken more targeted approaches.\
    \  For example, privacy extensions for\n   stateless address autoconfiguration\
    \ in IPv6 [RFC4941] support\n   addresses constructed dynamically rather than\
    \ generating addresses\n   based on interface Media Access Control (MAC) addresses,\
    \ which for\n   most users are persistent and unchangeable unique identifiers\
    \ that\n   could be used for long-term tracking.  While IPv6 privacy extensions\n\
    \   provide important protection against tracking and re-identification\n   by\
    \ remote endpoints, they do not prevent -- and were not meant to\n   prevent --\
    \ all parties from being able to associate an IP address\n   with a particular\
    \ user.  ISPs and governments still have means to\n   make such associations,\
    \ and remote endpoints have many other\n   mechanisms at their disposal to attempt\
    \ to identify users\n   persistently, albeit without using IPv6 addresses.\n \
    \  This kind of experience with developing privacy tools shows that\n   designing\
    \ privacy features into systems and protocols requires a\n   clear understanding\
    \ of the scope of the threats they are designed to\n   address.  This scope is\
    \ currently being debated in discussion about\n   developing \"Do Not Track\"\
    \ (DNT) mechanisms for the web and other\n   online contexts.  A number of different\
    \ approaches have been\n   proposed, including browser functionality to retain\
    \ opt-out cookies,\n   an HTTP header that expresses the user's preference not\
    \ to be\n   tracked, and a browser-based block list mechanism that prevents the\n\
    \   browser from communicating with tracking sites (for an overview, see\n   [OptOuts]).\
    \  Regardless of the approach, these mechanisms function\n   based on some understanding\
    \ of which \"tracking\" users should be able\n   to control, which in turn is\
    \ based on some notion of the threats\n   presented by different kinds of tracking\
    \ conducted by different kinds\n   of entities on the web.  Should DNT mechanisms\
    \ apply to sites with\n   which the user already has an established relationship?\
    \  Or sites\n   that use only aggregate, non-individualized data?  Does tracking\
    \ for\n   fraud prevention or customization present different threats than\n \
    \  tracking for advertising or marketing purposes?  The answers to these\n   questions\
    \ will dictate DNT design choices.\n   The space of privacy threats on the Internet\
    \ may appear particularly\n   broad from a protocol design perspective, because\
    \ many of the\n   protocols in widest use are designed generically to support\
    \ a variety\n   of applications and functionality.  HTTP, for example, is used\
    \ for a\n   wider variety of purposes than its original designers likely\n   anticipated;\
    \ it is unsurprising that some of these purposes include\n   obtaining and using\
    \ data about web users in ways that may be privacy-\n   infringing.  It is unreasonable\
    \ to ask protocol designers to mitigate\n   the potential privacy risks of every\
    \ possible deployment that may\n   result from a particular protocol design; the\
    \ key questions are about\n   how the responsibility for protecting against privacy\
    \ intrusion\n   should be split between protocols, APIs, applications, and services,\n\
    \   and which kinds of privacy features can best be implemented in each\n   place.\n"
- title: 4.2.  Tension between Privacy Protection and Usability
  contents:
  - "4.2.  Tension between Privacy Protection and Usability\n   The workshop discussions\
    \ highlighted the tension between providing\n   privacy protections and maintaining\
    \ usability.  Tor [Tor] provides\n   some salient examples of this tradeoff. \
    \ Tor seeks to provide\n   protection against network surveillance, but by lengthening\
    \ the\n   routing path, it may significantly increase round-trip time.  Tor\n\
    \   obscures endpoint IP addresses; thus, it also interferes with\n   IP-based\
    \ geolocation.  Web browsing using Tor is particularly\n   challenging, as most\
    \ browser plug-ins, much of JavaScript, and a\n   number of other browser-based\
    \ features need to be blocked or\n   overridden in order to meet Tor's anonymity\
    \ requirements.  With Tor,\n   privacy clearly comes at a price.\n   Even less\
    \ aggressive privacy features may come with usability\n   tradeoffs.  One example\
    \ is the blocking of HTTP referrer headers for\n   privacy protection reasons.\
    \  Some sites provide a customized\n   experience to users based on the referring\
    \ page, which means that\n   disabling referrer headers, as some browsers allow\
    \ users to do, may\n   sacrifice user experience features on certain sites.  Part\
    \ of the\n   challenge is the level of nuance involved in making decisions about\n\
    \   privacy -- how can users be made to understand the privacy tradeoffs\n   of\
    \ blocking HTTP referrer headers, for example, when the effects of\n   doing so\
    \ will vary from site to site, or when there is limited UI\n   space to communicate\
    \ the tradeoffs?  Even seemingly simple privacy\n   controls like private browsing\
    \ are not well understood.\n   The feature set that implementors choose to make\
    \ available is often\n   reflective of the tension between usability and privacy.\
    \  For\n   example, SIP [RFC3261] supports Secure/Multipurpose Internet Mail\n\
    \   Extensions (S/MIME) to secure SIP request bodies, but given its user\n   experience\
    \ impact, few implementations include S/MIME support.\n   Although usability challenges\
    \ are generally thought of as user-level\n   issues that are out of scope for\
    \ the IETF, to the extent that they\n   trickle down into implementation decisions,\
    \ they are highly relevant.\n   Although workshop participants reached few firm\
    \ conclusions about how\n   to tackle usability issues arising from privacy features,\
    \ the group\n   agreed that it may be beneficial for the W3C to do some more thinking\n\
    \   in this area, possibly toward the end of including usability\n   considerations\
    \ in individual specifications.  The challenge with such\n   an effort will be\
    \ to provide useful guidance without being overly\n   prescriptive about how implementations\
    \ should be designed.\n"
- title: 4.3.  Interaction between Business, Legal, and Technical Incentives
  contents:
  - '4.3.  Interaction between Business, Legal, and Technical Incentives

    '
- title: 4.3.1.  Role of Regulation
  contents:
  - "4.3.1.  Role of Regulation\n   The Internet has sustained commercial content\
    \ for decades.  Many\n   services are offered at little or no cost in exchange\
    \ for being able\n   to sell advertising or collect user data (or both).  As the\n\
    \   commercial value of the web in particular has exploded in recent\n   years,\
    \ the paradigm for regulating privacy has also begun to change,\n   albeit more\
    \ slowly.\n   At the dawn of the commercial Internet, few web sites had written\n\
    \   privacy policies that explained what they did with user data.  Under\n   regulatory\
    \ pressure, sites began to document their data collection\n   and usage practices\
    \ in publicly posted policies.  These policies\n   quickly became lengthy legal\
    \ documents that commercial sites could\n   use to limit their liability, often\
    \ by disclosing every possible\n   practice that the site might engage in, rather\
    \ than informing users\n   about the salient practices of relevance to them.\n\
    \   Because so many businesses are fueled by user data, any move to give\n   users\
    \ greater control over their data -- whether by better informing\n   them about\
    \ its use or providing tools and settings -- often requires\n   the force of regulatory\
    \ influence to succeed.  In recent years,\n   regulatory authorities have put\
    \ pressure on companies to improve\n   their privacy disclosures by making them\
    \ simpler, more concise, more\n   prominent, and more accessible (see the 2010\
    \ Federal Trade Commission\n   privacy report [FTC]).  Certain companies and industry\
    \ sectors have\n   responded by developing privacy icons, using short notices\
    \ in\n   addition to privacy policies, and making the language they use to\n \
    \  describe privacy practices more accessible and easier to understand.\n   Regulators\
    \ play an important role in shaping incentive structures.\n   Companies often\
    \ seek a balance between acting to limit their\n   liability and pushing the envelope\
    \ with respect to uses of consumer\n   data.  If regulators take a strong stand\
    \ against certain practices --\n   as, for example, European legislators have\
    \ against cookies being set\n   without user consent [Directive] -- legitimate\
    \ businesses will feel\n   compelled to comply.  But where there is regulatory\
    \ uncertainty,\n   business responses may differ according to different market\n\
    \   strategies.  The variety of potential responses to the emerging\n   discussion\
    \ about mechanisms to control web tracking demonstrates this\n   variation: some\
    \ businesses will embrace support for enhanced user\n   control, others may restrict\
    \ their offerings or charge fees if they\n   are unable to track users, and still\
    \ others may elect to circumvent\n   any new mechanisms put in place.  The absence\
    \ of regulatory pressure\n   tends to make the line between \"good\" and \"bad\"\
    \ actors less evident.\n"
- title: '4.3.2.  P3P: A Case Study of the Importance of Incentives'
  contents:
  - "4.3.2.  P3P: A Case Study of the Importance of Incentives\n   That absence of\
    \ regulatory pressure revealed itself in the case of\n   P3P.  The first version\
    \ of P3P was standardized in the early 2000s,\n   when legalistic privacy policies\
    \ were the norm and users had only\n   elementary controls over the data collected\
    \ about them on the web.\n   P3P challenged that paradigm by providing a way for\
    \ web sites to\n   express machine-readable privacy policies for browsers to vet\
    \ and\n   possibly override according to the user's preference.  The P3P policy\n\
    \   expression language was designed to allow sites to make complex\n   assertions\
    \ about how they intended to make use of data related to\n   users.\n   The designers\
    \ of Internet Explorer 6 made a crucial decision to only\n   allow sites to use\
    \ third-party cookies if they had installed adequate\n   P3P policies.  To avoid\
    \ having their cookies blocked, most commercial\n   sites adopted some P3P policy,\
    \ although many sites merely cut and\n   pasted from the example policies provided\
    \ by the W3C.  Today, large\n   numbers of sites are misrepresenting their privacy\
    \ practices in their\n   P3P policies, but little has been done in response [Policies],\
    \ and\n   browser support for P3P outside of IE is limited.\n   While theories\
    \ abound to explain the current status of P3P\n   implementations, there is no\
    \ doubt that the relationship between\n   regulatory and commercial incentives\
    \ played a significant role.  The\n   P3P policy expression language provided\
    \ support for companies to be\n   able to express in granular detail how they\
    \ handle user data, but the\n   companies had little reason to do so, preferring\
    \ to protect\n   themselves from the liability associated with revealing potentially\n\
    \   unsavory practices.  In theory, the threat of regulatory backlash\n   could\
    \ have served as an incentive to publish accurate P3P policies,\n   but at the\
    \ time of P3P's release, there was little regulatory\n   interest in moving beyond\
    \ long, legalistic privacy policies.  Even\n   today, regulators are reluctant\
    \ to bring enforcement actions against\n   companies with misleading policies,\
    \ perhaps because their own\n   incentive structure compels them to focus on other,\
    \ more prominent\n   matters.\n   The P3P experience is instructive in general\
    \ for attempts at crafting\n   privacy features that require the active participation\
    \ of both ends\n   of a communication.  Actors that are meant to articulate their\
    \ own\n   privacy preferences, whether they be companies or individuals,\n   require\
    \ incentives to do so, as do those that are meant to process\n   and react to\
    \ such preferences.  For example, the IETF's GEOPRIV\n   architecture allows for\
    \ expression of user preferences about location\n   information [RFC4119].  While\
    \ users may have more incentive to\n   disclose their privacy preferences than\
    \ companies did in the P3P\n   case, successful use of the GEOPRIV model will\
    \ require endpoints that\n   consume location information to abide by those preferences,\
    \ and in\n   certain contexts -- commercial or employment-related, for example\
    \ --\n   they may be unwilling, or regulatory pressure may be required to spur\n\
    \   a change in practice.\n   It is clearly not the prerogative of Internet protocol\
    \ developers to\n   seek to change existing incentive structures.  But acknowledging\
    \ what\n   motivates businesses, individuals, and regulators is crucial to\n \
    \  determining whether new privacy technologies will succeed or fail.\n"
- title: 5.  Conclusions and Next Steps
  contents:
  - '5.  Conclusions and Next Steps

    '
- title: 5.1.  IETF Outlook
  contents:
  - "5.1.  IETF Outlook\n   The workshop demonstrated that the understanding of how\
    \ to address\n   privacy within the Internet standards community is nascent. \
    \ The IETF\n   faces particular challenges, because IETF protocols generally do\
    \ not\n   mandate implementation styles or pre-conceive particular deployment\n\
    \   contexts, making the space of potential privacy threats attributable\n   to\
    \ any single protocol difficult to foresee at protocol design time.\n   Workshop\
    \ participants nonetheless outlined a number of potential next\n   steps.  Work\
    \ has already begun to attempt to provide guidance to\n   protocol designers about\
    \ the privacy impact of their specifications\n   [PrivCons].  In refining this\
    \ guidance, many of the questions raised\n   at the workshop will need to be confronted,\
    \ including those about how\n   to properly model privacy threats against generic\
    \ protocols, how to\n   anticipate privacy risks that have been exposed in the\
    \ previous\n   design efforts, and how to document risks that are more difficult\
    \ to\n   foresee and mitigate.  Workshop participants acknowledged that\n   developing\
    \ such guidance is likely necessary if document authors are\n   expected to incorporate\
    \ \"Privacy Considerations\" sections in their\n   documents, but even with guidance,\
    \ this is likely to be an uphill\n   battle for many authors for some time to\
    \ come.\n   As preliminary steps, those with privacy expertise may seek to apply\n\
    \   the current guidance to existing IETF protocols.  The security area\n   directors\
    \ have also created a privacy directorate where privacy\n   reviews of documents\
    \ coming before the IESG are being conducted.\n   Participants also expressed\
    \ an interest in further pursuing a number\n   of the technical topics discussed\
    \ at the workshop, including lessons\n   learned from the experience of Tor and\
    \ the fingerprinting\n   implications of HTTP, TCP, SIP, and other IETF protocols.\
    \  These and\n   other efforts may be explored within the Internet Research Task\
    \ Force\n   (IRTF) in addition to, or in lieu of, the IETF.\n"
- title: 5.2.  W3C Outlook
  contents:
  - "5.2.  W3C Outlook\n   The W3C is likewise in a position of seeking a more comprehensive\n\
    \   approach to privacy within the SDO.  Because the work of the W3C\n   operates\
    \ within a more defined scope than that of the IETF -- namely,\n   the web --\
    \ the questions before the W3C tend to lie more in the space\n   of distinguishing\
    \ between what can appropriately be accomplished\n   within W3C specifications\
    \ and what should be left to individual\n   implementations, a theme that repeated\
    \ itself again and again at the\n   workshop.\n   To further develop its approach\
    \ to privacy, the W3C will investigate\n   an interest group to discuss privacy\
    \ topics.  Some potential topics\n   that emerged from the workshop include the\
    \ fingerprinting impact of\n   W3C protocols, data minimization in APIs, dealing\
    \ with referrer\n   header privacy leakage, developing privacy considerations\
    \ for\n   non-browser-based protocols, and developing usability considerations\n\
    \   as part of specification design.\n"
- title: 5.3.  Other Future Work
  contents:
  - "5.3.  Other Future Work\n   The workshop covered a number of topics that may\
    \ deserve further\n   exploration in the IETF, the W3C, and the privacy community\
    \ at large.\n   These include development of privacy terminology; articulation\
    \ of\n   privacy threat models; analysis and experimentation with \"Do Not\n \
    \  Track\" mechanisms for the web; work on cross-site data sharing,\n   correlation,\
    \ and linkability in web and non-web contexts; and\n   investigation of policy\
    \ expression languages.\n"
- title: 6.  Acknowledgements
  contents:
  - "6.  Acknowledgements\n   Thanks to Bernard Aboba, Nick Doty, and Hannes Tschofenig\
    \ for their\n   early reviews.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   Workshop participants discussed security aspects\
    \ related to privacy,\n   acknowledging that while much of the standards community\
    \ may have\n   once viewed most relevant privacy concerns as being encompassed\
    \ by\n   security considerations, there is a growing realization of privacy\n\
    \   threats that lie outside the security realm.  These include concerns\n   related\
    \ to data minimization, identifiability, and secondary use.\n   Earlier security\
    \ work provided minimal provision for privacy\n   protection (e.g., the definition\
    \ of \"privacy\" in [RFC2828] and some\n   guidance about private information\
    \ in [RFC3552]).\n"
- title: 8.  Informative References
  contents:
  - "8.  Informative References\n   [ALTO]     IETF, \"Application-Layer Traffic Optimization\
    \ (alto)\",\n              2011, <http://datatracker.ietf.org/wg/alto/charter/>.\n\
    \   [Directive]\n              European Parliament and Council of the European\
    \ Union,\n              \"Directive 2009/136/EC of the European Parliament and\
    \ of\n              the Council\", November 2009, <http://eur-lex.europa.eu/\n\
    \              LexUriServ/\n              LexUriServ.do?uri=OJ:L:2009:337:0011:01:EN:HTML>.\n\
    \   [FTC]      Federal Trade Commission Staff, \"A Preliminary FTC Staff\n   \
    \           Report on Protecting Consumer Privacy in an Era of Rapid\n       \
    \       Change: A Proposed Framework for Businesses and\n              Policymakers\"\
    , December 2010,\n              <http://www.ftc.gov/opa/2010/12/privacyreport.shtm>.\n\
    \   [Geolocation]\n              Popescu, A., Ed., \"Geolocation API Specification\"\
    ,\n              September 2010,\n              <http://www.w3.org/TR/2010/CR-geolocation-API-20100907/>.\n\
    \   [IETF77]   Krishnamurthy, B., \"Privacy Leakage on the Internet\",\n     \
    \         March 2010, <http://www.ietf.org/proceedings/77/slides/\n          \
    \    plenaryt-5.pdf>.\n   [OptOuts]  Cooper, A. and H. Tschofenig, \"Overview\
    \ of Universal\n              Opt-Out Mechanisms for Web Tracking\", Work in Progress,\n\
    \              March 2011.\n   [P3P]      Wenning, R., Ed., and M. Schunter, Ed.,\
    \ \"The Platform for\n              Privacy Preferences 1.1 (P3P1.1) Specification\"\
    ,\n              November 2006, <http://www.w3.org/TR/P3P11/>.\n   [Panopticlick]\n\
    \              Electronic Frontier Foundation, \"Panopticlick\", 2011,\n     \
    \         <http://panopticlick.eff.org/>.\n   [Policies] Leon, P., Cranor, L.,\
    \ McDonald, A., and R. McGuire, \"Token\n              Attempt: The Misrepresentation\
    \ of Website Privacy Policies\n              through the Misuse of P3P Compact\
    \ Policy Tokens\",\n              September 2010, <http://www.cylab.cmu.edu/research/\n\
    \              techreports/2010/tr_cylab10014.html>.\n   [PrivCons] Cooper, A.,\
    \ Tschofenig, H., Aboba, B., Peterson, J., and\n              J. Morris, \"Privacy\
    \ Considerations for Internet\n              Protocols\", Work in Progress, October\
    \ 2011.\n   [PrivDiffus]\n              Krishnamurthy, B. and C. Wills, \"Privacy\
    \ Diffusion on the\n              Web: A Longitudinal Perspective\", Proceedings\
    \ of the World\n              Wide Web Conference, pages 541-550, Madrid, Spain,\n\
    \              April 2009, <http://www.cs.wpi.edu/~cew/papers/www09.pdf>.\n  \
    \ [PrivLoss] Krishnamurthy, B., Malandrino, D., and C. Wills,\n              \"\
    Measuring Privacy Loss and the Impact of Privacy\n              Protection in\
    \ Web Browsing\", Proceedings of the Symposium\n              on Usable Privacy\
    \ and Security, pages 52-63, Pittsburgh,\n              PA USA, ACM International\
    \ Conference Proceedings Series,\n              July 2007,\n              <http://www.cs.wpi.edu/~cew/papers/soups07.pdf>.\n\
    \   [RFC1543]  Postel, J., \"Instructions to RFC Authors\", RFC 1543,\n      \
    \        October 1993.\n   [RFC2616]  Fielding, R., Gettys, J., Mogul, J., Frystyk,\
    \ H.,\n              Masinter, L., Leach, P., and T. Berners-Lee, \"Hypertext\n\
    \              Transfer Protocol -- HTTP/1.1\", RFC 2616, June 1999.\n   [RFC2778]\
    \  Day, M., Rosenberg, J., and H. Sugano, \"A Model for\n              Presence\
    \ and Instant Messaging\", RFC 2778, February 2000.\n   [RFC2828]  Shirey, R.,\
    \ \"Internet Security Glossary\", RFC 2828,\n              May 2000.\n   [RFC3261]\
    \  Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston,\n              A.,\
    \ Peterson, J., Sparks, R., Handley, M., and E.\n              Schooler, \"SIP:\
    \ Session Initiation Protocol\", RFC 3261,\n              June 2002.\n   [RFC3323]\
    \  Peterson, J., \"A Privacy Mechanism for the Session\n              Initiation\
    \ Protocol (SIP)\", RFC 3323, November 2002.\n   [RFC3552]  Rescorla, E. and B.\
    \ Korver, \"Guidelines for Writing RFC\n              Text on Security Considerations\"\
    , BCP 72, RFC 3552,\n              July 2003.\n   [RFC3693]  Cuellar, J., Morris,\
    \ J., Mulligan, D., Peterson, J., and\n              J. Polk, \"Geopriv Requirements\"\
    , RFC 3693, February 2004.\n   [RFC4119]  Peterson, J., \"A Presence-based GEOPRIV\
    \ Location Object\n              Format\", RFC 4119, December 2005.\n   [RFC4941]\
    \  Narten, T., Draves, R., and S. Krishnan, \"Privacy\n              Extensions\
    \ for Stateless Address Autoconfiguration in\n              IPv6\", RFC 4941,\
    \ September 2007.\n   [RFC6265]  Barth, A., \"HTTP State Management Mechanism\"\
    , RFC 6265,\n              April 2011.\n   [Tor]      The Tor Project, Inc., \"\
    Tor\", 2011,\n              <https://www.torproject.org/>.\n   [Workshop] IAB,\
    \ W3C, ISOC, MIT CSAIL, \"Internet Privacy Workshop\n              2010\", 2011,\
    \ <http://www.iab.org/activities/workshops/\n              internet-privacy-workshop-2010/>.\n"
- title: Appendix A.  Workshop Materials
  contents:
  - "Appendix A.  Workshop Materials\n      Main page: http://www.iab.org/activities/workshops/\n\
    \      internet-privacy-workshop-2010/\n      Slides: http://www.iab.org/activities/workshops/\n\
    \      internet-privacy-workshop-2010/slides/\n      Minutes: http://www.iab.org/activities/workshops/\n\
    \      internet-privacy-workshop-2010/minutes/\n      Position papers: http://www.iab.org/activities/workshops/\n\
    \      internet-privacy-workshop-2010/papers/\n"
- title: Appendix B.  Workshop Participants
  contents:
  - "Appendix B.  Workshop Participants\n   o  Fu-Ming Shih, MIT\n   o  Ian Jacobi,\
    \ MIT\n   o  Steve Woodrow, MIT\n   o  Nick Mathewson, The Tor Project\n   o \
    \ Peter Eckersley, Electronic Frontier Foundation\n   o  John Klensin, IAB\n \
    \  o  Oliver Hanka, Technical University Munich\n   o  Alan Mislove, Northeastern\
    \ University\n   o  Ashkan Soltani, FTC\n   o  Sam Hartman, Painless Security\n\
    \   o  Kevin Trilli, TRUSTe\n   o  Dorothy Gellert, InterDigital\n   o  Aaron\
    \ Falk, Raytheon - BBN Technologies\n   o  Sean Turner, IECA\n   o  Wei-Yeh Lee,\
    \ NAVTEQ\n   o  Chad McClung, The Boeing Company\n   o  Jan Seedorf, NEC\n   o\
    \  Dave Crocker, Brandenburg InternetWorking\n   o  Lorrie Cranor, Carnegie Mellon\
    \ University\n   o  Noah Mendelsohn, W3C TAG Chair\n   o  Stefan Winter, RESTENA\n\
    \   o  Craig Wittenberg, Microsoft\n   o  Bernard Aboba, IAB/Microsoft\n   o \
    \ Heather West, Google\n   o  Blaine Cook, British Telecom\n   o  Kasey Chappelle,\
    \ Vodafone Group\n   o  Russ Housley, IETF Chair/Vigil Security, LLC\n   o  Daniel\
    \ Appelquist, Vodafone R&D\n   o  Olaf Kolkman, IAB Chair\n   o  Jon Peterson,\
    \ IAB/NeuStar, Inc.\n   o  Balachander Krishnamurthy, AT&T Labs--Research\n  \
    \ o  Marc Linsner, Cisco Systems\n   o  Jorge Cuellar, Siemens AG\n   o  Arvind\
    \ Narayanan, Stanford University\n   o  Eric Rescorla, Skype\n   o  Cullen Jennings,\
    \ Cisco\n   o  Christine Runnegar, Internet Society\n   o  Alissa Cooper, Center\
    \ for Democracy & Technology\n   o  Jim Fenton, Cisco\n   o  Oshani Seneviratne,\
    \ MIT\n   o  Lalana Kagal, MIT\n   o  Fred Carter, Information & Privacy Commissioner\
    \ of Ontario, Canada\n   o  Frederick Hirsch, Nokia\n   o  Benjamin Heitmann,\
    \ DERI, NUI Galway, Ireland\n   o  John Linn, RSA, The Security Division of EMC\n\
    \   o  Paul Trevithick, Azigo\n   o  Ari Schwartz, National Institute of Standards\
    \ and Technology\n   o  David Evans, University of Cambridge\n   o  Nick Doty,\
    \ UC Berkeley, School of Information\n   o  Sharon Paradesi, MIT\n   o  Jonathan\
    \ Mayer, Stanford University\n   o  David Maher, Intertrust\n   o  Brett McDowell,\
    \ PayPal\n   o  Leucio Antonio Cutillo, Eurecom\n   o  Susan Landau, Radcliffe\
    \ Institute for Advanced Study, Harvard\n      University\n   o  Christopher Soghoian,\
    \ FTC In-house Technologist, Center for\n      Applied Cybersecurity Research,\
    \ Indiana University\n   o  Trent Adams, Internet Society\n   o  Thomas Roessler,\
    \ W3C\n   o  Karen O'Donoghue, ISOC\n   o  Hannes Tschofenig, IAB/Nokia Siemens\
    \ Networks\n   o  Lucy Elizabeth Lynch, Internet Society\n   o  Karen Sollins,\
    \ MIT\n   o  Tim Berners-Lee, W3C\n"
- title: Appendix C.  Accepted Position Papers
  contents:
  - "Appendix C.  Accepted Position Papers\n   1.   \"Addressing the privacy management\
    \ crisis in online social\n        networks\" by Krishna Gummadi, Balachander\
    \ Krishnamurthy, and\n        Alan Mislove\n   2.   \"Thoughts on Adding \"Privacy\
    \ Considerations\" to Internet Drafts\"\n        by Alissa Cooper and John Morris\n\
    \   3.   \"Toward Objective Global Privacy Standards\" by Ari Schwartz\n   4.\
    \   \"SocialKeys: Transparent Cryptography via Key Distribution over\n       \
    \ Social Networks\" by Arvind Narayanan\n   5.   \"Web Crawlers and Privacy: The\
    \ Need to Reboot Robots.txt\" by\n        Arvind Narayanan and Pete Warden\n \
    \  6.   \"I Know What You Will Do Next Summer\" by Balachander\n        Krishnamurthy\n\
    \   7.   \"An architecture for privacy-enabled user profile portability on\n \
    \       the Web of Data\" by Benjamin Heitmann and Conor Hayes\n   8.   \"Addressing\
    \ Identity on the Web\" by Blaine Cook\n   9.   \"Protection-by-Design: Enhancing\
    \ ecosystem capabilities to\n        protect personal information\" by Jonathan\
    \ Fox and Brett McDowell\n   10.  \"Privacy-preserving identities for a safer,\
    \ more trusted\n        internet\" by Christian Paquin\n   11.  \"Why Private\
    \ Browsing Modes Do Not Deliver Real Privacy\" by\n        Christopher Soghoian\n\
    \   12.  \"Incentives for Privacy\" by Cullen Jennings\n   13.  \"Joint Privacy\
    \ Workshop: Position Comments by D. Crocker\" by\n        Dave Crocker\n   14.\
    \  \"Using properties of physical phenomena and information flow\n        control\
    \ to manage privacy\" by David Evans and David M. Eyers\n   15.  \"Privacy Approaches\
    \ for Internet Video Advertising\" by Dave\n        Maher\n   16.  \"Privacy on\
    \ the Internet\" by Dorothy Gellert\n   17.  \"Can We Have a Usable Internet Without\
    \ User Trackability?\" by\n        Eric Rescorla\n   18.  \"Privacy by Design:\
    \ The 7 Foundational Principles --\n        Implementation and Mapping of Fair\
    \ Information Practices\" by\n        Fred Carter and Ann Cavoukian\n   19.  \"\
    Internet Privacy Workshop Position Paper: Privacy and Device\n        APIs\" by\
    \ Frederick Hirsch\n   20.  \"Position Paper for Internet Privacy Workshop\" by\
    \ Heather West\n   21.  \"I 'like' you, but I hate your apps\" by Ian Glazer\n\
    \   22.  \"Privicons: A approach to communicating privacy preferences\n      \
    \  between Users\" by E. Forrest and J. Schallabock\n   23.  \"Privacy Preservation\
    \ Techniques to establish Trustworthiness\n        for Distributed, Inter-Provider\
    \ Monitoring\" by J. Seedorf, S.\n        Niccolini, A. Sarma, B. Trammell, and\
    \ G. Bianchi\n   24.  \"Trusted Intermediaries as Privacy Agents\" by Jim Fenton\n\
    \   25.  \"Protocols are for sharing\" by John Kemp\n   26.  \"On Technology and\
    \ Internet Privacy\" by John Linn\n   27.  \"Do Not Track: Universal Web Tracking\
    \ Opt-out\" by Jonathan Mayer\n        and Arvind Narayanan\n   28.  \"Location\
    \ Privacy Protection Through Obfuscation\" by Jorge\n        Cuellar\n   29. \
    \ \"Everything we thought we knew about privacy is wrong\" by Kasey\n        Chappelle\
    \ and Dan Appelquist\n   30.  \"TRUSTe Position Paper\" by Kevin Trilli\n   31.\
    \  \"Position Paper: Incentives for Adoption of Machine-Readable\n        Privacy\
    \ Notices\" by Lorrie Cranor\n   32.  \"Facilitate, don't mandate\" by Ari Rabkin,\
    \ Nick Doty, and\n        Deirdre K. Mulligan\n   33.  \"Location Privacy in Next\
    \ Generation Internet Architectures\" by\n        Oliver Hanka\n   34.  \"HTTPa:\
    \ Accountable HTTP\" by Oshani Seneviratne and Lalana Kagal\n   35.  \"Personal\
    \ Data Service\" by Paul Trevithick\n   36.  \"Several Pressing Problems in Hypertext\
    \ Privacy\" by Peter\n        Eckersley\n   37.  \"Adding Privacy in Existing\
    \ Security Systems\" by Sam Hartman\n   38.  \"Mobility and Privacy\" by S. Brim,\
    \ M. Linsner, B. McLaughlin,\n        and K. Wierenga\n   39.  \"Saveface: Save\
    \ George's faces in Social Networks where Contexts\n        Collapse\" by Fuming\
    \ Shih and Sharon Paradesi\n   40.  \"eduroam -- a world-wide network access roaming\
    \ consortium on\n        the edge of preserving privacy vs. identifying users\"\
    \ by Stefan\n        Winter\n   41.  \"Effective Device API Privacy: Protecting\
    \ Everyone (Not Just the\n        User)\" by Susan Landau\n   42.  \"Safebook:\
    \ Privacy Preserving Online Social Network\" by L.\n        Antonio Cutillo, R.\
    \ Molva, and M. Onen\n"
- title: Author's Address
  contents:
  - "Author's Address\n   Alissa Cooper\n   CDT\n   1634 I Street NW, Suite 1100\n\
    \   Washington, DC  20006\n   USA\n   EMail: acooper@cdt.org\n   URI:   http://www.cdt.org/\n"
