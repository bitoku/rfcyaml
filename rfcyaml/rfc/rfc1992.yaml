- title: __initial_text__
  contents:
  - '                    The Nimrod Routing Architecture

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  This memo\n   does not specify an Internet standard of any kind.  Distribution\
    \ of\n   this memo is unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   We present a scalable internetwork routing architecture, called\n\
    \   Nimrod.  The Nimrod architecture is designed to accommodate a dynamic\n  \
    \ internetwork of arbitrary size with heterogeneous service\n   requirements and\
    \ restrictions and to admit incremental deployment\n   throughout an internetwork.\
    \  The key to Nimrod's scalability is its\n   ability to represent and manipulate\
    \ routing-related information at\n   multiple levels of abstraction.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction . . . . . . . . . . . . . . . . . . . .\
    \ . . . . . . 2\n   2. Overview of Nimrod . . . . . . . . . . . . . . . . . .\
    \ . . . . . 3\n     2.1 Constraints of the Internetworking Environment  . . .\
    \ . . . . 3\n     2.2 The Basic Routing Functions . . . . . . . . . . . . . .\
    \ . . . 5\n     2.3 Scalability Features  . . . . . . . . . . . . . . . . . .\
    \ . . 6\n       2.3.1 Clustering and Abstraction  . . . . . . . . . . . . . .\
    \ . 6\n       2.3.2 Restricting Information Distribution  . . . . . . . . . .\
    \ 7\n       2.3.3 Local Selection of Feasible Routes  . . . . . . . . . . . 8\n\
    \       2.3.4 Caching . . . . . . . . . . . . . . . . . . . . . . . . . 8\n  \
    \     2.3.5 Limiting Forwarding Information . . . . . . . . . . . . . 8\n   3.\
    \ Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n     3.1\
    \ Endpoints   . . . . . . . . . . . . . . . . . . . . . . . . . 9\n     3.2 Nodes\
    \ and Adjacencies . . . . . . . . . . . . . . . . . . . . 9\n     3.3 Maps  .\
    \ . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n       3.3.1 Connectivity\
    \ Specifications  . . . . . . . . . . . . . . 10\n     3.4  Locators  . . . .\
    \ . . . . . . . . . . . . . . . . . . . . . 10\n     3.5 Node Attributes  . .\
    \ . . . . . . . . . . . . . . . . . . . . 11\n       3.5.1 Adjacencies  . . .\
    \ . . . . . . . . . . . . . . . . . . . 11\n       3.5.2 Internal Maps  . . .\
    \ . . . . . . . . . . . . . . . . . . 11\n       3.5.3 Transit Connectivity .\
    \ . . . . . . . . . . . . . . . . . 12\n       3.5.4 Inbound Connectivity . .\
    \ . . . . . . . . . . . . . . . . 12\n       3.5.5 Outbound Connectivity  . .\
    \ . . . . . . . . . . . . . . . 12\n   4. Physical Realization  . . . . . . .\
    \ . . . . . . . . . . . . . . 13\n     4.1 Contiguity   . . . . . . . . . . .\
    \ . . . . . . . . . . . . . 13\n     4.2 An Example . . . . . . . . . . . . .\
    \ . . . . . . . . . . . . 14\n     4.3 Multiple Locator Assignment  . . . . .\
    \ . . . . . . . . . . . 15\n   5. Forwarding  . . . . . . . . . . . . . . . .\
    \ . . . . . . . . . . 20\n     5.1 Policy . . . . . . . . . . . . . . . . . .\
    \ . . . . . . . . . 23\n     5.2 Trust  . . . . . . . . . . . . . . . . . . .\
    \ . . . . . . . . 23\n     5.3 Connectivity Specification (CSC) Mode  . . . .\
    \ . . . . . . . 24\n     5.4 Flow Mode  . . . . . . . . . . . . . . . . . . .\
    \ . . . . . . 25\n     5.5 Datagram Mode  . . . . . . . . . . . . . . . . . .\
    \ . . . . . 25\n     5.6 Connectivity Specification Sequence Mode . . . . . .\
    \ . . . . 26\n   6. Security Considerations . . . . . . . . . . . . . . . . .\
    \ . . . 26\n   7. References  . . . . . . . . . . . . . . . . . . . . . . . .\
    \ . . 26\n   7. Authors' Addresses  . . . . . . . . . . . . . . . . . . . . .\
    \ . 27\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   Nimrod is a scalable routing architecture designed to accommodate\
    \ a\n   continually expanding and diversifying internetwork.  First suggested\n\
    \   by Noel Chiappa, the Nimrod architecture has undergone revision and\n   refinement\
    \ through the efforts of the Nimrod working group of the\n   IETF. In this document,\
    \ we present a detailed description of this\n   architecture.\n   The goals of\
    \ Nimrod are as follows:\n   1. To support a dynamic internetwork of arbitrary\
    \ size by\n      providing mechanisms to control the amount of routing information\n\
    \      that must be known throughout an internetwork.\n   2. To provide service-specific\
    \ routing in the presence of multiple\n      constraints imposed by service providers\
    \ and users.\n   3. To admit incremental deployment throughout an internetwork.\n\
    \   We have designed the Nimrod architecture to meet these goals.  The\n   key\
    \ features of this architecture include:\n   1. Representation of internetwork\
    \ connectivity and services in the\n      form of maps at multiple levels of abstraction.\n\
    \   2. User-controlled route generation and selection based on maps and\n    \
    \  traffic service requirements.\n   3. User-directed packet forwarding along\
    \ established paths.\n   Nimrod is a general routing architecture that can be\
    \ applied to\n   routing both within a single routing domain and among multiple\n\
    \   routing domains.  As a general internetwork routing architecture\n   designed\
    \ to deal with increased internetwork size and diversity,\n   Nimrod is equally\
    \ applicable to both the TCP/IP and OSI environments.\n"
- title: 2. Overview of Nimrod
  contents:
  - "2. Overview of Nimrod\n   Before describing the Nimrod architecture in detail,\
    \ we provide an\n   overview.  We begin with the internetworking requirements,\
    \ followed\n   by the routing functions, and concluding with Nimrod's scaling\n\
    \   characteristics.\n"
- title: 2.1 Constraints of the Internetworking Environment
  contents:
  - "2.1 Constraints of the Internetworking Environment\n   Internetworks are growing\
    \ and evolving systems, in terms of number,\n   diversity, and interconnectivity\
    \ of service providers and users, and\n   therefore require a routing architecture\
    \ that can accommodate\n   internetwork growth and evolution.  A complicated mix\
    \ of factors such\n   as technological advances, political alliances, and service\
    \ supply\n   and demand economics will determine how an internetwork will change\n\
    \   over time.  However, correctly predicting all of these factors and\n   all\
    \ of their effects on an internetwork may not be possible.  Thus,\n   the flexibility\
    \ of an internetwork routing architecture is its key to\n   handling unanticipated\
    \ requirements.\n   In developing the Nimrod architecture, we first assembled\
    \ a list of\n   internetwork environmental constraints that have implications\
    \ for\n   routing.  This list, enumerated below, includes observations about\n\
    \   the present Internet; it also includes predictions about\n   internetworks\
    \ five to ten years in the future.\n   1. The Internet will grow to include O(10^9)\
    \ networks.\n   2. The number of internetwork users may be unbounded.\n   3. The\
    \ capacity of internetwork resources is steadily increasing but\n      so is the\
    \ demand for these resources.\n   4. Routers and hosts have finite processing\
    \ capacity and finite\n      memory, and networks have finite transmission capacity.\n\
    \   5. Internetworks comprise different types of communications media --\n   \
    \   including wireline, optical and wireless, terrestrial and\n      satellite,\
    \ shared multiaccess and point-to-point -- with different\n      service characteristics\
    \ in terms of throughput, delay, error and\n      loss distributions, and privacy.\n\
    \   6. Internetwork elements -- networks, routers, hosts, and processes --\n \
    \     may be mobile.\n   7. Service providers will specify offered services and\
    \ restrictions on\n      access to those services.  Restrictions may be in terms\
    \ of when a\n      service is available, how much the service costs, which users\
    \ may\n      subscribe to the service and for what purposes, and how the user\n\
    \      must shape its traffic in order to receive a service guarantee.\n   8.\
    \ Users will specify traffic service requirements which may vary\n      widely\
    \ among sessions.  These specifications may be in terms of\n      requested qualities\
    \ of service, the amounts they are willing to pay\n      for these services, the\
    \ times at which they want these services,\n      and the providers they wish\
    \ to use.\n   9. A user traffic session may include m sources and n destinations,\n\
    \      where m, n > or = 1.\n   10. Service providers and users have a synergistic\
    \ relationship.  That\n       is, as users develop more applications with special\
    \ service\n       requirements, service providers will respond with the services\
    \ to\n       meet these demands.  Moreover, as service providers deliver more\n\
    \       services, users will develop more applications that take advantage\n \
    \      of these services.\n   11. Support for varied and special services will\
    \ require more\n       processing, memory, and transmission bandwidth on the part\
    \ of both\n       the service providers offering these services and the users\n\
    \       requesting these services.  Hence, many routing-related activities\n \
    \      will likely be performed not by routers and hosts but rather by\n     \
    \  independent devices acting on their behalf to process, store, and\n       distribute\
    \ routing information.\n   12. Users requiring specialized services (e.g., high\
    \ guaranteed\n       throughput) will usually be willing to pay more for these\
    \ services\n       and to incur some delay in obtaining them.\n   13. Service\
    \ providers are reluctant to introduce complicated protocols\n       into their\
    \ networks, because they are more difficult to manage.\n   14. Vendors are reluctant\
    \ to implement complicated protocols in their\n       products, because they take\
    \ longer to develop.\n   Collectively, these constraints imply that a successful\
    \ internetwork\n   routing architecture must support special features, such as\
    \ service-\n   specific routing and component mobility in a large and changing\n\
    \   internetwork, using simple procedures that consume a minimal amount\n   of\
    \ internetwork resources.  We believe that the Nimrod architecture\n   meets these\
    \ goals, and we justify this claim in the remainder of this\n   document.\n"
- title: 2.2 The Basic Routing Functions
  contents:
  - "2.2 The Basic Routing Functions\n   The basic routing functions provided by Nimrod\
    \ are those provided by\n   any routing system, namely:\n   1. Collecting, assembling,\
    \ and distributing the information necessary\n      for route generation and selection.\n\
    \   2. Generating and selecting routes based on this information.\n   3. Establishing\
    \ in routers information necessary for forwarding\n      packets along the selected\
    \ routes.\n   4. Forwarding packets along the selected routes.\n   The Nimrod\
    \ approach to providing this routing functionality includes\n   map distribution\
    \ according to the \"link-state\" paradigm, localization\n   of route generation\
    \ and selection at traffic sources and\n   destinations, and specification of\
    \ packet forwarding through path\n   establishment by the sources and destinations.\n\
    \   Link-state map distribution permits each service provider to have\n   control\
    \ over the services it offers, through both distributing\n   restrictions in and\
    \ restricting distribution of its routing\n   information.  Restricting distribution\
    \ of routing information serves\n   to reduce the amount of routing information\
    \ maintained throughout an\n   internetwork and to keep certain routing information\
    \ private.\n   However, it also leads to inconsistent routing information databases\n\
    \   throughout an internetwork, as not all such databases will be\n   complete\
    \ or identical.  We expect routing information database\n   inconsistencies to\
    \ occur often in a large internetwork, regardless of\n   whether privacy is an\
    \ issue.  The reason is that we expect some\n   devices to be incapable of maintaining\
    \ the complete set of routing\n   information for the internetwork.  These devices\
    \ will select only\n   some of the distributed routing information for storage\
    \ in their\n   databases.\n   Route generation and selection, based on maps and\
    \ traffic service\n   requirements, may be completely controlled by the users\
    \ or, more\n   likely, by devices acting on their behalf and does not require\
    \ global\n   coordination among routers.  Thus these devices may generate routes\n\
    \   specific to the users' needs, and only those users pay the cost of\n   generating\
    \ those routes.  Locally-controlled route generation allows\n   incremental deployment\
    \ of and experimentation with new route\n   generation algorithms, as these algorithms\
    \ need not be the same at\n   each location in an internetwork.\n   Packet forwarding\
    \ according to paths may be completely controlled by\n   the users or the devices\
    \ acting on their behalf.  These paths may be\n   specified in as much detail\
    \ as the maps permit.  Such packet\n   forwarding provides freedom from forwarding\
    \ loops, even when routers\n   in a path have inconsistent routing information.\
    \  The reason is that\n   the forwarding path is a route computed by a single\
    \ device and based\n   on routing information maintained at a single device.\n\
    \   We note that the Nimrod architecture and Inter-Domain Policy Routing\n   (IDPR)\
    \ [1] share in common link-state routing information\n   distribution, localized\
    \ route generation and path-oriented message\n   forwarding.  In developing the\
    \ Nimrod architecture, we have drawn\n   upon experience gained in developing\
    \ and experimenting with IDPR.\n"
- title: 2.3 Scalability Features
  contents:
  - "2.3 Scalability Features\n   Nimrod must provide service-specific routing in\
    \ arbitrarily large\n   internetworks and hence must employ mechanisms that help\
    \ to contain\n   the amount of internetwork resources consumed by the routing\n\
    \   functions.  We provide a brief synopsis of such mechanisms below,\n   noting\
    \ that arbitrary use of these mechanisms does not guarantee a\n   scalable routing\
    \ architecture.  Instead, these mechanisms must be\n   used wisely, in order enable\
    \ a routing architecture to scale with\n   internetwork growth.\n"
- title: 2.3.1 Clustering and Abstraction
  contents:
  - "2.3.1 Clustering and Abstraction\n   The Nimrod architecture is capable of representing\
    \ an internetwork as\n   clusters of entities at multiple levels of abstraction.\
    \  Clustering\n   reduces the number of entities visible to routing.  Abstraction\n\
    \   reduces the amount of information required to characterize an entity\n   visible\
    \ to routing.\n   Clustering begins by aggregating internetwork elements such\
    \ as hosts,\n   routers, and networks according to some predetermined criteria.\n\
    \   These elements may be clustered according to relationships among\n   them,\
    \ such as \"managed by the same authority\", or so as to satisfy\n   some objective\
    \ function, such as \"minimize the expected amount of\n   forwarding information\
    \ stored at each router\".  Nimrod does not\n   mandate a particular cluster formation\
    \ algorithm.\n   New clusters may be formed by clustering together existing clusters.\n\
    \   Repeated clustering of entities produces a hierarchy of clusters with\n  \
    \ a unique universal cluster that contains all others.  The same\n   clustering\
    \ algorithm need not be applied at each level in the\n   hierarchy.\n   All elements\
    \ within a cluster must satisfy at least one relation,\n   namely connectivity.\
    \  That is, if all elements within a cluster are\n   operational, then any two\
    \ of them must be connected by at least one\n   route that lies entirely within\
    \ that cluster.  This condition\n   prohibits the formation of certain types of\
    \ separated clusters, such\n   as the following.  Suppose that a company has two\
    \ branches located at\n   opposite ends of a country and that these two branches\
    \ must\n   communicate over a public network not owned by the company.  Then the\n\
    \   two branches cannot be members of the same cluster, unless that\n   cluster\
    \ also includes the public network connecting them.\n   Once the clusters are\
    \ formed, their connectivity and service\n   information is abstracted to reduce\
    \ the representation of cluster\n   characteristics.  Example abstraction procedures\
    \ include elimination\n   of services provided by a small fraction of the elements\
    \ in the\n   cluster or expression of services in terms of average values.  Nimrod\n\
    \   does not mandate a particular abstraction algorithm.  The same\n   abstraction\
    \ algorithm need not be applied to each cluster, and\n   multiple abstraction\
    \ algorithms may be applied to a single cluster.\n   A particular combination\
    \ of clustering and abstraction algorithms\n   applied to an internetwork results\
    \ in an organization related to but\n   distinct from the physical organization\
    \ of the component hosts,\n   routers, and networks.  When a clustering is superimposed\
    \ over the\n   physical internetwork elements, the cluster boundaries may not\n\
    \   necessarily coincide with host, router, or network boundaries.\n   Nimrod\
    \ performs its routing functions with respect to the hierarchy\n   of entities\
    \ resulting from clustering and abstraction, not with\n   respect to the physical\
    \ realization of the internetwork.  In fact,\n   Nimrod need not even be aware\
    \ of the physical elements of an\n   internetwork.\n"
- title: 2.3.2 Restricting Information Distribution
  contents:
  - "2.3.2 Restricting Information Distribution\n   The Nimrod architecture supports\
    \ restricted distribution of routing\n   information, both to reduce resource\
    \ consumption associated with such\n   distribution and to permit information\
    \ hiding.  Each cluster\n   determines the portions of its routing information\
    \ to distribute and\n   the set of entities to which to distribute this information.\n\
    \   Moreover, recipients of routing information are selective in which\n   information\
    \ they retain.  Some examples are as follows.  Each cluster\n   might automatically\
    \ advertise its routing information to its siblings\n   (i.e., those clusters\
    \ with a common parent cluster).  In response to\n   requests, a cluster might\
    \ advertise information about specific\n   portions of the cluster or information\
    \ that applies only to specific\n   users.  A cluster might only retain routing\
    \ information from clusters\n   that provide universal access to their services.\n"
- title: 2.3.3 Local Selection of Feasible Routes
  contents:
  - "2.3.3 Local Selection of Feasible Routes\n   Generating routes that satisfy multiple\
    \ constraints is usually an\n   NP-complete problem and hence a computationally\
    \ intensive procedure.\n   With Nimrod, only those entities that require routes\
    \ with special\n   constraints need assume the computational load associated with\n\
    \   generation and selection of such routes.  Moreover, the Nimrod\n   architecture\
    \ allows individual entities to choose their own route\n   generation and selection\
    \ algorithms and hence the amount of resources\n   to devote to these functions.\n"
- title: 2.3.4 Caching
  contents:
  - "2.3.4 Caching\n   The Nimrod architecture encourages caching of acquired routing\n\
    \   information in order to reduce the amount of resources consumed and\n   delay\
    \ incurred in obtaining the information in the future.  The set\n   of routes\
    \ generated as a by-product of generating a particular route\n   is an example\
    \ of routing information that is amenable to caching;\n   future requests for\
    \ any of these routes may be satisfied directly\n   from the route cache.  However,\
    \ as with any caching scheme, the\n   cached information may become stale and\
    \ its use may result in poor\n   quality routes.  Hence, the routing information's\
    \ expected duration\n   of usefulness must be considered when determining whether\
    \ to cache\n   the information and for how long.\n"
- title: 2.3.5 Limiting Forwarding Information
  contents:
  - "2.3.5 Limiting Forwarding Information\n   The Nimrod architecture supports two\
    \ separate approaches for\n   containing the amount of forwarding information\
    \ that must be\n   maintained per router.  The first approach is to multiplex,\
    \ over a\n   single path (or tree, for multicast), multiple traffic flows with\n\
    \   similar service requirements.  The second approach is to install and\n   retain\
    \ forwarding information only for active traffic flows.\n   With Nimrod, the service\
    \ providers and users share responsibility for\n   the amount of forwarding information\
    \ in an internetwork.  Users have\n   control over the establishment of paths,\
    \ and service providers have\n   control over the maintenance of paths.  This\
    \ approach is different\n   from that of the current Internet, where forwarding\
    \ information is\n   established in routers independent of demand for this information.\n"
- title: 3. Architecture
  contents:
  - "3. Architecture\n   Nimrod is a hierarchical, map-based routing architecture\
    \ that has\n   been designed to support a wide range of user requirements and\
    \ to\n   scale to very large dynamic internets.  Given a traffic stream's\n  \
    \ description and requirements (both quality of service requirements\n   and usage-restriction\
    \ requirements), Nimrod's main function is to\n   manage in a scalable fashion\
    \ how much information about the\n   internetwork is required to choose a route\
    \ for that stream, in other\n   words, to manage the trade-off between amount\
    \ of information about\n   the internetwork and the quality of the computed route.\
    \  Nimrod is\n   implemented as a set of protocols and distributed databases.\
    \  The\n   following sections describe the basic architectural concepts used in\n\
    \   Nimrod.  The protocols and databases are specified in other\n   documents.\n"
- title: 3.1 Endpoints
  contents:
  - "3.1 Endpoints\n   The basic entity in Nimrod is the endpoint.  An endpoint represents\
    \ a\n   user of the internetwork layer: for example, a transport connection.\n\
    \   Each endpoint has at least one endpoint identifier (EID). Any given\n   EID\
    \ corresponds to a single endpoint.  EIDs are globally unique,\n   relatively\
    \ short \"computer-friendly\" bit strings---for example, small\n   multiples of\
    \ 64 bits.  EIDs have no topological significance\n   whatsoever.  For ease of\
    \ management, EIDs might be organized\n   hierarchically, but this is not required.\n\
    \   BEGIN COMMENT\n      In practice, EIDs will probably have a second form, which\
    \ we can\n      call the endpoint label (EL). ELs are ASCII strings of unlimited\n\
    \      length, structured to be used as keys in a distributed database\n     \
    \ (much like DNS names).  Information about an endpoint---for\n      example,\
    \ how to reach it---can be obtained by querying this\n      distributed database\
    \ using the endpoint's label as key.\n   END COMMENT\n"
- title: 3.2 Nodes and Adjacencies
  contents:
  - "3.2 Nodes and Adjacencies\n   A node represents a region of the physical network.\
    \  The region of\n   the network represented by a node can be as large or as small\
    \ as\n   desired: a node can represent a continent or a process running inside\n\
    \   a host.  Moreover, as explained in section 4, a region of the network\n  \
    \ can simultaneously be represented by more than one node.\n   An adjacency consists\
    \ of an ordered pair of nodes.  An adjacency\n   indicates that traffic can flow\
    \ from the first node to the second.\n"
- title: 3.3 Maps
  contents:
  - "3.3 Maps\n   The basic data structure used for routing is the map.  A map\n \
    \  expresses the available connectivity between different points of an\n   internetwork.\
    \  Different maps can represent the same region of a\n   physical network at different\
    \ levels of detail.\n   A map is a graph composed of nodes and adjacencies.  Properties\
    \ of\n   nodes are contained in attributes associated with them.  Adjacencies\n\
    \   have no attributes.  Nimrod defines languages to specify attributes\n   and\
    \ to describe maps.\n   Maps are used by routers to generate routes.  In general,\
    \ it is not\n   required that different routers have consistent maps.\n   BEGIN\
    \ COMMENT\n      Nimrod has been designed so that there will be no routing loops\n\
    \      even when the routing databases of different routers are not\n      consistent.\
    \  A consistency requirement would not permit\n      representing the same region\
    \ of the internetwork at different\n      levels of detail.  Also, a routing-database\
    \ consistency\n      requirement would be hard to guarantee in the very large\
    \ internets\n      Nimrod is designed to support.\n   END COMMENT\n   In this\
    \ document we speak only of routers.  By \"router\" we mean a\n   physical device\
    \ that implements functions related to routing: for\n   example, forwarding, route\
    \ calculation, path set-up.  A given device\n   need not be capable of doing all\
    \ of these to be called a router.  The\n   protocol specification document, see\
    \ [2], splits these\n   functionalities into specific agents.\n"
- title: 3.3.1 Connectivity Specifications
  contents:
  - "3.3.1 Connectivity Specifications\n   By connectivity between two points we mean\
    \ the available services and\n   the restrictions on their use.  Connectivity\
    \ specifications are among\n   the attributes associated with nodes.  The following\
    \ are informal\n   examples of connectivity specifications:\n  o \"Between these\
    \ two points, there exists best-effort service with no\n    restrictions.\"\n\
    \  o \"Between these two points, guaranteed 10 ms delay can be arranged for\n\
    \    traffic streams whose data rate is below 1 Mbyte/sec and that have low\n\
    \    (specified) burstiness.\"\n  o \"Between these two points, best-effort service\
    \ is offered, as long as\n    the traffic originates in and is destined for research\
    \ organizations.\"\n"
- title: 3.4 Locators
  contents:
  - "3.4 Locators\n   A locator is a string of binary digits that identifies a location\
    \ in\n   an internetwork.  Nodes and endpoint are assigned locators.\n   Different\
    \ nodes have necessarily different locators.  A node is\n   assigned only one\
    \ locator.  Locators identify nodes and specify\n   *where* a node is in the network.\
    \  Locators do *not* specify a path\n   to the node.  An endpoint can be assigned\
    \ more than one locator.  In\n   this sense, a locator might appear in more than\
    \ one location of an\n   internetwork.\n   In this document locators are written\
    \ as ASCII strings that include\n   colons to underline node structure: for example,\
    \ a:b:c.  This does\n   not mean that the representation of locators in packets\
    \ or in\n   databases will necessarily have something equivalent to the colons.\n\
    \   A given physical element of the network might help implement more\n   than\
    \ one node---for example, a router might be part of two different\n   nodes. \
    \ Though this physical element might therefore be associated\n   with more than\
    \ one locator, the nodes that this physical element\n   implements have each only\
    \ one locator.\n   The connectivity specifications of a node are identified by\
    \ a tuple\n   consisting of the node's locator and an ID number.\n   All map information\
    \ is expressed in terms of locators, and routing\n   selections are based on locators.\
    \  EIDs are *not* used in making\n   routing decisions---see section 5.\n"
- title: 3.5 Node Attributes
  contents:
  - "3.5 Node Attributes\n   The following are node attributes defined by Nimrod.\n"
- title: 3.5.1 Adjacencies
  contents:
  - "3.5.1 Adjacencies\n   Adjacencies appear in maps as attributes of both the nodes\
    \ in the\n   adjacency.  A node has two types of adjacencies associated with it:\n\
    \   those that identify a neighboring node to which the original node can\n  \
    \ send data to; and those that identivy a neighboring node that can\n   send data\
    \ to the original node.\n"
- title: 3.5.2 Internal Maps
  contents:
  - "3.5.2 Internal Maps\n   As part of its attributes, a node can have internal maps.\
    \  A router\n   can obtain a node's internal maps---or any other of the node's\n\
    \   attributes, for that matter---by requesting that information from a\n   representative\
    \ of that node.  (A router associated with that node can\n   be such a representative.)\
    \  A node's representative can in principle\n   reply with different internal\
    \ maps to different requests---for\n   example, because of security concerns.\
    \  This implies that different\n   routers in the network might have different\
    \ internal maps for the\n   same node.\n   A node is said to own those locators\
    \ that have as a prefix the\n   locator of the node.  In a node that has an internal\
    \ map, the\n   locators of all nodes in this internal map are prefixed by the\n\
    \   locator of the original node.\n   Given a map, a more detailed map can be\
    \ obtained by substituting one\n   of the map's nodes by one of that node's internal\
    \ maps.  This process\n   can be continued recursively.  Nimrod defines standard\
    \ internal maps\n   that are intended to be used for specific purposes.  A node's\n\
    \   \"detailed map\" gives more information about the region of the network\n\
    \   represented by the original node.  Typically, it is closer to the\n   physical\
    \ realization of the network than the original node.  The\n   nodes of this map\
    \ can themselves have detailed maps.\n"
- title: 3.5.3 Transit Connectivity
  contents:
  - "3.5.3 Transit Connectivity\n   For a given node, this attribute specifies the\
    \ services available\n   between nodes adjacent to the given node.  This attribute\
    \ is\n   requested and used when a router intends to route traffic *through* a\n\
    \   node.  Conceptually, the traffic connectivity attribute is a matrix\n   that\
    \ is indexed by a pair of locators: the locators of adjacent\n   nodes.  The entry\
    \ indexed by such a pair contains the connectivity\n   specifications of the services\
    \ available across the given node for\n   traffic entering from the first node\
    \ and exiting to the second node.\n   The actual format of this attribute need\
    \ not be a matrix.  This\n   document does not specify the format for this attribute.\n"
- title: 3.5.4 Inbound Connectivity
  contents:
  - "3.5.4 Inbound Connectivity\n   For a given node, this attribute represents connectivity\
    \ from\n   adjacent nodes to points within the given node.  This attribute is\n\
    \   requested and used when a router intends to route traffic to a point\n   within\
    \ the node but does not have, and either cannot or does not want\n   to obtain,\
    \ a detailed map of the node.  The inbound connectivity\n   attribute identifies\
    \ what connectivity specifications are available\n   between pairs of locators.\
    \  The first element of the pair is the\n   locator of an adjacent node; the second\
    \ is a locator owned by the\n   given node.\n"
- title: 3.5.5 Outbound Connectivity
  contents:
  - "3.5.5 Outbound Connectivity\n   For a given node, this attribute represents connectivity\
    \ from points\n   within the given node to adjacent nodes.  This attribute identifies\n\
    \   what connectivity specifications are available between pairs of\n   locators.\
    \  The first element of the pair is a locator owned by the\n   given node, the\
    \ second is the locator of an adjacent node.\n   The Transit, Inbound and Outbound\
    \ connectivity attributes together\n   wiht a list of adjacencies form the \"\
    abstract map.\"\n"
- title: 4. Physical Realization
  contents:
  - "4. Physical Realization\n   A network is modeled as being composed of physical\
    \ elements: routers,\n   hosts, and communication links.  The links can be either\
    \ point-to-\n   point---e.g., T1 links---or multi-point---e.g., ethernets, X.25\n\
    \   networks, IP-only networks, etc.\n   The physical representation of a network\
    \ can have associated with it\n   one or more Nimrod maps.  A Nimrod map is a\
    \ function not only of the\n   physical network, but also of the configured clustering\
    \ of elements\n   (locator assignment) and of the configured connectivity.\n \
    \  Nimrod has no pre-defined \"lowest level\": for example, it is possible\n \
    \  to define and advertise a map that is physically realized inside a\n   CPU.\
    \ In this map, a node could represent, for example, a process or a\n   group of\
    \ processes.  The user of this map need not necessarily know\n   or care.  (\"\
    It is turtles all the way down!\", in [3] page 63.)\n"
- title: 4.1 Contiguity
  contents:
  - "4.1 Contiguity\n   Locators sharing a prefix must be assigned to a contiguous\
    \ region of\n   a map.  That is, two nodes in a map that have been assigned locators\n\
    \   sharing a prefix should be connected to each other via nodes that\n   themselves\
    \ have been assigned locators with that prefix.  The main\n   consequence of this\
    \ requirement is that \"you cannot take your locator\n   with you.\"\n   As an\
    \ example of this, see figure 1, consider two providers x.net and\n   y.net (these\
    \ designations are *not* locators but DNS names) which\n   appear in a Nimrod\
    \ map as two nodes with locators A and B. Assume\n   that corporation z.com (also\
    \ a DNS name) was originally connected to\n   x.net.  Locators corresponding to\
    \ elements in z.com are, in this\n   example, A-prefixed.  Corporation z.com decides\
    \ to change providers-\n   --severing its physical connection to x.net.  The connectivity\n\
    \   requirement described in this section implies that, after the\n   provider\
    \ change has taken place, elements in z.com will have been, in\n   this example,\
    \ assigned B-prefixed locators and that it is not\n   possible for them to receive\
    \ data destined to A-prefixed locators\n   through y.net.\n                  A\
    \                 B\n               +------+          +------+\n             \
    \  | x.net|          | y.net|\n               +------+         /+------+\n   \
    \                            /\n                        +-----+\n            \
    \            |z.com|\n                        +-----+\n             Figure 1:\
    \  Connectivity after switching providers\n   The contiguity requirement simplifies\
    \ routing information exchange:\n   if it were permitted for z.com to receive\
    \ A-prefixed locators through\n   y.net, it would be necessary that a map that\
    \ contains node B include\n   information about the existence of a group of A-prefixed\
    \ locators\n   inside node B. Similarly, a map including node A would have to\n\
    \   include information that the set of A-prefixed locators asigned to\n   z.com\
    \ is not to be found within A. The more situations like this\n   happen, the more\
    \ the hierarchical nature of Nimrod is subverted to\n   \"flat routing.\" The\
    \ contiguity requirement can also be expressed as\n   \"EIDs are stable; locators\
    \ are ephemeral.\"\n"
- title: 4.2 An Example
  contents:
  - "4.2 An Example\n   Figure 2 shows a physical network.  Hosts are drawn as squares,\n\
    \   routers as diamonds, and communication links as lines.  The network\n   shown\
    \ has the following components: five ethernets ---EA through EE;\n   five routers---RA\
    \ through RE; and four hosts---HA through HD. Routers\n   RA, RB, and RC interconnect\
    \ the backbone ethernets---EB, EC and ED.\n   Router RD connects backbone EC to\
    \ a network consisting of ethernet EA\n   and hosts HA and HB.  Router RE interconnects\
    \ backbone ED to a\n   network consisting of ethernet EE and hosts HC and HD.\
    \ The assigned\n   locators appear in lower case beside the corresponding physical\n\
    \   entity.\n   Figure 3 shows a Nimrod map for that network.  The nodes of the\
    \ map\n   are represented as squares.  Lines connecting nodes represent two\n\
    \   adjacencies in opposite directions.  Different regions of the network\n  \
    \ are represented at different detail.  Backbone b1 is represented as a\n   single\
    \ node.  The region of the network with locators prefixed by \"a\"\n   is represented\
    \ as a single node.  The region of the network with\n   locators prefixed by \"\
    c\" is represented in full detail.\n"
- title: 4.3 Multiple Locator Assignment
  contents:
  - "4.3 Multiple Locator Assignment\n   Physical elements can form part of, or implement,\
    \ more than one node.\n   In this sense it can be said that they can be assigned\
    \ more than one\n   locator.  Consider figure 4, which shows a physical network.\
    \  This\n   network is composed of routers (RA, RB, RC, and RD), hosts (HA, HB,\n\
    \   and HC), and communication links.  Routers RA, RB, and RC are\n   connected\
    \ with point-to-point links.  The two horizontal lines in the\n   bottom of the\
    \ figure represent ethernets.  The figure also shows the\n   locators assigned\
    \ to hosts and routers.\n   In figure 4, RA and RB have each been assigned one\
    \ locator (a:t:r1\n   and b:t:r1, respectively).  RC has been assigned locators\
    \ a:y:r1 and\n   b:d:r1; one of these two locators shares a prefix with RA's locator,\n\
    \   the other shares a prefix with RB's locator.  Hosts HA and HB have\n   each\
    \ been assigned three locators.  Host HC has been assigned one\n   locator.  Depending\
    \ on what communication paths have been set up\n   between points, different Nimrod\
    \ maps result.  A possible Nimrod map\n   for this network is given in figure\
    \ 5.\n                                             a:h1 +--+      a:h2 +--+\n\
    \                                                  |HA|           |HB|\n     \
    \                                             |  |           |  |\n          \
    \                                        +--+           +--+\n               \
    \                            a:e1    |              |\n                      \
    \                         --------------------- EA\n                         \
    \                              |\n                                 /\\       \
    \             /\\\n                                /RB\\ b1:r1            /RD\\\
    \ b2:r1\n                               /\\  /\\                 \\  /\n     \
    \                         /  \\/  \\                 \\/\n    EB         b1:t:e1\
    \       /        \\                 |   EC\n    ------------------------     \
    \     -------------------------- b2:e1\n               /                     \
    \        \\\n              /                               \\\n             /\\\
    \                                \\\n            /RA\\ b1:r2                 \
    \         \\/\\\n            \\  /                                /RC\\  b2:t:r2\n\
    \             \\/                                 \\  /\n               \\   \
    \                              \\/\n                \\                       \
    \        /   ED\n                  ----------------------------------- b3:t:e1\n\
    \                                    |\n                                    |\n\
    \                                    |\n                                   /\\\
    \n                                  /RE\\ b3:t:r1\n                          \
    \        \\  /\n                      EE           \\/\n                     \
    \ -----------------------------   c:e1\n                         |           \
    \        |\n                        +--+                +--+\n               \
    \         |HC|   c:h1         |HD|    c:h2\n                        |  |     \
    \           |  |\n                        +--+                +--+\n         \
    \           Figure 2:  Example Physical Network\n                            \
    \ +-----+               +-----+\n   +----------+              |     |        \
    \       |     |\n   |          |--------------| b2  | --------------| a   |\n\
    \   |          |              |     |               |     |\n   |    b1    | \
    \             +-----+               +-----+\n   |          |                 |\n\
    \   |          |                 |\n   |          |                 |\n   +----------+\
    \                 |\n               \\                |\n                \\  \
    \             |\n                 \\              |\n                  \\    \
    \         |\n                   \\         +--------+\n                    \\\
    \        |        |\n                     ------- | b3:t:e1|\n               \
    \              |        |\n                             +--------+\n         \
    \                       |\n                                |\n               \
    \                 |\n                                |\n                     \
    \        +-------+\n                             |       |\n                 \
    \            |b3:t:r1|\n                             |       |\n             \
    \                +-------+\n                                  |\n            \
    \     +-----+       +-----+     +-----+\n                 |     |       |    \
    \ |     |     |\n                 | c:h1|-------| c:e1|-----| c:h2|\n        \
    \         |     |       |     |     |     |\n                 +-----+       +-----+\
    \     +-----+\n                           Figure 3:  Nimrod Map\n            \
    \          a:t:r1              b:t:r1\n                         +--+         \
    \   +--+\n                         |RA|------------|RB|\n                    \
    \     +--+            +--+\n                           \\             /\n    \
    \                        \\           /\n                             \\     \
    \    /\n                              \\       /\n                           \
    \    \\     /\n                                \\   /\n                      \
    \           \\ /\n                                  \\\n                     \
    \            +--+\n                                 |RC|  a:y:r1\n           \
    \                      +--+  b:d:r1\n                                  |\n   \
    \                  ---------------------------\n                      |      \
    \  |             |\n             a:y:h1  +--+     +--+          +--+    a:y:h2\n\
    \             b:d:h2  |HA|     |RD| c:r1     |HB|    b:d:h1\n             c:h1\
    \    +--+     +--+          +--+    c:h2\n                                |\n\
    \                                |\n                         --------------------\n\
    \                                  |\n                                 +--+\n\
    \                                 |HC| c:h3\n                                \
    \ +--+\n                        Figure 4:  Multiple Locators\n           a   \
    \                    b                   c\n     +-------------+       +-------------+\
    \         +---------------+\n     |             |       |             |      \
    \   |               |\n     |        a:t  |       |      b:t    |         |  \
    \             |\n     |   +--+      |       |  +--+       |         |        \
    \       |\n     |   |  |--------------|--|  |       |         |              \
    \ |\n     |   +--+      |       |  +--+       |         |               |\n  \
    \   |     |       |       |    |        |         |               |\n     |  \
    \ +--+      |       |  +--+       |         |               |\n     |   +  + \
    \     |       |  +  +       |         |               |\n     |   +--+ a:y  |\
    \       |  +--+ b:d   |         |               |\n     |             |      \
    \ |             |         |               |\n     +-------------+       +-------------+\
    \         +---------------+\n                           Figure 5:  Nimrod Map\n\
    \   Nodes and adjacencies represent the *configured* clustering and\n   connectivity\
    \ of the network.  Notice that even though a:y and b:d are\n   defined on the\
    \ same hardware, the map shows no connection between\n   them: this connection\
    \ has not been configured.  A packet given to\n   node `a' addressed to a locator\
    \ prefixed with \"b:d\" would have to\n   travel from node a to node b via the\
    \ arc joining them before being\n   directed towards its destination.  Similarly,\
    \ the map shows no\n   connection between the c node and the other two top level\
    \ nodes.  If\n   desired, these connections could be established, which would\n\
    \   necessitate setting up the exchange of routing information.  Figure 6\n  \
    \ shows the map when these connections have been established.\n   In the strict\
    \ sense, Nimrod nodes do not overlap: they are distinct\n   entities.  But, as\
    \ we have seen in the previous example, a physical\n   element can be given more\
    \ than one locator, and, in that sense,\n   participate in implementing more than\
    \ one node.  That is, two\n   different nodes might be defined on the same hardware.\
    \  In this\n   sense, Nimrod nodes can be said to overlap.  But to notice this\n\
    \   overlap one would have to know the physical-to-map correspondence.\n   It\
    \ is not possible to know when two nodes share physical assets by\n   looking\
    \ only at a Nimrod map.\n"
- title: 5. Forwarding
  contents:
  - "5. Forwarding\n   Nimrod supports four forwarding modes:\n 1. Connectivity Specification\
    \ Chain (CSC) mode: In this mode, packets\n    carry a list of connectivity specifications.\
    \  The packet is\n    required to go through the nodes that own the connectivity\n\
    \    specifications using the services specified.  The nodes associated\n    with\
    \ the listed connectivity specifications should define a\n    continuous path\
    \ in the map.  A more detailed description of the\n    requirements of this mode\
    \ is given in section 5.3.\n   +--------+                                    \
    \           +--------+\n   |        |                                        \
    \       |        |\n   | a:t:r1 |-----------------------------------------------|\
    \ b:t:r1 |\n   |        |                                               |    \
    \    |\n   +--------+                                               +--------+\n\
    \     |                                                             |\n     |\
    \                                                             |\n     |      \
    \   /-----------------------------------------\\         |\n     |         | \
    \                                        |         |\n     |         |       \
    \                                  |         |\n     |  +--------+       +--------+\
    \                    +--------+  |\n     |  |        |       |        |      \
    \              |        |  |\n     |  | a:y:h1 --------|  c:h1  |--------------------|\
    \ b:d:h1 |  |\n     |  |        |       |        |                    |      \
    \  |  |\n     |  +--------+       +--------+                    +--------+  |\n\
    \     |    |    |           |    |                        |    |    |\n   +--------+\
    \  |           |  +------+  +------+         |  +--------+\n   |        |  | \
    \          |  |      |  |      |         |  |        |\n   | a:y:r1 |  |     \
    \      |  | c:r1 |--| c:h3 |         |  | b:d:r1 |\n   |        |  |         \
    \  |  |      |  |      |         |  |        |\n   +--------+  |           | \
    \ +------+  +------+         |  +--------+\n     |    |    |           |    |\
    \                        |    |    |\n     |  +--------+       +--------+    \
    \                +--------+  |\n     |  |        |       |        |          \
    \          |        |  |\n     |  | a:y:h2 |--------  c:h2  |--------------------|\
    \ b:d:h2 |  |\n     |  |        |       |        |                    |      \
    \  |  |\n     |  +--------+       +--------+                    +--------+  |\n\
    \     |         |                                         |         |\n     |\
    \         |                                         |         |\n     |      \
    \   |                                         |         |\n     |         \\-----------------------------------------/\
    \         |\n     \\-------------------------------------------------------------/\n\
    \                          Figure 6:  Nimrod Map II\n 2. Connectivity Specifications\
    \ Sequence (CSS) mode: In this mode,\n    packets carry a list of connectivity\
    \ specifications.  The packet\n    is supposed to go sequentially through the\
    \ nodes that own each one\n    of the listed connectivity specifications in the\
    \ order they were\n    specified.  The nodes need not be adjacent.  This mode\
    \ can be seen\n    as a generalization of the CSC mode.  Notice that CSCs are\
    \ said to\n    be a *chains* of locators, CSSs are *sequences* of locators.  This\n\
    \    difference emphasizes the contiguity requirement in CSCs.  A\n    detailed\
    \ description of this mode is in section 5.6.\n 3. Flow mode: In this mode, the\
    \ packet includes a path-id that\n    indexes state that has been previously set\
    \ up in routers along the\n    path.  Packet forwarding when flow state has been\
    \ established is\n    relatively simple: follow the instructions in the routers'\
    \ state.\n    Nimrod includes a mechanism for setting up this state.  A more\n\
    \    detailed description of this mode can be found in section 5.4.\n 4. Datagram\
    \ mode: in this mode, every packet carries source and\n    destination locators.\
    \  This mode can be seen as a special case of\n    the CSS mode.  Forwarding is\
    \ done following procedures as\n    indicated in section 5.5.\n    BEGIN COMMENT\n\
    \    The obvious parallels are between CSC mode and IPV4's strict\n    source\
    \ route and between CSS mode and IPV4's loose source route.\n    END COMMENT\n\
    \   In all of these modes, the packet may also carry locators and EIDs\n   for\
    \ the source and destinations.  In normal operation, forwarding\n   does not take\
    \ the EIDs into account, only the receiver does.  EIDs\n   may be carried for\
    \ demultiplexing at the receiver, and to detect\n   certain error conditions.\
    \  For example, if the EID is unknown at the\n   receiver, the locator and EID\
    \ of the source included in the packet\n   could be used to generate an error\
    \ message to return to the source\n   (as usual, this error message itself should\
    \ probably not be allowed\n   to be the cause of other error messages).  Forwarding\
    \ can also use\n   the source locator and EID to respond to error conditions,\
    \ for\n   example, to indicate to the source that the state for a path-id\n  \
    \ cannot be found.\n   Packets can be visualized as moving between nodes in a\
    \ map.  A packet\n   indicates, implicitly or explicitly, a destination locator.\
    \  In a\n   packet that uses the datagram, CSC, or CSS forwarding mode, the\n\
    \   destination locator is explicitly indicated .  In a packet that uses\n   the\
    \ flow forwarding mode, the destination locator is implied by the\n   path-id\
    \ and the distributed state in the network (it might also be\n   included explicitly).\
    \  Given a map, a packet moves to the node in\n   this map to which the associated\
    \ destination locator belongs.  If the\n   destination node has a \"detailed\"\
    \ internal map, the destination\n   locator must belong to one of the nodes in\
    \ this internal map\n   (otherwise it is an error).  The packet goes to this node\
    \ (and so on,\n   recursively).\n"
- title: 5.1 Policy
  contents:
  - "5.1 Policy\n   CSC and CSS mode implement policy by specifying the connectivity\n\
    \   specifications associated with those nodes that the packet should\n   traverse.\
    \  Strictly speaking, there is no policy information included\n   in the packet.\
    \  That is, in principle, it is not possible to\n   determine what criteria were\
    \ used to select the route by looking at\n   the packet.  The packet only contains\
    \ the results of the route\n   generation process.  Similarly, in a flow mode\
    \ packet, policy is\n   implicit in the chosen route.\n   A datagram-mode packet\
    \ can indicate a limited form of policy routing\n   by the choice of destination\
    \ and source locators.  For this choice to\n   exist, the source or destination\
    \ endpoints must have several locators\n   associated with them.  This type of\
    \ policy routing is capable of, for\n   example, choosing providers.\n"
- title: 5.2 Trust
  contents:
  - "5.2 Trust\n   A node that chooses not to divulge its internal map can work\n\
    \   internally any way its administrators decide, as long as the node\n   satisfies\
    \ its external characterization as given in its Nimrod map\n   advertisements.\
    \  Therefore, the advertised Nimrod map should be\n   consistent with a node's\
    \ actual capabilities.  For example, consider\n   the network shown in figure\
    \ 7 which shows a physical network and the\n   advertised Nimrod map.  The physical\
    \ network consists of hosts and a\n   router connected together by an ethernet.\
    \  This node can be sub-\n   divided into component nodes by assigning locators\
    \ as shown in the\n   figure and advertising the map shown.  The map seems to\
    \ imply that it\n   is possible to send packets to node a:x without these being\n\
    \   observable by node a:y; however, this is actually not enforceable.\n   In\
    \ general, it is reasonable to ask how much trust should be put in\n   the maps\
    \ obtained by a router.  Even when a node is \"trustworthy,\"\n   and the information\
    \ received from the node has been authenticated,\n   there is always the possibility\
    \ of an honest mistake.\n                                 +--+\n             \
    \                    |RA| a:r1\n                                 +--+\n      \
    \                            |\n                                  |\n        \
    \                          |\n                                  |\n          \
    \           -------------------------------\n                         |      \
    \                 |\n                        +--+                    +--+\n  \
    \                      |Ha| a:x:h1             |Ha| a:y:h2\n                 \
    \       +--+                    +--+\n                               Physical\
    \ Network\n                      a             |\n                   +----------------|--------------------\n\
    \                   |                |                   |\n                 \
    \  |              +----+                |\n                   |              |a:r1|\
    \                |\n                   |   a:x        +----+  a:y           |\n\
    \                   |   +------+  /      \\ +-------+     |\n                \
    \   |   |      | /        \\|       |     |\n                   |   |      | \
    \          |       |     |\n                   |   |      |           |      \
    \ |     |\n                   |   +------+           +-------+     |\n       \
    \            |                                    |\n                   + -----------------------------------+\n\
    \                               Advertised Nimrod Map\n                    Figure\
    \ 7:  Example of Misleading Map\n"
- title: 5.3 Connectivity Specification (CSC) Mode
  contents:
  - "5.3 Connectivity Specification (CSC) Mode\n   Routing for a CSC packet is specified\
    \ by a list of connectivity\n   specifications carried in the packet.  These are\
    \ the connectivity\n   specifications that make the specified path, in the order\
    \ that they\n   appear along the path.  These connectivity specifications are\n\
    \   attributes of nodes.  The route indicated by a CSC packet is specifed\n  \
    \ in terms of connectivity specifications rather than physical\n   entities: \
    \ a connectivity specification in a CSC-mode packet would\n   correspond to a\
    \ type of service between two points of the network\n   without specifying the\
    \ physical path.\n   Given two connectivity specifications that appear consecutively\
    \ in\n   the a CSC-mode packet, there should exist an adjacency going from the\n\
    \   node corresponding to the first connectivity specification to the\n   node\
    \ corresponding to the second connectivity specification.  The\n   first connectivity\
    \ specification referenced in a CSC-mode packet\n   should be an outbound connectivity\
    \ specification; similarly, the last\n   connectivity specification referenced\
    \ in a CSC-mode packet should be\n   an inbound connectivity specification; the\
    \ rest should be transit\n   connectivity specifications.\n"
- title: 5.4 Flow Mode
  contents:
  - "5.4 Flow Mode\n   A flow mode packet includes a path-id field.  This field identifies\n\
    \   state that has been established in intermediate routers.  The packet\n   might\
    \ also contain locators and EIDs for the source and destination.\n   The setup\
    \ packet also includes resource requirements.  Nimrod\n   includes protocols to\
    \ set up and modify flow-related state in\n   intermediate routers.  These protocols\
    \ not only identify the\n   requested route, but also describe the resources requested\
    \ by the\n   flow---e.g., bandwidth, delay, etc.  The result of a set-up attempt\n\
    \   might be either confirmation of the set-up or notification of its\n   failure.\
    \  The source-specified routes in flow mode setup are\n   specified in terms of\
    \ CSSs.\n"
- title: 5.5 Datagram Mode
  contents:
  - "5.5 Datagram Mode\n   A realistic routing architecture must include an optimization\
    \ for\n   datagram traffic, by which we mean user transactions which consist of\n\
    \   single packets, such as a lookup in a remote translation database.\n   Either\
    \ of the two previous modes contains unacceptable overhead if\n   much of the\
    \ network traffic consists of such datagram transactions.\n   A mechanism is needed\
    \ which is approximately as efficient as the\n   existing IPv4 \"hop-by-hop\"\
    \ mechanism.  Nimrod has such a mechanism.\n   The scheme can be characterized\
    \ by the way it divides the state in a\n   datagram network between routers and\
    \ the actual packets.  In IPv4,\n   most packets currently contain only a small\
    \ amount of state\n   associated with the forwarding process (\"forwarding state\"\
    )---the hop\n   count.  Nimrod proposes that enlarging the amount of forwarding\
    \ state\n   in packets can produce a system with useful properties.  It was\n\
    \   partially inspired by the efficient source routing mechanism in SIP\n   [5],\
    \ and the locator pointer mechanism in PIP [6]).\n   Nimrod datagram mode uses\
    \ pre-set flow-mode state to support a\n   strictly non-looping path, but without\
    \ a source-route.\n"
- title: 5.6 Connectivity Specification Sequence Mode
  contents:
  - "5.6 Connectivity Specification Sequence Mode\n   The connectivity specification\
    \ sequence mode specifies a route by a\n   list of connectivity specifications.\
    \  There are no contiguity\n   restrictions on consecutive connectivity specifications.\n\
    \    BEGIN COMMENT\n    The CSS and CSC modes can be seen as combination of the\
    \ datagram\n    and flow modes.  Therefore, in a sense, the basic forwarding modes\n\
    \    of Nimrod are just these last two.\n    END COMMENT\n"
- title: 6. Security Considerations
  contents:
  - "6. Security Considerations\n   Security issues are not addressed in this document.\n"
- title: 7. References
  contents:
  - "7. References\n   [1] Steenstrup, M., \"Inter-Domain Policy Routing Protocol\n\
    \       Specification: Version 1,\" RFC 1479, June 1993.\n   [2] Steenstrup M.,\
    \ and R. Ramanathan, \"Nimrod Functionality and\n       Protocols Specification,\"\
    \ Work in Progress, February 1996.\n   [3] Wright, R., \"Three Scientists and\
    \ their Gods Looking for Meaning\n       in an Age of Information\", New York:\
    \ Times Book, first ed., 1988.\n   [4] Deering, S., \"SIP: Simple Internet Protocol,\"\
    \ IEEE Network, vol.\n       7, May 1993.\n   [5] Francis, P., \"A Near-Term Architecture\
    \ for Deploying Pip,\" IEEE\n       Network, vol. 7, May 1993.\n"
- title: 8. Authors' Addresses
  contents:
  - "8. Authors' Addresses\n   Isidro Castineyra\n   BBN Systems and Technologies\n\
    \   10 Moulton Street\n   Cambridge, MA 02138\n   Phone:  (617) 873-6233\n   EMail:\
    \  isidro@bbn.com\n   Noel Chiappa\n   EMail:  gnc@ginger.lcs.mit.edu\n   Martha\
    \ Steenstrup\n   BBN Systems and Technologies\n   10 Moulton Street\n   Cambridge,\
    \ MA 02138\n   Phone:  (617) 873-3192\n   EMail:  msteenst@bbn.com\n"
