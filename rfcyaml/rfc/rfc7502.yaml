- title: __initial_text__
  contents:
  - ''
- title: 'Methodology for Benchmarking Session Initiation Protocol (SIP) Devices:'
  contents:
  - "Methodology for Benchmarking Session Initiation Protocol (SIP) Devices:\n   \
    \               Basic Session Setup and Registration\n"
- title: Abstract
  contents:
  - "Abstract\n   This document provides a methodology for benchmarking the Session\n\
    \   Initiation Protocol (SIP) performance of devices.  Terminology\n   related\
    \ to benchmarking SIP devices is described in the companion\n   terminology document\
    \ (RFC 7501).  Using these two documents,\n   benchmarks can be obtained and compared\
    \ for different types of\n   devices such as SIP Proxy Servers, Registrars, and\
    \ Session Border\n   Controllers.  The term \"performance\" in this context means\
    \ the\n   capacity of the Device Under Test (DUT) to process SIP messages.\n \
    \  Media streams are used only to study how they impact the signaling\n   behavior.\
    \  The intent of the two documents is to provide a normalized\n   set of tests\
    \ that will enable an objective comparison of the capacity\n   of SIP devices.\
    \  Test setup parameters and a methodology are\n   necessary because SIP allows\
    \ a wide range of configurations and\n   operational conditions that can influence\
    \ performance benchmark\n   measurements.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7502.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2015 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   4\n   2.  Terminology . . . . . . . . . . . . . . . . . . . . .\
    \ . . . .   5\n   3.  Benchmarking Topologies . . . . . . . . . . . . . . . .\
    \ . . .   5\n   4.  Test Setup Parameters . . . . . . . . . . . . . . . . . .\
    \ . .   7\n     4.1.  Selection of SIP Transport Protocol . . . . . . . . . .\
    \ .   7\n     4.2.  Connection-Oriented Transport Management  . . . . . . . .\
    \   7\n     4.3.  Signaling Server  . . . . . . . . . . . . . . . . . . . .  \
    \ 7\n     4.4.  Associated Media  . . . . . . . . . . . . . . . . . . . .   8\n\
    \     4.5.  Selection of Associated Media Protocol  . . . . . . . . .   8\n  \
    \   4.6.  Number of Associated Media Streams per SIP Session  . . .   8\n    \
    \ 4.7.  Codec Type  . . . . . . . . . . . . . . . . . . . . . . .   8\n     4.8.\
    \  Session Duration  . . . . . . . . . . . . . . . . . . . .   8\n     4.9.  Attempted\
    \ Sessions per Second (sps) . . . . . . . . . . .   8\n     4.10. Benchmarking\
    \ Algorithm  . . . . . . . . . . . . . . . . .   9\n   5.  Reporting Format  .\
    \ . . . . . . . . . . . . . . . . . . . . .  11\n     5.1.  Test Setup Report\
    \ . . . . . . . . . . . . . . . . . . . .  11\n     5.2.  Device Benchmarks for\
    \ Session Setup . . . . . . . . . . .  12\n     5.3.  Device Benchmarks for Registrations\
    \ . . . . . . . . . . .  12\n   6.  Test Cases  . . . . . . . . . . . . . . .\
    \ . . . . . . . . . .  13\n     6.1.  Baseline Session Establishment Rate of the\
    \ Testbed  . . .  13\n     6.2.  Session Establishment Rate without Media  . .\
    \ . . . . . .  13\n     6.3.  Session Establishment Rate with Media Not on DUT\
    \  . . . .  13\n     6.4.  Session Establishment Rate with Media on DUT  . . .\
    \ . . .  14\n     6.5.  Session Establishment Rate with TLS-Encrypted SIP . .\
    \ . .  14\n     6.6.  Session Establishment Rate with IPsec-Encrypted SIP . .\
    \ .  15\n     6.7.  Registration Rate . . . . . . . . . . . . . . . . . . . .\
    \  15\n     6.8.  Re-registration Rate  . . . . . . . . . . . . . . . . . .  16\n\
    \   7.  Security Considerations . . . . . . . . . . . . . . . . . . .  16\n  \
    \ 8.  References  . . . . . . . . . . . . . . . . . . . . . . . . .  17\n    \
    \ 8.1.  Normative References  . . . . . . . . . . . . . . . . . .  17\n     8.2.\
    \  Informative References  . . . . . . . . . . . . . . . . .  17\n   Appendix\
    \ A.  R Code Component to Simulate Benchmarking Algorithm   18\n   Acknowledgments\
    \ . . . . . . . . . . . . . . . . . . . . . . . . .  20\n   Authors' Addresses\
    \  . . . . . . . . . . . . . . . . . . . . . . .  21\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document describes the methodology for benchmarking\
    \ Session\n   Initiation Protocol (SIP) performance as described in the Terminology\n\
    \   document [RFC7501].  The methodology and terminology are to be used\n   for\
    \ benchmarking signaling plane performance with varying signaling\n   and media\
    \ load.  Media streams, when used, are used only to study how\n   they impact\
    \ the signaling behavior.  This document concentrates on\n   benchmarking SIP\
    \ session setup and SIP registrations only.\n   The Device Under Test (DUT) is\
    \ a network intermediary that is RFC\n   3261 [RFC3261] capable and that plays\
    \ the role of a registrar,\n   redirect server, stateful proxy, a Session Border\
    \ Controller (SBC) or\n   a B2BUA.  This document does not require the intermediary\
    \ to assume\n   the role of a stateless proxy.  Benchmarks can be obtained and\n\
    \   compared for different types of devices such as a SIP proxy server,\n   Session\
    \ Border Controllers (SBC), SIP registrars and a SIP proxy\n   server paired with\
    \ a media relay.\n   The test cases provide metrics for benchmarking the maximum\
    \ 'SIP\n   Registration Rate' and maximum 'SIP Session Establishment Rate' that\n\
    \   the DUT can sustain over an extended period of time without failures\n   (extended\
    \ period of time is defined in the algorithm in\n   Section 4.10).  Some cases\
    \ are included to cover encrypted SIP.  The\n   test topologies that can be used\
    \ are described in the Test Setup\n   section.  Topologies in which the DUT handles\
    \ media as well as those\n   in which the DUT does not handle media are both considered.\
    \  The\n   measurement of the performance characteristics of the media itself\
    \ is\n   outside the scope of these documents.\n   Benchmark metrics could possibly\
    \ be impacted by Associated Media.\n   The selected values for Session Duration\
    \ and Media Streams per\n   Session enable benchmark metrics to be benchmarked\
    \ without Associated\n   Media.  Session Setup Rate could possibly be impacted\
    \ by the selected\n   value for Maximum Sessions Attempted.  The benchmark for\
    \ Session\n   Establishment Rate is measured with a fixed value for maximum Session\n\
    \   Attempts.\n   Finally, the overall value of these tests is to serve as a comparison\n\
    \   function between multiple SIP implementations.  One way to use these\n   tests\
    \ is to derive benchmarks with SIP devices from Vendor-A, derive\n   a new set\
    \ of benchmarks with similar SIP devices from Vendor-B and\n   perform a comparison\
    \ on the results of Vendor-A and Vendor-B.  This\n   document does not make any\
    \ claims on the interpretation of such\n   results.\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   In this document, the key words \"MUST\", \"MUST NOT\", \"\
    REQUIRED\",\n   \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\"\
    , \"NOT\n   RECOMMENDED\", \"MAY\", and \"OPTIONAL\" are to be interpreted as\n\
    \   described in BCP 14, conforming to [RFC2119] and indicate requirement\n  \
    \ levels for compliant implementations.\n   RFC 2119 defines the use of these\
    \ key words to help make the intent\n   of Standards Track documents as clear\
    \ as possible.  While this\n   document uses these keywords, this document is\
    \ not a Standards Track\n   document.\n   Terms specific to SIP [RFC3261] performance\
    \ benchmarking are defined\n   in [RFC7501].\n"
- title: 3.  Benchmarking Topologies
  contents:
  - "3.  Benchmarking Topologies\n   Test organizations need to be aware that these\
    \ tests generate large\n   volumes of data and consequently ensure that networking\
    \ devices like\n   hubs, switches, or routers are able to handle the generated\
    \ volume.\n   The test cases enumerated in Sections 6.1 to 6.6 operate on two\
    \ test\n   topologies: one in which the DUT does not process the media\n   (Figure\
    \ 1) and the other in which it does process media (Figure 2).\n   In both cases,\
    \ the tester or Emulated Agent (EA) sends traffic into\n   the DUT and absorbs\
    \ traffic from the DUT.  The diagrams in Figures 1\n   and 2 represent the logical\
    \ flow of information and do not dictate a\n   particular physical arrangement\
    \ of the entities.\n   Figure 1 depicts a layout in which the DUT is an intermediary\
    \ between\n   the two interfaces of the EA.  If the test case requires the exchange\n\
    \   of media, the media does not flow through the DUT but rather passes\n   directly\
    \ between the two endpoints.  Figure 2 shows the DUT as an\n   intermediary between\
    \ the two interfaces of the EA.  If the test case\n   requires the exchange of\
    \ media, the media flows through the DUT\n   between the endpoints.\n      +--------+\
    \   Session   +--------+  Session    +--------+\n      |        |   Attempt  \
    \ |        |  Attempt    |        |\n      |        |------------>+        |------------>+\
    \        |\n      |        |             |        |             |        |\n \
    \     |        |   Response  |        |  Response   |        |\n      | Tester\
    \ +<------------|  DUT   +<------------| Tester |\n      |  (EA)  |          \
    \   |        |             |  (EA)  |\n      |        |             |        |\
    \             |        |\n      +--------+             +--------+            \
    \ +--------+\n         /|\\                                            /|\\\n\
    \          |              Media (optional)                |\n          +==============================================+\n\
    \            Figure 1: DUT as an Intermediary, End-to-End Media\n      +--------+\
    \   Session   +--------+  Session    +--------+\n      |        |   Attempt  \
    \ |        |  Attempt    |        |\n      |        |------------>+        |------------>+\
    \        |\n      |        |             |        |             |        |\n \
    \     |        |   Response  |        |  Response   |        |\n      | Tester\
    \ +<------------|  DUT   +<------------| Tester |\n      |  (EA)  |          \
    \   |        |             |  (EA)  |\n      |        |<===========>|        |<===========>|\
    \        |\n      +--------+   Media     +--------+    Media    +--------+\n \
    \                (Optional)             (Optional)\n             Figure 2: DUT\
    \ as an Intermediary Forwarding Media\n   The test cases enumerated in Sections\
    \ 6.7 and 6.8 use the topology in\n   Figure 3 below.\n      +--------+ Registration\
    \ +--------+\n      |        |   request    |        |\n      |        |------------->+\
    \        |\n      |        |              |        |\n      |        |   Response\
    \   |        |\n      | Tester +<-------------|  DUT   |\n      |  (EA)  |   \
    \           |        |\n      |        |              |        |\n      +--------+\
    \              +--------+\n             Figure 3: Registration and Re-registration\
    \ Tests\n   During registration or re-registration, the DUT may involve backend\n\
    \   network elements and data stores.  These network elements and data\n   stores\
    \ are not shown in Figure 3, but it is understood that they will\n   impact the\
    \ time required for the DUT to generate a response.\n   This document explicitly\
    \ separates a registration test (Section 6.7)\n   from a re-registration test\
    \ (Section 6.8) because in certain\n   networks, the time to re-register may vary\
    \ from the time to perform\n   an initial registration due to the backend processing\
    \ involved.  It\n   is expected that the registration tests and the re-registration\
    \ test\n   will be performed with the same set of backend network elements in\n\
    \   order to derive a stable metric.\n"
- title: 4.  Test Setup Parameters
  contents:
  - '4.  Test Setup Parameters

    '
- title: 4.1.  Selection of SIP Transport Protocol
  contents:
  - "4.1.  Selection of SIP Transport Protocol\n   Test cases may be performed with\
    \ any transport protocol supported by\n   SIP.  This includes, but is not limited\
    \ to, TCP, UDP, TLS, and\n   websockets.  The protocol used for the SIP transport\
    \ protocol must be\n   reported with benchmarking results.\n   SIP allows a DUT\
    \ to use different transports for signaling on either\n   side of the connection\
    \ to the EAs.  Therefore, this document assumes\n   that the same transport is\
    \ used on both sides of the connection; if\n   this is not the case in any of\
    \ the tests, the transport on each side\n   of the connection MUST be reported\
    \ in the test-reporting template.\n"
- title: 4.2.  Connection-Oriented Transport Management
  contents:
  - "4.2.  Connection-Oriented Transport Management\n   SIP allows a device to open\
    \ one connection and send multiple requests\n   over the same connection (responses\
    \ are normally received over the\n   same connection that the request was sent\
    \ out on).  The protocol also\n   allows a device to open a new connection for\
    \ each individual request.\n   A connection management strategy will have an impact\
    \ on the results\n   obtained from the test cases, especially for connection-oriented\n\
    \   transports such as TLS.  For such transports, the cryptographic\n   handshake\
    \ must occur every time a connection is opened.\n   The connection management\
    \ strategy, i.e., use of one connection to\n   send all requests or closing an\
    \ existing connection and opening a new\n   connection to send each request, MUST\
    \ be reported with the\n   benchmarking result.\n"
- title: 4.3.  Signaling Server
  contents:
  - "4.3.  Signaling Server\n   The Signaling Server is defined in the companion terminology\
    \ document\n   ([RFC7501], Section 3.2.2).  The Signaling Server is a DUT.\n"
- title: 4.4.  Associated Media
  contents:
  - "4.4.  Associated Media\n   Some tests require Associated Media to be present\
    \ for each SIP\n   session.  The test topologies to be used when benchmarking\
    \ DUT\n   performance for Associated Media are shown in Figure 1 and Figure 2.\n"
- title: 4.5.  Selection of Associated Media Protocol
  contents:
  - "4.5.  Selection of Associated Media Protocol\n   The test cases specified in\
    \ this document provide SIP performance\n   independent of the protocol used for\
    \ the media stream.  Any media\n   protocol supported by SIP may be used.  This\
    \ includes, but is not\n   limited to, RTP and SRTP.  The protocol used for Associated\
    \ Media\n   MUST be reported with benchmarking results.\n"
- title: 4.6.  Number of Associated Media Streams per SIP Session
  contents:
  - "4.6.  Number of Associated Media Streams per SIP Session\n   Benchmarking results\
    \ may vary with the number of media streams per\n   SIP session.  When benchmarking\
    \ a DUT for voice, a single media\n   stream is used.  When benchmarking a DUT\
    \ for voice and video, two\n   media streams are used.  The number of Associated\
    \ Media Streams MUST\n   be reported with benchmarking results.\n"
- title: 4.7.  Codec Type
  contents:
  - "4.7.  Codec Type\n   The test cases specified in this document provide SIP performance\n\
    \   independent of the media stream codec.  Any codec supported by the\n   EAs\
    \ may be used.  The codec used for Associated Media MUST be\n   reported with\
    \ the benchmarking results.\n"
- title: 4.8.  Session Duration
  contents:
  - "4.8.  Session Duration\n   The value of the DUT's performance benchmarks may\
    \ vary with the\n   duration of SIP sessions.  Session Duration MUST be reported\
    \ with\n   benchmarking results.  A Session Duration of zero seconds indicates\n\
    \   transmission of a BYE immediately following a successful SIP\n   establishment.\
    \  Setting this parameter to the value '0' indicates\n   that a BYE will be sent\
    \ by the EA immediately after the EA receives a\n   200 OK to the INVITE.  Setting\
    \ this parameter to a time value greater\n   than the duration of the test indicates\
    \ that a BYE will never be\n   sent.  Setting this parameter to a time value greater\
    \ than the\n   duration of the test indicates that a BYE is never sent.\n"
- title: 4.9.  Attempted Sessions per Second (sps)
  contents:
  - "4.9.  Attempted Sessions per Second (sps)\n   The value of the DUT's performance\
    \ benchmarks may vary with the\n   Session Attempt Rate offered by the tester.\
    \  Session Attempt Rate\n   MUST be reported with the benchmarking results.\n\
    \   The test cases enumerated in Sections 6.1 to 6.6 require that the EA\n   is\
    \ configured to send the final 2xx-class response as quickly as it\n   can.  This\
    \ document does not require the tester to add any delay\n   between receiving\
    \ a request and generating a final response.\n"
- title: 4.10.  Benchmarking Algorithm
  contents:
  - "4.10.  Benchmarking Algorithm\n   In order to benchmark the test cases uniformly\
    \ in Section 6, the\n   algorithm described in this section should be used.  A\
    \ prosaic\n   description of the algorithm and a pseudocode description are\n\
    \   provided below, and a simulation written in the R statistical\n   language\
    \ [Rtool] is provided in Appendix A.\n   The goal is to find the largest value,\
    \ R, a SIP Session Attempt Rate,\n   measured in sessions per second (sps), which\
    \ the DUT can process with\n   zero errors over a defined, extended period.  This\
    \ period is defined\n   as the amount of time needed to attempt N SIP sessions,\
    \ where N is a\n   parameter of test, at the attempt rate, R.  An iterative process\
    \ is\n   used to find this rate.  The algorithm corresponding to this process\n\
    \   converges to R.\n   If the DUT vendor provides a value for R, the tester can\
    \ use this\n   value.  In cases where the DUT vendor does not provide a value\
    \ for R,\n   or where the tester wants to establish the R of a system using local\n\
    \   media characteristics, the algorithm should be run by setting \"r\",\n   the\
    \ session attempt rate, equal to a value of the tester's choice.\n   For example,\
    \ the tester may initialize \"r = 100\" to start the\n   algorithm and observe\
    \ the value at convergence.  The algorithm\n   dynamically increases and decreases\
    \ \"r\" as it converges to the\n   maximum sps value for R.  The dynamic increase\
    \ and decrease rate is\n   controlled by the weights \"w\" and \"d\", respectively.\n\
    \   The pseudocode corresponding to the description above follows, and a\n   simulation\
    \ written in the R statistical language is provided in\n   Appendix A.\n     \
    \    ; ---- Parameters of test; adjust as needed\n         N  := 50000  ; Global\
    \ maximum; once largest session rate has\n                      ; been established,\
    \ send this many requests before\n                      ; calling the test a success\n\
    \         m  := {...}  ; Other attributes that affect testing, such\n        \
    \              ; as media streams, etc.\n         r  := 100    ; Initial session\
    \ attempt rate (in sessions/sec).\n                      ; Adjust as needed (for\
    \ example, if DUT can handle\n                      ; thousands of calls in steady\
    \ state, set to\n                      ; appropriate value in the thousands).\n\
    \         w  := 0.10   ; Traffic increase weight (0 < w <= 1.0)\n         d  :=\
    \ max(0.10, w / 2)    ; Traffic decrease weight\n         ; ---- End of parameters\
    \ of test\n         proc find_R\n            R = max_sps(r, m, N)  ; Setup r sps,\
    \ each with m media\n            ; characteristics until N sessions have been\
    \ attempted.\n            ; Note that if a DUT vendor provides this number, the\
    \ tester\n            ; can use the number as a Session Attempt Rate, R, instead\n\
    \            ; of invoking max_sps()\n         end proc\n         ; Iterative\
    \ process to figure out the largest number of\n         ; sps that we can achieve\
    \ in order to setup n sessions.\n         ; This function converges to R, the\
    \ Session Attempt Rate.\n         proc max_sps(r, m, n)\n            s     :=\
    \ 0    ; session setup rate\n            old_r := 0    ; old session setup rate\n\
    \            h     := 0    ; Return value, R\n            count := 0\n       \
    \     ; Note that if w is small (say, 0.10) and r is small\n            ; (say,\
    \ <= 9), the algorithm will not converge since it\n            ; uses floor()\
    \ to increment r dynamically.  It is best\n            ; to start with the defaults\
    \ (w = 0.10 and r >= 100).\n            while (TRUE) {\n               s := send_traffic(r,\
    \ m, n) ; Send r sps, with m media\n               ; characteristics until n sessions\
    \ have been attempted.\n               if (s == n)  {\n                   if (r\
    \ > old_r)  {\n                       old_r = r\n                   }\n      \
    \             else  {\n                       count = count + 1\n            \
    \            if (count >= 10)  {\n                            # We've converged.\n\
    \                            h := max(r, old_r)\n                            break\n\
    \                        }\n                    }\n                    r  := floor(r\
    \ + (w * r))\n                }\n                else  {\n                   \
    \ r := floor(r - (d * r))\n                    d := max(0.10, d / 2)\n       \
    \             w := max(0.10, w / 2)\n                }\n             }\n     \
    \        return h\n          end proc\n"
- title: 5.  Reporting Format
  contents:
  - '5.  Reporting Format

    '
- title: 5.1.  Test Setup Report
  contents:
  - "5.1.  Test Setup Report\n      SIP Transport Protocol = ___________________________\n\
    \      (valid values: TCP|UDP|TLS|SCTP|websockets|specify-other)\n      (Specify\
    \ if same transport used for connections to the DUT\n      and connections from\
    \ the DUT.  If different transports\n      used on each connection, enumerate\
    \ the transports used.)\n      Connection management strategy for connection oriented\n\
    \      transports\n         DUT receives requests on one connection = _______\n\
    \         (Yes or no.  If no, DUT accepts a new connection for\n         every\
    \ incoming request, sends a response on that\n         connection, and closes\
    \ the connection.)\n         DUT sends requests on one connection = __________\n\
    \         (Yes or no.  If no, DUT initiates a new connection to\n         send\
    \ out each request, gets a response on that\n         connection, and closes the\
    \ connection.)\n      Session Attempt Rate  _______________________________\n\
    \      (Session attempts/sec)\n      (The initial value for \"r\" in benchmarking\
    \ algorithm in\n      Section 4.10.)\n      Session Duration = _________________________________\n\
    \      (In seconds)\n      Total Sessions Attempted = _________________________\n\
    \      (Total sessions to be created over duration of test)\n      Media Streams\
    \ per Session =  _______________________\n      (number of streams per session)\n\
    \      Associated Media Protocol =  _______________________\n      (RTP|SRTP|specify-other)\n\
    \      Codec = ____________________________________________\n      (Codec type\
    \ as identified by the organization that\n      specifies the codec)\n      Media\
    \ Packet Size (audio only) =  __________________\n      (Number of bytes in an\
    \ audio packet)\n      Establishment Threshold time =  ____________________\n\
    \      (Seconds)\n      TLS ciphersuite used\n      (for tests involving TLS)\
    \ = ________________________\n      (e.g., TLS_RSA_WITH_AES_128_CBC_SHA)\n   \
    \   IPsec profile used\n      (For tests involving IPsec) = _____________________\n"
- title: 5.2.  Device Benchmarks for Session Setup
  contents:
  - "5.2.  Device Benchmarks for Session Setup\n      Session Establishment Rate,\
    \ \"R\" = __________________\n      (sessions per second)\n      Is DUT acting\
    \ as a media relay? (yes/no) = _________\n"
- title: 5.3.  Device Benchmarks for Registrations
  contents:
  - "5.3.  Device Benchmarks for Registrations\n      Registration Rate =  ____________________________\n\
    \      (registrations per second)\n      Re-registration Rate =  ____________________________\n\
    \      (registrations per second)\n      Notes = ____________________________________________\n\
    \      (List any specific backend processing required or\n      other parameters\
    \ that may impact the rate)\n"
- title: 6.  Test Cases
  contents:
  - '6.  Test Cases

    '
- title: 6.1.  Baseline Session Establishment Rate of the Testbed
  contents:
  - "6.1.  Baseline Session Establishment Rate of the Testbed\n   Objective:\n   \
    \   To benchmark the Session Establishment Rate of the Emulated Agent\n      (EA)\
    \ with zero failures.\n   Procedure:\n      1.  Configure the DUT in the test\
    \ topology shown in Figure 1.\n      2.  Set Media Streams per Session to 0.\n\
    \      3.  Execute benchmarking algorithm as defined in Section 4.10 to\n    \
    \      get the baseline Session Establishment Rate.  This rate MUST\n        \
    \  be recorded using any pertinent parameters as shown in the\n          reporting\
    \ format of Section 5.1.\n   Expected Results:  This is the scenario to obtain\
    \ the maximum Session\n      Establishment Rate of the EA and the testbed when\
    \ no DUT is\n      present.  The results of this test might be used to normalize\
    \ test\n      results performed on different testbeds or simply to better\n  \
    \    understand the impact of the DUT on the testbed in question.\n"
- title: 6.2.  Session Establishment Rate without Media
  contents:
  - "6.2.  Session Establishment Rate without Media\n   Objective:\n      To benchmark\
    \ the Session Establishment Rate of the DUT with no\n      Associated Media and\
    \ zero failures.\n   Procedure:\n      1.  Configure a DUT according to the test\
    \ topology shown in\n          Figure 1 or Figure 2.\n      2.  Set Media Streams\
    \ per Session to 0.\n      3.  Execute benchmarking algorithm as defined in Section\
    \ 4.10 to\n          get the Session Establishment Rate.  This rate MUST be\n\
    \          recorded using any pertinent parameters as shown in the\n         \
    \ reporting format of Section 5.1.\n   Expected Results:  Find the Session Establishment\
    \ Rate of the DUT\n      when the EA is not sending media streams.\n"
- title: 6.3.  Session Establishment Rate with Media Not on DUT
  contents:
  - "6.3.  Session Establishment Rate with Media Not on DUT\n   Objective:\n     \
    \ To benchmark the Session Establishment Rate of the DUT with zero\n      failures\
    \ when Associated Media is included in the benchmark test\n      but the media\
    \ is not running through the DUT.\n   Procedure:\n      1.  Configure a DUT according\
    \ to the test topology shown in\n          Figure 1.\n      2.  Set Media Streams\
    \ per Session to 1.\n      3.  Execute benchmarking algorithm as defined in Section\
    \ 4.10 to\n          get the session establishment rate with media.  This rate\
    \ MUST\n          be recorded using any pertinent parameters as shown in the\n\
    \          reporting format of Section 5.1.\n   Expected Results:  Session Establishment\
    \ Rate results obtained with\n      Associated Media with any number of media\
    \ streams per SIP session\n      are expected to be identical to the Session Establishment\
    \ Rate\n      results obtained without media in the case where the DUT is\n  \
    \    running on a platform separate from the Media Relay.\n"
- title: 6.4.  Session Establishment Rate with Media on DUT
  contents:
  - "6.4.  Session Establishment Rate with Media on DUT\n   Objective:\n      To benchmark\
    \ the Session Establishment Rate of the DUT with zero\n      failures when Associated\
    \ Media is included in the benchmark test\n      and the media is running through\
    \ the DUT.\n   Procedure:\n      1.  Configure a DUT according to the test topology\
    \ shown in\n          Figure 2.\n      2.  Set Media Streams per Session to 1.\n\
    \      3.  Execute benchmarking algorithm as defined in Section 4.10 to\n    \
    \      get the Session Establishment Rate with media.  This rate MUST\n      \
    \    be recorded using any pertinent parameters as shown in the\n          reporting\
    \ format of Section 5.1.\n   Expected Results:  Session Establishment Rate results\
    \ obtained with\n      Associated Media may be lower than those obtained without\
    \ media in\n      the case where the DUT and the Media Relay are running on the\
    \ same\n      platform.  It may be helpful for the tester to be aware of the\n\
    \      reasons for this degradation, although these reasons are not\n      parameters\
    \ of the test.  For example, the degree of performance\n      degradation may\
    \ be due to what the DUT does with the media (e.g.,\n      relaying vs. transcoding),\
    \ the type of media (audio vs. video vs.\n      data), and the codec used for\
    \ the media.  There may also be cases\n      where there is no performance impact,\
    \ if the DUT has dedicated\n      media-path hardware.\n"
- title: 6.5.  Session Establishment Rate with TLS-Encrypted SIP
  contents:
  - "6.5.  Session Establishment Rate with TLS-Encrypted SIP\n   Objective:\n    \
    \  To benchmark the Session Establishment Rate of the DUT with zero\n      failures\
    \ when using TLS-encrypted SIP signaling.\n   Procedure:\n      1.  If the DUT\
    \ is being benchmarked as a proxy or B2BUA, then\n          configure the DUT\
    \ in the test topology shown in Figure 1 or\n          Figure 2.\n      2.  Configure\
    \ the tester to enable TLS over the transport being\n          used during benchmarking.\
    \  Note the ciphersuite being used for\n          TLS and record it in Section\
    \ 5.1.\n      3.  Set Media Streams per Session to 0 (media is not used in this\n\
    \          test).\n      4.  Execute benchmarking algorithm as defined in Section\
    \ 4.10 to\n          get the Session Establishment Rate with TLS encryption.\n\
    \   Expected Results:  Session Establishment Rate results obtained with\n    \
    \  TLS-encrypted SIP may be lower than those obtained with plaintext\n      SIP.\n"
- title: 6.6.  Session Establishment Rate with IPsec-Encrypted SIP
  contents:
  - "6.6.  Session Establishment Rate with IPsec-Encrypted SIP\n   Objective:\n  \
    \    To benchmark the Session Establishment Rate of the DUT with zero\n      failures\
    \ when using IPsec-encrypted SIP signaling.\n   Procedure:\n      1.  Configure\
    \ a DUT according to the test topology shown in\n          Figure 1 or Figure\
    \ 2.\n      2.  Set Media Streams per Session to 0 (media is not used in this\n\
    \          test).\n      3.  Configure tester for IPsec.  Note the IPsec profile\
    \ being used\n          for IPsec and record it in Section 5.1.\n      4.  Execute\
    \ benchmarking algorithm as defined in Section 4.10 to\n          get the Session\
    \ Establishment Rate with encryption.\n   Expected Results:  Session Establishment\
    \ Rate results obtained with\n      IPsec-encrypted SIP may be lower than those\
    \ obtained with\n      plaintext SIP.\n"
- title: 6.7.  Registration Rate
  contents:
  - "6.7.  Registration Rate\n   Objective:\n      To benchmark the maximum registration\
    \ rate the DUT can handle over\n      an extended time period with zero failures.\n\
    \   Procedure:\n      1.  Configure a DUT according to the test topology shown\
    \ in\n          Figure 3.\n      2.  Set the registration timeout value to at\
    \ least 3600 seconds.\n      3.  Each register request MUST be made to a distinct\
    \ Address of\n          Record (AoR).  Execute benchmarking algorithm as defined\
    \ in\n          Section 4.10 to get the maximum registration rate.  This rate\n\
    \          MUST be recorded using any pertinent parameters as shown in\n     \
    \     the reporting format of Section 5.1.  For example, the use of\n        \
    \  TLS or IPsec during registration must be noted in the\n          reporting\
    \ format.  In the same vein, any specific backend\n          processing (use of\
    \ databases, authentication servers, etc.)\n          SHOULD be recorded as well.\n\
    \   Expected Results:  Provides a maximum registration rate.\n"
- title: 6.8.  Re-registration Rate
  contents:
  - "6.8.  Re-registration Rate\n   Objective:\n      To benchmark the re-registration\
    \ rate of the DUT with zero\n      failures using the same backend processing\
    \ and parameters used\n      during Section 6.7.\n   Procedure:\n      1.  Configure\
    \ a DUT according to the test topology shown in\n          Figure 3.\n      2.\
    \  Execute the test detailed in Section 6.7 to register the\n          endpoints\
    \ with the registrar and obtain the registration rate.\n      3.  After at least\
    \ 5 minutes of performing Step 2, but no more\n          than 10 minutes after\
    \ Step 2 has been performed, re-register\n          the same AoRs used in Step\
    \ 3 of Section 6.7.  This will count\n          as a re-registration because the\
    \ SIP AoRs have not yet\n          expired.\n   Expected Results:  Note the rate\
    \ obtained through this test for\n      comparison with the rate obtained in Section\
    \ 6.7.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   Documents of this type do not directly affect\
    \ the security of the\n   Internet or corporate networks as long as benchmarking\
    \ is not\n   performed on devices or systems connected to production networks.\n\
    \   Security threats and how to counter these in SIP and the media layer\n   is\
    \ discussed in RFC 3261, RFC 3550, and RFC 3711, and various other\n   documents.\
    \  This document attempts to formalize a set of common\n   methodology for benchmarking\
    \ performance of SIP devices in a lab\n   environment.\n"
- title: 8.  References
  contents:
  - '8.  References

    '
- title: 8.1.  Normative References
  contents:
  - "8.1.  Normative References\n   [RFC2119]  Bradner, S., \"Key words for use in\
    \ RFCs to Indicate\n              Requirement Levels\", BCP 14, RFC 2119, March\
    \ 1997,\n              <http://www.rfc-editor.org/info/rfc2119>.\n   [RFC7501]\
    \  Davids, C., Gurbani, V., and S. Poretsky, \"Terminology for\n             \
    \ Benchmarking Session Initiation Protocol (SIP) Devices:\n              Basic\
    \ Session Setup and Registration\", RFC 7501, April\n              2015, <http://www.rfc-editor.org/info/rfc7501>.\n"
- title: 8.2.  Informative References
  contents:
  - "8.2.  Informative References\n   [RFC3261]  Rosenberg, J., Schulzrinne, H., Camarillo,\
    \ G., Johnston,\n              A., Peterson, J., Sparks, R., Handley, M., and\
    \ E.\n              Schooler, \"SIP: Session Initiation Protocol\", RFC 3261,\n\
    \              June 2002, <http://www.rfc-editor.org/info/rfc3261>.\n   [Rtool]\
    \    R Development Core Team, \"R: A Language and Environment\n              for\
    \ Statistical Computing\", R Foundation for Statistical\n              Computing\
    \ Vienna, Austria, ISBN 3-900051-07-0, 2011,\n              <http://www.R-project.org>.\n"
- title: Appendix A.  R Code Component to Simulate Benchmarking Algorithm
  contents:
  - "Appendix A.  R Code Component to Simulate Benchmarking Algorithm\n      # Copyright\
    \ (c) 2015 IETF Trust and the persons identified as\n      # authors of the code.\
    \  All rights reserved.\n      #\n      # Redistribution and use in source and\
    \ binary forms, with or\n      # without modification, are permitted provided\
    \ that the following\n      # conditions are met:\n      #\n      # The author\
    \ of this code is Vijay K. Gurbani.\n      #\n      # - Redistributions of source\
    \ code must retain the above copyright\n      #   notice, this list of conditions\
    \ and\n      #   the following disclaimer.\n      #\n      # - Redistributions\
    \ in binary form must reproduce the above\n      #   copyright notice, this list\
    \ of conditions and the following\n      #   disclaimer in the documentation and/or\
    \ other materials\n      #   provided with the distribution.\n      #\n      #\
    \ - Neither the name of Internet Society, IETF or IETF Trust,\n      #   nor the\
    \ names of specific contributors, may be used to\n      #   endorse or promote\
    \ products derived from this software\n      #   without specific prior written\
    \ permission.\n      #\n      # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS\
    \ AND\n      # CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES,\n\
    \      # INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n      # MERCHANTABILITY\
    \ AND FITNESS FOR A PARTICULAR PURPOSE ARE\n      # DISCLAIMED. IN NO EVENT SHALL\
    \ THE COPYRIGHT OWNER OR\n      # CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n\
    \      # INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n      # (INCLUDING,\
    \ BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE\n      # GOODS OR SERVICES; LOSS\
    \ OF USE, DATA, OR PROFITS; OR BUSINESS\n      # INTERRUPTION) HOWEVER CAUSED\
    \ AND ON ANY THEORY OF LIABILITY,\n      # WHETHER IN CONTRACT, STRICT LIABILITY,\
    \ OR TORT (INCLUDING\n      # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT\
    \ OF THE USE\n      # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\
    \ SUCH\n      # DAMAGE.\n      w = 0.10\n      d = max(0.10, w / 2)\n      DUT_max_sps\
    \ = 460     # Change as needed to set the max sps value\n                    \
    \        # for a DUT\n      # Returns R, given r (initial session attempt rate).\n\
    \      # E.g., assume that a DUT handles 460 sps in steady state\n      # and\
    \ you have saved this code in a file simulate.r.  Then,\n      # start an R session\
    \ and do the following:\n      #\n      # > source(\"simulate.r\")\n      # >\
    \ find_R(100)\n      # ... debug output omitted ...\n      # [1] 458\n      #\n\
    \      # Thus, the max sps that the DUT can handle is 458 sps, which is\n    \
    \  # close to the absolute maximum of 460 sps the DUT is specified to\n      #\
    \ do.\n      find_R <- function(r)  {\n         s     = 0\n         old_r = 0\n\
    \         h     = 0\n         count = 0\n         # Note that if w is small (say,\
    \ 0.10) and r is small\n         # (say, <= 9), the algorithm will not converge\
    \ since it\n         # uses floor() to increment r dynamically.  It is best\n\
    \         # to start with the defaults (w = 0.10 and r >= 100).\n         cat(\"\
    r   old_r    w     d \\n\")\n         while (TRUE)  {\n            cat(r, ' ',\
    \ old_r, ' ', w, ' ', d, '\\n')\n            s = send_traffic(r)\n           \
    \ if (s == TRUE)  {     # All sessions succeeded\n                if (r > old_r)\
    \  {\n                    old_r = r\n                }\n                else \
    \ {\n                    count = count + 1\n                    if (count >= 10)\
    \  {\n                        # We've converged.\n                        h =\
    \ max(r, old_r)\n                        break\n                    }\n      \
    \          }\n                r  = floor(r + (w * r))\n            }\n       \
    \     else  {\n                r = floor(r - (d * r))\n                d = max(0.10,\
    \ d / 2)\n                w = max(0.10, w / 2)\n            }\n         }\n  \
    \       h\n      }\n      send_traffic <- function(r)  {\n         n = TRUE\n\
    \         if (r > DUT_max_sps)  {\n             n = FALSE\n         }\n      \
    \   n\n      }\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   The authors would like to thank Keith Drage and Daryl Malas\
    \ for their\n   contributions to this document.  Dale Worley provided an extensive\n\
    \   review that led to improvements in the documents.  We are grateful to\n  \
    \ Barry Constantine, William Cerveny, and Robert Sparks for providing\n   valuable\
    \ comments during the documents' last calls and expert\n   reviews.  Al Morton\
    \ and Sarah Banks have been exemplary working group\n   chairs; we thank them\
    \ for tracking this work to completion.  Tom\n   Taylor provided an in-depth review\
    \ and subsequent comments on the\n   benchmarking convergence algorithm in Section\
    \ 4.10.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Carol Davids\n   Illinois Institute of Technology\n  \
    \ 201 East Loop Road\n   Wheaton, IL  60187\n   United States\n   Phone: +1 630\
    \ 682 6024\n   EMail: davids@iit.edu\n   Vijay K. Gurbani\n   Bell Laboratories,\
    \ Alcatel-Lucent\n   1960 Lucent Lane\n   Rm 9C-533\n   Naperville, IL  60566\n\
    \   United States\n   Phone: +1 630 224 0216\n   EMail: vkg@bell-labs.com\n  \
    \ Scott Poretsky\n   Allot Communications\n   300 TradeCenter, Suite 4680\n  \
    \ Woburn, MA  08101\n   United States\n   Phone: +1 508 309 2179\n   EMail: sporetsky@allot.com\n"
