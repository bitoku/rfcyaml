Abstract This document defines a set of criteria by which a DSA implementation may be judged.
Particular issues covered include conformance to standards; performance; demonstrated interoperability.
The intention is that the replies to the questions posed provide a fairly full description of a DSA.
Some of the questions will yield answers which are purely descriptive; others, however, are intended to elicit answers which give some measure of the utility of the DSA.
The marks awarded for a DSA in each particular area should give a good indication of the DSA's capabilities, and its suitability for particular uses.
Please send comments to the authors or to the discussion group <osi ds@CS.UCL.AC.UK>.
The purpose of this document is to define some metrics by which DSA products can be measured.
Such metrics are valuable as whilst an X.500 DSA must conform to the specification in the standard this is a sine qua non protocol conformance is not in itself the hallmark of a usable implementation.
A DSA must perform operations within a reasonable time; a DSA must offer good throughput of queries; a DSA must be able to handle a reasonable volume of data; if modification operations are provided, some sort of access control must be provided; a DSA and its data must be manageable.
In many respects, it is almost impossible to say that one DSA is better than other from looking at the responses to questions in this document.
For some, the cost or level of support will be the key criterion.
For another user, the flexibility of the schema management facilities, or the feasibility of running the DSA over an existing relational database, will be of prime importance.
In many respects DSAs will just be different, rather than better or worse.
However, all other things being equal, the look up speed of a DSA is very obviously measurable, and there is a substantial number of questions on the speed of the various X.500 operations, and in particular on the look up operations.
Throughout this document, some of the questions posed are annotated with a square bracketed points score and an explanation as to how the points should be allocated.
For example, a question might be appended with "[2 if yes]", indicating score 2 points for an affirmative answer to that question.
These points scores should be collated in Table 1 at the end of the document.
The questions on DSA performance are judged to be important enough to have a separate table for those results:
they appear in Table 2 (and optionally Table 3).
Together, these tables constitute a measure of the DSA.
The metrics are on a section by section basis, which should help the reader who is seeking, for example, a DSA with fast look up capabilities and extensive access control facilities, to focus on the critical aspects of a DSA for their particular requirement.
No conclusions should be inferred from adding the scores together into one overall grand total and comparing such totals for different DSAs, as no attempt is made to assign weights to the different characteristics.
Whilst much of this document should usually be completed by the developers or suppliers of an implementation, the section on performance could be completed by anyone running the implementation.
Indeed, it will be beneficial if several sets of performance figures can be gathered for each implementation, for a variety of hardware platforms.
This section contains general information about the implementation under discussion.
Are there plans to implement the additional features describe in the 1992/3 standard?
Describe the hardware and software platforms supported by the DSA
O/S (state version if critical)
(be sure to indicate which flavour e.g., SYSV [1]
Name any other software required to run the system which is not supplied with the operating system or with the DSA software itself.
Is this DSA an integrated part of a software package, and in such case which ?
If the DSA needs other packages, are these also freely available?
[3 if completely free, 1 if requires commercial software package]
Is commercial support available for this implementation?
Is free, best effort support available from the developers?
Is free support available via user groups or email lists?
Conformance to OSI Standards 3.1  Directory protocols 13.
Does the DSA implement DAP?
Does the DSA implement DSP?
Statement requirements according to section 9.2.1 in X.519.
(b)  Capable of acting as first level DSA?
All attribute types according to X.520?
All object classes according to X.521?
Does the implementation meet the conformance clauses in section 9.2.2 and 9.2.3 of X.519?
Static requirements [2 if yes on all]
Please list all conformance testing work applied to the implementation (specify conformance test version number).
3.2  Implementors' agreements and profiles Does the DSA conform to the following implementors' agreements?
If so, state parts and version numbers.
If so, state which version numbers.
Which of the following transport and network layer protocols does the DSA support: (a)
TP.x over CONS (state transport class)?
A suggested DIT structure, detailing an object class hierarchy, is presented in X.521.
(b)  Allow the enforcement of this hierarchy?
Are structure rules optional or mandatory?
Not everybody uses OSI protocols at the network layer.
Does the DSA support other "network" layer protocols?
(b)  State any other options supported.
Does the DSA also run over any lightweight stack?
Can local DUAs access the DSA directly by some method of inter process communications?
Extensions to the 1988 Standard 5.1  Schema 28.
Does the DSA fully support RFC1274, "The COSINE and Internet X.500 Schema"?
If not, please supply a list of all those object classes, attribute types and attribute syntaxes in RFC1274 which are supported on a separate sheet.
This might be summarised by saying, for example, "all those with standard attribute syntaxes", or "all except fooBar".
Does the DSA implement the schema management defined in the 1992 standard?
If not, is the schema stored in the Directory?
In a distributed manner[2] or centralised[1] ?
Can a DSA manager extend the schema and add new (a)  Attribute types with existing syntaxes?
5.2  Support for replication 33.
Does the DSA support the replication mechanisms as described in the 1992 standard [2]?
Does the DSA support any other replication mechanisms?
If the DSA supports replication, does it support:
(a)  Replication of a single entry?
(b)  Replication of a set of sibling entries?
(c)  Replication of a subtree?
5.3  Support for access control 36.
Does the DSA support access control as described in the 1992 standard [3]?
If not, does the DSA have any access control mechanisms at all?
If yes, does the access control scheme support the following:
(a)  Allow a user to maintain their own entry?
(b)  Allow a user to maintain some attributes in their own entry, but not all attributes?
(c)  Give management rights to a DSA manager in a fashion analogous to the privileges given to a UNIX super user?
Give management rights to a data manager on a per subtree basis?
(e)  Give management rights (to an entry, group of entries, subtree, etc) to a group of users?
Give access rights to users on the basis of the leading portion of their Distinguished Name?
Is it possible to define a protection mechanism for each individual attribute type in one entry?
Maximum number of Distinguished Names that can be defined for one access right to one attribute in one entry?
If unlimited, state the constraints.
Does the DSA support the extended access control techniques described in "An Access Control approach for Searching and Listing" by Hardcastle Kille and Howes, in the Internet Draft, OSI DS 21?
If there are features of the access control mechanisms which are not brought out by the above questions, please describe these additional features [up to 2 for wonderful additional features!]
Does the DSA fully support RFC1276, "Replication and Distributed Operations extensions to provide an Internet Directory using X.500"?
If not, please give a list of features that are supported.
Does the DSA use its own database, or can it be used in conjunction with a general purpose database package such as Oracle?
[1 for own, 1 for ability to map onto general purpose databases,
If the DSA runs as a static server, state the start up time for a DSA with a database of 20000 entries.
If this varies widely according to configuration options, give figures for the various options.
What is the maximum number of simultaneous associations that the DSA may have open?
Maximum database size, in entries, megabytes, or as appropriate.
If none, state what the constraints are.
What is the run time size of an entry as specified in section 10 (on performance)?
This should be the marginal size of an entry and thus should include the overhead of default indexes, etc.
What is the on disk database size of an entry as specified in section 10 on performance?
Does the DSA make of indexing?
Can the database be fully inverted?
If not, state for which: i.  attributes
What is the increase in on disk database size of adding another index?
What sort of approximate match algorithm does the DSA use?
Does the DSA attempt to use relay DSAs (which have access to more than one network) in order to achieve connectivity with DSAs which are not on the same network?
Management tools 7.1  Dynamic system management 50.
Are there tools for monitoring DSA activity, using: (a)  DAP?
Are there tools for controlling a run time DSA?
If knowledge information is stored within the DIT, are there tools for knowledge management?
Are there tools for checking that attributes with Distinguished Name syntax
contain values of entries in the DIT (i.e., they do not contain "dangling pointers")?
If the DSA doesn't use a general purpose database package, what data management tools are available?
Are there any tools for arboriculture the moving, copying or deleting of DIT subtrees?
The DSA may have lots of wonderful features   on paper!
But has the DSA been shown to work in practice?
The following measures are intended to give some measure of confidence that the DSA's viability has been demonstrated.
How many entries in the largest DSA in use in operational use?
What is the largest set of DSAs supporting an organisation?
What is the estimated number of organisations using this implementation for service use?
[8 if more than 100 organisations, 5 if more than 50 organisations,
Is this DSA used commercially with an installed base of more than 10 customers?
The X.500 Directory is the OSI Directory.
OSI stands for Open Systems Interconnection   DSAs
have to be able to inter operate.
They also have to be seen to interoperate.
Is this DSA in use in X.500 pilots?
Is this DSA in use anywhere in the COSINE/Internet Pilot?
(b)  Is this DSA in use in any other major pilot?
Name any other systems which you believe the system to interoperate with.
(It is not sufficient to say "any system which supports the conformance clauses ...
This section should give an outline to the expected performance of the DSA.
A number of operations are timed in order to give a feel for the DSA's speed and throughput.
Note that all operations should be resolvable within a single DSA.
Chaining and referral are not assessed, although it should be possible to infer, albeit approximately, the speed of distributed operations.
The tests should be made against an organisational database of 20000 entries.
Some tests are against subsets of this data, and so the database should be set up according to the following instructions.
Create an organisational DSA with 20000 entries below the organisation node.
Sub divide this data into a number of organisational units, one of which should contain 1000 entries, another of which should contain 100 entries, and a third which should contain just 10 entries.
The entries, which should differ, should be created with the following attributes: (a)  Common Name (b)  Surname (c)  Telephone number (d)  Postal Address (of 100 characters) (e)  Object class ii.
In all the tests, two timings should be taken.
In order to normalise the test results as much as possible, it is suggested that these tests be undertaken on an otherwise lightly loaded machine.
A typical "cold start" reading should be given.
In this case the system will not have the advantage of any benefits that derive from operating system paging, or caching.
(b)  A best possible figure should be given, which indicates the upper limit of DSA performance.
The timings should relate to the default set up, and should be entered in Table 2.
If significant performance gains can be made by use of configuration options, such as building extra indexes to support searches, measures of the improved performance may also be given, and should be entered in Table 3.
Attention should be also drawn to any optimisations, heuristic or otherwise, which are not evidenced in the following tests.
Please note that the tests should be made using a DUA and DSA with full 7 layer stacks, rather than some lightweight protocol.
10.1  Speed for various operations
The tests are described, one subsection per operation.
The results should be entered in Table 2 (and Table 3 if a non default set up is also measured).
The time it takes for a DUA to bind to the Directory.
This time should include all the initialisation time a DUA process needs before it can query the Directory:
e.g., reading of tailor files, schema information, etc.
Give the bind time for each of the following levels of authentication.
a" if the implementation does not support a particular level of authentication.
List Give the time for listing a set of organisational unit sibling entries.
In this section, two sets of search operations should be performed on the DSA.
A single level search of 100 entries within an organisational unit.
An organisation subtree search, on the subtree of 20000 entries.
The following searches should be tried.
Unless otherwise stated, the "XXX" or "YYY" part of the search filter should be chosen in such a way as to return a single result.
Unless stated otherwise the results should return all attributes for the entry.
Exact match for a surname: surname XXX 70.
Leading substring match for a common name:
Any substring match for a common name: commonName  XXX  72.
Trailing substring match for a common name:
Approximate match for a common name:
More complex filter, searching by object class and two other attribute types: objectClass person AND (commonName XXX
Search returning all entries (i.e., 100 entries in the single level search, and all 20000 entries in the subtree search: objectClass
In this case, no attribute values should be returned in the result set.
A single read operation, returning all attributes.
Add an entry beneath an entry which has: (a)  0 children (b)  10 children (c)  1000 children 10.1.6
an attribute value, other than an RDN value, for an entry which has 1.
Modify an entry (a)  Add description attribute (b)
an RDN value for an entry with the following number of siblings.
Modify RDN (a)  10 siblings
As the time taken for a single read will usually be negligible, the following list and set of reads should give a clearer indication of the query rate.
A list to return 100 entries for persons, and then a read of each entry returning all attribute values.
The results of the tests just described should be entered in Table 2 (and optionally Table 3), at the end of the document.
The results will be directly correlated to the test set up used, and in particular, the hardware.
If disk based DSA, disk I/
Protocols in transport layer and below
Speed of operations non default set up Security Considerations Security issues are not discussed in this memo.
