- title: __initial_text__
  contents:
  - ''
- title: Independent Submission                                      P. Garg, Ed.
  contents:
  - "Independent Submission                                      P. Garg, Ed.\n  \
    \ NVGRE: Network Virtualization Using Generic Routing Encapsulation\n"
- title: Abstract
  contents:
  - "Abstract\n   This document describes the usage of the Generic Routing\n   Encapsulation\
    \ (GRE) header for Network Virtualization (NVGRE) in\n   multi-tenant data centers.\
    \  Network Virtualization decouples virtual\n   networks and addresses from physical\
    \ network infrastructure,\n   providing isolation and concurrency between multiple\
    \ virtual networks\n   on the same physical network infrastructure.  This document\
    \ also\n   introduces a Network Virtualization framework to illustrate the use\n\
    \   cases, but the focus is on specifying the data-plane aspect of NVGRE.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This is a contribution to\
    \ the RFC Series, independently of any other\n   RFC stream.  The RFC Editor has\
    \ chosen to publish this document at\n   its discretion and makes no statement\
    \ about its value for\n   implementation or deployment.  Documents approved for\
    \ publication by\n   the RFC Editor are not a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7637.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2015 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................2\n\
    \      1.1. Terminology ................................................4\n  \
    \ 2. Conventions Used in This Document ...............................4\n   3.\
    \ Network Virtualization Using GRE (NVGRE) ........................4\n      3.1.\
    \ NVGRE Endpoint .............................................5\n      3.2. NVGRE\
    \ Frame Format .........................................5\n      3.3. Inner Tag\
    \ as Defined by IEEE 802.1Q ........................8\n      3.4. Reserved VSID\
    \ ..............................................8\n   4. NVGRE Deployment Considerations\
    \ .................................9\n      4.1. ECMP Support ...............................................9\n\
    \      4.2. Broadcast and Multicast Traffic ............................9\n  \
    \    4.3. Unicast Traffic ............................................9\n    \
    \  4.4. IP Fragmentation ..........................................10\n      4.5.\
    \ Address/Policy Management and Routing .....................10\n      4.6. Cross-Subnet,\
    \ Cross-Premise Communication .................10\n      4.7. Internet Connectivity\
    \ .....................................12\n      4.8. Management and Control Planes\
    \ .............................12\n      4.9. NVGRE-Aware Devices .......................................12\n\
    \      4.10. Network Scalability with NVGRE ...........................13\n  \
    \ 5. Security Considerations ........................................14\n   6.\
    \ Normative References ...........................................14\n   Contributors\
    \ ......................................................16\n   Authors' Addresses\
    \ ................................................17\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Conventional data center network designs cater to largely\
    \ static\n   workloads and cause fragmentation of network and server capacity\
    \ [6]\n   [7].  There are several issues that limit dynamic allocation and\n \
    \  consolidation of capacity.  Layer 2 networks use the Rapid Spanning\n   Tree\
    \ Protocol (RSTP), which is designed to eliminate loops by\n   blocking redundant\
    \ paths.  These eliminated paths translate to wasted\n   capacity and a highly\
    \ oversubscribed network.  There are alternative\n   approaches such as the Transparent\
    \ Interconnection of Lots of Links\n   (TRILL) that address this problem [13].\n\
    \   The network utilization inefficiencies are exacerbated by network\n   fragmentation\
    \ due to the use of VLANs for broadcast isolation.  VLANs\n   are used for traffic\
    \ management and also as the mechanism for\n   providing security and performance\
    \ isolation among services belonging\n   to different tenants.  The Layer 2 network\
    \ is carved into smaller-\n   sized subnets (typically, one subnet per VLAN),\
    \ with VLAN tags\n   configured on all the Layer 2 switches connected to server\
    \ racks that\n   host a given tenant's services.  The current VLAN limits\n  \
    \ theoretically allow for 4,000 such subnets to be created.  The size\n   of the\
    \ broadcast domain is typically restricted due to the overhead\n   of broadcast\
    \ traffic.  The 4,000-subnet limit on VLANs is no longer\n   sufficient in a shared\
    \ infrastructure servicing multiple tenants.\n   Data center operators must be\
    \ able to achieve high utilization of\n   server and network capacity.  In order\
    \ to achieve efficiency, it\n   should be possible to assign workloads that operate\
    \ in a single Layer\n   2 network to any server in any rack in the network.  It\
    \ should also\n   be possible to migrate workloads to any server anywhere in the\n\
    \   network while retaining the workloads' addresses.  This can be\n   achieved\
    \ today by stretching VLANs; however, when workloads migrate,\n   the network\
    \ needs to be reconfigured and that is typically error\n   prone.  By decoupling\
    \ the workload's location on the LAN from its\n   network address, the network\
    \ administrator configures the network\n   once, not every time a service migrates.\
    \  This decoupling enables any\n   server to become part of any server resource\
    \ pool.\n   The following are key design objectives for next-generation data\n\
    \   centers:\n      a) location-independent addressing\n      b) the ability to\
    \ a scale the number of logical Layer 2 / Layer 3\n         networks, irrespective\
    \ of the underlying physical topology or\n         the number of VLANs\n     \
    \ c) preserving Layer 2 semantics for services and allowing them to\n        \
    \ retain their addresses as they move within and across data\n         centers\n\
    \      d) providing broadcast isolation as workloads move around without\n   \
    \      burdening the network control plane\n   This document describes use of\
    \ the Generic Routing Encapsulation\n   (GRE) header [3] [4] for network virtualization.\
    \  Network\n   virtualization decouples a virtual network from the underlying\n\
    \   physical network infrastructure by virtualizing network addresses.\n   Combined\
    \ with a management and control plane for the virtual-to-\n   physical mapping,\
    \ network virtualization can enable flexible virtual\n   machine placement and\
    \ movement and provide network isolation for a\n   multi-tenant data center.\n\
    \   Network virtualization enables customers to bring their own address\n   spaces\
    \ into a multi-tenant data center, while the data center\n   administrators can\
    \ place the customer virtual machines anywhere in\n   the data center without\
    \ reconfiguring their network switches or\n   routers, irrespective of the customer\
    \ address spaces.\n"
- title: 1.1.  Terminology
  contents:
  - "1.1.  Terminology\n   Please refer to RFCs 7364 [10] and 7365 [11] for more formal\n\
    \   definitions of terminology.  The following terms are used in this\n   document.\n\
    \   Customer Address (CA): This is the virtual IP address assigned and\n   configured\
    \ on the virtual Network Interface Controller (NIC) within\n   each VM.  This\
    \ is the only address visible to VMs and applications\n   running within VMs.\n\
    \   Network Virtualization Edge (NVE): This is an entity that performs\n   the\
    \ network virtualization encapsulation and decapsulation.\n   Provider Address\
    \ (PA): This is the IP address used in the physical\n   network.  PAs are associated\
    \ with VM CAs through the network\n   virtualization mapping policy.\n   Virtual\
    \ Machine (VM): This is an instance of an OS running on top of\n   the hypervisor\
    \ over a physical machine or server.  Multiple VMs can\n   share the same physical\
    \ server via the hypervisor, yet are completely\n   isolated from each other in\
    \ terms of CPU usage, storage, and other OS\n   resources.\n   Virtual Subnet\
    \ Identifier (VSID): This is a 24-bit ID that uniquely\n   identifies a virtual\
    \ subnet or virtual Layer 2 broadcast domain.\n"
- title: 2.  Conventions Used in This Document
  contents:
  - "2.  Conventions Used in This Document\n   The key words \"MUST\", \"MUST NOT\"\
    , \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\"\
    , \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in RFC 2119 [1].\n   In this document, these words will appear with that interpretation\n\
    \   only when in ALL CAPS.  Lowercase uses of these words are not to be\n   interpreted\
    \ as carrying the significance defined in RFC 2119.\n"
- title: 3.  Network Virtualization Using GRE (NVGRE)
  contents:
  - "3.  Network Virtualization Using GRE (NVGRE)\n   This section describes Network\
    \ Virtualization using GRE (NVGRE).\n   Network virtualization involves creating\
    \ virtual Layer 2 topologies\n   on top of a physical Layer 3 network.  Connectivity\
    \ in the virtual\n   topology is provided by tunneling Ethernet frames in GRE\
    \ over IP over\n   the physical network.\n   In NVGRE, every virtual Layer 2 network\
    \ is associated with a 24-bit\n   identifier, called a Virtual Subnet Identifier\
    \ (VSID).  A VSID is\n   carried in an outer header as defined in Section 3.2.\
    \  This allows\n   unique identification of a tenant's virtual subnet to various\
    \ devices\n   in the network.  A 24-bit VSID supports up to 16 million virtual\n\
    \   subnets in the same management domain, in contrast to only 4,000 that\n  \
    \ is achievable with VLANs.  Each VSID represents a virtual Layer 2\n   broadcast\
    \ domain, which can be used to identify a virtual subnet of a\n   given tenant.\
    \  To support multi-subnet virtual topology, data center\n   administrators can\
    \ configure routes to facilitate communication\n   between virtual subnets of\
    \ the same tenant.\n   GRE is a Proposed Standard from the IETF [3] [4] and provides\
    \ a way\n   for encapsulating an arbitrary protocol over IP.  NVGRE leverages\
    \ the\n   GRE header to carry VSID information in each packet.  The VSID\n   information\
    \ in each packet can be used to build multi-tenant-aware\n   tools for traffic\
    \ analysis, traffic inspection, and monitoring.\n   The following sections detail\
    \ the packet format for NVGRE; describe\n   the functions of an NVGRE endpoint;\
    \ illustrate typical traffic flow\n   both within and across data centers; and\
    \ discuss address/policy\n   management, and deployment considerations.\n"
- title: 3.1.  NVGRE Endpoint
  contents:
  - "3.1.  NVGRE Endpoint\n   NVGRE endpoints are the ingress/egress points between\
    \ the virtual and\n   the physical networks.  The NVGRE endpoints are the NVEs\
    \ as defined\n   in the Network Virtualization over Layer 3 (NVO3) Framework document\n\
    \   [11].  Any physical server or network device can be an NVGRE\n   endpoint.\
    \  One common deployment is for the endpoint to be part of a\n   hypervisor. \
    \ The primary function of this endpoint is to\n   encapsulate/decapsulate Ethernet\
    \ data frames to and from the GRE\n   tunnel, ensure Layer 2 semantics, and apply\
    \ isolation policy scoped\n   on VSID.  The endpoint can optionally participate\
    \ in routing and\n   function as a gateway in the virtual topology.  To encapsulate\
    \ an\n   Ethernet frame, the endpoint needs to know the location information\n\
    \   for the destination address in the frame.  This information can be\n   provisioned\
    \ via a management plane or obtained via a combination of\n   control-plane distribution\
    \ or data-plane learning approaches.  This\n   document assumes that the location\
    \ information, including VSID, is\n   available to the NVGRE endpoint.\n"
- title: 3.2.  NVGRE Frame Format
  contents:
  - "3.2.  NVGRE Frame Format\n   The GRE header format as specified in RFCs 2784\
    \ [3] and 2890 [4] is\n   used for communication between NVGRE endpoints.  NVGRE\
    \ leverages the\n   Key extension specified in RFC 2890 [4] to carry the VSID.\
    \  The\n   packet format for Layer 2 encapsulation in GRE is shown in Figure 1.\n\
    \   Outer Ethernet Header:\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2\
    \ 3 4 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                (Outer) Destination MAC Address                |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |(Outer)Destination MAC Address |  (Outer)Source MAC Address    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                  (Outer) Source MAC Address                   |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |Optional Ethertype=C-Tag 802.1Q| Outer VLAN Tag Information    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |       Ethertype 0x0800        |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   Outer IPv4 Header:\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |Version|  HL   |Type of Service|          Total Length         |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |         Identification        |Flags|      Fragment Offset    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |  Time to Live | Protocol 0x2F |         Header Checksum       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                      (Outer) Source Address                   |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                  (Outer) Destination Address                  |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   GRE Header:\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |0| |1|0|   Reserved0     | Ver |   Protocol Type 0x6558        |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |               Virtual Subnet ID (VSID)        |    FlowID     |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   Inner Ethernet Header\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                (Inner) Destination MAC Address                |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |(Inner)Destination MAC Address |  (Inner)Source MAC Address    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                  (Inner) Source MAC Address                   |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |       Ethertype 0x0800        |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   Inner IPv4 Header:\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |Version|  HL   |Type of Service|          Total Length         |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |         Identification        |Flags|      Fragment Offset    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |  Time to Live |    Protocol   |         Header Checksum       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                       Source Address                          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                    Destination Address                        |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                    Options                    |    Padding    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \   |                      Original IP Payload                      |\n   |  \
    \                                                             |\n   |        \
    \                                                       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\
    \               Figure 1: GRE Encapsulation Frame Format\n   Note: HL stands for\
    \ Header Length.\n   The outer/delivery headers include the outer Ethernet header\
    \ and the\n   outer IP header:\n   o  The outer Ethernet header: The source Ethernet\
    \ address in the\n      outer frame is set to the MAC address associated with\
    \ the NVGRE\n      endpoint.  The destination endpoint may or may not be on the\
    \ same\n      physical subnet.  The destination Ethernet address is set to the\n\
    \      MAC address of the next-hop IP address for the destination NVE.\n     \
    \ The outer VLAN tag information is optional and can be used for\n      traffic\
    \ management and broadcast scalability on the physical\n      network.\n   o \
    \ The outer IP header: Both IPv4 and IPv6 can be used as the\n      delivery protocol\
    \ for GRE.  The IPv4 header is shown for\n      illustrative purposes.  Henceforth,\
    \ the IP address in the outer\n      frame is referred to as the Provider Address\
    \ (PA).  There can be\n      one or more PA associated with an NVGRE endpoint,\
    \ with policy\n      controlling the choice of which PA to use for a given Customer\n\
    \      Address (CA) for a customer VM.\n   In the GRE header:\n   o  The C (Checksum\
    \ Present) and S (Sequence Number Present) bits in\n      the GRE header MUST\
    \ be zero.\n   o  The K (Key Present) bit in the GRE header MUST be set to one.\
    \  The\n      32-bit Key field in the GRE header is used to carry the Virtual\n\
    \      Subnet ID (VSID) and the FlowID:\n      -  Virtual Subnet ID (VSID): This\
    \ is a 24-bit value that is used\n         to identify the NVGRE-based Virtual\
    \ Layer 2 Network.\n      -  FlowID: This is an 8-bit value that is used to provide\
    \ per-flow\n         entropy for flows in the same VSID.  The FlowID MUST NOT\
    \ be\n         modified by transit devices.  The encapsulating NVE SHOULD\n  \
    \       provide as much entropy as possible in the FlowID.  If a FlowID\n    \
    \     is not generated, it MUST be set to all zeros.\n   o  The Protocol Type\
    \ field in the GRE header is set to 0x6558\n      (Transparent Ethernet Bridging)\
    \ [2].\n   In the inner headers (headers of the GRE payload):\n   o  The inner\
    \ Ethernet frame comprises an inner Ethernet header\n      followed by optional\
    \ inner IP header, followed by the IP payload.\n      The inner frame could be\
    \ any Ethernet data frame not just IP.\n      Note that the inner Ethernet frame's\
    \ Frame Check Sequence (FCS) is\n      not encapsulated.\n   o  For illustrative\
    \ purposes, IPv4 headers are shown as the inner IP\n      headers, but IPv6 headers\
    \ may be used.  Henceforth, the IP address\n      contained in the inner frame\
    \ is referred to as the Customer\n      Address (CA).\n"
- title: 3.3.  Inner Tag as Defined by IEEE 802.1Q
  contents:
  - "3.3.  Inner Tag as Defined by IEEE 802.1Q\n   The inner Ethernet header of NVGRE\
    \ MUST NOT contain the tag as\n   defined by IEEE 802.1Q [5].  The encapsulating\
    \ NVE MUST remove any\n   existing IEEE 802.1Q tag before encapsulation of the\
    \ frame in NVGRE.\n   A decapsulating NVE MUST drop the frame if the inner Ethernet\
    \ frame\n   contains an IEEE 802.1Q tag.\n"
- title: 3.4.  Reserved VSID
  contents:
  - "3.4.  Reserved VSID\n   The VSID range from 0-0xFFF is reserved for future use.\n\
    \   The VSID 0xFFFFFF is reserved for vendor-specific NVE-to-NVE\n   communication.\
    \  The sender NVE SHOULD verify the receiver NVE's\n   vendor before sending a\
    \ packet using this VSID; however, such a\n   verification mechanism is out of\
    \ scope of this document.\n   Implementations SHOULD choose a mechanism that meets\
    \ their\n   requirements.\n"
- title: 4.  NVGRE Deployment Considerations
  contents:
  - '4.  NVGRE Deployment Considerations

    '
- title: 4.1.  ECMP Support
  contents:
  - "4.1.  ECMP Support\n   Equal-Cost Multipath (ECMP) may be used to provide load\
    \ balancing.\n   If ECMP is used, it is RECOMMENDED that the ECMP hash is calculated\n\
    \   either using the outer IP frame fields and entire Key field (32 bits)\n  \
    \ or the inner IP and transport frame fields.\n"
- title: 4.2.  Broadcast and Multicast Traffic
  contents:
  - "4.2.  Broadcast and Multicast Traffic\n   To support broadcast and multicast\
    \ traffic inside a virtual subnet,\n   one or more administratively scoped multicast\
    \ addresses [8] [9] can\n   be assigned for the VSID.  All multicast or broadcast\
    \ traffic\n   originating from within a VSID is encapsulated and sent to the\n\
    \   assigned multicast address.  From an administrative standpoint, it is\n  \
    \ possible for network operators to configure a PA multicast address\n   for each\
    \ multicast address that is used inside a VSID; this\n   facilitates optimal multicast\
    \ handling.  Depending on the hardware\n   capabilities of the physical network\
    \ devices and the physical network\n   architecture, multiple virtual subnets\
    \ may use the same physical IP\n   multicast address.\n   Alternatively, based\
    \ upon the configuration at the NVE, broadcast and\n   multicast in the virtual\
    \ subnet can be supported using N-way unicast.\n   In N-way unicast, the sender\
    \ NVE would send one encapsulated packet\n   to every NVE in the virtual subnet.\
    \  The sender NVE can encapsulate\n   and send the packet as described in Section\
    \ 4.3 (\"Unicast Traffic\").\n   This alleviates the need for multicast support\
    \ in the physical\n   network.\n"
- title: 4.3.  Unicast Traffic
  contents:
  - "4.3.  Unicast Traffic\n   The NVGRE endpoint encapsulates a Layer 2 packet in\
    \ GRE using the\n   source PA associated with the endpoint with the destination\
    \ PA\n   corresponding to the location of the destination endpoint.  As\n   outlined\
    \ earlier, there can be one or more PAs associated with an\n   endpoint and policy\
    \ will control which ones get used for\n   communication.  The encapsulated GRE\
    \ packet is bridged and routed\n   normally by the physical network to the destination\
    \ PA.  Bridging\n   uses the outer Ethernet encapsulation for scope on the LAN.\
    \  The only\n   requirement is bidirectional IP connectivity from the underlying\n\
    \   physical network.  On the destination, the NVGRE endpoint\n   decapsulates\
    \ the GRE packet to recover the original Layer 2 frame.\n   Traffic flows similarly\
    \ on the reverse path.\n"
- title: 4.4.  IP Fragmentation
  contents:
  - "4.4.  IP Fragmentation\n   Section 5.1 of RFC 2003 [12] specifies mechanisms\
    \ for handling\n   fragmentation when encapsulating IP within IP.  The subset\
    \ of\n   mechanisms NVGRE selects are intended to ensure that NVGRE-\n   encapsulated\
    \ frames are not fragmented after encapsulation en route\n   to the destination\
    \ NVGRE endpoint and that traffic sources can\n   leverage Path MTU discovery.\n\
    \   A sender NVE MUST NOT fragment NVGRE packets.  A receiver NVE MAY\n   discard\
    \ fragmented NVGRE packets.  It is RECOMMENDED that the MTU of\n   the physical\
    \ network accommodates the larger frame size due to\n   encapsulation.  Path MTU\
    \ or configuration via control plane can be\n   used to meet this requirement.\n"
- title: 4.5.  Address/Policy Management and Routing
  contents:
  - "4.5.  Address/Policy Management and Routing\n   Address acquisition is beyond\
    \ the scope of this document and can be\n   obtained statically, dynamically,\
    \ or using stateless address\n   autoconfiguration.  CA and PA space can be either\
    \ IPv4 or IPv6.  In\n   fact, the address families don't have to match; for example,\
    \ a CA can\n   be IPv4 while the PA is IPv6, and vice versa.\n"
- title: 4.6.  Cross-Subnet, Cross-Premise Communication
  contents:
  - "4.6.  Cross-Subnet, Cross-Premise Communication\n   One application of this framework\
    \ is that it provides a seamless path\n   for enterprises looking to expand their\
    \ virtual machine hosting\n   capabilities into public clouds.  Enterprises can\
    \ bring their entire\n   IP subnet(s) and isolation policies, thus making the\
    \ transition to or\n   from the cloud simpler.  It is possible to move portions\
    \ of an IP\n   subnet to the cloud; however, that requires additional configuration\n\
    \   on the enterprise network and is not discussed in this document.\n   Enterprises\
    \ can continue to use existing communications models like\n   site-to-site VPN\
    \ to secure their traffic.\n   A VPN gateway is used to establish a secure site-to-site\
    \ tunnel over\n   the Internet, and all the enterprise services running in virtual\n\
    \   machines in the cloud use the VPN gateway to communicate back to the\n   enterprise.\
    \  For simplicity, we use a VPN gateway configured as a VM\n   (shown in Figure\
    \ 2) to illustrate cross-subnet, cross-premise\n   communication.\n   +-----------------------+\
    \        +-----------------------+\n   |       Server 1        |        |    \
    \   Server 2        |\n   | +--------+ +--------+ |        | +-------------------+\
    \ |\n   | | VM1    | | VM2    | |        | |    VPN Gateway    | |\n   | | IP=CA1\
    \ | | IP=CA2 | |        | | Internal  External| |\n   | |        | |        |\
    \ |        | |  IP=CAg   IP=GAdc | |\n   | +--------+ +--------+ |        | +-------------------+\
    \ |\n   |       Hypervisor      |        |     | Hypervisor| ^   |\n   +-----------------------+\
    \        +-------------------:---+\n               | IP=PA1                  \
    \ | IP=PA4    | :\n               |                          |           | :\n\
    \               |     +-------------------------+      | : VPN\n             \
    \  +-----|     Layer 3 Network     |------+ : Tunnel\n                     +-------------------------+\
    \        :\n                                  |                     :\n      \
    \  +-----------------------------------------------:--+\n        |           \
    \                                    :  |\n        |                     Internet\
    \                  :  |\n        |                                           \
    \    :  |\n        +-----------------------------------------------:--+\n    \
    \                              |                     v\n                     \
    \             |   +-------------------+\n                                  | \
    \  |    VPN Gateway    |\n                                  |---|            \
    \       |\n                             IP=GAcorp| External IP=GAcorp|\n     \
    \                                 +-------------------+\n                    \
    \                            |\n                                    +-----------------------+\n\
    \                                    |  Corp Layer 3 Network |\n             \
    \                       |      (In CA Space)    |\n                          \
    \          +-----------------------+\n                                       \
    \         |\n                                   +---------------------------+\n\
    \                                   |       Server X            |\n          \
    \                         | +----------+ +----------+ |\n                    \
    \               | | Corp VMe1| | Corp VMe2| |\n                              \
    \     | |  IP=CAe1 | |  IP=CAe2 | |\n                                   | +----------+\
    \ +----------+ |\n                                   |         Hypervisor    \
    \    |\n                                   +---------------------------+\n   \
    \         Figure 2: Cross-Subnet, Cross-Premise Communication\n   The packet flow\
    \ is similar to the unicast traffic flow between VMs;\n   the key difference in\
    \ this case is that the packet needs to be sent\n   to a VPN gateway before it\
    \ gets forwarded to the destination.  As\n   part of routing configuration in\
    \ the CA space, a per-tenant VPN\n   gateway is provisioned for communication\
    \ back to the enterprise.  The\n   example illustrates an outbound connection\
    \ between VM1 inside the\n   data center and VMe1 inside the enterprise network.\
    \  When the\n   outbound packet from CA1 to CAe1 reaches the hypervisor on Server\
    \ 1,\n   the NVE in Server 1 can perform the equivalent of a route lookup on\n\
    \   the packet.  The cross-premise packet will match the default gateway\n   rule,\
    \ as CAe1 is not part of the tenant virtual network in the data\n   center.  The\
    \ virtualization policy will indicate the packet to be\n   encapsulated and sent\
    \ to the PA of the tenant VPN gateway (PA4)\n   running as a VM on Server 2. \
    \ The packet is decapsulated on Server 2\n   and delivered to the VM gateway.\
    \  The gateway in turn validates and\n   sends the packet on the site-to-site\
    \ VPN tunnel back to the\n   enterprise network.  As the communication here is\
    \ external to the\n   data center, the PA address for the VPN tunnel is globally\
    \ routable.\n   The outer header of this packet is sourced from GAdc destined\
    \ to\n   GAcorp.  This packet is routed through the Internet to the enterprise\n\
    \   VPN gateway, which is the other end of the site-to-site tunnel; at\n   that\
    \ point, the VPN gateway decapsulates the packet and sends it\n   inside the enterprise\
    \ where the CAe1 is routable on the network.  The\n   reverse path is similar\
    \ once the packet reaches the enterprise VPN\n   gateway.\n"
- title: 4.7.  Internet Connectivity
  contents:
  - "4.7.  Internet Connectivity\n   To enable connectivity to the Internet, an Internet\
    \ gateway is needed\n   that bridges the virtualized CA space to the public Internet\
    \ address\n   space.  The gateway needs to perform translation between the\n \
    \  virtualized world and the Internet.  For example, the NVGRE endpoint\n   can\
    \ be part of a load balancer or a NAT that replaces the VPN Gateway\n   on Server\
    \ 2 shown in Figure 2.\n"
- title: 4.8.  Management and Control Planes
  contents:
  - "4.8.  Management and Control Planes\n   There are several protocols that can\
    \ manage and distribute policy;\n   however, it is outside the scope of this document.\
    \  Implementations\n   SHOULD choose a mechanism that meets their scale requirements.\n"
- title: 4.9.  NVGRE-Aware Devices
  contents:
  - "4.9.  NVGRE-Aware Devices\n   One example of a typical deployment consists of\
    \ virtualized servers\n   deployed across multiple racks connected by one or more\
    \ layers of\n   Layer 2 switches, which in turn may be connected to a Layer 3\
    \ routing\n   domain.  Even though routing in the physical infrastructure will\
    \ work\n   without any modification with NVGRE, devices that perform specialized\n\
    \   processing in the network need to be able to parse GRE to get access\n   to\
    \ tenant-specific information.  Devices that understand and parse\n   the VSID\
    \ can provide rich multi-tenant-aware services inside the data\n   center.  As\
    \ outlined earlier, it is imperative to exploit multiple\n   paths inside the\
    \ network through techniques such as ECMP.  The Key\n   field (a 32-bit field,\
    \ including both the VSID and the optional\n   FlowID) can provide additional\
    \ entropy to the switches to exploit\n   path diversity inside the network.  A\
    \ diverse ecosystem is expected\n   to emerge as more and more devices become\
    \ multi-tenant aware.  In the\n   interim, without requiring any hardware upgrades,\
    \ there are\n   alternatives to exploit path diversity with GRE by associating\n\
    \   multiple PAs with NVGRE endpoints with policy controlling the choice\n   of\
    \ which PA to use.\n   It is expected that communication can span multiple data\
    \ centers and\n   also cross the virtual/physical boundary.  Typical scenarios\
    \ that\n   require virtual-to-physical communication include access to storage\n\
    \   and databases.  Scenarios demanding lossless Ethernet functionality\n   may\
    \ not be amenable to NVGRE, as traffic is carried over an IP\n   network.  NVGRE\
    \ endpoints mediate between the network-virtualized and\n   non-network-virtualized\
    \ environments.  This functionality can be\n   incorporated into Top-of-Rack switches,\
    \ storage appliances, load\n   balancers, routers, etc., or built as a stand-alone\
    \ appliance.\n   It is imperative to consider the impact of any solution on host\n\
    \   performance.  Today's server operating systems employ sophisticated\n   acceleration\
    \ techniques such as checksum offload, Large Send Offload\n   (LSO), Receive Segment\
    \ Coalescing (RSC), Receive Side Scaling (RSS),\n   Virtual Machine Queue (VMQ),\
    \ etc.  These technologies should become\n   NVGRE aware.  IPsec Security Associations\
    \ (SAs) can be offloaded to\n   the NIC so that computationally expensive cryptographic\
    \ operations\n   are performed at line rate in the NIC hardware.  These SAs are\
    \ based\n   on the IP addresses of the endpoints.  As each packet on the wire\n\
    \   gets translated, the NVGRE endpoint SHOULD intercept the offload\n   requests\
    \ and do the appropriate address translation.  This will\n   ensure that IPsec\
    \ continues to be usable with network virtualization\n   while taking advantage\
    \ of hardware offload capabilities for improved\n   performance.\n"
- title: 4.10.  Network Scalability with NVGRE
  contents:
  - "4.10.  Network Scalability with NVGRE\n   One of the key benefits of using NVGRE\
    \ is the IP address scalability\n   and in turn MAC address table scalability\
    \ that can be achieved.  An\n   NVGRE endpoint can use one PA to represent multiple\
    \ CAs.  This lowers\n   the burden on the MAC address table sizes at the Top-of-Rack\n\
    \   switches.  One obvious benefit is in the context of server\n   virtualization,\
    \ which has increased the demands on the network\n   infrastructure.  By embedding\
    \ an NVGRE endpoint in a hypervisor, it\n   is possible to scale significantly.\
    \  This framework enables location\n   information to be preconfigured inside\
    \ an NVGRE endpoint, thus\n   allowing broadcast ARP traffic to be proxied locally.\
    \  This approach\n   can scale to large-sized virtual subnets.  These virtual\
    \ subnets can\n   be spread across multiple Layer 3 physical subnets.  It allows\n\
    \   workloads to be moved around without imposing a huge burden on the\n   network\
    \ control plane.  By eliminating most broadcast traffic and\n   converting others\
    \ to multicast, the routers and switches can function\n   more optimally by building\
    \ efficient multicast trees.  By using\n   server and network capacity efficiently,\
    \ it is possible to drive down\n   the cost of building and managing data centers.\n"
- title: 5.  Security Considerations
  contents:
  - "5.  Security Considerations\n   This proposal extends the Layer 2 subnet across\
    \ the data center and\n   increases the scope for spoofing attacks.  Mitigations\
    \ of such\n   attacks are possible with authentication/encryption using IPsec\
    \ or\n   any other IP-based mechanism.  The control plane for policy\n   distribution\
    \ is expected to be secured by using any of the existing\n   security protocols.\
    \  Further management traffic can be isolated in a\n   separate subnet/VLAN.\n\
    \   The checksum in the GRE header is not supported.  The mitigation of\n   this\
    \ is to deploy an NVGRE-based solution in a network that provides\n   error detection\
    \ along the NVGRE packet path, for example, using\n   Ethernet Cyclic Redundancy\
    \ Check (CRC) or IPsec or any other error\n   detection mechanism.\n"
- title: 6.  Normative References
  contents:
  - "6.  Normative References\n   [1]  Bradner, S., \"Key words for use in RFCs to\
    \ Indicate Requirement\n        Levels\", BCP 14, RFC 2119, DOI 10.17487/RFC2119,\
    \ March 1997,\n        <http://www.rfc-editor.org/info/rfc2119>.\n   [2]  IANA,\
    \ \"IEEE 802 Numbers\",\n        <http://www.iana.org/assignments/ieee-802-numbers>.\n\
    \   [3]  Farinacci, D., Li, T., Hanks, S., Meyer, D., and P. Traina,\n       \
    \ \"Generic Routing Encapsulation (GRE)\", RFC 2784,\n        DOI 10.17487/RFC2784,\
    \ March 2000,\n        <http://www.rfc-editor.org/info/rfc2784>.\n   [4]  Dommety,\
    \ G., \"Key and Sequence Number Extensions to GRE\",\n        RFC 2890, DOI 10.17487/RFC2890,\
    \ September 2000,\n        <http://www.rfc-editor.org/info/rfc2890>.\n   [5] \
    \ IEEE, \"IEEE Standard for Local and metropolitan area\n        networks--Media\
    \ Access Control (MAC) Bridges and Virtual Bridged\n        Local Area Networks\"\
    , IEEE Std 802.1Q.\n   [6]  Greenberg, A., et al., \"VL2: A Scalable and Flexible\
    \ Data Center\n        Network\", Communications of the ACM,\n        DOI 10.1145/1897852.1897877,\
    \ 2011.\n   [7]  Greenberg, A., et al., \"The Cost of a Cloud: Research Problems\n\
    \        in Data Center Networks\", ACM SIGCOMM Computer Communication\n     \
    \   Review, DOI 10.1145/1496091.1496103, 2009.\n   [8]  Hinden, R. and S. Deering,\
    \ \"IP Version 6 Addressing\n        Architecture\", RFC 4291, DOI 10.17487/RFC4291,\
    \ February 2006,\n        <http://www.rfc-editor.org/info/rfc4291>.\n   [9]  Meyer,\
    \ D., \"Administratively Scoped IP Multicast\", BCP 23,\n        RFC 2365, DOI\
    \ 10.17487/RFC2365, July 1998,\n        <http://www.rfc-editor.org/info/rfc2365>.\n\
    \   [10] Narten, T., Ed., Gray, E., Ed., Black, D., Fang, L., Kreeger,\n     \
    \   L., and M. Napierala, \"Problem Statement: Overlays for Network\n        Virtualization\"\
    , RFC 7364, DOI 10.17487/RFC7364, October 2014,\n        <http://www.rfc-editor.org/info/rfc7364>.\n\
    \   [11] Lasserre, M., Balus, F., Morin, T., Bitar, N., and Y. Rekhter,\n    \
    \    \"Framework for Data Center (DC) Network Virtualization\",\n        RFC 7365,\
    \ DOI 10.17487/RFC7365, October 2014,\n        <http://www.rfc-editor.org/info/rfc7365>.\n\
    \   [12] Perkins, C., \"IP Encapsulation within IP\", RFC 2003,\n        DOI 10.17487/RFC2003,\
    \ October 1996,\n        <http://www.rfc-editor.org/info/rfc2003>.\n   [13] Touch,\
    \ J. and R. Perlman, \"Transparent Interconnection of Lots\n        of Links (TRILL):\
    \ Problem and Applicability Statement\",\n        RFC 5556, DOI 10.17487/RFC5556,\
    \ May 2009,\n        <http://www.rfc-editor.org/info/rfc5556>.\n"
- title: Contributors
  contents:
  - "Contributors\n   Murari Sridharan\n   Microsoft Corporation\n   1 Microsoft Way\n\
    \   Redmond, WA 98052\n   United States\n   Email: muraris@microsoft.com\n   Albert\
    \ Greenberg\n   Microsoft Corporation\n   1 Microsoft Way\n   Redmond, WA 98052\n\
    \   United States\n   Email: albert@microsoft.com\n   Narasimhan Venkataramiah\n\
    \   Microsoft Corporation\n   1 Microsoft Way\n   Redmond, WA 98052\n   United\
    \ States\n   Email: navenkat@microsoft.com\n   Kenneth Duda\n   Arista Networks,\
    \ Inc.\n   5470 Great America Pkwy\n   Santa Clara, CA 95054\n   United States\n\
    \   Email: kduda@aristanetworks.com\n   Ilango Ganga\n   Intel Corporation\n \
    \  2200 Mission College Blvd.\n   M/S: SC12-325\n   Santa Clara, CA 95054\n  \
    \ United States\n   Email: ilango.s.ganga@intel.com\n   Geng Lin\n   Google\n\
    \   1600 Amphitheatre Parkway\n   Mountain View, CA 94043\n   United States\n\
    \   Email: genglin@google.com\n   Mark Pearson\n   Hewlett-Packard Co.\n   8000\
    \ Foothills Blvd.\n   Roseville, CA 95747\n   United States\n   Email: mark.pearson@hp.com\n\
    \   Patricia Thaler\n   Broadcom Corporation\n   3151 Zanker Road\n   San Jose,\
    \ CA 95134\n   United States\n   Email: pthaler@broadcom.com\n   Chait Tumuluri\n\
    \   Emulex Corporation\n   3333 Susan Street\n   Costa Mesa, CA 92626\n   United\
    \ States\n   Email: chait@emulex.com\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Pankaj Garg (editor)\n   Microsoft Corporation\n   1 Microsoft\
    \ Way\n   Redmond, WA 98052\n   United States\n   Email: pankajg@microsoft.com\n\
    \   Yu-Shun Wang (editor)\n   Microsoft Corporation\n   1 Microsoft Way\n   Redmond,\
    \ WA 98052\n   United States\n   Email: yushwang@microsoft.com\n"
