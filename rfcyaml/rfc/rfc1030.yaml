- title: __initial_text__
  contents:
  - '          On Testing the NETBLT Protocol over Divers Networks

    '
- title: STATUS OF THIS MEMO
  contents:
  - "STATUS OF THIS MEMO\n   This RFC describes the results gathered from testing\
    \ NETBLT over\n   three networks of differing bandwidths and round-trip delays.\
    \  While\n   the results are not complete, the information gathered so far has\n\
    \   been very promising and supports RFC-998's assertion that that NETBLT\n  \
    \ can provide very high throughput over networks with very different\n   characteristics.\
    \  Distribution of this memo is unlimited.\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   NETBLT (NETwork BLock Transfer) is a transport level protocol\n\
    \   intended for the rapid transfer of a large quantity of data between\n   computers.\
    \  It provides a transfer that is reliable and flow\n   controlled, and is designed\
    \ to provide maximum throughput over a wide\n   variety of networks.  The NETBLT\
    \ protocol is specified in RFC-998;\n   this document assumes an understanding\
    \ of the specification as\n   described in RFC-998.\n   Tests over three different\
    \ networks are described in this document.\n   The first network, a 10 megabit-per-second\
    \ Proteon Token Ring, served\n   as a \"reference environment\" to determine NETBLT's\
    \ best possible\n   performance.  The second network, a 10 megabit-per-second\
    \ Ethernet,\n   served as an access path to the third network, the 3 megabit-per-\n\
    \   second Wideband satellite network.  Determining NETBLT's performance\n   over\
    \ the Ethernet allowed us to account for Ethernet-caused behaviour\n   in NETBLT\
    \ transfers that used the Wideband network.  Test results for\n   each network\
    \ are described in separate sections.  The final section\n   presents some conclusions\
    \ and further directions of research.  The\n   document's appendices list test\
    \ results in detail.\n"
- title: 2. Acknowledgements
  contents:
  - "2. Acknowledgements\n   Many thanks are due Bob Braden, Stephen Casner, and Annette\
    \ DeSchon\n   of ISI for the time they spent analyzing and commenting on test\n\
    \   results gathered at the ISI end of the NETBLT Wideband network tests.\n  \
    \ Bob Braden was also responsible for porting the IBM PC/AT NETBLT\n   implementation\
    \ to a SUN-3 workstation running UNIX.  Thanks are also\n   due Mike Brescia,\
    \ Steven Storch, Claudio Topolcic and others at BBN\n   who provided much useful\
    \ information about the Wideband network, and\n   helped monitor it during testing.\n"
- title: 3. Implementations and Test Programs
  contents:
  - "3. Implementations and Test Programs\n   This section briefly describes the NETBLT\
    \ implementations and test\n   programs used in the testing.  Currently, NETBLT\
    \ runs on three\n   machine types: Symbolics LISP machines, IBM PC/ATs, and SUN-3s.\
    \  The\n   test results described in this paper were gathered using the IBM\n\
    \   PC/AT and SUN-3 NETBLT implementations.  The IBM and SUN\n   implementations\
    \ are very similar; most differences lie in timer and\n   multi-tasking library\
    \ implementations.  The SUN NETBLT implementation\n   uses UNIX's user-accessible\
    \ raw IP socket; it is not implemented in\n   the UNIX kernel.\n   The test application\
    \ performs a simple memory-to-memory transfer of\n   an arbitrary amount of data.\
    \  All data are actually allocated by the\n   application, given to the protocol\
    \ layer, and copied into NETBLT\n   packets.  The results are therefore fairly\
    \ realistic and, with\n   appropriately large amounts of buffering, could be attained\
    \ by disk-\n   based applications as well.\n   The test application provides several\
    \ parameters that can be varied\n   to alter NETBLT's performance characteristics.\
    \  The most important of\n   these parameters are:\n        burst interval  The\
    \ number of milliseconds from the start of one\n                        burst\
    \ transmission to the start of the next burst\n                        transmission.\n\
    \        burst size      The number of packets transmitted per burst.\n      \
    \  buffer size     The number of bytes in a NETBLT buffer (all\n             \
    \           buffers must be the same size, save the last,\n                  \
    \      which can be any size required to complete the\n                      \
    \  transfer).\n        data packet size\n                        The number of\
    \ bytes contained in a NETBLT DATA\n                        packet's data segment.\n\
    \        number of outstanding buffers\n                       The number of buffers\
    \ which can be in\n                       transmission/error recovery at any given\
    \ moment.\n   The protocol's throughput is measured in two ways.  First, the \"\
    real\n   throughput\" is throughput as viewed by the user: the number of bits\n\
    \   transferred divided by the time from program start to program finish.\n  \
    \ Although this is a useful measurement from the user's point of view,\n   another\
    \ throughput measurement is more useful for analyzing NETBLT's\n   performance.\
    \  The \"steady-state throughput\" is the rate at which data\n   is transmitted\
    \ as the transfer size approaches infinity.  It does not\n   take into account\
    \ connection setup time, and (more importantly), does\n   not take into account\
    \ the time spent recovering from packet-loss\n   errors that occur after the last\
    \ buffer in the transmission is sent\n   out.  For NETBLT transfers using networks\
    \ with long round-trip delays\n   (and consequently with large numbers of outstanding\
    \ buffers), this\n   \"late\" recovery phase can add large amounts of time to\
    \ the\n   transmission, time which does not reflect NETBLT's peak transmission\n\
    \   rate.  The throughputs listed in the test cases that follow are all\n   steady-state\
    \ throughputs.\n"
- title: 4. Implementation Performance
  contents:
  - "4. Implementation Performance\n   This section describes the theoretical performance\
    \ of the IBM PC/AT\n   NETBLT implementation on both the transmitting and receiving\
    \ sides.\n   Theoretical performance was measured on two LANs: a 10 megabit-per-\n\
    \   second Proteon Token Ring and a 10 megabit-per-second Ethernet.\n   \"Theoretical\
    \ performance\" is defined to be the performance achieved\n   if the sending NETBLT\
    \ did nothing but transmit data packets, and the\n   receiving NETBLT did nothing\
    \ but receive data packets.\n   Measuring the send-side's theoretical performance\
    \ is fairly easy,\n   since the sending NETBLT does very little more than transmit\
    \ packets\n   at a predetermined rate.  There are few, if any, factors which can\n\
    \   influence the processing speed one way or another.\n   Using a Proteon P1300\
    \ interface on a Proteon Token Ring, the IBM\n   PC/AT NETBLT implementation can\
    \ copy a maximum-sized packet (1990\n   bytes excluding protocol headers) from\
    \ NETBLT buffer to NETBLT data\n   packet, format the packet header, and transmit\
    \ the packet onto the\n   network in about 8 milliseconds.  This translates to\
    \ a maximum\n   theoretical throughput of 1.99 megabits per second.\n   Using\
    \ a 3COM 3C500 interface on an Ethernet LAN, the same\n   implementation can transmit\
    \ a maximum-sized packet (1438 bytes\n   excluding protocol headers) in 6.0 milliseconds,\
    \ for a maximum\n   theoretical throughput of 1.92 megabits per second.\n   Measuring\
    \ the receive-side's theoretical performance is more\n   difficult.  Since all\
    \ timer management and message ACK overhead is\n   incurred at the receiving NETBLT's\
    \ end, the processing speed can be\n   slightly slower than the sending NETBLT's\
    \ processing speed (this does\n   not even take into account the demultiplexing\
    \ overhead that the\n   receiver incurs while matching packets with protocol handling\n\
    \   functions and connections).  In fact, the amount by which the two\n   processing\
    \ speeds differ is dependent on several factors, the most\n   important of which\
    \ are: length of the NETBLT buffer list, the number\n   of data timers which may\
    \ need to be set, and the number of control\n   messages which are ACKed by the\
    \ data packet.  Almost all of this\n   added overhead is directly related to the\
    \ number of outstanding\n   buffers allowable during the transfer.  The fewer\
    \ the number of\n   outstanding buffers, the shorter the NETBLT buffer list, and\
    \ the\n   faster a scan through the buffer list and the shorter the list of\n\
    \   unacknowledged control messages.\n   Assuming a single-outstanding-buffer\
    \ transfer, the receiving-side\n   NETBLT can DMA a maximum-sized data packet\
    \ from the Proteon Token\n   Ring into its network interface, copy it from the\
    \ interface into a\n   packet buffer and finally copy the packet into the correct\
    \ NETBLT\n   buffer in 8 milliseconds: the same speed as the sender of data.\n\
    \   Under the same conditions, the implementation can receive a maximum-\n   sized\
    \ packet from the Ethernet in 6.1 milliseconds, for a maximum\n   theoretical\
    \ throughput of 1.89 megabits per second.\n"
- title: 5. Testing on a Proteon Token Ring
  contents:
  - "5. Testing on a Proteon Token Ring\n   The Proteon Token Ring used for testing\
    \ is a 10 megabit-per-second\n   LAN supporting about 40 hosts.  The machines\
    \ on either end of the\n   transfer were IBM PC/ATs using Proteon P1300 network\
    \ interfaces.  The\n   Token Ring provides high bandwidth with low round-trip\
    \ delay and\n   negligible packet loss, a good debugging environment in situations\n\
    \   where packet loss, packet reordering, and long round-trip time would\n   hinder\
    \ debugging.  Also contributing to high performance is the large\n   (maximum\
    \ 2046 bytes) network MTU.  The larger packets take somewhat\n   longer to transmit\
    \ than do smaller packets (8 milliseconds per 2046\n   byte packet versus 6 milliseconds\
    \ per 1500 byte packet), but the\n   lessened per-byte computational overhead\
    \ increases throughput\n   somewhat.\n   The fastest single-outstanding-buffer\
    \ transmission rate was 1.49\n   megabits per second, and was achieved using a\
    \ test case with the\n   following parameters:\n      transfer size   2-5 million\
    \ bytes\n      data packet size\n                      1990 bytes\n      buffer\
    \ size     19900 bytes\n      burst size      5 packets\n      burst interval\
    \  40 milliseconds.  The timer code on the IBM PC/AT\n                      is\
    \ accurate to within 1 millisecond, so a 40\n                      millisecond\
    \ burst can be timed very accurately.\n   Allowing only one outstanding buffer\
    \ reduced the protocol to running\n   \"lock-step\" (the receiver of data sends\
    \ a GO, the sender sends data,\n   the receiver sends an OK, followed by a GO\
    \ for the next buffer).\n   Since the lock-step test incurred one round-trip-delay's\
    \ worth of\n   overhead per buffer (between transmission of a buffer's last data\n\
    \   packet and receipt of an OK for that buffer/GO for the next buffer),\n   a\
    \ test with two outstanding buffers (providing essentially constant\n   packet\
    \ transmission) should have resulted in higher throughput.\n   A second test,\
    \ this time with two outstanding buffers, was performed,\n   with the above parameters\
    \ identical save for an increased burst\n   interval of 43 milliseconds.  The\
    \ highest throughput recorded was\n   1.75 megabits per second.  This represents\
    \ 95% efficiency (5 1990-\n   byte packets every 43 milliseconds gives a maximum\
    \ theoretical\n   throughput of 1.85 megabits per second).  The increase in throughput\n\
    \   over a single-outstanding-buffer transmission occurs because, with\n   two\
    \ outstanding buffers, there is no round-trip-delay lag between\n   buffer transmissions\
    \ and the sending NETBLT can transmit constantly.\n   Because the P1300 interface\
    \ can transmit and receive concurrently, no\n   packets were dropped due to collision\
    \ on the interface.\n   As mentioned previously, the minimum transmission time\
    \ for a\n   maximum-sized packet on the Proteon Ring is 8 milliseconds.  One\n\
    \   would expect, therefore, that the maximum throughput for a double-\n   buffered\
    \ transmission would occur with a burst interval of 8\n   milliseconds times 5\
    \ packets per burst, or 40 milliseconds.  This\n   would allow the sender of data\
    \ to transmit bursts with no \"dead time\"\n   in between bursts.  Unfortunately,\
    \ the sender of data must take time\n   to process incoming control messages,\
    \ which typically forces a 2-3\n   millisecond gap between bursts, lowering the\
    \ throughput.  With a\n   burst interval of 43 milliseconds, the incoming packets\
    \ are processed\n   during the 3 millisecond-per-burst \"dead time\", making the\
    \ protocol\n   more efficient.\n"
- title: 6. Testing on an Ethernet
  contents:
  - "6. Testing on an Ethernet\n   The network used in performing this series of tests\
    \ was a 10 megabit\n   per second Ethernet supporting about 150 hosts.  The machines\
    \ at\n   either end of the NETBLT connection were IBM PC/ATs using 3COM 3C500\n\
    \   network interfaces.  As with the Proteon Token Ring, the Ethernet\n   provides\
    \ high bandwidth with low delay.  Unfortunately, the\n   particular Ethernet used\
    \ for testing (MIT's infamous Subnet 26) is\n   known for being somewhat noisy.\
    \  In addition, the 3COM 3C500 Ethernet\n   interfaces are relatively unsophisticated,\
    \ with only a single\n   hardware packet buffer for both transmitting and receiving\
    \ packets.\n   This gives the interface an annoying tendency to drop packets under\n\
    \   heavy load.  The combination of these factors made protocol\n   performance\
    \ analysis somewhat more difficult than on the Proteon\n   Ring.\n   The fastest\
    \ single-buffer transmission rate was 1.45 megabits per\n   second, and was achieved\
    \ using a test case with the following\n   parameters:\n      transfer size  \
    \ 2-5 million bytes\n      data packet size\n                      1438 bytes\
    \ (maximum size excluding protocol\n                      headers).\n      buffer\
    \ size     14380 bytes\n      burst size      5 packets\n      burst interval\
    \  30 milliseconds (6.0 milliseconds x 5 packets).\n   A second test, this one\
    \ with parameters identical to the first save\n   for number of outstanding buffers\
    \ (2 instead of 1) resulted in\n   substantially lower throughput (994 kilobits\
    \ per second), with a\n   large number of packets retransmitted (10%).  The retransmissions\n\
    \   occurred because the 3COM 3C500 network interface has only one\n   hardware\
    \ packet buffer and cannot hold a transmitting and receiving\n   packet at the\
    \ same time.  With two outstanding buffers, the sender of\n   data can transmit\
    \ constantly; this means that when the receiver of\n   data attempts to send a\
    \ packet, its interface's receive hardware goes\n   deaf to the network and any\
    \ packets being transmitted at the time by\n   the sender of data are lost.  A\
    \ symmetrical problem occurs with\n   control messages sent from receiver of data\
    \ to sender of data, but\n   the number of control messages sent is small enough\
    \ and the\n   retransmission algorithm redundant enough that little performance\n\
    \   degradation occurs due to control message loss.\n   When the burst interval\
    \ was lengthened from 30 milliseconds per 5\n   packet burst to 45 milliseconds\
    \ per 5 packet burst, a third as many\n   packets were dropped, and throughput\
    \ climbed accordingly, to 1.12\n   megabits per second.  Presumably, the longer\
    \ burst interval allowed\n   more dead time between bursts and less likelihood\
    \ of the receiver of\n   data's interface being deaf to the net while the sender\
    \ of data was\n   sending a packet.  An interesting note is that, when the same\
    \ test\n   was conducted on a special Ethernet LAN with the only two hosts\n \
    \  attached being the two NETBLT machines, no packets were dropped once\n   the\
    \ burst interval rose above 40 milliseconds/5 packet burst.  The\n   improved\
    \ performance was doubtless due to the absence of extra\n   network traffic.\n"
- title: 7. Testing on the Wideband Network
  contents:
  - "7. Testing on the Wideband Network\n   The following section describes results\
    \ gathered using the Wideband\n   network.  The Wideband network is a satellite-based\
    \ network with ten\n   stations competing for a raw satellite channel bandwidth\
    \ of 3\n   megabits per second.  Since the various tests resulted in substantial\n\
    \   changes to the NETBLT specification and implementation, some of the\n   major\
    \ changes are described along with the results and problems that\n   forced those\
    \ changes.\n   The Wideband network has several characteristics that make it an\n\
    \   excellent environment for testing NETBLT.  First, it has an extremely\n  \
    \ long round-trip delay (1.8 seconds).  This provides a good test of\n   NETBLT's\
    \ rate control and multiple-buffering capabilities.  NETBLT's\n   rate control\
    \ allows the packet transmission rate to be regulated\n   independently of the\
    \ maximum allowable amount of outstanding data,\n   providing flow control as\
    \ well as very large \"windows\".  NETBLT's\n   multiple-buffering capability\
    \ enables data to still be transmitted\n   while earlier data are awaiting retransmission\
    \ and subsequent data\n   are being prepared for transmission.  On a network with\
    \ a long\n   round-trip delay, the alternative \"lock-step\" approach would require\n\
    \   a 1.8 second gap between each buffer transmission, degrading\n   performance.\n\
    \   Another interesting characteristic of the Wideband network is its\n   throughput.\
    \  Although its raw bandwidth is 3 megabits per second, at\n   the time of these\
    \ tests fully 2/3 of that was consumed by low-level\n   network overhead and hardware\
    \ limitations.  (A detailed analysis of\n   the overhead appears at the end of\
    \ this document.)  This reduces the\n   available bandwidth to just over 1 megabit\
    \ per second.  Since the\n   NETBLT implementation can run substantially faster\
    \ than that, testing\n   over the Wideband net allows us to measure NETBLT's ability\
    \ to\n   utilize very high percentages of available bandwidth.\n   Finally, the\
    \ Wideband net has some interesting packet reorder and\n   delay characteristics\
    \ that provide a good test of NETBLT's ability to\n   deal with these problems.\n\
    \   Testing progressed in several phases.  The first phase involved using\n  \
    \ source-routed packets in a path from an IBM PC/AT on MIT's Subnet 26,\n   through\
    \ a BBN Butterfly Gateway, over a T1 link to BBN, onto the\n   Wideband network,\
    \ back down into a BBN Voice Funnel, and onto ISI's\n   Ethernet to another IBM\
    \ PC/AT.  Testing proceeded fairly slowly, due\n   to gateway software and source-routing\
    \ bugs.  Once a connection was\n   finally established, we recorded a best throughput\
    \ of approximately\n   90K bits per second.\n   Several problems contributed to\
    \ the low throughput.  First, the\n   gateways at either end were forwarding packets\
    \ onto their respective\n   LANs faster than the IBM PC/AT's could accept them\
    \ (the 3COM 3C500\n   interface would not have time to re-enable input before\
    \ another\n   packet would arrive from the gateway).  Even with bursts of size\
    \ 1,\n   spaced 6 milliseconds apart, the gateways would aggregate groups of\n\
    \   packets coming from the same satellite frame, and send them faster\n   than\
    \ the PC could receive them.  The obvious result was many dropped\n   packets,\
    \ and degraded performance.  Also, the half-duplex nature of\n   the 3COM interface\
    \ caused incoming packets to be dropped when packets\n   were being sent.\n  \
    \ The number of packets dropped on the sending NETBLT side due to the\n   long\
    \ interface re-enable time was reduced by packing as many control\n   messages\
    \ as possible into a single control packet (rather than\n   placing only one message\
    \ in a control packet).  This reduced the\n   number of control packets transmitted\
    \ to one per buffer transmission,\n   which the PC was able to handle.  In particular,\
    \ messages of the form\n   OK(n) were combined with messages of the form GO(n\
    \ + 1), in order to\n   prevent two control packets from arriving too close together\
    \ to both\n   be received.\n   Performance degradation from dropped control packets\
    \ was also\n   minimized by changing to a highly redundant control packet\n  \
    \ transmission algorithm.  Control messages are now stored in a single\n   long-lived\
    \ packet, with ACKed messages continuously bumped off the\n   head of the packet\
    \ and new messages added at the tail of the packet.\n   Every time a new message\
    \ needs to be transmitted, any unACKed old\n   messages are transmitted as well.\
    \  The sending NETBLT, which receives\n   these control messages, is tuned to\
    \ ignore duplicate messages with\n   almost no overhead.  This transmission redundancy\
    \ puts little\n   reliance on the NETBLT control timer, further reducing performance\n\
    \   degradation from lost control packets.\n   Although the effect of dropped\
    \ packets on the receiving NETBLT could\n   not be completely eliminated, it was\
    \ reduced somewhat by some changes\n   to the implementation.  Data packets from\
    \ the sending NETBLT are\n   guaranteed to be transmitted by buffer number, lowest\
    \ number first.\n   In some cases, this allowed the receiving NETBLT to make retransmit-\n\
    \   request decisions for a buffer N, if packets for N were expected but\n   none\
    \ were received at the time packets for a buffer N+M were\n   received.  This\
    \ optimization was somewhat complicated, but improved\n   NETBLT's performance\
    \ in the face of missing packets.  Unfortunately,\n   the dropped-packet problem\
    \ remained until the NETBLT implementation\n   was ported to a SUN-3 workstation.\
    \  The SUN is able to handle the\n   incoming packets quite well, dropping only\
    \ 0.5% of the data packets\n   (as opposed to the PC's 15 - 20%).\n   Another\
    \ problem with the Wideband network was its tendency to re-\n   order and delay\
    \ packets.  Dealing with these problems required\n   several changes in the implementation.\
    \  Previously, the NETBLT\n   implementation was \"optimized\" to generate retransmit\
    \ requests as\n   soon as possible, if possible not relying on expiration of a\
    \ data\n   timer.  For instance, when the receiving NETBLT received an LDATA\n\
    \   packet for a buffer N, and other packets in buffer N had not arrived,\n  \
    \ the receiver would immediately generate a RESEND for the missing\n   packets.\
    \  Similarly, under certain circumstances, the receiver would\n   generate a RESEND\
    \ for a buffer N if packets for N were expected and\n   had not arrived before\
    \ packets for a buffer N+M.  Obviously, packet-\n   reordering made these \"optimizations\"\
    \ generate retransmit requests\n   unnecessarily.  In the first case, the implementation\
    \ was changed to\n   no longer generate a retransmit request on receipt of an\
    \ LDATA with\n   other packets missing in the buffer.  In the second case, a data\n\
    \   timer was set with an updated (and presumably more accurate) value,\n   hopefully\
    \ allowing any re-ordered packets to arrive before timing out\n   and generating\
    \ a retransmit request.\n   It is difficult to accommodate Wideband network packet\
    \ delay in the\n   NETBLT implementation.  Packet delays tend to occur in multiples\
    \ of\n   600 milliseconds, due to the Wideband network's datagram reservation\n\
    \   scheme.  A timer value calculation algorithm that used a fixed\n   variance\
    \ on the order of 600 milliseconds would cause performance\n   degradation when\
    \ packets were lost.  On the other hand, short fixed\n   variance values would\
    \ not react well to the long delays possible on\n   the Wideband net.  Our solution\
    \ has been to use an adaptive data\n   timer value calculation algorithm.  The\
    \ algorithm maintains an\n   average inter-packet arrival value, and uses that\
    \ to determine the\n   data timer value.  If the inter-packet arrival time increases,\
    \ the\n   data timer value will lengthen.\n   At this point, testing proceeded\
    \ between NETBLT implementations on a\n   SUN-3 workstation and an IBM PC/AT.\
    \  The arrival of a Butterfly\n   Gateway at ISI eliminated the need for source-routed\
    \ packets; some\n   performance improvement was also expected because the Butterfly\n\
    \   Gateway is optimized for IP datagram traffic.\n   In order to put the best\
    \ Wideband network test results in context, a\n   short analysis follows, showing\
    \ the best throughput expected on a\n   fully loaded channel.  Again, a detailed\
    \ analysis of the numbers that\n   follow appears at the end of this document.\n\
    \   The best possible datagram rate over the current Wideband\n   configuration\
    \ is 24,054 bits per channel frame, or 3006 bytes every\n   21.22 milliseconds.\
    \  Since the transmission route begins and ends on\n   an Ethernet, the largest\
    \ amount of data transmissible (after\n   accounting for packet header overhead)\
    \ is 1438 bytes per packet.\n   This translates to approximately 2 packets per\
    \ frame.  Since we want\n   to avoid overflowing the channel, we should transmit\
    \ slightly slower\n   than the channel frame rate of 21.2 milliseconds.  We therefore\
    \ came\n   up with a best possible throughput of 2 1438-byte packets every 22\n\
    \   milliseconds, or 1.05 megabits per second.\n   Because of possible software\
    \ bugs in either the Butterfly Gateway or\n   the BSAT (gateway-to-earth-station\
    \ interface), 1438-byte packets were\n   fragmented before transmission over the\
    \ Wideband network, causing\n   packet delay and poor performance.  The best throughput\
    \ was achieved\n   with the following values:\n      transfer size   500,000 -\
    \ 750,000 bytes\n      data packet size\n                      1432 bytes\n  \
    \    buffer size     14320 bytes\n      burst size      5 packets\n      burst\
    \ interval  55 milliseconds\n   Steady-state throughputs ranged from 926 kilobits\
    \ per second to 942\n   kilobits per second, approximately 90% channel utilization.\
    \  The\n   amount of data transmitted should have been an order of magnitude\n\
    \   higher, in order to get a longer steady-state period; unfortunately\n   at\
    \ the time we were testing, the Ethernet interface of ISI's\n   Butterfly Gateway\
    \ would lock up fairly quickly (in 40-60 seconds) at\n   packet rates of approximately\
    \ 90 per second, forcing a gateway reset.\n   Transmissions therefore had to take\
    \ less than this amount of time.\n   This problem has reportedly been fixed since\
    \ the tests were\n   conducted.\n   In order to test the Wideband network under\
    \ overload conditions, we\n   attempted several tests at rates of 5 1432-byte\
    \ packets every 50\n   milliseconds.  At this rate, the Wideband network ground\
    \ to a halt as\n   four of the ten network BSATs immediately crashed and reset\
    \ their\n   channel processor nodes.  Apparently, the BSATs crash because the\
    \ ESI\n   (Earth Station Interface), which sends data from the BSAT to the\n \
    \  satellite, stops its transmit clock to the BSAT if it runs out of\n   buffer\
    \ space.  The BIO interface connecting BSAT and ESI does not\n   tolerate this\
    \ clock-stopping, and typically locks up, forcing the\n   channel processor node\
    \ to reset.  A more sophisticated interface,\n   allowing faster transmissions,\
    \ is being installed in the near future.\n"
- title: 8. Future Directions
  contents:
  - "8. Future Directions\n   Some more testing needs to be performed over the Wideband\
    \ Network in\n   order to get a complete analysis of NETBLT's performance.  Once\
    \ the\n   Butterfly Gateway Ethernet interface lockup problem described earlier\n\
    \   has been fixed, we want to perform transmissions of 10 to 50 million\n   bytes\
    \ to get accurate steady-state throughput results.  We also want\n   to run several\
    \ NETBLT processes in parallel, each tuned to take a\n   fraction of the Wideband\
    \ Network's available bandwidth.  Hopefully,\n   this will demonstrate whether\
    \ or not burst synchronization across\n   different NETBLT processes will cause\
    \ network congestion or failure.\n   Once the BIO BSAT-ESI interface is upgraded,\
    \ we will want to try for\n   higher throughputs, as well as greater hardware\
    \ stability under\n   overload conditions.\n   As far as future directions of\
    \ research into NETBLT, one important\n   area needs to be explored.  A series\
    \ of algorithms need to be\n   developed to allow dynamic selection and control\
    \ of NETBLT's\n   transmission parameters (burst size, burst interval, and number\
    \ of\n   outstanding buffers).  Ideally, this dynamic control will not require\n\
    \   any information from outside sources such as gateways; instead,\n   NETBLT\
    \ processes will use end-to-end information in order to make\n   transmission\
    \ rate decisions in the face of noisy channels and network\n   congestion.  Some\
    \ research on dynamic rate control is taking place\n   now, but much more work\
    \ needs done before the results can be\n   integrated into NETBLT.\n"
- title: I. Wideband Bandwidth Analysis
  contents:
  - "I. Wideband Bandwidth Analysis\n   Although the raw bandwidth of the Wideband\
    \ Network is 3 megabits per\n   second, currently only about 1 megabit per second\
    \ of it is available\n   to transmit data.  The large amount of overhead is due\
    \ to the channel\n   control strategy (which uses a fixed-width control subframe\
    \ based on\n   the maximum number of stations sharing the channel) and the low-\n\
    \   performance BIO interface between BBN's BSAT (Butterfly Satellite\n   Interface)\
    \ and Linkabit's ESI (Earth Station Interface).  Higher-\n   performance BSMI\
    \ interfaces are soon to be installed in all Wideband\n   sites, which should\
    \ improve the amount of available bandwidth.\n   Bandwidth on the Wideband network\
    \ is divided up into frames, each of\n   which has multiple subframes.  A frame\
    \ is 32768 channel symbols, at 2\n   bits per symbol.  One frame is available\
    \ for transmission every 21.22\n   milliseconds, giving a raw bandwidth of 65536\
    \ bits / 21.22 ms, or\n   3.081 megabits per second.\n   Each frame contains two\
    \ subframes, a control subframe and a data\n   subframe.  The control subframe\
    \ is subdivided into ten slots, one per\n   earth station.  Control information\
    \ takes up 200 symbols per station.\n   Because the communications interface between\
    \ BSAT and ESI only runs\n   at 2 megabits per second, there must be a padding\
    \ interval of 1263\n   symbols between each slot of information, bringing the\
    \ total control\n   subframe size up to 1463 symbols x 10 stations, or 14630 symbols.\n\
    \   The data subframe then has 18138 symbols available.  The maximum\n   datagram\
    \ size is currently expressed as a 14-bit quantity, further\n   dropping the maximum\
    \ amount of data in a frame to 16384 symbols.\n   After header information is\
    \ taken into account, this value drops to\n   16,036 symbols.  At 2 bits per symbol,\
    \ using a 3/4 coding rate, the\n   actual amount of usable data in a frame is\
    \ 24,054 bits, or\n   approximately 3006 bytes.  Thus the theoretical usable bandwidth\
    \ is\n   24,054 bits every 21.22 milliseconds, or 1.13 megabits per second.\n\
    \   Since the NETBLT implementations are running on Ethernet LANs\n   gatewayed\
    \ to the Wideband network, the 3006 bytes per channel frame\n   of usable bandwidth\
    \ translates to two maximum-sized (1500 bytes)\n   Ethernet packets per channel\
    \ frame, or 1.045 megabits per second.\n"
- title: II. Detailed Proteon Ring LAN Test Results
  contents:
  - "II. Detailed Proteon Ring LAN Test Results\n   Following is a table of some of\
    \ the test results gathered from\n   testing NETBLT between two IBM PC/ATs on\
    \ a Proteon Token Ring LAN.\n   The table headers have the following definitions:\n\
    \      BS/BI           burst size in packets and burst interval in\n         \
    \             milliseconds\n      PSZ             number of bytes in DATA/LDATA\
    \ packet data segment\n      BFSZ            number of bytes in NETBLT buffer\n\
    \      XFSZ            number of kilobytes in transfer\n      NBUFS          \
    \ number of outstanding buffers\n      #LOSS           number of data packets\
    \ lost\n      #RXM            number of data packets retransmitted\n      DTMOS\
    \           number of data timeouts on receiving end\n      SPEED           steady-state\
    \ throughput in megabits per second\n      BS/BI  PSZ    BFSZ   XFSZ   NBUFS \
    \ #LOSS  #RXM   DTMOS  SPEED\n      5/25   1438   14380  1438   1      0     \
    \ 0      0      1.45\n      5/25   1438   14380  1438   1      0      0      0\
    \      1.45\n      5/30   1438   14380  1438   1      0      0      0      1.45\n\
    \      5/30   1438   14380  1438   1      0      0      0      1.45\n      5/35\
    \   1438   14380  1438   1      0      0      0      1.40\n      5/35   1438 \
    \  14380  1438   1      0      0      0      1.41\n      5/40   1438   14380 \
    \ 1438   1      0      0      0      1.33\n      5/40   1438   14380  1438   1\
    \      0      0      0      1.33\n      5/25   1438   14380  1438   2      0 \
    \     0      0      1.62\n      5/25   1438   14380  1438   2      0      0  \
    \    0      1.61\n      5/30   1438   14380  1438   2      0      0      0   \
    \   1.60\n      5/30   1438   14380  1438   2      0      0      0      1.61\n\
    \      5/34   1438   14380  1438   2      0      0      0      1.59\n      5/35\
    \   1438   14380  1438   2      0      0      0      1.58\n      5/25   1990 \
    \  19900  1990   1      0      0      0      1.48\n      5/25   1990   19900 \
    \ 1990   1      0      0      0      1.49\n      5/30   1990   19900  1990   1\
    \      0      0      0      1.48\n      5/30   1990   19900  1990   1      0 \
    \     0      0      1.48\n      5/35   1990   19900  1990   1      0      0  \
    \    0      1.49\n      5/35   1990   19900  1990   1      0      0      0   \
    \   1.48\n      5/40   1990   19900  1990   1      0      0      0      1.49\n\
    \      5/40   1990   19900  1990   1      0      0      0      1.49\n      5/45\
    \   1990   19900  1990   1      0      0      0      1.45\n      5/45   1990 \
    \  19900  1990   1      0      0      0      1.46\n      5/25   1990   19900 \
    \ 1990   2      0      0      0      1.75\n      5/25   1990   19900  1990   2\
    \      0      0      0      1.75\n      5/30   1990   19900  1990   2      0 \
    \     0      0      1.74\n      5/30   1990   19900  1990   2      0      0  \
    \    0      1.75\n      5/35   1990   19900  1990   2      0      0      0   \
    \   1.74\n      5/35   1990   19900  1990   2      0      0      0      1.74\n\
    \      5/40   1990   19900  1990   2      0      0      0      1.75\n      5/40\
    \   1990   19900  1990   2      0      0      0      1.74\n      5/43   1990 \
    \  19900  1990   2      0      0      0      1.75\n      5/43   1990   19900 \
    \ 1990   2      0      0      0      1.74\n      5/43   1990   19900  1990   2\
    \      0      0      0      1.75\n      5/44   1990   19900  1990   2      0 \
    \     0      0      1.73\n      5/44   1990   19900  1990   2      0      0  \
    \    0      1.72\n      5/45   1990   19900  1990   2      0      0      0   \
    \   1.70\n      5/45   1990   19900  1990   2      0      0      0      1.72\n"
- title: III. Detailed Ethernet LAN Testing Results
  contents:
  - "III. Detailed Ethernet LAN Testing Results\n   Following is a table of some of\
    \ the test results gathered from\n   testing NETBLT between two IBM PC/ATs on\
    \ an Ethernet LAN.  See\n   previous appendix for table header definitions.\n\
    \      BS/BI  PSZ    BFSZ   XFSZ   NBUFS  #LOSS  #RXM   DTMOS  SPEED\n      5/30\
    \   1438   14380  1438   1      9      9      6      1.42\n      5/30   1438 \
    \  14380  1438   1      2      2      2      1.45\n      5/30   1438   14380 \
    \ 1438   1      5      5      4      1.44\n      5/35   1438   14380  1438   1\
    \      7      7      7      1.38\n      5/35   1438   14380  1438   1      6 \
    \     6      5      1.38\n      5/40   1438   14380  1438   1      48     48 \
    \    44     1.15\n      5/40   1438   14380  1438   1      50     50     38  \
    \   1.17\n      5/40   1438   14380  1438   1      13     13     11     1.28\n\
    \      5/40   1438   14380  1438   1      7      7      5      1.30\n      5/30\
    \   1438   14380  1438   2      206    206    198    0.995\n      5/30   1438\
    \   14380  1438   2      213    213    198    0.994\n      5/40   1438   14380\
    \  1438   2      117    121    129    1.05\n      5/40   1438   14380  1438  \
    \ 2      178    181    166    0.892\n      5/40   1438   14380  1438   2     \
    \ 135    138    130    1.03\n      5/45   1438   14380  1438   2      57     57\
    \     52     1.12\n      5/45   1438   14380  1438   2      97     97     99 \
    \    1.02\n      5/45   1438   14380  1438   2      62     62     51     1.09\n\
    \      5/15   512    10240  2048   1      6      6      4      0.909\n      5/15\
    \   512    10240  2048   1      10     11     7      0.907\n      5/18   512 \
    \   10240  2048   1      11     11     8      0.891\n      5/18   512    10240\
    \  2048   1      5      5      9      0.906\n      5/19   512    10240  2048 \
    \  1      3      3      3      0.905\n      5/19   512    10240  2048   1    \
    \  8      8      7      0.898\n      5/20   512    10240  2048   1      7    \
    \  7      4      0.876\n      5/20   512    10240  2048   1      11     12   \
    \  5      0.871\n      5/20   512    10240  2048   1      8      9      5    \
    \  0.874\n      5/30   512    10240  2048   2      113    116    84     0.599\n\
    \      5/30   512    10240  2048   2      20     20     14     0.661\n      5/30\
    \   512    10240  2048   2      49     50     40     0.638\n"
- title: IV. Detailed Wideband Network Testing Results
  contents:
  - "IV. Detailed Wideband Network Testing Results\n   Following is a table of some\
    \ of the test results gathered from\n   testing NETBLT between an IBM PC/AT and\
    \ a SUN-3 using the Wideband\n   satellite network.  See previous appendix for\
    \ table header\n   definitions.\n      BS/BI  PSZ    BFSZ   XFSZ   NBUFS  #LOSS\
    \  #RXM   SPEED\n      5/90   1400   14000  500    22     9      10     0.584\n\
    \      5/90   1400   14000  500    22     12     12     0.576\n      5/90   1400\
    \   14000  500    22     3      3      0.591\n      5/90   1420   14200  500 \
    \   22     12     12     0.591\n      5/90   1420   14200  500    22     6   \
    \   6      0.600\n      5/90   1430   14300  500    22     9      10     0.600\n\
    \      5/90   1430   14300  500    22     15     15     0.591\n      5/90   1430\
    \   14300  500    22     12     12     0.590\n      5/90   1432   14320  716 \
    \   22     13     16     0.591\n      5/90   1434   14340  717    22     33  \
    \   147    0.483\n      5/90   1436   14360  718    22     25     122    0.500\n\
    \      5/90   1436   14360  718    22     25     109    0.512\n      5/90   1436\
    \   14360  718    22     28     153    0.476\n      5/90   1438   14380  719 \
    \   22     6      109    0.533\n      5/80   1432   14320  716    22     56  \
    \   68     0.673\n      5/80   1432   14320  716    22     14     14     0.666\n\
    \      5/80   1432   14320  716    22     15     16     0.661\n      5/60   1432\
    \   14320  716    22     19     22     0.856\n      5/60   1432   14320  716 \
    \   22     84     95     0.699\n      5/60   1432   14320  716    22     18  \
    \   21     0.871\n      5/60   1432   14320  716    30     38     40     0.837\n\
    \      5/60   1432   14320  716    30     25     26     0.869\n      5/55   1432\
    \   14320  716    22     13     13     0.935\n      5/55   1432   14320  716 \
    \   22     25     25     0.926\n      5/55   1432   14320  716    22     25  \
    \   25     0.926\n      5/55   1432   14320  716    22     20     20     0.932\n\
    \      5/55   1432   14320  716    22     17     19     0.934\n      5/55   1432\
    \   14320  716    22     13     14     0.942\n"
