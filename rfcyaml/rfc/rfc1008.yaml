- title: __initial_text__
  contents:
  - "                             IMPLEMENTATION GUIDE \n                        \
    \    ISO TRANSPORT PROTOCOL\n"
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This RFC is being distributed to members of the Internet\
    \ community\n   in order to solicit comments on the Implementors Guide. While\
    \ this\n   document may not be directly relevant to the research problems\n  \
    \ of the Internet, it may be of some interest to a number of researchers\n   and\
    \ implementors. Distribution of this memo is unlimited.\n            IMPLEMENTATION\
    \ GUIDE FOR THE ISO TRANSPORT PROTOCOL\n"
- title: 1   Interpretation of formal description.
  contents:
  - "1   Interpretation of formal description.\n   It is assumed that the reader is\
    \ familiar with both the formal\n   description technique, Estelle [ISO85a], and\
    \ the transport protocol\n   as described in IS 8073 [ISO84a] and in N3756 [ISO85b].\n"
- title: 1.1   General interpretation guide.
  contents:
  - "1.1   General interpretation guide.\n   The development of the formal description\
    \ of the ISO Transport\n   Protocol was guided by the three following assumptions.\n\
    \                      1. A generality principle\n   The formal description is\
    \ intended to express all of the behavior\n   that any implementation is to demonstrate,\
    \ while not being bound\n   to the way that any particular implementation would\
    \ realize that\n   behavior within its operating context.\n                  \
    \    2. Preservation of the deliberate\n                         nondeterminism\
    \ of IS 8073\n   The text description in the IS 8073 contains deliberate expressions\n\
    \   of nondeterminism and indeterminism in the behavior of the\n   transport protocol\
    \ for the sake of flexibility in application.\n   (Nondeterminism in this context\
    \ means that the order of execution\n   for a set of actions that can be taken\
    \ is not specified.\n   Indeterminism means that the execution of a given action\
    \ cannot be\n   predicted on the basis of system state or the executions of other\n\
    \   actions.)\n                      3. Discipline in the usage of Estelle\n \
    \  A given feature of Estelle was to be used only if the nature of\n   the mechanism\
    \ to be described strongly indicates its usage,\n   or to adhere to the generality\
    \ principle, or to retain the\n   nondeterminism of IS 8073.\n   Implementation\
    \ efficiency was not a particular goal nor was there\n   an attempt to directly\
    \ correlate Estelle mechanisms and features\n   to implementation mechanisms and\
    \ features.  Thus, the description\n   does not represent optimal behavior for\
    \ the implemented protocol.\n   These assumptions imply that the formal description\
    \ contains higher\n   levels of abstraction than would be expected in a description\
    \ for\n   a particular operating environment.  Such abstraction is essential,\n\
    \   because of the diversity of networks and network elements by which\n   implementation\
    \ and design decisions are influenced.  Even when\n   operating environments are\
    \ essentially identical, design choice and\n   originality in solving a technical\
    \ problem must be allowed.\n   The same behavior may be expressed in many different\
    \ ways.  The\n   goal in producing the transport formal description was to attempt\n\
    \   to capture this equivalence.  Some mechanisms of transport are not\n   fully\
    \ described or appear to be overly complicated because of the\n   adherence to\
    \ the generality principle.  Resolution of these\n   situations may require significant\
    \ effort on the part of the\n   implementor.\n   Since the description does not\
    \ represent optimal behavior for the\n   implemented protocol, implementors should\
    \ take the three assumptions\n   above into account when using the description\
    \ to implement the\n   protocol.  It may be advisable to adapt the standard description\
    \ in\n   such a way that:\n     a.   abstractions (such as modules, channels,\
    \ spontaneous\n          transitions and binding comments) are interpreted and\
    \ realized\n          as mechanisms appropriate to the operating environment and\n\
    \          service requirements;\n     b.   modules, transitions, functions and\
    \ procedures containing\n          material irrelevant to the classes or options\
    \ to be supported\n          are reduced or eliminated as needed; and\n     c.\
    \   desired real-time behavior is accounted for.\n   The use in the formal description\
    \ of an Estelle feature (for\n   instance, \"process\"), does not imply that an\
    \ implementation must\n   necessarily realize the feature by a synonymous feature\
    \ of the\n   operating context.  Thus, a module declared to be a \"process\" in\
    \ an\n   Estelle description need not represent a real process as seen by a\n\
    \   host operating system; \"process\" in Estelle refers to the\n   synchronization\
    \ properties of a set of procedures (transitions).\n   Realizations of Estelle\
    \ features and mechanisms are dependent in an\n   essential way upon the performance\
    \ and service an implementation is\n   to provide.  Implementations for operational\
    \ usage have much more\n   stringent requirements for optimal behavior and robustness\
    \ than do\n   implementations used for simulated operation (e.g., correctness\
    \ or\n   conformance testing).  It is thus important that an operational\n   implementation\
    \ realize the abstract features and mechanisms of a\n   formal description in\
    \ an efficient and effective manner.\n   For operational usage, two useful criteria\
    \ for interpretation of\n   formal mechanisms are:\n        [1] minimization of\
    \ delays caused by the mechanism\n            itself; e.g.,\n               --transit\
    \ delay for a medium that realizes a\n                 channel\n             \
    \  --access delay or latency for channel medium\n               --scheduling delay\
    \ for timed transitions\n                 (spontaneous transitions with delay\
    \ clause)\n               --execution scheduling for modules using\n         \
    \        exported variables (delay in accessing\n                 variable)\n\
    \        [2] minimization of the \"handling\" required by each\n            invocation\
    \ of the mechanism; e.g.,\n               --module execution scheduling and context\n\
    \                 switching\n               --synchronization or protocols for\
    \ realized\n                 channel\n               --predicate evaluation for\
    \ spontaneous\n                 transitions\n   Spontaneous transitions represent\
    \ nondeterminism and indeterminism,\n   so that uniform realization of them in\
    \ an implementation must be\n   questioned as an implementation strategy.  The\
    \ time at which the\n   action described by a spontaneous transition will actually\
    \ take\n   place cannot be specified because of one or more of the following\n\
    \   situations:\n     a.   it is not known when, relative to any specific event\
    \ defining\n          the protocol (e.g., input network, input from user, timer\n\
    \          expirations), the conditions enabling the transition will\n       \
    \   actually occur;\n     b.   even if the enabling conditions are ultimately\
    \ deterministic,\n          it is not practical to describe all the possible ways\
    \ this\n          could occur, given the different ways in which implementations\n\
    \          will examine these conditions; and\n     c.   a particular implementation\
    \ may not be concerned with the\n          enabling conditions or will account\
    \ for them in some other\n          way; i.e., it is irrelevant when the action\
    \ takes place, if\n          ever.\n   As an example of a), consider the situation\
    \ when splitting over the\n   network connection, in Class 4, in which all of\
    \ the network\n   connections to which the transport connection has been assigned\
    \ have\n   all disconnected, with the transport connection still in the OPEN\n\
    \   state.  There is no way to predict when this will happen, nor is\n   there\
    \ any specific event signalling its occurrence.  When it does\n   occur, the transport\
    \ protocol machine may want to attempt to obtain\n   a new network connection.\n\
    \   As an example of b), consider that timers may be expressed\n   succinctly\
    \ in Estelle by transitions similar to the following:\n                 from A\
    \ to B\n                 provided predicate delay( timer_interval )\n        \
    \         begin\n                 (* action driven by timeout *)\n           \
    \      end;\n   But there are operations for which the timer period may need to\n\
    \   be very accurate (close to real time) and others in which some\n   delay in\
    \ executing the action can be tolerated.  The implementor\n   must determine the\
    \ optimal behavior desired for each instance\n   and use an appropriate mechanism\
    \ to realize it, rather than\n   using a uniform approach to implementing all\
    \ spontaneous\n   transitions.\n   As an example of the situation in c), consider\
    \ the closing of an\n   unused network connection.  If the network is such that\
    \ the cost\n   of letting the network connection remain open is small compared\n\
    \   cost of opening it, then an implementation might not want to\n   consider\
    \ closing the network connection until, say, the weekend.\n   Another implementation\
    \ might decide to close the network\n   connection within 30 msec after discovering\
    \ that the connection\n   is not busy.  For still another implementation, this\
    \ could be\n   meaningless because it operates over a connectionless network\n\
    \   service.\n   If a description has only a very few spontaneous transitions,\
    \ then\n   it may be relatively easy to implement them literally (i.e., to\n \
    \  schedule and execute them as Estelle abstractly does) and not\n   incur the\
    \ overhead from examining all of the variables that occur\n   in the enabling\
    \ conditions.  However, the number and complexity of\n   the enabling conditions\
    \ for spontaneous transitions in the transport\n   description strongly suggests\
    \ that an implementation which realizes\n   spontaneous transitions literally\
    \ will suffer badly from such\n   overhead.\n"
- title: 1.2   Guide to the formal description.
  contents:
  - "1.2   Guide to the formal description.\n   So that implementors gain insight\
    \ into interpretation of the\n   mechanisms and features of the formal description\
    \ of transport, the\n   following paragraphs discuss the meanings of such mechanisms\
    \ and\n   features as intended by the editors of the formal description.\n"
- title: 1.2.1   Transport Protocol Entity.
  contents:
  - '1.2.1   Transport Protocol Entity.

    '
- title: 1.2.1.1   Structure.
  contents:
  - "1.2.1.1   Structure.\n   The diagram below shows the general structure of the\
    \ Transport\n   Protocol Entity (TPE) module, as given in the formal description.\n\
    \   >From an abstract operational viewpoint, the transport protocol\n   Machines\
    \ (TPMs) and the Slaves operate as child processes of the the\n   TPE process.\
    \  Each TPM represents the endpoint actions of the\n   protocol on a single transport\
    \ connection.  The Slave represents\n   control of data output to the network.\
    \  The internal operations of\n   the TPMs and the Slave are discussed below in\
    \ separate sections.\n   This structure permits describing multiple connections,\
    \ multiplexing\n   and splitting on network connections, dynamic existence of\
    \ endpoints\n   and class negotiation.  In the diagram, interaction points are\n\
    \   denoted by the symbol \"O\", while (Estelle) channels joining these\n   interaction\
    \ points are denoted by\n             *\n             *\n             *\n   The\
    \ symbol \"X\" represents a logical association through variables,\n   and the\
    \ denotations\n           <<<<<<<\n           >>>>>>>\n              V\n     \
    \         V\n              V\n   indicate the passage of data, in the direction\
    \ of the symbol\n   vertices, by way of these associations.  The acronyms TSAP\
    \ and\n   NSAP denote Transport Service Access Point and Network Service\n   Access\
    \ Point, respectively.  The structure of the TSAPs and\n   NSAPs shown is discussed\
    \ further on, in Parts 1.2.2.1 and\n   1.2.2.2.\n             |<-----------------TSAP---------------->|\n\
    \   ----------O---------O---------O---------O---------O---------\n   |  TPE  \
    \  *                   *         *                  |\n   |         *        \
    \           *         *                  |\n   |     ____O____           ____O____\
    \ ____O____              |\n   |     |       |           |       | |       | \
    \             |\n   |     |  TPM  |           |  TPM  | |  TPM  |            \
    \  |\n   |     |       |           |       | |       |              |\n   |  \
    \   |___X___|           |__X_X__| |___X___|              |\n   |         V   \
    \               V V        V                  |\n   |         V   multiplex  \
    \    V V        V                  |\n   |         >>>>>>>> <<<<<<<<<<< V    \
    \    V                  |\n   |                V V     split V        V      \
    \            |\n   |                V V           V        V                 \
    \ |\n   |              ---X----     ---X---- ---X----              |\n   |   \
    \           |Slave |     |Slave | |Slave |              |\n   |              |__O___|\
    \     |__O___| |__O___|              |\n   |                 V            V  \
    \      V                  |\n   |                 V            V        V    \
    \              |\n   |-----------------O------------O--------O------------------|\n\
    \                   NSAP           |<------>|\n                              \
    \ NSAP\n   The structuring principles of Estelle provide a formal means of\n \
    \  expressing and enforcing certain synchronization properties between\n   communicating\
    \ processes.  It must be stressed that the scheduling\n   implied by Estelle descriptions\
    \ need not and in some cases should\n   not be implemented.  The intent of the\
    \ structure in the transport\n   formal description is to state formally the synchronization\
    \ of\n   access tovariables shared by the transport entity and the transport\n\
    \   connection endpoints and to permit expression of dynamic objects\n   within\
    \ the entity.  In nearly all aspects of operation except these,\n   it may be\
    \ more efficient in some implementation environments to\n   permit the TPE and\
    \ the TPMs to run in parallel (the Estelle\n   scheduling specifically excludes\
    \ the parallel operation of the TPE\n   and the TPMs). This is particularly true\
    \ of internal management\n   (\"housekeeping\") actions and those actions not\
    \ directly related to\n   communication between the TPE and the TPMs or instantiation\
    \ of TPMs.\n   Typical actions of this latter sort are: receipt of NSDUs from\
    \ the\n   network, integrity checking and decoding of TPDUs, and network\n   connection\
    \ management. Such actions could have been collected into\n   other modules for\
    \ scheduling closer to that of an implementation,\n   but surely at the risk of\
    \ further complicating the description.\n   Consequently, the formal description\
    \ structure should be understood\n   as expressing relationships among actions\
    \ and objects and not\n   explicit implementation behavior.\n"
- title: 1.2.1.2   Transport protocol entity operation.
  contents:
  - "1.2.1.2   Transport protocol entity operation.\n   The details of the operation\
    \ of the TPE from a conceptual point of\n   view are given in the SYS section\
    \ of the formal description.\n   However, there are several further comments that\
    \ can be made\n   regarding the design of the TPE.  The Estelle body for the TPE\n\
    \   module has no state variable.  This means that any transition of\n   the TPE\
    \ may be enabled and executed at any time.  Choice of\n   transition is determined\
    \ primarily by priority.  This suggests\n   that the semantics of the TPE transitions\
    \ is that of interrupt\n   traps.\n   The TPE handles only the T-CONNECT-request\
    \ from the user and the TPM\n   handle all other user input.  All network events\
    \ are handled by the\n   TPE, in addition to resource management to the extent\
    \ defined in the\n   description.  The TPE also manages all aspects of connection\n\
    \   references, including reference freezing.  The TPE does not\n   explicitly\
    \ manage the CPU resource for the TPMs, since this is\n   implied by the Estelle\
    \ scheduling across the module hierarchy.\n   Instantiation of TPMs is also the\
    \ responsibility of the TPE, as is\n   TPM release when the transport connection\
    \ is to be closed.  Once a\n   TPM is created, the TPE does not in general interfere\
    \ with TPM's\n   activities, with the following exceptions:  the TPE may reduce\
    \ credit\n   to a Class 4 TPM without notice;  the TPE may dissociate a Class\
    \ 4\n   TPM from a network connection when splitting is being used.\n   Communication\
    \ between the TPE and the TPMs is through a set of\n   exported variables owned\
    \ by the TPMs, and through a channel which\n   passes TPDUs to be transmitted\
    \ to the remote peer.  This channel is\n   not directly connected to any network\
    \ connection, so each\n   interaction on it carries a reference number indicating\
    \ which network\n   connection is to be used. Since the reference is only a reference,\n\
    \   this permits usage of this mechanism when the network service is\n   connectionless,\
    \ as well.  The mechanism provides flexibility for\n   both splitting and multiplexing\
    \ on network connections.\n   One major function that the TPE performs for all\
    \ its TPMs is that of\n   initial processing of received TPDUs.  First, a set\
    \ of integrity\n   checks is made to determine if each TPDU in an NSDU is decodable:\n\
    \     a.   PDU length indicators and their sums are checked against the\n    \
    \      NSDU length for consistency;\n     b.   TPDU types versus minimum header\
    \ lengths for the types are\n          checked, so that if the TPDU can be decoded,\
    \ then proper\n          association to TPMs can be made without any problem;\n\
    \     c.   TPDUs are searched for checksums and the local checksum is\n      \
    \    computed for any checksum found; and\n     d.   parameter codes in variable\
    \ part of headers are checked where\n          applicable.\n   These integrity\
    \ checks guarantee that an NSDU passing the check can\n   be separated as necessary\
    \ into TPDUs, these TPDUs can be associated\n   to the transport connections or\
    \ to the Slave as appropriate and they\n   can be further decoded without error.\n\
    \   The TPE next decodes the fixed part of the TPDU headers to determine\n   the\
    \ disposition of the TPDU.  The Slave gets TPDUs that cannot be\n   assigned to\
    \ a TPM (spurious TPDU).  New TPMs are created in response\n   to CR TPDUs that\
    \ correspond to a TSAP for this TPE.\n   All management of NSAPs is done by the\
    \ TPE.  This consists of keeping\n   track of all network connections, their service\
    \ quality\n   characteristics and their availability, informing the TPMs associated\n\
    \   with these network connections.\n   The TPE has no timer module as such. \
    \ Timing is handled by using the\n   DELAY feature of Estelle, since this feature\
    \ captures the essence of\n   timing without specifying how the actual timing\
    \ is to be achieved\n   within the operating environment.  See Part 1.2.5 for\
    \ more details.\n"
- title: 1.2.2   Service Access Points.
  contents:
  - "1.2.2   Service Access Points.\n   The service access points (SAP) of the transport\
    \ entity are modeled\n   using the Estelle channel/interaction point formalism.\
    \  (Note: The\n   term \"channel\" in Estelle is a keyword that denotes a set\
    \ of\n   interactions which may be exchanged at interaction points [LIN85].\n\
    \   However, it is useful conceptually to think of \"channel\" as denoting\n \
    \  a communication path that carries the interactions between modules.)\n   The\
    \ abstract service primitives for a SAP are interactions on\n   channels entering\
    \ and leaving the TPE.  The transport user is\n   considered to be at the end\
    \ of the channel connected to the transport\n   SAP (TSAP) and the network service\
    \ provider is considered to be at\n   the end of the channel connected to the\
    \ network SAP (NSAP).  An\n   interaction put into a channel by some module can\
    \ be considered to\n   move instantaneously over the channel onto a queue at the\
    \ other end.\n   The sender of such an interaction no longer has access to the\n\
    \   interaction once it has been put into the channel.  The operation of\n   the\
    \ system modeled by the formal description has been designed with\n   this semantics\
    \ in mind, rather than the equivalent but much more\n   abstract Estelle semantics.\
    \  (In the Estelle semantics, each\n   interaction point is considered to have\
    \ associated with it an\n   unbounded queue.  The \"attach\" and \"connect\" primitives\
    \ bind two\n   interaction points, such that an action, implied by the keyword\n\
    \   \"out\", at one interaction point causes a specified interaction to be\n \
    \  placed onto the queue associated with the other interaction point.)\n   The\
    \ sections that follow discuss the TSAP and the NSAP and the way\n   that these\
    \ SAPs are described in the formal description.\n"
- title: 1.2.2.1   Transport Service Access Point.
  contents:
  - "1.2.2.1   Transport Service Access Point.\n   The international transport standard\
    \ allows for more than one TSAP to\n   be associated with a transport entity,\
    \ and multiple users may be\n   associated with a given TSAP.  A situation in\
    \ which this is useful is\n   when it is desirable to have a certain quality of\
    \ service correlated\n   with a given TSAP.  For example, one TSAP could be reserved\
    \ for\n   applications requiring a high throughput, such as file transfer.  The\n\
    \   operation of transport connections associated with this TSAP could\n   then\
    \ be designed to favor throughput.  Another TSAP might serve users\n   requiring\
    \ short response time, such as terminals.  Still another TSAP\n   could be reserved\
    \ for encryption reasons.\n   In order to provide a way of referencing users associated\
    \ with TSAPs,\n   the user access to transport in the formal description is through\
    \ an\n   array of Estelle interaction points.  This array is indexed by a TSAP\n\
    \   address (T_address) and a Transport Connection Endpoint Identifier\n   (TCEP_id).\
    \  Note that this dimensional object (TSAP) is considered\n   simply to be a uniform\
    \ set of abstract interfaces.  The indices must\n   be of (Pascal) ordinal type\
    \ in Estelle.  However, the actual address\n   structure of TSAPs may not conform\
    \ easily to such typing in an\n   implementation.  Consequently, the indices as\
    \ they appear in the\n   formal description should be viewed as an organizational\
    \ mechanism\n   rather than as an explicit way of associating objects in an\n\
    \   operational setting.  For example, actual TSAP addresses might be\n   kept\
    \ in some kind of table, with the table index being used to\n   reference objects\
    \ associated with the TSAP.\n   One particular issue concerned with realizing\
    \ TSAPs is that of making\n   known to the users the means of referencing the\
    \ transport interface,\n   i.e., somehow providing the T_addresses and TCEP_ids\
    \ to the users.\n   This issue is not considered in any detail by either IS 7498\
    \ [ISO84b]\n   or IS 8073.  Abstractly, the required reference is the\n   T_address/TCEP_id\
    \ pair.  However, this gives no insight as to how the\n   mechanism could work.\
    \  Some approaches to this problem are discussed\n   in Part 5.\n   Another issue\
    \ is that of flow control on the TSAP channels.  Flow\n   control is not part\
    \ of the semantics for the Estelle channel, so the\n   problem must be dealt with\
    \ in another way.  The formal description\n   gives an abstract definition of\
    \ interface flow control using Pascal\n   and Estelle mechanisms.  This abstraction\
    \ resembles many actual\n   schemes for flow control, but the realization of flow\
    \ control will\n   still be dependent on the way the interface is implemented.\
    \  Part 3.2\n   discusses this in more detail.\n"
- title: 1.2.2.2   Network Service Access Point.
  contents:
  - "1.2.2.2   Network Service Access Point.\n   An NSAP may also have more than one\
    \ network connection associated\n   with it.  For example, the virtual circuits\
    \ of X.25 correspond with\n   this notion.  On the other hand, an NSAP may have\
    \ no network\n   connection associated with it, for example when the service at\
    \ the\n   NSAP is connectionless.  This certainly will be the case when\n   transport\
    \ operates on a LAN or over IP.  Consequently, although the\n   syntactical appearance\
    \ of the NSAP in the formal description is\n   similar to that for the TSAP, the\
    \ semantics are essentially distinct\n   [NTI85].\n   Distinct NSAPs can correspond\
    \ or not to physically distinct networks.\n   Thus, one NSAP could access X.25\
    \ service, another might access an\n   IEEE 802.3 LAN, while a third might access\
    \ a satellite link.  On the\n   other hand, distinct NSAPs could correspond to\
    \ different addresses on\n   the same network, with no particular rationale other\
    \ than facile\n   management for the distinction.  There are performance and system\n\
    \   design issues that arise in considering how NSAPs should be managed\n   in\
    \ such situations.  For example, if distinct NSAPs represent\n   distinct networks,\
    \ then a transport entity which must handle all\n   resource management for the\
    \ transport connections and operate these\n   connections as well may have trouble\
    \ keeping pace with data arriving\n   concurrently from two LANs and a satellite\
    \ link.  It might be a\n   better design solution to separate the management of\
    \ the transport\n   connection resources from that of the NSAP resources and inputs,\
    \ or\n   even to provide separate transport entities to handle some of the\n \
    \  different network services, depending on the service quality to be\n   maintained.\
    \  It may be helpful to think of the (total) transport\n   service as not necessarily\
    \ being provided by a single monolithic\n   entity--several distinct entities\
    \ can reside at the transport layer\n   on the same end-system.\n   The issues\
    \ of NSAP management come primarily from connection-oriented\n   network services.\
    \  This is because a connectionless service is either\n   available to all transport\
    \ connections or it is available to none,\n   representing infinite degrees of\
    \ multiplexing and splitting. In the\n   connection-oriented case, NSAP management\
    \ is complicated by\n   multiplexing, splitting, service quality considerations\
    \ and the\n   particular character of the network service.  These issues are\n\
    \   discussed further in Part 3.4.1.  In the formal description, network\n   connection\
    \ management is carried out by means of a record associated\n   with each possible\
    \ connection and an array, associated with each TPM,\n   each array member corresponding\
    \ to a possible network connection.\n   Since there is, on some network services,\
    \ a very large number of\n   possible network connections, it is clear that in\
    \ an implementation\n   these data structures may need to be made dynamic rather\
    \ than static.\n   The connection record, indexed by NSAP and NCEP_id, consists\
    \ of a\n   Slave module reference, virtual data connections to the TPMs to be\n\
    \   associated with the network connection, a data connection (out) to\n   the\
    \ NSAP, and a data connection to the Slave.  There is also a\n   \"state\" variable\
    \ for keeping track of the availability of the\n   connection, variables for managing\
    \ the Slave and an internal\n   reference number to identify the connection to\
    \ TPMs.  A member of the\n   network connection array associated with a TPM provides\
    \ the TPM with\n   status information on the network connection and input data\
    \ (network)\n   events and TPDUs).  A considerable amount of management of the\n\
    \   network connections is provided by the formal description, including\n   splitting,\
    \ multiplexing, service quality (when defined), interface\n   flow control, and\
    \ concatenation of TPDUs. This management is carried\n   out solely by the transport\
    \ entity, leaving the TPMs free to handle\n   only the explicit transport connection\
    \ issues.  This management\n   scheme is flexible enough that it can be simplified\
    \ and adapted to\n   handle the NSAP for a connectionless service.\n   The principal\
    \ issue for management of connectionless NSAPs is that of\n   buffering, particularly\
    \ if the data transmission rates are high, or\n   there is a large number of transport\
    \ connections being served.  It\n   may also be desirable for the transport entity\
    \ to monitor the service\n   it is getting from the network.  This would entail,\
    \ for example,\n   periodically computing the mean transmission delays for adjusting\n\
    \   timers or to exert backpressure on the transport connections if\n   network\
    \ access delay rises, indicating loading.  (In the formal\n   description, the\
    \ Slave processor provides a simple form of output\n   buffer management: when\
    \ its queue exceeds a threshold, it shuts off\n   data from the TPMs associated\
    \ with it.  Through primitive functions,\n   the threshold is loosely correlated\
    \ with network behavior.  However,\n   this mechanism is not intended to be a\
    \ solution to this difficult\n   performance problem.)\n"
- title: 1.2.3   Transport Protocol Machine.
  contents:
  - "1.2.3   Transport Protocol Machine.\n   Transport Protocol Machines (TPM) in\
    \ the formal description are in\n   six classes: General, Class 0, Class 1, Class\
    \ 2, Class 3 and Class 4.\n   Only the General, Class 2 and Class 4 TPMs are discussed\
    \ here.  The\n   reason for this diversity is to facilitate describing class\n\
    \   negotiations and to show clearly the actions of each class in the\n   data\
    \ transfer phase.  The General TPM is instantiated when a\n   connection request\
    \ is received from a transport user or when a CR\n   TPDU is received from a remote\
    \ peer entity.  This TPM is replaced by\n   a class-specific TPM when the connect\
    \ response is received from the\n   responding user or when the CC TPDU is received\
    \ from the responding\n   peer entity.\n   The General, Class 2 and Class 4 TPMs\
    \ are discussed below in more\n   detail.  In an implementation, it probably will\
    \ be prudent to merge\n   the Class 2 and Class 4 operations with that of the\
    \ General TPM, with\n   new variables selecting the class-specific operation as\
    \ necessary\n   (see also Part 9.4 for information on obtaining Class 2 operation\n\
    \   from a Class 4 implementation).  This may simplify and improve the\n   behavior\
    \ of the implemented protocol overall.\n"
- title: 1.2.3.1   General Transport Protocol Machine.
  contents:
  - "1.2.3.1   General Transport Protocol Machine.\n   Connection negotiation and\
    \ establishment for all classes can be\n   handled by the General Transport Protocol\
    \ Machine.  Some parts of the\n   description of this TPM are sufficiently class\
    \ dependent that they\n   can safely be removed if that class is not implemented.\
    \  Other parts\n   are general and must be retained for proper operation of the\
    \ TPM. The\n   General TPM handles only connection establishment and negotiation,\
    \ so\n   that only CR, CC, DR and DC TPDUs are sent or received (the TPE\n   prevents\
    \ other kinds of TPDUs from reaching the General TPM).\n   Since the General TPM\
    \ is not instantiated until a T-CONNECT-request\n   or a CR TPDU is received,\
    \ the TPE creates a special internal\n   connection to the module's TSAP interaction\
    \ point to pass the\n   T-CONNECT-request event to the TPM.  This provides automaton\n\
    \   completeness according to the specfication of the protocol.  When the\n  \
    \ TPM is to be replaced by a class-specific TPM, the sent or received\n   CC is\
    \ copied to the new TPM so that negotiation information is not\n   lost.\n   In\
    \ the IS 8073 state tables for the various classes, the majority of\n   the behavioral\
    \ information for the automaton is contained in the\n   connection establishment\
    \ phase.  The editors of the formal\n   description have retained most of the\
    \ information contained in the\n   state tables of IS 8073 in the description\
    \ of the General TPM.\n"
- title: 1.2.3.2   Class 2 Transport Protocol Machine.
  contents:
  - "1.2.3.2   Class 2 Transport Protocol Machine.\n   The formal description of the\
    \ Class 2 TPM closely resembles that of\n   Class 4, in many respects.  This is\
    \ not accidental, in that: the\n   conformance statement in IS 8073 links Class\
    \ 2 with Class 4; and the\n   editors of the formal description produced the Class\
    \ 2 TPM\n   description by copying the Class 4 TPM description and removing\n\
    \   material on timers, checksums, and the like that is not part of the\n   Class\
    \ 2 operation.  The suggestion of obtaining Class 2 operation\n   from a Class\
    \ 4 implementation, described in Part 9.4, is in fact\n   based on this adaptation.\n\
    \   One feature of Class 2 that does not appear in Class 4, however, is\n   the\
    \ option to not use end-to-end flow control.  In this mode of\n   operation, Class\
    \ 2 is essentially Class 0 with multiplexing.  In\n   fact, the formal description\
    \ of the Class 0 TPM was derived from\n   Class 2 (in IS 8073, these two classes\
    \ have essentially identical\n   state tables).  This implies that Class 0 operation\
    \ could be obtained\n   from Class 2 by not multiplexing, not sending DC TPDUs,\
    \ electing not\n   to use flow control and terminating the network connection\
    \ when a DR\n   TPDU is received (expedited data cannot be used if flow control\
    \ is\n   not used).  When Class 2 is operated in this mode, a somewhat\n   different\
    \ procedure is used to handle data flow internal to the TPM\n   than is used when\
    \ end-to-end flow control is present.\n"
- title: 1.2.3.3   Class 4 Transport Protocol Machine.
  contents:
  - "1.2.3.3   Class 4 Transport Protocol Machine.\n   Dynamic queues model the buffering\
    \ of TPDUs in both the Class 4 and\n   Class 2 TPMs.  This provides a more general\
    \ model of implementations\n   than does the fixed array representation and is\
    \ easier to describe.\n   Also, the fixed array representation has semantics that,\
    \ carried\n   into an implementation, would produce inefficiency.  Consequently,\n\
    \   linked lists with queue management functions make up the TPDU\n   storage\
    \ description, despite the fact that pointers have a very\n   implementation-like\
    \ flavor.  One of the queue management functions\n   permits removing several\
    \ TPDUs from the head of the send queue, to\n   model the acknowledgement of several\
    \ TPDUs at once, as specified in\n   IS 8073.  Each TPDU record in the queue carries\
    \ the number of\n   retransmissions tried, for timer control (not present in the\
    \ Class 2\n   TPDU records).\n   There are two states of the Class 4 TPM that\
    \ do not appear in IS\n   8073. One of these was put in solely to facilitate obtaining\
    \ credit\n   in case no credit was granted for the CR or CC TPDU.  The other state\n\
    \   was put in to clarify operations when there is unacknowledged\n   expedited\
    \ data outstanding (Class 2 does not have this state).\n   The timers used in\
    \ the Class 4 TPM are discussed below, as is the\n   description of end-to-end\
    \ flow control.\n   For simplicity in description, the editors of the formal description\n\
    \   assumed that no queueing of expedited data would occur at the user\n   interface\
    \ of the receiving entity.  The user has the capability to\n   block the up-flow\
    \ of expedited data until it is ready.  This\n   assumption has several implications.\
    \ First, an ED TPDU cannot be\n   acknowledged until the user is ready to accept\
    \ it.  This is because\n   the receipt of an EA TPDU would indicate to the sending\
    \ peer that the\n   receiver is ready to receive the next ED TPDU, which would\
    \ not be\n   true.  Second, because of the way normal data flow is blocked by\
    \ the\n   sending of an ED TPDU, normal data flow ceases until the receiving\n\
    \   user is ready for the ED TPDU.  This suggests that the user\n   interface\
    \ should employ separate and noninterfering mechanisms\n   for passing normal\
    \ and expedited data to the user.  Moreover,\n   the mechanism for expedited data\
    \ passage should be blocked only in\n   dire operational conditions.  This means\
    \ that receipt of expedited\n   data by the user should be a procedure (transition)\
    \ that operates\n   at nearly the highest priority in the user process.  The alternative\n\
    \   to describing the expedited data handling in this way would entail a\n   scheme\
    \ of properly synchronizing the queued ED TPDUs with the DT\n   TPDUs received.\
    \  This requires some intricate handling of DT and ED\n   sequence numbers. While\
    \ this alternative may be attractive for\n   implementations, for clarity in the\
    \ formal description it provides\n   only unnecessary complication.\n   The description\
    \ of normal data TSDU processing is based on the\n   assumption that the data\
    \ the T-DATA-request refers to is potentially\n   arbitrarily long.  The semantic\
    \ of the TSDU in this case is analogous\n   to that of a file pointer, in the\
    \ sense that any file pointer is a\n   reference to a finite but arbitrarily large\
    \ set of octet-strings.\n   The formation of TPDUs from this string is analogous\
    \ to reading the\n   file in  fixed-length segments--records or blocks, for example.\
    \  The\n   reassembly of TPDUs into a string is analogous to appending each TPDU\n\
    \   to the tail of a file; the file is passed when the end-of-TSDU\n   (end-of-file)\
    \ is received.  This scheme permits conceptual buffering\n   of the entire TSDU\
    \ in the receiver and avoids the question of whether\n   or not received data\
    \ can be passed to the user before the EOT is\n   received.  (The file pointer\
    \ may refer to a file owned by the user,\n   so that the question then becomes\
    \ moot.)\n   The encoding of TPDUs is completely described, using Pascal functions\n\
    \   and some special data manipulation functions of Estelle (these are\n   not\
    \ normally part of Pascal).  There is one encoding function\n   corresponding\
    \ to each TPDU type, rather than a single parameterized\n   function that does\
    \ all of them.  This was done so that the separate\n   structures of the individual\
    \ types could be readily discerned, since\n   the purpose of the functions is\
    \ descriptive and not necessarily\n   computational.\n   The output of TPDUs from\
    \ the TPM is guarded by an internal flow\n   control flag.  When the TPDU is first\
    \ sent, this flag is ignored,\n   since if the TPDU does not get through, a retransmission\
    \ may take\n   care of it.  However, when a retransmission is tried, the flag\
    \ is\n   heeded and the TPDU is not sent, but the retransmission count is\n  \
    \ incremented.  This guarantees that either the TPDU will eventually\n   be sent\
    \ or the connection will time out (this despite the fact that\n   the peer will\
    \ never have received any TPDU to acknowledge).\n   Checksum computations are\
    \ done in the TPM rather than by the TPE,\n   since the TPE must handle all classes.\
    \  Also, if the TPMs can be\n   made to truly run in parallel, the performance\
    \ may be greatly\n   enhanced.\n   The decoding of received TPDUs is partially\
    \ described in the Class 4\n   TPM description.  Only the CR and CC TPDUs present\
    \ any problems in\n   decoding, and these are largely due to the nondeterministic\
    \ order of\n   parameters in the variable part of the TPDU headers and the\n \
    \  locality-and class-dependent content of this variable part.  Since\n   contents\
    \ of this variable part (except the TSAP-IDs) do not affect\n   the association\
    \ of the TPDU with a transport connection, the\n   decoding of the variable part\
    \ is not described in detail.  Such a\n   description would be very lengthy indeed\
    \ because of all the\n   possibilities and would not contribute measurably to\
    \ understanding\n   by the reader.\n"
- title: 1.2.4   Network Slave.
  contents:
  - "1.2.4   Network Slave.\n   The primary functions of the Network Slave are to\
    \ provide downward\n   flow control in the TPE, to concatenate TPDUs into a single\
    \ NSDU and\n   to respond to the receipt of spurious TPDUs.  The Slave has an\n\
    \   internal queue on which it keeps TPDUs until the network is ready to\n   accept\
    \ them for transmission.  The TPE is kept informed as to the\n   length of queue,\
    \ and the output of the TPMs is throttled if the\n   length exceeds this some\
    \ threshold.  This threshold can be adjusted\n   to meet current operating conditions.\
    \  The Slave will concatenate\n   the TPDUs in its queue if the option to concatenate\
    \ is exercised and\n   the conditions for concatenating are met.  Concatenation\
    \ is a TPE\n   option, which may be exercised or not at any time.\n"
- title: 1.2.5   Timers.
  contents:
  - "1.2.5   Timers.\n   In the formal description timers are all modeled using a\
    \ spontaneous\n   transition with delay, where the delay parameter is the timer\
    \ period.\n   To activate the timer, a timer identifier is placed into a set,\n\
    \   thereby satisfying a predicate of the form\n   provided timer_x in active_timers\n\
    \   However, the transition code is not executed until the elapsed time\n   ;from\
    \ the placement of the identifier in the set is at least equal\n   to the delay\
    \ parameter.  The editors of the formal description chose\n   to model timers\
    \ in this fashion because it provided a simply\n   expressed description of timer\
    \ behavior and eliminated having to\n   consider how timing is done in a real\
    \ system or to provide special\n   timer modules and communication to them.  It\
    \ is thus recommended that\n   implementors not follow the timer model closely\
    \ in implementations,\n   considering instead the simplest and most efficient\
    \ means of timing\n   permitted by the implementation environment.  Implementors\
    \ should\n   also note that the delay parameter is typed \"integer\" in the formal\n\
    \   description. No scale conversion from actual time is expressed in the\n  \
    \ timer transition, so that this scale conversion must be considered\n   when\
    \ timers are realized.\n"
- title: 1.2.5.1   Transport Protocol Entity timers.
  contents:
  - "1.2.5.1   Transport Protocol Entity timers.\n   There is only one timer given\
    \ in the formal description of the\n   TPE--the reference timer.  The reference\
    \ timer was placed here ;so\n   that it can be used by all classes and all connections,\
    \ as needed.\n   There is actually little justification for having a reference\
    \ timer\n   within the TPM--it wastes resources by holding the transport\n   endpoint,\
    \ even though the TPM is incapable of responding to any\n   input.  Consequently,\
    \ the TPE is responsible for all aspects of\n   reference management, including\
    \ the timeouts.\n"
- title: 1.2.5.2   Transport Protocol Machine timers.
  contents:
  - "1.2.5.2   Transport Protocol Machine timers.\n   Class 2 transport does not have\
    \ any timers that are required by IS\n   8073.  However, the standard does recommend\
    \ that an optional timer be\n   used by Class 2 in certain cases to avoid deadlock.\
    \  The formal\n   description provides this timer, with comments to justify its\
    \ usage.\n   It is recommended that such a timer be provided for Class 2\n   operation.\
    \  Class 4 transport has several timers for connection\n   control, flow control\
    \ and retransmissions of unacknowledged data.\n   Each of these timers is discussed\
    \ briefly below in terms of how they\n   were related to the Class 4 operations\
    \ in the formal description.\n   Further discussion of these timers is given in\
    \ Part 8.\n"
- title: 1.2.5.2.1   Window timer.
  contents:
  - "1.2.5.2.1   Window timer.\n   The window timer is used for transport connection\
    \ control as well as\n   providing timely updates of flow control credit information.\
    \  One of\n   these timers is provided in each TPM.   It is reset each time an\
    \ AK\n   TPDU is sent, except during fast retransmission of AKs for flow\n   control\
    \ confirmation, when it is disabled.\n"
- title: 1.2.5.2.2   Inactivity timer.
  contents:
  - "1.2.5.2.2   Inactivity timer.\n   The primary usage of the inactivity timer is\
    \ to detect when the\n   remote peer has ceased to send anything (including AK\
    \ TPDUs).  This\n   timer is mandatory when operating over a connectionless network\n\
    \   service, since there is no other way to determine whether or not the\n   remote\
    \ peer is still functioning.  On a connection-oriented network\n   service it\
    \ has an additional usage since to some extent the continued\n   existence of\
    \ the network connection indicates that the peer host has\n   not crashed.\n \
    \  Because of splitting, it is useful to provide an inactivity timer on\n   each\
    \ network connection to which a TPM is assigned.  In this manner,\n   if a network\
    \ connection is unused for some time, it can be released,\n   even though a TPM\
    \ assigned to it continues to operate over other\n   network connections. The\
    \ formal description provides this capability\n   in each TPM.\n"
- title: 1.2.5.2.3   Network connection timer.
  contents:
  - "1.2.5.2.3   Network connection timer.\n   This timer is an optional timer used\
    \ to ensure that every network\n   connection to which a TPM is assigned gets\
    \ used periodically.  This\n   prevents the expiration of the peer entity's inactivity\
    \ timer for a\n   network connection.  There is one timer for each network connection\n\
    \   to which the TPM is assigned.  If there is a DT or ED TPDU waiting to\n  \
    \ be sent, then it is chosen to be sent on the network connection.  If\n   no\
    \ such TPDU is waiting, then an AK TPDU is sent.  Thus, the NC timer\n   serves\
    \ somewhat the same purpose as the window timer, but is broader\n   in scope.\n"
- title: 1.2.5.2.4   Give-up timer.
  contents:
  - "1.2.5.2.4   Give-up timer.\n   There is one give-up timer for a TPM which is\
    \ set whenever the\n   retransmission limit for any CR, CC, DT, ED or DR TPDU\
    \ is reached.\n   Upon expiration of this timer, the transport connection is closed.\n"
- title: 1.2.5.2.5   Retransmission timers.
  contents:
  - "1.2.5.2.5   Retransmission timers.\n   Retransmission timers are provided for\
    \ CR, CC, DT, ED and DR TPDUs.\n   The formal description provides distinct timers\
    \ for each of these\n   TPDU types, for each TPM.  However, this is for clarity\
    \ in the\n   description, and Part 8.2.5 presents arguments for other strategies\n\
    \   to be used in implementations.  Also, DT TPDUs with distinct sequence\n  \
    \ numbers are each provided with timers, as well.  There is a primitive\n   function\
    \ which determines the range within the send window for which\n   timers will\
    \ be set.  This has been done to express flexibility in the\n   retransmission\
    \ scheme.\n   The flow control confirmation scheme specified in IS 8073 also\n\
    \   provides for a \"fast\" retransmission timer to ensure the reception of\n\
    \   an AK TPDU carrying window resynchronization after credit reduction\n   or\
    \ when opening a window that was previously closed.  The formal\n   description\
    \ permits one such timer for a TPM.  It is disabled after\n   the peer entity\
    \ has confirmed the window information.\n"
- title: 1.2.5.2.6   Error transport protocol data unit timer.
  contents:
  - "1.2.5.2.6   Error transport protocol data unit timer.\n   In IS 8073, there is\
    \ a provision for an optional timeout to limit the\n   wait for a response by\
    \ the peer entity to an ER TPDU.  When this\n   timer expires, the transport connection\
    \ is terminated.  Each Class 2\n   or Class 4 TPM is provided with one of these\
    \ timers in N3756.\n"
- title: 1.2.6   End-to-end Flow Control.
  contents:
  - "1.2.6   End-to-end Flow Control.\n   Flow control in the formal description has\
    \ been written in such a way\n   as to permit flexibility in credit control schemes\
    \ and\n   acknowledgement strategies.\n"
- title: 1.2.6.1   Credit control.
  contents:
  - "1.2.6.1   Credit control.\n   The credit mechanism in the formal description\
    \ provides for actual\n   management of credit by the TPE.  This is done through\
    \ variables\n   exported by the TPMs which indicate to the TPE when credit is\
    \ needed\n   and for the TPE to indicate when credit has been granted.  In this\n\
    \   manner, the TPE has control over the credit a TPM has.  The mechanism\n  \
    \ allows for reduction in credit (Class 4 only) and the possibility of\n   precipitous\
    \ window closure.  The mechanism does not preclude the use\n   of credit granted\
    \ by the user or other sources, since credit need is\n   expressed as current\
    \ credit being less than some threshold.  Setting\n   the threshold to zero permits\
    \ these other schemes.  An AK TPDU is\n   sent each time credit is updated.\n\
    \   The end-to-end flow control is also coupled to the interface flow\n   control\
    \ to the user.  If the user has blocked the interface up-flow,\n   then the TPM\
    \ is prohibited from requesting more credit when the\n   current window is used\
    \ up.\n"
- title: 1.2.6.2   Acknowledgement.
  contents:
  - "1.2.6.2   Acknowledgement.\n   The mechanism for acknowledging normal data provides\
    \ flexibility\n   sufficient to send an AK TPDU in response to every Nth DT TPDU\n\
    \   received where N > 0 and N may be constant or dynamically determined.\n  \
    \ Each TPM is provided with this, independent of all other TPMs, so\n   that acknowledgement\
    \ strategy can be determined separately for each\n   transport connection.  The\
    \ capability of altering the acknowledgement\n   strategy is useful in operation\
    \ over networks with varying error\n   rates.\n"
- title: 1.2.6.3  Sequencing of received data.
  contents:
  - "1.2.6.3  Sequencing of received data.\n   It is not specified in IS 8073 what\
    \ must be done with out-of-sequence\n   but within-window DT TPDUs received, except\
    \ that an AK TPDU with\n   current window and sequence information be sent.  There\
    \ are\n   performance reasons why such DT TPDUs should be held (cached): in\n\
    \   particular, avoidance of retransmissions.  However, this buffering\n   scheme\
    \ is complicated to implement and worse to describe formally\n   without resorting\
    \ to mechanisms too closely resembling\n   implementation.  Thus, the formal description\
    \ mechanism discards such\n   DT TPDUs and relies on retransmission to fill the\
    \ gaps in the window\n   sequence, for the sake of simplicity in the description.\n"
- title: 1.2.7   Expedited data.
  contents:
  - "1.2.7   Expedited data.\n   The transmission of expedited data, as expressed\
    \ by IS 8073, requires\n   the blockage of normal data transmission until the\
    \ acknowledgement is\n   received.  This is handled in the formal description\
    \ by providing a\n   special state in which normal data transmission cannot take\
    \ place.\n   However, recent experiments with Class 4 transport over network\n\
    \   services with high bandwidth, high transit delay and high error\n   rates,\
    \ undertaken by the NBS and COMSAT Laboratories, have shown that\n   the protocol\
    \ suffers a marked decline in its performance in such\n   conditions.  This situation\
    \ has been presented to ISO, with the\n   result that the the protocol will be\
    \ modified to permit the sending\n   of normal data already accepted by the transport\
    \ entity from the user\n   before the expedited data request but not yet put onto\
    \ the network.\n   When the modification is incorporated into IS 8073, the formal\n\
    \   description will be appropriately aligned.\n"
- title: 2   Environment of implementation.
  contents:
  - "2   Environment of implementation.\n   The following sections describe some general\
    \ approaches to\n   implementing the transport protocol and the advantages and\n\
    \   disadvantages of each.  Certain commercial products are identified\n   throughout\
    \ the rest of this document.  In no case does such\n   identification imply the\
    \ recommendation or endorsement of these\n   products by the Department of Defense,\
    \ nor does it imply that the\n   products identified are the best available for\
    \ the purpose described.\n   In all cases such identification is intended only\
    \ to illustrate the\n   possibility of implementation of an idea or approach.\
    \  UNIX is a\n   trademark of AT&T Bell Laboratories.\n   Most of the discussions\
    \ in the remainder of the document deal with\n   Class 4 exclusively, since there\
    \ are far more implementation issues\n   with Class 4 than for Class 2.  Also,\
    \ since Class 2 is logically a\n   special case of Class 4, it is possible to\
    \ implement Class 4 alone,\n   with special provisions to behave as Class 2 when\
    \ necessary.\n"
- title: 2.1   Host operating system program.
  contents:
  - "2.1   Host operating system program.\n   A common method of implementing the\
    \ OSI transport service is to\n   integrate the required code into the specific\
    \ operating system\n   supporting the data communications applications.  The particular\n\
    \   technique for integration usually depends upon the structure and\n   facilities\
    \ of the operating system to be used.  For example, the\n   transport software\
    \ might be implemented in the operating system\n   kernel, accessible through\
    \ a standard set of system calls.  This\n   scheme is typically used when implementing\
    \ transport for the UNIX\n   operating system.  Class 4 transport has been implemented\
    \ using this\n   technique for System V by AT&T and for BSD 4.2 by several\n \
    \  organizations.  As another example, the transport service might be\n   structured\
    \ as a device driver.  This approach is used by DEC for the\n   VAX/VMS implementation\
    \ of classes 0, 2, and 4 of the OSI transport\n   protocol.  The Intel iRMX-86\
    \ implementation of Class 4 transport is\n   another example.  Intel implements\
    \ the transport software as a first\n   level job within the operating system.\
    \  Such an approach allows the\n   software to be linked to the operating system\
    \ and loaded with every\n   boot of the system.\n   Several advantages may accrue\
    \ to the communications user when\n   transport is implemented as an integral\
    \ part of the operating system.\n   First,  the interface to data communications\
    \ services is well known\n   to the application programmer since the same principles\
    \ are followed\n   as for other operating system services.  This allows the fast\n\
    \   implementation of communications applications without the need for\n   retraining\
    \ of programmers.  Second, the operating system can support\n   several different\
    \ suites of protocols without the need to change\n   application programs.  This\
    \ advantage can be realized only with\n   careful engineering and control of the\
    \ user-system call interface to\n   the transport services.  Third, the transport\
    \ software may take\n   advantage of the normally available operating system services\
    \ such as\n   scheduling, flow control, memory management, and interprocess\n\
    \   communication.  This saves time in the development and maintenance of\n  \
    \ the transport software.\n   The disadvantages that exist with operating system\
    \ integration of the\n   TP are primarily dependent upon the specific operating\
    \ system.\n   However, the major disadvantage, degradation of host application\n\
    \   performance, is always present.  Since the communications software\n   requires\
    \ the attention of the processor to handle interrupts and\n   process protocol\
    \ events, some degradation will occur in the\n   performance of host applications.\
    \  The degree of degradation is\n   largely a feature of the hardware architecture\
    \ and processing\n   resources required by the protocol.  Other disadvantages\
    \ that may\n   appear relate to limited performance on the part of the\n   communications\
    \ service.  This limited performance is usually a\n   function of the particular\
    \ operating system and is most directly\n   related to the method of interprocess\
    \ communication provided with the\n   operating system.  In general, the more\
    \ times a message must be\n   copied from one area of memory to another, the poorer\
    \ the\n   communications software will perform.  The method of copying and the\n\
    \   number  of copies is often a function of the specific operating\n   system.\
    \  For example, copying could be optimized if true shared\n   memory is supported\
    \ in the operating system.  In this case, a\n   significant amount of copying\
    \ can be reduced to pointer-passing.\n"
- title: 2.2   User program.
  contents:
  - "2.2   User program.\n   The OSI transport service can be implemented as a user\
    \ job within any\n   operating system provided a means of multi-task communications\
    \ is\n   available or can be implemented.  This approach is almost always a\n\
    \   bad one.  Performance problems will usually exist because the\n   communication\
    \ task is competing for resources like any other\n   application program.  The\
    \ only justification for this approach is the\n   need to develop a simple implementation\
    \ of the transport service\n   quickly.  The NBS implemented the transport protocol\
    \ using this\n   approach as the basis for a transport protocol correctness testing\n\
    \   system.  Since performance was not a goal of the NBS implementation,\n   the\
    \ ease of development and maintenance made this approach\n   attractive.\n"
- title: 2.3   Independent processing element attached to a system bus.
  contents:
  - "2.3   Independent processing element attached to a system bus.\n   Implementation\
    \ of the transport service on an independent processor\n   that attaches to the\
    \ system bus may provide substantial performance\n   improvements over other approaches.\
    \  As computing power and memory\n   have become cheaper this approach has become\
    \ realistic.  Examples\n   include the Intel implementation of iNA-961 on a variety\
    \ of multibus\n   boards such as the iSBC 186/51 and the iSXM 554.  Similar products\n\
    \   have been developed by Motorola and by several independent vendors of\n  \
    \ IBM PC add-ons.  This approach requires that the transport software\n   operate\
    \ on an independent hardware set running under operating system\n   code developed\
    \ to support the communications software environment.\n   Communication with the\
    \ application programs takes place across the\n   system bus using some simple,\
    \ proprietary vendor protocol.  Careful\n   engineering can provide the application\
    \ programmer with a standard\n   interface to the communications processor that\
    \ is similar to the\n   interface to the input/output subsystem.\n   The advantages\
    \ of this approach are mainly concentrated upon enhanced\n   performance both\
    \ for the host applications and the communications\n   service.  Depending on\
    \ such factors as the speed of the\n   communications processor and the system\
    \ bus, data communications\n   throughput may improve by one or two orders of\
    \ magnitude over that\n   available from host operating system integrated implementations.\n\
    \   Throughput for host applications should also improve since the\n   communications\
    \ processing and interrupt handling for timers and data\n   links have been removed\
    \ from the host processor.  The communications\n   mechanism used between the\
    \ host and communication processors is\n   usually sufficiently simple that no\
    \ real burden is added to either\n   processor.\n   The disadvantages for this\
    \ approach are caused by complexity in\n   developing the communications software.\
    \  Software development for the\n   communications board cannot be supported with\
    \ the standard operating\n   system tools.  A method of downloading the processor\
    \ board and\n   debugging the communications software may be required; a trade-off\n\
    \   could be to put the code into firmware or microcode.  The\n   communications\
    \ software must include at least a hardware monitor and,\n   more typically, a\
    \ small operating system to support such functions as\n   interprocess communication,\
    \ buffer management, flow control, and task\n   synchronization.  Debugging of\
    \ the user to communication subsystem\n   interface may involve several levels\
    \ of system software and hardware.\n   The design of the processing element can\
    \ follow conventional lines,\n   in which a single processor handling almost all\
    \ of the operation of\n   the protocol.  However, with inexpensive processor and\
    \ memory chips\n   now available, a multiprocessor design is economically viable.\
    \  The\n   diagram below shows one such design, which almost directly\n   corresponds\
    \ to the structure of the formal description.  There are\n   several advantages\
    \ to this design:\n    1) management of CPU and memory resources is at a minimum;\n\
    \    2) essentially no resource contention;\n    3) transport connection operation\
    \ can be written in microcode,\n       separate from network service handling;\n\
    \    4) transport connections can run with true parallelism;\n    5) throughput\
    \ is not limited by contention of connections for CPU\n       and network access;\
    \ and\n    6) lower software complexity, due to functional separation.\n   Possible\
    \ disadvantages are greater inflexibility and hardware\n   complexity.  However,\
    \ these might be offset by lower development\n   costs for microcode, since the\
    \ code separation should provide overall\n   lower code complexity in the TPE\
    \ and the TPM implementations.\n   In this system, the TPE instantiates a TPM\
    \ by enabling its clock.\n   Incoming Outgoing are passed to the TPMs along the\
    \ memory bus.  TPDUs\n   TPDUs from a TPM are sent on the output data bus.  The\
    \ user interface\n   controller accepts connect requests from the user and directs\
    \ them to\n   the TPE.  The TPE assigns a connection reference and informs the\n\
    \   interface controller to direct further inputs for this connection to\n   the\
    \ designated TPM.  The shared TPM memory is analogous to the\n   exported variables\
    \ of the TPM modules in the formal description, and\n   is used by the TPE to\
    \ input TPDUs and other information to the TPM.\n   In summary, the off-loading\
    \ of communications protocols onto\n   independent processing systems attached\
    \ to a host processor across a\n   system bus is quite common.  As processing\
    \ power and memory become\n   cheaper, the amount of software off-loaded grows.\
    \  it is now typical\n   to fine transport service available for several system\
    \ buses with\n   interfaces to operating systems such as UNIX, XENIX, iRMX, MS-DOS,\n\
    \   and VERSADOS.\n   Legend:    ****  data channel\n              ....  control\
    \ channel\n              ====  interface i/o bus\n               O    channel\
    \ or bus connection point\n                  user\n                  input\n \
    \                   *\n                    *\n          __________V_________\n\
    \          |  user interface  |       input bus\n          |    controller   \
    \ |=================O==============O=======\n          |__________________|  \
    \               *              *\n                    *                      \
    \    *              *\n                    *                          *      \
    \ _______*_______\n                    *                          *       | data\
    \ buffers|\n                    *                          *    ...|     TPM1\
    \    |\n                    *                          *    :  |_____________|\n\
    \                    *                          *    :         *\n           \
    \         *                          *    :         *\n   _________   _____*__________\
    \   ________   __*____:______   *\n   |  TPE  |   | TPE processor|   |shared|\
    \   |    TPM1    |   *\n   |buffers|***|              |   | TPM1 |***|  processor\
    \ |   *\n   |_______|   |______________|   | mem. |   |____________|   *\n   \
    \    *         :    :    *      |______|        :           *\n       *      \
    \   :    :    *          *           :           *\n       *         :    :  \
    \  ***********O***********:********************\n       *         :    :     \
    \  memory bus          :           *\n       *         :    :                \
    \           :           *\n       *         :    :...........................O...........*........\n\
    \   ____*_________:___         clock enable                    *\n   |    network\
    \     |                                         *\n   |   interface    |=========================================O========\n\
    \   |   controller   |         output data bus\n   |________________|\n      \
    \     *\n           *\n           V\n      to network\n       interface\n"
- title: 2.4   Front end processor.
  contents:
  - "2.4   Front end processor.\n   A more traditional approach to off-loading communications\
    \ protocols\n   involves the use of a free-standing front end processor, an approach\n\
    \   very similar to that of placing the transport service onto a board\n   attached\
    \ to the system bus.  The difference is one of scale.  Typical\n   front end p\
    \ interface locally as desirable, as long as such additions\n   are strictly local\
    \ (i.e., the invoking of such services does not\n   result in the exchange of\
    \ TPDUs with the peer entity).\n   The interface between the  user  and  transport\
    \  is  by nature\n   asynchronous (although some hypothetical implementation that\
    \ is\n   wholly synchronous could be conjectured).  This characteristic  is\n\
    \   due  to two factors: 1) the interprocess communications (IPC)\n   mechanism--used\
    \  between  the  user  and transport--decouples the\n   two, and to avoid blocking\
    \ the user process (while waiting for a\n   response) requires  an  asynchronous\
    \ response  mechanism,  and  2)\n   there are some asynchronously-generated transport\
    \ indications that\n   must  be handled (e.g.,  the  arrival of user data or the\
    \ abrupt\n   termination of  the  transport  connection  due  to  network errors).\n\
    \   If it is assumed that the user interface to transport is\n   asynchronous,\
    \  there are other aspects of the interface that are also\n   predetermined. \
    \ The most important of these is that transport\n   service  requests are confirmed\
    \ twice.  The first confirmation occurs\n   at the time  of  the  transport  service\
    \ request  initiation.  Here,\n   interface routines can be used to identify invalid\
    \ sequences of\n   requests, such as a request to  send  data  on  a  connection\
    \ that is\n   not yet open.  The second confirmation occurs when the service\n\
    \   request crosses the interface into the transport entity.  The entity\n   may\
    \ accept or reject the request, depending on its resources and its\n   assessment\
    \ of connection (transport and network) status, priority,\n   service quality.\n\
    \   If the interface is to be asynchronous, then some mechanism must be\n   provided\
    \ to handle the asynchronous (and sometimes unexpected)\n   events.  Two ways\
    \ this is commonly achieved are: 1) by polling, and\n   2) by a software interrupt\
    \ mechanism.  The first of these can be\n   wasteful of host resources in a multiprogramming\
    \ environment, while\n   the second may be complicated to implement.  However,\
    \ if the\n   interface is a combination of hardware and software, as in the cases\n\
    \   discussed in Parts 2.3 and 2.4, then hardware interrupts may be\n   available.\n\
    \   One way of implementing the abstract services is to associate with\n   each\
    \ service primitive an actual function that is invoked.  Such\n   functions could\
    \ be held in a special interface library with other\n   functions and procedures\
    \ that realize the interface.  Each service\n   primitive function would access\
    \ the interprocess communication (IPC)\n   mechanism as necessary to pass parameters\
    \ to/from the transport\n   entity.\n   The description of the abstract service\
    \ in IS 8073 and N3756 implies\n   that the interface must handle TSDUs of arbitrary\
    \ length.  This\n   situation suggests that it may be useful to implement a TSDU\
    \ as an\n   object such as a file-pointer rather than as the message itself. \
    \ In\n   this way, in the sending entity, TPDUs can be formed by reading\n   segments\
    \ of TPDU-size from the file designated, without regard for\n   the actual length\
    \ of the file.  In the receiving entity, each new\n   TPDU could be buffered in\
    \ a file designated by a file-pointer, which\n   would then be passed to the user\
    \ when the EOT arrives.  In the formal\n   description of transport, this procedure\
    \ is actually described,\n   although explicit file-pointers and files are not\
    \ used in the\n   description.  This method of implementing the data interface\
    \ is not\n   essentially different from maintaining a linked list of buffers.\
    \  (A\n   disk file is arranged in precisely this fashion, although the file\n\
    \   user is usually not aware of the structure.)\n   The abstract service definition\
    \ describes  the  set  of parameters\n   that must be passed in each of the service\
    \ primitives so that\n   transport can act properly on  behalf  of  the user.\
    \   These\n   parameters are required for the transport protocol to operate\n\
    \   correctly (e.g., a called address  must  be passed  with  the\n   connect\
    \  request and the connect response must contain a responding\n   address).  \
    \ The  abstract  service defintion does not preclude,\n   however, the inclusion\
    \ of local parameters.  Local parameters may be\n   included in the implementation\
    \  of  the  service  interface  for use\n   by the local entity.  One example\
    \ is a buffer management parameter\n   passed from  the  user  in connect requests\
    \ and confirms, providing\n   the transport entity with expected buffer  usage\
    \  estimates.  The\n   local  entity  could  use  this  in implementing a more\
    \ efficient\n   buffer management strategy than would otherwise be possible.\n\
    \   One issue that is  of  importance  when  designing  and implementing\n   a\
    \ transport entity is the provision of a registration mechanism for\n   transport\
    \ users.  This facility provides a means of identifying to\n   the transport entity\
    \ those users who are willing to participate in\n   communications with remote\
    \ users.  An example of such a user is a\n   data base management system, which\
    \ ordinarily responds to connections\n   requests rather than to initiate them.\
    \  This procedure of user\n   identification is sometimes called a \"passive open\"\
    .  There are\n   several ways in which registration can be implemented.  One is\
    \ to\n   install the set of users that  provide services  in  a table at\n   system\
    \ generation time.  This method may have the disadvantage of\n   being  inflexible.\
    \   A  more flexible  approach is to implement a\n   local transport service primitive,\
    \ \"listen\", to indicate a waiting\n   user.   The  user then  registers  its\
    \ transport suffix with the\n   transport entity via the listen primitive.  Another\
    \ possibility is a\n   combination of predefined table and listen primitive. \
    \ Other\n   parameters may also be included,  such  as a partially or fully\n\
    \   qualified transport address from which the user is willing  to\n   receive\
    \  connections.  A  variant  on  this  approach  is  to\n   provide  an ACTIVE/PASSIVE\
    \ local parameter on the connect  request\n   service primitive.  Part 5 discusses\
    \ this issue in more detail.\n"
- title: 3.2   Flow control.
  contents:
  - "3.2   Flow control.\n   Interface flow control is generally considered to be\
    \ a local\n   implementation issue.  However, in order to completely specify the\n\
    \   behavior of the transport entity, it was necessary to include in the\n   formal\
    \ description a model of the control of data flow across the\n   service boundaries\
    \ of transport.  The international standards for\n   transport and the OSI reference\
    \ model state only that interface flow\n   control shall be provided but give\
    \ no guidance on its features.\n   The actual mechanisms used to accomplish flow\
    \ control, which need not\n   explicitly follow the model in the formal description,\
    \ are dependent\n   on the way in which the interface itself is realized, i.e.,\
    \ what\n   TSDUs and service primitives really are and how the transport entity\n\
    \   actually communicates with its user, its environment, and the network\n  \
    \ service.  For example, if the transport entity communicates with its\n   user\
    \ by means of named (UNIX) pipes, then flow control can be\n   realized using\
    \ a special interface library routine, which the\n   receiving process invokes,\
    \ to control the pipe.  This approach also\n   entails some consideration for\
    \ the capacity of the pipe and blocking\n   of the sending process when the pipe\
    \ is full (discussed further in\n   Part 3.3).  The close correspondence of this\
    \ interpretation to the\n   model is clear.  However, such an interpretation is\
    \ apparently not\n   workable if the user process and the transport entity are\
    \ in\n   physically separate processors.  In this situation, an explicit\n   protocol\
    \ between the receiving process and the sending process must\n   be provided,\
    \ which could have the complexity of the data transfer\n   portion of the Class\
    \ 0 transport protocol (Class 2 if flow\n   controlled).  Note that the formal\
    \ model, under proper\n   interpretation, also describes this mechanism.\n"
- title: 3.3   Interprocess communication.
  contents:
  - "3.3   Interprocess communication.\n   One of the most important elements of a\
    \ data communication system is\n   the approach to interprocess communication\
    \ (IPC).  This is true\n   because suites of protocols are often implemented as\
    \ groups of\n   cooperating tasks.  Even if the protocol suites are not implemented\n\
    \   as task groups, the communication system is a funnel for service\n   requests\
    \ from multiple user processes.  The services are normally\n   communicated through\
    \ some interprocess pathway.  Usually, the\n   implementation environment places\
    \ some restrictions upon the\n   interprocess communications method that can be\
    \ used.  This section\n   describes the desired traits of IPC for use in data\
    \ communications\n   protocol implementations, outlines some possible uses for\
    \ IPC, and\n   discusses three common and generic approaches to IPC.\n   To support\
    \ the implementation of data communications protocols, IPC\n   should possess\
    \ several desirable traits.  First,  IPC should be\n   transaction based.  This\
    \ permits sending a message without the\n   overhead of establishing and maintaining\
    \ a connection.  The\n   transactions should be confirmed so that a sender can\
    \ detect and\n   respond to non-delivery.  Second,  IPC should support both the\n\
    \   synchronous and the asynchronous modes of message exchange.  An IPC\n   receiver\
    \ should be able to ask for delivery of any pending messages\n   and not be blocked\
    \ from continuing if no messages are present.\n   Optionally, the receiver should\
    \ be permitted to wait if no messages\n   are present, or to continue if the path\
    \ to the destination is\n   congested.  Third, IPC should preserve the order of\
    \ messages sent to\n   the same destination.  This allows the use of the IPC without\n\
    \   modification to support protocols that preserve user data sequence.\n   Fourth,\
    \ IPC should provide a flow control mechanism to allow pacing\n   of the sender's\
    \ transmission speed to that of the receiver.\n   The uses of IPC in implementation\
    \ of data communication systems are\n   many and varied.  A common and expected\
    \ use for IPC is that of\n   passing user messages among the protocol tasks that\
    \ are cooperating\n   to perform the data communication functions.  The user messages\
    \ may\n   contain the actual data or, more efficiently, references to the\n  \
    \ location of the user data.  Another common use for the IPC is\n   implementation\
    \ and enforcement of local interface flow control.  By\n   limiting the number\
    \ of IPC messages queued on a particular address,\n   senders can be slowed to\
    \ a rate appropriate for the IPC consumer.  A\n   third typical use for IPC is\
    \ the synchronization of processes.  Two\n   cooperating tasks can coordinate\
    \ their activities or access to shared\n   resources by passing IPC messages at\
    \ particular events in their\n   processing.\n   More creative uses of IPC include\
    \ buffer, timer, and scheduling\n   management.  By establishing buffers as a\
    \ list of messages available\n   at a known address at system initialization time,\
    \ the potential\n   exists to manage buffers simply and efficiently.  A process\
    \ requiring\n   a buffer would simply read an IPC message from the known address.\
    \  If\n   no messages (i.e., buffers) are available, the process could block\n\
    \   (or continue, as an option).  A process that owned a buffer and\n   wished\
    \ to release it would simply write a message to the known\n   address, thus unblocking\
    \ any processes waiting for a buffer.\n   To manage timers, messages can be sent\
    \ to a known address that\n   represents the timer module.  The timer module can\
    \ then maintain the\n   list of timer messages with respect to a hardware clock.\
    \  Upon\n   expiration of a timer, the associated message can be returned to the\n\
    \   originator via IPC.  This provides a convenient method to process the\n  \
    \ set of countdown timers required by the transport protocol.\n   Scheduling management\
    \ can be achieved by using separate IPC addresses\n   for message classes.  A\
    \ receiving process can enforce a scheduling\n   discipline by the order in which\
    \ the message queues are read.  For\n   example, a transport process might possess\
    \ three queues:  1) normal\n   data from the user, 2) expedited data from the\
    \ user, and 3) messages\n   from the network.  If the transport process then wants\
    \ to give top\n   priority to network messages, middle priority to expedited user\n\
    \   messages, and lowest priority to normal user messages, all that is\n   required\
    \ is receipt of IPC messages on the highest priority queue\n   until no more messages\
    \ are available.  Then the receiver moves to the\n   next lower in priority and\
    \ so on.  More sophistication is possible by\n   setting limits upon the number\
    \ of consecutive messages received from\n   each queue and/or varying the order\
    \ in which each queue is examined.\n   It is easy to see how a round-robin scheduling\
    \ discipline could be\n   implemented using this form of IPC.\n   Approaches to\
    \ IPC can be placed into one of three classes:  1) shared\n   memory, 2) memory-memory\
    \ copying, and 3) input/output channel\n   copying. Shared memory is the most\
    \ desirable of the three classes\n   because the amount of data movement is kept\
    \ to a minimum.  To pass\n   IPC messages using shared memory, the sender builds\
    \ a small message\n   referencing a potentially large amount of user data.  The\
    \ small\n   message is then either copied from the sender's process space to the\n\
    \   receiver's process space or the small message is mapped from one\n   process\
    \ space to another using techniques specific to the operating\n   system and hardware\
    \ involved.  These approaches to shared memory are\n   equivalent since the amount\
    \ of data movement is kept to a minimum.\n   The price to be paid for using this\
    \ approach is due to the\n   synchronization of access to the shared memory. \
    \ This type of sharing\n   is well understood, and several efficient and simple\
    \ techniques exist\n   to manage the sharing.\n   Memory-memory copying is an\
    \ approach that has been commonly used for\n   IPC in UNIX operating system implementations.\
    \  To pass an IPC message\n   under UNIX data is copied from the sender's buffer\
    \ to a kernel buffer\n   and then from a kernel buffer to the receiver's buffer.\
    \  Thus two\n   copy operations are required for each IPC message. Other methods\n\
    \   might only involve a single copy operation.  Also note that if one of\n  \
    \ the processes involved is the transport protocol implemented in the\n   kernel,\
    \ the IPC message must only be copied once.  The main\n   disadvantage of this\
    \ approach is inefficiency.  The major advantage\n   is simplicity.\n   When the\
    \ processes that must exchange messages reside on physically\n   separate computer\
    \ systems (e.g., a host and front end), an\n   input/output channel of some type\
    \ must be used to support the IPC.\n   In such a case, the problem is similar\
    \ to that of the general problem\n   of a transport protocol.  The sender must\
    \ provide his IPC message to\n   some standard operating system output mechanism\
    \ from where it will be\n   transmitted via some physical medium to the receiver's\
    \ operating\n   system.  The receiver's operating system will then pass the message\n\
    \   on to the receiving process via some standard operating system input\n   mechanism.\
    \  This set of procedures can vary greatly in efficiency and\n   complexity depending\
    \ upon the operating systems and hardware\n   involved.  Usually this approach\
    \ to IPC is used only when the\n   circumstances require it.\n"
- title: 3.4   Interface to real networks.
  contents:
  - "3.4   Interface to real networks.\n   Implementations of the class 4 transport\
    \ protocol have been operated\n   over a wide variety of networks including: \
    \ 1) ARPANET, 2) X.25\n   networks, 3) satellite channels, 4) CSMA/CD local area\
    \ networks, 5)\n   token bus local area networks, and  6) token ring local area\n\
    \   networks.  This section briefly describes known instances of each use\n  \
    \ of class 4 transport and provides some quantitative evaluation of the\n   performance\
    \ expectations for transport over each network type.\n"
- title: 3.4.1   Issues.
  contents:
  - "3.4.1   Issues.\n   The interface of the transport entity to the network service\
    \ in\n   general will be realized in a different way from the user interface.\n\
    \   The network service processor is often separate from the host CPU,\n   connected\
    \ to it by a bus, direct memory access (DMA), or other link.\n   A typical way\
    \ to access the network service is by means of a device\n   driver.  The transfer\
    \ of data across the interface in this instance\n   would be by buffer-copying.\
    \  The use of double-buffering reduces some\n   of the complexity of flow control,\
    \ which is usually accomplished by\n   examining the capacity of the target buffer.\
    \  If the transport\n   processor and the network processor are distinct and connected\
    \ by a\n   bus or external link, the network access may be more complicated\n\
    \   since copying will take place across the bus or link rather than\n   across\
    \ the memory board.  In any case, the network service\n   primitives, as they\
    \ appear in the formal description and IS 8073 must\n   be carefully correlated\
    \ to the actual access scheme, so that the\n   semantics of the primitives is\
    \ preserved.  One way to do this is to\n   create a library of routines, each\
    \ of which corresponds to one of the\n   service primitives.  Each routine is\
    \ responsible for sending the\n   proper signal to the network interface unit,\
    \ whether this\n   communication is directly, as on a bus, or indirectly via a\
    \ device\n   driver.  In the case of a connectionless network service, there is\n\
    \   only one primitive, the N_DATA_request (or N_UNIT_DATA_request),\n   which\
    \ has to be realized.\n   In the formal description, flow control to the NSAP\
    \ is controlled by\n   by a Slave module, which exerts the \"backpressure\" on\
    \ the TPM if its\n   internal queue gets too long.  Incoming flow, however, is\
    \ controlled\n   in much the same way as the flow to the transport user is controlled.\n\
    \   The implementor is reminded that the formal description of the flow\n   control\
    \ is specified for completeness and not as an implementation\n   guide.  Thus,\
    \ an implementation should depend upon actual interfaces\n   in the operating\
    \ environment to realize necessary functions.\n"
- title: 3.4.2   Instances of operation.
  contents:
  - '3.4.2   Instances of operation.

    '
- title: 3.4.2.1   ARPANET
  contents:
  - "3.4.2.1   ARPANET\n   An early implementation of the class 4 transport protocol\
    \ was\n   developed by the NBS as a basis for conformance tests [NBS83].  This\n\
    \   implementation was used over the ARPANET to communicate between NBS,\n   BBN,\
    \ and DCA.  The early NBS implementation was executed on a\n   PDP-11/70.  A later\
    \ revision of the NBS implementation has been moved\n   to a VAX-11/750 and VAX-11/7;80.\
    \ The Norwegian Telecommunication\n   Administration (NTA) has implemented class\
    \ 4 transport for the UNIX\n   BSD 4.2 operating system to run on a VAX [NTA84].\
    \  A later NTA\n   implementation runs on a Sun 2-120 workstation.  The University\
    \ of\n   Wisconsin has also implemented the class 4 transport protocol on a\n\
    \   VAX-11/750 [BRI85]. The Wisconsin implementation is embedded in the\n   BSD\
    \ 4.2 UNIX kernel.  For most of these implementations class 4\n   transport runs\
    \ above the DOD IP and below DOD application protocols.\n"
- title: 3.4.2.2   X.25 networks
  contents:
  - "3.4.2.2   X.25 networks\n   The NBS implementations have been used over Telenet,\
    \ an X.25 public\n   data network (PDN).  The heaviest use has been testing of\
    \ class 4\n   transport between the NBS and several remotely located vendors,\
    \ in\n   preparation for a demonstration at the 1984 National Computing\n   Conference\
    \ and the 1985 Autofact demonstration.  Several approaches\n   to implementation\
    \ were seen in the vendors' systems, including ones\n   similar to those discussed\
    \ in Part 6.2.  At the Autofact\n   demonstration many vendors operated class\
    \ 4 transport and the ISO\n   internetwork protocol across an internetwork of\
    \ CSMA/CD and token bus\n   local networks and Accunet, an AT&T X.25 public data\
    \ network.\n"
- title: 3.4.2.3   Satellite channels.
  contents:
  - "3.4.2.3   Satellite channels.\n   The COMSAT Laboratories have implemented class\
    \ 4 transport for\n   operation over point-to-point satellite channels with data\
    \ rates up\n   to 1.544 Mbps [CHO85].  This implementation has been used for\n\
    \   experiments between the NBS and COMSAT.  As a result of these\n   experiments\
    \ several improvements have been made to the class 4\n   transport specification\
    \ within the international standards arena\n   (both ISO and CCITT). The COMSAT\
    \ implementation runs under a\n   proprietary multiprocessing operating system\
    \ known as COSMOS.  The\n   hardware base includes multiple Motorola 68010 CPUs\
    \ with local memory\n   and Multibus shared memory for data messages.\n"
- title: 3.4.2.4   CSMA/CD networks.
  contents:
  - "3.4.2.4   CSMA/CD networks.\n   The CSMA/CD network as defined by the IEEE 802.3\
    \ standard is the most\n   popular network over which the class 4 transport has\
    \ been\n   implemented. Implementations of transport over CSMA/CD networks have\n\
    \   been demonstrated by: AT&T, Charles River Data Systems,\n   Computervision,\
    \ DEC, Hewlitt-Packard, ICL, Intel, Intergraph, NCR and\n   SUN.  Most of these\
    \ were demonstrated at the 1984 National Computer\n   Conference [MIL85b] and\
    \ again at the 1985 Autofact Conference.\n   Several of these vendors are now\
    \ delivering products based on the\n   demonstration software.\n"
- title: 3.4.2.5   Token bus networks.
  contents:
  - "3.4.2.5   Token bus networks.\n   Due to the establishment of class 4 transport\
    \ as a mandatory protocol\n   within the General Motor's manufacturing automation\
    \ protocol (MAP),\n   many implementations have been demonstrated operating over\
    \ a token\n   bus network as defined by the IEEE 802.4 standard.  Most past\n\
    \   implementations relied upon a Concord Data Systems token interface\n   module\
    \ (TIM) to gain access to the 5 Mbps broadband 802.4 service.\n   Several vendors\
    \ have recently announced boards supporting a 10 Mbps\n   broadband 802.4 service.\
    \  The newer boards plug directly into\n   computer system buses while the TIM's\
    \ are accessed across a high\n   level data link control (HDLC) serial channel.\
    \  Vendors demonstrating\n   class 4 transport over IEEE 802.4 networks include\
    \ Allen-Bradley,\n   AT&T, DEC, Gould, Hewlett-Packard, Honeywell, IBM, Intel,\
    \ Motorola,\n   NCR and Siemens.\n"
- title: 3.4.2.6   Token ring networks.
  contents:
  - "3.4.2.6   Token ring networks.\n   The class 4 transport implementations by the\
    \ University of Wisconsin\n   and by the NTA run over a 10 Mbps token ring network\
    \ in addition to\n   ARPANET.  The ring used is from Proteon rather than the recently\n\
    \   finished IEEE 802.5 standard.\n"
- title: 3.4.3   Performance expectations.
  contents:
  - "3.4.3   Performance expectations.\n   Performance research regarding the class\
    \ 4 transport protocol has\n   been limited.  Some work has been done at the University\
    \ of\n   Wisconsin, at NTA, at Intel, at COMSAT, and at the NBS.  The material\n\
    \   presented below draws from this limited body of research to provide\n   an\
    \ implementor with some quantitative feeling for the performance\n   that can\
    \ be expected from class 4 transport implementations using\n   different network\
    \ types.  More detail is available from several\n   published reports [NTA84,\
    \ BRI85, INT85, MIL85b, COL85].  Some of the\n   results reported derive from\
    \ actual measurements while other results\n   arise from simulation.  This distinction\
    \ is clearly noted.\n"
- title: 3.4.3.1   Throughput.
  contents:
  - "3.4.3.1   Throughput.\n   Several live experiments have been conducted to determine\
    \ the\n   throughput possible with implementations of class 4 transport.\n   Achievable\
    \ throughput depends upon many factors including:  1) CPU\n   capabilities, 2)\
    \ use or non-use of transport checksum, 3) IPC\n   mechanism, 4) buffer management\
    \ technique, 5) receiver resequencing,\n   6) network error properties, 7) transport\
    \ flow control, 8) network\n   congestion and 9) TPDU size.  Some of these are\
    \ specifically\n   discussed elsewhere in this document.  The reader must keep\
    \ in mind\n   these issues when interpreting the throughput measures presented\n\
    \   here.\n   The University of Wisconsin implemented class 4 transport in the\
    \ UNIX\n   kernel for a VAX-11/750 with the express purpose of measuring the\n\
    \   achievable throughput.  Throughputs observed over the ARPANET ranged\n   between\
    \ 10.4 Kbps and 14.4 Kbps.  On an unloaded Proteon ring local\n   network, observed\
    \ throughput with checksum ranged between 280 Kbps\n   and 560 Kbps.  Without\
    \ checksum, throughput ranged between 384 Kbps\n   and 1 Mbps.\n   The COMSAT\
    \ Laboratories implemented class 4 transport under a\n   proprietary multiprocessor\
    \ operating system for a multiprocessor\n   68010 hardware architecture.  The\
    \ transport implementation executed\n   on one 68010 while the traffic generator\
    \ and link drivers executed on\n   a second 68010.  All user messages were created\
    \ in a global shared\n   memory and were copied only for transmission on the satellite\
    \ link.\n   Throughputs as high as 1.4 Mbps were observed without transport\n\
    \   checksumming while up to 535 Kbps could be achieved when transport\n   checksums\
    \ were used.  Note that when the 1.4 Mbps was achieved the\n   transport CPU was\
    \ idle 20% of the time (i.e., the 1.544 Mbps\n   satellite link was the bottleneck).\
    \  Thus, the transport\n   implementation used here could probably achieve around\
    \ 1.9 Mbps user\n   throughput with the experiment parameters remaining unchanged.\n\
    \   Higher throughputs are possible by increasing the TPDU size; however,\n  \
    \ larger messages stand an increased chance of damage during\n   transmission.\n\
    \   Intel has implemented a class 4 transport product for operation over\n   a\
    \ CSMA/CD local network (iNA-960 running on the iSBC 186/51 or iSXM\n   552).\
    \  Intel has measured throughputs achieved with this combination\n   and  has\
    \ published the results in a technical analysis comparing\n   iNA-960 performance\
    \ on the 186/51 with iNA-960 on the 552.  The CPU\n   used to run transport was\
    \ a 6 MHz 80186.  An 82586 co-processor was\n   used to handle the medium access\
    \ control.  Throughputs measured\n   ranged between 360 Kbps and 1.32 Mbps, depending\
    \ on the parameter\n   values used.\n   Simulation of class 4 transport via a\
    \ model developed at the NBS has\n   been used to predict the performance of the\
    \ COMSAT implementation and\n   is now being used to predict the performance of\
    \ a three processor\n   architecture that includes an 8 MHz host connected to\
    \ an 8 MHz front\n   end over a system bus.  The third processor provides medium\
    \ access\n   control for the specific local networks  being modeled.  Early model\n\
    \   results predict throughputs over an unloaded CSMA/CD local network of\n  \
    \ up to 1.8 Mbps.  The same system modeled over a token bus local\n   network\
    \ with the same transport parameters yields throughput\n   estimates of up to\
    \ 1.6 Mbps.  The token bus technology, however,\n   permits larger message sizes\
    \ than CSMA/CD does.  When TPDUs of 5120\n   bytes are used, throughput on the\
    \ token bus network is predicted to\n   reach 4.3 Mbps.\n"
- title: 3.4.3.2   Delay.
  contents:
  - "3.4.3.2   Delay.\n   The one-way delay between sending transport user and receiving\n\
    \   transport user is determined by a complex set of factors.  Readers\n   should\
    \ also note that, in general, this is a difficult measure to\n   make and little\
    \ work has been done to date with respect to expected\n   one-way delays with\
    \ class 4 transport implementations.  In this\n   section a tutorial is given\
    \ to explain the factors that determine the\n   one-way delay to be expected by\
    \ a transport user.  Delay experiments\n   performed by Intel are reported [INT85],\
    \ as well as some simulation\n   experiments conducted by the NBS [MIL85a].\n\
    \   The transport user can generally expect one-way delays to be\n   determined\
    \ by the following equation.\n     D = TS + ND + TR + [IS] + [IR]        (1)\n\
    \   where:\n      [.] means the enclosed quantity may be 0\n      D is the one-way\
    \ transport user delay,\n      TS is the transport data send processing time,\n\
    \      IS is the internet datagram send processing time,\n      ND is the network\
    \ delay,\n      IR is the internet datagram receive processing\n      time, and\n\
    \      TR is the transport data receive processing time.\n   Although no performance\
    \ measurements are available for the ISO\n   internetwork protocol (ISO IP), the\
    \ ISO IP is so similar to the DOD\n   IP that processing times associated with\
    \ sending and receiving\n   datagrams should be the about the same for both IPs.\
    \  Thus, the IS\n   and IR terms given above are ignored from this point on in\
    \ the\n   discussion.  Note that many of these factors vary depending upon the\n\
    \   application traffic pattern and loads seen by a transport\n   implementation.\
    \  In the following discussion, the transport traffic\n   is assumed to be a single\
    \ message.\n   The value for TS depends upon the CPU used, the IPC mechanism,\
    \ the\n   use or non-use of checksum, the size of the user message and the size\n\
    \   of TPDUs, the buffer management scheme in use, and the method chosen\n   for\
    \ timer management.  Checksum processing times have been observed\n   that include\
    \ 3.9 us per octet for a VAX-11/750, 7.5 us per octet on a\n   Motorola 68010,\
    \ and 6 us per octet on an Intel 80186.  The class 4\n   transport checksum algorithm\
    \ has considerable effect on achievable\n   performance. This is discussed further\
    \ in Part 7.  Typical values for\n   TS, excluding the processing due to the checksum,\
    \ are about 4 ms for\n   CPUs such as the Motorola 68010 and the Intel 80186.\
    \  For 1024 octet\n   TPDUs, checksum calculation can increase the TS value to\
    \ about 12 ms.\n   The value of TR depends upon similar details as TS.  An additional\n\
    \   consideration is whether or not the receiver caches (buffers) out of\n   order\
    \ TPDUs.  If so, the TR will be higher when no packets are lost\n   (because of\
    \ the overhead incurred by the resequencing logic).  Also,\n   when packets are\
    \ lost, TR can appear to increase due to transport\n   resequencing delay.  When\
    \ out of order packets are not cached, lost\n   packets increase D because each\
    \ unacknowledged packet must be\n   retransmitted (and then only after a delay\
    \ waiting for the\n   retransmission timer to expire).  These details are not\
    \ taken into\n   account in equation 1.  Typical TR values that can be expected\
    \ with\n   non-caching implementations on Motorola 68010 and Intel 80186 CPUs\n\
    \   are approximately 3 to 3.5 ms.  When transport checksumming is used\n   on\
    \ these CPUs, TR becomes about 11 ms for 1024 byte TPDUs.\n   The value of ND\
    \ is highly variable, depending on the specific network\n   technology in use\
    \ and on the conditions in that network.  In general,\n   ND can be defined by\
    \ the following equation.\n     ND = NQ + MA + TX + PD + TQ   (2)\n   where:\n\
    \     NQ is network queuing delay,\n     MA is medium access delay,\n     TX is\
    \ message transmission time,\n     PD is network propagation delay, and\n    \
    \ TQ is transport receive queuing delay.\n   Each term of the equation is discussed\
    \ in the following paragraphs.\n   Network queuing delay (NQ) is the time that\
    \ a TPDU waits on a network\n   transmit queue until that TPDU is the first in\
    \ line for transmission.\n   NQ depends on the size of the network transmit queue,\
    \ the rate at\n   which the queue is emptied, and the number of TPDUs already\
    \ on the\n   queue.  The size of the transmit queue is usually an implementation\n\
    \   parameter and is generally at least two messages.  The rate at which\n   the\
    \ queue empties depends upon MA and TX (see the discussion below).\n   The number\
    \ of TPDUs already on the queue is determined by the traffic\n   intensity (ratio\
    \ of mean arrival rate to mean service rate).  As an\n   example, consider an\
    \ 8 Kbps point-to-point link serving an eight\n   message queue that contains\
    \ 4 messages with an average size of 200\n   bytes per message.  The next message\
    \ to be placed into the transmit\n   queue would experience an NQ of 800 ms (i.e.,\
    \ 4 messages times 200\n   ms).  In this example, MA is zero.  These basic facts\
    \ permit the\n   computation of NQ for particular environments.  Note that if\
    \ the\n   network send queue is full, back pressure flow control will force\n\
    \   TPDUs to queue in transport transmit buffers and cause TS to appear\n   to\
    \ increase by the amount of the transport queuing delay.  This\n   condition depends\
    \ on application traffic patterns but is ignored for\n   the purpose of this discussion.\n\
    \   The value of MA depends upon the network access method and on the\n   network\
    \ congestion or load.  For a point-to-point link MA is zero.\n   For CSMA/CD networks\
    \ MA depends upon the load, the number of\n   stations, the arrival pattern, and\
    \ the propagation delay.  For\n   CSMA/CD networks MA has values that typically\
    \ range from zero (no\n   load) up to about 3 ms (80% loads).  Note that the value\
    \ of MA as\n   seen by individual stations on a CSMA/CD network is predicted (by\
    \ NBS\n   simulation studies) to be as high as 27 ms under 70% loads.  Thus,\n\
    \   depending upon the traffic patterns, individual stations may see an\n   average\
    \ MA value that is much greater than the average MA value for\n   the network\
    \ as a whole. On token bus networks MA is determined by the\n   token rotation\
    \ time (TRT) which depends upon the load, the number of\n   stations, the arrival\
    \ pattern, the propagation delay, and the values\n   of the token holding time\
    \ and target rotation times at each station.\n   For small networks of 12 stations\
    \ with a propagation delay of 8 ns,\n   NBS simulation studies predict TRT values\
    \ of about 1 ms for zero load\n   and 4.5 ms for 70% loads for 200 byte messages\
    \ arriving with\n   exponential arrival distribution.  Traffic patterns also appear\
    \ to be\n   an important determinant of target rotation time.  When a pair of\n\
    \   stations performs a continuous file transfer, average TRT for the\n   simulated\
    \ network is predicted to be 3 ms for zero background load\n   and 12.5 ms for\
    \ 70% background load (total network load of 85%).\n   The message size and the\
    \ network transmission speed directly\n   determine TX.  Typical transmission\
    \ speeds include 5 and 10 Mbps for\n   standard local networks;  64 Kbps, 384\
    \ Kbps, or 1.544 Mbps for\n   point-to-point satellite channels;  and 9.6 Kbps\
    \ or 56 Kbps for\n   public data network access links.\n   The properties of the\
    \ network in use determine the values of PD. On\n   an IEEE 802.3 network, PD\
    \ is limited to 25.6 us.  For IEEE 802.4\n   networks, the signal is propagated\
    \ up-link to a head end and then\n   down-link from the head end.  Propagation\
    \ delay in these networks\n   depends on the distance of the source and destination\
    \ stations from\n   the head end and on the head end latency. Because the maximum\
    \ network\n   length is much greater than with IEEE 802.3 networks, the PD values\n\
    \   can also be much greater.  The IEEE 802.4 standard requires that a\n   network\
    \ provider give a value for the maximum transmission path\n   delay.  For satellite\
    \ channels PD is typically between 280 and 330\n   ms.  For the ARPANET, PD depends\
    \ upon the number of hops that a\n   message makes between source and destination\
    \ nodes.  The NBS and NTIA\n   measured ARPANET PD average values of about 190\
    \ ms [NTI85].  In the\n   ARPA internet system the PD is quite variable, depending\
    \ on the\n   number of internet gateway hops and the PD values of any intervening\n\
    \   networks (possibly containing satellite channels).  In experiments on\n  \
    \ an internetwork containing a a satellite link to Korea, it was\n   determined\
    \ by David Mills [RFC85] that internet PD values could range\n   from 19 ms to\
    \ 1500 ms.  Thus, PD values ranging from 300 to 600 ms\n   can be considered as\
    \ typical for ARPANET internetwork operation.\n   The amount of time a TPDU waits\
    \ in the network receive queue before\n   being processed by the receiving transport\
    \ is represented by TQ,\n   similar to NQ in that the value of TQ depends upon\
    \ the size of the\n   queue, the number of TPDUs already in the queue, and the\
    \ rate at\n   which the queue is emptied by transport.\n   Often the user delay\
    \ D will be dominated by one of the components. On\n   a satellite channel the\
    \ principal component of D is PD, which implies\n   that ND is a principal component\
    \ by equation (2).  On an unloaded\n   LAN, TS and TR might contribute most to\
    \ D.  On a highly loaded LAN,\n   MA may cause NQ to rise, again implying that\
    \ ND is a major factor in\n   determining D.\n   Some one-way delay measures have\
    \ been made by Intel for the iNA-960\n   product running on a 6 MHz 80186.  For\
    \ an unloaded 10 Mbps CSMA/CD\n   network the Intel measures show delays as low\
    \ as 22 ms.  The NBS has\n   done some simulations of class 4 transport over 10\
    \ Mbps CSMA/CD and\n   token bus networks.  These (unvalidated) predictions show\
    \ one-way\n   delays as low as 6 ms on unloaded LANs and as high as 372 ms on\n\
    \   CSMA/CD LANs with 70% load.\n"
- title: 3.4.3.3   Response time.
  contents:
  - "3.4.3.3   Response time.\n   Determination of transport user response time (i.e.,\
    \ two-way delay)\n   depends upon many of the same factors discussed above for\
    \ one-way\n   delay.  In fact, response time can be represented by equation 3\
    \ as\n   shown below.\n      R = 2D + AS + AR     (3)\n   where:\n     R is transport\
    \ user response time,\n     D is one-way transport user delay,\n     AS is acknowledgement\
    \ send processing time, and\n     AR is acknowledgement receive processing time.\n\
    \   D has been explained above.  AS and AR deal with the acknowledgement\n   sent\
    \ by transport in response to the TPDU that embodies the user\n   request.\n \
    \  AS is simply the amount of time that the receiving transport must\n   spend\
    \ to generate an AK TPDU.  Typical times for this function are\n   about 2 to\
    \ 3 ms on processors such as the Motorola 68010 and the\n   Intel 80186.  Of course\
    \ the actual time required depends upon factors\n   such as those explained for\
    \ TS above.\n   The amount of time, AR, that the sending transport must spend\
    \ to\n   process a received AK TPDU.  Determination of the actual time\n   required\
    \ depends upon factors previously described.  Note that for AR\n   and AS, processing\
    \ when the checksum is included takes somewhat\n   longer. However, AK TPDUs are\
    \ usually between 10 and 20 octets in\n   length and therefore the increased time\
    \ due to checksum processing is\n   much less than for DT TPDUs.\n   No class\
    \ 4 transport user response time measures are available;\n   however, some simulations\
    \ have been done at the NBS.  These\n   predictions are based upon implementation\
    \ strategies that have been\n   used by commercial vendors in building microprocessor-based\
    \ class 4\n   transport products.  Average response times of about 21 ms on an\n\
    \   unloaded 10 Mbps token bus network, 25 ms with 70% loading, were\n   predicted\
    \ by the simulations.  On a 10 Mbps CSMA/CD network, the\n   simulations predict\
    \ response times of about 17 ms for no load and 54\n   ms for a 70% load.\n"
- title: 3.5   Error and status reporting.
  contents:
  - "3.5   Error and status reporting.\n   Although the abstract service definition\
    \ for the  transport protocol\n   specifies  a set of services to be offered,\
    \ the actual set of\n   services  provided  by  an  implementation need  not \
    \ be limited to\n   these.  In particular, local status and error information\
    \ can be\n   provided as a confirmed service (request/response) and as an\n  \
    \ asynchronous \"interrupt\" (indication).  One use for this service  is\n   to\
    \  allow  users  to query the transport entity about the status of\n   their connections.\
    \  An example of information  that  could  be\n   returned from the entity is:\n\
    \        o  connection state\n        o  current send sequence number\n      \
    \  o  current receive and transmit credit windows\n        o  transport/network\
    \ interface status\n        o  number of retransmissions\n        o  number of\
    \ DTs and AKs sent and received\n        o  current timer values\n   Another use\
    \ for the local status and error reporting service is  for\n   administration\
    \  purposes.   Using  the  service, an administrator can\n   gather information\
    \ such as described above for  each open connection.\n   In addition, statistics\
    \ concerning the transport entity as a whole\n   can be obtained, such as number\
    \ of transport connections open,\n   average number of connections open over a\
    \  given  reporting  period,\n   buffer  use statistics, and total number of retransmitted\
    \ DT TPDUs.\n   The administrator might also be given the  authority  to  cancel\n\
    \   connections,  restart  the  entity,  or  manually  set timer values.\n"
- title: 4   Entity resource management.
  contents:
  - '4   Entity resource management.

    '
- title: 4.1   CPU management.
  contents:
  - "4.1   CPU management.\n   The formal description has implicit scheduling of TPM\
    \ modules, due to\n   the semantics of the Estelle structuring principles.  However,\
    \ the\n   implementor should not depend on this scheduling to obtain optimal\n\
    \   behavior, since, as stated in Part 1, the structures in the formal\n   description\
    \ were imposed for purposes other than operational\n   efficiency.\n   Whether\
    \ by design or by default,  every  implementation of the\n   transport protocol\
    \ embodies some decision about allocating the CPU\n   resource among transport\
    \ connections.   The resource may be\n   monolithic, i.e. a single CPU, or it\
    \ may be distributed, as in the\n   example design given in Part 2.3.  In the\
    \ former, there are  two\n   simple techniques  for apportioning CPU processing\
    \ time  among\n   transport  connections.   The first of these,\n   first-come/first-served,\
    \ consists of the transport entity handling\n   user service requests in the order\
    \ in which they arrive.  No\n   attempt  is  made  to  prevent one transport connection\
    \ from using\n   an inordinate amount of the CPU.\n   The second simple technique\
    \ is  round-robin  scheduling of\n   connections.   Under this method, each transport\
    \ connection is\n   serviced in turn.  For  each  connection,  transport processes\
    \ one\n   user service request, if there is one present at the interface,\n  \
    \ before proceeding to the next connection.\n   The quality of service parameters\
    \ provided in the connection request\n   can be used to provide a finer-grained\
    \ strategy for managing the CPU.\n   The CPU could be allocated to connections\
    \ requiring low delay more\n   often while those requiring high throughput would\
    \ be served less\n   often but for longer periods (i.e., several connections requiring\n\
    \   high throughput might be serviced in a concurrent cluster).\n   For example,\
    \ in the service sequence below, let \"T\" represent\n   m > 0 service requests,\
    \ each requiring high throughput, let \"D\"\n   represent one service request\
    \ requiring low delay and let the suffix\n   n = 1,2,3 represent a connection\
    \ identifier, unique only within a\n   particular service requirement type (T,D).\
    \  Thus T1 represents a set\n   of service requests for connection 1 of the service\
    \ requirement type\n   T, and D1 represents a service set (with one member) for\
    \ connection 1\n   of service requirement type D.\n   D1___D2___D3___T1___D1___D2___D3___T2___D1___D2___D3___T1...\n\
    \   If m = 4 in this service sequence, then service set D1 will get\n   worst-case\
    \ service once every seventh service request processed.\n   Service set T1 receives\
    \ service on its four requests only once in\n   fourteen requests processed.\n\
    \   D1___D2___D3___T1___D1___D2___D3___T2___D1___D2___D3___T1...\n   |       \
    \       |    |              |    |              |\n   |  3 requests  |  4 |  \
    \     3      |  4 |       3      |\n   This means that the CPU is allocated to\
    \ T1 29% ( 4/14 ) of the\n   available time, whereas D1 obtains service 14% (\
    \ 1/7 ) of the time,\n   assuming processing requirements for all service requests\
    \ to be\n   equal.  Now assume that, on average, there is a service request\n\
    \   arriving for one out of three of the service requirement type D\n   connections.\
    \  The CPU is then allocated to the T type 40% ( 4/10 )\n   while the D type is\
    \ allocated 10% ( 1/10 ).\n"
- title: 4.2   Buffer management.
  contents:
  - "4.2   Buffer management.\n   Buffers are used as temporary storage areas for\
    \ data on its  way to\n   or arriving from the network.  Decisions must be made\
    \ about buffer\n   management in two areas.  The first is the overall  strategy\
    \  for\n   managing  buffers in a multi-layered protocol environment.  The\n \
    \  second  is  specifically  how  to allocate buffers within the\n   transport\
    \ entity.\n   In the formal description no details of buffer strategy are given,\n\
    \   since such strategy depends so heavily on the implementation\n   environment.\
    \  Only a general mechanism is discussed in the formal\n   description for allocating\
    \ receive credit to a transport connection,\n   without any expression as to how\
    \ this resource is managed.\n   Good buffer management should correlate to the\
    \ traffic presented by\n   the applications using the transport service.  This\
    \ traffic has\n   implications as well for the performance of the protocol. At\
    \ present,\n   the relationship of buffer strategy to optimal service for a given\n\
    \   traffic distribution is not well understood.  Some work has been\n   done,\
    \ however, and the reader is referred to the work of Jeffery\n   Spirn [SPI82,\
    \ SPI83] and to the experiment plan for research by the\n   NBS [HEA85] on the\
    \ effect of application traffic patterns on the\n   performance of Class 4 transport.\n"
- title: 4.2.1   Overall buffer strategy.
  contents:
  - "4.2.1   Overall buffer strategy.\n   Three schemes for management of  buffers\
    \  in  a  multilayered\n   environment  are described here.  These represent a\
    \ spectrum of\n   possibilities available to the implementor.  The first  of these\
    \ is a\n   strictly layered approach in which each entity in the protocol\n  \
    \ hierarchy, as a process, manages its own pool of buffers\n   independently \
    \ of  entities  at  other layers.  One advantage of this\n   approach  is  simplicity;\
    \   it is not necessary for an entity  to\n   coordinate  buffer  usage with a\
    \ resource manager which is serving\n   the needs of numerous  protocol entities.\
    \  Another advantage is\n   modularity.  The interface presented to entities in\
    \ other layers is\n   well  defined; protocol  service  requests and responses\
    \ are passed\n   between layers by value (copying) versus by reference (pointer\n\
    \   copying). In particular, this is a strict interpretation of the OSI\n   reference\
    \ model, IS 7498 [ISO84b], and the protocol entities hide\n   message details\
    \ from each other, simplifying handling at the entity\n   interfaces.\n   The\
    \ single disadvantage to a  strictly  layered  scheme derives  from\n   the  value-passing\
    \  nature  of the interface.  Each time protocol\n   data and control  information\
    \  is  passed from  one layer to another\n   it must be copied from one layer's\
    \ buffers to those of another layer.\n   Copying  between  layers in  a  multi-layered\
    \  environment is\n   expensive and imposes a severe penalty on the performance\
    \ of the\n   communications system, as  well as the computer system on which it\
    \ is\n   running as a whole.\n   The second scheme for managing buffers  among\
    \  multiple protocol\n   layers  is  buffer  sharing.   In  this  approach, buffers\
    \ are a\n   shared resource among multiple protocol  entities; protocol data and\n\
    \   control information contained in the buffers is exchanged by passing\n   a\
    \ buffer pointer, or  reference, rather  than  the  values  as in the\n   strictly\
    \ layered approach  described  above.   The  advantage  to\n   passing buffers\
    \ by reference is that only a small amount of\n   information, the buffer pointer,\
    \ is copied  from  layer  to  layer.\n   The  resulting  performance  is much\
    \ better than that of the strictly\n   layered approach.\n   There are several\
    \ requirements  that  must  be  met  to implement\n   buffer sharing.  First,\
    \ the host system architecture must allow\n   memory sharing among protocol entities\
    \  that are  sharing the\n   buffers.  This can be achieved in a variety of ways:\
    \  multiple\n   protocol entities may be  implemented  as one  process, all sharing\n\
    \   the same process space (e.g., kernel space),  or  the  host  system\n   architecture\
    \  may  allow processes  to  map portions of their address\n   space to common\
    \ buffer areas at some known location in physical\n   memory.\n   A buffer manager\
    \ is another requirement for implementing shared\n   buffers.  The buffer manager\
    \ has the responsibility of providing\n   buffers  to  protocol entities when\
    \ needed from a list of free\n   buffers and recycling used buffers  back into\
    \  the  free  list. The\n   pool may consist of one or more lists, depending on\
    \ the level of\n   control desired.  For example, there  could be separate lists\
    \ of\n   buffers for outgoing and incoming messages.\n   The protocol entities\
    \ must be implemented in such a way as to\n   cooperate with the buffer manager.\
    \  While this appears to be an\n   obvious condition, it has important implications\
    \ for the strategy\n   used by implementors to develop the communications system.\
    \  This\n   cooperation can be described as follows:  an entity at layer N\n \
    \  requests and is allocated a buffer by the manager; each such buffer\n   is\
    \ returned to the manager by some entity at layer N - k (outgoing\n   data) or\
    \ N + k (incoming data).\n   Protocol  entities also must be designed to cooperate\
    \ with each\n   other.  As buffers are allocated and sent towards the  network\
    \  from\n   higher  layers, allowance must be made for protocol control\n   information\
    \ to be added at lower layers.  This usually means\n   allocating  oversized buffers\
    \ to allow space for headers to be\n   prepended at lower layers.  Similarly,\
    \ as buffers move upward from\n   the network, each protocol entity processes\
    \ its headers before\n   passing the buffer on.  These  manipulations  can  be\
    \ handled by\n   managing pointers into the buffer header space.\n   In their\
    \ pure forms, both strictly layered  and  shared buffer\n   schemes are not practical.\
    \  In the former, there is a performance\n   penalty for copying buffers.  On\
    \ the other hand, it  is not practical\n   to implement buffers that are shared\
    \ by entities in all layers of the\n   protocol hierarchy: the  lower protocol\
    \ layers (OSI layers 1 - 4)\n   have essentially static buffer requirements, whereas\
    \ the upper\n   protocol layers (OSI layers 5 - 7) tend to be dynamic in their\
    \ buffer\n   requirements.  That is, several different applications may be running\n\
    \   concurrently, with buffer requirements varying as the set of\n   applications\
    \ varies.  However, at the transport layer, this latter\n   variation is not visible\
    \ and variations in buffer requirements will\n   depend more on service quality\
    \ considerations than on the specific\n   nature of the applications being served.\
    \  This suggests a hybrid\n   scheme in which the entities in OSI layers 1 - 4\
    \ share buffers while\n   the entities in each of the OSI layers 5 - 7 share in\
    \ a buffer pool\n   associated with each layer.  This approach provides most of\
    \ the\n   efficiency of a pure shared buffer scheme and allows for simple,\n \
    \  modular interfaces where they are most appropriate.\n"
- title: 4.2.2   Buffer management in the transport entity.
  contents:
  - "4.2.2   Buffer management in the transport entity.\n   Buffers are allocated\
    \ in the transport entity  for  two purposes:\n   sending and receiving data.\
    \  For sending data, the decision of how\n   much buffer space to allocate is\
    \  relatively simple;  enough  space\n   should be allocated for outgoing data\
    \ to hold the maximum number of\n   data messages that the  entity will have outstanding\
    \ (i.e., sent but\n   unacknowledged) at any time.  The send buffer space is determined\
    \  by\n   one  of  two values,  whichever  is lower:  the send credit received\n\
    \   from the receiving transport entity, or a maximum  value  imposed by\n   the\
    \  local  implementation,  based  on  such  factors as overall\n   buffer capacity.\n\
    \   The allocation of receive buffers is a more interesting problem\n   because\
    \  it is directly related to the credit value transmitted the\n   peer transport\
    \ entity in CR (or CC) and AK TPDUs.  If the total\n   credit offered to the peer\
    \ entity exceeds the total available buffer\n   space and credit reduction  is\
    \  not  implemented, deadlock  may\n   occur, causing termination of one or more\
    \ transport connections.  For\n   the purposes of  this discussion,  offered \
    \ credit  is assumed to be\n   equivalent to available buffer space.\n   The simplest\
    \ scheme for receive buffer  allocation  is allocation of\n   a fixed amount per\
    \ transport connection.  This amount is allocated\n   regardless of how the connection\
    \  is  to be  used.   This  scheme is\n   fair in that all connections are treated\
    \ equally.  The implementation\n   approach in Part 2.3, in which each transport\
    \ connection is handled\n   by a physically separate processor, obviously could\
    \ use this scheme,\n   since the allocation would be in the form of memory chips\
    \ assigned by\n   the system designer when the system is built.\n   A more flexible\
    \ method  of  allocating  receive  buffer space  is\n   based  on the connection\
    \ quality of service (QOS) requested by the\n   user.  For instance, a QOS indicating\
    \  high throughput would be given\n   more send and receive buffer space than\
    \ one a QOS indicating low\n   delay.  Similarly, connection priority can  be\
    \  used  to  determine\n   send and receive buffer allocation, with important\
    \ (i.e., high\n   priority) connections  allocated  more buffer space.\n   A slightly\
    \ more complex scheme is to apportion send and receive\n   buffer  space using\
    \ both QOS and priority.  For each connection, QOS\n   indicates a general category\
    \ of  operation  (e.g., high throughput or\n   low delay).  Within the general\
    \ category, priority determines the\n   specific  amount  of  buffer  space allocated\
    \  from  a range of\n   possible values.  The general categories may well overlap,\
    \ resulting,\n   for example, in a high priority connection with low throughput\n\
    \   requirements being allocated more buffer space than low priority\n   connection\
    \ requiring a high throughput.\n"
- title: 5   Management of Transport service endpoints.
  contents:
  - "5   Management of Transport service endpoints.\n   As mentioned in Part 1.2.1.1,\
    \ a transport entity needs some way of\n   referencing a transport connection\
    \ endpoint within the end system: a\n   TCEP_id.  There are several factors influencing\
    \ the management of\n   TCEP_ids:\n    1)  IPC mechanism between the transport\
    \ entity and the session\n        entity (Part 3.3);\n    2)  transport entity\
    \ resources and resource management (Part 4);\n    3)  number of distinct TSAPs\
    \ supported by the entity (Part 1.2.2.1);\n        and\n    4)  user process rendezvous\
    \ mechanism (the means by which session\n        processes identify themselves\
    \ to the transport entity, at a\n        given TSAP, for association with a transport\
    \ connection);\n   The IPC mechanism and the user process rendezvous mechanism\
    \ have more\n   direct influence than the other two factors on how the TCEP_id\n\
    \   management is implemented.\n   The number of TCEP_ids available should reflect\
    \ the resources that\n   are available to the transport entity, since each TCEP_id\
    \ in use\n   represents a potential transport connection.  The formal description\n\
    \   assumes that there is a function in the TPE which can decide, on the\n   basis\
    \ of current resource availability, whether or not to issue a\n   TCEP_id for\
    \ any connection request received.  If the TCEP_id is\n   issued, then resources\
    \ are allocated for the connection endpoint.\n   However, there is a somewhat\
    \ different problem for the users of\n   transport.  Here, the transport entity\
    \ must somehow inform the\n   session entity as to the TCEP_ids available at a\
    \ given TSAP.\n   In the formal description, a T-CONNECT-request is permitted\
    \ to enter\n   at any TSAP/TCEP_id.  A function in the TPE considers whether or\
    \ not\n   resources are availble to support the requested connection.  There is\n\
    \   also a function which checks to see if a TSAP/TCEP_id is busy by\n   seeing\
    \ if there is a TPM allocated to it.  But this function is not\n   useful to the\
    \ session entity which does not have access to the\n   transport entity's operations.\
    \  This description of the procedure is\n   clearly too loose for an implementation.\n\
    \   One solution to this problem is to provide a new (abstract) service,\n   T-REGISTER,\
    \ locally, at the interface between transport and session.\n   ___________________________________________________________________\n\
    \   |           Primitives                       Parameters           |\n   |_________________________________________________________________|\n\
    \   |  T-REGISTER        request     |  Session process  identifier   |\n   |________________________________|________________________________|\n\
    \   |  T-REGISTER        indication  |  Transport endpoint identifier,|\n   |\
    \                                |  Session process  identifier   |\n   |________________________________|________________________________|\n\
    \   |  T-REGISTER        refusal     |  Session process  identifier   |\n   |________________________________|________________________________|\n\
    \   This service is used as follows:\n     1)   A session process is identified\
    \ to the transport entity by a\n          T-REGISTER-request event.  If a TCEP_id\
    \ is available,  the\n          transport entity selects a TCEP_id and places\
    \ it into a table\n          corresponding to the TSAP at which the T-REGISTER-request\n\
    \          event occurred, along with the session process identifier. The\n  \
    \        TCEP_id and the session process identifier are then\n          transmitted\
    \ to the session entity by means of the T-REGISTER-\n          indication event.\
    \ If no TCEP_id is available, then a T-\n          REGISTER-refusal event carrying\
    \ the session process identifier\n          is returned.  At any time that an\
    \ assigned TCEP_id is not\n          associated with an active transport connection\
    \ process\n          (allocated TPM), the transport entity can issue a T-REGISTER-\n\
    \          refusal to the session entity to indicate, for example, that\n    \
    \      resources are no longer available to support a connection,\n          since\
    \ TC resources are not allocated at registration time.\n     2)   If the session\
    \ entity is to initiate the transport connection,\n          it issues a T-CONNECT-request\
    \ with the TCEP_id as a parameter.\n          (Note that this procedure is at\
    \ a slight variance to the\n          procedure in N3756, which specifies no such\
    \ parameter, due to\n          the requirement of alignment of the formal description\
    \ with\n          the service description of transport and the definition of the\n\
    \          session protocol.) If the session entity is expecting a\n         \
    \ connection request from a remote peer at this TSAP, then the\n          transport\
    \ does nothing with the TCEP_id until a CR TPDU\n          addressed to the TSAP\
    \ arrives.  When such a CR TPDU arrives,\n          the transport entity issues\
    \ a T-CONNECT-indication to the\n          session entity with a TCEP_id as a\
    \ parameter.  As a management\n          aid, the table entry for the TCEP_id\
    \ can be marked \"busy\" when\n          the TCEP_id is associated with an allocated\
    \ TPM.\n     3)   If a CR TPDU is received and no TCEP_id is in the table for\n\
    \          the TSAP addressed, then the transport selects a TCEP_id,\n       \
    \   includes it as a parameter in the T-CONNECT-indication sent to\n         \
    \ the session entity, and places it in the table. The T-\n          CONNECT-response\
    \ returned by the session entity will carry the\n          TCEP_id and the session\
    \ process identifier.  If the session\n          process identifier is already\
    \ in the table, the new one is\n          discarded; otherwise it is placed into\
    \ the table. This\n          procedure is also followed if the table has entries\
    \ but they\n          are all marked busy or are empty.  If the table is full\
    \ and\n          all entries ar marked busy, then the transport entity\n     \
    \     transmits a DR TPDU to the peer transport entity to indicate\n         \
    \ that the connection cannot be made.  Note that the transport\n          entity\
    \ can disable a TSAP by marking all its table entries\n          busy.\n   The\
    \ realization of the T-REGISTER service will depend on the IPC\n   mechanisms\
    \ available between the transport and session entities. The\n   problem of user\
    \ process rendezvous is solved in general by the T-\n   REGISTER service, which\
    \ is based on a solution proposed by Michael\n   Chernik of the NBS [CHK85].\n"
- title: 6   Management of Network service endpoints in Transport.
  contents:
  - '6   Management of Network service endpoints in Transport.

    '
- title: 6.1   Endpoint identification.
  contents:
  - "6.1   Endpoint identification.\n   The identification of endpoints at an NSAP\
    \ is different from that for\n   the TSAP.  The nature of the services at distinct\
    \ TSAPs is\n   fundamentally the same, although the quality could vary, as a local\n\
    \   choice.  However, it is possible for distinct NSAPs to represent\n   access\
    \ to essentially different network services.  For example, one\n   NSAP may provide\
    \ access to a connectionless network service by means\n   of an internetwork protocol.\
    \  Another NSAP may provide access to a\n   connection-oriented service, for use\
    \ in communicating on a local\n   subnetwork.  It is also possible to have several\
    \ distinct NSAPs on\n   the same subnetwork, each of which provides some service\
    \ features of\n   local interest that distinguishes it from the other NSAPs.\n\
    \   A transport entity accessing an X.25 service could use the logical\n   channel\
    \ numbers for the virtual circuits as NCEP_ids.  An NSAP\n   providing access\
    \ only to a permanent virtual circuit would need only\n   a single NCEP_id to\
    \ multiplex the transport connections.  Similarly,\n   a CSMA/CD network would\
    \ need only a single NCEP_id, although the\n   network is connectionless.\n"
- title: 6.2   Management issues.
  contents:
  - "6.2   Management issues.\n   The Class 4 transport protocol has been succesfully\
    \ operated over\n   both connectionless and connection-oriented network services.\
    \  In\n   both modes of operation there exists some information about the\n  \
    \ network service that a transport implementation could make use of to\n   enhance\
    \ performance.  For example, knowledge of expected delay to a\n   destination\
    \ would permit optimal selection of retransmission timer\n   value for a connection\
    \ instance.  The information that transport\n   implementations could use and\
    \ the mechanisms for obtaining and\n   managing that information are, as a group,\
    \ not well understood.\n   Projects are underway within ISO committees to address\
    \ the management\n   of OSI as an architecture and the management of the transport\
    \ layer\n   as a layer.\n   For operation of the Class 4 transport protocol over\n\
    \   connection-oriented network service several issues must be addressed\n   including:\n\
    \     a.   When should a new network connection be opened to support a\n     \
    \     transport connection (versus multiplexing)?\n     b.   When a network connection\
    \ is no longer being used by any\n          transport connection, should the network\
    \ connection be closed\n          or remain open awaiting a new transport connection?\n\
    \     c.   When a network connection is aborted, how should the peer\n       \
    \   transport entities that were using the connection cooperate to\n         \
    \ re-establish it?  If splitting is not to be used, how can this\n          re-establishment\
    \ be achieved such that one and only one\n          network connection results?\n\
    \   The Class 4 transport specification permits a transport entity to\n   multiplex\
    \ several transport connections (TCs) over a single network\n   connection (NC)\
    \ and to split a single TC across several NCs.  The\n   implementor must decide\
    \ whether to support these options and, if so,\n   how.  Even when the implementor\
    \ decides never to initiate splitting\n   or multiplexing the transport entity\
    \ must be prepared to accept this\n   behavior from other transport implementations.\
    \  When multiplexing is\n   used TPDUs from multiple TCs can be concatenated into\
    \ a single\n   network service data unit (NSDU).  Therefore, damage to an NSDU\
    \ may\n   effect several TCs.  In general, Class 2 connections should not be\n\
    \   multiplexed with Class 4 connections.  The reason for this is that if\n  \
    \ the error rate on the network connection is high enough that the\n   error recovery\
    \ capability of Class 4 is needed, then it is too high\n   for Class 2 operation.\
    \  The deciding criterion is the tolerance of\n   the user for frequent disconnection\
    \ and data errors.\n   Several issues in splitting must be considered:\n    1)\
    \ maximum number of NCs that can be assigned to a given TC;\n    2) minimum number\
    \ of NCs required by a TC to maintain the \"quality\n       of service\" expected\
    \ (default of 1);\n    3) when to split;\n    4) inactivity control;\n    5) assignment\
    \ of received TPDU to TC; and\n    6) notification to TC of NC status (assigned,\
    \ dissociated, etc ).\n   All of these except 3) are covered in the formal description.\
    \  The\n   methods used in the formal description need not be used explicitly,\n\
    \   but they suggest approaches to implementation.\n   To support the possibility\
    \ of multiplexing and splitting the\n   implementor must provide a common function\
    \ below the TC state\n   machines that maps a set of TCs to a set of NCs.  The\
    \ formal\n   description provides a general means of doing this, requiring mainly\n\
    \   implementation environment details to complete the mechanism.\n   Decisions\
    \ about when network connections are to be opened or closed\n   can be made locally\
    \ using local decision criteria.  Factors that may\n   effect the decision include\
    \ costs of establishing an NC, costs of\n   maintaining an open NC with little\
    \ traffic flowing, and estimates of\n   the probability of data flow between the\
    \ source node and known\n   destinations.  Management of this type is feasible\
    \ when a priori\n   knowledge exists but is very difficult when a need exists\
    \ to adapt to\n   dynamic traffic patterns and/or fluctuating network charging\n\
    \   mechanisms.\n   To handle the issue of re-establishment of the NC after failure,\
    \ the\n   ISO has proposed an addendum N3279 [ISO85c] to the basic transport\n\
    \   standard describing a network connection management subprotocol\n   (NCMS)\
    \ to be used in conjunction with the transport protocol.\n"
- title: 7   Enhanced checksum algorithm.
  contents:
  - '7   Enhanced checksum algorithm.

    '
- title: 7.1   Effect of checksum on transport performance.
  contents:
  - "7.1   Effect of checksum on transport performance.\n   Performance experiments\
    \ with Class 4 transport at the NBS have\n   revealed that straightforward implementation\
    \ of the Fletcher checksum\n   using the algorithm recommended in the ISO transport\
    \ standard leads\n   to severe reduction of transport throughput.  Early modeling\n\
    \   indicated throughput drops of as much as 66% when using the checksum.\n  \
    \ Work by Anastase Nakassis [NAK85] of the NBS led to several improved\n   implementations.\
    \  The performance degradation due to checksum is now\n   in the range of 40-55%,\
    \ when using the improved implementations.\n   It is possible that transport may\
    \ be used over a network that does\n   not provide error detection.  In such a\
    \ case the transport checksum\n   is necessary to ensure data integrity. In many\
    \ instances, the\n   underlying subnetwork provides some error checking mechanism.\
    \  The\n   HDLC frame check sequence as used by X.25, IEEE 802.3 and 802.4 rely\n\
    \   on a 32 bit cyclic redundancy check and satellite link hardware\n   frequently\
    \ provides the HDLC frame check sequence.  However, these\n   are all link or\
    \ physical layer error detection mechanisms which\n   operate only point-to-point\
    \ and not end-to-end as the transport\n   checksum does.  Some links provide error\
    \ recovery while other links\n   simply discard damaged messages.  If adequate\
    \ error recovery is\n   provided, then the transport checksum is extra overhead,\
    \ since\n   transport will detect when the link mechanism has discarded a message\n\
    \   and will retransmit the message.  Even when the IP fragments the\n   TPDU,\
    \ the receiving IP will discover a hole in the reassembly buffer\n   and discard\
    \ the partially assembled datagram (i.e., TPDU).  Transport\n   will detect this\
    \ missing TPDU and recover by means of the\n   retransmission mechanism.\n"
- title: 7.2   Enhanced algorithm.
  contents:
  - "7.2   Enhanced algorithm.\n   The Fletcher checksum algorithm given in an annex\
    \ to IS 8073 is not\n   part of the standard, and is included in the annex as\
    \ a suggestion to\n   implementors.  This was done so that as improvements or\
    \ new\n   algorithms came along, they could be incorporated without the\n   necessity\
    \ to change the standard.\n   Nakassis has provided three ways of coding the algorithm,\
    \ shown\n   below, to provide implementors with insight rather than universally\n\
    \   transportable code.  One version uses a high order language (C).  A\n   second\
    \ version uses C and VAX assembler, while a third uses only VAX\n   assembler.\
    \  In all the versions, the constant MODX appears.  This\n   represents the maximum\
    \ number of sums that can be taken without\n   experiencing overflow.  This constant\
    \ depends on the processor's word\n   size and the arithmetic mode, as follows:\n\
    \    Choose n such that\n     (n+1)*(254 + 255*n/2) <= 2**N - 1\n   where N is\
    \ the number of usable bits for signed (unsigned)\n   arithmetic.  Nakassis shows\
    \ [NAK85] that it is sufficient\n   to take\n     n <= sqrt( 2*(2**N - 1)/255\
    \ )\n   and that n = sqrt( 2*(2**N - 1)/255 ) - 2 generally yields\n   usable\
    \ values.  The constant MODX then is taken to be n.\n   Some typical values for\
    \ MODX are given in the following table.\n    BITS/WORD                MODX  \
    \        ARITHMETIC\n        15                     14             signed\n  \
    \      16                     21           unsigned\n        31              \
    \     4102             signed\n        32                   5802           unsigned\n\
    \   This constant is used to reduce the number of times mod 255 addition\n   is\
    \ invoked, by way of speeding up the algorithm.\n   It should be noted that it\
    \ is also possible to implement the checksum\n   in separate hardware.  However,\
    \ because of the placement of the\n   checksum within the TPDU header rather than\
    \ at the end of the TPDU,\n   implementing this with registers and an adder will\
    \ require\n   significant associated logic to access and process each octet of\
    \ the\n   TPDU and to move the checksum octets in to the proper positions in the\n\
    \   TPDU. An alternative to designing this supporting logic is to use a\n   fast,\
    \ microcoded 8-bit CPU to handle this access and the computation.\n   Although\
    \ there is some speed penalty over separate logic, savings\n   may be realized\
    \ through a reduced chip count and development time.\n"
- title: 7.2.1   C language algorithm.
  contents:
  - "7.2.1   C language algorithm.\n   #define MODX 4102\n     encodecc( mess,len,k\
    \ )\n     unsigned char mess[] ;    /* the TPDU to be checksummed */\n     int\
    \      len,\n              k;               /* position of first checksum octet\n\
    \                                  as an offset from mess[0]  */\n     { int ip,\n\
    \           iq,\n           ir,\n           c0,\n           c1;\n       unsigned\
    \ char *p,*p1,*p2,*p3 ;\n       p = mess ; p3 = mess + len ;\n       if ( k >\
    \ 0) { mess[k-1] = 0x00 ; mess[k] = 0x00 ; }\n            /* insert zeros for\
    \ checksum octets */\n       c0 = 0 ; c1 = 0  ; p1 = mess ;\n       while (p1\
    \ < p3)    /* outer sum accumulation loop */\n       {\n        p2 = p1 + MODX\
    \ ; if (p2 > p3) p2 = p3 ;\n        for (p = p1 ; p < p2 ; p++) /*  inner sum\
    \ accumulation loop */\n        { c0 = c0 + (*p) ; c1 = c1 + c0 ;\n        }\n\
    \        c0 = c0%255 ; c1 = c1%255 ; p1 = p2 ;\n            /* adjust accumulated\
    \ sums to mod 255 */\n        }\n        ip = (c1 << 8) + c0 ;     /* concatenate\
    \ c1 and c0 */\n        if (k > 0)\n        {     /* compute and insert checksum\
    \ octets */\n         iq = ((len-k)*c0 - c1)%255 ; if (iq <= 0) iq = iq + 255\
    \ ;\n         mess[k-1] = iq ;\n         ir = (510 - c0 - iq) ;\n         if (ir\
    \ > 255) ir = ir - 255 ; mess[k] = ir ;\n       }\n       return(ip) ;\n     }\n"
- title: 7.2.2   C/assembler algorithm.
  contents:
  - "7.2.2   C/assembler algorithm.\n   #include <math>\n     encodecm(mess,len,k)\n\
    \     unsigned char *mess ;\n     int      len,k      ;\n     {\n       int i,ip,c0,c1\
    \ ;\n       if (k > 0) { mess[k-1] = 0x00 ; mess[k] = 0x00 ; }\n       ip = optm1(mess,len,&c0,&c1)\
    \ ;\n       if (k > 0)\n       { i = ( (len-k)*c0 - c1)%255 ; if (i <= 0) i =\
    \ i + 255 ;\n         mess[k-1] = i ;\n         i = (510 - c0 - i) ; if (i > 255)\
    \ i = i - 255 ;\n         mess[k] = i ;\n       }\n       return(ip) ;\n     }\n\
    \    ;       calling sequence optm(message,length,&c0,&c1) where\n    ;      \
    \ message is an array of bytes\n    ;       length   is the length of the array\n\
    \    ;       &c0 and &c1 are the addresses of the counters to hold the\n    ;\
    \       remainder of; the first and second order partial sums\n    ;       mod(255).\n\
    \            .ENTRY   optm1,^M<r2,r3,r4,r5,r6,r7,r8,r9,r10,r11>\n            movl\
    \     4(ap),r8      ; r8---> message\n            movl     8(ap),r9      ; r9=length\n\
    \            clrq     r4            ; r5=r4=0\n            clrq     r6       \
    \     ; r7=r6=0\n            clrl     r3            ; clear high order bytes of\
    \ r3\n            movl     #255,r10      ; r10 holds the value 255\n         \
    \   movl     #4102,r11     ; r11= MODX\n    xloop:  movl     r11,r7        ; if\
    \ r7=MODX\n            cmpl     r9,r7         ; is r9>=r7 ?\n            bgeq\
    \     yloop         ; if yes, go and execute the inner\n                     \
    \              ; loop MODX times.\n            movl     r9,r7         ; otherwise\
    \ set r7, the inner loop\n                                   ; counter,\n    yloop:\
    \  movb     (r8)+,r3      ;\n            addl2    r3,r4         ; sum1=sum1+byte\n\
    \            addl2    r4,r6         ; sum2=sum2+sum1\n            sobgtr   r7,yloop\
    \      ; while r7>0 return to iloop\n                              ; for mod 255\
    \ addition\n      ediv     r10,r6,r0,r6  ; r6=remainder\n      ediv     r10,r4,r0,r4\
    \  ;\n      subl2    r11,r9        ; adjust r9\n      bgtr     xloop         ;\
    \ go for another loop if necessary\n      movl     r4,@12(ap)    ; first argument\n\
    \      movl     r6,@16(ap)    ; second argument\n      ashl     #8,r6,r0     \
    \ ;\n      addl2    r4,r0         ;\n      ret\n"
- title: 7.2.3  Assembler algorithm.
  contents:
  - "7.2.3  Assembler algorithm.\n   buff0:  .blkb   3              ; allocate 3 bytes\
    \ so that aloop is\n                          ; optimally aligned\n   ;      \
    \ macro implementation of Fletcher's algorithm.\n   ;       calling sequence ip=encodemm(message,length,k)\
    \ where\n   ;       message is an array of bytes\n   ;       length   is the length\
    \ of the array\n   ;       k        is the location of the check octets if >0,\n\
    \   ;                an indication not to encode if 0.\n   ;\n   movl     4(ap),r8\
    \      ; r8---> message\n   movl     8(ap),r9      ; r9=length\n   clrq     r4\
    \            ; r5=r4=0\n   clrq     r6            ; r7=r6=0\n   clrl     r3  \
    \          ; clear high order bytes of r3\n   movl     #255,r10      ; r10 holds\
    \ the value 255\n   movl     12(ap),r2     ; r2=k\n   bleq     bloop         ;\
    \ if r2<=0, we do not encode\n   subl3    r2,r9,r11     ; set r11=L-k\n   addl2\
    \    r8,r2         ; r2---> octet k+1\n   clrb     (r2)          ; clear check\
    \ octet k+1\n   clrb     -(r2)         ; clear check octet k, r2---> octet k.\n\
    \   bloop:  movw     #4102,r7   ; set r7 (inner loop counter) = to MODX\n   cmpl\
    \     r9,r7         ; if r9>=MODX, then go directly to adjust r9\n   bgeq    \
    \ aloop         ; and execute the inner loop MODX times.\n   movl     r9,r7  \
    \       ; otherwise set r7, the inner loop counter,\n                        \
    \  ; equal to r9, the number of the\n                          ; unprocessed characters\n\
    \   aloop:  movb     (r8)+,r3      ;\n   addl2    r3,r4         ; c0=c0+byte\n\
    \   addl2    r4,r6         ; sum2=sum2+sum1\n   sobgtr   r7,aloop      ; while\
    \ r7>0 return to iloop\n                                  ; for mod 255 addition\n\
    \   ediv     r10,r6,r0,r6  ; r6=remainder\n   ediv     r10,r4,r0,r4  ;\n   subl2\
    \    #4102,r9      ;\n   bgtr     bloop         ; go for another loop if necessary\n\
    \   ashl     #8,r6,r0      ; r0=256*r6\n   addl2    r4,r0         ; r0=256*r6+r4\n\
    \   cmpl     r2,r7         ; since r7=0, we are checking if r2 is\n   bleq   \
    \  exit          ; zero or less: if yes we bypass\n                          \
    \        ; the encoding.\n   movl     r6,r8         ; r8=c1\n   mull3    r11,r4,r6\
    \     ; r6=(L-k)*c0\n   ediv     r10,r6,r7,r6  ; r6 = (L-k)*c0 mod(255)\n   subl2\
    \    r8,r6         ; r6= ((L-k)*c0)%255 -c1 and if negative,\n   bgtr     byte1\
    \         ; we must\n   addl2    r10,r6        ; add 255\n   byte1:  movb    \
    \ r6,(r2)+ ; save the octet and let r2---> octet k+1\n   addl2    r6,r4      \
    \   ; r4=r4+r6=(x+c0)\n   subl3    r4,r10,r4     ; r4=255-(x+c0)\n   bgtr    \
    \ byte2         ; if >0 r4=octet (k+1)\n   addl2    r10,r4        ; r4=255+r4\n\
    \   byte2:  movb     r4,(r2)       ; save y in octet k+1\n   exit:   ret\n"
- title: 8   Parameter selection.
  contents:
  - '8   Parameter selection.

    '
- title: 8.1   Connection control.
  contents:
  - "8.1   Connection control.\n   Expressions for timer values used to control the\
    \ general transport\n   connection behavior are given in IS 8073.  However, values\
    \ for the\n   specific factors in the expressions are not given and the expressions\n\
    \   are only estimates.  The derivation of timer values from these\n   expressions\
    \ is not mandatory in the standard.  The timer value\n   expressions in IS 8073\
    \ are for a connection-oriented network service\n   and may not apply to a connectionless\
    \ network service.\n   The following symbols are used to denote factors contributing\
    \ to\n   timer values, throughout the remainder of this Part.\n    Elr = expected\
    \ maximum transit delay, local to remote\n    Erl = expected maximum transit delay,\
    \ remote to local\n    Ar  = time needed by remote entity to generate an acknowledgement\n\
    \    Al  = time needed by local entity to generate an acknowledgement\n    x \
    \  = local processing time for an incoming TPDU\n    Mlr = maximum NSDU lifetime,\
    \ local to remote\n    Mrl = maximum NSDU lifetime, remote to local\n    T1  =\
    \ bound for maximum time local entity will wait for\n          acknowledgement\
    \ before retransmitting a TPDU\n    R   = bound for maximum local entity will\
    \ continue to transmit a\n          TPDU that requires acknowledgment\n    N \
    \  = bound for maximum number of times local entity  will transmit\n         \
    \ a TPDU requiring acknowledgement\n    L   = bound for the maximum time between\
    \ the transmission of a\n          TPDU and the receipt of any acknowledgment\
    \ relating to it.\n    I   = bound for the time after which an entity will initiate\n\
    \          procedures to terminate a transport connection if a TPDU is\n     \
    \     not received from the peer entity\n    W   = bound for the maximum time\
    \ an entity will wait before\n          transmitting up-to-date window information\n\
    \   These symbols and their definitions correspond to those given in\n   Clause\
    \ 12 of IS 8073.\n"
- title: 8.1.1   Give-up timer.
  contents:
  - "8.1.1   Give-up timer.\n   The give-up timer determines the  amount  of  time\
    \  the transport\n   entity  will continue to await an acknowledgement (or other\n\
    \   appropriate reply) of a transmitted message  after the  message\n   has  been\
    \  retransmitted the maximum number of times.    The\n   recommendation given\
    \ in IS 8073 for values of this timer is\n   expressed by\n    T1 + W + Mrl, for\
    \ DT and ED TPDUs\n    T1 + Mrl, for CR, CC, and DR TPDUs,\n   where\n    T1 =\
    \ Elr + Erl + Ar + x.\n   However, it should be noted that Ar will not be known\
    \ for either the\n   CR or the CC TPDU, and that Elr and Erl may vary considerably\
    \ due to\n   routing in some conectionless network services.  In Part 8.3.1, the\n\
    \   determination of values for T1 is discussed in more detail.  Values\n   for\
    \ Mrl generally are relatively fixed for a given network service.\n   Since Mrl\
    \ is usually much larger than expected values of T1, a\n   rule-of-thumb for the\
    \ give-up timer value is 2*Mrl + Al + x for the\n   CR, CC and DR TPDUs and 2*Mrl\
    \ + W for DT and ED TPDUs.\n"
- title: 8.1.2   Inactivity timer.
  contents:
  - "8.1.2   Inactivity timer.\n   This timer measures  the  maximum  time  period\
    \  during which a\n   transport connection can be inactive, i.e., the maximum\
    \ time an\n   entity can wait without receiving incoming messages.  A usable value\n\
    \   for the inactivity timer is\n    I = 2*( max( T1,W )*N ).\n   This accounts\
    \ for the possibility that the remote peer is using a\n   window timer value different\
    \ from that of the local peer.  Note that\n   an inactivity timer is important\
    \ for operation over connectionless\n   network services, since the periodic receipt\
    \ of AK TPDUs is the only\n   way that the local entity can be certain that its\
    \ peer is still\n   functioning.\n"
- title: 8.1.3   Window timer.
  contents:
  - "8.1.3   Window timer.\n   The window timer has two purposes.  It is used to assure\
    \ that the\n   remote peer entity periodically receives the current state of the\n\
    \   local entity's flow control, and it ensures that the remote peer\n   entity\
    \ is aware that the local entity is still functioning.  The\n   first purpose\
    \ is necessary to place an upper bound on the time\n   necessary to resynchronize\
    \ the flow control should an AK TPDU which\n   notifies the remote peer of increases\
    \ in credit be lost.  The second\n   purpose is necessary to prevent the inactivity\
    \ timer of the remote\n   peerfrom expiring.  The value for the window timer,\
    \ W, depends on\n   several factors, among which are the transit delay, the\n\
    \   acknowledgement strategy, and the probability of TPDU loss in the\n   network.\
    \  Generally, W should satisfy the following condition:\n     W > C*(Erl + x)\n\
    \   where C is the maximum amount of credit offered.  The rationale for\n   this\
    \ condition is that the right-hand side represents the maximum\n   time for receiving\
    \ the entire window.  The protocol requires that all\n   data received be acknowledged\
    \ when the upper edge of the window is\n   seen as a sequence number in a received\
    \ DT TPDU.  Since the window\n   timer is reset each time an AK TPDU is transmitted,\
    \ there is usually\n   no need to set the timer to any less than the value on\
    \ the right-hand\n   side of the condition.  An exception is when both C and the\
    \ maximum\n   TPDU size are large, and Erl is large.\n   When the probability\
    \ that a TPDU will be lost is small, the value of\n   W can be quite large, on\
    \ the order of several minutes.  However, this\n   increases the delay the peer\
    \ entity will experience in detecting the\n   deactivation of the local transport\
    \ entity.  Thus, the value of W\n   should be given some consideration in terms\
    \ of how soon the peer\n   entity needs to detect inactivity.  This could be done\
    \ by placing\n   such information into a quality of service record associated\
    \ with the\n   peer's address.\n   When the expected network error rate is high,\
    \ it may be necessary to\n   reduce the value of W to ensure that AK TPDUs are\
    \ being received by\n   the remote entity, especially when both entities are quiescent\
    \ for\n   some period of time.\n"
- title: 8.1.4   Reference timer.
  contents:
  - "8.1.4   Reference timer.\n   The reference timer measures  the  time  period\
    \  during which a\n   source reference must not be reassigned to another transport\n\
    \   connection, in order that spurious duplicate  messages not\n   interfere \
    \ with a new connection.  The value for this timer\n   given in IS 8073 is\n \
    \   L = Mlr + Mrl + R + Ar\n   where\n    R = T1*N + z\n   in which z is a small\
    \ tolerance quantity to allow for factors\n   internal to the entity.  The use\
    \ of L as a bound, however, must be\n   considered carefully.  In some cases,\
    \ L may be very large, and not\n   realistic as an upper or a lower bound.  Such\
    \ cases may be\n   encountered on routes over several catenated networks where\
    \ R is set\n   high to provide adequate recovery from TPDU loss.  In other cases\
    \ L\n   may be very small, as when transmission is carried out over a LAN and\n\
    \   R is set small due to low probability of TPDU loss.  When L is\n   computed\
    \ to be very small, the reference need not be timed out at\n   all, since the\
    \ probability of interference is zero.  On the other\n   hand, if L is computed\
    \ to be very large a smaller value can be used.\n   One choice for the value \
    \ might be\n    L = min( R,(Mrl + Mlr)/2 )\n   If the reference number assigned\
    \ to  a  new  connection  by  an\n   entity  is monotonically incremented for\
    \ each new connection through\n   the entire available reference space (maximum\
    \ 2**16 - 1), the timer\n   is not critical: the sequence space is large enough\
    \ that it is likely\n   that there will be no spurious messages in  the network\
    \ by the time\n   reference numbers are reused.\n"
- title: 8.2   Flow control.
  contents:
  - "8.2   Flow control.\n   The peer-to-peer flow control mechanism  in  the  transport\
    \ protocol\n   determines  the  upper bound on the pace of data exchange that\
    \ occurs\n   on  transport  connections.   The transport  entity  at  each end\
    \ of\n   a connection offers a credit to its peer representing the number of\n\
    \   data  messages it  is  currently willing to accept.  All received\n   data\
    \ messages are acknowledged,  with  the  acknowledgement  message\n   containing\
    \  the  current  receive  credit  information.  The three\n   credit allocation\
    \ schemes discussed  below  present  a diverse  set\n   of  examples  of  how\
    \ one might derive receive credit values.\n"
- title: 8.2.1   Pessimistic credit allocation.
  contents:
  - "8.2.1   Pessimistic credit allocation.\n   Pessimistic credit allocation is perhaps\
    \ the simplest form of flow\n   control.  It is similar in concept to X-on/X-off\
    \ control.  In this\n   method, the receiver always offers a credit of one TPDU.\
    \  When the DT\n   TPDU is received, the receiver responds with an AK TPDU carrying\
    \ a\n   credit of zero.  When the DT TPDU has been processed by the receiving\n\
    \   entity, an additional AK TPDU carrying a credit of one will be sent.\n   The\
    \ advantage to this approach is  that  the data  exchange  is  very\n   tightly\
    \ controlled by the receiving entity.  The disadvantages are:\n   1) the  exchange\
    \  is  slow, every data  message requiring at least\n   the time of two round\
    \ trips to complete the transfer transfer, and 2)\n   the ratio of acknowledgement\
    \ to data messages sent is 2:1.  While not\n   recommeneded, this scheme illustrates\
    \ one extreme method of credit\n   allocation.\n"
- title: 8.2.2   Optimistic credit allocation.
  contents:
  - "8.2.2   Optimistic credit allocation.\n   At the other extreme from pessimistic\
    \ credit allocation is optimistic\n   credit  allocation,  in  which  the  receiver\
    \ offers more credit than\n   it has buffers.  This scheme  has  two  dangers.\
    \  First, if the\n   receiving user is not accepting data at a fast enough rate,\
    \ the\n   receiving transport's  buffers  will  become filled.  Since  the\n \
    \  credit  offered  was optimistic, the sending entity will continue to\n   transmit\
    \ data, which must be dropped  by the receiving entity for\n   lack of buffers.\
    \ Eventually,  the  sender  may  reach  the  maximum\n   number   of retransmissions\
    \ and terminate the connection.\n   The second danger in using optimistic flow\
    \  control  is that the\n   sending entity may transmit faster than the receiving\
    \ entity can\n   consume.  This could result from  the  sender being  implemented\
    \  on\n   a faster machine or being a more efficient implementation.  The\n  \
    \ resultant behavior is essentially the same as described above:\n   receive buffer\
    \ saturation, dropped data messages, and connection\n   termination.\n   The two\
    \ dangers  cited  above  can  be  ameliorated  by implementing\n   the credit\
    \ reduction scheme as specified in the protocol.  However,\n   optimistic credit\
    \ allocation works  well only  in  limited\n   circumstances.   In most situations\
    \ it is inappropriate and\n   inefficient even when using credit reduction.  Rather\
    \  than seeking\n   to avoid congestion, optimistic allocation causes it, in most\
    \ cases,\n   and credit reduction simply allows  one to recover from congestion\n\
    \   once it has happened.  Note that optimistic credit allocation\n   combined\
    \ with caching out-of-sequence messages requires a\n   sophisticated buffer management\
    \ scheme to avoid reasssembly deadlock\n   and subsequent loss of the transport\
    \ connection.\n"
- title: 8.2.3   Buffer-based credit allocation.
  contents:
  - "8.2.3   Buffer-based credit allocation.\n   Basing the receive  credit  offered\
    \  on  the  actual availability  of\n   receive  buffers  is  a  better method\
    \ for achieving flow control.\n   Indeed, with few exceptions, the implementations\
    \ that have been\n   studied used this method.  It continuous  flow  of  data\
    \  and\n   eliminating the need for the credit-restoring  acknowledgements.\n\
    \   Since  only  available buffer space is offered, the dangers of\n   optimistic\
    \ credit allocation are also avoided.\n   The amount of buffer space needed to\
    \  maintain  a  continuous bulk\n   data  transfer,  which represents the maximum\
    \ buffer requirement, is\n   dependent on round trip  delay  and network  speed.\
    \  Generally, one\n   would want the buffer space, and hence the credit, large\
    \ enough to\n   allow  the  sender  to send continuously, so that incremental\
    \ credit\n   updates arrive just prior to the sending entity  exhausting  the\n\
    \   available credit.   One example is a single-hop satellite link\n   operating\
    \ at 1.544  Mbits/sec.   One  report [COL85]  indicates  that\n   the buffer requirement\
    \ necessary for continuous flow is approximately\n   120 Kbytes.  For 10 Mbits/sec.\
    \ IEEE 802.3 and 802.4 LANs, the figure\n   is on the order of 10K to 15K bytes\
    \ [BRI85, INT85, MIL85].\n   An interesting modification to the buffer-based \
    \ credit allocation\n   scheme is suggested by R.K. Jain [JAI85].  Whereas the\
    \ approach\n   described above is based strictly on the available buffer space,\
    \ Jain\n   suggests a scheme in which credit is reduced  voluntarily  by  the\n\
    \   sending  entity  when  network congestion  is  detected.  Congestion\n   is\
    \ implied by the occurrence of retransmissions.  The sending\n   entity,  recognizing\
    \ retransmissions,  reduces  the local value of\n   credit to one, slowly raising\
    \ it to the actual receive credit\n   allocation as error-free transmissions continue\
    \ to occur.  This\n   technique can overcome various types of network congestion\
    \ occurring\n   when a fast sender overruns a slow receiver when no link level\
    \ flow\n   control is available.\n"
- title: 8.2.4   Acknowledgement policies.
  contents:
  - "8.2.4   Acknowledgement policies.\n   It is useful first to  review the four\
    \ uses of the acknowledgement\n   message in Class 4 transport.  An acknowledgement\
    \ message:\n          1) confirms correct receipt of data messages,\n        \
    \  2) contains a credit allocation, indicating how  many\n             data  messages\
    \  the  entity  is willing to receive\n             from the correspondent entity,\n\
    \          3) may  optionally  contain  fields   which   confirm\n           \
    \  receipt   of  critical  acknowledgement  messages,\n             known as flow\
    \ control confirmation (FCC), and\n          4) is sent upon expiration of  the\
    \  window  timer  to\n             maintain  a minimum level of traffic on an\n\
    \             otherwise quiescent connection.\n   In choosing an acknowledgement\
    \ strategy, the first and  third uses\n   mentioned  above,  data  confirmation\
    \ and FCC, are the most relevant;\n   the second, credit allocation, is  determined\
    \ according  to  the\n   flow  control  strategy  chosen, and the fourth,  the\
    \  window\n   acknowledgement,  is  only   mentioned briefly in the discussion\
    \ on\n   flow control confirmation.\n"
- title: 8.2.4.1   Acknowledgement of data.
  contents:
  - "8.2.4.1   Acknowledgement of data.\n   The primary purpose of the acknowledgement\
    \  message  is to  confirm\n   correct  receipt  of  data messages.  There are\
    \ several choices that\n   the implementor must make when  designing a  specific\n\
    \   implementation.   Which  choice to make is based largely on the\n   operating\
    \  environment  (e.g.,  network error  characteristics).\n   The issues to be\
    \ decided upon are discussed in the sections below.\n"
- title: 8.2.4.1.1  Misordered data messages.
  contents:
  - "8.2.4.1.1  Misordered data messages.\n   Data messages received out  of  order\
    \  due  to  network misordering\n   or loss can be cached or discarded.  There\
    \ is no single determinant\n   that guides the implementor to one or  the  other\
    \ choice.  Rather,\n   there are a number of issues to be considered.\n   One\
    \ issue is the importance of maintaining a low  delay as  perceived\n   by  the\
    \ user.  If transport data messages are lost or damaged in\n   transit, the absence\
    \ of a  positive acknowledgement  will trigger a\n   retransmission at the sending\
    \ entity.  When the retransmitted data\n   message arrives at  the receiving \
    \ transport,  it  can be delivered\n   to the user.  If subsequent data messages\
    \ had  been  cached,  they\n   could  be delivered  to  the user at the same time.\
    \  The delay\n   between the sending  and  receiving  users  would,  on  average,\
    \ be\n   shorter  than  if messages subsequent to a lost message were\n   dependent\
    \ on retransmission for recovery.\n   A second factor that influences the caching\
    \ choice is  the cost of\n   transmission.  If transmission costs are high, it\
    \ is more economical\n   to cache  misordered  data,  in  conjunction with the\
    \ use of\n   selective acknowledgement (described below), to avoid\n   retransmissions.\n\
    \   There are two resources that are conserved by not caching misordered\n   data:\
    \ design and implementation time for the transport entity and CPU\n   processing\
    \ time during execution.  Savings  in  both  categories\n   accrue  because a\
    \ non-caching implementation is simpler in its buffer\n   management.  Data TPDUs\
    \ are discarded rather than being reordered.\n   This avoids the overhead of managing\
    \ the gaps  in  the  received\n   data  sequence space, searching of sequenced\
    \ message lists, and\n   inserting retransmitted data messages into the lists.\n"
- title: 8.2.4.1.2   Nth acknowledgement.
  contents:
  - "8.2.4.1.2   Nth acknowledgement.\n   In general, an acknowledgement message \
    \ is  sent  after receipt of\n   every N data messages on a connection. If N is\
    \ small compared to the\n   credit offered, then a finer granularity of buffer\
    \  control  is\n   afforded  to  the  data sender's buffer management function.\
    \  Data\n   messages are confirmed in small groups,  allowing buffers to be\n\
    \   reused sooner than if N were larger.  The cost of having N small is\n   twofold.\
    \  First, more acknowledgement  messages must be generated by\n   one transport\
    \ entity and processed by another, consuming some of  the\n   CPU resource  at\
    \  both  ends  of a connection.  Second, the\n   acknowledgement messages consume\
    \ transmission bandwidth,  which may\n   be expensive or limited.\n   For larger\
    \  N,  buffer  management  is  less  efficient because the\n   granularity with\
    \ which buffers are controlled is N times the maximum\n   TPDU size.  For example,\
    \ when data  messages are  transmitted to a\n   receiving entity employing this\
    \ strategy with large N, N data\n   messages must be  sent  before an  acknowledgement\
    \  is  returned\n   (although the window timer causes the acknowledgement to \
    \ be  sent\n   eventually regardless  of  N).  If the minimum credit allocation\
    \ for\n   continuous operation is actually  a  fraction  of  N,  a credit  of\
    \ N\n   must still be offered, and N receive buffers reserved, to achieve a\n\
    \   continuous  flow  of  data  messages.  Thus,  more  receive  buffers\n   are\
    \ used than are actually needed.  (Alternatively, if one relies on\n   the timer,\
    \  which  must  be adjusted to the receipt time for N and\n   will not expire\
    \ until some time after the fraction of N has been\n   sent,  there  may be idle\
    \ time.)\n   The choice of values for N depends on several factors.  First, if\
    \ the\n   rate at which DT TDPUs are arriving is relatively low, then there is\n\
    \   not much justification for using a value for N that exceeds 2.  On\n   the\
    \ other hand, if the DT TPDU arrival rates is high or the TPDU's\n   arrive in\
    \ large groups (e.g., in a frame from a satellite link), then\n   it may be reasonable\
    \ to use a larger value for N, simply to avoid the\n   overhead of generating\
    \ and sending the acknowledgements while\n   procesing the DT TPDUs.  Second,\
    \ the value of N should be related to\n   the maximum credit to be offered. Letting\
    \ C be the maximum credit to\n   be offered, one should choose N < C/2, since\
    \ the receipt of C TPDUs\n   without acknowledging will provoke sending one in\
    \ any case. However,\n   since the extended formats option for transport provides\
    \ max C =\n   2**16 - 1, a choice of N = 2**15 - 2 is likely to cause some of\
    \ the\n   sender's retransmission timers to expire.  Since the retransmitted\n\
    \   TPDU's will arrive out of sequence, they will provoke the sending of\n   AK\
    \ TPDU's.  Thus, not much is gained by using an N large.  A better\n   choice\
    \ is N = log C (base 2).  Third, the value of should be related\n   to the maximum\
    \ TPDU size used on the connection and the overall\n   buffer management. For\
    \ example, the buffer management may be tied to\n   the largest TPDU that any\
    \ connection will use, with each connection\n   managing the actual way in which\
    \ the negotiated TPDU size relates to\n   this buffer size.  In such case, if\
    \ a connection has negotiated a\n   maximum TPDU size of 128 octets and the buffers\
    \ are 2048 octets, it\n   may provide better management to partially fill a buffer\
    \ before\n   acknowledging.  If the example connection has two buffers and has\n\
    \   based offered credit on this, then one choice for N could be 2*log(\n   2048/128\
    \ ) = 8.  This would mean that an AK TPDU would be sent when a\n   buffer is half\
    \ filled ( 2048/128 = 16 ), and a double buffering\n   scheme used to manage the\
    \ use of the two buffers.  the use of the t\n   There are two studies which indicate\
    \ that, in many cases, 2 is a good\n   choice for N [COL85, BRI85].  The increased\
    \ granularity in buffer\n   management is reasonably small when compared to the\
    \ credit\n   allocation, which ranges from 8K to 120K octets in the studies cited.\n\
    \   The benefit is that the number of acknowledgements generated (and\n   consumed)\
    \ is cut approximately in half.\n"
- title: 8.2.4.1.3   Selective acknowledgement.
  contents:
  - "8.2.4.1.3   Selective acknowledgement.\n   Selective acknowledgement is an option\
    \ that allows misordered data\n   messages to be confirmed even in the presence\
    \ of gaps in the received\n   message sequence.   (Note that selective  acknowledgement\
    \  is  only\n   meaningul whe caching out-of-orderdata messags.)  The  advantage\
    \  to\n   using  this mechanism  is hat i grealy reduces the number of\n   unnecessary\
    \ retransmissions, thus saving both  computing  time  and\n   transmission bandwidth\
    \ [COL85] (see the discussion in Part 8.2.4.1.1\n   for  more  details).\n"
- title: 8.2.4.2   Flow control confirmation and fast retransmission.
  contents:
  - "8.2.4.2   Flow control confirmation and fast retransmission.\n   Flow control\
    \ confirmation (FCC) is a mechanism of the transport\n   protocol whereby acknowledgement\
    \ messages containing critical flow\n   control information are confirmed.  The\
    \ critical  acknowledgement\n   messages are those  that open a closed flow control\
    \ window and\n   certain ones that occur subsequent  to a credit reduction.  In\n\
    \   principle, if these critical messages are lost, proper\n   resynchroniztion\
    \ of the flow control relies on the window timer,\n   which is generally of relatively\
    \ long duration.   In order to reduce\n   delay in resynchronizing the flow control,\
    \ the receiving entity can\n   repeatedly send, within short intervals, AK TPDUs\
    \ carrying a request\n   for confirmation of the flow control state, a procedure\
    \ known as\n   \"fast\" retransmission (of the acknowledgement).  If the sender\n\
    \   responds with an AK TPDU carrying an FCC parameter, fast\n   retransmission\
    \ is halted.  If no AK TPDU carrying the FCC parameter\n   is received, the fast\
    \ transmission halts after having reached a\n   maximum number of retransmissions,\
    \ and the window timer resumes\n   control of AK TPDU transmission.  It should\
    \ be noted that FCC is an\n   optional mechanism of transport and the data sender\
    \ is not required\n   to respond to a request for confirmation of the flow control\
    \ state\n   wih an AK TPDU carrying the FCC parameter.\n   Some considerations\
    \ for deciding whether or not to use FCC and fast\n   retransmisson procedures\
    \ are as follows:\n    1) likelihood of credit reduction on a given transport\
    \ connection;\n    2) probability of TPDU loss;\n    3) expected window timer\
    \ period;\n    4) window size; and\n    5) acknowledgement strategy.\n   At this\
    \ time, there is no reported experience with using FCC and fast\n   retransmission.\
    \  Thus, it is not known whether or not the procedures\n   produce sufficient\
    \ reduction of resynchronization delay to warrant\n   implementing them.\n   When\
    \ implementing fast retransmission, it is suggested that the timer\n   used for\
    \ the window timer be employed as the fast timer, since the\n   window is disabled\
    \ during fast retransmission in any case.  This will\n   avoid having to manage\
    \ another timer.  The formal description\n   expressed the fast retransmission\
    \ timer as a separate timer for\n   clarity.\n"
- title: 8.2.4.3   Concatenation of acknowledgement and data.
  contents:
  - "8.2.4.3   Concatenation of acknowledgement and data.\n   When full duplex communication\
    \ is being operated by two transport\n   entities, data and acknowledgement TPDUs\
    \ from each one of the\n   entities travel in the same direction.  The transport\
    \ protocol\n   permits concatenating AK TPDUs in the same NSDU as a DT TPDU. \
    \ The\n   advantage of using this feaure in an implementation is that fewer\n\
    \   NSDUs will be transmitted, and, consequently, fewer total octets will\n  \
    \ be sent, due to the reduced number of network headers transmitted.\n   However,\
    \ when operating over the IP, this advantage may not\n   necessarily be recognized,\
    \ due to the possible fragmentation of the\n   NSDU by the IP.  A careful analysis\
    \ of the treatment of the NSDU in\n   internetwork environments should be done\
    \ to determine whether or not\n   concatenation of TPDUs is of sufficient benefit\
    \ to justify its use in\n   that situation.\n"
- title: 8.2.5   Retransmission policies.
  contents:
  - "8.2.5   Retransmission policies.\n   There are primarily two  retransmission\
    \  policies  that can be\n   employed in a transport implementation.  In the first\
    \ of these, a\n   separate retransmission timer  is  initiated  for each  data\
    \  message\n   sent by the transport entity.  At first glance, this approach appears\n\
    \   to be simple and  straightforward to implement.  The deficiency of\n   this\
    \ scheme is that it is inefficient.  This derives from two\n   sources.  First,\
    \  for each data message transmitted, a timer must be\n   initiated and cancelled,\
    \ which consumes a significant amount of  CPU\n   processing  time  [BRI85]. \
    \  Second, as the list of outstanding\n   timers grows, management of the list\
    \ also  becomes  increasingly\n   expensive.   There  are  techniques  which \
    \ make list management more\n   efficient, such as a list per connection and hashing,\
    \  but\n   implementing  a  policy of one retransmission timer per transport\n\
    \   connection is a superior choice.\n   The second retransmission policy, implementing\
    \ one retransmission\n   timer for each transport conenction, avoids some of the\n\
    \   inefficiencies cited above: the  list  of  outstanding  timers  is\n   shorter\
    \ by approximately an order of magnitude.  However, if the\n   entity receiving\
    \ the data is generating an  acknowledgement for\n   every  data message, the\
    \ timer must still be cancelled and restarted\n   for each  data/acknowledgement\
    \  message pair  (this is an additional\n   impetus for implementing an Nth acknowledgement\
    \ policy with N=2).\n   The rules governing the  single  timer  per  connection\
    \ scheme are\n   listed below.\n          1) If  a  data  message  is   transmitted\
    \   and   the\n             retransmission  timer  for  the  connection is not\n\
    \             already running, the timer is started.\n          2) If an acknowledgement\
    \ for previously unacknowledged\n             data is received, the retransmission\
    \ timer is restarted.\n          3) If an acknowledgement message is received\
    \ for  the\n             last  outstanding  data  message on the connection\n\
    \             then the timer is cancelled.\n          4) If the retransmission\
    \ timer expires, one  or  more\n             unacknowledged  data  messages  are\
    \ retransmitted,\n             beginning with the one sent earliest.  (Two\n \
    \            reports [HEA85, BRI85] suggest that the number\n             to retransmit\
    \ is one.)\n"
- title: 8.3   Protocol control.
  contents:
  - '8.3   Protocol control.

    '
- title: 8.3.1   Retransmission timer values.
  contents:
  - '8.3.1   Retransmission timer values.

    '
- title: 8.3.1.1   Data retransmission timer.
  contents:
  - "8.3.1.1   Data retransmission timer.\n   The value for the reference timer may\
    \ have a significant impact on\n   the performance of the transport protocol [COL85].\
    \  However,\n   determining the proper value to use is sometimes difficult.\n\
    \   According to IS 8073, the value for the timer is computed using the\n   transit\
    \ delays, Erl and Elr, the acknowledgement delay, Ar, and the\n   local TPDU processing\
    \ time, x:\n    T1 = Erl + Elr + Ar + x\n   The  difficulty  in  arriving at a\
    \ good retransmission timer value is\n   directly related to the variability of\
    \  these  factors Of the two,\n   Erl and Elr are the most susceptible to variation,\
    \ and therefore have\n   the most impact on  determining a  good  timer  value.\
    \   The\n   following  paragraphs  discuss methods for choosing retransmission\n\
    \   timer  values  that  are appropriate in several network environments.\n  \
    \ In a single-hop satellite environment, network delay (Erl or Elr) has\n   small\
    \ variance because of the constant propagation delay of about 270\n   ms., which\
    \ overshadows the other components  of network  delay.\n   Consequently, a fixed\
    \ retransmission timer provides good performance.\n   For example, for a 64K \
    \ bit/sec.  link  speed and network queue size\n   of four, 650 ms. provides good\
    \ performance [COL85].\n   Local area  networks  also  have  constant  propagation\
    \ delay.\n   However, propagation delay is a relatively unimportant factor in\n\
    \   total network delay for a local area network.  Medium  access  delay\n   and\
    \  queuing delay are the significant components of network delay,\n   and (Ar\
    \ + x) also plays a significant  role  in determining an\n   appropriate retransmission\
    \ timer.  From the discussion presented in\n   Part 3.4.3.2 typical numbers for\
    \ (Ar + x) are on the order of 5 - 6.5\n   ms and for Erl or Elr, 5 - 35 ms. \
    \ Consequently, a reasonable value\n   for  the  retransmission  timer is 100\
    \ ms.  This value works well for\n   local area networks, according to one cited\
    \ report [INT85] and\n   simulation work performed at the NBS.\n   For better\
    \ performance in an environment with long propagation\n   delays and significant\
    \ variance, such as an internetwork an adaptive\n   algorithm is preferred, such\
    \ as the one suggested value  for  TCP/IP\n   [ISI81].  As analyzed by Jain [JAI85],\
    \ the algorithm uses an\n   exponential averaging scheme to  derive  a round trip\
    \ delay estimate:\n               D(i)  = b * D(i-1)  +  (1-b) * S(i)\n   where\
    \ D(i) is the update of the delay estimate, S(i) is  the sample\n   round  trip\
    \  time measured between transmission of a given packet and\n   receipt of its\
    \ acknowledgement, and b is  a weighting   factor\n   between  0  and  1,  usually\
    \  0.5.   The retransmission timer is\n   expressed as some multiplier, k,  of\
    \ D.  Small values of k cause\n   quick detection of lost packets, but result\
    \ in a higher number of\n   false timeouts and,  therefore, unnecessary   retransmissions.\
    \    In\n   addition,  the retransmission timer should  be  increased\n   arbitrarily\
    \  for each case of multiple transmissions; an exponential\n   increase is suggested,\
    \ such that\n               D(i) = c * D(i-1)\n   where c is a dimensionless parameter\
    \ greater than one.\n   The remaining parameter for the adaptive  algorithm  is\
    \ the  initial\n   delay  estimate,  D(0).   It  is preferable to choose a slightly\n\
    \   larger value than needed, so that unnecessary retransmissions  do\n   not\
    \  occur at the beginning.  One possibility is to measure the round\n   trip delay\
    \  during connection  establishment.   In  any  case, the\n   timer converges\
    \ except under conditions of sustained congestion.\n"
- title: 8.3.1.2   Expedited data retransmission timer.
  contents:
  - "8.3.1.2   Expedited data retransmission timer.\n   The timer which  governs \
    \ retransmission  of  expedited data should\n   be set using the normal data retransmission\
    \ timer value.\n"
- title: 8.3.1.3   Connect-request/confirm retransmission timer.
  contents:
  - "8.3.1.3   Connect-request/confirm retransmission timer.\n   Connect request and\
    \ confirm  messages  are  subject  to Erl + Elr,\n   total network delay, plus\
    \  processing  time  at  the receiving\n   transport entity, if these values are\
    \ known.  If an accurate estimate\n   of the round trip time is not known, two\
    \  views  can be espoused in\n   choosing the value for this timer.  First,  since\
    \  this  timer\n   governs  connection establishment, it is desirable to minimize\
    \ delay\n   and so a small value can be chosen, possibly resulting in unnecessary\n\
    \   retransmissions.  Alternatively, a larger value can be used, reducing\n  \
    \ the possibility of unnecessary retransmissions, but resulting in\n   longer\
    \ delay in connection establishment should the connect request\n   or confirm\
    \ message be lost.  The choice between these two views is\n   dictated largely\
    \ by local requirements.\n"
- title: 8.3.1.4  Disconnect-request retransmission timer.
  contents:
  - "8.3.1.4  Disconnect-request retransmission timer.\n   The timer which governs\
    \ retransmission of  the  disconnect request\n   message  should  be  set from\
    \ the normal data retransmission timer\n   value.\n"
- title: 8.3.1.5   Fast retransmission timer.
  contents:
  - "8.3.1.5   Fast retransmission timer.\n   The fast  retransmission  timer  causes\
    \  critical acknowledgement\n   messages to be retransmitted avoiding delay in\
    \ resynchronizing\n   credit.  This timer should be set to approximately Erl +\
    \ Elr.\n"
- title: 8.3.2   Maximum number of retransmissions.
  contents:
  - "8.3.2   Maximum number of retransmissions.\n   This transport parameter determines\
    \ the maximum  number of  times  a\n   data message will be retransmitted.  A\
    \ typical value is eight.  If\n   monitoring of network service is performed then\
    \ this value can be\n   adjusted according to observed error rates.  As a high\
    \ error rate\n   implies a high probability of TPDU loss, when it is desirable\
    \ to\n   continue sending despite the decline in quality of service, the\n   number\
    \ of TPDU retransmissions (N) should be increased and the\n   retransmission interval\
    \ (T1) reduced.\n"
- title: 8.4   Selection of maximum Transport Protocol data unit size.
  contents:
  - "8.4   Selection of maximum Transport Protocol data unit size.\n   The choice\
    \ of maximum size for TPDUs in negotiation proposals depends\n   on the application\
    \ to be served and the service quality of the\n   supporting network.  In general,\
    \ an application which produces large\n   TSDUs should use as large TPDUs as can\
    \ be negotiated, to reduce the\n   overhead due to a large number of small TPDUs.\
    \  An application which\n   produces small TSDUs should not be affected by the\
    \ choice of a large\n   maximum TPDU size, since a TPDU need not be filled to\
    \ the maximum\n   size to be sent.  Consequently, applications such as file transfers\n\
    \   would need larger TPDUs while terminals would not.  On a high\n   bandwidth\
    \ network service, large TPDUs give better channel\n   utilization than do smaller\
    \ ones.  However, when error rates are\n   high, the likelihood for a given TPDU\
    \ to be damaged is correlated to\n   the size and the frequency of the TPDUs.\
    \  Thus, smaller TPDU size in\n   the condition of high error rates will yield\
    \ a smaller probability\n   that any particular TPDU will be lost.\n   The implementor\
    \ must choose whether or not to apply a uniform maximum\n   TPDU size to all connections.\
    \  If the network service is uniform in\n   service quality, then the selection\
    \ of a uniform maximum can simplify\n   the implementation.  However, if the network\
    \ quality is not uniform\n   and it is desirable to optimize the service provided\
    \ to the transport\n   user as much as possible, then it may be better to determine\
    \ the\n   maximum size on an individual connection basis.  This can be done at\n\
    \   the time of the network service access if the characteristics of the\n   subnetwork\
    \ are known.\n   NOTE: The maximum TPDU size is important in the calculation of\
    \ the\n   flow control credit, which is in numbers of TPDUs offered.  If buffer\n\
    \   space is granted on an octet base, then credit must be granted as\n   buffer\
    \ space divided by maximum TPDU size.  Use of a smaller TPDU\n   size can be equivalent\
    \ to optimistic credit allocation and can lead\n   to the expected problems, if\
    \ proper analysis of the management is not\n   done.\n"
- title: 9   Special options.
  contents:
  - "9   Special options.\n   Special options may be obtained by taking advantage\
    \ of the manner in\n   which IS 8073 and N3756 have been written.  It must be\
    \ emphasized\n   that these options in no way violate the intentions of the standards\n\
    \   bodies that produced the standards.  Flexibility was deliberately\n   written\
    \ into the standards to ensure that they do not constrain\n   applicability to\
    \ a wide variety of situations.\n"
- title: 9.1   Negotiations.
  contents:
  - "9.1   Negotiations.\n   The negotiation procedures in IS 8073 have deliberate\
    \ ambiguities in\n   them to permit flexibility of usage within closed groups\
    \ of\n   communicants (the standard defines explicitly only the behavior among\n\
    \   open communicants).  A closed group of communicants in an open system\n  \
    \ is one which, by reason of organization, security or other special\n   needs,\
    \ carries on certain communication among its members which is\n   not of interest\
    \ or not accessible to other open system members.\n   Examples of some closed\
    \ groups within DOD might be:  an Air Force\n   Command, such as the SAC; a Navy\
    \ base or an Army post; a ship;\n   Defense Intelligence; Joint Chiefs of Staff.\
    \ Use of this\n   characteristic does not constitute standard behavior, but it\
    \ does not\n   violate conformance to the standard, since the effects of such\
    \ usage\n   are not visible to non-members of the closed group.  Using the\n \
    \  procedures in this way permits options not provided by the standard.\n   Such\
    \ options might permit,for example, carrying special protection\n   codes on protocol\
    \ data units or for identifying DT TPDUs as carrying\n   a particular kind of\
    \ message.\n   Standard negotiation procedures state that any parameter in a\n\
    \   received CR TPDU that is not defined by the standard shall be\n   ignored.\
    \  This defines only the behavior that is to be exhibited\n   between two open\
    \ systems.  It does not say that an implementation\n   which recognizes such non-standard\
    \ parameters shall not be operated\n   in networks supporting open systems interconnection.\
    \  Further, any\n   other type TPDU containing non-standard parameters is to be\
    \ treated\n   as a protocol error when received.  The presumption here is that\
    \ the\n   non-standard parameter is not recognized, since it has not been\n  \
    \ defined.  Now consider the following example:\n   Entity A sends Entity B a\
    \ CR TPDU containing a non-standard\n   parameter.\n   Entity B has been implemented\
    \ to recognize the non-standard parameter\n   and to interpret its presence to\
    \ mean that Entity A will be sending\n   DT TPDUs to Entity B with a special protection\
    \ identifier parameter\n   included.\n   Entity B sends a CC TPDU containing the\
    \ non-standard parameter to\n   indicate to Entity A that it has received and\
    \ understood the\n   parameter, and is prepared to receive the specially marked\
    \ DT TPDUs\n   from Entity A.  Since Entity A originally sent the non-standard\n\
    \   parameter, it recognizes the parameter in the CC TPDU and does not\n   treat\
    \ it as a protocol error.\n   Entity A may now send the specially marked DT TPDUs\
    \ to Entity B and\n   Entity B will not reject them as protocol errors.\n   Note\
    \ that Entity B sends a CC TPDU with the non-standard parameter\n   only if it\
    \ receives a CR TPDU containing the parameter, so that it\n   does not create\
    \ a protocol error for an initiating entity that does\n   not use the parameter.\
    \  Note also that if Entity B had not recognized\n   the parameter in the CR TPDU,\
    \ it would have ignored it and not\n   returned a CC TPDU containing the parameter.\
    \  This non-standard\n   behavior is clearly invisible and inaccessible to Transport\
    \ entities\n   outside the closed group that has chosen to implement it, since\
    \ they\n   are incapable of distinguishing it from errors in protocol.\n"
- title: 9.2   Recovery from peer deactivation.
  contents:
  - "9.2   Recovery from peer deactivation.\n   Transport does not directly support\
    \ the recovery of the transport\n   connection from a crashed remote transport\
    \ entity.  A partial\n   recovery is possible, given proper interpretation of\
    \ the state tables\n   in Annex A to IS 8073 and implementation design.  The interpretation\n\
    \   of the Class 4 state tables necessary to effect this operation is as\n   follows:\n\
    \   Whenever a CR TPDU is received in the state OPEN, the entity is\n   required\
    \ only to record the new network connection and to reset the\n   inactivity timer.\
    \  Thus, if the initiator of the original connection\n   is the peer which crashed,\
    \ it may send a new CR TPDU to the surviving\n   peer, somehow communicating to\
    \ it the original reference numbers\n   (there are several ways that this can\
    \ be done).\n      Whenever a CC TPDU is received in the\n   state OPEN, the receiver\
    \ is required only to record the new network\n   connection, reset the inactivity\
    \ timer and send either an AK, DT or\n   ED TPDU.  Thus, if the responder for\
    \ the original connection is the\n   peer which crashed, it may send a new CC\
    \ TPDU to the surviving peer,\n   communicating to it the original reference numbers.\n\
    \   In order for this procedure to operate properly, the situation in a.,\n  \
    \ above, requires a CC TPDU to be sent in response.  This could be the\n   original\
    \ CC TPDU that was sent, except for new reference numbers.\n   The original initiator\
    \ will have sent a new reference number in the\n   new CR TPDU, so this would\
    \ go directly into the CC TPDU to be\n   returned.  The new reference number for\
    \ the responder could just be a\n   new assignment, with the old reference number\
    \ frozen.  In the\n   situation in b., the originator could retain its reference\
    \ number (or\n   assign a new one if necessary), since the CC TPDU should carry\
    \ both\n   old reference numbers and a new one for the responder (see below).\n\
    \   In either situation, only the new reference numbers need be extracted\n  \
    \ from the CR/CC TPDUs, since the options and parameters will have been\n   previously\
    \ negotiated.  This procedure evidently requires that the CR\n   and CC TPDUs\
    \ of each connection be stored by the peers in nonvolatile\n   memory, plus particulars\
    \ of the negotiations.\n   To transfer the new reference numbers, it is suggested\
    \ that the a new\n   parameter in the CR and CC TPDU be defined, as in Part 9.1,\
    \ above.\n   This parameter could also carry the state of data transfer, to aid\
    \ in\n   resynchronizing, in the following form:\n    1) the last DT sequence\
    \ number received by the peer that crashed;\n    2) the last DT sequence number\
    \ sent by the peer that\n       crashed;\n    3) the credit last extended by the\
    \ peer that crashed;\n    4) the last credit perceived as offered by the surviving\
    \ peer;\n    5) the next DT sequence number the peer that crashed expects to\n\
    \       send (this may not be the same as the last one sent, if the last\n   \
    \    one sent was never acknowledged);\n    6) the sequence number of an unacknowledged\
    \ ED TPDU, if any;\n    7) the normal data sequence number corresponding to the\n\
    \       transmission of an unacknowledged ED TPDU, if any (this is to\n      \
    \ ensure the proper ordering of the ED TPDU in the normal data\n       flow);\n\
    \   A number of other considerations must be taken into account when\n   attempting\
    \ data transfer resynchronization.  First, the recovery will\n   be greatly complicated\
    \ if subsequencing or flow control confirmation\n   is in effect when the crash\
    \ occurs.  Careful analysis should be done\n   to determine whether or not these\
    \ features provide sufficient benefit\n   to warrant their inclusion in a survivable\
    \ system.  Second,\n   non-volatile storage of TPDUs which are unacknowledged\
    \ must be used\n   in order that data loss at the time of recovery can be minimized.\n\
    \   Third, the values for the retranmsission timers for the communicating\n  \
    \ peers must allow sufficient time for the recovery to be attempted.\n   This\
    \ may result in longer delays in retransmitting when TPDUs are\n   lost under\
    \ normal conditions. One way that this might be achieved is\n   for the peers\
    \ to exchange in the original CR/CC TPDU exchange, their\n   expected lower bounds\
    \ for the retransmission timers, following the\n   procedure in Part 9.1.  In\
    \ this manner, the peer that crashed may be\n   determine whether or not a new\
    \ connection should be attempted. Fourth,\n   while the recovery involves directly\
    \ only the transport peers when\n   operating over a connectionless network service,\
    \ recovery when\n   operating over a connection-oriented network service requires\
    \ some\n   sort of agreement as to when a new network connection is to be\n  \
    \ established (if necessary) and which peer is responsible for doing\n   it. \
    \ This is required to ensure that unnecessary network\n   connections are not\
    \ opened as a result of the recovery.  Splitting\n   network connections may help\
    \ to ameliorate this problem.\n"
- title: 9.3   Selection of transport connection reference numbers.
  contents:
  - "9.3   Selection of transport connection reference numbers.\n   In N3756, when\
    \ the reference wait period for a connection begins, the\n   resources associated\
    \ with the connection are released and the\n   reference number is placed in a\
    \ set of frozen references.  A timer\n   associated with this number is started,\
    \ and when it expires, the\n   number is removed from the set.  A function which\
    \ chooses reference\n   numbers checks this set before assigning the next reference\
    \ number.\n   If it is desired to provide a much longer period by the use of a\n\
    \   large reference number space, this can be met by replacing the\n   implementation\
    \ dependent function \"select_local_ref\" (page TPE-17 of\n   N3756) by the following\
    \ code:\n    function select_local_ref : reference_type;\n    begin\n    last_ref\
    \ := (last_ref + 1) mod( N+1 ) + 1;\n    while last_ref in frozen_ref[class_4]\
    \ do\n              last_ref := (last_ref + 1) mod( N+1 ) + 1;\n    select_local_ref\
    \ := last_ref;\n    end;\n   where \"last_ref\" is a new variable to be defined\
    \ in declarations\n   (pages TPE-10 - TPE-11), used to keep track of the last\
    \ reference\n   value assigned, and N is the length of the reference number cycle,\n\
    \   which cannot exceed 2**16 - 1 since the reference number fields in\n   TPDUs\
    \ are restricted to 16 bits in length.\n"
- title: 9.4   Obtaining Class 2 operation from a Class 4 implementation.
  contents:
  - "9.4   Obtaining Class 2 operation from a Class 4 implementation.\n   The operation\
    \ of Class 4 as described in IS 8073 logically contains\n   that of the Class\
    \ 2 protocol.  The formal description, however, is\n   written assuming Class\
    \ 4 and Class 2 to be distinct.  This was done\n   because the description must\
    \ reflect the conformance statement of IS\n   8073, which provides that Class\
    \ 2 alone may be implemented.\n   However, Class 2 operation can be obtained from\
    \ a Class 4\n   implementation, which would yield the advantages of lower complexity,\n\
    \   smaller memory requirements, and lower implementation costs as\n   compared\
    \ to implementing the classes separately.  The implementor\n   will have to make\
    \ the following provisions in the transport entity\n   and the Class 4 transport\
    \ machine to realize Class 2 operation.\n     1)   Disable all timers.  In the\
    \ formal description, all Class 4\n          timers except the reference timer\
    \ are in the Class 4 TPM.\n          These timers can be designed at the outset\
    \ to be enabled or\n          not at the instantiation of the TPM.  The reference\
    \ timer is\n          in the Transport Entity module (TPE) and is activated by\
    \ the\n          TPE recognizing that the TPM has set its \"please_kill_me\"\n\
    \          variable to \"freeze\".  If the TPM sets this variable instead\n  \
    \        to \"now\", the reference timer for that transport connection is\n  \
    \        never started.  However, IS 8073 provides that the reference\n      \
    \    timer can be used, as a local entity management decision, for\n         \
    \ Class 2.\n          The above procedure should be used when negotiating from\
    \ Class\n          4 to Class 2.  If Class 2 is proposed as the preferred class,\n\
    \          then it is advisable to not disable the inactivity timer, to\n    \
    \      avoid the possibility of deadlock during connection\n          establishment\
    \ if the peer entity never responds to the CR\n          TPDU.  The inactivity\
    \ timer should be set when the CR TPDU is\n          sent and deactivated when\
    \ the CC TPDU is received.\n     2)   Disable checksums.  This can be done simply\
    \ by ensuring that\n          the boolean variable \"use_checksums\" is always\
    \ set to \"false\"\n          whenever Class 2 is to be proposed or negotiated.\n\
    \     3)   Never permit flow control credit reduction. The formal\n          description\
    \ makes flow control credit management a function of\n          the TPE operations\
    \ and such management is not reflected in the\n          operation of the TPM.\
    \  Thus, this provision may be handled by\n          always making the \"credit-granting\"\
    \ mechanism aware of the\n          class of the TPM being served.\n     4)  \
    \ Include Class 2 reaction to network service events.  The Class\n          4\
    \ handling of network service events is more flexible than\n          that of\
    \ Class 2 to provide the recovery behavior\n          characteristic of Class\
    \ 4.  Thus, an option should be provided\n          on the handling of N_DISCONNECT_indication\
    \ and\n          N_RESET_indication for Class 2 operation.  This consists of\n\
    \          sending a T_DISCONNECT_indication to the Transport User,\n        \
    \  setting \"please_kill_me\" to \"now\" (optionally to \"freeze\"),\n       \
    \   and transitioning to the CLOSED state, for both events.  (The\n          Class\
    \ 4 action in the case of the N_DISCONNECT is to remove\n          the network\
    \ connection from the set of those associated with\n          the transport connection\
    \ and to attempt to obtain a new\n          network connection if the set becomes\
    \ empty.  The action on\n          receipt of the N_RESET is to do nothing, since\
    \ the TPE has\n          already issued the N_RESET_response.)\n     5)   Ensure\
    \ that TPDU parameters conform to Class 2.  This implies\n          that subsequence\
    \ numbers should not be used on AK TPDUs, and\n          no flow control confirmation\
    \ parameters should ever appear in\n          an AK TPDU.  The checksum parameter\
    \ is prevented from\n          appearing by the \"false\" value of the \"use_checksums\"\
    \n          variable.  (The acknowledgement time parameter in the CR and\n   \
    \       CC TPDUs will not be used, by virtue of the negotiation\n          procedure.\
    \  No special assurance for its non-use is\n          necessary.)\n          The\
    \ TPE management of network connections should see to it\n          that splitting\
    \ is never attempted with Class 4 TPMs running as\n          Class 2.  The handling\
    \ of multiplexing is the same for both\n          classes, but it is not good\
    \ practice to multiplex Class 4 and\n          Class 2 together on the same network\
    \ connection.\n"
- title: 10   References.
  contents:
  - "10   References.\n     [BRI85]  Bricker, A., L. Landweber, T.  Lebeck,  M.  Vernon,\n\
    \              \"ISO  Transport Protocol Experiments,\" Draft Report\n       \
    \       prepared by DLS Associates for the  Mitre  Corporation,\n            \
    \  October 1985.\n     [COL85]  Colella, Richard,  Marnie  Wheatley,  Kevin  Mills,\n\
    \              \"COMSAT/NBS  Experiment Plan for Transport Protocol,\"\n     \
    \         NBS, Report No. NBSIR 85-3141, May l985.\n     [CHK85]  Chernik, C.\
    \ Michael, \"An NBS Host to Front End\n              Protocol,\" NBSIR 85-3236,\
    \ August 1985.\n     [CHO85]  Chong, H.Y., \"Software Development and Implementation\n\
    \              of NBS Class 4 Transport Protocol,\" October 1985\n           \
    \   (available from the author).\n     [HEA85]  Heatley, Sharon, Richard Colella,\
    \ \"Experiment Plan:\n              ISO Transport Over IEEE 802.3 Local Area Network,\"\
    \n              NBS, Draft Report (available from the authors),\n            \
    \  October 1985.\n     [INT85]  \"Performance Comparison Between  186/51  and\
    \  552,\"\n              The  Intel Corporation, Reference No. COM,08, January\n\
    \              1985.\n     [ISO84a] IS 8073 Information Processing - Open Systems\n\
    \              Interconnection - Transport Protocol Specification,\n         \
    \     available from ISO TC97/SC6 Secretariat, ANSI,\n              1430 Broadway,\
    \ New York, NY 10018.\n     [ISO84b] IS 7498 Information Processing - Open Systems\n\
    \              Interconnection - Basic Reference Model, available\n          \
    \    from ANSI, address above.\n     [ISO85a] DP 9074 Estelle - A Formal Description\
    \ Technique\n              Based on an Extended State Transition Model,\n    \
    \          available from ISO TC97/SC21 Secretariat, ANSI,\n              address\
    \ above.\n     [ISO85b] N3756 Information Processing - Open Systems\n        \
    \      Interconnection - Formal Description of IS 8073\n              in Estelle.\
    \ (Working Draft, ISO TC97/SC6)\n     [ISO85c] N3279 Information Processing -\
    \ Open Systems\n              Interconnection - DAD1, Draft Addendum to IS 8073\n\
    \              to Provide a Network Connection Management\n              Service,\
    \ ISO TC97/SC6 N3279, available from\n              SC6 Secretariat, ANSI, address\
    \ above.\n     [JAI85]  Jain, Rajendra K., \"CUTE: A Timeout  Based  Congestion\n\
    \              Control Scheme for Digitial Network Architecture,\"\n         \
    \     Digital Equipment Corporation (available from the\n              author),\
    \ March 1985.\n     [LIN85]  Linn, R.J., \"The Features and Facilities of Estelle,\"\
    \n              Proceedings of the IFIP WG 6.1 Fifth International\n         \
    \     Workshop on Protocol Specification, Testing and\n              Verification,\
    \ North Holland Publishing, Amsterdam,\n              June 1985.\n     [MIL85a]\
    \ Mills, Kevin L., Marnie Wheatley, Sharon Heatley,\n              \"Predicting\
    \ Transport Protocol Performance\",\n              (in preparation).\n     [MIL85b]\
    \ Mills, Kevin L., Jeff Gura, C. Michael Chernik,\n              \"Performance\
    \ Measurement of OSI Class 4 Transport\n              Implementations,\" NBSIR\
    \ 85-3104, January 1985.\n     [NAK85]  Nakassis, Anastase, \"Fletcher's Error\
    \ Detection\n              Algorithm: How to Implement It Efficiently and\n  \
    \            How to Avoid the Most Common Pitfalls,\" NBS,\n              (in\
    \ preparation).\n     [NBS83]  \"Specification of a Transport Protocol for\n \
    \             Computer Communications, Volume 3: Class 4\n              Protocol,\"\
    \ February 1983 (available from\n              the National Technical Information\
    \ Service).\n     [NTA84]  Hvinden, Oyvind, \"NBS Class 4 Transport Protocol,\n\
    \              UNIX 4.2 BSD Implementation and User Interface\n              Description,\"\
    \ Norwegian Telecommunications\n              Administration Establishment, Technical\
    \ Report\n              No. 84-4053, December 1984.\n     [NTI82]  \"User-Oriented\
    \ Performance Measurements on the\n              ARPANET: The Testing of a Proposed\
    \ Federal\n              Standard,\" NTIA Report 82-112 (available from\n    \
    \          NTIA, Boulder CO)\n     [NTI85]  \"The OSI Network Layer Addressing\
    \ Scheme, Its\n              Implications, and Considerations for Implementation\"\
    ,\n              NTIA Report 85-186, (available from NTIA, Boulder CO)\n     [RFC85]\
    \  Mills, David, \"Internet Delay Experiments,\" RFC889,\n              December\
    \ 1983 (available from the Network Information\n              Center).\n     [SPI82]\
    \  Spirn, Jeffery R., \"Network Modeling with Bursty\n              Traffic and\
    \ Finite Buffer Space,\" Performance\n              Evaluation Review, vol. 2,\
    \ no. 1, April 1982.\n     [SPI84]  Spirn, Jeffery R., Jade Chien, William Hawe,\n\
    \              \"Bursty Traffic Local Area Network Modeling,\"\n             \
    \ IEEE Journal on Selected Areas in Communications,\n              vol. SAC-2,\
    \ no. 1, January 1984.\n"
