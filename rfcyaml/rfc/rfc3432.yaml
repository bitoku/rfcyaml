- contents:
  - '         Network performance measurement with periodic streams

    '
  title: __initial_text__
- contents:
  - "Status of this Memo\n   This document specifies an Internet standards track protocol
    for the\n   Internet community, and requests discussion and suggestions for\n
    \  improvements.  Please refer to the current edition of the \"Internet\n   Official
    Protocol Standards\" (STD 1) for the standardization state\n   and status of this
    protocol.  Distribution of this memo is unlimited.\n"
  title: Status of this Memo
- contents:
  - "Copyright Notice\n   Copyright (C) The Internet Society (2002).  All Rights Reserved.\n"
  title: Copyright Notice
- contents:
  - "Abstract\n   This memo describes a periodic sampling method and relevant metrics\n
    \  for assessing the performance of IP networks.  First, the memo\n   motivates
    periodic sampling and addresses the question of its value\n   as an alternative
    to the Poisson sampling described in RFC 2330.  The\n   benefits include applicability
    to active and passive measurements,\n   simulation of constant bit rate (CBR)
    traffic (typical of multimedia\n   communication, or nearly CBR, as found with
    voice activity\n   detection), and several instances in which analysis can be\n
    \  simplified.  The sampling method avoids predictability by mandating\n   random
    start times and finite length tests.  Following descriptions\n   of the sampling
    method and sample metric parameters, measurement\n   methods and errors are discussed.
    \ Finally, we give additional\n   information on periodic measurements, including
    security\n   considerations.\n"
  title: Abstract
- contents:
  - "Table of Contents\n   1.  Conventions used in this document...........................
    \ 2\n   2.  Introduction................................................  3\n
    \      2.1 Motivation..............................................  3\n   3.
    \ Periodic Sampling Methodology...............................  4\n   4.  Sample
    metrics for periodic streams.........................  5\n       4.1 Metric name.............................................
    \ 5\n       4.2 Metric parameters.......................................  5\n
    \      4.3 High level description of the procedure to collect a\n           sample..................................................
    \ 7\n       4.4 Discussion..............................................  8\n
    \      4.5 Additional Methodology Aspects..........................  9\n       4.6
    Errors and uncertainties................................  9\n       4.7 Reporting...............................................
    13\n   5.  Additional discussion on periodic sampling.................. 14\n       5.1
    Measurement applications................................ 15\n       5.2 Statistics
    calculable from one sample................... 18\n       5.3 Statistics calculable
    from multiple samples............. 18\n       5.4 Background conditions...................................
    19\n       5.5 Considerations related to delay......................... 19\n   6.
    \ Security Considerations..................................... 19\n       6.1
    Denial of Service Attacks............................... 19\n       6.2 User data
    confidentiality............................... 20\n       6.3 Interference with
    the metric............................ 20\n   7.  IANA Considerations.........................................
    20\n   8.  Normative References........................................ 20\n   9.
    \ Informative References...................................... 21\n   10. Acknowledgments.............................................
    21\n   11. Author's Addresses.......................................... 22\n   12.
    Full Copyright Statement.................................... 23\n"
  title: Table of Contents
- contents:
  - "1. Conventions used in this document\n   The key words \"MUST\", \"MUST NOT\",
    \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\",
    \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described
    in BCP 14, RFC 2119 [2].\n   Although RFC 2119 was written with protocols in mind,
    the key words\n   are used in this document for similar reasons.  They are used
    to\n   ensure that the results of measurements from two different\n   implementations
    are comparable, and to note instances in which an\n   implementation could perturb
    the network.\n"
  title: 1. Conventions used in this document
- contents:
  - "2. Introduction\n   This memo describes a sampling method and performance metrics\n
    \  relevant to certain applications of IP networks.  The original driver\n   for
    this work was Quality of Service of interactive periodic streams,\n   such as
    multimedia conferencing over IP, but the idea of periodic\n   sampling and measurement
    has wider applicability.  Interactive\n   multimedia traffic is used as an example
    below to illustrate the\n   concept.\n   Transmitting equally sized packets (or
    mostly same-size packets)\n   through a network at regular intervals simulates
    a constant bit-rate\n   (CBR), or a nearly CBR multimedia bit stream.  Hereafter,
    these\n   packets are called periodic streams.  Cases of \"mostly same-size\n
    \  packets\" may be found in applications that have multiple coding\n   methods
    (e.g.  digitally coded comfort noise during silence gaps in\n   speech).\n   In
    the following sections, a sampling methodology and metrics are\n   presented for
    periodic streams.  The measurement results may be used\n   in derivative metrics
    such as average and maximum delays.  The memo\n   seeks to formalize periodic
    stream measurements to achieve comparable\n   results between independent implementations.\n"
  - contents:
    - "2.1 Motivation\n   As noted in the IPPM framework RFC 2330 [3], a sample metric
      using\n   regularly spaced singleton tests has some limitations when considered\n
      \  from a general measurement point of view: only part of the network\n   performance
      spectrum is sampled.  However, some applications also\n   sample this limited
      performance spectrum and their performance may be\n   of critical interest.\n
      \  Periodic sampling is useful for the following reasons:\n   * It is applicable
      to passive measurement, as well as active\n     measurement.\n   * An active
      measurement can be configured to match the\n     characteristics of media flows,
      and simplifies the estimation of\n     application performance.\n   * Measurements
      of many network impairments (e.g., delay variation,\n     consecutive loss,
      reordering) are sensitive to the sampling\n     frequency.  When the impairments
      themselves are time-varying (and\n     the variations are somewhat rare, yet
      important), a constant\n     sampling frequency simplifies analysis.\n   * Frequency
      Domain analysis is simplified when the samples are\n     equally spaced.\n   Simulation
      of CBR flows with periodic streams encourages dense\n   sampling of network
      performance, since typical multimedia flows have\n   10 to 100 packets in each
      second.  Dense sampling permits the\n   characterization of network phenomena
      with short duration.\n"
    title: 2.1 Motivation
  title: 2. Introduction
- contents:
  - "3. Periodic Sampling Methodology\n   The Framework RFC [3] points out the following
    potential problems\n   with Periodic Sampling:\n   1. The performance sampled
    may be synchronized with some other\n      periodic behavior, or the samples may
    be anticipated and the\n      results manipulated.  Unpredictable sampling is
    preferred.\n   2. Active measurements can cause congestion, and periodic sampling\n
    \     might drive congestion-aware senders into a synchronized state,\n      producing
    atypical results.\n   Poisson sampling produces an unbiased sample for the various
    IP\n   performance metrics, yet there are situations where alternative\n   sampling
    methods are advantageous (as discussed under Motivation).\n   We can prescribe
    periodic sampling methods that address the problems\n   listed above.  Predictability
    and some forms of synchronization can\n   be mitigated through the use of random
    start times and limited stream\n   duration over a test interval.  The periodic
    sampling parameters\n   produce bias, and judicious selection can produce a known
    bias of\n   interest.  The total traffic generated by this or any sampling method\n
    \  should be limited to avoid adverse affects on non-test traffic\n   (packet
    size, packet rate, and sample duration and frequency should\n   all be considered).\n
    \  The configuration parameters of periodic sampling are:\n   +  T, the beginning
    of a time interval where a periodic sample is\n      desired.\n   +  dT, the duration
    of the interval for allowed sample start times.\n   +  T0, a time that MUST be
    selected at random from the interval\n      [T, T+dT] to start generating packets
    and taking measurements.\n   +  Tf, a time, greater than T0, for stopping generation
    of packets\n      for a sample (Tf may be relative to T0 if desired).\n   +  incT,
    the nominal duration of inter-packet interval, first bit to\n      first bit.\n
    \  T0 may be drawn from a uniform distribution, or T0 = T + Unif(0,dT).\n   Other
    distributions may also be appropriate.  Start times in\n   successive time intervals
    MUST use an independent value drawn from\n   the distribution.  In passive measurement,
    the arrival of user media\n   flows may have sufficient randomness, or a randomized
    start time of\n   the measurement during a flow may be needed to meet this requirement.\n
    \  When a mix of packet sizes is desired, passive measurements usually\n   possess
    the sequence and statistics of sizes in actual use, while\n   active measurements
    would need to reproduce the intended distribution\n   of sizes.\n"
  title: 3. Periodic Sampling Methodology
- contents:
  - "4. Sample metrics for periodic streams\n   The sample metric presented here is
    similar to the sample metric\n   Type-P-One-way-Delay-Poisson-Stream presented
    in RFC 2679[4].\n   Singletons defined in [3] and [4] are applicable here.\n"
  - contents:
    - "4.1 Metric name\n   Type-P-One-way-Delay-Periodic-Stream\n"
    title: 4.1 Metric name
  - contents:
    - '4.2 Metric parameters

      '
    - contents:
      - "4.2.1 Global metric parameters\n   These parameters apply in the following
        sub-sections (4.2.2, 4.2.3,\n   and 4.2.4).\n   Parameters that each Singleton
        usually includes:\n     +  Src, the IP address of a host\n     +  Dst, the
        IP address of a host\n     +  IPV, the IP version (IPv4/IPv6) used in the
        measurement\n     +  dTloss, a time interval, the maximum waiting time for
        a packet\n        before declaring it lost.\n     +  packet size p(j), the
        desired number of bytes in the Type-P\n        packet, where j is the size
        index.\n   Optional parameters:\n     +  PktType, any additional qualifiers
        (transport address)\n     +  Tcons, a time interval for consolidating parameters
        collected at\n        the measurement points.\n   While a number of applications
        will use one packet size (j = 1),\n   other applications may use packets of
        different sizes (j > 1).\n   Especially in cases of congestion, it may be
        useful to use packets\n   smaller than the maximum or predominant size of
        packets in the\n   periodic stream.\n   A topology where Src and Dst are separate
        from the measurement points\n   is assumed.\n"
      title: 4.2.1 Global metric parameters
    - contents:
      - "4.2.2 Parameters collected at the measurement point MP(Src)\n   Parameters
        that each Singleton usually includes:\n   +  Tstamp(Src)[i], for each packet
        [i], the time of the packet as\n      measured at MP(Src)\n   Additional parameters:\n
        \  +  PktID(Src) [i], for each packet [i], a unique identification or\n      sequence
        number.\n   +  PktSi(Src) [i], for each packet [i], the actual packet size.\n
        \  Some applications may use packets of different sizes, either because\n
        \  of application requirements or in response to IP performance\n   experienced.\n"
      title: 4.2.2 Parameters collected at the measurement point MP(Src)
    - contents:
      - "4.2.3 Parameters collected at the measurement point MP(Dst)\n   +  Tstamp(Dst)[i],
        for each packet [i], the time of the packet as\n      measured at MP(Dst)\n
        \  +  PktID(Dst) [i], for each packet [i], a unique identification or\n      sequence
        number.\n   +  PktSi(Dst) [i], for each packet [i], the actual packet size.\n
        \  Optional parameters:\n   +  dTstop, a time interval, used to add to time
        Tf to determine when\n      to stop collecting metrics for a sample\n   +
        \ PktStatus [i], for each packet [i], the status of the packet\n      received.
        \ Possible status includes OK, packet header corrupt,\n      packet payload
        corrupt, duplicate, fragment. The criteria to\n      determine the status
        MUST be specified, if used.\n"
      title: 4.2.3 Parameters collected at the measurement point MP(Dst)
    - contents:
      - "4.2.4 Sample Metrics resulting from combining parameters at MP(Src)\n      and
        MP(Dst)\n   Using the parameters above, a delay singleton would be calculated
        as\n   follows:\n   +  Delay [i], for each packet [i], the time interval\n
        \                  Delay[i] = Tstamp(Dst)[i] - Tstamp(Src)[i]\n   For the
        following conditions, it will not be possible to compute\n   delay singletons:\n
        \  Spurious: There will be no Tstamp(Src)[i] time\n   Not received: There
        will be no Tstamp (Dst) [i]\n   Corrupt packet header: There will be no Tstamp
        (Dst) [i]\n   Duplicate:  Only the first non-corrupt copy of the packet\n
        \  received at  Dst should have Delay [i] computed.\n   A sample metric for
        average delay is as follows\n           AveDelay = (1/N)Sum(from i=1 to N,
        Delay[i])\n   assuming all packets i= 1 through N have valid singletons.\n
        \  A delay variation [5] singleton can also be computed:\n   + IPDV[i], for
        each packet [i] except the first one, delay variation\n     between successive
        packets would be calculated as\n                     IPDV[i] = Delay[i] -
        Delay [i-1]\n   IPDV[i] may be negative, zero, or positive. Delay singletons
        for\n   packets i and i-1 must be calculable or IPDV[i] is undefined.\n   An
        example metric for the IPDV sample is the range:\n                   RangeIPDV
        = max(IPDV[]) - min(IPDV[])\n"
      title: 4.2.4 Sample Metrics resulting from combining parameters at MP(Src)
    title: 4.2 Metric parameters
  - contents:
    - "4.3 High level description of the procedure to collect a sample\n   Beginning
      on or after time T0, Type-P packets are generated by Src\n   and sent to Dst
      until time Tf is reached with a nominal interval\n   between the first bit of
      successive packets of incT, as measured at\n   MP(Src).  incT may be nominal
      due to a number of reasons: variation\n   in packet generation at Src, clock
      issues (see section 4.6), etc.\n   MP(Src) records the parameters above only
      for packets with timestamps\n   between and including T0 and Tf having the required
      Src, Dst, and any\n   other qualifiers.  MP (Dst) also records for packets with
      time stamps\n   between T0 and (Tf + dTstop).\n   Optionally at a time Tf +
      Tcons (but eventually in all cases), the\n   data from MP(Src) and MP(Dst) are
      consolidated to derive the sample\n   metric results.  To prevent stopping data
      collection too soon, dTcons\n   should be greater than or equal to dTstop.  Conversely,
      to keep data\n   collection reasonably efficient, dTstop should be some reasonable\n
      \  time interval  (seconds/minutes/hours), even if dTloss is infinite or\n   extremely
      long.\n"
    title: 4.3 High level description of the procedure to collect a sample
  - contents:
    - "4.4 Discussion\n   This sampling methodology is intended to quantify the delays
      and the\n   delay variation as experienced by multimedia streams of an\n   application.
      \ Due to the definitions of these metrics, packet loss\n   status is also recorded.
      \ The nominal interval between packets\n   assesses network performance variations
      on a specific time scale.\n   There are a number of factors that should be taken
      into account when\n   collecting a sample metric of Type-P-One-way-Delay-Periodic-Stream.\n
      \  +  The interval T0 to Tf should be specified to cover a long enough\n      time
      interval to represent a reasonable use of the application\n      under test,
      yet not excessively long in the same context (e.g.\n      phone calls last longer
      than 100ms, but less than one week).\n   +  The nominal interval between packets
      (incT) and the packet size(s)\n      (p(j)) should not define an equivalent
      bit rate that exceeds the\n      capacity of the egress port of Src, the ingress
      port of Dst, or\n      the capacity of the intervening network(s), if known.
      \ There may\n      be exceptional cases to test the response of the application
      to\n      overload conditions in the transport networks, but these cases\n      should
      be strictly controlled.\n   +  Real delay values will be positive.  Therefore,
      it does not make\n      sense to report a negative value as a real delay.  However,
      an\n      individual zero or negative delay value might be useful as part of\n
      \     a stream when trying to discover a distribution of the delay\n      errors.\n
      \  +  Depending on measurement topology, delay values may be as low as\n      100
      usec to 10 msec, whereby it may be important for Src and Dst\n      to synchronize
      very closely.  GPS systems afford one way to\n      achieve synchronization
      to within several 10s of usec.  Ordinary\n      application of NTP may allow
      synchronization to within several\n      msec, but this depends on the stability
      and symmetry of delay\n      properties among the NTP agents used, and this
      delay is what we\n      are trying to measure.\n   +  A given methodology will
      have to include a way to determine\n      whether a packet was lost or whether
      delay is merely very large\n      (and  the packet is yet to arrive at Dst).
      \ The global metric\n      parameter dTloss defines a time interval such that
      delays larger\n      than dTloss are interpreted as losses.  {Comment: For many\n
      \     applications, the treatment of a large delay as infinite/loss will\n      be
      inconsequential.  A TCP data packet, for example, that arrives\n      only after
      several multiples of the usual RTT may as well have\n      been lost.}\n"
    title: 4.4 Discussion
  - contents:
    - "4.5 Additional Methodology Aspects\n   As with other Type-P-* metrics, the
      detailed methodology will depend\n   on the Type-P (e.g., protocol number, UDP/TCP
      port number, size,\n   precedence).\n"
    title: 4.5 Additional Methodology Aspects
  - contents:
    - "4.6 Errors and uncertainties\n   The description of any specific measurement
      method should include an\n   accounting and analysis of various sources of error
      or uncertainty.\n   The Framework RFC [3] provides general guidance on this
      point, but we\n   note here the following specifics related to periodic streams
      and\n   delay metrics:\n   +  Error due to variation of incT.  The reasons for
      this can be\n      uneven process scheduling, possibly due to CPU load.\n   +
      \ Errors or uncertainties due to uncertainties in the clocks of the\n      MP(Src)
      and MP(Dst) measurement points.\n   +  Errors or uncertainties due to the difference
      between 'wire time'\n      and 'host time'.\n"
    - contents:
      - "4.6.1. Errors or uncertainties related to Clocks\n   The uncertainty in a
        measurement of one-way delay is related, in\n   part, to uncertainties in
        the clocks of MP(Src) and MP(Dst).  In the\n   following, we refer to the
        clock used to measure when the packet was\n   measured at MP(Src) as the MP(Src)
        clock and we refer to the clock\n   used to measure when the packet was received
        at MP(Dst) as the\n   MP(Dst) clock.  Alluding to the notions of synchronization,
        accuracy,\n   resolution, and skew, we note the following:\n   +  Any error
        in the synchronization between the MP(Src) clock and the\n      MP(Dst) clock
        will contribute to error in the delay measurement.\n      We say that the
        MP(Src) clock and the MP(Dst) clock have a\n      synchronization error of
        Tsynch if the MP(Src) clock is Tsynch\n      ahead of the MP(Dst) clock.  Thus,
        if we know the value of Tsynch\n      exactly, we could correct for clock
        synchronization by adding\n      Tsynch to the uncorrected value of Tstamp(Dst)[i]
        - Tstamp(Src)\n      [i].\n   +  The resolution of a clock adds to uncertainty
        about any time\n      measured with it.  Thus, if the MP(Src) clock has a
        resolution of\n      10 msec, then this adds 10 msec of uncertainty to any
        time value\n      measured with it.  We will denote the resolution of the
        source\n      clock and the MP(Dst) clock as ResMP(Src) and ResMP(Dst),\n
        \     respectively.\n   +  The skew of a clock is not so much an additional
        issue as it is a\n      realization of the fact that Tsynch is itself a function
        of time.\n      Thus, if we attempt to measure or to bound Tsynch, this\n
        \     measurement or calculation must be repeated periodically.  Over\n      some
        periods of time, this function can be approximated as a\n      linear function
        plus some higher order terms; in these cases, one\n      option is to use
        knowledge of the linear component to correct the\n      clock.  Using this
        correction, the residual Tsynch is made\n      smaller, but remains a source
        of uncertainty that must be\n      accounted for.  We use the function Esynch(t)
        to denote an upper\n      bound on the uncertainty in synchronization.  Thus,
        |Tsynch(t)| <=\n      Esynch(t).\n   Taking these items together, we note
        that naive computation\n   Tstamp(Dst)[i] - Tstamp(Src) [i] will be off by
        Tsynch(t) +/-\n   (ResMP(SRc) + ResMP(Dst)).  Using the notion of Esynch(t),
        we note\n   that these clock-related problems introduce a total uncertainty
        of\n   Esynch(t)+ Rsource + Rdest.  This estimate of total clock-related\n
        \  uncertainty should be included in the error/uncertainty analysis of\n   any
        measurement implementation.\n"
      title: 4.6.1. Errors or uncertainties related to Clocks
    - contents:
      - "4.6.2. Errors or uncertainties related to wire time vs host time\n   We would
        like to measure the time between when a packet is measured\n   and time-stamped
        at MP(Src) and when it arrives and is time-stamped\n   at MP(Dst); we refer
        to these as \"wire times.\"  However, if\n   timestamps are applied by software
        on Src and Dst, then this software\n   can only directly measure the time
        between when Src generates the\n   packet just prior to sending the test packet
        and when Dst has started\n   to process the packet after having received the
        test packet; we refer\n   to these two points as \"host times\".\n   To the
        extent that the difference between wire time and host time is\n   accurately
        known, this knowledge can be used to correct for wire time\n   measurements.
        \ The corrected value more accurately estimates the\n   desired (host time)
        metric, and visa-versa.\n   To the extent, however, that the difference between
        wire time and\n   host time is uncertain, this uncertainty must be accounted
        for in an\n   analysis of a given measurement method.  We denote by Hsource
        an\n   upper bound on the uncertainty in the difference between wire time
        of\n   MP(Src) and host time on the Src host, and similarly define Hdest for\n
        \  the difference between the host time on the Dst host and the wire\n   time
        of MP(Dst).  We then note that these problems introduce a total\n   uncertainty
        of Hsource+Hdest.  This estimate of total wire-vs-host\n   uncertainty should
        be included in the error/uncertainty analysis of\n   any measurement implementation.\n"
      title: 4.6.2. Errors or uncertainties related to wire time vs host time
    - contents:
      - "4.6.3. Calibration\n   Generally, the measured values can be decomposed as
        follows:\n     measured value = true value + systematic error + random error\n
        \  If the systematic error (the constant bias in measured values) can be\n
        \  determined, it can be compensated for in the reported results.\n     reported
        value = measured value - systematic error\n   therefore\n     reported value
        = true value + random error\n   The goal of calibration is to determine the
        systematic and random\n   error generated by the instruments themselves in
        as much detail as\n   possible.  At a minimum, a bound (\"e\") should be found
        such that the\n   reported value is in the range (true value - e) to (true
        value + e)\n   at least 95 percent of the time.  We call \"e\" the calibration
        error\n   for the measurements.  It represents the degree to which the values\n
        \  produced by the measurement instrument are repeatable; that is, how\n   closely
        an actual delay of 30 ms is reported as 30 ms.  {Comment: 95\n   percent was
        chosen due to reasons discussed in [4], briefly\n   summarized as (1) some
        confidence level is desirable to be able to\n   remove outliers, which will
        be found in measuring any physical\n   property; (2) a particular confidence
        level should be specified so\n   that the results of independent implementations
        can be compared.}\n   From the discussion in the previous two sections, the
        error in\n   measurements could be bounded by determining all the individual\n
        \  uncertainties, and adding them together to form:\n           Esynch(t)
        + ResMP(Src) + ResMP(Dst) + Hsource + Hdest\n   However, reasonable bounds
        on both the clock-related uncertainty\n   captured by the first three terms
        and the host-related uncertainty\n   captured by the last two terms should
        be possible by careful design\n   techniques and calibrating the instruments
        using a known, isolated,\n   network in a lab.\n   For example, the clock-related
        uncertainties are greatly reduced\n   through the use of a GPS time source.
        \ The sum of Esynch(t) +\n   ResMP(Src) + ResMP(Dst) is small, and is also
        bounded for the\n   duration of the measurement because of the global time
        source.  The\n   host-related uncertainties, Hsource + Hdest, could be bounded
        by\n   connecting two instruments back-to-back with a high-speed serial link\n
        \  or isolated LAN segment.  In this case, repeated measurements are\n   measuring
        the same one-way delay.\n   If the test packets are small, such a network
        connection has a\n   minimal delay that may be approximated by zero.  The
        measured delay\n   therefore contains only systematic and random error in
        the\n   instrumentation.  The \"average value\" of repeated measurements is
        the\n   systematic error, and the variation is the random error.  One way
        to\n   compute the systematic error, and the random error, to a 95%\n   confidence,
        is to repeat the experiment many times - at least\n   hundreds of tests.  The
        systematic error would then be the median.\n   The random error could then
        be found by removing the systematic error\n   from the measured values.  The
        95% confidence interval would be the\n   range from the 2.5th percentile to
        the 97.5th percentile of these\n   deviations from the true value.  The calibration
        error \"e\" could then\n   be taken to be the largest absolute value of these
        two numbers, plus\n   the clock-related uncertainty.  {Comment: as described,
        this bound is\n   relatively loose since the uncertainties are added, and
        the absolute\n   value of the largest deviation is used.  As long as the resulting\n
        \  value is not a significant fraction of the measured values, it is a\n   reasonable
        bound.  If the resulting value is a significant fraction\n   of the measured
        values, then more exact methods will be needed to\n   compute the calibration
        error.}\n   Note that random error is a function of measurement load.  For\n
        \  example, if many paths will be measured by one instrument, this might\n
        \  increase interrupts, process scheduling, and disk I/O (for example,\n   recording
        the measurements), all of which may increase the random\n   error in measured
        singletons.  Therefore, in addition to minimal load\n   measurements to find
        the systematic error, calibration measurements\n   should be performed with
        the same measurement load that the\n   instruments will see in the field.\n
        \  We wish to reiterate that this statistical treatment refers to the\n   calibration
        of the instrument; it is used to \"calibrate the meter\n   stick\" and say
        how well the meter stick reflects reality.\n"
      title: 4.6.3. Calibration
    - contents:
      - "4.6.4 Errors in incT\n   The nominal interval between packets, incT, can
        vary during either\n   active or passive measurements.  In passive measurement,
        packet\n   headers may include a timestamp applied prior to most of the protocol\n
        \  stack, and the actual sending time may vary due to processor\n   scheduling.
        \ For example, H.323 systems are required to have packets\n   ready for the
        network stack within 5 ms of their ideal time.  There\n   may be additional
        variation from the network between the Src and the\n   MP(Src).  Active measurement
        systems may encounter similar errors,\n   but to a lesser extent.  These errors
        must be accounted for in some\n   types of analysis.\n"
      title: 4.6.4 Errors in incT
    title: 4.6 Errors and uncertainties
  - contents:
    - "4.7 Reporting\n   The calibration and context in which the method is used MUST
      be\n   carefully considered, and SHOULD always be reported along with metric\n
      \  results.  We next present five items to consider: the Type-P of test\n   packets,
      the threshold of delay equivalent to loss, error\n   calibration, the path traversed
      by the test packets, and background\n   conditions at Src, Dst, and the intervening
      networks during a sample.\n   This list is not exhaustive; any additional information
      that could be\n   useful in interpreting applications of the metrics should
      also be\n   reported.\n"
    - contents:
      - "4.7.1. Type-P\n   As noted in the Framework document [3], the value of a
        metric may\n   depend on the type of IP packets used to make the measurement,
        or\n   \"type-P\".  The value of Type-P-One-way-Periodic-Delay could change
        if\n   the protocol (UDP or TCP), port number, size, or arrangement for\n
        \  special treatment (e.g., IP precedence or RSVP) changes.  The exact\n   Type-P
        used to make the measurements MUST be reported.\n"
      title: 4.7.1. Type-P
    - contents:
      - "4.7.2. Threshold for delay equivalent to loss\n   In addition, the threshold
        for delay equivalent to loss (or\n   methodology to determine this threshold)
        MUST be reported.\n"
      title: 4.7.2. Threshold for delay equivalent to loss
    - contents:
      - "4.7.3. Calibration results\n   +  If the systematic error can be determined,
        it SHOULD be removed\n      from the measured values.\n   +  You SHOULD also
        report the calibration error, e, such that the\n      true value is the reported
        value plus or minus e, with 95%\n      confidence (see the last section.)\n
        \  +  If possible, the conditions under which a test packet with finite\n
        \     delay is reported as lost due to resource exhaustion on the\n      measurement
        instrument SHOULD be reported.\n"
      title: 4.7.3. Calibration results
    - contents:
      - "4.7.4. Path\n   The path traversed by the packets SHOULD be reported, if
        possible.\n   In general, it is impractical to know the precise path a given
        packet\n   takes through the network.  The precise path may be known for certain\n
        \  Type-P packets on short or stable paths.  If Type-P includes the\n   record
        route (or loose-source route) option in the IP header, and the\n   path is
        short enough, and all routers on the path support record (or\n   loose-source)
        route, then the path will be precisely recorded.\n   This may be impractical
        because the route must be short enough.  Many\n   routers do not support (or
        are not configured for) record route, and\n   use of this feature would often
        artificially worsen the performance\n   observed by removing the packet from
        common-case processing.\n   However, partial information is still valuable
        context.  For example,\n   if a host can choose between two links (and hence
        two separate routes\n   from Src to Dst), then the initial link used is valuable
        context.\n   {Comment: For example, with one commercial setup, a Src on one
        NAP\n   can reach a Dst on another NAP by either of several different\n   backbone
        networks.}\n"
      title: 4.7.4. Path
    title: 4.7 Reporting
  title: 4. Sample metrics for periodic streams
- contents:
  - "5. Additional discussion on periodic sampling\n   Fig.1 illustrates measurements
    on multiple protocol levels that are\n   relevant to this memo.  The user's focus
    is on transport quality\n   evaluation from the application point of view.  However,
    to properly\n   separate the quality contribution of the operating system and
    codec\n   on packet voice, for example, it is beneficial to be able to measure\n
    \  quality at the IP level [6].  Link layer monitoring provides a way of\n   accounting
    for link layer characteristics such as bit error rates.\n        ---------------\n
    \       | application |\n        ---------------\n        |  transport  | <--\n
    \       ---------------\n        |   network   | <--\n        ---------------\n
    \       |    link     | <--\n        ---------------\n        |   physical  |\n
    \       ---------------\n   Fig. 1: Different possibilities for performing measurements:
    a\n   protocol view.  Above, \"application\" refers to all layers above L4\n   and
    is not used in the OSI sense.\n   In general, the results of measurements may
    be influenced by\n   individual application requirements/responses related to
    the\n   following issues:\n   +  Lost packets: Applications may have varying tolerance
    to lost\n      packets.  Another consideration is the distribution of lost\n      packets
    (i.e. random or bursty).\n   +  Long delays: Many applications will consider packets
    delayed\n      longer than a certain value to be equivalent to lost packets (i.e.\n
    \     real time applications).\n   +  Duplicate packets: Some applications may
    be perturbed if duplicate\n      packets are received.\n   +  Reordering: Some
    applications may be perturbed if packets arrive\n      out of sequence.  This
    may be in addition to the possibility of\n      exceeding the \"long\" delay threshold
    as a result of being out of\n      sequence.\n   +  Corrupt packet header: Most
    applications will probably treat a\n      packet with a corrupt header as equivalent
    to a lost packet.\n   +  Corrupt packet payload: Some applications (e.g. digital
    voice\n      codecs) may accept corrupt packet payload.  In some cases, the\n
    \     packet payload may contain application specific forward error\n      correction
    (FEC) that can compensate for some level of corruption.\n   +  Spurious packet:
    Dst may receive spurious packets (i.e. packets\n      that are not sent by the
    Src as part of the metric).  Many\n      applications may be perturbed by spurious
    packets.\n   Depending, e.g., on the observed protocol level, some issues listed\n
    \  above may be indistinguishable from others by the application, it may\n   be
    important to preserve the distinction for the operators of Src,\n   Dst, and/or
    the intermediate network(s).\n"
  - contents:
    - "5.1 Measurement applications\n   This sampling method provides a way to perform
      measurements\n   irrespective of the possible QoS mechanisms utilized in the
      IP\n   network. As an example, for a QoS mechanism without hard guarantees,\n
      \  measurements may be used to ascertain that the \"best\" class gets the\n
      \  service that has been promised for the traffic class in question.\n   Moreover,
      an operator could study the quality of a cheap, low-\n   guarantee service implemented
      using possible slack bandwidth in other\n   classes. Such measurements could
      be made either in studying the\n   feasibility of a new service, or on a regular
      basis.\n   IP delivery service measurements have been discussed within the\n
      \  International Telecommunications Union (ITU).  A framework for IP\n   service
      level measurements (with references to the framework for IP\n   performance
      [3]) that is intended to be suitable for service planning\n   has been approved
      as I.380 [7].  ITU-T Recommendation I.380 covers\n   abstract definitions of
      performance metrics.  This memo describes a\n   method that is useful, both
      for service planning and end-user testing\n   purposes, in both active and passive
      measurements.\n   Delay measurements can be one-way [3,4], paired one-way, or
      round-\n   trip [8]. Accordingly, the measurements may be performed either with\n
      \  synchronized or unsynchronized Src/Dst host clocks.  Different\n   possibilities
      are listed below.\n   The reference measurement setup for all measurement types
      is shown in\n   Fig. 2.\n        ----------------< IP >--------------------\n
      \       |          |                  |          |\n      -------   -------
      \          --------    --------\n      | Src |   | MP  |           | MP   |
      \   | Dst  |\n      -------   |(Src)|           |(Dst) |    --------\n                -------
      \          --------\n                    Fig. 2: Example measurement setup.\n
      \  An example of the use of the method is a setup with a source host\n   (Src),
      a destination host (Dst), and corresponding measurement points\n   (MP(Src)
      and MP(Dst)) as shown in Figure 2.  Separate equipment for\n   measurement points
      may be used if having Src and/or Dst conduct the\n   measurement may significantly
      affect the delay performance to be\n   measured.  MP(Src) should be placed/measured
      close to the egress\n   point  of packets from Src.  MP(Dst) should be placed/measure
      close\n   to the ingress point of packets for Dst.  \"Close\" is defined as
      a\n   distance sufficiently small so that application-level performance\n   characteristics
      measured (such as delay) can be expected to follow\n   the corresponding performance
      characteristic between Src and Dst to\n   an adequate accuracy. The basic principle
      here is that measurement\n   results between MP(Src) and MP(Dst) should be the
      same as for a\n   measurement between Src and Dst, within the general error
      margin\n   target of the measurement (e.g., < 1 ms; number of lost packets is\n
      \  the same).  If this is not possible, the difference between MP-MP\n   measurement
      and Src-Dst measurement should preferably be systematic.\n   The test setup
      just described fulfills two important criteria:\n   1) The test is made with
      realistic stream metrics, emulating - for\n      example - a full-duplex Voice
      over IP (VoIP) call.\n   2) Either one-way or round-trip characteristics may
      be obtained.\n   It is also possible to have intermediate measurement points
      between\n   MP(Src) and MP(Dst), but that is beyond the scope of this document.\n"
    - contents:
      - "5.1.1 One way measurement\n   In the interests of specifying metrics that
        are as generally\n   applicable as possible, application-level measurements
        based on one-\n   way delays are used in the example metrics.  The implication
        of\n   application-level measurement for bi-directional applications, such\n
        \  as interactive multimedia conferencing, is discussed below.\n   Performing
        a single one-way measurement only yields information on\n   network behavior
        in one direction.  Moreover, the stream at the\n   network transport level
        does not emulate accurately a full-duplex\n   multimedia connection.\n"
      title: 5.1.1 One way measurement
    - contents:
      - "5.1.2 Paired one way measurement\n   Paired one way delay refers to two multimedia
        streams: Src to Dst and\n   Dst to Src for the same Src and Dst.  By way of
        example, for some\n   applications, the delay performance of each one way
        path is more\n   important than the round trip delay.  This is the case for
        delay-\n   limited signals such as VoIP.  Possible reasons for the difference\n
        \  between one-way delays is different routing of streams from Src to\n   Dst
        vs. Dst to Src.\n   For example, a paired one way measurement may show that
        Src to Dst\n   has an average delay of 30ms, while Dst to Src has an average
        delay\n   of 120ms.  To a round trip delay measurement, this example would
        look\n   like an average of 150ms delay.  Without the knowledge of the\n   asymmetry,
        we might miss a problem that the application at either end\n   may have with
        delays averaging more than 100ms.\n   Moreover, paired one way delay measurement
        emulates a full-duplex\n   VoIP call more accurately than a single one-way
        measurement only.\n"
      title: 5.1.2 Paired one way measurement
    - contents:
      - "5.1.3 Round trip measurement\n   From the point of view of periodic multimedia
        streams, round-trip\n   measurements have two advantages: they avoid the need
        of host clock\n   synchronization and they allow for a simulation of full-duplex\n
        \  communication.  The former aspect means that a measurement is easily\n
        \  performed, since no special equipment or NTP setup is needed.  The\n   latter
        property means that measurement streams are transmitted in\n   both directions.
        \ Thus, the measurement provides information on\n   quality of service as
        experienced by two-way applications.\n   The downsides of round-trip measurement
        are the need for more\n   bandwidth than a one-way test and more complex accounting
        of packet\n   loss.  Moreover, the stream that is returning towards the original\n
        \  sender may be more bursty than the one on the first \"leg\" of the\n   round-trip
        journey.  The last issue, however, means in practice that\n   the returning
        stream may experience worse QoS than the out-going one,\n   and the performance
        estimates thus obtained are pessimistic ones.\n   The possibility of asymmetric
        routing and queuing must be taken into\n   account during an analysis of the
        results.\n   Note that with suitable arrangements, round-trip measurements
        may be\n   performed using paired one way measurements.\n"
      title: 5.1.3 Round trip measurement
    title: 5.1 Measurement applications
  - contents:
    - "5.2 Statistics calculable from one sample\n   Some statistics may be particularly
      relevant to applications\n   simulated by periodic streams, such as the range
      of delay values\n   recorded during the sample.\n   For example, a sample metric
      generates 100 packets at MP(Src) with\n   the following measurements at MP(Dst):\n
      \  +  80 packets received with delay [i] <= 20 ms\n   +   8 packets received
      with delay [i] > 20 ms\n   +   5 packets received with corrupt packet headers\n
      \  +   4 packets from MP(Src) with no matching packet recorded at\n      MP(Dst)
      (effectively lost)\n   +   3 packets received with corrupt packet payload and
      delay\n      [i] <= 20 ms\n   +   2 packets that duplicate one of the 80 packets
      received correctly\n      as indicated in the first item\n   For this example,
      packets are considered acceptable if they are\n   received with less than or
      equal to 20ms delays and without corrupt\n   packet headers or packet payload.
      \ In this case, the percentage of\n   acceptable packets is 80/100 = 80%.\n
      \  For a different application that will accept packets with corrupt\n   packet
      payload and no delay bounds (so long as the packet is\n   received), the percentage
      of acceptable packets is (80+8+3)/100 =\n   91%.\n"
    title: 5.2 Statistics calculable from one sample
  - contents:
    - "5.3 Statistics calculable from multiple samples\n   There may be value in running
      multiple tests using this method to\n   collect a \"sample of samples\".  For
      example, it may be more\n   appropriate to simulate 1,000 two-minute VoIP calls
      rather than a\n   single 2,000 minute call.  When considering a collection of
      multiple\n   samples, issues like the interval between samples (e.g. minutes,\n
      \  hours), composition of samples (e.g. equal Tf-T0 duration, different\n   packet
      sizes), and network considerations (e.g. run different samples\n   over different
      intervening link-host combinations) should be taken\n   into account.  For items
      like the interval between samples, the usage\n   pattern for the application
      of interest should be considered.\n   When computing statistics for multiple
      samples, more general\n   statistics (e.g. median, percentile, etc.) may have
      relevance with a\n   larger number of packets.\n"
    title: 5.3 Statistics calculable from multiple samples
  - contents:
    - "5.4 Background conditions\n   In many cases, the results may be influenced
      by conditions at Src,\n   Dst, and/or any intervening networks.  Factors that
      may affect the\n   results include: traffic levels and/or bursts during the
      sample, link\n   and/or host failures, etc.  Information about the background\n
      \  conditions may only be available by external means (e.g. phone calls,\n   television)
      and may only become available days after samples are\n   taken.\n"
    title: 5.4 Background conditions
  - contents:
    - "5.5 Considerations related to delay\n   For interactive multimedia sessions,
      end-to-end delay is an important\n   factor.  Too large a delay reduces the
      quality of the multimedia\n   session as perceived by the participants.  One
      approach for managing\n   end-to-end delays on an Internet path involving heterogeneous
      link\n   layer technologies is to use per-domain delay quotas (e.g. 50 ms for\n
      \  a particular IP domain).  However, this scheme has clear\n   inefficiencies,
      and can over-constrain the problem of achieving some\n   end-to-end delay objective.
      \ A more flexible implementation ought to\n   address issues like the possibility
      of asymmetric delays on paths,\n   and sensitivity of an application to delay
      variations in a given\n   domain. There are several alternatives as to the delay
      statistic one\n   ought to use in managing end-to-end QoS.  This question, although\n
      \  very interesting, is not within the scope of this memo and is not\n   discussed
      further here.\n"
    title: 5.5 Considerations related to delay
  title: 5. Additional discussion on periodic sampling
- contents:
  - '6. Security Considerations

    '
  - contents:
    - "6.1 Denial of Service Attacks\n   This method generates a periodic stream of
      packets from one host\n   (Src) to another host (Dst) through intervening networks.
      \ This\n   method could be abused for denial of service attacks directed at
      Dst\n   and/or the intervening network(s).\n   Administrators of Src, Dst, and
      the intervening network(s) should\n   establish bilateral or multi-lateral agreements
      regarding the timing,\n   size, and frequency of collection of sample metrics.
      \ Use of this\n   method in excess of the terms agreed between the participants
      may be\n   cause for immediate rejection, discard of packets, or other\n   escalation
      procedures defined between the affected parties.\n"
    title: 6.1 Denial of Service Attacks
  - contents:
    - "6.2 User data confidentiality\n   Active use of this method generates packets
      for a sample, rather than\n   taking samples based on user data, and does not
      threaten user data\n   confidentiality.  Passive measurement must restrict attention
      to the\n   headers of interest.  Since user payloads may be temporarily stored\n
      \  for length analysis, suitable precautions MUST be taken to keep this\n   information
      safe and confidential.\n"
    title: 6.2 User data confidentiality
  - contents:
    - "6.3 Interference with the metric\n   It may be possible to identify that a
      certain packet or stream of\n   packets is part of a sample.  With that knowledge
      at Dst and/or the\n   intervening networks, it is possible to change the processing
      of the\n   packets (e.g. increasing or decreasing delay) that may distort the\n
      \  measured performance.  It may also be possible to generate additional\n   packets
      that appear to be part of the sample metric.  These\n   additional packets are
      likely to perturb the results of the sample\n   measurement.\n   To discourage
      the kind of interference mentioned above, packet\n   interference checks, such
      as cryptographic hash, MAY be used.\n"
    title: 6.3 Interference with the metric
  title: 6. Security Considerations
- contents:
  - "7. IANA Considerations\n   Since this method and metric do not define a protocol
    or well-known\n   values, there are no IANA considerations in this memo.\n"
  title: 7. IANA Considerations
- contents:
  - "8. Normative References\n   [1]  Bradner, S., \"The Internet Standards Process
    -- Revision 3\", BCP\n        9, RFC 2026, October 1996.\n   [2]  Bradner, S.,
    \"Key words for use in RFCs to Indicate Requirement\n        Levels\", BCP 14,
    RFC 2119, March 1997.\n   [3]  Paxson, V., Almes, G., Mahdavi, J. and M. Mathis,
    \"Framework for\n        IP Performance Metrics\", RFC 2330, May 1998.\n   [4]
    \ Almes, G., Kalidindi, S. and M. Zekauskas, \"A one-way delay\n        metric
    for IPPM\", RFC 2679, September 1999.\n   [5]  Demichelis, C. and P. Chimento,
    \"IP Packet Delay Variation\n        Metric for IP Performance Metrics (IPPM)\",
    RFC 3393, November\n        2002.\n"
  title: 8. Normative References
- contents:
  - "9. Informative References\n   [6] \"End-to-end Quality of Service in TIPHON systems;
    Part 5: Quality\n        of Service (QoS) measurement methodologies\", ETSI TS
    101 329-5\n        V1.1.2, January 2002.\n   [7]  International Telecommunications
    Union, \"Internet protocol data\n        communication service _ IP packet transfer
    and availability\n        performance parameters\", Telecommunications Sector\n
    \       Recommendation I.380 (re-numbered Y.1540), February 1999.\n   [8]  Almes,
    G., Kalidindi, S. and M. Zekauskas, \"A round-trip delay\n        metric for IPPM\",
    RFC 2681, September 1999.\n"
  title: 9. Informative References
- contents:
  - "10. Acknowledgments\n   The authors wish to thank the chairs of the IPPM WG (Matt
    Zekauskas\n   and Merike Kaeo) for comments that have made the present document\n
    \  more clear and focused.  Howard Stanislevic and Will Leland have also\n   presented
    useful comments and questions.  We also gratefully\n   acknowledge Henk Uijterwaal's
    continued challenge to develop the\n   motivation for this method.  The authors
    have built on the\n   substantial foundation laid by the authors of the framework
    for IP\n   performance [3].\n"
  title: 10. Acknowledgments
- contents:
  - "11. Author's Addresses\n   Vilho Raisanen\n   Nokia Networks\n   P.O. Box 300\n
    \  FIN-00045 Nokia Group\n   Finland\n   Phone: +358 7180 8000\n   Fax:   +358
    9 4376 6852\n   EMail: Vilho.Raisanen@nokia.com\n   Glenn Grotefeld\n   Motorola,
    Inc.\n   1501 W. Shure Drive, MS 2F1\n   Arlington Heights, IL 60004 USA\n   Phone:
    \ +1 847 435-0730\n   Fax:    +1 847 632-6800\n   EMail: g.grotefeld@motorola.com\n
    \  Al Morton\n   AT&T Labs\n   Room D3 - 3C06\n   200 Laurel Ave. South\n   Middletown,
    NJ 07748 USA\n   Phone:  +1 732 420 1571\n   Fax:    +1 732 368 1192\n   EMail:
    acmorton@att.com\n"
  title: 11. Author's Addresses
- contents:
  - "12.  Full Copyright Statement\n   Copyright (C) The Internet Society (2002).
    \ All Rights Reserved.\n   This document and translations of it may be copied
    and furnished to\n   others, and derivative works that comment on or otherwise
    explain it\n   or assist in its implementation may be prepared, copied, published\n
    \  and distributed, in whole or in part, without restriction of any\n   kind,
    provided that the above copyright notice and this paragraph are\n   included on
    all such copies and derivative works.  However, this\n   document itself may not
    be modified in any way, such as by removing\n   the copyright notice or references
    to the Internet Society or other\n   Internet organizations, except as needed
    for the purpose of\n   developing Internet standards in which case the procedures
    for\n   copyrights defined in the Internet Standards process must be\n   followed,
    or as required to translate it into languages other than\n   English.\n   The
    limited permissions granted above are perpetual and will not be\n   revoked by
    the Internet Society or its successors or assigns.\n   This document and the information
    contained herein is provided on an\n   \"AS IS\" basis and THE INTERNET SOCIETY
    AND THE INTERNET ENGINEERING\n   TASK FORCE DISCLAIMS ALL WARRANTIES, EXPRESS
    OR IMPLIED, INCLUDING\n   BUT NOT LIMITED TO ANY WARRANTY THAT THE USE OF THE
    INFORMATION\n   HEREIN WILL NOT INFRINGE ANY RIGHTS OR ANY IMPLIED WARRANTIES
    OF\n   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.\n"
  title: 12.  Full Copyright Statement
- contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided
    by the\n   Internet Society.\n"
  title: Acknowledgement
