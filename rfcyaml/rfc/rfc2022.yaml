- contents:
  - '       Support for Multicast over UNI 3.0/3.1 based ATM Networks.

    '
  title: __initial_text__
- contents:
  - "Status of this Memo\n   This document specifies an Internet standards track protocol
    for the\n   Internet community, and requests discussion and suggestions for\n
    \  improvements.  Please refer to the current edition of the \"Internet\n   Official
    Protocol Standards\" (STD 1) for the standardization state\n   and status of this
    protocol.  Distribution of this memo is unlimited\n"
  title: Status of this Memo
- contents:
  - "Abstract\n   Mapping the connectionless IP multicast service over the connection\n
    \  oriented ATM services provided by UNI 3.0/3.1 is a non-trivial task.\n   This
    memo describes a mechanism to support the multicast needs of\n   Layer 3 protocols
    in general, and describes its application to IP\n   multicasting in particular.\n
    \  ATM based IP hosts and routers use a Multicast Address Resolution\n   Server
    (MARS) to support RFC 1112 style Level 2 IP multicast over the\n   ATM Forum's
    UNI 3.0/3.1 point to multipoint connection service.\n   Clusters of endpoints
    share a MARS and use it to track and\n   disseminate information identifying the
    nodes listed as receivers for\n   given multicast groups. This allows endpoints
    to establish and manage\n   point to multipoint VCs when transmitting to the group.\n
    \  The MARS behaviour allows Layer 3 multicasting to be supported using\n   either
    meshes of VCs or ATM level multicast servers. This choice may\n   be made on a
    per-group basis, and is transparent to the endpoints.\n"
  title: Abstract
- contents:
  - "Table of Contents\n   1. Introduction.................................................
    \  4\n    1.1 The Multicast Address Resolution Server (MARS).............   5\n
    \   1.2 The ATM level multicast Cluster............................   5\n    1.3
    Document overview..........................................   6\n    1.4 Conventions................................................
    \  7\n   2. The IP multicast service model...............................   7\n
    \  3. UNI 3.0/3.1 support for intra-cluster multicasting...........   8\n    3.1
    VC meshes..................................................   9\n    3.2 Multicast
    Servers..........................................   9\n    3.3 Tradeoffs..................................................
    \ 10\n    3.4 Interaction with local UNI 3.0/3.1 signalling entity.......  11\n
    \  4. Overview of the MARS.........................................  12\n    4.1
    Architecture...............................................  12\n    4.2 Control
    message format.....................................  12\n    4.3 Fixed header
    fields in MARS control messages...............  13\n      4.3.1 Hardware type..........................................
    \ 14\n      4.3.2 Protocol type..........................................  14\n
    \     4.3.3 Checksum...............................................  15\n      4.3.4
    Extensions Offset......................................  15\n      4.3.5 Operation
    code.........................................  16\n      4.3.6 Reserved...............................................
    \ 16\n   5. Endpoint (MARS client) interface behaviour...................  16\n
    \   5.1 Transmit side behaviour....................................  17\n      5.1.1
    Retrieving Group Membership from the MARS..............  18\n      5.1.2 MARS_REQUEST,
    MARS_MULTI, and MARS_NAK messages........  20\n      5.1.3 Establishing the outgoing
    multipoint VC................  22\n      5.1.4 Monitoring updates on ClusterControlVC.................
    \ 24\n        5.1.4.1 Updating the active VCs............................  24\n
    \       5.1.4.2 Tracking the Cluster Sequence Number...............  25\n      5.1.5
    Revalidating a VC's leaf nodes.........................  26\n        5.1.5.1 When
    leaf node drops itself........................  27\n        5.1.5.2 When a jump
    is detected in the CSN.................  27\n      5.1.6 'Migrating' the outgoing
    multipoint VC.................  27\n    5.2. Receive side behaviour....................................
    \ 29\n      5.2.1 Format of the MARS_JOIN and MARS_LEAVE Messages........  30\n
    \       5.2.1.1 Important IPv4 default values......................  32\n      5.2.2
    Retransmission of MARS_JOIN and MARS_LEAVE messages....  33\n      5.2.3 Cluster
    member registration and deregistration.........  34\n    5.3 Support for Layer
    3 group management.......................  34\n    5.4 Support for redundant/backup
    MARS entities.................  36\n      5.4.1 First response to MARS problems........................
    \ 36\n      5.4.2 Connecting to a backup MARS............................  37\n
    \     5.4.3 Dynamic backup lists, and soft redirects...............  37\n    5.5
    Data path LLC/SNAP encapsulations..........................  40\n      5.5.1 Type
    #1 encapsulation..................................  40\n      5.5.2 Type #2 encapsulation..................................
    \ 41\n      5.5.3 A Type #1 example......................................  42\n
    \  6. The MARS in greater detail...................................  42\n    6.1
    Basic interface to Cluster members.........................  43\n      6.1.1 Response
    to MARS_REQUEST...............................  43\n      6.1.2 Response to MARS_JOIN
    and MARS_LEAVE...................  43\n      6.1.3 Generating MARS_REDIRECT_MAP...........................
    \ 45\n      6.1.4 Cluster Sequence Numbers...............................  45\n
    \   6.2 MARS interface to Multicast Servers (MCSs).................  46\n      6.2.1
    MARS_REQUESTs for MCS supported groups.................  47\n      6.2.2 MARS_MSERV
    and MARS_UNSERV messages....................  47\n      6.2.3 Registering a Multicast
    Server (MCS)...................  49\n      6.2.4 Modified response to MARS_JOIN
    and MARS_LEAVE..........  49\n      6.2.5 Sequence numbers for ServerControlVC
    traffic...........  51\n    6.3 Why global sequence numbers?...............................
    \ 52\n    6.4 Redundant/Backup MARS Architectures........................  52\n
    \  7. How an MCS utilises a MARS...................................  53\n    7.1
    Association with a particular Layer 3 group................  53\n    7.2 Termination
    of incoming VCs................................  54\n    7.3 Management of outgoing
    VC..................................  54\n    7.4 Use of a backup MARS.......................................
    \ 54\n   8. Support for IP multicast routers.............................  54\n
    \   8.1 Forwarding into a Cluster..................................  55\n    8.2
    Joining in 'promiscuous' mode..............................  55\n    8.3 Forwarding
    across the cluster..............................  56\n    8.4 Joining in 'semi-promiscous'
    mode..........................  56\n    8.5 An alternative to IGMP Queries.............................
    \ 57\n    8.6 CMIs across multiple interfaces............................  58\n
    \  9. Multiprotocol applications of the MARS and MARS clients......  59\n   10.
    Supplementary parameter processing..........................  60\n    10.1 Interpreting
    the mar$extoff field.........................  60\n    10.2 The format of TLVs........................................
    \ 60\n    10.3 Processing MARS messages with TLVs........................  62\n
    \   10.4 Initial set of TLV elements...............................  62\n   11.
    Key Decisions and open issues...............................  62\n   Security
    Considerations.........................................  65\n   Acknowledgments.................................................
    \ 65\n   Author's Address................................................  65\n
    \  References......................................................  66\n   Appendix
    A. Hole punching algorithms............................  67\n   Appendix B. Minimising
    the impact of IGMP in IPv4 environments..  69\n   Appendix C. Further comments
    on 'Clusters'......................  71\n   Appendix D. TLV list parsing algorithm..........................
    \ 72\n   Appendix E. Summary of timer values.............................  73\n
    \  Appendix F. Pseudo code for MARS operation......................  74\n"
  title: Table of Contents
- contents:
  - "1.  Introduction.\n   Multicasting is the process whereby a source host or protocol
    entity\n   sends a packet to multiple destinations simultaneously using a\n   single,
    local 'transmit' operation. The more familiar cases of\n   Unicasting and Broadcasting
    may be considered to be special cases of\n   Multicasting (with the packet delivered
    to one destination, or 'all'\n   destinations, respectively).\n   Most network
    layer models, like the one described in RFC 1112 [1] for\n   IP multicasting,
    assume sources may send their packets to abstract\n   'multicast group addresses'.
    \ Link layer support for such an\n   abstraction is assumed to exist, and is provided
    by technologies such\n   as Ethernet.\n   ATM is being utilized as a new link
    layer technology to support a\n   variety of protocols, including IP. With RFC
    1483 [2] the IETF\n   defined a multiprotocol mechanism for encapsulating and
    transmitting\n   packets using AAL5 over ATM Virtual Channels (VCs). However,
    the ATM\n   Forum's currently published signalling specifications (UNI 3.0 [8]\n
    \  and UNI 3.1 [4]) does not provide the multicast address abstraction.\n   Unicast
    connections are supported by point to point, bidirectional\n   VCs. Multicasting
    is supported through point to multipoint\n   unidirectional VCs. The key limitation
    is that the sender must have\n   prior knowledge of each intended recipient, and
    explicitly establish\n   a VC with itself as the root node and the recipients
    as the leaf\n   nodes.\n   This document has two broad goals:\n      Define a
    group address registration and membership distribution\n      mechanism that allows
    UNI 3.0/3.1 based networks to support the\n      multicast service of protocols
    such as IP.\n      Define specific endpoint behaviours for managing point to\n
    \     multipoint VCs to achieve multicasting of layer 3 packets.\n   As the IETF
    is currently in the forefront of using wide area\n   multicasting this document's
    descriptions will often focus on IP\n   service model of RFC 1112.  A final chapter
    will note the\n   multiprotocol application of the architecture.\n   This document
    avoids discussion of one highly non-trivial aspect of\n   using ATM - the specification
    of QoS for VCs being established in\n   response to higher layer needs. Research
    in this area is still very\n   formative [7], and so it is assumed that future
    documents will\n   clarify the mapping of QoS requirements to VC establishment.
    The\n   default at this time is that VCs are established with a request for\n
    \  Unspecified Bit Rate (UBR) service, as typified by the IETF's use of\n   VCs
    for unicast IP, described in RFC 1755 [6].\n"
  - contents:
    - "1.1  The Multicast Address Resolution Server (MARS).\n   The Multicast Address
      Resolution Server (MARS) is an extended analog\n   of the ATM ARP Server introduced
      in RFC 1577 [3].  It acts as a\n   registry, associating layer 3 multicast group
      identifiers with the\n   ATM interfaces representing the group's members.  MARS
      messages\n   support the distribution of multicast group membership information\n
      \  between MARS and endpoints (hosts or routers).  Endpoint address\n   resolution
      entities query the MARS when a layer 3 address needs to be\n   resolved to the
      set of ATM endpoints making up the group at any one\n   time. Endpoints keep
      the MARS informed when they need to join or\n   leave particular layer 3 groups.
      \ To provide for asynchronous\n   notification of group membership changes the
      MARS manages a point to\n   multipoint VC out to all endpoints desiring multicast
      support\n   Valid arguments can be made for two different approaches to ATM
      level\n   multicasting of layer 3 packets - through meshes of point to\n   multipoint
      VCs, or ATM level multicast servers (MCS). The MARS\n   architecture allows
      either VC meshes or MCSs to be used on a per-\n   group basis.\n"
    title: 1.1  The Multicast Address Resolution Server (MARS).
  - contents:
    - "1.2  The ATM level multicast Cluster.\n   Each MARS manages a 'cluster' of
      ATM-attached endpoints. A Cluster is\n   defined as\n      The set of ATM interfaces
      choosing to participate in direct ATM\n      connections to achieve multicasting
      of AAL_SDUs between\n      themselves.\n   In practice, a Cluster is the set
      of endpoints that choose to use the\n   same MARS to register their memberships
      and receive their updates\n   from.\n   By implication of this definition, traffic
      between interfaces\n   belonging to different Clusters passes through an inter-cluster\n
      \  device. (In the IP world an inter-cluster device would be an IP\n   multicast
      router with logical interfaces into each Cluster.) This\n   document explicitly
      avoids specifying the nature of inter-cluster\n   (layer 3) routing protocols.\n
      \  The mapping of clusters to other constrained sets of endpoints (such\n   as
      unicast Logical IP Subnets) is left to each network administrator.\n   However,
      for the purposes of conformance with this document network\n   administrators
      MUST ensure that each Logical IP Subnet (LIS) is\n   served by a separate MARS,
      creating a one-to-one mapping between\n   cluster and unicast LIS.  IP multicast
      routers then interconnect each\n   LIS as they do with conventional subnets.
      (Relaxation of this\n   restriction MAY only occur after future research on
      the interaction\n   between existing layer 3 multicast routing protocols and
      unicast\n   subnet boundaries.)\n   The term 'Cluster Member' will be used in
      this document to refer to\n   an endpoint that is currently using a MARS for
      multicast support.\n   Thus potential scope of a cluster may be the entire membership
      of a\n   LIS, while the actual scope of a cluster depends on which endpoints\n
      \  are actually cluster members at any given time.\n"
    title: 1.2  The ATM level multicast Cluster.
  - contents:
    - "1.3  Document overview.\n   This document assumes an understanding of concepts
      explained in\n   greater detail in RFC 1112, RFC 1577, UNI 3.0/3.1, and RFC
      1755 [6].\n   Section 2 provides an overview of IP multicast and what RFC 1112\n
      \  required from Ethernet.\n   Section 3 describes in more detail the multicast
      support services\n   offered by UNI 3.0/3.1, and outlines the differences between
      VC\n   meshes and multicast servers (MCSs) as mechanisms for distributing\n
      \  packets to multiple destinations.\n   Section 4 provides an overview of the
      MARS and its relationship to\n   ATM endpoints. This section also discusses
      the encapsulation and\n   structure of MARS control messages.\n   Section 5
      substantially defines the entire cluster member endpoint\n   behaviour, on both
      receive and transmit sides. This includes both\n   normal operation and error
      recovery.\n   Section 6 summarises the required behaviour of a MARS.\n   Section
      7 looks at how a multicast server (MCS) interacts with a\n   MARS.\n   Section
      8 discusses how IP multicast routers may make novel use of\n   promiscuous and
      semi-promiscuous group joins. Also discussed is a\n   mechanism designed to
      reduce the amount of IGMP traffic issued by\n   routers.\n   Section 9 discusses
      how this document applies in the more general\n   (non-IP) case.\n   Section
      10 summarises the key proposals, and identifies areas for\n   future research
      that are generated by this MARS architecture.\n   The appendices provide discussion
      on issues that arise out of the\n   implementation of this document. Appendix
      A discusses MARS and\n   endpoint algorithms for parsing MARS messages. Appendix
      B describes\n   the particular problems introduced by the current IGMP paradigms,
      and\n   possible interim work-arounds.  Appendix C discusses the 'cluster'\n
      \  concept in further detail, while Appendix D briefly outlines an\n   algorithm
      for parsing TLV lists.  Appendix E summarises various timer\n   values used
      in this document, and Appendix F provides example\n   pseudo-code for a MARS
      entity.\n"
    title: 1.3  Document overview.
  - contents:
    - "1.4  Conventions.\n   In this document the following coding and packet representation
      rules\n   are used:\n      All multi-octet parameters are encoded in big-endian
      form (i.e.\n      the most significant octet comes first).\n      In all multi-bit
      parameters bit numbering begins at 0 for the\n      least significant bit when
      stored in memory (i.e. the n'th bit has\n      weight of 2^n).\n      A bit
      that is 'set', 'on', or 'one' holds the value 1.\n      A bit that is 'reset',
      'off', 'clear', or 'zero' holds the value\n      0.\n"
    title: 1.4  Conventions.
  title: 1.  Introduction.
- contents:
  - "2.  Summary of the IP multicast service model.\n   Under IP version 4 (IPv4),
    addresses in the range between 224.0.0.0\n   and 239.255.255.255 (224.0.0.0/4)
    are termed 'Class D' or 'multicast\n   group' addresses. These abstractly represent
    all the IP hosts in the\n   Internet (or some constrained subset of the Internet)
    who have\n   decided to 'join' the specified group.\n   RFC1112 requires that
    a multicast-capable IP interface must support\n   the transmission of IP packets
    to an IP multicast group address,\n   whether or not the node considers itself
    a 'member' of that group.\n   Consequently, group membership is effectively irrelevant
    to the\n   transmit side of the link layer interfaces. When Ethernet is used as\n
    \  the link layer (the example used in RFC1112), no address resolution\n   is
    required to transmit packets. An algorithmic mapping from IP\n   multicast address
    to Ethernet multicast address is performed locally\n   before the packet is sent
    out the local interface in the same 'send\n   and forget' manner as a unicast
    IP packet.\n   Joining and Leaving an IP multicast group is more explicit on the\n
    \  receive side - with the primitives JoinLocalGroup and LeaveLocalGroup\n   affecting
    what groups the local link layer interface should accept\n   packets from. When
    the IP layer wants to receive packets from a\n   group, it issues JoinLocalGroup.
    When it no longer wants to receive\n   packets, it issues LeaveLocalGroup. A key
    point to note is that\n   changing state is a local issue, it has no effect on
    other hosts\n   attached to the Ethernet.\n   IGMP is defined in RFC 1112 to support
    IP multicast routers attached\n   to a given subnet. Hosts issue IGMP Report messages
    when they perform\n   a JoinLocalGroup, or in response to an IP multicast router
    sending an\n   IGMP Query. By periodically transmitting queries IP multicast routers\n
    \  are able to identify what IP multicast groups have non-zero\n   membership
    on a given subnet.\n   A specific IP multicast address, 224.0.0.1, is allocated
    for the\n   transmission of IGMP Query messages. Host IP layers issue a\n   JoinLocalGroup
    for 224.0.0.1 when they intend to participate in IP\n   multicasting, and issue
    a LeaveLocalGroup for 224.0.0.1 when they've\n   ceased participating in IP multicasting.\n
    \  Each host keeps a list of IP multicast groups it has been\n   JoinLocalGroup'd
    to. When a router issues an IGMP Query on 224.0.0.1\n   each host begins to send
    IGMP Reports for each group it is a member\n   of. IGMP Reports are sent to the
    group address, not 224.0.0.1, \"so\n   that other members of the same group on
    the same network can overhear\n   the Report\" and not bother sending one of their
    own. IP multicast\n   routers conclude that a group has no members on the subnet
    when IGMP\n   Queries no longer elicit associated replies.\n"
  title: 2.  Summary of the IP multicast service model.
- contents:
  - "3. UNI 3.0/3.1 support for intra-cluster multicasting.\n   For the purposes of
    the MARS protocol, both UNI 3.0 and UNI 3.1\n   provide equivalent support for
    multicasting. Differences between UNI\n   3.0 and UNI 3.1 in required signalling
    elements are covered in RFC\n   1755.\n   This document will describe its operation
    in terms of 'generic'\n   functions that should be available to clients of a UNI
    3.0/3.1\n   signalling entity in a given ATM endpoint. The ATM model broadly\n
    \  describes an 'AAL User' as any entity that establishes and manages\n   VCs
    and underlying AAL services to exchange data. An IP over ATM\n   interface is
    a form of 'AAL User' (although the default LLC/SNAP\n   encapsulation mode specified
    in RFC1755 really requires that an 'LLC\n   entity' is the AAL User, which in
    turn supports the IP/ATM\n   interface).\n   The most fundamental limitations
    of UNI 3.0/3.1's multicast support\n   are:\n      Only point to multipoint, unidirectional
    VCs may be established.\n      Only the root (source) node of a given VC may add
    or remove leaf\n      nodes.\n   Leaf nodes are identified by their unicast ATM
    addresses.  UNI\n   3.0/3.1 defines two ATM address formats - native E.164 and
    NSAP\n   (although it must be stressed that the NSAP address is so called\n   because
    it uses the NSAP format - an ATM endpoint is NOT a Network\n   layer termination
    point).  In UNI 3.0/3.1 an 'ATM Number' is the\n   primary identification of an
    ATM endpoint, and it may use either\n   format. Under some circumstances an ATM
    endpoint must be identified\n   by both a native E.164 address (identifying the
    attachment point of a\n   private network to a public network), and an NSAP address
    ('ATM\n   Subaddress') identifying the final endpoint within the private\n   network.
    For the rest of this document the term will be used to mean\n   either a single
    'ATM Number' or an 'ATM Number' combined with an 'ATM\n   Subaddress'.\n"
  - contents:
    - "3.1 VC meshes.\n   The most fundamental approach to intra-cluster multicasting
      is the\n   multicast VC mesh.  Each source establishes its own independent point\n
      \  to multipoint VC (a single multicast tree) to the set of leaf nodes\n   (destinations)
      that it has been told are members of the group it\n   wishes to send packets
      to.\n   Interfaces that are both senders and group members (leaf nodes) to a\n
      \  given group will originate one point to multipoint VC, and terminate\n   one
      VC for every other active sender to the group. This criss-\n   crossing of VCs
      across the ATM network gives rise to the name 'VC\n   mesh'.\n"
    title: 3.1 VC meshes.
  - contents:
    - "3.2 Multicast Servers.\n   An alternative model has each source establish a
      VC to an\n   intermediate node - the multicast server (MCS). The multicast server\n
      \  itself establishes and manages a point to multipoint VC out to the\n   actual
      desired destinations.\n   The MCS reassembles AAL_SDUs arriving on all the incoming
      VCs, and\n   then queues them for transmission on its single outgoing point
      to\n   multipoint VC. (Reassembly of incoming AAL_SDUs is required at the\n
      \  multicast server as AAL5 does not support cell level multiplexing of\n   different
      AAL_SDUs on a single outgoing VC.)\n   The leaf nodes of the multicast server's
      point to multipoint VC must\n   be established prior to packet transmission,
      and the multicast server\n   requires an external mechanism to identify them.
      A side-effect of\n   this method is that ATM interfaces that are both sources
      and group\n   members will receive copies of their own packets back from the
      MCS\n   (An alternative method is for the multicast server to explicitly\n   retransmit
      packets on individual VCs between itself and group\n   members. A benefit of
      this second approach is that the multicast\n   server can ensure that sources
      do not receive copies of their own\n   packets.)\n   The simplest MCS pays no
      attention to the contents of each AAL_SDU.\n   It is purely an AAL/ATM level
      device. More complex MCS architectures\n   (where a single endpoint serves multiple
      layer 3 groups) are\n   possible, but are beyond the scope of this document.
      More detailed\n   discussion is provided in section 7.\n"
    title: 3.2 Multicast Servers.
  - contents:
    - "3.3 Tradeoffs.\n   Arguments over the relative merits of VC meshes and multicast
      servers\n   have raged for some time. Ultimately the choice depends on the\n
      \  relative trade-offs a system administrator must make between\n   throughput,
      latency, congestion, and resource consumption. Even\n   criteria such as latency
      can mean different things to different\n   people - is it end to end packet
      time, or the time it takes for a\n   group to settle after a membership change?
      The final choice depends\n   on the characteristics of the applications generating
      the multicast\n   traffic.\n   If we focussed on the data path we might prefer
      the VC mesh because\n   it lacks the obvious single congestion point of an MCS.
      \ Throughput\n   is likely to be higher, and end to end latency lower, because
      the\n   mesh lacks the intermediate AAL_SDU reassembly that must occur in\n
      \  MCSs. The underlying ATM signalling system also has greater\n   opportunity
      to ensure optimal branching points at ATM switches along\n   the multicast trees
      originating on each source.\n   However, resource consumption will be higher.
      Every group member's\n   ATM interface must terminate a VC per sender (consuming
      on-board\n   memory for state information, instance of an AAL service, and\n
      \  buffering in accordance with the vendors particular architecture). On\n   the
      contrary, with a multicast server only 2 VCs (one out, one in)\n   are required,
      independent of the number of senders. The allocation of\n   VC related resources
      is also lower within the ATM cloud when using a\n   multicast server. These
      points may be considered to have merit in\n   environments where VCs across
      the UNI or within the ATM cloud are\n   valuable (e.g. the ATM provider charges
      on a per VC basis), or AAL\n   contexts are limited in the ATM interfaces of
      endpoints.\n   If we focus on the signalling load then MCSs have the advantage
      when\n   faced with dynamic sets of receivers. Every time the membership of
      a\n   multicast group changes (a leaf node needs to be added or dropped),\n
      \  only a single point to multipoint VC needs to be modified when using\n   an
      MCS. This generates a single signalling event across the MCS's\n   UNI. However,
      when membership change occurs in a VC mesh, signalling\n   events occur at the
      UNIs of every traffic source - the transient\n   signalling load scales with
      the number of sources. This has obvious\n   ramifications if you define latency
      as the time for a group's\n   connectivity to stabilise after change (especially
      as the number of\n   senders increases).\n   Finally, as noted above, MCSs introduce
      a 'reflected packet' problem,\n   which requires additional per-AAL_SDU information
      to be carried in\n   order for layer 3 sources to detect their own AAL_SDUs
      coming back.\n   The MARS architecture allows system administrators to utilize
      either\n   approach on a group by group basis.\n"
    title: 3.3 Tradeoffs.
  - contents:
    - "3.4 Interaction with local UNI 3.0/3.1 signalling entity.\n   The following
      generic signalling functions are presumed to be\n   available to local AAL Users:\n
      \  L_CALL_RQ     - Establish a unicast VC to a specific endpoint.\n   L_MULTI_RQ
      \   - Establish multicast VC to a specific endpoint.\n   L_MULTI_ADD   - Add
      new leaf node to previously established VC.\n   L_MULTI_DROP  - Remove specific
      leaf node from established VC.\n   L_RELEASE     - Release unicast VC, or all
      Leaves of a multicast VC.\n   The signalling exchanges and local information
      passed between AAL\n   User and UNI 3.0/3.1 signalling entity with these functions
      are\n   outside the scope of this document.\n   The following indications are
      assumed to be available to AAL Users,\n   generated by the local UNI 3.0/3.1
      signalling entity:\n   L_ACK          - Succesful completion of a local request.\n
      \  L_REMOTE_CALL  - A new VC has been established to the AAL User.\n   ERR_L_RQFAILED
      - A remote ATM endpoint rejected an L_CALL_RQ,\n                    L_MULTI_RQ,
      or L_MULTI_ADD.\n   ERR_L_DROP     - A remote ATM endpoint dropped off an existing
      VC.\n   ERR_L_RELEASE  - An existing VC was terminated.\n   The signalling exchanges
      and local information passed between AAL\n   User and UNI 3.0/3.1 signalling
      entity with these functions are\n   outside the scope of this document.\n"
    title: 3.4 Interaction with local UNI 3.0/3.1 signalling entity.
  title: 3. UNI 3.0/3.1 support for intra-cluster multicasting.
- contents:
  - "4.  Overview of the MARS.\n   The MARS may reside within any ATM endpoint that
    is directly\n   addressable by the endpoints it is serving. Endpoints wishing
    to join\n   a multicast cluster must be configured with the ATM address of the\n
    \  node on which the cluster's MARS resides.  (Section 5.4 describes how\n   backup
    MARSs may be added to support the activities of a cluster.\n   References to 'the
    MARS' in following sections will be assumed to\n   mean the acting MARS for the
    cluster.)\n"
  - contents:
    - "4.1  Architecture.\n   Architecturally the MARS is an evolution of the RFC
      1577 ARP Server.\n   Whilst the ARP Server keeps a table of {IP,ATM} address
      pairs for all\n   IP endpoints in an LIS, the MARS keeps extended tables of
      {layer 3\n   address, ATM.1, ATM.2, ..... ATM.n} mappings. It can either be\n
      \  configured with certain mappings, or dynamically 'learn' mappings.\n   The
      format of the {layer 3 address} field is generally not\n   interpreted by the
      MARS.\n   A single ATM node may support multiple logical MARSs, each of which\n
      \  support a separate cluster. The restriction is that each MARS has a\n   unique
      ATM address (e.g. a different SEL field in the NSAP address of\n   the node
      on which the multiple MARSs reside).  By definition a single\n   instance of
      a MARS may not support more than one cluster.\n   The MARS distributes group
      membership update information to cluster\n   members over a point to multipoint
      VC known as the ClusterControlVC.\n   Additionally, when Multicast Servers (MCSs)
      are being used it also\n   establishes a separate point to multipoint VC out
      to registered MCSs,\n   known as the ServerControlVC.  All cluster members are
      leaf nodes of\n   ClusterControlVC. All registered multicast servers are leaf
      nodes of\n   ServerControlVC (described further in section 6).\n   The MARS
      does NOT take part in the actual multicasting of layer 3\n   data packets.\n"
    title: 4.1  Architecture.
  - contents:
    - "4.2  Control message format.\n   By default all MARS control messages MUST
      be LLC/SNAP encapsulated\n   using the following codepoints:\n      [0xAA-AA-03][0x00-00-5E][0x00-03][MARS
      control message]\n          (LLC)       (OUI)     (PID)\n   (This is a PID from
      the IANA OUI.)\n   MARS control messages are made up of 4 major components:\n
      \     [Fixed header][Mandatory fields][Addresses][Supplementary TLVs]\n   [Fixed
      header] contains fields indicating the operation being\n   performed and the
      layer 3 protocol being referred to (e.g IPv4, IPv6,\n   AppleTalk, etc). The
      fixed header also carries checksum information,\n   and hooks to allow this
      basic control message structure to be re-used\n   by other query/response protocols.\n
      \  The [Mandatory fields] section carries fixed width parameters that\n   depend
      on the operation type indicated in [Fixed header].\n   The following [Addresses]
      area carries variable length fields for\n   source and target addresses - both
      hardware (e.g. ATM) and layer 3\n   (e.g. IPv4). These provide the fundamental
      information that the\n   registrations, queries, and updates use and operate
      on. For the MARS\n   protocol fields in [Fixed header] indicate how to interpret
      the\n   contents of [Addresses].\n   [Supplementary TLVs] represents an optional
      list of TLV (type,\n   length, value) encoded information elements that may
      be appended to\n   provide supplementary information.  This feature is described
      in\n   further detail in section 10.\n   MARS messages contain variable length
      address fields. In all cases\n   null addresses SHALL be encoded as zero length,
      and have no space\n   allocated in the message.\n   (Unique LLC/SNAP encapsulation
      of MARS control messages means MARS\n   and ARP Server functionality may be
      implemented within a common\n   entity, and share a client-server VC, if the
      implementor so chooses.\n   Note that the LLC/SNAP codepoint for MARS is different
      to the\n   codepoint used for ATMARP.)\n"
    title: 4.2  Control message format.
  - contents:
    - "4.3  Fixed header fields in MARS control messages.\n   The [Fixed header] has
      the following format:\n      Data:\n       mar$afn      16 bits  Address Family
      (0x000F).\n       mar$pro      56 bits  Protocol Identification.\n       mar$hdrrsv
      \  24 bits  Reserved. Unused by MARS control protocol.\n       mar$chksum   16
      bits  Checksum across entire MARS message.\n       mar$extoff   16 bits  Extensions
      Offset.\n       mar$op       16 bits  Operation code.\n       mar$shtl      8
      bits  Type & length of source ATM number. (r)\n       mar$sstl      8 bits  Type
      & length of source ATM subaddress. (q)\n   mar$shtl and mar$sstl provide information
      regarding the source's\n   hardware (ATM) address. In the MARS protocol these
      fields are always\n   present, as every MARS message carries a non-null source
      ATM address.\n   In all cases the source ATM address is the first variable length\n
      \  field in the [Addresses] section.\n   The other fields in [Fixed header]
      are described in the following\n   subsections.\n"
    - contents:
      - "4.3.1  Hardware type.\n   mar$afn defines the type of link layer addresses
        being carried. The\n   value of 0x000F SHALL be used by MARS messages generated
        in\n   accordance with this document. The encoding of ATM addresses and\n
        \  subaddresses when mar$afn = 0x000F is described in section 5.1.2.\n   Encodings
        when mar$afn != 0x000F are outside the scope of this\n   document.\n"
      title: 4.3.1  Hardware type.
    - contents:
      - "4.3.2  Protocol type.\n   The mar$pro field is made up of two subfields:\n
        \     mar$pro.type 16 bits  Protocol type.\n      mar$pro.snap 40 bits  Optional
        SNAP extension to protocol type.\n   The mar$pro.type field is a 16 bit unsigned
        integer representing the\n   following number space:\n      0x0000 to 0x00FF
        \ Protocols defined by the equivalent NLPIDs.\n      0x0100 to 0x03FF  Reserved
        for future use by the IETF.\n      0x0400 to 0x04FF  Allocated for use by
        the ATM Forum.\n      0x0500 to 0x05FF  Experimental/Local use.\n      0x0600
        to 0xFFFF  Protocols defined by the equivalent Ethertypes.\n   (based on the
        observations that valid Ethertypes are never smaller\n   than 0x600, and NLPIDs
        never larger than 0xFF.)\n   The NLPID value of 0x80 is used to indicate a
        SNAP encoded extension\n   is being used to encode the protocol type. When
        mar$pro.type == 0x80\n   the SNAP extension is encoded in the mar$pro.snap
        field.  This is\n   termed the 'long form' protocol ID.\n   If mar$pro.type
        != 0x80 then the mar$pro.snap field MUST be zero on\n   transmit and ignored
        on receive. The mar$pro.type field itself\n   identifies the protocol being
        referred to. This is termed the 'short\n   form' protocol ID.\n   In all cases,
        where a protocol has an assigned number in the\n   mar$pro.type space (excluding
        0x80) the short form MUST be used when\n   transmitting MARS messages. Additionally,
        where a protocol has valid\n   short and long forms of identification, receivers
        MAY choose to\n   recognise the long form.\n   mar$pro.type values other than
        0x80 MAY have 'long forms' defined in\n   future documents.\n   For the remainder
        of this document references to mar$pro SHALL be\n   interpreted to mean mar$pro.type,
        or mar$pro.type in combination with\n   mar$pro.snap as appropriate.\n   The
        use of different protocol types is described further in section\n   9.\n"
      title: 4.3.2  Protocol type.
    - contents:
      - "4.3.3 Checksum.\n   The mar$chksum field carries a standard IP checksum calculated
        across\n   the entire MARS control message (excluding the LLC/SNAP header).
        The\n   field is set to zero before performing the checksum calculation.\n
        \  As the entire LLC/SNAP encapsulated MARS message is protected by the\n
        \  32 bit CRC of the AAL5 transport, implementors MAY choose to ignore\n   the
        checksum facility. If no checksum is calculated these bits MUST\n   be reset
        before transmission. If no checksum is performed on\n   reception, this field
        MUST be ignored. If a receiver is capable of\n   validating a checksum it
        MUST only perform the validation when the\n   received mar$chksum field is
        non-zero. Messages arriving with\n   mar$chksum of 0 are always considered
        valid.\n"
      title: 4.3.3 Checksum.
    - contents:
      - "4.3.4 Extensions Offset.\n   The mar$extoff field identifies the existence
        and location of an\n   optional supplementary parameters list. Its use is
        described in\n   section 10.\n"
      title: 4.3.4 Extensions Offset.
    - contents:
      - "4.3.5 Operation code.\n   The mar$op field is further subdivided into two
        8 bit fields -\n   mar$op.version (leading octet) and mar$op.type (trailing
        octet).\n   Together they indicate the nature of the control message, and
        the\n   context within which its [Mandatory fields], [Addresses], and\n   [Supplementary
        TLVs] should be interpreted.\n      mar$op.version\n         0               MARS
        protocol defined in this document.\n         0x01 - 0xEF     Reserved for
        future use by the IETF.\n         0xF0 - 0xFE     Allocated for use by the
        ATM Forum.\n         0xFF            Experimental/Local use.\n      mar$op.type\n
        \        Value indicates operation being performed, within context of\n         the
        control protocol version indicated by mar$op.version.\n   For the rest of
        this document references to the mar$op value SHALL be\n   taken to mean mar$op.type,
        with mar$op.version = 0x00. The values\n   used in this document are summarised
        in section 11.\n   (Note this number space is independent of the ATMARP operation
        code\n   number space.)\n"
      title: 4.3.5 Operation code.
    - contents:
      - "4.3.6 Reserved.\n   mar$hdrrsv may be subdivided and assigned specific meanings
        for other\n   control protocols indicated by mar$op.version != 0.\n"
      title: 4.3.6 Reserved.
    title: 4.3  Fixed header fields in MARS control messages.
  title: 4.  Overview of the MARS.
- contents:
  - "5.  Endpoint (MARS client) interface behaviour.\n   An endpoint is best thought
    of as a 'shim' or 'convergence' layer,\n   sitting between a layer 3 protocol's
    link layer interface and the\n   underlying UNI 3.0/3.1 service. An endpoint in
    this context can exist\n   in a host or a router - any entity that requires a
    generic 'layer 3\n   over ATM' interface to support layer 3 multicast.  It is
    broken into\n   two key subsections - one for the transmit side, and one for the\n
    \  receive side.\n   Multiple logical ATM interfaces may be supported by a single
    physical\n   ATM interface (for example, using different SEL values in the NSAP\n
    \  formatted address assigned to the physical ATM interface). Therefore\n   implementors
    MUST allow for multiple independent 'layer 3 over ATM'\n   interfaces too, each
    with its own configured MARS (or table of MARSs,\n   as discussed in section 5.4),
    and ability to be attached to the same\n   or different clusters.\n   The initial
    signalling path between a MARS client (managing an\n   endpoint) and its associated
    MARS is a transient point to point,\n   bidirectional VC.  This VC is established
    by the MARS client, and is\n   used to send queries to, and receive replies from,
    the MARS. It has\n   an associated idle timer, and is dismantled if not used for
    a\n   configurable period of time. The minimum suggested value for this\n   time
    is 1 minute, and the RECOMMENDED default is 20 minutes.  (Where\n   the MARS and
    ARP Server are co-resident, this VC may be used for both\n   ATM ARP traffic and
    MARS control traffic.)\n   The remaining signalling path is ClusterControlVC,
    to which the MARS\n   client is added as a leaf node when it registers (described
    in\n   section 5.2.3).\n   The majority of this document covers the distribution
    of information\n   allowing endpoints to establish and manage outgoing point to\n
    \  multipoint VCs - the forwarding paths for multicast traffic to\n   particular
    multicast groups. The actual format of the AAL_SDUs sent\n   on these VCs is almost
    completely outside the scope of this\n   specification.  However, endpoints are
    not expected to know whether\n   their forwarding path leads directly to a multicast
    group's members\n   or to an MCS (described in section 3). This requires additional
    per-\n   packet encapsulation (described in section 5.5) to aid in the the\n   detection
    of reflected AAL_SDUs.\n"
  - contents:
    - "5.1  Transmit side behaviour.\n   The following description will often be in
      terms of an IPv4/ATM\n   interface that is capable of transmitting packets to
      a Class D\n   address at any time, without prior warning. It should be trivial
      for\n   an implementor to generalise this behaviour to the requirements of\n
      \  another layer 3 data protocol.\n   When a local Layer 3 entity passes down
      a packet for transmission,\n   the endpoint first ascertains whether an outbound
      path to the\n   destination multicast group already exists. If it does not,
      the MARS\n   is queried for a set of ATM endpoints that represent an appropriate\n
      \  forwarding path. (The ATM endpoints may represent the actual group\n   members
      within the cluster, or a set of one or more MCSs. The\n   endpoint does not
      distinguish between either case. Section 6.2\n   describes the MARS behaviour
      that leads to MCSs being supplied as the\n   forwarding path for a multicast
      group.)\n   The query is executed by issuing a MARS_REQUEST.  The reply from
      the\n   MARS may take one of two forms:\n      MARS_MULTI - Sequence of MARS_MULTI
      messages returning the set of\n                   ATM endpoints that are to
      be leaf nodes of an\n                   outgoing point to multipoint VC (the
      forwarding\n                   path).\n      MARS_NAK - No mapping found, group
      is empty.\n   The formats of these messages are described in section 5.1.2.\n
      \  Outgoing VCs are established with a request for Unspecified Bit Rate\n   (UBR)
      service, as typified by the IETF's use of VCs for unicast IP,\n   described
      in RFC 1755 [6].  Future documents may vary this approach\n   and allow the
      specification of different ATM traffic parameters from\n   locally configured
      information or parameters obtained through some\n   external means.\n"
    - contents:
      - "5.1.1   Retrieving Group Membership from the MARS.\n   If the MARS had no
        mapping for the desired Class D address a MARS_NAK\n   will be returned. In
        this case the IP packet MUST be discarded\n   silently. If a match is found
        in the MARS's tables it proceeds to\n   return addresses ATM.1 through ATM.n
        in a sequence of one or more\n   MARS_MULTIs.  A simple mechanism is used
        to detect and recover from\n   loss of MARS_MULTI messages.\n   (If the client
        learns that there is no other group member in the\n   cluster - the MARS returns
        a MARS_NAK or returns a MARS_MULTI with\n   the client as the only member
        - it MUST delay sending out a new\n   MARS_REQUEST for that group for a period
        no less than 5 seconds and\n   no more than 10 seconds.)\n   Each MARS_MULTI
        carries a boolean field x, and a 15 bit integer field\n   y - expressed as
        MARS_MULTI(x,y). Field y acts as a sequence number,\n   starting at 1 and
        incrementing for each MARS_MULTI sent.  Field x\n   acts as an 'end of reply'
        marker. When x == 1 the MARS response is\n   considered complete.\n   In addition,
        each MARS_MULTI may carry multiple ATM addresses from\n   the set {ATM.1,
        ATM.2, .... ATM.n}. A MARS MUST minimise the number\n   of MARS_MULTIs transmitted
        by placing as many group members'\n   addresses in a single MARS_MULTI as
        possible. The limit on the length\n   of an individual MARS_MULTI message
        MUST be the MTU of the underlying\n   VC.\n   For example, assume n ATM addresses
        must be returned, each MARS_MULTI\n   is limited to only p ATM addresses,
        and p << n. This would require a\n   sequence of k MARS_MULTI messages (where
        k = (n/p)+1, using integer\n   arithmetic), transmitted as follows:\n      MARS_MULTI(0,1)
        carries back {ATM.1 ... ATM.p}\n      MARS_MULTI(0,2) carries back {ATM.(p+1)
        ... ATM.(2p)}\n            [.......]\n      MARS_MULTI(1,k) carries back {
        ... ATM.n}\n   If k == 1 then only MARS_MULTI(1,1) is sent.\n   Typical failure
        mode will be losing one or more of MARS_MULTI(0,1)\n   through MARS_MULTI(0,k-1).
        This is detected when y jumps by more than\n   one between consecutive MARS_MULTI's.
        An alternative failure mode is\n   losing MARS_MULTI(1,k).  A timer MUST be
        implemented to flag the\n   failure of the last MARS_MULTI to arrive. A default
        value of 10\n   seconds is RECOMMENDED.\n   If a 'sequence jump' is detected,
        the host MUST wait for the\n   MARS_MULTI(1,k), discard all results, and repeat
        the MARS_REQUEST.\n   If a timeout occurs, the host MUST discard all results,
        and repeat\n   the MARS_REQUEST.\n   A final failure mode involves the MARS
        Sequence Number (described in\n   section 5.1.4.2 and carried in each part
        of a multi-part MARS_MULTI).\n   If its value changes during the reception
        of a multi-part MARS_MULTI\n   the host MUST wait for the MARS_MULTI(1,k),
        discard all results, and\n   repeat the MARS_REQUEST.\n   (Corruption of cell
        contents will lead to loss of a MARS_MULTI\n   through AAL5 CPCS_PDU reassembly
        failure, which will be detected\n   through the mechanisms described above.)\n
        \  If the MARS is managing a cluster of endpoints spread across\n   different
        but directly accessible ATM networks it will not be able to\n   return all
        the group members in a single MARS_MULTI. The MARS_MULTI\n   message format
        allows for either E.164, ISO NSAP, or (E.164 + NSAP)\n   to be returned as
        ATM addresses. However, each MARS_MULTI message may\n   only return ATM addresses
        of the same type and length. The returned\n   addresses MUST be grouped according
        to type (E.164, ISO NSAP, or\n   both) and returned in a sequence of separate
        MARS_MULTI parts.\n"
      title: 5.1.1   Retrieving Group Membership from the MARS.
    - contents:
      - "5.1.2   MARS_REQUEST, MARS_MULTI, and MARS_NAK messages.\n   MARS_REQUEST
        is shown below. It is indicated by an 'operation type\n   value' (mar$op)
        of 1.\n   The multicast address being resolved is placed into the the target\n
        \  protocol address field (mar$tpa), and the target hardware address is\n
        \  set to null (mar$thtl and mar$tstl both zero).\n   In IPv4 environments
        the protocol type (mar$pro) is 0x800 and the\n   target protocol address length
        (mar$tpln) MUST be set to 4. The\n   source fields MUST contain the ATM number
        and subaddress of the\n   client issuing the MARS_REQUEST (the subaddress
        MAY be null).\n      Data:\n       mar$afn      16 bits  Address Family (0x000F).\n
        \      mar$pro      56 bits  Protocol Identification.\n       mar$hdrrsv   24
        bits  Reserved. Unused by MARS control protocol.\n       mar$chksum   16 bits
        \ Checksum across entire MARS message.\n       mar$extoff   16 bits  Extensions
        Offset.\n       mar$op       16 bits  Operation code (MARS_REQUEST = 1)\n
        \      mar$shtl      8 bits  Type & length of source ATM number. (r)\n       mar$sstl
        \     8 bits  Type & length of source ATM subaddress. (q)\n       mar$spln
        \     8 bits  Length of source protocol address (s)\n       mar$thtl      8
        bits  Type & length of target ATM number (x)\n       mar$tstl      8 bits
        \ Type & length of target ATM subaddress (y)\n       mar$tpln      8 bits
        \ Length of target group address (z)\n       mar$pad      64 bits  Padding
        (aligns mar$sha with MARS_MULTI).\n       mar$sha      roctets  source ATM
        number\n       mar$ssa      qoctets  source ATM subaddress\n       mar$spa
        \     soctets  source protocol address\n       mar$tpa      zoctets  target
        multicast group address\n       mar$tha      xoctets  target ATM number\n
        \      mar$tsa      yoctets  target ATM subaddress\n   Following the RFC1577
        approach, the mar$shtl, mar$sstl, mar$thtl and\n   mar$tstl fields are coded
        as follows:\n                7 6 5 4 3 2 1 0\n               +-+-+-+-+-+-+-+-+\n
        \              |0|x|  length   |\n               +-+-+-+-+-+-+-+-+\n   The
        most significant bit is reserved and MUST be set to zero.  The\n   second
        most significant bit (x) is a flag indicating whether the ATM\n   address
        being referred to is in:\n      - ATM Forum NSAPA format (x = 0).\n      -
        Native E.164 format (x = 1).\n   The bottom 6 bits is an unsigned integer
        value indicating the length\n   of the associated ATM address in octets. If
        this value is zero the\n   flag x is ignored.\n   The mar$spln and mar$tpln
        fields are unsigned 8 bit integers, giving\n   the length in octets of the
        source and target protocol address fields\n   respectively.\n   MARS packets
        use true variable length fields. A null (non-existant)\n   address MUST be
        coded as zero length, and no space allocated for it\n   in the message body.\n
        \  MARS_NAK is the MARS_REQUEST returned with operation type value of 6.\n
        \  All other fields are left unchanged from the MARS_REQUEST (e.g. do\n   not
        transpose the source and target information. In all cases MARS\n   clients
        use the source address fields to identify their own messages\n   coming back).\n
        \  The MARS_MULTI message is identified by an mar$op value of 2. The\n   message
        format is:\n      Data:\n       mar$afn      16 bits  Address Family (0x000F).\n
        \      mar$pro      56 bits  Protocol Identification.\n       mar$hdrrsv   24
        bits  Reserved. Unused by MARS control protocol.\n       mar$chksum   16 bits
        \ Checksum across entire MARS message.\n       mar$extoff   16 bits  Extensions
        Offset.\n       mar$op       16 bits  Operation code (MARS_MULTI = 2).\n       mar$shtl
        \     8 bits  Type & length of source ATM number. (r)\n       mar$sstl      8
        bits  Type & length of source ATM subaddress. (q)\n       mar$spln      8
        bits  Length of source protocol address (s)\n       mar$thtl      8 bits  Type
        & length of target ATM number (x)\n       mar$tstl      8 bits  Type & length
        of target ATM subaddress (y)\n       mar$tpln      8 bits  Length of target
        group address (z)\n       mar$tnum     16 bits  Number of target ATM addresses
        returned (N)\n       mar$seqxy    16 bits  Boolean flag x and sequence number
        y.\n       mar$msn      32 bits  MARS Sequence Number.\n       mar$sha      roctets
        \ source ATM number\n       mar$ssa      qoctets  source ATM subaddress\n
        \      mar$spa      soctets  source protocol address\n       mar$tpa      zoctets
        \ target multicast group address\n       mar$tha.1    xoctets  target ATM
        number 1\n       mar$tsa.1    yoctets  target ATM subaddress 1\n       mar$tha.2
        \   xoctets  target ATM number 2\n       mar$tsa.2    yoctets  target ATM
        subaddress 2\n                 [.......]\n       mar$tha.N    xoctets  target
        ATM number N\n       mar$tsa.N    yoctets  target ATM subaddress N\n   The
        source protocol and ATM address fields are copied directly from\n   the MARS_REQUEST
        that this MARS_MULTI is in response to (not the MARS\n   itself).\n   mar$seqxy
        is coded with flag x in the leading bit, and sequence\n   number y coded as
        an unsigned integer in the remaining 15 bits.\n          |  1st octet    |
        \  2nd octet   |\n           7 6 5 4 3 2 1 0 7 6 5 4 3 2 1 0\n          +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n
        \         |x|                 y           |\n          +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n
        \  mar$tnum is an unsigned integer indicating how many pairs of\n   {mar$tha,mar$tsa}
        (i.e. how many group member's ATM addresses) are\n   present in the message.
        mar$msn is an unsigned 32 bit number filled\n   in by the MARS before transmitting
        each MARS_MULTI. Its use is\n   described further in section 5.1.4.\n   As
        an example, assume we have a multicast cluster using 4 byte\n   protocol addresses,
        20 byte ATM numbers, and 0 byte ATM subaddresses.\n   For n group members
        in a single MARS_MULTI we require a (60 + 20n)\n   byte message. If we assume
        the default MTU of 9180 bytes, we can\n   return a maximum of 456 group member's
        addresses in a single\n   MARS_MULTI.\n"
      title: 5.1.2   MARS_REQUEST, MARS_MULTI, and MARS_NAK messages.
    - contents:
      - "5.1.3   Establishing the outgoing multipoint VC.\n   Following the completion
        of the MARS_MULTI reply the endpoint may\n   establish a new point to multipoint
        VC, or reuse an existing one.\n   If establishing a new VC, an L_MULTI_RQ
        is issued for ATM.1, followed\n   by an L_MULTI_ADD for every member of the
        set {ATM.2, ....ATM.n}\n   (assuming the set is non-null). The packet is then
        transmitted over\n   the newly created VC just as it would be for a unicast
        VC.\n   After transmitting the packet, the local interface holds the VC open\n
        \  and marks it as the active path out of the host for any subsequent IP\n
        \  packets being sent to that Class D address.\n   When establishing a new
        multicast VC it is possible that one or more\n   L_MULTI_RQ or L_MULTI_ADD
        may fail.  The UNI 3.0/3.1 failure cause\n   must be returned in the ERR_L_RQFAILED
        signal from the local\n   signalling entity to the AAL User. If the failure
        cause is not 49\n   (Quality of Service unavailable), 51 (user cell rate not
        available -\n   UNI 3.0), 37 (user cell rate not available - UNI 3.1), or
        41\n   (Temporary failure), the endpoint's ATM address is dropped from the\n
        \  set {ATM.1, ATM.2, ..., ATM.n} returned by the MARS.  Otherwise, the\n
        \  L_MULTI_RQ or L_MULTI_ADD should be reissued after a random delay of\n
        \  5 to 10 seconds.  If the request fails again, another request should\n
        \  be issued after twice the previous delay has elapsed.  This process\n   should
        be continued until the call succeeds or the multipoint VC gets\n   released.\n
        \  If the initial L_MULTI_RQ fails for ATM.1, and n is greater than 1\n   (i.e.
        the returned set of ATM addresses contains 2 or more addresses)\n   a new
        L_MULTI_RQ should be immediately issued for the next ATM\n   address in the
        set. This procedure is repeated until an L_MULTI_RQ\n   succeeds, as no L_MULTI_ADDs
        may be issued until an initial outgoing\n   VC is established.\n   Each ATM
        address for which an L_MULTI_RQ failed with cause 49, 51,\n   37, or 41 MUST
        be tagged rather than deleted. An L_MULTI_ADD is\n   issued for these tagged
        addresses using the random delay procedure\n   outlined above.\n   The VC
        MAY be considered 'up' before failed L_MULTI_ADDs have been\n   successfully
        re-issued. An endpoint MAY implement a concurrent\n   mechanism that allows
        data to start flowing out the new VC even while\n   failed L_MULTI_ADDs are
        being re-tried. (The alternative of waiting\n   for each leaf node to accept
        the connection could lead to significant\n   delays in transmitting the first
        packet.)\n   Each VC MUST have a configurable inactivity timer associated
        with it.\n   If the timer expires, an L_RELEASE is issued for that VC, and
        the\n   Class D address is no longer considered to have an active path out
        of\n   the local host. The timer SHOULD be no less than 1 minute, and a\n
        \  default of 20 minutes is RECOMMENDED. Choice of specific timer\n   periods
        is beyond the scope of this document.\n   VC consumption may also be reduced
        by endpoints noting when a new\n   group's set of {ATM.1, ....ATM.n} matches
        that of a pre-existing VC\n   out to another group. With careful local management,
        and assuming the\n   QoS of the existing VC is sufficient for both groups,
        a new pt to mpt\n   VC may not be necessary.  Under certain circumstances
        endpoints may\n   decide that it is sufficient to re-use an existing VC whose
        set of\n   leaf nodes is a superset of the new group's membership (in which
        case\n   some endpoints will receive multicast traffic for a layer 3 group\n
        \  they haven't joined, and must filter them above the ATM interface).\n   Algorithms
        for performing this type of optimization are not discussed\n   here, and are
        not required for conformance with this document.\n"
      title: 5.1.3   Establishing the outgoing multipoint VC.
    - contents:
      - "5.1.4   Tracking subsequent group updates.\n   Once a new VC has been established,
        the transmit side of the cluster\n   member's interface needs to monitor subsequent
        group changes - adding\n   or dropping leaf nodes as appropriate. This is
        achieved by watching\n   for MARS_JOIN and MARS_LEAVE messages from the MARS
        itself. These\n   messages are described in detail in section 5.2 - at this
        point it is\n   sufficient to note that they carry:\n      - The ATM address
        of a node joining or leaving a group.\n      - The layer 3 address of the
        group(s) being joined or left.\n      - A Cluster Sequence Number (CSN) from
        the MARS.\n   MARS_JOIN and MARS_LEAVE messages arrive at each cluster member\n
        \  across ClusterControlVC. MARS_JOIN or MARS_LEAVE messages that simply\n
        \  confirm information already held by the cluster member are used to\n   track
        the Cluster Sequence Number, but are otherwise ignored.\n"
      - contents:
        - "5.1.4.1   Updating the active VCs.\n   If a MARS_JOIN is seen that refers
          to (or encompasses) a group for\n   which the transmit side already has
          a VC open, the new member's ATM\n   address is extracted and an L_MULTI_ADD
          issued locally. This ensures\n   that endpoints already sending to a given
          group will immediately add\n   the new member to their list of recipients.\n
          \  If a MARS_LEAVE is seen that refers to (or encompasses) a group for\n
          \  which the transmit side already has a VC open, the old member's ATM\n
          \  address is extracted and an L_MULTI_DROP issued locally. This ensures\n
          \  that endpoints already sending to a given group will immediately drop\n
          \  the old member from their list of recipients. When the last leaf of a\n
          \  VC is dropped, the VC is closed completely and the affected group no\n
          \  longer has a path out of the local endpoint (the next outbound packet\n
          \  to that group's address will trigger the creation of a new VC, as\n   described
          in sections 5.1.1 to 5.1.3).\n   The transmit side of the interface MUST
          NOT shut down an active VC to\n   a group for which the receive side has
          just executed a\n   LeaveLocalGroup.  (This behaviour is consistent with
          the model of\n   hosts transmitting to groups regardless of their own membership\n
          \  status.)\n   If a MARS_JOIN or MARS_LEAVE arrives with mar$pnum == 0
          it carries no\n   <min,max> pairs, and is only used for tracking the CSN.\n"
        title: 5.1.4.1   Updating the active VCs.
      - contents:
        - "5.1.4.2   Tracking the Cluster Sequence Number.\n   It is important that
          endpoints do not miss group membership updates\n   issued by the MARS over
          ClusterControlVC. However, this will happen\n   from time to time. The Cluster
          Sequence Number is carried as an\n   unsigned 32 bit value in the mar$msn
          field of many MARS messages\n   (except for MARS_REQUEST and MARS_NAK).
          \ It increments once for every\n   transmission the MARS makes on ClusterControlVC,
          regardless of\n   whether the transmission represents a change in the MARS
          database or\n   not. By tracking this counter, cluster members can determine
          whether\n   they have missed a previous message on ClusterControlVC, and
          possibly\n   a membership change. This is then used to trigger revalidation\n
          \  (described in section 5.1.5).\n   The current CSN is copied into the
          mar$msn field of MARS messages\n   being sent to cluster members, whether
          out ClusterControlVC or on a\n   point to point VC.\n   Calculations on
          the sequence numbers MUST be performed as unsigned 32\n   bit arithmetic.\n
          \  Every cluster member keeps its own 32 bit Host Sequence Number (HSN)\n
          \  to track the MARS's sequence number. Whenever a message is received\n
          \  that carries an mar$msn field the following processing is performed:\n
          \        Seq.diff = mar$msn - HSN\n         mar$msn -> HSN\n         {...process
          MARS message as appropriate...}\n         if ((Seq.diff != 1) && (Seq.diff
          != 0))\n            then {...revalidate group membership information...}\n
          \  The basic result is that the cluster member attempts to keep locked\n
          \  in step with membership changes noted by the MARS. If it ever detects\n
          \  that a membership change occurred (in any group) without it noticing,\n
          \  it re-validates the membership of all groups it currently has\n   multicast
          VCs open to.\n   The mar$msn value in an individual MARS_MULTI is not used
          to update\n   the HSN until all parts of the MARS_MULTI (if more than 1)
          have\n   arrived. (If the mar$msn changes the MARS_MULTI is discarded, as\n
          \  described in section 5.1.1.)\n   The MARS is free to choose an initial
          value of CSN. When a new\n   cluster member starts up it should initialise
          HSN to zero. When the\n   cluster member sends the MARS_JOIN to register
          (described later), the\n   HSN will be correctly updated to the current
          CSN value when the\n   endpoint receives the copy of its MARS_JOIN back
          from the MARS.\n"
        title: 5.1.4.2   Tracking the Cluster Sequence Number.
      title: 5.1.4   Tracking subsequent group updates.
    - contents:
      - "5.1.5   Revalidating a VC's leaf nodes.\n   Certain events may inform a cluster
        member that it has incorrect\n   information about the sets of leaf nodes
        it should be sending to.  If\n   an error occurs on a VC associated with a
        particular group, the\n   cluster member initiates revalidation procedures
        for that specific\n   group. If a jump is detected in the Cluster Sequence
        Number, this\n   initiates revalidation of all groups to which the cluster
        member\n   currently has open point to multipoint VCs.\n   Each open and active
        multipoint VC has a flag associated with it\n   called 'VC_revalidate'. This
        flag is checked everytime a packet is\n   queued for transmission on that
        VC. If the flag is false, the packet\n   is transmitted and no further action
        is required.\n   However, if the VC_revalidate flag is true then the packet
        is\n   transmitted and a new sequence of events is started locally.\n   Revalidation
        begins with re-issuing a MARS_REQUEST for the group\n   being revalidated.
        \ The returned set of members {NewATM.1, NewATM.2,\n   .... NewATM.n} is compared
        with the set already held locally.\n   L_MULTI_DROPs are issued on the group's
        VC for each node that appears\n   in the original set of members but not in
        the revalidated set of\n   members. L_MULTI_ADDs are issued on the group's
        VC for each node that\n   appears in the revalidated set of members but not
        in the original set\n   of members. The VC_revalidate flag is reset when revalidation\n
        \  concludes for the given group. Implementation specific mechanisms\n   will
        be needed to flag the 'revalidation in progress' state.\n   The key difference
        between constructing a VC (section 5.1.3) and\n   revalidating a VC is that
        packet transmission continues on the open\n   VC while it is being revalidated.
        This minimises the disruption to\n   existing traffic.\n   The algorithm for
        initiating revalidation is:\n      - When a packet arrives for transmission
        on a given group,\n        the groups membership is revalidated if VC_revalidate
        == TRUE.\n        Revalidation resets VC_revalidate.\n      - When an event
        occurs that demands revalidation, every\n        group has its VC_revalidate
        flag set TRUE at a random time\n        between 1 and 10 seconds.\n   Benefit:
        Revalidation of active groups occurs quickly, and\n   essentially idle groups
        are revalidated as needed. Randomly\n   distributed setting of VC_revalidate
        flag improves chances of\n   staggered revalidation requests from senders
        when a sequence number\n   jump is detected.\n"
      - contents:
        - "5.1.5.1   When leaf node drops itself.\n   During the life of a multipoint
          VC an ERR_L_DROP may be received\n   indicating that a leaf node has terminated
          its participation at the\n   ATM level. The ATM endpoint associated with
          the ERR_L_DROP MUST be\n   removed from the locally held set {ATM.1, ATM.2,
          .... ATM.n}\n   associated with the VC.\n   After a random period of time
          between 1 and 10 seconds the\n   VC_revalidate flag associated with that
          VC MUST be set true.\n   If an ERR_L_RELEASE is received then the entire
          set {ATM.1, ATM.2,\n   .... ATM.n} is cleared and the VC is considered to
          be completely shut\n   down. Further packet transmission to the group served
          by this VC will\n   result in a new VC being established as described in
          section 5.1.3.\n"
        title: 5.1.5.1   When leaf node drops itself.
      - contents:
        - "5.1.5.2   When a jump is detected in the CSN.\n   Section 5.1.4.2 describes
          how a CSN jump is detected. If a CSN jump\n   is detected upon receipt of
          a MARS_JOIN or a MARS_LEAVE then every\n   outgoing multicast VC MUST have
          its VC_revalidate flag set true at\n   some random interval between 1 and
          10 seconds from when the CSN jump\n   was detected.\n   The only exception
          to this rule is if a sequence number jump is\n   detected during the establishment
          of a new group's VC (i.e. a\n   MARS_MULTI reply was correctly received,
          but its mar$msn indicated\n   that some previous MARS traffic had been missed
          on ClusterControlVC).\n   In this case every open VC, EXCEPT the one just
          established, MUST\n   have its VC_revalidate flag set true at some random
          interval between\n   1 and 10 seconds from when the CSN jump was detected.
          \ (The VC being\n   established at the time is considered already validated.)\n"
        title: 5.1.5.2   When a jump is detected in the CSN.
      title: 5.1.5   Revalidating a VC's leaf nodes.
    - contents:
      - "5.1.6  'Migrating' the outgoing multipoint VC\n   In addition to the group
        tracking described in section 5.1.4, the\n   transmit side of a cluster member
        must respond to 'migration'\n   requests by the MARS. This is triggered by
        the reception of a\n   MARS_MIGRATE message from ClusterControlVC. The MARS_MIGRATE
        message\n   is shown below, with an mar$op code of 13.\n      Data:\n       mar$afn
        \     16 bits  Address Family (0x000F).\n       mar$pro      56 bits  Protocol
        Identification.\n       mar$hdrrsv   24 bits  Reserved. Unused by MARS control
        protocol.\n       mar$chksum   16 bits  Checksum across entire MARS message.\n
        \      mar$extoff   16 bits  Extensions Offset.\n       mar$op       16 bits
        \ Operation code (MARS_MIGRATE = 13).\n       mar$shtl      8 bits  Type &
        length of source ATM number. (r)\n       mar$sstl      8 bits  Type & length
        of source ATM subaddress. (q)\n       mar$spln      8 bits  Length of source
        protocol address (s)\n       mar$thtl      8 bits  Type & length of target
        ATM number (x)\n       mar$tstl      8 bits  Type & length of target ATM subaddress
        (y)\n       mar$tpln      8 bits  Length of target group address (z)\n       mar$tnum
        \    16 bits  Number of target ATM addresses returned (N)\n       mar$resv
        \    16 bits  Reserved.\n       mar$msn      32 bits  MARS Sequence Number.\n
        \      mar$sha      roctets  source ATM number\n       mar$ssa      qoctets
        \ source ATM subaddress\n       mar$spa      soctets  source protocol address\n
        \      mar$tpa      zoctets  target multicast group address\n       mar$tha.1
        \   xoctets  target ATM number 1\n       mar$tsa.1    yoctets  target ATM
        subaddress 1\n       mar$tha.2    xoctets  target ATM number 2\n       mar$tsa.2
        \   yoctets  target ATM subaddress 2\n                                 [.......]\n
        \      mar$tha.N    xoctets  target ATM number N\n       mar$tsa.N    yoctets
        \ target ATM subaddress N\n   A migration is requested when the MARS determines
        that it no longer\n   wants cluster members forwarding their packets directly
        to the ATM\n   addresses it had previously specified (through MARS_REQUESTs
        or\n   MARS_JOINs). When a MARS_MIGRATE is received each cluster member MUST\n
        \  perform the following steps:\n      Close down any existing outgoing VC
        associated with the group\n      carried in the mar$tpa field (L_RELEASE),
        or dissociate the group\n      from any outgoing VC it may have been sharing
        (as described in\n      section 5.1.3).\n      Establish a new outgoing VC
        for the specified group, using the\n      algorithm described in section 5.1.3
        and taking the set of ATM\n      addresses supplied in the MARS_MIGRATE as
        the group's new set of\n      members {ATM.1, .... ATM.n}.\n   The MARS_MIGRATE
        carries the new set of members {ATM.1, .... ATM.n}\n   in a single message,
        in similar manner to a single part MARS_MULTI.\n   As with other messages
        from the MARS, the Cluster Sequence Number\n   carried in mar$msn is checked
        as described in section 5.1.4.2.\n"
      title: 5.1.6  'Migrating' the outgoing multipoint VC
    title: 5.1  Transmit side behaviour.
  - contents:
    - "5.2.   Receive side behaviour.\n   A cluster member is a 'group member' (in
      the sense that it receives\n   packets directed at a given multicast group)
      when its ATM address\n   appears in the MARS's table entry for the group's multicast
      address.\n   A key function within each cluster is the distribution of group\n
      \  membership information from the MARS to cluster members.\n   An endpoint
      may wish to 'join a group' in response to a local, higher\n   level request
      for membership of a group, or because the endpoint\n   supports a layer 3 multicast
      forwarding engine that requires the\n   ability to 'see' intra-cluster traffic
      in order to forward it.\n   Two messages support these requirements - MARS_JOIN
      and MARS_LEAVE.\n   These are sent to the MARS by endpoints when the local layer
      3/ATM\n   interface is requested to join or leave a multicast group. The MARS\n
      \  propagates these messages back out over ClusterControlVC, to ensure\n   the
      knowledge of the group's membership change is distributed in a\n   timely fashion
      to other cluster members.\n   Certain models of layer 3 endpoints (e.g. IP multicast
      routers)\n   expect to be able to receive packet traffic 'promiscuously' across\n
      \  all groups.  This functionality may be emulated by allowing routers\n   to
      request that the MARS returns them as 'wild card' members of all\n   Class D
      addresses.  However, a problem inherent in the current ATM\n   model is that
      a completely promiscuous router may exhaust the local\n   reassembly resources
      in its ATM interface. MARS_JOIN supports a\n   generalisation to the notion
      of 'wild card' entries, enabling routers\n   to limit themselves to 'blocks'
      of the Class D address space. Use of\n   this facility is described in greater
      detail in Section 8.\n   A block can be as small as 1 (a single group) or as
      large as the\n   entire multicast address space (e.g. default IPv4 'promiscuous'\n
      \  behaviour).  A block is defined as all addresses between, and\n   inclusive
      of, a <min,max> address pair. A MARS_JOIN or MARS_LEAVE may\n   carry multiple
      <min,max> pairs.\n   Cluster members MUST provide ONLY a single <min,max> pair
      in each\n   JOIN/LEAVE message they issue. However, they MUST be able to process\n
      \  multiple <min,max> pairs in JOIN/LEAVE messages when performing VC\n   management
      as described in section 5.1.4 (the interpretation being\n   that the join/leave
      operation applies to all addresses in the range\n   from <min> to <max> inclusive,
      for every <min,max> pair).\n   In RFC1112 environments a MARS_JOIN for a single
      group is triggered\n   by a JoinLocalGroup signal from the IP layer. A MARS_LEAVE
      for a\n   single group is triggered by a LeaveLocalGroup signal from the IP\n
      \  layer.\n   Cluster members with special requirements (e.g. multicast routers)\n
      \  may issue MARS_JOINs and MARS_LEAVEs specifying a single block of 2\n   or
      more multicast group addresses. However, a cluster member SHALL\n   NOT issue
      such a multi-group block join for an address range fully or\n   partially overlapped
      by multi-group block join(s) that the cluster\n   member has previously issued
      and not yet retracted. A cluster member\n   MAY issue combinations of single
      group MARS_JOINs that overlap with a\n   multi-group block MARS_JOIN.\n   An
      endpoint MUST register with a MARS in order to become a member of\n   a cluster
      and be added as a leaf to ClusterControlVC.  Registration\n   is covered in
      section 5.2.3.\n   Finally, the endpoint MUST be capable of terminating unidirectional\n
      \  VCs (i.e. act as a leaf node of a UNI 3.0/3.1 point to multipoint VC,\n   with
      zero bandwidth assigned on the return path). RFC 1755 describes\n   the signalling
      information required to terminate VCs carrying\n   LLC/SNAP encapsulated traffic
      (discussed further in section 5.5).\n"
    - contents:
      - "5.2.1 Format of the MARS_JOIN and MARS_LEAVE Messages.\n   The MARS_JOIN
        message is indicated by an operation type value of 4.\n   MARS_LEAVE has the
        same format and operation type value of 5. The\n   message format is:\n      Data:\n
        \      mar$afn      16 bits  Address Family (0x000F).\n       mar$pro      56
        bits  Protocol Identification.\n       mar$hdrrsv   24 bits  Reserved. Unused
        by MARS control protocol.\n       mar$chksum   16 bits  Checksum across entire
        MARS message.\n       mar$extoff   16 bits  Extensions Offset.\n       mar$op
        \      16 bits  Operation code (MARS_JOIN or MARS_LEAVE).\n       mar$shtl
        \     8 bits  Type & length of source ATM number. (r)\n       mar$sstl      8
        bits  Type & length of source ATM subaddress. (q)\n       mar$spln      8
        bits  Length of source protocol address (s)\n       mar$tpln      8 bits  Length
        of group address (z)\n       mar$pnum     16 bits  Number of group address
        pairs (N)\n       mar$flags    16 bits  layer3grp, copy, and register flags.\n
        \      mar$cmi      16 bits  Cluster Member ID\n       mar$msn      32 bits
        \ MARS Sequence Number.\n       mar$sha      roctets  source ATM number.\n
        \      mar$ssa      qoctets  source ATM subaddress.\n       mar$spa      soctets
        \ source protocol address\n       mar$min.1    zoctets  Minimum multicast
        group address - pair.1\n       mar$max.1    zoctets  Maximum multicast group
        address - pair.1\n                 [.......]\n       mar$min.N    zoctets
        \ Minimum multicast group address - pair.N\n       mar$max.N    zoctets  Maximum
        multicast group address - pair.N\n   mar$spln indicates the number of bytes
        in the source endpoint's\n   protocol address, and is interpreted in the context
        of the protocol\n   indicated by the mar$pro field. (e.g. in IPv4 environments
        mar$pro\n   will be 0x800, mar$spln is 4, and mar$tpln is 4.)\n   The mar$flags
        field contains three flags:\n      Bit 15  - mar$flags.layer3grp.\n      Bit
        14  - mar$flags.copy.\n      Bit 13  - mar$flags.register.\n      Bit 12  -
        mar$flags.punched.\n      Bit 0-7 - mar$flags.sequence.\n   Bits 8 to 11 are
        reserved and MUST be zero.\n   mar$flags.sequence is set by cluster members,
        and MUST always be\n   passed on unmodified by the MARS when retransmitting
        MARS_JOIN or\n   MARS_LEAVE messages. It is source specific, and MUST be ignored
        by\n   other cluster members. Its use is described in section 5.2.2.\n   mar$flags.punched
        MUST be zero when the MARS_JOIN or MARS_LEAVE is\n   transmitted to the MARS.
        Its use is described in section 5.2.2 and\n   section 6.2.4.\n   mar$flags.copy
        MUST be set to 0 when the message is being sent from a\n   MARS client, and
        MUST be set to 1 when the message is being sent from\n   a MARS. (This flag
        is intended to support integrating the MARS\n   function with one of the MARS
        clients in your cluster. The\n   destination of an incoming MARS_JOIN can
        be determined from its\n   value.)\n   mar$flags.layer3grp allows the MARS
        to provide the group membership\n   information described further in section
        5.3. The rules for its use\n   are:\n      mar$flags.layer3grp MUST be set
        when the cluster member is issuing\n      the MARS_JOIN as the result of a
        layer 3 multicast group being\n      explicitly joined. (e.g. as a result
        of a JoinHostGroup operation\n      in an RFC1112 compliant host).\n      mar$flags.layer3grp
        MUST be reset in each MARS_JOIN if the\n      MARS_JOIN is simply the local
        ip/atm interface registering to\n      receive traffic on that group for its
        own reasons.\n      mar$flags.layer3grp is ignored and MUST be treated as
        reset by the\n      MARS for any MARS_JOIN that specifies a block covering
        more than a\n      single group (e.g. a block join from a router ensuring
        their\n      forwarding engines 'see' all traffic).\n   mar$flags.register
        indicates whether the MARS_JOIN or MARS_LEAVE is\n   being used to register
        or deregister a cluster member (described in\n   section 5.2.3). When used
        to join or leave specific groups the\n   mar$register flag MUST be zero.\n
        \  mar$pnum indicates how many <min,max> pairs are included in the\n   message.
        This field MUST be 1 when the message is sent from a cluster\n   member. A
        MARS MAY return a MARS_JOIN or MARS_LEAVE with any mar$pnum\n   value, including
        zero.  This will be explained futher in section\n   6.2.4.\n   The mar$cmi
        field MUST be zeroed by cluster members, and is used by\n   the MARS during
        cluster member registration, described in section\n   5.2.3.\n   mar$msn MUST
        be zero when transmitted by an endpoint. It is set to\n   the current value
        of the Cluster Sequence Number by the MARS when the\n   MARS_JOIN or MARS_LEAVE
        is retransmitted. Its use has been described\n   in section 5.1.4.\n   To
        simplify construction and parsing of MARS_JOIN and MARS_LEAVE\n   messages,
        the following restrictions are imposed on the <min,max>\n   pairs:\n      Assume
        max(N) is the <max> field from the Nth <min,max> pair.\n      Assume min(N)
        is the <min> field from the Nth <min,max> pair.\n      Assume a join/leave
        message arrives with K <min,max> pairs.\n      The following must hold:\n
        \        max(N) < min(N+1) for 1 <= N < K\n         max(N) >= min(N) for 1
        <= N <= K\n   In plain language, the set must specify an ascending sequence
        of\n   address blocks. The definition of \"greater\" or \"less than\" may
        be\n   protocol specific. In IPv4 environments the addresses are treated as\n
        \  32 bit, unsigned binary values (most significant byte first).\n"
      - contents:
        - "5.2.1.1 Important IPv4 default values.\n   The JoinLocalGroup and LeaveLocalGroup
          operations are only valid for\n   a single group. For any arbitrary group
          address X the associated\n   MARS_JOIN or MARS_LEAVE MUST specify a single
          pair <X, X>.\n   mar$flags.layer3grp MUST be set under these circumstances.\n
          \  A router choosing to behave strictly in accordance with RFC1112 MUST\n
          \  specify the entire Class D space. The associated MARS_JOIN or\n   MARS_LEAVE
          MUST specify a single pair <224.0.0.0, 239.255.255.255>.\n   Whenever a
          router issues a MARS_JOIN only in order to forward IP\n   traffic it MUST
          reset mar$flags.layer3grp.\n   The use of alternative <min, max> values
          by multicast routers is\n   discussed in Section 8.\n"
        title: 5.2.1.1 Important IPv4 default values.
      title: 5.2.1 Format of the MARS_JOIN and MARS_LEAVE Messages.
    - contents:
      - "5.2.2   Retransmission of MARS_JOIN and MARS_LEAVE messages.\n   Transient
        problems may result in the loss of messages between the\n   MARS and cluster
        members\n   A simple algorithm is used to solve this problem. Cluster members\n
        \  retransmit each MARS_JOIN and MARS_LEAVE message at regular intervals\n
        \  until they receive a copy back again, either on ClusterControlVC or\n   the
        VC on which they are sending the message.  At this point the\n   local endpoint
        can be certain that the MARS received and processed\n   it.\n   The interval
        should be no shorter than 5 seconds, and a default value\n   of 10 seconds
        is recommended. After 5 retransmissions the attempt\n   should be flagged
        locally as a failure. This MUST be considered as a\n   MARS failure, and triggers
        the MARS reconnection described in section\n   5.4.\n   A 'copy' is defined
        as a received message with the following fields\n   matching a previously
        transmitted MARS_JOIN/LEAVE:\n      - mar$op\n      - mar$flags.register\n
        \     - mar$flags.sequence\n      - mar$pnum\n      - Source ATM address\n
        \     - First <min,max> pair\n   In addition, a valid copy MUST have the following
        field values:\n      - mar$flags.punched = 0\n      - mar$flags.copy = 1\n
        \  The mar$flags.sequence field is never modified or checked by a MARS.\n
        \  Implementors MAY choose to utilize locally significant sequence\n   number
        schemes, which MAY differ from one cluster member to the next.\n   In the
        absence of such schemes the default value for\n   mar$flags.sequence MUST
        be zero.\n   Careful implementations MAY have more than one unacknowledged\n
        \  MARS_JOIN/LEAVE outstanding at a time.\n"
      title: 5.2.2   Retransmission of MARS_JOIN and MARS_LEAVE messages.
    - contents:
      - "5.2.3   Cluster member registration and deregistration.\n   To become a cluster
        member an endpoint must register with the MARS.\n   This achieves two things
        - the endpoint is added as a leaf node of\n   ClusterControlVC, and the endpoint
        is assigned a 16 bit Cluster\n   Member Identifier (CMI). The CMI uniquely
        identifies each endpoint\n   that is attached to the cluster.\n   Registration
        with the MARS occurs when an endpoint issues a MARS_JOIN\n   with the mar$flags.register
        flag set to one (bit 13 of the mar$flags\n   field).\n   The cluster member
        MUST include its source ATM address, and MAY\n   choose to specify a null
        source protocol address when registering.\n   No protocol specific group addresses
        are included in a registration\n   MARS_JOIN.\n   The cluster member retransmits
        this MARS_JOIN in accordance with\n   section 5.2.2 until it confirms that
        the MARS has received it.\n   When the registration MARS_JOIN is returned
        it contains a non-zero\n   value in mar$cmi. This value MUST be noted by the
        cluster member, and\n   used whenever circumstances require the cluster member's
        CMI.\n   An endpoint may also choose to de-register, using a MARS_LEAVE with\n
        \  mar$flags.register set. This would result in the MARS dropping the\n   endpoint
        from ClusterControlVC, removing all references to the member\n   in the mapping
        database, and freeing up its CMI.\n   As for registration, a deregistration
        request MUST include the\n   correct source ATM address for the cluster member,
        but MAY choose to\n   specify a null source protocol address.\n   The cluster
        member retransmits this MARS_LEAVE in accordance with\n   section 5.2.2 until
        it confirms that the MARS has received it.\n"
      title: 5.2.3   Cluster member registration and deregistration.
    title: 5.2.   Receive side behaviour.
  - contents:
    - "5.3   Support for Layer 3 group management.\n   Whilst the intention of this
      specification is to be independent of\n   layer 3 issues, an attempt is being
      made to assist the operation of\n   layer 3 multicast routing protocols that
      need to ascertain if any\n   groups have members within a cluster.\n   One example
      is IP, where IGMP is used (as described in section 2)\n   simply to determine
      whether any other cluster members are listening\n   to a group because they
      have higher layer applications that want to\n   receive a group's traffic.\n
      \  Routers may choose to query the MARS for this information, rather\n   than
      multicasting IGMP queries to 224.0.0.1 and incurring the\n   associated cost
      of setting up a VC to all systems in the cluster.\n   The query is issued by
      sending a MARS_GROUPLIST_REQUEST to the MARS.\n   MARS_GROUPLIST_REQUEST is
      built from a MARS_JOIN, but it has an\n   operation code of 10. The first <min,max>
      pair will be used by the\n   MARS to identify the range of groups in which the
      querying cluster\n   member is interested. Any additional <min,max> pairs will
      be ignored.\n   A request with mar$pnum = 0 will be ignored.\n   The response
      from the MARS is a MARS_GROUPLIST_REPLY, carrying a list\n   of the multicast
      groups within the specified <min,max> block that\n   have Layer 3 members.  A
      group is noted in this list if one or more\n   of the MARS_JOINs that generated
      its mapping entry in the MARS\n   contained a set mar$flags.layer3grp flag.\n
      \  MARS_GROUPLIST_REPLYs are transmitted back to the querying cluster\n   member
      on the VC used to send the MARS_GROUPLIST_REQUEST.\n   MARS_GROUPLIST_REPLY
      is derived from the MARS_MULTI but with mar$op =\n   11. It may have multiple
      parts if needed, and is received in a\n   similar manner to a MARS_MULTI.\n
      \     Data:\n       mar$afn      16 bits  Address Family (0x000F).\n       mar$pro
      \     56 bits  Protocol Identification.\n       mar$hdrrsv   24 bits  Reserved.
      Unused by MARS control protocol.\n       mar$chksum   16 bits  Checksum across
      entire MARS message.\n       mar$extoff   16 bits  Extensions Offset.\n       mar$op
      \      16 bits  Operation code (MARS_GROUPLIST_REPLY).\n       mar$shtl      8
      bits  Type & length of source ATM number. (r)\n       mar$sstl      8 bits  Type
      & length of source ATM subaddress. (q)\n       mar$spln      8 bits  Length
      of source protocol address (s)\n       mar$thtl      8 bits  Unused - set to
      zero.\n       mar$tstl      8 bits  Unused - set to zero.\n       mar$tpln      8
      bits  Length of target group address (z)\n       mar$tnum     16 bits  Number
      of group addresses returned (N).\n       mar$seqxy    16 bits  Boolean flag
      x and sequence number y.\n       mar$msn      32 bits  MARS Sequence Number.\n
      \      mar$sha      roctets  source ATM number.\n       mar$ssa      qoctets
      \ source ATM subaddress.\n       mar$spa      soctets  source protocol address\n
      \      mar$mgrp.1   zoctets  Group address 1\n                 [.......]\n       mar$mgrp.N
      \  zoctets  Group address N\n   mar$seqxy is coded as for the MARS_MULTI - multiple\n
      \  MARS_GROUPLIST_REPLY components are transmitted and received using\n   the
      same algorithm as described in section 5.1.1 for MARS_MULTI. The\n   only difference
      is that protocol addresses are being returned rather\n   than ATM addresses.\n
      \  As for MARS_MULTIs, if an error occurs in the reception of a multi\n   part
      MARS_GROUPLIST_REPLY the whole thing MUST be discarded and the\n   MARS_GROUPLIST_REQUEST
      re-issued. (This includes the mar$msn value\n   being constant.)\n   Note that
      the ability to generate MARS_GROUPLIST_REQUEST messages,\n   and receive MARS_GROUPLIST_REPLY
      messages, is not required for\n   general host interface implementations. It
      is optional for interfaces\n   being implemented to support layer 3 multicast
      forwarding engines.\n   However, this functionality MUST be supported by the
      MARS.\n"
    title: 5.3   Support for Layer 3 group management.
  - contents:
    - "5.4   Support for redundant/backup MARS entities.\n   Endpoints are assumed
      to have been configured with the ATM address of\n   at least one MARS. Endpoints
      MAY choose to maintain a table of ATM\n   addresses, representing alternative
      MARSs that will be contacted in\n   the event that normal operation with the
      original MARS is deemed to\n   have failed. It is assumed that this table orders
      the ATM addresses\n   in descending order of preference.\n   An endpoint will
      typically decide there are problems with the MARS\n   when:\n      - It fails
      to establish a point to point VC to the MARS.\n      - MARS_REQUESTs fail (section
      5.1.1).\n      - MARS_JOIN/MARS_LEAVEs fail (section 5.2.2).\n      - It has
      not received a MARS_REDIRECT_MAP in the last 4 minutes\n      (section 5.4.3).\n
      \  (If it is able to discern which connection represents\n   ClusterControlVC,
      it may also use connection failures on this VC to\n   indicate problems with
      the MARS).\n"
    - contents:
      - "5.4.1   First response to MARS problems.\n   The first response is to assume
        a transient problem with the MARS\n   being used at the time. The cluster
        member should wait a random\n   period of time between 1 and 10 seconds before
        attempting to re-\n   connect and re-register with the MARS. If the registration
        MARS_JOIN\n   is successful then:\n        The cluster member MUST then proceed
        to rejoin every group that\n        its local higher layer protocol(s) have
        joined. It is\n        recommended that a random delay between 1 and 10 seconds
        be\n        inserted before attempting each MARS_JOIN.\n        The cluster
        member MUST initiate the revalidation of every\n        multicast group it
        was sending to (as though a sequence number\n        jump had been detected,
        section 5.1.5).\n        The rejoin and revalidation procedure must not disrupt
        the\n        cluster member's use of multipoint VCs that were already open
        at\n        the time of the MARS failure.\n   If re-registration with the
        current MARS fails, and there are no\n   backup MARS addresses configured,
        the cluster member MUST wait for at\n   least 1 minute before repeating the
        re-registration procedure. It is\n   RECOMMENDED that the cluster member signals
        an error condition in\n   some locally significant fashion.\n   This procedure
        may repeat until network administrators manually\n   intervene or the current
        MARS returns to normal operation.\n"
      title: 5.4.1   First response to MARS problems.
    - contents:
      - "5.4.2   Connecting to a backup MARS.\n   If the re-registration with the
        current MARS fails, and other MARS\n   addresses have been configured, the
        next MARS address on the list is\n   chosen to be the current MARS, and the
        cluster member immediately\n   restarts the re-registration procedure described
        in section 5.4.1. If\n   this is succesful the cluster member will resume
        normal operation\n   using the new MARS. It is RECOMMENDED that the cluster
        member signals\n   a warning of this condition in some locally significant
        fashion.\n   If the attempt at re-registration with the new MARS fails, the\n
        \  cluster member MUST wait for at least 1 minute before choosing the\n   next
        MARS address in the table and repeating the procedure. If the\n   end of the
        table has been reached, the cluster member starts again at\n   the top of
        the table (which should be the original MARS that the\n   cluster member started
        with).\n   In the worst case scenario this will result in cluster members\n
        \  looping through their table of possible MARS addresses until network\n
        \  administrators manually intervene.\n"
      title: 5.4.2   Connecting to a backup MARS.
    - contents:
      - "5.4.3   Dynamic backup lists, and soft redirects.\n   To support some level
        of autoconfiguration, a MARS message is defined\n   that allows the current
        MARS to broadcast on ClusterControlVC a table\n   of backup MARS addresses.
        When this message is received, cluster\n   members that maintain a list of
        backup MARS addresses MUST insert\n   this information at the top of their
        locally held list (i.e. the\n   information provided by the MARS has a higher
        preference than\n   addresses that may have been manually configured into
        the cluster\n   member).\n   The message is MARS_REDIRECT_MAP. It is based
        on the MARS_MULTI\n   message, with the following changes:\n      - mar$tpln
        field replaced by mar$redirf.\n      - mar$spln field reserved.\n      - mar$tpa
        and mar$spa eliminated.\n   MARS_REDIRECT_MAP has an operation type code of
        12 decimal.\n      Data:\n       mar$afn      16 bits  Address Family (0x000F).\n
        \      mar$pro      56 bits  Protocol Identification.\n       mar$hdrrsv   24
        bits  Reserved. Unused by MARS control protocol.\n       mar$chksum   16 bits
        \ Checksum across entire MARS message.\n       mar$extoff   16 bits  Extensions
        Offset.\n       mar$op       16 bits  Operation code (MARS_REDIRECT_MAP).\n
        \      mar$shtl      8 bits  Type & length of source ATM number. (r)\n       mar$sstl
        \     8 bits  Type & length of source ATM subaddress. (q)\n       mar$spln
        \     8 bits  Length of source protocol address (s)\n       mar$thtl      8
        bits  Type & length of target ATM number (x)\n       mar$tstl      8 bits
        \ Type & length of target ATM subaddress (y)\n       mar$redirf    8 bits
        \ Flag controlling client redirect behaviour.\n       mar$tnum     16 bits
        \ Number of MARS addresses returned (N).\n       mar$seqxy    16 bits  Boolean
        flag x and sequence number y.\n       mar$msn      32 bits  MARS Sequence
        Number.\n       mar$sha      roctets  source ATM number\n       mar$ssa      qoctets
        \ source ATM subaddress\n       mar$tha.1    xoctets  ATM number for MARS
        1\n       mar$tsa.1    yoctets  ATM subaddress for MARS 1\n       mar$tha.2
        \   xoctets  ATM number for MARS 2\n       mar$tsa.2    yoctets  ATM subaddress
        for MARS 2\n                 [.......]\n       mar$tha.N    xoctets  ATM number
        for MARS N\n       mar$tsa.N    yoctets  ATM subaddress for MARS N\n   The
        source ATM address field(s) MUST identify the originating MARS.\n   A multi-part
        MARS_REDIRECT_MAP may be transmitted and reassembled\n   using the mar$seqxy
        field in the same manner as a multi-part\n   MARS_MULTI (section 5.1.1). If
        a failure occurs during the reassembly\n   of a multi-part MARS_REDIRECT_MAP
        (a part lost, reassembly timeout,\n   or illegal MARS Sequence Number jump)
        the entire message MUST be\n   discarded.\n   This message is transmitted
        regularly by the MARS (it MUST be\n   transmitted at least every 2 minutes,
        it is RECOMMENDED that it is\n   transmitted every 1 minute).\n   The MARS_REDIRECT_MAP
        is also used to force cluster members to shift\n   from one MARS to another.
        If the ATM address of the first MARS\n   contained in a MARS_REDIRECT_MAP
        table is not the address of cluster\n   member's current MARS the client MUST
        'redirect' to the new MARS. The\n   mar$redirf field controls how the redirection
        occurs.\n   mar$redirf has the following format:\n                7 6 5 4
        3 2 1 0\n               +-+-+-+-+-+-+-+-+\n               |x|             |\n
        \              +-+-+-+-+-+-+-+-+\n   If Bit 7 (the most significant bit) of
        mar$redirf is 1 then the\n   cluster member MUST perform a 'hard' redirect.
        Having installed the\n   new table of MARS addresses carried by the MARS_REDIRECT_MAP,
        the\n   cluster member re-registers with the MARS now at the top of the table\n
        \  using the mechanism described in sections 5.4.1 and 5.4.2.\n   If Bit 7
        of mar$redirf is 0 then the cluster member MUST perform a\n   'soft' redirect,
        beginning with the following actions:\n      - open a point to point VC to
        the first ATM address.\n      - attempt a registration (section 5.2.3).\n
        \  If the registration succeeds, the cluster member shuts down its point\n
        \  to point VC to the current MARS (if it had one open), and then\n   proceeds
        to use the newly opened point to point VC as its connection\n   to the 'current
        MARS'. The cluster member does NOT attempt to rejoin\n   the groups it is
        a member of, or revalidate groups it is currently\n   sending to.\n   This
        is termed a 'soft redirect' because it avoids the extra\n   rejoining and
        revalidation processing that occurs when a MARS failure\n   is being recovered
        from. It assumes some external synchronisation\n   mechanisms exist between
        the old and new MARS - mechanisms that are\n   outside the scope of this specification.\n
        \  Some level of trust is required before initiating a soft redirect. A\n
        \  cluster member MUST check that the calling party at the other end of\n
        \  the VC on which the MARS_REDIRECT_MAP arrived (supposedly\n   ClusterControlVC)
        is in fact the node it trusts as the current MARS.\n   Additional applications
        of this function are for further study.\n"
      title: 5.4.3   Dynamic backup lists, and soft redirects.
    title: 5.4   Support for redundant/backup MARS entities.
  - contents:
    - "5.5  Data path LLC/SNAP encapsulations.\n   An extended encapsulation scheme
      is required to support the filtering\n   of possible reflected packets (section
      3.3).\n   Two LLC/SNAP codepoints are allocated from the IANA OUI space. These\n
      \  support two different mechanisms for detecting reflected packets.\n   They
      are called Type #1 and Type #2 multicast encapsulations.\n   Type #1\n      [0xAA-AA-03][0x00-00-5E][0x00-01][Type
      #1 Extended Layer 3 packet]\n          LLC         OUI        PID\n   Type #2\n
      \     [0xAA-AA-03][0x00-00-5E][0x00-04][Type #2 Extended Layer 3 packet]\n          LLC
      \        OUI        PID\n   For conformance with this document MARS clients:\n
      \     MUST transmit data using Type #1 encapsulation.\n      MUST be able to
      correctly receive traffic using Type #1 OR Type #2\n      encapsulation.\n      MUST
      NOT transmit using Type #2 encapsulation.\n"
    - contents:
      - "5.5.1 Type #1 encapsulation.\n   The Type #1 Extended layer 3 packet carries
        within it a copy of the\n   source's Cluster Member ID (CMI) and either the
        'short form' or 'long\n   form' of the protocol type as appropriate (section
        4.3).\n   When carrying packets belonging to protocols with valid short form\n
        \  representations the [Type #1 Extended Layer 3 packet] is encoded as:\n
        \     [pkt$cmi][pkt$pro][Original Layer 3 packet]\n        2octet   2octet
        \       N octet\n   The first 2 octets (pkt$cmi) carry the CMI assigned when
        an endpoint\n   registers with the MARS (section 5.2.3). The second 2 octets\n
        \  (pkt$pro) indicate the protocol type of the packet carried in the\n   remainder
        of the payload. This is copied from the mar$pro field used\n   in the MARS
        control messages.\n   When carrying packets belonging to protocols that only
        have a long\n   form representation (pkt$pro = 0x80) the overhead SHALL be
        further\n   extended to carry the 5 byte mar$pro.snap field (with padding
        for 32\n   bit alignment). The encoded form SHALL be:\n      [pkt$cmi][0x00-80][mar$pro.snap][padding][Original
        Layer 3 packet]\n        2octet   2octet   5 octets   3 octets        N octet\n
        \  The CMI is copied into the pkt$cmi field of every outgoing Type #1\n   packet.
        \ When an endpoint interface receives an AAL_SDU with the\n   LLC/SNAP codepoint
        indicating Type #1 encapsulation it compares the\n   CMI field with its own
        Cluster Member ID for the indicated protocol.\n   The packet is discarded
        silently if they match. Otherwise the packet\n   is accepted for processing
        by the local protocol entity identified by\n   the pkt$pro (and possibly SNAP)
        field(s).\n   Where a protocol has valid short and long forms of identification,\n
        \  receivers MAY choose to additionally recognise the long form.\n"
      title: '5.5.1 Type #1 encapsulation.'
    - contents:
      - "5.5.2 Type #2 encapsulation.\n   Future developments may enable direct multicasting
        of AAL_SDUs beyond\n   cluster boundaries. Expanding the set of possible sources
        in this way\n   may cause the CMI to become an inadequate parameter with which
        to\n   detect reflected packets.  A larger source identification field may\n
        \  be required.\n   The Type #2 Extended layer 3 packet carries within it
        an 8 octet\n   source ID field and either the 'short form' or 'long form'
        of the\n   protocol type as appropriate (section 4.3).  The form and content
        of\n   the source ID field is currently unspecified, and is not relevant to\n
        \  any MARS client built in conformance with this document. Received\n   Type
        #2 encapsulated packets MUST always be accepted and passed up to\n   the higher
        layer indicated by the protocol identifier.\n   When carrying packets belonging
        to protocols with valid short form\n   representations the [Type #2 Extended
        Layer 3 packet] is encoded as:\n      [8 octet sourceID][mar$pro.type][Null
        pad][Original Layer 3\n      packet]\n                           2octets     2octets\n
        \  When carrying packets belonging to protocols that only have a long\n   form
        representation (pkt$pro = 0x80) the overhead SHALL be further\n   extended
        to carry the 5 byte mar$pro.snap field (with padding for 32\n   bit alignment).
        The encoded form SHALL be:\n      [8 octet sourceID][mar$pro.type][mar$pro.snap][Null
        pad][Layer 3\n      packet]\n                           2octets      5octets
        \     1octet\n   (Note that in this case the padding after the SNAP field
        is 1 octet\n   rather than the 3 octets used in Type #1.)\n   Where a protocol
        has valid short and long forms of identification,\n   receivers MAY choose
        to additionally recognise the long form.\n   (Future documents may specify
        the contents of the source ID field.\n   This will only be relevant to implementations
        sending Type #2\n   encapsulated packets, as they are the only entities that
        need to be\n   concerned about detecting reflected Type #2 packets.)\n"
      title: '5.5.2 Type #2 encapsulation.'
    - contents:
      - "5.5.3 A Type #1 example.\n   An IPv4 packet (fully identified by an Ethertype
        of 0x800, therefore\n   requiring 'short form' protocol type encoding) would
        be transmitted\n   as:\n      [0xAA-AA-03][0x00-00-5E][0x00-01][pkt$cmi][0x800][IPv4
        packet]\n      The different LLC/SNAP codepoints for unicast and multicast
        packet\n      transmission allows a single IPv4/ATM interface to support both
        by\n      demuxing on the LLC/SNAP header.\n"
      title: '5.5.3 A Type #1 example.'
    title: 5.5  Data path LLC/SNAP encapsulations.
  title: 5.  Endpoint (MARS client) interface behaviour.
- contents:
  - "6. The MARS in greater detail.\n   Section 5 implies a lot about the MARS's basic
    behaviour as observed\n   by cluster members. This section summarises the behaviour
    of the MARS\n   for groups that are VC mesh based, and describes how a MARSs\n
    \  behaviour changes when an MCS is registered to support a group.\n   The MARS
    is intended to be a multiprotocol entity - all its mapping\n   tables, CMIs, and
    control VCs MUST be managed within the context of\n   the mar$pro field in incoming
    MARS messages. For example, a MARS\n   supports completely separate ClusterControlVCs
    for each layer 3\n   protocol that it is registering members for. If a MARS receives\n
    \  messages with a mar$pro that it does not support, the message is\n   dropped.\n
    \  In general the MARS treats protocol addresses as arbitrary byte\n   strings.
    For example, the MARS will not apply IPv4 specific 'class'\n   checks to addresses
    supplied under mar$pro = 0x800.  It is sufficient\n   for the MARS to simply assume
    that endpoints know how to interpret\n   the protocol addresses that they are
    establishing and releasing\n   mappings for.\n   The MARS requires control messages
    to carry the originator's identity\n   in the source ATM address field(s). Messages
    that arrive with an\n   empty ATM Number field are silently discarded prior to
    any other\n   processing by the MARS. (Only the ATM Number field needs to be\n
    \  checked. An empty ATM Number field combined with a non-empty ATM\n   Subaddress
    field does not represent a valid ATM address.)\n   (Some example pseudo-code for
    a MARS can be found in Appendix F.)\n"
  - contents:
    - "6.1 Basic interface to Cluster members.\n   The following MARS messages are
      used or required by cluster members:\n      1    MARS_REQUEST\n      2    MARS_MULTI\n
      \     4    MARS_JOIN\n      5    MARS_LEAVE\n      6    MARS_NAK\n      10   MARS_GROUPLIST_REQUEST\n
      \     11   MARS_GROUPLIST_REPLY\n      12   MARS_REDIRECT_MAP\n"
    - contents:
      - "6.1.1  Response to MARS_REQUEST.\n   Except as described in section 6.2,
        if a MARS_REQUEST arrives whose\n   source ATM address does not match that
        of any registered Cluster\n   member the message MUST be dropped and ignored.\n"
      title: 6.1.1  Response to MARS_REQUEST.
    - contents:
      - "6.1.2  Response to MARS_JOIN and MARS_LEAVE.\n   When a registration MARS_JOIN
        arrives (described in section 5.2.3)\n   the MARS performs the following actions:\n
        \     - Adds the node to ClusterControlVC.\n      - Allocates a new Cluster
        Member ID (CMI).\n      - Inserts the new CMI into the mar$cmi field of the
        MARS_JOIN.\n      - Retransmits the MARS_JOIN back privately.\n   If the node
        is already a registered member of the cluster associated\n   with the specified
        protocol type then its existing CMI is simply\n   copied into the MARS_JOIN,
        and the MARS_JOIN retransmitted back to\n   the node.  A single node may register
        multiple times if it supports\n   multiple layer 3 protocols. The CMIs allocated
        by the MARS for each\n   such registration may or may not be the same.\n   The
        retransmitted registration MARS_JOIN must NOT be sent on\n   ClusterControlVC.
        \ If a cluster member issues a deregistration\n   MARS_LEAVE it too is retransmitted
        privately.\n   Non-registration MARS_JOIN and MARS_LEAVE messages are ignored
        if\n   they arrive from a node that is not registered as a cluster member.\n
        \  MARS_JOIN or MARS_LEAVE messages MUST arrive at the MARS with\n   mar$flags.copy
        set to 0, otherwise the message is silently ignored.\n   All outgoing MARS_JOIN
        or MARS_LEAVE messages SHALL have\n   mar$flags.copy set to 1, and mar$msn
        set to the current Cluster\n   Sequence Number for ClusterControlVC (Section
        5.1.4.2).\n   mar$flags.layer3grp (section 5.3) MUST be treated as reset for\n
        \  MARS_JOINs specifying a single <min,max> pair covering more than a\n   single
        group. If a MARS_JOIN/LEAVE is received that contains more\n   than one <min,max>
        pair, the MARS MUST silently drop the message.\n   If one or more MCSs have
        registered with the MARS, message processing\n   continues as described in
        section 6.2.4.\n   The MARS database is updated to add the node to any indicated\n
        \  group(s) that it was not already considered a member of, and message\n
        \  processing continues as follows:\n   If a single group was being joined
        or left:\n      mar$flags.punched is set to 0.\n      If the joining (leaving)
        node was already (is still) considered a\n      member of the specified group,
        the message is retransmitted\n      privately back to the cluster member.
        \ Otherwise the message is\n      retransmitted on ClusterControlVC.\n   If
        a single block covering 2 or more groups was being joined or left:\n      A
        copy of the original MARS_JOIN/LEAVE is made. This copy then has\n      its
        <min,max> block replaced with a 'hole punched' set of zero or\n      more
        <min,max> pairs.  The 'hole punched' set of <min,max> pairs\n      covers
        the entire address range specified by the original\n      <min,max> pair,
        but excludes those addresses/groups which the\n      joining (leaving) node
        is already (still) a member of due to a\n      previous single group join.\n
        \     If no 'holes' were punched in the specified block, the original\n      MARS_JOIN/LEAVE
        is retransmitted out on ClusterControlVC.\n      Otherwise the following occurs:\n
        \        The original MARS_JOIN/LEAVE is transmitted back to the source\n
        \        cluster member unchanged, using the VC it arrived on. The\n         mar$flags.punched
        field MUST be reset to 0 in this message.\n         If the hole-punched set
        contains 1 or more <min,max> pair, the\n         copy of the original MARS_JOIN/LEAVE
        is transmitted on\n         ClusterControlVC, carrying the new <min,max> list.
        The\n         mar$flags.punched field MUST be set to 1 in this message.  (The\n
        \        mar$flags.punched field is set to ensure the hole-punched copy\n
        \        is ignored by the message's source when trying to match\n         received
        MARS_JOIN/LEAVE messages with ones previously sent\n         (section 5.2.2)).\n
        \  If the MARS receives a deregistration MARS_LEAVE (described in\n   section
        5.2.3) that member's ATM address MUST be removed from all\n   groups for which
        it may have joined, dropped from ClusterControlVC,\n   and the CMI released.\n
        \  If the MARS receives an ERR_L_RELEASE on ClusterControlVC indicating\n
        \  that a cluster member has disconnected, that member's ATM address\n   MUST
        be removed from all groups for which it may have joined, and the\n   CMI released.\n"
      title: 6.1.2  Response to MARS_JOIN and MARS_LEAVE.
    - contents:
      - "6.1.3  Generating MARS_REDIRECT_MAP.\n   A MARS_REDIRECT_MAP message (described
        in section 5.4.3) MUST be\n   regularly transmitted on ClusterControlVC.  It
        is RECOMMENDED that\n   this occur every 1 minute, and it MUST occur at least
        every 2\n   minutes. If the MARS has no knowledge of other backup MARSs serving\n
        \  the cluster, it MUST include its own address as the only entry in the\n
        \  MARS_REDIRECT_MAP message (in addition to filling in the source\n   address
        fields).\n   The design and use of backup MARS entities is beyond the scope
        of\n   this document, and will be covered in future work.\n"
      title: 6.1.3  Generating MARS_REDIRECT_MAP.
    - contents:
      - "6.1.4  Cluster Sequence Numbers.\n   The Cluster Sequence Number (CSN) is
        described in section 5.1.4, and\n   is carried in the mar$msn field of MARS
        messages being sent to\n   cluster members (either out ClusterControlVC or
        on an individual VC).\n   The MARS increments the CSN after every transmission
        of a message on\n   ClusterControlVC.  The current CSN is copied into the
        mar$msn field\n   of MARS messages being sent to cluster members, whether
        out\n   ClusterControlVC or on a private VC.\n   A MARS should be carefully
        designed to minimise the possibility of\n   the CSN jumping unnecessarily.
        Under normal operation only cluster\n   members affected by transient link
        problems will miss CSN updates and\n   be forced to revalidate. If the MARS
        itself glitches, it will be\n   innundated with requests for a period as every
        cluster member\n   attempts to revalidate.\n   Calculations on the CSN MUST
        be performed as unsigned 32 bit\n   arithmetic.\n   One implication of this
        mechanism is that the MARS should serialize\n   its processing of 'simultaneous'
        MARS_REQUEST, MARS_JOIN and\n   MARS_LEAVE messages. Join and Leave operations
        should be queued\n   within the MARS along with MARS_REQUESTS, and not processed
        until all\n   the reply packets of a preceeding MARS_REQUEST have been transmitted.\n
        \  The transmission of MARS_REDIRECT_MAP should also be similarly\n   queued.\n
        \  (The regular transmission of MARS_REDIRECT_MAP serves a secondary\n   purpose
        of allowing cluster members to track the CSN, even if they\n   miss an earlier
        MARS_JOIN or MARS_LEAVE.)\n"
      title: 6.1.4  Cluster Sequence Numbers.
    title: 6.1 Basic interface to Cluster members.
  - contents:
    - "6.2   MARS interface to Multicast Servers (MCS).\n   When the MARS returns
      the actual addresses of group members, the\n   endpoint behaviour described
      in section 5 results in all groups being\n   supported by meshes of point to
      multipoint VCs. However, when MCSs\n   register to support particular layer
      3 multicast groups the MARS\n   modifies its use of various MARS messages to
      fool endpoints into\n   using the MCS instead.\n   The following MARS messages
      are associated with interaction between\n   the MARS and MCSs.\n      3   MARS_MSERV\n
      \     7   MARS_UNSERV\n      8   MARS_SJOIN\n      9   MARS_SLEAVE\n   The following
      MARS messages are treated in a slightly different\n   manner when MCSs have
      registered to support certain group addresses:\n      1   MARS_REQUEST\n      4
      \  MARS_JOIN\n      5   MARS_LEAVE\n   A MARS must keep two sets of mappings
      for each layer 3 group using\n   MCS support.  The original {layer 3 address,
      ATM.1, ATM.2, ... ATM.n}\n   mapping (now termed the 'host map', although it
      includes routers) is\n   augmented by a parallel {layer 3 address, server.1,
      server.2, ....\n   server.K} mapping (the 'server map'). It is assumed that
      no ATM\n   addresses appear in both the server and host maps for the same\n
      \  multicast group. Typically K will be 1, but it will be larger if\n   multiple
      MCSs are configured to support a given group.\n   The MARS also maintains a
      point to multipoint VC out to any MCSs\n   registered with it, called ServerControlVC
      (section 6.2.3). This\n   serves an analogous role to ClusterControlVC, allowing
      the MARS to\n   update the MCSs with group membership changes as they occur.
      A MARS\n   MUST also send its regular MARS_REDIRECT_MAP transmissions on both\n
      \  ServerControlVC and ClusterControlVC.\n"
    - contents:
      - "6.2.1   Response to a MARS_REQUEST if MCS is registered.\n   When the MARS
        receives a MARS_REQUEST for an address that has both\n   host and server maps
        it generates a response based on the identity of\n   the request's source.
        If the requestor is a member of the server map\n   for the requested group
        then the MARS returns the contents of the\n   host map in a sequence of one
        or more MARS_MULTIs.  Otherwise, if the\n   source is a valid cluster member,
        the MARS returns the contents of\n   the server map in a sequence of one or
        more MARS_MULTIs.  If the\n   source is neither a cluster member, nor a member
        of the server map\n   for the group, the request is dropped and ignored.\n
        \  Servers use the host map to establish a basic distribution VC for the\n
        \  group. Cluster members will establish outgoing multipoint VCs to\n   members
        of the group's server map, without being aware that their\n   packets will
        not be going directly to the multicast group's members.\n"
      title: 6.2.1   Response to a MARS_REQUEST if MCS is registered.
    - contents:
      - "6.2.2   MARS_MSERV and MARS_UNSERV messages.\n   MARS_MSERV and MARS_UNSERV
        are identical to the MARS_JOIN message.\n   An MCS uses a MARS_MSERV with
        a <min,max> pair of <X,X> to specify\n   the multicast group X that it is
        willing to support. A single group\n   MARS_UNSERV indicates the group that
        the MCS is no longer willing to\n   support.  The operation code for MARS_MSERV
        is 3 (decimal), and\n   MARS_UNSERV is 7 (decimal).\n   Both of these messages
        are sent to the MARS over a point to point VC\n   (between MCS and MARS).
        After processing, they are retransmitted on\n   ServerControlVC to allow other
        MCSs to note the new node.\n   When registering or deregistering support for
        specific groups the\n   mar$flags.register flag MUST be zero. (This flag is
        only one when the\n   MCS is registering as a member of ServerControlVC, as
        described in\n   section 6.2.3.)\n   When an MCS issues a MARS_MSERV for a
        specific group the message MUST\n   be dropped and ignored if the source has
        not already registered with\n   the MARS as a multicast server (section 6.2.3).
        \ Otherwise, the MARS\n   adds the new ATM address to the server map for the
        specified group,\n   possibly constructing a new server map if this is the
        first MCS for\n   the group.\n   If a MARS_MSERV represents the first MCS
        to register for a particular\n   group, and there exists a non null host map
        serving that particular\n   group, the MARS issues a MARS_MIGRATE (section
        5.1.6) on\n   ClusterControlVC. The MARS's own identity is placed in the source\n
        \  protocol and hardware address fields of the MARS_MIGRATE.  The ATM\n   address
        of the MCS is placed as the first and only target ATM\n   address. The address
        of the affected group is placed in the target\n   multicast group address
        field.\n   If a MARS_MSERV is not the first MCS to register for a particular\n
        \  group the MARS simply changes its operation code to MARS_JOIN, and\n   sends
        a copy of the message on ClusterControlVC.  This fools the\n   cluster members
        into thinking a new leaf node has been added to the\n   group specified. In
        the retransmitted MARS_JOIN mar$flags.layer3grp\n   MUST be zero, mar$flags.copy
        MUST be one, and mar$flags.register MUST\n   be zero.\n   When an MCS issues
        a MARS_UNSERV the MARS removes its ATM address\n   from the server maps for
        each specified group, deleting any server\n   maps that end up being null
        after the operation.\n   The operation code is then changed to MARS_LEAVE
        and the MARS sends a\n   copy of the message on ClusterControlVC. This fools
        the cluster\n   members into thinking a leaf node has been dropped from the
        group\n   specified. In the retransmitted MARS_LEAVE mar$flags.layer3grp MUST\n
        \  be zero, mar$flags.copy MUST be one, and mar$flags.register MUST be\n   zero.\n
        \  The MARS retransmits redundant MARS_MSERV and MARS_UNSERV messages\n   directly
        back to the MCS generating them. MARS_MIGRATE messages are\n   never repeated
        in response to redundant MARS_MSERVs.\n   The last or only MCS for a group
        MAY choose to issue a MARS_UNSERV\n   while the group still has members. When
        the MARS_UNSERV is processed\n   by the MARS the 'server map' will be deleted.
        When the associated\n   MARS_LEAVE is issued on ClusterControlVC, all cluster
        members with a\n   VC open to the MCS for that group will close down the VC
        (in\n   accordance with section 5.1.4, since the MCS was their only leaf\n
        \  node). When cluster members subsequently find they need to transmit\n   packets
        to the group, they will begin again with the\n   MARS_REQUEST/MARS_MULTI sequence
        to establish a new VC. Since the\n   MARS will have deleted the server map,
        this will result in the host\n   map being returned, and the group reverts
        to being supported by a VC\n   mesh.\n   The reverse process is achieved through
        the MARS_MIGRATE message when\n   the first MCS registers to support a group.
        \ This ensures that\n   cluster members explicitly dismantle any VC mesh they
        may have had\n   up, and re-establish their multicast forwarding path with
        the MCS as\n   its termination point.\n"
      title: 6.2.2   MARS_MSERV and MARS_UNSERV messages.
    - contents:
      - "6.2.3  Registering a Multicast Server (MCS).\n   Section 5.2.3 describes
        how endpoints register as cluster members,\n   and hence get added as leaf
        nodes to ClusterControlVC. The same\n   approach is used to register endpoints
        that intend to provide MCS\n   support.\n   Registration with the MARS occurs
        when an endpoint issues a\n   MARS_MSERV with mar$flags.register set to one.
        \ Upon registration the\n   endpoint is added as a leaf node to ServerControlVC,
        and the\n   MARS_MSERV is returned to the MCS privately.\n   The MCS retransmits
        this MARS_MSERV until it confirms that the MARS\n   has received it (by receiving
        a copy back, in an analogous way to the\n   mechanism described in section
        5.2.2 for reliably transmitting\n   MARS_JOINs).\n   The mar$cmi field in
        MARS_MSERVs MUST be set to zero by both MCS and\n   MARS.\n   An MCS may also
        choose to de-register, using a MARS_UNSERV with\n   mar$flags.register set
        to one. When this occurs the MARS MUST remove\n   all references to that MCS
        in all servermaps associated with the\n   protocol (mar$pro) specified in
        the MARS_UNSERV, and drop the MCS\n   from ServerControlVC.\n   Note that
        multiple logical MCSs may share the same physical ATM\n   interface, provided
        that each MCS uses a separate ATM address (e.g. a\n   different SEL field
        in the NSAP format address). In fact, an MCS may\n   share the ATM interface
        of a node that is also a cluster member\n   (either host or router), provided
        each logical entity has a different\n   ATM address.\n   A MARS MUST be capable
        of handling a multi-entry servermap. However,\n   the possible use of multiple
        MCSs registering to support the same\n   group is a subject for further study.
        In the absence of an MCS\n   synchronisation protocol a system administrator
        MUST NOT allow more\n   than one logical MCS to register for a given group.\n"
      title: 6.2.3  Registering a Multicast Server (MCS).
    - contents:
      - "6.2.4   Modified response to MARS_JOIN and MARS_LEAVE.\n   The existence
        of MCSs supporting some groups but not others requires\n   the MARS to modify
        its distribution of single and block join/leave\n   updates to cluster members.
        The MARS also adds two new messages -\n   MARS_SJOIN and MARS_SLEAVE - for
        communicating group changes to MCSs\n   over ServerControlVC.\n   The MARS_SJOIN
        and MARS_SLEAVE messages are identical to MARS_JOIN,\n   with operation codes
        18 and 19 (decimal) respectively.\n   When a cluster member issues MARS_JOIN
        or MARS_LEAVE for a single\n   group, the MARS checks to see if the group
        has an associated server\n   map. If the specified group does not have a server
        map processing\n   continues as described in section 6.1.2.\n   However, if
        a server map exists for the group a new set of actions\n   are taken.\n      If
        the joining (leaving) node was not already (is no longer)\n      considered
        a member of the specified group, a copy of the\n      MARS_JOIN/LEAVE is made
        with type MARS_SJOIN or MARS_SLEAVE as\n      appropriate, and transmitted
        on ServerControlVC.  This allows the\n      MCS(s) supporting the group to
        note the new member and update\n      their data VCs.\n      The original
        message is transmitted back to the source cluster\n      member unchanged,
        using the VC it arrived on rather than\n      ClusterControlVC.  The mar$flags.punched
        field MUST be reset to 0\n      in this message.\n   (Section 5.2.2 requires
        cluster members have a mechanism to confirm\n   the reception of their message
        by the MARS. For mesh supported\n   groups, using ClusterControlVC serves
        dual purpose of providing this\n   confirmation and distributing group update
        information. When a group\n   is MCS supported, there is no reason for all
        cluster members to\n   process null join/leave messages on ClusterControlVC,
        so they are\n   sent back on the private VC between cluster member and MARS.)\n
        \  Receipt of a block MARS_JOIN (e.g. from a router coming on-line) or\n   MARS_LEAVE
        requires a more complex response. The single <min,max>\n   block may simultaneously
        cover mesh supported and MCS supported\n   groups.  However, cluster members
        only need to be informed of the\n   mesh supported groups that the endpoint
        has joined. Only the MCSs\n   need to know if the endpoint is joining any
        MCS supported groups.\n   The solution is to modify the MARS_JOIN or MARS_LEAVE
        that is\n   retransmitted on ClusterControlVC. The following action is taken:\n
        \     A copy of the MARS_JOIN/LEAVE is made with type MARS_SJOIN or\n      MARS_SLEAVE
        as appropriate, with its <min,max> block replaced with\n      a 'hole punched'
        set of zero or more <min,max> pairs.  The 'hole\n      punched' set of <min,max>
        pairs covers the entire address range\n      specified by the original <min,max>
        pair, but excludes those\n      addresses/groups which the joining (leaving)
        node is already\n      (still) a member of due to a previous single group
        join.\n      Before transmission on the ClusterControlVC, the original\n      MARS_JOIN/LEAVE
        then has its <min,max> block replaced with a 'hole\n      punched' set of
        zero or more <min,max> pairs.  The 'hole punched'\n      set of <min,max>
        pairs covers the entire address range specified\n      by the original <min,max>
        pair, but excludes those\n      addresses/groups supported by MCSs or which
        the joining (leaving)\n      node is already (still) a member of due to a
        previous single group\n      join.\n      If no 'holes' were punched in the
        specified block, the original\n      MARS_JOIN/LEAVE is re-transmitted out
        on ClusterControlVC\n      unchanged.  Otherwise the following occurs:\n         The
        original MARS_JOIN/LEAVE is transmitted back to the source\n         cluster
        member unchanged, using the VC it arrived on. The\n         mar$flags.punched
        field MUST be reset to 0 in this message.\n         If the hole-punched set
        contains 1 or more <min,max> pair, a\n         copy of the original MARS_JOIN/LEAVE
        is transmitted on\n         ClusterControlVC, carrying the new <min,max> list.
        The\n         mar$flags.punched field MUST be set to 1 in this message.\n
        \     The mar$flags.punched field is set to ensure the hole-punched copy\n
        \     is ignored by the message's source when trying to match received\n      MARS_JOIN/LEAVE
        messages with ones previously sent (section\n      5.2.2).\n   (Appendix A
        discusses some algorithms for 'hole punching'.)\n   It is assumed that MCSs
        use the MARS_SJOINs and MARS_SLEAVEs to\n   update their own VCs out to the
        actual group's members.\n   mar$flags.layer3grp is copied over into the messages
        transmitted by\n   the MARS. mar$flags.copy MUST be set to one.\n"
      title: 6.2.4   Modified response to MARS_JOIN and MARS_LEAVE.
    - contents:
      - "6.2.5  Sequence numbers for ServerControlVC traffic.\n   In an analogous
        fashion to the Cluster Sequence Number, the MARS\n   keeps a Server Sequence
        Number (SSN) that is incremented after every\n   transmission on ServerControlVC.
        The current value of the SSN is\n   inserted into the mar$msn field of every
        message the MARS issues that\n   it believes is destined for an MCS. This
        includes MARS_MULTIs that\n   are being returned in response to a MARS_REQUEST
        from an MCS, and\n   MARS_REDIRECT_MAP being sent on ServerControlVC.  The
        MARS must check\n   the MARS_REQUESTs source, and if it is a registered MCS
        the SSN is\n   copied into the mar$msn field, otherwise the CSN is copied
        into the\n   mar$msn field.\n   MCSs are expected to track and use the SSNs
        in an analogous manner to\n   the way endpoints use the CSN in section 5.1
        (to trigger revalidation\n   of group membership information).\n   A MARS
        should be carefully designed to minimise the possibility of\n   the SSN jumping
        unnecessarily. Under normal operation only MCSs that\n   are affected by transient
        link problems will miss mar$msn updates and\n   be forced to revalidate. If
        the MARS itself glitches it will be\n   innundated with requests for a period
        as every MCS attempts to\n   revalidate.\n"
      title: 6.2.5  Sequence numbers for ServerControlVC traffic.
    title: 6.2   MARS interface to Multicast Servers (MCS).
  - contents:
    - "6.3 Why global sequence numbers?\n   The CSN and SSN are global within the
      context of a given protocol\n   (e.g. IPv4, mar$pro = 0x800).  They count ClusterControlVC
      and\n   ServerControlVC activity without reference to the multicast group(s)\n
      \  involved.  This may be perceived as a limitation, because there is no\n   way
      for cluster members or multicast servers to isolate exactly which\n   multicast
      group they may have missed an update for. An alternative\n   was to try and
      provide a per-group sequence number.\n   Unfortunately per-group sequence numbers
      are not practical. The\n   current mechanism allows sequence information to
      be piggy-backed onto\n   MARS messages already in transit for other reasons.
      The ability to\n   specify blocks of multicast addresses with a single MARS_JOIN
      or\n   MARS_LEAVE means that a single message can refer to membership change\n
      \  for multiple groups simultaneously. A single mar$msn field cannot\n   provide
      meaningful information about each group's sequence.  Multiple\n   mar$msn fields
      would have been unwieldy.\n   Any MARS or cluster member that supports different
      protocols MUST\n   keep separate mapping tables and sequence numbers for each
      protocol.\n"
    title: 6.3 Why global sequence numbers?
  - contents:
    - "6.4 Redundant/Backup MARS Architectures.\n   If backup MARSs exist for a given
      cluster then mechanisms are needed\n   to ensure consistency between their mapping
      tables and those of the\n   active, current MARS.\n   (Cluster members will
      consider backup MARSs to exist if they have\n   been configured with a table
      of MARS addresses, or the regular\n   MARS_REDIRECT_MAP messages contain a list
      of 2 or more addresses.)\n   The definition of an MARS-synchronization protocol
      is beyond the\n   current scope of this document, and is expected to be the
      subject of\n   further research work.  However, the following observations may
      be\n   made:\n      MARS_REDIRECT_MAP messages exist, enabling one MARS to force\n
      \     endpoints to move to another MARS (e.g. in the aftermath of a MARS\n      failure,
      the chosen backup MARS will eventually wish to hand\n      control of the cluster
      over to the main MARS when it is\n      functioning properly again).\n      Cluster
      members and MCSs do not need to start up with knowledge of\n      more than
      one MARS, provided that MARS correctly issues\n      MARS_REDIRECT_MAP messages
      with the full list of MARSs for that\n      cluster.\n   Any mechanism for synchronising
      backup MARSs (and coping with the\n   aftermath of MARS failures) should be
      compatible with the cluster\n   member behaviour described in this document.\n"
    title: 6.4 Redundant/Backup MARS Architectures.
  title: 6. The MARS in greater detail.
- contents:
  - "7.   How an MCS utilises a MARS.\n   When an MCS supports a multicast group it
    acts as a proxy cluster\n   endpoint for the senders to the group. It also behaves
    in an\n   analogous manner to a sender, managing a single outgoing point to\n
    \  multipoint VC to the real group members.\n   Detailed description of possible
    MCS architectures are beyond the\n   scope of this document. This section will
    outline the main issues.\n"
  - contents:
    - "7.1   Association with a particular Layer 3 group.\n   When an MCS issues a
      MARS_MSERV it forces all senders to the\n   specified layer 3 group to terminate
      their VCs on the supplied source\n   ATM address.\n   The simplest MCS architecture
      involves taking incoming AAL_SDUs and\n   simply flipping them back out a single
      point to multipoint VC. Such\n   an MCS cannot support more than one group at
      once, as it has no way\n   to differentiate between traffic destined for different
      groups.\n   Using this architecture, a physical node would provide MCS support\n
      \  for multiple groups by creating multiple logical instances of the\n   MCS,
      each with different ATM Addresses (e.g. a different SEL value in\n   the node's
      NSAPA).\n   A slightly more complex approach would be to add minimal layer 3\n
      \  specific processing into the MCS. This would look inside the received\n   AAL_SDUs
      and determine which layer 3 group they are destined for. A\n   single instance
      of such an MCS might register its ATM Address with\n   the MARS for multiple
      layer 3 groups, and manage multiple independent\n   outgoing point to multipoint
      VCs (one for each group).\n   When an MCS starts up it MUST register with the
      MARS as described in\n   section 6.2.3, identifying the protocol it supports
      with the mar$pro\n   field of the MARS_MSERV. This also applies to logical MCSs,
      even if\n   they share the same physical ATM interface. This is important so
      that\n   the MARS can react to the loss of an MCS when it drops off\n   ServerControlVC.
      (One consequence is that 'simple' MCS architectures\n   end up with one ServerControlVC
      member per group.  MCSs with layer 3\n   specific processing may support multiple
      groups while still only\n   registering as one member of ServerControlVC.)\n
      \  An MCS MUST NOT share the same ATM address as a cluster member,\n   although
      it may share the same physical ATM interface.\n"
    title: 7.1   Association with a particular Layer 3 group.
  - contents:
    - "7.2   Termination of incoming VCs.\n   An MCS MUST terminate unidirectional
      VCs in the same manner as a\n   cluster member.  (e.g. terminate on an LLC entity
      when LLC/SNAP\n   encapsulation is used, as described in RFC 1755 for unicast\n
      \  endpoints.)\n"
    title: 7.2   Termination of incoming VCs.
  - contents:
    - "7.3   Management of outgoing VC.\n   An MCS MUST establish and manage its outgoing
      point to multipoint VC\n   as a cluster member does (section 5.1).\n   MARS_REQUEST
      is used by the MCS to establish the initial leaf nodes\n   for the MCS's outgoing
      point to multipoint VC. After the VC is\n   established, the MCS reacts to MARS_SJOINs
      and MARS_SLEAVEs in the\n   same way a cluster member reacts to MARS_JOINs and
      MARS_LEAVEs.\n   The MCS tracks the Server Sequence Number from the mar$msn
      fields of\n   messages from the MARS, and revalidates its outgoing point to\n
      \  multipoint VC(s) when a sequence number jump occurs.\n"
    title: 7.3   Management of outgoing VC.
  - contents:
    - "7.4   Use of a backup MARS.\n   The MCS uses the same approach to backup MARSs
      as a cluster member\n   (section 5.4), tracking MARS_REDIRECT_MAP messages on\n
      \  ServerControlVC.\n"
    title: 7.4   Use of a backup MARS.
  title: 7.   How an MCS utilises a MARS.
- contents:
  - "8.   Support for IP multicast routers.\n   Multicast routers are required for
    the propagation of multicast\n   traffic beyond the constraints of a single cluster
    (inter-cluster\n   traffic).  (In a sense, they are multicast servers acting at
    the next\n   higher layer, with clusters, rather than individual endpoints, as\n
    \  their abstract sources and destinations.)\n   Multicast routers typically participate
    in higher layer multicast\n   routing algorithms and policies that are beyond
    the scope of this\n   memo (e.g. DVMRP [5] in the IPv4 environment).\n   It is
    assumed that the multicast routers will be implemented over the\n   same sort
    of IP/ATM interface that a multicast host would use.  Their\n   IP/ATM interfaces
    will register with the MARS as cluster members,\n   joining and leaving multicast
    groups as necessary. As noted in\n   section 5, multiple logical 'endpoints' may
    be implemented over a\n   single physical ATM interface. Routers use this approach
    to provide\n   interfaces into each of the clusters they will be routing between.\n
    \  The rest of this section will assume a simple IPv4 scenario where the\n   scope
    of a cluster has been limited to a particular LIS that is part\n   of an overlaid
    IP network. Not all members of the LIS are necessarily\n   registered cluster
    members (you may have unicast-only hosts in the\n   LIS).\n"
  - contents:
    - "8.1    Forwarding into a Cluster.\n   If the multicast router needs to transmit
      a packet to a group within\n   the cluster its IP/ATM interface opens a VC in
      the same manner as a\n   normal host would. Once a VC is open, the router watches
      for\n   MARS_JOIN and MARS_LEAVE messages and responds to them as a normal\n
      \  host would.\n   The multicast router's transmit side MUST implement inactivity
      timers\n   to shut down idle outgoing VCs, as for normal hosts.\n   As with
      normal host, the multicast router does not need to be a\n   member of a group
      it is sending to.\n"
    title: 8.1    Forwarding into a Cluster.
  - contents:
    - "8.2    Joining in 'promiscuous' mode.\n   Once registered and initialised,
      the simplest model of IPv4 multicast\n   router operation is for it to issue
      a MARS_JOIN encompassing the\n   entire Class D address space.  In effect it
      becomes 'promiscuous', as\n   it will be a leaf node to all present and future
      multipoint VCs\n   established to IPv4 groups on the cluster.\n   How a router
      chooses which groups to propagate outside the cluster is\n   beyond the scope
      of this document.\n   Consistent with RFC 1112, IP multicast routers may retain
      the use of\n   IGMP Query and IGMP Report messages to ascertain group membership.\n
      \  However, certain optimisations are possible, and are described in\n   section
      8.5.\n"
    title: 8.2    Joining in 'promiscuous' mode.
  - contents:
    - "8.3    Forwarding across the cluster.\n   Under some circumstances the cluster
      may simply be another hop\n   between IP subnets that have participants in a
      multicast group.\n      [LAN.1] ----- IPmcR.1 -- [cluster/LIS] -- IPmcR.2 -----
      [LAN.2]\n   LAN.1 and LAN.2 are subnets (such as Ethernet) with attached hosts\n
      \  that are members of group X.\n   IPmcR.1 and IPmcR.2 are multicast routers
      with interfaces to the LIS.\n   A traditional solution would be to treat the
      LIS as a unicast subnet,\n   and use tunneling routers. However, this would
      not allow hosts on the\n   LIS to participate in the cross-LIS traffic.\n   Assume
      IPmcR.1 is receiving packets promiscuously on its LAN.1\n   interface. Assume
      further it is configured to propagate multicast\n   traffic to all attached
      interfaces. In this case that means the LIS.\n   When a packet for group X arrives
      on its LAN.1 interface, IPmcR.1\n   simply sends the packet to group X on the
      LIS interface as a normal\n   host would (Issuing MARS_REQUEST for group X,
      creating the VC,\n   sending the packet).\n   Assuming IPmcR.2 initialised itself
      with the MARS as a member of the\n   entire Class D space, it will have been
      returned as a member of X\n   even if no other nodes on the LIS were members.
      All packets for group\n   X received on IPmcR.2's LIS interface may be retransmitted
      on LAN.2.\n   If IPmcR.1 is similarly initialised the reverse process will apply\n
      \  for multicast traffic from LAN.2 to LAN.1, for any multicast group.\n   The
      benefit of this scenario is that cluster members within the LIS\n   may also
      join and leave group X at anytime.\n"
    title: 8.3    Forwarding across the cluster.
  - contents:
    - "8.4   Joining in 'semi-promiscuous' mode.\n   Both unicast and multicast IP
      routers have a common problem -\n   limitations on the number of AAL contexts
      available at their ATM\n   interfaces.  Being 'promiscuous' in the RFC 1112
      sense means that for\n   every M hosts sending to N groups, a multicast router's
      ATM interface\n   will have M*N incoming reassembly engines tied up.\n   It
      is not hard to envisage situations where a number of multicast\n   groups are
      active within the LIS but are not required to be\n   propagated beyond the LIS
      itself. An example might be a distributed\n   simulation system specifically
      designed to use the high speed IP/ATM\n   environment. There may be no practical
      way its traffic could be\n   utilised on 'the other side' of the multicast router,
      yet under the\n   conventional scheme the router would have to be a leaf to
      each\n   participating host anyway.\n   As this problem occurs below the IP
      layer, it is worth noting that\n   'scoping' mechanisms at the IP multicast
      routing level do not provide\n   a solution. An IP level scope would still result
      in the router's ATM\n   interface receiving traffic on the scoped groups, only
      to drop it.\n   In this situation the network administrator might configure
      their\n   multicast routers to exclude sections of the Class D address space\n
      \  when issuing MARS_JOIN(s). Multicast groups that will never be\n   propagated
      beyond the cluster will not have the router listed as a\n   member, and the
      router will never have to receive (and simply ignore)\n   traffic from those
      groups.\n   Another scenario involves the product M*N exceeding the capacity
      of a\n   single router's interface (especially if the same interface must also\n
      \  support a unicast IP router service).\n   A network administrator may choose
      to add a second node, to function\n   as a parallel IP multicast router. Each
      router would be configured to\n   be 'promiscuous' over separate parts of the
      Class D address space,\n   thus exposing themselves to only part of the VC load.
      This sharing\n   would be completely transparent to IP hosts within the LIS.\n
      \  Restricted promiscuous mode does not break RFC 1112's use of IGMP\n   Report
      messages. If the router is configured to serve a given block\n   of Class D
      addresses, it will receive the IGMP Report.  If the router\n   is not configured
      to support a given block, then the existence of an\n   IGMP Report for a group
      in that block is irrelevant to the router.\n   All routers are able to track
      membership changes through the\n   MARS_JOIN and MARS_LEAVE traffic anyway.
      (Section 8.5 discusses a\n   better alternative to IGMP within a cluster.)\n
      \  Mechanisms and reasons for establishing these modes of operation are\n   beyond
      the scope of this document.\n"
    title: 8.4   Joining in 'semi-promiscuous' mode.
  - contents:
    - "8.5   An alternative to IGMP Queries.\n   An unfortunate aspect of IGMP is
      that it assumes multicasting of IP\n   packets is a cheap and trivial event
      at the link layer. As a\n   consequence, regular IGMP Queries are multicasted
      by routers to group\n   224.0.0.1. These queries are intended to trigger IGMP
      Replies by\n   cluster members that have layer 3 members of particular groups.\n
      \  The MARS_GROUPLIST_REQUEST and MARS_GROUPLIST_REPLY messages were\n   designed
      to allow routers to avoid actually transmitting IGMP Queries\n   out into a
      cluster.\n   Whenever the router's forwarding engine wishes to transmit an IGMP\n
      \  query, a MARS_GROUPLIST_REQUEST can be sent to the MARS instead. The\n   resulting
      MARS_GROUPLIST_REPLY(s) (described in section 5.3) from the\n   MARS carry all
      the information that the router would have ascertained\n   from IGMP replies.\n
      \  It is RECOMMENDED that multicast routers utilise this MARS service to\n   minimise
      IGMP traffic within the cluster.\n   By default a MARS_GROUPLIST_REQUEST SHOULD
      specify the entire address\n   space (e.g. <224.0.0.0, 239.255.255.255> in an
      IPv4 environment).\n   However, routers serving part of the address space (as
      described in\n   section 8.4) MAY choose to issue MARS_GROUPLIST_REQUESTs that
      specify\n   only the subset of the address space they are serving.\n   (On the
      surface it would also seem useful for multicast routers to\n   track MARS_JOINs
      and MARS_LEAVEs that arrive with mar$flags.layer3grp\n   set. These might be
      used in lieu of IGMP Reports, to provide the\n   router with timely indication
      that a new layer 3 group member exists\n   within the cluster. However, this
      only works on VC mesh supported\n   groups, and is therefore NOT recommended).\n
      \  Appendix B discusses less elegant mechanisms for reducing the impact\n   of
      IGMP traffic within a cluster, on the assumption that the IP/ATM\n   interfaces
      to the cluster are being used by un-optimised IP\n   multicasting code.\n"
    title: 8.5   An alternative to IGMP Queries.
  - contents:
    - "8.6   CMIs across multiple interfaces.\n   The Cluster Member ID is only unique
      within the Cluster managed by a\n   given MARS. On the surface this might appear
      to leave us with a\n   problem when a multicast router is routing between two
      or more\n   Clusters using a single physical ATM interface.  The router will\n
      \  register with two or more MARSs, and thereby acquire two or more\n   independent
      CMI's. Given that each MARS has no reason to synchronise\n   their CMI allocations,
      it is possible for a host in one cluster to\n   have the same CMI has the router's
      interface to another Cluster. How\n   does the router distinguish between its
      own reflected packets, and\n   packets from that other host?\n   The answer
      lies in the fact that routers (and hosts) actually\n   implement logical IP/ATM
      interfaces over a single physical ATM\n   interface. Each logical interface
      will have a unique ATM Address (eg.\n   an NSAP with different SELector fields,
      one for each logical\n   interface).\n   Each logical IP/ATM interface is configured
      with the address of a\n   single MARS, attaches to only one cluster, and so
      has only one CMI to\n   worry about. Each of the MARSs that the router is registered
      with\n   will have been given a different ATM Address (corresponding to the\n
      \  different logical IP/ATM interfaces) in each registration MARS_JOIN.\n   When
      hosts in a cluster add the router as a leaf node, they'll\n   specify the ATM
      Address of the appropriate logical IP/ATM interface\n   on the router in the
      L_MULTI_ADD message. Thus, each logical IP/ATM\n   interface will only have
      to check and filter on CMIs assigned by its\n   own MARS.\n   In essence the
      cluster differentiation is achieved by ensuring that\n   logical IP/ATM interfaces
      are assigned different ATM Addresses.\n"
    title: 8.6   CMIs across multiple interfaces.
  title: 8.   Support for IP multicast routers.
- contents:
  - "9.    Multiprotocol applications of the MARS and MARS clients.\n   A deliberate
    attempt has been made to describe the MARS and\n   associated mechanisms in a
    manner independent of a specific higher\n   layer protocol being run over the
    ATM cloud. The immediate\n   application of this document will be in an IPv4 environment,
    and this\n   is reflected by the focus of key examples.  However, the mar$pro.type\n
    \  and mar$pro.snap fields in every MARS control message allow any\n   higher
    layer protocol that has a 'short form' or 'long form' of\n   protocol identification
    (section 4.3) to be supported by a MARS.\n   Every MARS MUST implement entirely
    separate logical mapping tables\n   and support. Every cluster member must interpret
    messages from the\n   MARS in the context of the protocol type that the MARS message
    refers\n   to.\n   Every MARS and MARS client MUST treat Cluster Member IDs in
    the\n   context of the protocol type carried in the MARS message or data\n   packet
    containing the CMI.\n   For example, IPv6 has been allocated an Ethertype of 0x86DD.
    \ This\n   means the 'short form' of protocol identification must be used in the\n
    \  MARS control messages and the data path encapsulation (section 5.5).\n   An
    IPv6 multicasting client sets the mar$pro.type field of every MARS\n   message
    to 0x86DD.  When carrying IPv6 addresses the mar$spln and\n   mar$tpln fields
    are either 0 (for null or non-existent information)\n   or 16 (for the full IPv6
    address).\n   Following the rules in section 5.5, an IPv6 data packet is\n   encapsulated
    as:\n      [0xAA-AA-03][0x00-00-5E][0x00-01][pkt$cmi][0x86DD][IPv6 packet]\n   A
    host or endpoint interface that is using the same MARS to support\n   multicasting
    needs of multiple protocols MUST not assume their CMI\n   will be the same for
    each protocol.\n"
  title: 9.    Multiprotocol applications of the MARS and MARS clients.
- contents:
  - "10.    Supplementary parameter processing.\n   The mar$extoff field in the [Fixed
    header] indicates whether\n   supplementary parameters are being carried by a
    MARS control message.\n   This mechanism is intended to enable the addition of
    new\n   functionality to the MARS protocol in later documents.\n   Supplementary
    parameters are conveyed as a list of TLV (type, length,\n   value) encoded information
    elements.  The TLV(s) begin on the first\n   32 bit boundary following the [Addresses]
    field in the MARS control\n   message (e.g. after mar$tsa.N in a MARS_MULTI, after
    mar$max.N in a\n   MARS_JOIN, etc).\n"
  - contents:
    - "10.1   Interpreting the mar$extoff field.\n   If the mar$extoff field is non-zero
      it indicates that a list of one\n   or more TLVs have been appended to the MARS
      message.  The first TLV\n   is found by treating mar$extoff as an unsigned integer
      representing\n   an offset (in octets) from the beginning of the MARS message
      (the MSB\n   of the mar$afn field).\n   As TLVs are 32 bit aligned the bottom
      2 bits of mar$extoff are also\n   reserved. A receiver MUST mask off these two
      bits before calculating\n   the octet offset to the TLV list.  A sender MUST
      set these two bits\n   to zero.\n   If mar$extoff is zero no TLVs have been
      appended.\n"
    title: 10.1   Interpreting the mar$extoff field.
  - contents:
    - "10.2   The format of TLVs.\n   When they exist, TLVs begin on 32 bit boundaries,
      are multiples of 32\n   bits in length, and form a sequential list terminated
      by a NULL TLV.\n   The TLV structure is:\n      [Type - 2 octets][Length - 2
      octets][Value - n*4 octets]\n   The Type subfield indicates how the contents
      of the Value subfield\n   are to be interpreted.\n   The Length subfield indicates
      the number of VALID octets in the Value\n   subfield. Valid octets in the Value
      subfield start immediately after\n   the Length subfield.  The offset (in octets)
      from the start of this\n   TLV to the start of the next TLV in the list is given
      by the\n   following formula:\n      offset = (length + 4 + ((4-(length & 3))
      % 4))\n   (where % is the modulus operator)\n   The Value subfield is padded
      with 0, 1, 2, or 3 octets to ensure the\n   next TLV is 32 bit aligned. The
      padded locations MUST be set to zero.\n   (For example, a TLV that needed only
      5 valid octets of information\n   would be 12 octets long. The Length subfield
      would hold the value 5,\n   and the Value subfield would be padded out to 8
      bytes.  The 5 valid\n   octets of information begin at the first octet of the
      Value\n   subfield.)\n   The Type subfield is formatted in the following way:\n
      \         |   1st octet   |   2nd octet   |\n           7 6 5 4 3 2 1 0 7 6
      5 4 3 2 1 0\n          +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n          | x |               y
      \          |\n          +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   The most significant
      2 bits (Type.x) determine how a recipient should\n   behave when it doesn't
      recognise the TLV type indicated by the lower\n   14 bits (Type.y). The required
      behaviours are:\n      Type.x = 0   Skip the TLV, continue processing the list.\n
      \     Type.x = 1   Stop processing, silently drop the MARS message.\n      Type.x
      = 2   Stop processing, drop message, give error indication.\n      Type.x =
      3   Reserved. (currently treat as x = 0)\n   (The error indication generated
      when Type.x = 2 SHOULD be logged in\n   some locally significant fashion. Consequential
      MARS message activity\n   in response to such an error condition will be defined
      in future\n   documents.)\n   The TLV type space (Type.y) is further subdivided
      to encourage use\n   outside the IETF.\n      0                       Null TLV.\n
      \     0x0001 - 0x0FFF         Reserved for the IETF.\n      0x1000 - 0x11FF
      \        Allocated to the ATM Forum.\n      0x1200 - 0x37FF         Reserved
      for the IETF.\n      0x3800 - 0x3FFF         Experimental use.\n"
    title: 10.2   The format of TLVs.
  - contents:
    - "10.3   Processing MARS messages with TLVs.\n   Supplementary parameters act
      as modifiers to the basic behaviour\n   specified by the mar$op field of any
      given MARS message.\n   If a MARS message arrives with a non-zero mar$extoff
      field its TLV\n   list MUST be parsed before handling the MARS message in accordance\n
      \  with the mar$op value. Unrecognised TLVs MUST be handled as required\n   by
      their Type.x value.\n   How TLVs modify basic MARS operations will be mar$op
      and TLV\n   specific.\n"
    title: 10.3   Processing MARS messages with TLVs.
  - contents:
    - "10.4   Initial set of TLV elements.\n   Conformance with this document only
      REQUIRES the recognition of one\n   TLV, the Null TLV. This terminates a list
      of TLVs, and MUST be\n   present if mar$extoff is non-zero in a MARS message.
      It MAY be the\n   only TLV present.\n   The Null TLV is coded as:\n      [0x00-00][0x00-00]\n
      \  Future documents will describe the formats, contents, and\n   interpretations
      of additional TLVs. The minimal parsing requirements\n   imposed by this document
      are intended to allow conformant MARS and\n   MARS client implementations to
      deal gracefully and predictably with\n   future TLV developments.\n"
    title: 10.4   Initial set of TLV elements.
  title: 10.    Supplementary parameter processing.
- contents:
  - "11.    Key Decisions and open issues.\n   The key decisions this document proposes:\n
    \     A Multicast Address Resolution Server (MARS) is proposed to co-\n      ordinate
    and distribute mappings of ATM endpoint addresses to\n      arbitrary higher layer
    'multicast group addresses'. The specific\n      case of IPv4 multicast is used
    as the example.\n      The concept of 'clusters' is introduced to define the scope
    of a\n      MARS's responsibility, and the set of ATM endpoints willing to\n      participate
    in link level multicasting.\n      A MARS is described with the functionality
    required to support\n      intra-cluster multicasting using either VC meshes or
    ATM level\n      multicast servers (MCSs).\n      LLC/SNAP encapsulation of MARS
    control messages allows MARS and\n      ATMARP traffic to share VCs, and allows
    partially co-resident MARS\n      and ATMARP entities.\n      New message types:\n
    \        MARS_JOIN, MARS_LEAVE, MARS_REQUEST. Allow endpoints to join,\n         leave,
    and request the current membership list of multicast\n         groups.\n         MARS_MULTI.
    Allows multiple ATM addresses to be returned by the\n         MARS in response
    to a MARS_REQUEST.\n         MARS_MSERV, MARS_UNSERV. Allow multicast servers
    to register\n         and deregister themselves with the MARS.\n         MARS_SJOIN,
    MARS_SLEAVE. Allow MARS to pass on group membership\n         changes to multicast
    servers.\n         MARS_GROUPLIST_REQUEST, MARS_GROUPLIST_REPLY.  Allow MARS to\n
    \        indicate which groups have actual layer 3 members. May be used\n         to
    support IGMP in IPv4 environments, and similar functions in\n         other environments.\n
    \        MARS_REDIRECT_MAP.  Allow MARS to specify a set of backup MARS\n         addresses.\n
    \        MARS_MIGRATE.  Allows MARS to force cluster members to shift\n         from
    VC mesh to MCS based forwarding tree in single operation.\n      'wild card' MARS
    mapping table entries are possible, where a\n      single ATM address is simultaneously
    associated with blocks of\n      multicast group addresses.\n   For the MARS protocol
    mar$op.version = 0. The complete set of MARS\n   control messages and mar$op.type
    values is:\n      1   MARS_REQUEST\n      2   MARS_MULTI\n      3   MARS_MSERV\n
    \     4   MARS_JOIN\n      5   MARS_LEAVE\n      6   MARS_NAK\n      7   MARS_UNSERV\n
    \     8   MARS_SJOIN\n      9   MARS_SLEAVE\n      10  MARS_GROUPLIST_REQUEST\n
    \     11  MARS_GROUPLIST_REPLY\n      12  MARS_REDIRECT_MAP\n      13  MARS_MIGRATE\n
    \  A number of issues are left open at this stage, and are likely to be\n   the
    subject of on-going research and additional documents that build\n   upon this
    one.\n      The specified endpoint behaviour allows the use of\n      redundant/backup
    MARSs within a cluster. However, no\n      specifications yet exist on how these
    MARSs co-ordinate amongst\n      themselves. (The default is to only have one
    MARS per cluster.)\n      The specified endpoint behaviour and MARS service allows
    the use\n      of multiple MCSs per group.  However, no specifications yet exist\n
    \     on how this may be used, or how these MCSs co-ordinate amongst\n      themselves.
    \ Until futher work is done on MCS co-ordination\n      protocols the default
    is to only have one MCS per group.\n      The MARS relies on the cluster member
    dropping off\n      ClusterControlVC if the cluster member dies. It is not clear
    if\n      additional mechanisms are needed to detect and delete 'dead'\n      cluster
    members.\n      Supporting layer 3 'broadcast' as a special case of multicasting\n
    \     (where the 'group' encompasses all cluster members) has not been\n      explicitly
    discussed.\n      Supporting layer 3 'unicast' as a special case of multicasting\n
    \     (where the 'group' is a single cluster member, identified by the\n      cluster
    member's unicast protocol address) has not been explicitly\n      discussed.\n
    \     The future development of ATM Group Addresses and Leaf Initiated\n      Join
    to ATM Forum's UNI specification has not been addressed.\n      (However, the
    problems identified in this document with respect to\n      VC scarcity and impact
    on AAL contexts will not be fixed by such\n      developments in the signalling
    protocol.)\n      Possible modifications to the interpretation of the mar$hrdrsv
    and\n      mar$afn fields in the Fixed header, based on different values for\n
    \     mar$op.version, are for further study.\n"
  - "Security Considerations\n   Security issues are not addressed in this document.\n"
  - "Acknowledgments\n   The discussions within the IP over ATM Working Group have
    helped\n   clarify the ideas expressed in this document. John Moy (Cascade\n   Communications
    Corp.) initially suggested the idea of wild-card\n   entries in the ARP Server.
    \ Drew Perkins (Fore Systems) provided\n   rigorous and useful critique of early
    proposed mechanisms for\n   distributing and validating group membership information.
    \ Susan\n   Symington (and co-workers at MITRE Corp., Don Chirieleison, and Bill\n
    \  Barns) clearly articulated the need for multicast server support,\n   proposed
    a solution, and challenged earlier block join/leave\n   mechanisms. John Shirron
    (Fore Systems) provided useful improvements\n   on my original revalidation procedures.\n
    \  Susan Symington and Bryan Gleeson (Adaptec) independently championed\n   the
    need for the service provided by MARS_GROUPLIST_REQUEST/REPLY.\n   The new encapsulation
    scheme arose from WG discussions, captured by\n   Bryan Gleeson in an interim
    Work in Progress (with Keith McCloghrie\n   (Cisco), Andy Malis (Ascom Nexion),
    and Andrew Smith (Bay Networks)\n   as key contributors).  James Watt (Newbridge)
    and Joel Halpern\n   (Newbridge) motivated the development of a more multiprotocol
    MARS\n   control message format, evolving it away from its original ATMARP\n   roots.
    \ They also motivated the development of Type #1 and Type #2\n   data path encapsulations.
    \ Rajesh Talpade (Georgia Tech) helped\n   clarify the need for the MARS_MIGRATE
    function.\n   Maryann Maher (ISI) provided valuable sanity and implementation\n
    \  checking during the latter stages of the document's development.\n   Finally,
    Jim Rubas (IBM) supplied the MARS pseudo-code in Appendix F\n   and also provided
    detailed proof-reading in the latter stages of the\n   document's development.\n"
  - "Author's Address\n   Grenville Armitage\n   Bellcore, 445 South Street\n   Morristown,
    NJ, 07960\n   USA\n   EMail: gja@thumper.bellcore.com\n   Phone: +1 201 829 2635\n"
  - "References\n   [1] Deering, S., \"Host Extensions for IP Multicasting\", STD
    3, RFC\n   1112, Stanford University, August 1989.\n   [2] Heinanen, J., \"Multiprotocol
    Encapsulation over ATM Adaption\n   Layer 5\", RFC 1483, Telecom Finland, July
    1993.\n   [3] Laubach, M., \"Classical IP and ARP over ATM\", RFC 1577, Hewlett-\n
    \  Packard Laboratories, December 1993.\n   [4] ATM Forum, \"ATM User Network
    Interface (UNI) Specification\n   Version 3.1\", ISBN 0-13-393828-X, Prentice
    Hall, Englewood Cliffs,\n   NJ, June 1995.\n   [5] Waitzman, D., Partridge, C.,
    and S. Deering, \"Distance Vector\n   Multicast Routing Protocol\", RFC 1075,
    November 1988.\n   [6] Perez, M., Liaw, F., Grossman, D., Mankin, A., Hoffman,
    E., and\n   A.  Malis, \"ATM Signaling Support for IP over ATM\", RFC 1755,\n
    \  February 1995.\n   [7] Borden, M., Crawley, E., Davie, B., and S. Batsell,
    \"Integration\n   of Real-time Services in an IP-ATM Network Architecture.\",
    RFC 1821,\n   August 1995.\n   [8] ATM Forum, \"ATM User-Network Interface Specification
    Version\n   3.0\", Englewood Cliffs, NJ: Prentice Hall, September 1993.\n"
  title: 11.    Key Decisions and open issues.
- contents:
  - "Appendix A.  Hole punching algorithms.\n   Implementations are entirely free
    to comply with the body of this\n   memo in any way they see fit. This appendix
    is purely for\n   clarification.\n   A MARS implementation might pre-construct
    a set of <min,max> pairs\n   (P) that reflects the entire Class D space, excluding
    any addresses\n   currently supported by multicast servers. The <min> field of
    the\n   first pair MUST be 224.0.0.0, and the <max> field of the last pair\n   must
    be 239.255.255.255. The first and last pair may be the same.\n   This set is updated
    whenever a multicast server registers or\n   deregisters.\n   When the MARS must
    perform 'hole punching' it might consider the\n   following algorithm:\n      Assume
    the MARS_JOIN/LEAVE received by the MARS from the cluster\n      member specified
    the block <Emin, Emax>.\n      Assume Pmin(N) and Pmax(N) are the <min> and <max>
    fields from the\n      Nth pair in the MARS's current set P.\n      Assume set
    P has K pairs. Pmin(1) MUST equal 224.0.0.0, and\n      Pmax(M) MUST equal 239.255.255.255.
    (If K == 1 then no hole\n      punching is required).\n      Execute pseudo-code:\n
    \        create copy of set P, call it set C.\n         index1 = 1;\n         while
    (Pmax(index1) <= Emin)\n            index1++;\n         index2 = K;\n         while
    (Pmin(index2) >= Emax)\n            index2--;\n         if (index1 > index2)\n
    \           Exit, as the hole-punched set is null.\n         if (Pmin(index1)
    < Emin)\n            Cmin(index1) = Emin;\n         if (Pmax(index2) > Emax)\n
    \           Cmax(index2) = Emax;\n         Set C is the required 'hole punched'
    set of address blocks.\n   The resulting set C retains all the MARS's pre-constructed
    'holes'\n   covering the multicast servers, but will have been pruned to cover\n
    \  the section of the Class D space specified by the originating host's\n   <Emin,Emax>
    values.\n   The host end should keep a table, H, of open VCs in ascending order\n
    \  of Class D address.\n      Assume H(x).addr is the Class address associated
    with VC.x.\n      Assume H(x).addr < H(x+1).addr.\n   The pseudo code for updating
    VCs based on an incoming JOIN/LEAVE\n   might be:\n      x = 1;\n      N = 1;\n
    \     while (x < no.of VCs open)\n      {\n            while (H(x).addr > max(N))\n
    \           {\n                  N++;\n                  if (N > no. of pairs
    in JOIN/LEAVE)\n                        return(0);\n            }\n            if
    ((H(x).addr <= max(N) &&\n                        ((H(x).addr >= min(N))\n                              perform_VC_update();\n
    \           x++;\n      }\n"
  title: Appendix A.  Hole punching algorithms.
- contents:
  - "Appendix B.  Minimising the impact of IGMP in IPv4 environments.\n   Implementing
    any part of this appendix is not required for\n   conformance with this document.
    \ It is provided solely to document\n   issues that have been identified.\n   The
    intent of section 5.1 is for cluster members to only have\n   outgoing point to
    multipoint VCs when they are actually sending data\n   to a particular multicast
    group. However, in most IPv4 environments\n   the multicast routers attached to
    a cluster will periodically issue\n   IGMP Queries to ascertain if particular
    groups have members.  The\n   current IGMP specification attempts to avoid having
    every group\n   member respond by insisting that each group member wait a random\n
    \  period, and responding if no other member has responded before them.\n   The
    IGMP reply is sent to the multicast address of the group being\n   queried.\n
    \  Unfortunately, as it stands the IGMP algorithm will be a nuisance for\n   cluster
    members that are essentially passive receivers within a given\n   multicast group.
    It is just as likely that a passive member, with no\n   outgoing VC already established
    to the group, will decide to send an\n   IGMP reply - causing a VC to be established
    where there was no need\n   for one. This is not a fatal problem for small clusters,
    but will\n   seriously impact on the ability of a cluster to scale.\n   The most
    obvious solution is for routers to use the\n   MARS_GROUPLIST_REQUEST and MARS_GROUPLIST_REPLY
    messages, as\n   described in section 8.5. This would remove the regular IGMP
    Queries,\n   resulting in cluster members only sending an IGMP Report when they\n
    \  first join a group.\n   Alternative solutions do exist. One would be to modify
    the IGMP reply\n   algorithm, for example:\n      If the group member has VC open
    to the group proceed as per RFC\n      1112 (picking a random reply delay between
    0 and 10 seconds).\n      If the group member does not have VC already open to
    the group,\n      pick random reply delay between 10 and 20 seconds instead, and\n
    \     then proceed as per RFC 1112.\n   If even one group member is sending to
    the group at the time the IGMP\n   Query is issued then all the passive receivers
    will find the IGMP\n   Reply has been transmitted before their delay expires,
    so no new VC\n   is required. If all group members are passive at the time of
    the IGMP\n   Query then a response will eventually arrive, but 10 seconds later\n
    \  than under conventional circumstances.\n   The preceding solution requires
    re-writing existing IGMP code, and\n   implies the ability of the IGMP entity
    to ascertain the status of VCs\n   on the underlying ATM interface. This is not
    likely to be available\n   in the short term.\n   One short term solution is to
    provide something like the preceding\n   functionality with a 'hack' at the IP/ATM
    driver level within cluster\n   members. Arrange for the IP/ATM driver to snoop
    inside IP packets\n   looking for IGMP traffic. If an IGMP packet is accepted
    for\n   transmission, the IP/ATM driver can buffer it locally if there is no\n
    \  VC already active to that group. A 10 second timer is started, and if\n   an
    IGMP Reply for that group is received from elsewhere on the\n   cluster the timer
    is reset. If the timer expires, the IP/ATM driver\n   then establishes a VC to
    the group as it would for a normal IP\n   multicast packet.\n   Some network implementors
    may find it advantageous to configure a\n   multicast server to support the group
    224.0.0.1, rather than rely on\n   a mesh. Given that IP multicast routers regularly
    send IGMP queries\n   to this address, a mesh will mean that each router will
    permanently\n   consume an AAL context within each cluster member. In clusters
    served\n   by multiple routers the VC load within switches in the underlying ATM\n
    \  network will become a scaling problem.\n   Finally, if a multicast server is
    used to support 224.0.0.1, another\n   ATM driver level hack becomes a possible
    solution to IGMP Reply\n   traffic.  The ATM driver may choose to grab all outgoing
    IGMP packets\n   and send them out on the VC established for sending to 224.0.0.1,\n
    \  regardless of the Class D address the IGMP message was actually for.\n   Given
    that all hosts and routers must be members of 224.0.0.1, the\n   intended recipients
    will still receive the IGMP Replies. The negative\n   impact is that all cluster
    members will receive the IGMP Replies.\n"
  title: Appendix B.  Minimising the impact of IGMP in IPv4 environments.
- contents:
  - "Appendix C.   Further comments on 'Clusters'.\n   The cluster concept was introduced
    in section 1 for two reasons.  The\n   more well known term of Logical IP Subnet
    is both very IP specific,\n   and constrained to unicast routing boundaries. As
    the architecture\n   described in this document may be re-used in non-IP environments
    a\n   more neutral term was needed. As the needs of multicasting are not\n   always
    bound by the same scopes as unicasting, it was not immediately\n   obvious that
    apriori limiting ourselves to LISs was beneficial in the\n   long term.\n   It
    must be stressed that Clusters are purely an administrative being.\n   You choose
    their size (i.e. the number of endpoints that register\n   with the same MARS)
    based on your multicasting needs, and the\n   resource consumption you are willing
    to put up with. The larger the\n   number of ATM attached hosts you require multicast
    support for, the\n   more individual clusters you might choose to establish (along
    with\n   multicast routers to provide inter-cluster traffic paths).\n   Given
    that not all the hosts in any given LIS may require multicast\n   support, it
    becomes conceivable that you might assign a single MARS\n   to support hosts from
    across multiple LISs. In effect you have a\n   cluster covering multiple LISs,
    and have achieved 'cut through'\n   routing for multicast traffic. Under these
    circumstances increasing\n   the geographical size of a cluster might be considered
    a good thing.\n   However, practical considerations limit the size of clusters.
    \ Having\n   a cluster span multiple LISs may not always be a particular 'win'\n
    \  situation.  As the number of multicast capable hosts in your LISs\n   increases
    it becomes more likely that you'll want to constrain a\n   cluster's size and
    force multicast traffic to aggregate at multicast\n   routers scattered across
    your ATM cloud.\n   Finally, multi-LIS clusters require a degree of care when
    deploying\n   IP multicast routers. Under the Classical IP model you need unicast\n
    \  routers on the edges of LISs. Under the MARS architecture you only\n   need
    multicast routers at the edges of clusters. If your cluster\n   spans multiple
    LISs, then the multicast routers will perceive\n   themselves to have a single
    interface that is simultaneously attached\n   to multiple unicast subnets. Whether
    this situation will work depends\n   on the inter-domain multicast routing protocols
    you use, and your\n   multicast router's ability to understand the new relationship
    between\n   unicast and multicast topologies.\n   In the absence of futher research
    in this area, networks deployed in\n   conformance to this document MUST make
    their IP cluster and IP LIS\n   coincide, so as to avoid these complications.\n"
  title: Appendix C.   Further comments on 'Clusters'.
- contents:
  - "Appendix D.  TLV list parsing algorithm.\n   The following pseudo-code represents
    how the TLV list format\n   described in section 10 could be handled by a MARS
    or MARS client.\n      list = (mar$extoff & 0xFFFC);\n      if (list == 0) exit;\n
    \     list = list + message_base;\n      while (list->Type.y != 0)\n            {\n
    \                 switch (list->Type.y)\n                  {\n                        default:\n
    \                         {\n                           if (list->Type.x == 0)
    break;\n                           if (list->Type.x == 1) exit;\n                           if
    (list->Type.x == 2) log-error-and-exit;\n                          }\n                        [...other
    handling goes here..]\n                  }\n                  list += (list->Length
    + 4 + ((4-(list->Length & 3)) %\n                  4));\n            }\n      return;\n"
  title: Appendix D.  TLV list parsing algorithm.
- contents:
  - "Appendix E.  Summary of timer values.\n   This appendix summarises various timers
    or limits mentioned in the\n   main body of the document. Values are specified
    in the following\n   format:  [x, y, z] indicating a minimum value of x, a recommended\n
    \  value of y, and a maximum value of z. A '-' will indicate that a\n   category
    has no value specified. Values in minutes are followed by\n   'min', values in
    seconds are followed by 'sec'.\n      Idle time for MARS - MARS client pt to pt
    VC:\n                                        [1 min, 20 min, -]\n      Idle time
    for multipoint VCs from client.\n                                        [1 min,
    20 min, -]\n      Allowed time between MARS_MULTI components.\n                                        [-,
    -, 10 sec]\n      Initial random L_MULTI_RQ/ADD retransmit timer range.\n                                        [5
    sec, -, 10 sec]\n      Random time to set VC_revalidate flag.\n                                        [1
    sec, -, 10 sec]\n      MARS_JOIN/LEAVE retransmit interval.\n                                        [5
    sec, 10 sec, -]\n      MARS_JOIN/LEAVE retransmit limit.\n                                        [-,
    -, 5]\n      Random time to re-register with MARS.\n                                        [1
    sec, -, 10 sec]\n      Force wait if MARS re-registration is looping.\n                                        [1
    min, -, -]\n      Transmission interval for MARS_REDIRECT_MAP.\n                                        [1
    min, 1 min, 2 min]\n      Limit for client to miss MARS_REDIRECT_MAPs.\n                                        [-,
    -, 4 min]\n"
  title: Appendix E.  Summary of timer values.
- contents:
  - "Appendix F.  Pseudo code for MARS operation.\n   Implementations are entirely
    free to comply with the body of this\n   memo in any way they see fit. This appendix
    is purely for possible\n   clarification.\n   A MARS implementation might be built
    along the lines suggested in\n   this pseudo-code.\n   1. Main\n    1.1 Initilization\n
    \        Define a server list as the list of leaf nodes\n                                            on
    ServerControlVC.\n         Define a cluster list as the list of leaf nodes\n                                            on
    ClusterControlVC.\n         Define a host map as the list of hosts that are\n
    \                                           members of a group.\n         Define
    a server map as the list of hosts (MCSs)\n                                            that
    are serving a group.\n         Read config file.\n         Allocate message queues.\n
    \        Allocate internal tables.\n         Set up passive open VC connection.\n
    \        Set up redirect_map timer.\n         Establish logging.\n    1.2 Message
    Processing\n         Forever {\n           If the message has a TLV then {\n             If
    TLV is unsupported then {\n               process as defined in TLV type field.\n
    \            } /* unknown TLV */\n           } /* TLV present */\n           Place
    incoming message in the queue.\n           For (all messages in the queue) {\n
    \            If the message is not a JOIN/LEAVE/MSERV/UNSERV with\n               mar$flags.register
    == 1 then {\n               If the message source is (not a member of server list)
    &&\n                (not a member of cluster list) then {\n                Drop
    the message silently.\n              }\n             }\n             If (mar$pro.type
    is not supported) or\n                (the ATM source address is missing) then
    {\n                Continue.\n             }\n             Determine type of message.\n
    \            If an ERR_L_RELEASE arrives on ClusterControlVC then {\n               Remove
    the endpoints ATM address from all groups\n               for which it has joined.\n
    \              Release the CMI.\n               Continue.\n             } /* error
    on CCVC */\n             Call specific message handling routine.\n             If
    redirect_map timer pops {\n               Call MARS_REDIRECT_MAP message handling
    routine.\n             } /* redirect timer pop */\n           } /* all msgs in
    the queue */\n         } /* forever loop */\n   2. Message Handler\n    2.1 Messages:\n
    \      - MARS_REQUEST\n         Indicate no MARS_MULTI support of TLV.\n         If
    the supported TLV is not NULL then {\n           Indicate MARS_MULTI support of
    TLV.\n           Process as required.\n         } else { /* TLV NULL */\n            Indicate
    message to be sent on Private VC.\n            If the message source is a member
    of server list then {\n              If the group has a non-null host map then
    {\n                Call MARS_MULTI with the host map for the group.\n              }
    else { /* no group */\n                 Call MARS_NAK message routine.\n              }
    /* no group */\n            } else { /* source is cluster list */\n               If
    the group has a non-null server map then {\n                 Call MARS_MULTI with
    the server map for the group.\n               } else { /* cluster member but no
    server map */\n                  If the group has a non-null host map then {\n
    \                   Call MARS_MULTI with the host map for the group.\n                  }
    else { /* no group */\n                     Call MARS_NAK message routine.\n                  }
    /* no group */\n                 } /* cluster member but no server map */\n              }
    /* source is a cluster list */\n            } /* TLV NULL */\n         If a message
    exists then {\n           Send message as indicated.\n         }\n         Return.\n
    \      - MARS_MULTI\n         Construct a MARS_MULTI for the specified map.\n
    \        If the param indicates TLV support then {\n           Process the TLV
    as required.\n         }\n         Return.\n     - MARS_JOIN\n        If (mar$flags.copy
    != 0) silently ignore the message.\n        If more than a single <min,max> pair
    is specified then\n        silently ignore the message.\n        Indicate message
    to be sent on private VC.\n        If (mar$flags.register == 1) then {\n          If
    the node is already a registered member of the cluster\n          associated with
    protocol type then { /*previous register*/\n            Copy the existing CMI
    into the MARS_JOIN.\n          } else { /* new register */\n             Add the
    node to ClusterControlVC.\n             Add the node to cluster list.\n             mar$cmi
    = obtain CMI.\n            } /* new register */\n         } else { /* not a register
    */\n           If the group is a duplicate of a previous MARS_JOIN then {\n             mar$msn
    = current csn.\n             Indicate message to be sent on Private VC.\n           }
    else {\n              Indicate no message to be sent.\n              If the message
    source is in server map then {\n                Drop the message silently.\n              }
    else {\n                 If the first <min,max> encompasses any group with\n                                                a
    server map then {\n                   Call the Modified JOIN/LEAVE Processing
    routine.\n                 } else {\n                    If the MARS_JOIN is for
    a multi group then {\n                     Call the MultiGroup JOIN/LEAVE Processing
    Routine.\n                    } else {\n                       Indicate message
    to be sent on ClusterControlVC.\n                    } /* not for a multi group
    */\n                  } /* group not handled by server */\n                 }
    /* msg src not in server map */\n                Update internal tables.\n              }
    /* not a duplicate */\n             } /* not a register */\n        If a message
    exists then {\n          mar$flags.copy = 1.\n          Send message as indicated.\n
    \       }\n        Return.\n     - MARS_LEAVE\n        If (mar$flags.copy != 0)
    silently ignore the message.\n        If more than a single <min,max> pair is
    specified then\n        silently ignore the message.\n        Indicate message
    to be sent on ClusterControlVC.\n        If (mar$flags.register == 1) then { /*
    deregistration */\n          Update internal tables to remove the member's ATM
    addr\n          from all groups it has joined.\n          Drop the endpoint from
    ClusterControlVC.\n          Drop the endpoint from cluster list.\n          Release
    the CMI.\n          Indicate message to be sent on Private VC.\n        } else
    { /* not a deregistration */\n           If the group is a duplicate of a previous
    MARS_LEAVE then {\n             mar$msn = current csn.\n             Indicate
    message to be sent on Private VC.\n           } else {\n              Indicate
    no message to be sent.\n              If the first <min,max> encompasses any group
    with\n                                             a server map then {\n                Call
    the Modified JOIN/LEAVE Processing routine.\n              } else {\n                 If
    the MARS_LEAVE is for a multi group then {\n                   Call the MultiGroup
    JOIN/LEAVE Processing Routine.\n                 } else {\n                    Indicate
    message to be sent on ClusterControlVC.\n                 }\n               }\n
    \          Update internal tables.\n          } /* not a duplicate */\n        }
    /* not a deregistration */\n        If a message exists then {\n          mar$flags.copy
    = 1.\n          Send message as indicated.\n        }\n        Return.\n    -
    MARS_MSERV\n         If (mar$flags.register == 1) then { /* server register */\n
    \          Add the endpoint as a leaf node to ServerControlVC.\n           Add
    the endpoint to the server list.\n           Indicate the message to be sent on
    Private VC.\n           mar$cmi = 0.\n         } else { /* not a register */\n
    \        If the source has not registered then {\n                 Drop and ignore
    the message.\n                 Indicate no message to be sent.\n               }
    else {  /* source is registered */\n                  If MCS is already member
    of indicated server map {\n                     Indicate message to be sent on
    Private VC.\n                     mar$flags.layer3grp = 0;\n                     mar$flags.copy
    = 1.\n                  } else { /* New MCS to add. */\n                     Add
    the server ATM addr to server map for group.\n                     Indicate message
    to be sent on ServerControlVC.\n                     Send message as indicated.\n
    \                    Make a copy of the message.\n                     Indicate
    message to be sent on ClusterControlVC.\n                     If new server map
    was just created {\n                          Construct MARS_MIGRATE, with MCS
    as target.\n                      } else {\n                          Change the
    op code to MARS_JOIN.\n                          mar$flags.layer3grp = 0.\n                          mar$flags.copy
    = 1.\n                      } /* new server map */\n                  } /* New
    MCS to add. */\n               } /* source is registered */\n         } /* not
    a register */\n         If a message exists then {\n           Send message as
    indicated.\n         }\n         Return.\n    - MARS_UNSERV\n      If (mar$flags.register
    == 1) then { /* deregister */\n        Remove the ATM addr of the MCS from all
    server maps.\n        If a server map becomes null then delete it.\n        Remove
    the endpoint as a leaf of ServerControlVC.\n        Remove the endpoint from server
    list.\n        Indicate the message to be sent on Private VC.\n      } else {
    /* not a deregister */\n         If the source is not a member of server list
    then {\n          Drop and ignore the message.\n          Indicate no message
    to be sent.\n         } else {  /* source is registered */\n            If MCS
    is not member of indicated server map {\n               Indicate message to be
    sent on Private VC.\n               mar$flags.layer3grp = 0;\n               mar$flags.copy
    = 1.\n             } else { /* MCS existed, must be removed. */\n               Remove
    ATM addr of the MCS from indicated server map.\n               If a server map
    is null then delete it.\n               Indicate the message to be sent on ServerControlVC.\n
    \              Send message as indicated.\n               Make a copy of the message.\n
    \              Change the op code to MARS_LEAVE.\n               Indicate message
    (copy) to be sent on ClusterControlVC.\n               mar$flags.layer3grp = 0;\n
    \              mar$flags.copy = 1.\n             } /* MCS existed, must be removed.
    */\n           } /* source is registered */\n        } /* not a deregister */\n
    \     If a message exists then {\n        Send message as indicated.\n      }\n
    \     Return.\n    - MARS_NAK\n      Build command.\n      Return.\n    - MARS_GROUPLIST_REQUEST\n
    \     If (mar$pnum != 1) then Return.\n      Call MARS_GROUPLIST_REPLY with the
    range and output VC.\n      Return.\n    - MARS_GROUPLIST_REPLY\n      Build command
    for specified range.\n      Indicate message to be sent on specified VC.\n      Send
    message as indicated.\n      Return.\n    - MARS_REDIRECT_MAP\n       Include
    the MARSs own address in the message.\n       If there are backup MARSs then include
    their addresses.\n       Indicate MARS_REDIRECT_MAP is to be sent on ClusterControlVC.\n
    \      Send message back as indicated.\n       Return.\n   3. Send Message Handler\n
    \     If (the message is going out ClusterControlVC) &&\n              (a new
    csn is required) then {\n       mar$msn = obtain a CSN\n      }\n      If (the
    message is going out ServerControlVC) &&\n              (a new ssn is required)
    then {\n       mar$msn = obtain a SSN\n      }\n      Return.\n   4.  Number Generator\n
    \  4.1 Cluster Sequence Number\n       Generate the next sequence number.\n       Return.\n
    \  4.2 Server Sequence Number\n       Generate the next sequence number.\n       Return.\n
    \  4.3 CMI\n       CMIs are allocated uniquely per registered cluster member\n
    \      within the context of a particular layer 3 protocol type.\n       A single
    node may register multiple times if it supports\n       multiple layer 3 protocols.\n
    \      The CMIs allocated for each such registration may or may\n       not be
    the same.\n       Generate a CMI for this protocol.\n       Return.\n   5. Modified
    JOIN/LEAVE Processing\n      This routine processes JOIN/LEAVE when a server map
    exists.\n      Make a copy of the message.\n      Change the type of the copy
    to MARS_SJOIN.\n      If the message is a MARS_LEAVE then {\n       Change the
    type of the copy to MARS_SLEAVE.\n      }\n      mar$flags.copy = 1 (copy).\n
    \     Hole punch the <min,max> group by excluding\n        from the range those
    groups which the joining\n        (leaving) node is already (still) a member of\n
    \       due to it having previously issued a single group\n        join.\n      Indicate
    the message to be sent on ServerControlVC.\n      If the message (copy) contains
    one or more <min,max> pair {\n        Send message (copy) as indicated.\n      }\n
    \     mar$flags.punched = 0 in the original message.\n      Indicate the message
    to be sent on Private VC.\n      Send message (original) as indicated.\n      Hole
    punch the <min,max> group by excluding\n        from the range those groups that
    are served by MCSs\n        or which the joining (leaving) node is already\n        (still)
    a member of due to it having previously\n        issued a single group join.\n
    \     Indicate the (original) message to be sent on ClusterControlVC.\n      If
    (number of holes punched > 0) then { /* punched holes */\n        In original
    message do {\n         mar$flags.punched = 1.\n         old punched list <- new
    punched list.\n        }\n      } /* punched holes */\n      mar$flags.copy =
    1.\n      Send message as indicated.\n      Return.\n   5.1 MultiGroup JOIN/LEAVE
    Processing\n      This routine processes JOIN/LEAVE when a multi group exists.\n
    \     If (mar$flags.layer3grp) {\n       Ignore this setting, consider it reset.\n
    \     }\n      mar$flags.copy = 1.\n      Make a copy of the message.\n      From
    the copy hole punch the <min,max> group by\n       excluding from the range those
    groups that this\n       node has already joined or left.\n      If (number of
    holes punched > 0) then {\n       mar$flags.punch = 0 in original message.\n       Indicate
    original message to be sent on Private VC.\n       Send original message as indicated.\n
    \      mar$flags.punch = 1 in copy message.\n       old group range <- new punched
    list.\n       Indicate message to be sent on ClusterControlVC.\n       Send copy
    of message as indicated.\n      } else {\n         Indicate message to be sent
    on ClusterControlVC.\n         Send original message as indicated.\n      } /*
    no holes punched */\n      Return.\n"
  title: Appendix F.  Pseudo code for MARS operation.
