- title: __initial_text__
  contents:
  - '       Endpoint Security Posture Assessment: Enterprise Use Cases

    '
- title: Abstract
  contents:
  - "Abstract\n   This memo documents a sampling of use cases for securely aggregating\n\
    \   configuration and operational data and evaluating that data to\n   determine\
    \ an organization's security posture.  From these operational\n   use cases, we\
    \ can derive common functional capabilities and\n   requirements to guide development\
    \ of vendor-neutral, interoperable\n   standards for aggregating and evaluating\
    \ data relevant to security\n   posture.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7632.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2015 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   3\n   2.  Endpoint Posture Assessment . . . . . . . . . . . . .\
    \ . . . .   4\n     2.1.  Use Cases . . . . . . . . . . . . . . . . . . . . .\
    \ . . .   5\n       2.1.1.  Define, Publish, Query, and Retrieve Security\n  \
    \             Automation Data . . . . . . . . . . . . . . . . . . .   6\n    \
    \   2.1.2.  Endpoint Identification and Assessment Planning . . .   9\n      \
    \ 2.1.3.  Endpoint Posture Attribute Value Collection . . . . .  11\n       2.1.4.\
    \  Posture Attribute Evaluation  . . . . . . . . . . . .  11\n     2.2.  Usage\
    \ Scenarios . . . . . . . . . . . . . . . . . . . . .  13\n       2.2.1.  Definition\
    \ and Publication of Automatable\n               Configuration Checklists  . .\
    \ . . . . . . . . . . . .  13\n       2.2.2.  Automated Checklist Verification\
    \  . . . . . . . . . .  14\n       2.2.3.  Detection of Posture Deviations . .\
    \ . . . . . . . . .  17\n       2.2.4.  Endpoint Information Analysis and Reporting\
    \ . . . . .  18\n       2.2.5.  Asynchronous Compliance/Vulnerability Assessment\
    \ at\n               Ice Station Zebra . . . . . . . . . . . . . . . . . .  18\n\
    \       2.2.6.  Identification and Retrieval of Guidance  . . . . . .  20\n  \
    \     2.2.7.  Guidance Change Detection . . . . . . . . . . . . . .  21\n   3.\
    \  Security Considerations . . . . . . . . . . . . . . . . . . .  22\n   4.  Informative\
    \ References  . . . . . . . . . . . . . . . . . . .  22\n   Acknowledgements \
    \ . . . . . . . . . . . . . . . . . . . . . . . .  23\n   Authors' Addresses \
    \ . . . . . . . . . . . . . . . . . . . . . . .  23\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document describes the core set of use cases for endpoint\n\
    \   posture assessment for enterprises.  It provides a discussion of\n   these\
    \ use cases and associated building-block capabilities.  The\n   described use\
    \ cases support:\n   o  securely collecting and aggregating configuration and\
    \ operational\n      data, and\n   o  evaluating that data to determine the security\
    \ posture of\n      individual endpoints.\n   Additionally, this document describes\
    \ a set of usage scenarios that\n   provide examples for using the use cases and\
    \ associated building\n   blocks to address a variety of operational functions.\n\
    \   These operational use cases and related usage scenarios cross many IT\n  \
    \ security domains.  The use cases enable the derivation of common:\n   o  concepts\
    \ that are expressed as building blocks in this document,\n   o  characteristics\
    \ to inform development of a requirements document,\n   o  information concepts\
    \ to inform development of an information model\n      document, and\n   o  functional\
    \ capabilities to inform development of an architecture\n      document.\n   Together,\
    \ these ideas will be used to guide development of vendor-\n   neutral, interoperable\
    \ standards for collecting, aggregating, and\n   evaluating data relevant to security\
    \ posture.\n   Using this standard data, tools can analyze the state of endpoints\
    \ as\n   well as user activities and behaviour, and evaluate the security\n  \
    \ posture of an organization.  Common expression of information should\n   enable\
    \ interoperability between tools (whether customized,\n   commercial, or freely\
    \ available), and the ability to automate\n   portions of security processes to\
    \ gain efficiency, react to new\n   threats in a timely manner, and free up security\
    \ personnel to work on\n   more advanced problems.\n   The goal is to enable organizations\
    \ to make informed decisions that\n   support organizational objectives, to enforce\
    \ policies for hardening\n   systems, to prevent network misuse, to quantify business\
    \ risk, and to\n   collaborate with partners to identify and mitigate threats.\n\
    \   It is expected that use cases for enterprises and for service\n   providers\
    \ will largely overlap.  When considering this overlap, there\n   are additional\
    \ complications for service providers, especially in\n   handling information\
    \ that crosses administrative domains.\n   The output of endpoint posture assessment\
    \ is expected to feed into\n   additional processes, such as policy-based enforcement\
    \ of acceptable\n   state, verification and monitoring of security controls, and\n\
    \   compliance to regulatory requirements.\n"
- title: 2.  Endpoint Posture Assessment
  contents:
  - "2.  Endpoint Posture Assessment\n   Endpoint posture assessment involves orchestrating\
    \ and performing\n   data collection and evaluating the posture of a given endpoint.\n\
    \   Typically, endpoint posture information is gathered and then\n   published\
    \ to appropriate data repositories to make collected\n   information available\
    \ for further analysis supporting organizational\n   security processes.\n   Endpoint\
    \ posture assessment typically includes:\n   o  collecting the attributes of a\
    \ given endpoint;\n   o  making the attributes available for evaluation and action;\
    \ and\n   o  verifying that the endpoint's posture is in compliance with\n   \
    \   enterprise standards and policy.\n   As part of these activities, it is often\
    \ necessary to identify and\n   acquire any supporting security automation data\
    \ that is needed to\n   drive and feed data collection and evaluation processes.\n\
    \   The following is a typical workflow scenario for assessing endpoint\n   posture:\n\
    \   1.  Some type of trigger initiates the workflow.  For example, an\n      \
    \ operator or an application might trigger the process with a\n       request,\
    \ or the endpoint might trigger the process using an\n       event-driven notification.\n\
    \   2.  An operator/application selects one or more target endpoints to\n    \
    \   be assessed.\n   3.  An operator/application selects which policies are applicable\
    \ to\n       the targets.\n   4.  For each target:\n       A.  The application\
    \ determines which (sets of) posture attributes\n           need to be collected\
    \ for evaluation.  Implementations should\n           be able to support (possibly\
    \ mixed) sets of standardized and\n           proprietary attributes.\n      \
    \ B.  The application might retrieve previously collected\n           information\
    \ from a cache or data store, such as a data store\n           populated by an\
    \ asset management system.\n       C.  The application might establish communication\
    \ with the\n           target, mutually authenticate identities and authorizations,\n\
    \           and collect posture attributes from the target.\n       D.  The application\
    \ might establish communication with one or\n           more intermediaries or\
    \ agents, which may be local or\n           external.  When establishing connections\
    \ with an intermediary\n           or agent, the application can mutually authenticate\
    \ their\n           identities and determine authorizations, and collect posture\n\
    \           attributes about the target from the intermediaries or\n         \
    \  agents.\n       E.  The application communicates target identity and (sets\
    \ of)\n           collected attributes to an evaluator, which is possibly an\n\
    \           external process or external system.\n       F.  The evaluator compares\
    \ the collected posture attributes with\n           expected values as expressed\
    \ in policies.\n       G.  The evaluator reports the evaluation result for the\
    \ requested\n           assessment, in a standardized or proprietary format, such\
    \ as\n           a report, a log entry, a database entry, or a notification.\n"
- title: 2.1.  Use Cases
  contents:
  - "2.1.  Use Cases\n   The following subsections detail specific use cases for assessment\n\
    \   planning, data collection, analysis, and related operations\n   pertaining\
    \ to the publication and use of supporting data.  Each use\n   case is defined\
    \ by a short summary containing a simple problem\n   statement, followed by a\
    \ discussion of related concepts, and a\n   listing of associated building blocks\
    \ that represent the capabilities\n   needed to support the use case.  These use\
    \ cases and building blocks\n   identify separate units of functionality that\
    \ may be supported by\n   different components of an architectural model.\n"
- title: 2.1.1.  Define, Publish, Query, and Retrieve Security Automation Data
  contents:
  - "2.1.1.  Define, Publish, Query, and Retrieve Security Automation Data\n   This\
    \ use case describes the need for security automation data to be\n   defined and\
    \ published to one or more data stores, as well as queried\n   and retrieved from\
    \ these data stores for the explicit use of posture\n   collection and evaluation.\n\
    \   Security automation data is a general concept that refers to any data\n  \
    \ expression that may be generated and/or used as part of the process\n   of collecting\
    \ and evaluating endpoint posture.  Different types of\n   security automation\
    \ data will generally fall into one of three\n   categories:\n   Guidance:  Instructions\
    \ and related metadata that guide the attribute\n         collection and evaluation\
    \ processes.  The purpose of this data\n         is to allow implementations to\
    \ be data-driven, thus enabling\n         their behavior to be customized without\
    \ requiring changes to\n         deployed software.\n         This type of data\
    \ tends to change in units of months and days.\n         In cases where assessments\
    \ are made more dynamic, it may be\n         necessary to handle changes in the\
    \ scope of hours or minutes.\n         This data will typically be provided by\
    \ large organizations,\n         product vendors, and some third parties.  Thus,\
    \ it will tend to\n         be shared across large enterprises and customer communities.\n\
    \         In some cases, access may be controlled to specific\n         authenticated\
    \ users.  In other cases, the data may be provided\n         broadly with little\
    \ to no access control.\n         This includes:\n         *  Listings of attribute\
    \ identifiers for which values may be\n            collected and evaluated.\n\
    \         *  Lists of attributes that are to be collected along with\n       \
    \     metadata that includes: when to collect a set of attributes\n          \
    \  based on a defined interval or event, the duration of\n            collection,\
    \ and how to go about collecting a set of\n            attributes.\n         *\
    \  Guidance that specifies how old collected data can be when\n            used\
    \ for evaluation.\n         *  Policies that define how to target and perform\
    \ the\n            evaluation of a set of attributes for different kinds or\n\
    \            groups of endpoints and the assets they are composed of.  In\n  \
    \          some cases, it may be desirable to maintain hierarchies of\n      \
    \      policies as well.\n         *  References to human-oriented data that provide\
    \ technical,\n            organizational, and/or policy context.  This might include\n\
    \            references to: best practices documents, legal guidance and\n   \
    \         legislation, and instructional materials related to the\n          \
    \  automation data in question.\n   Attribute Data:  Data collected through automated\
    \ and manual\n         mechanisms describing organizational and posture details\n\
    \         pertaining to specific endpoints and the assets that they are\n    \
    \     composed of (e.g., hardware, software, accounts).  The purpose\n       \
    \  of this type of data is to characterize an endpoint (e.g.,\n         endpoint\
    \ type, organizationally expected function/role) and to\n         provide actual\
    \ and expected state data pertaining to one or\n         more endpoints.  This\
    \ data is used to determine what posture\n         attributes to collect from\
    \ which endpoints and to feed one or\n         more evaluations.\n         This\
    \ type of data tends to change in units of days, minutes,\n         and seconds,\
    \ with posture attribute values typically changing\n         more frequently than\
    \ endpoint characterizations.  This data\n         tends to be organizationally\
    \ and endpoint specific, with\n         specific operational groups of endpoints\
    \ tending to exhibit\n         similar attribute profiles.  Generally, this data\
    \ will not be\n         shared outside an organizational boundary and will require\n\
    \         authentication with specific access controls.\n         This includes:\n\
    \         *  Endpoint characterization data that describes the endpoint\n    \
    \        type, organizationally expected function/role, etc.\n         *  Collected\
    \ endpoint posture attribute values and related\n            context including:\
    \ time of collection, tools used for\n            collection, etc.\n         *\
    \  Organizationally defined expected posture attribute values\n            targeted\
    \ to specific evaluation guidance and endpoint\n            characteristics. \
    \ This allows a common set of guidance to be\n            parameterized for use\
    \ with different groups of endpoints.\n   Processing Artifacts:  Data that is\
    \ generated by, and is specific to,\n         an individual assessment process.\
    \  This data may be used as\n         part of the interactions between architectural\
    \ components to\n         drive and coordinate collection and evaluation activities.\
    \  Its\n         lifespan will be bounded by the lifespan of the assessment. \
    \ It\n         may also be exchanged and stored to provide historic context\n\
    \         around an assessment activity so that individual assessments\n     \
    \    can be grouped, evaluated, and reported in an enterprise\n         context.\n\
    \         This includes:\n         *  The identified set of endpoints for which\
    \ an assessment\n            should be performed.\n         *  The identified\
    \ set of posture attributes that need to be\n            collected from specific\
    \ endpoints to perform an evaluation.\n         *  The resulting data generated\
    \ by an evaluation process\n            including the context of what was assessed,\
    \ what it was\n            assessed against, what collected data was used, when\
    \ it was\n            collected, and when the evaluation was performed.\n   The\
    \ information model for security automation data must support a\n   variety of\
    \ different data types as described above, along with the\n   associated metadata\
    \ that is needed to support publication, query, and\n   retrieval operations.\
    \  It is expected that multiple data models will\n   be used to express specific\
    \ data types requiring specialized or\n   extensible security automation data\
    \ repositories.  The different\n   temporal characteristics, access patterns,\
    \ and access control\n   dimensions of each data type may also require different\
    \ protocols and\n   data models to be supported furthering the potential requirement\
    \ for\n   specialized data repositories.  See [RFC3444] for a description and\n\
    \   discussion of distinctions between an information and data model.  It\n  \
    \ is likely that additional kinds of data will be identified through\n   the process\
    \ of defining requirements and an architectural model.\n   Implementations supporting\
    \ this building block will need to be\n   extensible to accommodate the addition\
    \ of new types of data, whether\n   proprietary or (preferably) using a standard\
    \ format.\n   The building blocks of this use case are:\n   Data Definition: \
    \ Security automation data will guide and inform\n         collection and evaluation\
    \ processes.  This data may be designed\n         by a variety of roles -- application\
    \ implementers may build\n         security automation data into their applications;\n\
    \         administrators may define guidance based on organizational\n       \
    \  policies; operators may define guidance and attribute data as\n         needed\
    \ for evaluation at runtime; and so on.  Data producers\n         may choose to\
    \ reuse data from existing stores of security\n         automation data and/or\
    \ may create new data.  Data producers may\n         develop data based on available\
    \ standardized or proprietary\n         data models, such as those used for network\
    \ management and/or\n         host management.\n   Data Publication:  The capability\
    \ to enable data producers to publish\n         data to a security automation\
    \ data store for further use.\n         Published data may be made publicly available\
    \ or access may be\n         based on an authorization decision using authenticated\n\
    \         credentials.  As a result, the visibility of specific security\n   \
    \      automation data to an operator or application may be public,\n        \
    \ enterprise-scoped, private, or controlled within any other\n         scope.\n\
    \   Data Query:  An operator or application should be able to query a\n      \
    \   security automation data store using a set of specified\n         criteria.\
    \  The result of the query will be a listing matching\n         the query.  The\
    \ query result listing may contain publication\n         metadata (e.g., create\
    \ date, modified date, publisher, etc.)\n         and/or the full data, a summary,\
    \ snippet, or the location to\n         retrieve the data.\n   Data Retrieval:\
    \  A user, operator, or application acquires one or\n         more specific security\
    \ automation data entries.  The location\n         of the data may be known a\
    \ priori, or may be determined based\n         on decisions made using information\
    \ from a previous query.\n   Data Change Detection:  An operator or application\
    \ needs to know when\n         security automation data they are interested in\
    \ has been\n         published to, updated in, or deleted from a security automation\n\
    \         data store that they have been authorized to access.\n   These building\
    \ blocks are used to enable acquisition of various\n   instances of security automation\
    \ data based on specific data models\n   that are used to drive assessment planning\
    \ (see Section 2.1.2),\n   posture attribute value collection (see Section 2.1.3),\
    \ and posture\n   evaluation (see Section 2.1.4).\n"
- title: 2.1.2.  Endpoint Identification and Assessment Planning
  contents:
  - "2.1.2.  Endpoint Identification and Assessment Planning\n   This use case describes\
    \ the process of discovering endpoints,\n   understanding their composition, identifying\
    \ the desired state to\n   assess against, and calculating what posture attributes\
    \ to collect to\n   enable evaluation.  This process may be a set of manual, automated,\n\
    \   or hybrid steps that are performed for each assessment.\n   The building blocks\
    \ of this use case are:\n   Endpoint Discovery:  To determine the current or historic\
    \ presence of\n         endpoints in the environment that are available for posture\n\
    \         assessment.  Endpoints are identified in support of discovery\n    \
    \     by using information previously obtained or using other\n         collection\
    \ mechanisms to gather identification and\n         characterization data.  Previously\
    \ obtained data may originate\n         from sources such as network authentication\
    \ exchanges.\n   Endpoint Characterization:  The act of acquiring, through automated\n\
    \         collection or manual input, and organizing attributes\n         associated\
    \ with an endpoint (e.g., type, organizationally\n         expected function/role,\
    \ hardware/software versions).\n   Endpoint Target Identification:  Determine\
    \ the candidate endpoint\n         target(s) against which to perform the assessment.\
    \  Depending\n         on the assessment trigger, a single endpoint or multiple\n\
    \         endpoints may be targeted based on characterized endpoint\n        \
    \ attributes.  Guidance describing the assessment to be performed\n         may\
    \ contain instructions or references used to determine the\n         applicable\
    \ assessment targets.  In this case, the Data Query\n         and/or Data Retrieval\
    \ building blocks (see Section 2.1.1) may\n         be used to acquire this data.\n\
    \   Endpoint Component Inventory:  To determine what applicable desired\n    \
    \     states should be assessed, it is first necessary to acquire the\n      \
    \   inventory of software, hardware, and accounts associated with\n         the\
    \ targeted endpoint(s).  If the assessment of the endpoint is\n         not dependent\
    \ on the these details, then this capability is not\n         required for use\
    \ in performing the assessment.  This process\n         can be treated as a collection\
    \ use case for specific posture\n         attributes.  In this case, the building\
    \ blocks for\n         Endpoint Posture Attribute Value Collection (see Section\
    \ 2.1.3)\n         can be used.\n   Posture Attribute Identification:  Once the\
    \ endpoint targets and\n         their associated asset inventory is known, it\
    \ is then necessary\n         to calculate what posture attributes are required\
    \ to be\n         collected to perform the desired evaluation.  When available,\n\
    \         existing posture data is queried for suitability using the Data\n  \
    \       Query building block (see Section 2.1.1).  Such posture data is\n    \
    \     suitable if it is complete and current enough for use in the\n         evaluation.\
    \  Any unsuitable posture data is identified for\n         collection.\n     \
    \    If this is driven by guidance, then the Data Query and/or Data\n        \
    \ Retrieval building blocks (see Section 2.1.1) may be used to\n         acquire\
    \ this data.\n   At this point, the set of posture attribute values to use for\n\
    \   evaluation are known, and they can be collected if necessary (see\n   Section\
    \ 2.1.3).\n"
- title: 2.1.3.  Endpoint Posture Attribute Value Collection
  contents:
  - "2.1.3.  Endpoint Posture Attribute Value Collection\n   This use case describes\
    \ the process of collecting a set of posture\n   attribute values related to one\
    \ or more endpoints.  This use case can\n   be initiated by a variety of triggers\
    \ including:\n   1.  a posture change or significant event on the endpoint.\n\
    \   2.  a network event (e.g., endpoint connects to a network/VPN,\n       specific\
    \ netflow [RFC3954] is detected).\n   3.  a scheduled or ad hoc collection task.\n\
    \   The building blocks of this use case are:\n   Collection Guidance Acquisition:\
    \  If guidance is required to drive\n         the collection of posture attributes\
    \ values, this capability is\n         used to acquire this data from one or more\
    \ security automation\n         data stores.  Depending on the trigger, the specific\
    \ guidance\n         to acquire might be known.  If not, it may be necessary to\n\
    \         determine the guidance to use based on the component inventory\n   \
    \      or other assessment criteria.  The Data Query and/or Data\n         Retrieval\
    \ building blocks (see Section 2.1.1) may be used to\n         acquire this guidance.\n\
    \   Posture Attribute Value Collection:  The accumulation of posture\n       \
    \  attribute values.  This may be based on collection guidance\n         that\
    \ is associated with the posture attributes.\n   Once the posture attribute values\
    \ are collected, they may be\n   persisted for later use or they may be immediately\
    \ used for posture\n   evaluation.\n"
- title: 2.1.4.  Posture Attribute Evaluation
  contents:
  - "2.1.4.  Posture Attribute Evaluation\n   This use case represents the action\
    \ of analyzing collected posture\n   attribute values as part of an assessment.\
    \  The primary focus of this\n   use case is to support evaluation of actual endpoint\
    \ state against\n   the expected state selected for the assessment.\n   This use\
    \ case can be initiated by a variety of triggers including:\n   1.  a posture\
    \ change or significant event on the endpoint.\n   2.  a network event (e.g.,\
    \ endpoint connects to a network/VPN,\n       specific netflow [RFC3954] is detected).\n\
    \   3.  a scheduled or ad hoc evaluation task.\n   The building blocks of this\
    \ use case are:\n   Collected Posture Change Detection:  An operator or application\
    \ has a\n         mechanism to detect the availability of new posture attribute\n\
    \         values or changes to existing ones.  The timeliness of\n         detection\
    \ may vary from immediate to on-demand.  Having the\n         ability to filter\
    \ what changes are detected will allow the\n         operator to focus on the\
    \ changes that are relevant to their use\n         and will enable evaluation\
    \ to occur dynamically based on\n         detected changes.\n   Posture Attribute\
    \ Value Query:  If previously collected posture\n         attribute values are\
    \ needed, the appropriate data stores are\n         queried to retrieve them using\
    \ the Data Query building block\n         (see Section 2.1.1).  If all posture\
    \ attribute values are\n         provided directly for evaluation, then this capability\
    \ may not\n         be needed.\n   Evaluation Guidance Acquisition:  If guidance\
    \ is required to drive\n         the evaluation of posture attributes values,\
    \ this capability is\n         used to acquire this data from one or more security\
    \ automation\n         data stores.  Depending on the trigger, the specific guidance\n\
    \         to acquire might be known.  If not, it may be necessary to\n       \
    \  determine the guidance to use based on the component inventory\n         or\
    \ other assessment criteria.  The Data Query and/or Data\n         Retrieval building\
    \ blocks (see Section 2.1.1) may be used to\n         acquire this guidance.\n\
    \   Posture Attribute Evaluation:  The comparison of posture attribute\n     \
    \    values against their expected values as expressed in the\n         specified\
    \ guidance.  The result of this comparison is output as\n         a set of posture\
    \ evaluation results.  Such results include\n         metadata required to provide\
    \ a level of assurance with respect\n         to the posture attribute data and,\
    \ therefore, evaluation\n         results.  Examples of such metadata include\
    \ provenance and or\n         availability data.\n   While the primary focus of\
    \ this use case is around enabling the\n   comparison of expected vs. actual state,\
    \ the same building blocks can\n   support other analysis techniques that are\
    \ applied to collected\n   posture attribute data (e.g., trending, historic analysis).\n\
    \   Completion of this process represents a complete assessment cycle as\n   defined\
    \ in Section 2.\n"
- title: 2.2.  Usage Scenarios
  contents:
  - "2.2.  Usage Scenarios\n   In this section, we describe a number of usage scenarios\
    \ that utilize\n   aspects of endpoint posture assessment.  These are examples\
    \ of common\n   problems that can be solved with the building blocks defined above.\n"
- title: 2.2.1.  Definition and Publication of Automatable Configuration
  contents:
  - "2.2.1.  Definition and Publication of Automatable Configuration\n        Checklists\n\
    \   A vendor manufactures a number of specialized endpoint devices.  They\n  \
    \ also develop and maintain an operating system for these devices that\n   enables\
    \ end-user organizations to configure a number of security and\n   operational\
    \ settings.  As part of their customer support activities,\n   they publish a\
    \ number of secure configuration guides that provide\n   minimum security guidelines\
    \ for configuring their devices.\n   Each guide they produce applies to a specific\
    \ model of device and\n   version of the operating system and provides a number\
    \ of specialized\n   configurations depending on the device's intended function\
    \ and what\n   add-on hardware modules and software licenses are installed on\
    \ the\n   device.  To enable their customers to evaluate the security posture\n\
    \   of their devices to ensure that all appropriate minimal security\n   settings\
    \ are enabled, they publish automatable configuration\n   checklists using a popular\
    \ data format that defines what settings to\n   collect using a network management\
    \ protocol and appropriate values\n   for each setting.  They publish these checklists\
    \ to a public security\n   automation data store that customers can query to retrieve\
    \ applicable\n   checklist(s) for their deployed specialized endpoint devices.\n\
    \   Automatable configuration checklists could also come from sources\n   other\
    \ than a device vendor, such as industry groups or regulatory\n   authorities,\
    \ or enterprises could develop their own checklists.\n   This usage scenario employs\
    \ the following building blocks defined in\n   Section 2.1.1 above:\n   Data Definition:\
    \  To allow guidance to be defined using standardized\n         or proprietary\
    \ data models that will drive collection and\n         evaluation.\n   Data Publication:\
    \  Providing a mechanism to publish created guidance\n         to a security automation\
    \ data store.\n   Data Query:  To locate and select existing guidance that may\
    \ be\n         reused.\n   Data Retrieval  To retrieve specific guidance from\
    \ a security\n         automation data store for editing.\n   While each building\
    \ block can be used in a manual fashion by a human\n   operator, it is also likely\
    \ that these capabilities will be\n   implemented together in some form of a guidance\
    \ editor or generator\n   application.\n"
- title: 2.2.2.  Automated Checklist Verification
  contents:
  - "2.2.2.  Automated Checklist Verification\n   A financial services company operates\
    \ a heterogeneous IT environment.\n   In support of their risk management program,\
    \ they utilize vendor-\n   provided automatable security configuration checklists\
    \ for each\n   operating system and application used within their IT environment.\n\
    \   Multiple checklists are used from different vendors to ensure\n   adequate\
    \ coverage of all IT assets.\n   To identify what checklists are needed, they\
    \ use automation to gather\n   an inventory of the software versions utilized\
    \ by all IT assets in\n   the enterprise.  This data gathering will involve querying\
    \ existing\n   data stores of previously collected endpoint software inventory\n\
    \   posture data and actively collecting data from reachable endpoints as\n  \
    \ needed, utilizing network and systems management protocols.\n   Previously collected\
    \ data may be provided by periodic data\n   collection, network connection-driven\
    \ data collection, or ongoing\n   event-driven monitoring of endpoint posture\
    \ changes.\n   Appropriate checklists are queried, located, and downloaded from\
    \ the\n   relevant guidance data stores.  The specific data stores queried and\n\
    \   the specifics of each query may be driven by data including:\n   o  collected\
    \ hardware and software inventory data, and\n   o  associated asset characterization\
    \ data that may indicate the\n      organizationally defined functions of each\
    \ endpoint.\n   Checklists may be sourced from guidance data stores maintained\
    \ by an\n   application or OS vendor, an industry group, a regulatory authority,\n\
    \   or directly by the enterprise.\n   The retrieved guidance is cached locally\
    \ to reduce the need to\n   retrieve the data multiple times.\n   Driven by the\
    \ setting data provided in the checklist, a combination\n   of existing configuration\
    \ data stores and data collection methods are\n   used to gather the appropriate\
    \ posture attributes from (or pertaining\n   to) each endpoint.  Specific posture\
    \ attribute values are gathered\n   based on the defined enterprise function and\
    \ software inventory of\n   each endpoint.  The collection mechanisms used to\
    \ collect software\n   inventory posture will be used again for this purpose.\
    \  Once the data\n   is gathered, the actual state is evaluated against the expected\
    \ state\n   criteria defined in each applicable checklist.\n   A checklist can\
    \ be assessed as a whole, or a specific subset of the\n   checklist can be assessed\
    \ resulting in partial data collection and\n   evaluation.\n   The results of\
    \ checklist evaluation are provided to appropriate\n   operators and applications\
    \ to drive additional business logic.\n   Specific applications for checklist\
    \ evaluation results are out of\n   scope for current SACM (Security Automation\
    \ and Continuous\n   Monitoring) efforts.  Irrespective of specific applications,\
    \ the\n   availability, timeliness, and liveness of results are often of\n   general\
    \ concern.  Network latency and available bandwidth often\n   create operational\
    \ constraints that require trade-offs between these\n   concerns and need to be\
    \ considered.\n   Uses of checklists and associated evaluation results may include,\
    \ but\n   are not limited to:\n   o  Detecting endpoint posture deviations as\
    \ part of a change\n      management program to identify:\n      *  missing required\
    \ patches,\n      *  unauthorized changes to hardware and software inventory,\
    \ and\n      *  unauthorized changes to configuration items.\n   o  Determining\
    \ compliance with organizational policies governing\n      endpoint posture.\n\
    \   o  Informing configuration management, patch management, and\n      vulnerability\
    \ mitigation and remediation decisions.\n   o  Searching for current and historic\
    \ indicators of compromise.\n   o  Detecting current and historic infection by\
    \ malware and\n      determining the scope of infection within an enterprise.\n\
    \   o  Detecting performance, attack, and vulnerable conditions that\n      warrant\
    \ additional network diagnostics, monitoring, and analysis.\n   o  Informing network\
    \ access control decision-making for wired,\n      wireless, or VPN connections.\n\
    \   This usage scenario employs the following building blocks defined in\n   Section\
    \ 2.1.1 above:\n   Endpoint Discovery:  The purpose of discovery is to determine\
    \ the\n         type of endpoint to be posture assessed.\n   Endpoint Target Identification:\
    \  To identify what potential endpoint\n         targets the checklist should\
    \ apply to based on organizational\n         policies.\n   Endpoint Component\
    \ Inventory:  Collecting and consuming the software\n         and hardware inventory\
    \ for the target endpoints.\n   Posture Attribute Identification:  To determine\
    \ what data needs to be\n         collected to support evaluation, the checklist\
    \ is evaluated\n         against the component inventory and other endpoint metadata\
    \ to\n         determine the set of posture attribute values that are needed.\n\
    \   Collection Guidance Acquisition:  Based on the identified posture\n      \
    \   attributes, the application will query appropriate security\n         automation\
    \ data stores to find the \"applicable\" collection\n         guidance for each\
    \ endpoint in question.\n   Posture Attribute Value Collection:  For each endpoint,\
    \ the values\n         for the required posture attributes are collected.\n  \
    \ Posture Attribute Value Query:  If previously collected posture\n         attribute\
    \ values are used, they are queried from the\n         appropriate data stores\
    \ for the target endpoint(s).\n   Evaluation Guidance Acquisition:  Any guidance\
    \ that is needed to\n         support evaluation is queried and retrieved.\n \
    \  Posture Attribute Evaluation:  The resulting posture attribute values\n   \
    \      from previous collection processes are evaluated using the\n         evaluation\
    \ guidance to provide a set of posture results.\n"
- title: 2.2.3.  Detection of Posture Deviations
  contents:
  - "2.2.3.  Detection of Posture Deviations\n   Example Corporation has established\
    \ secure configuration baselines\n   for each different type of endpoint within\
    \ their enterprise\n   including: network infrastructure, mobile, client, and\
    \ server\n   computing platforms.  These baselines define an approved list of\n\
    \   hardware, software (i.e., operating system, applications, and\n   patches),\
    \ and associated required configurations.  When an endpoint\n   connects to the\
    \ network, the appropriate baseline configuration is\n   communicated to the endpoint\
    \ based on its location in the network,\n   the expected function of the device,\
    \ and other asset management data.\n   It is checked for compliance with the baseline,\
    \ and any deviations\n   are indicated to the device's operators.  Once the baseline\
    \ has been\n   established, the endpoint is monitored for any change events\n\
    \   pertaining to the baseline on an ongoing basis.  When a change occurs\n  \
    \ to posture defined in the baseline, updated posture information is\n   exchanged,\
    \ allowing operators to be notified and/or automated action\n   to be taken.\n\
    \   Like the Automated Checklist Verification usage scenario (see\n   Section\
    \ 2.2.2), this usage scenario supports assessment based on\n   automatable checklists.\
    \  It differs from that scenario by monitoring\n   for specific endpoint posture\
    \ changes on an ongoing basis.  When the\n   endpoint detects a posture change,\
    \ an alert is generated identifying\n   the specific changes in posture, thus\
    \ allowing assessment of the\n   delta to be performed instead of a full assessment\
    \ as in the previous\n   case.  This usage scenario employs the same building\
    \ blocks as\n   Automated Checklist Verification (see section 2.2.2).  It differs\n\
    \   slightly in how it uses the following building blocks:\n   Endpoint Component\
    \ Inventory:  Additionally, changes to the hardware\n         and software inventory\
    \ are monitored, with changes causing\n         alerts to be issued.\n   Posture\
    \ Attribute Value Collection:  After the initial assessment,\n         posture\
    \ attributes are monitored for changes.  If any of the\n         selected posture\
    \ attribute values change, an alert is issued.\n   Posture Attribute Value Query:\
    \  The previous state of posture\n         attributes are tracked, allowing changes\
    \ to be detected.\n   Posture Attribute Evaluation:  After the initial assessment,\
    \ a\n         partial evaluation is performed based on changes to specific\n \
    \        posture attributes.\n   This usage scenario highlights the need to query\
    \ a data store to\n   prepare a compliance report for a specific endpoint and\
    \ also the need\n   for a change in endpoint state to trigger Collection and Evaluation.\n"
- title: 2.2.4.  Endpoint Information Analysis and Reporting
  contents:
  - "2.2.4.  Endpoint Information Analysis and Reporting\n   Freed from the drudgery\
    \ of manual endpoint compliance monitoring, one\n   of the security administrators\
    \ at Example Corporation notices (not\n   using SACM standards) that five endpoints\
    \ have been uploading lots of\n   data to a suspicious server on the Internet.\
    \  The administrator\n   queries data stores for specific endpoint posture to\
    \ see what\n   software is installed on those endpoints and finds that they all\
    \ have\n   a particular program installed.  She then queries the appropriate\n\
    \   data stores to see which other endpoints have that program installed.\n  \
    \ All these endpoints are monitored carefully (not using SACM\n   standards),\
    \ which allows the administrator to detect that the other\n   endpoints are also\
    \ infected.\n   This is just one example of the useful analysis that a skilled\n\
    \   analyst can do using data stores of endpoint posture.\n   This usage scenario\
    \ employs the following building blocks defined in\n   Section 2.1.1 above:\n\
    \   Posture Attribute Value Query:  Previously collected posture\n         attribute\
    \ values for the target endpoint(s) are queried from\n         the appropriate\
    \ data stores using a standardized method.\n   This usage scenario highlights\
    \ the need to query a repository for\n   attributes to see which attributes certain\
    \ endpoints have in common.\n"
- title: 2.2.5.  Asynchronous Compliance/Vulnerability Assessment at Ice Station
  contents:
  - "2.2.5.  Asynchronous Compliance/Vulnerability Assessment at Ice Station\n   \
    \     Zebra\n   A university team receives a grant to do research at a government\n\
    \   facility in the Arctic.  The only network communications will be via\n   an\
    \ intermittent, low-speed, high-latency, high-cost satellite link.\n   During\
    \ their extended expedition, they will need to show continued\n   compliance with\
    \ the security policies of the university, the\n   government, and the provider\
    \ of the satellite network, as well as\n   keep current on vulnerability testing.\
    \  Interactive assessments are\n   therefore not reliable, and since the researchers\
    \ have very limited\n   funding, they need to minimize how much money they spend\
    \ on network\n   data.\n   Prior to departure, they register all equipment with\
    \ an asset\n   management system owned by the university, which will also initiate\n\
    \   and track assessments.\n   On a periodic basis -- either after a maximum time\
    \ delta or when the\n   security automation data store has received a threshold\
    \ level of new\n   vulnerability definitions -- the university uses the information\
    \ in\n   the asset management system to put together a collection request for\n\
    \   all of the deployed assets that encompasses the minimal set of\n   artifacts\
    \ necessary to evaluate all three security policies as well\n   as vulnerability\
    \ testing.\n   In the case of new critical vulnerabilities, this collection request\n\
    \   consists only of the artifacts necessary for those vulnerabilities,\n   and\
    \ collection is only initiated for those assets that could\n   potentially have\
    \ a new vulnerability.\n   (Optional) Asset artifacts are cached in a local configuration\n\
    \   management database (CMDB).  When new vulnerabilities are reported to\n  \
    \ the security automation data store, a request to the live asset is\n   only\
    \ done if the artifacts in the CMDB are incomplete and/or not\n   current enough.\n\
    \   The collection request is queued for the next window of connectivity.\n  \
    \ The deployed assets eventually receive the request, fulfill it, and\n   queue\
    \ the results for the next return opportunity.\n   The collected artifacts eventually\
    \ make it back to the university\n   where the level of compliance and vulnerability\
    \ exposed is calculated\n   and asset characteristics are compared to what is\
    \ in the asset\n   management system for accuracy and completeness.\n   Like the\
    \ Automated Checklist Verification usage scenario (see section\n   2.2.2), this\
    \ usage scenario supports assessment based on checklists.\n   It differs from\
    \ that scenario in how guidance, collected posture\n   attribute values, and evaluation\
    \ results are exchanged due to\n   bandwidth limitations and availability.  This\
    \ usage scenario employs\n   the same building blocks as Automated Checklist Verification\
    \ (see\n   section 2.2.2).  It differs slightly in how it uses the following\n\
    \   building blocks:\n   Endpoint Component Inventory:  It is likely that the\
    \ component\n         inventory will not change.  If it does, this information\
    \ will\n         need to be batched and transmitted during the next\n        \
    \ communication window.\n   Collection Guidance Acquisition:  Due to intermittent\
    \ communication\n         windows and bandwidth constraints, changes to collection\n\
    \         guidance will need to batched and transmitted during the next\n    \
    \     communication window.  Guidance will need to be cached locally\n       \
    \  to avoid the need for remote communications.\n   Posture Attribute Value Collection:\
    \  The specific posture attribute\n         values to be collected are identified\
    \ remotely and batched for\n         collection during the next communication\
    \ window.  If a delay is\n         introduced for collection to complete, results\
    \ will need to be\n         batched and transmitted.\n   Posture Attribute Value\
    \ Query:  Previously collected posture\n         attribute values will be stored\
    \ in a remote data store for use\n         at the university.\n   Evaluation Guidance\
    \ Acquisition:  Due to intermittent communication\n         windows and bandwidth\
    \ constraints, changes to evaluation\n         guidance will need to batched and\
    \ transmitted during the next\n         communication window.  Guidance will need\
    \ to be cached locally\n         to avoid the need for remote communications.\n\
    \   Posture Attribute Evaluation:  Due to the caching of posture\n         attribute\
    \ values and evaluation guidance, evaluation may be\n         performed at both\
    \ the university campus as well as the\n         satellite site.\n   This usage\
    \ scenario highlights the need to support low-bandwidth,\n   intermittent, or\
    \ high-latency links.\n"
- title: 2.2.6.  Identification and Retrieval of Guidance
  contents:
  - "2.2.6.  Identification and Retrieval of Guidance\n   In preparation for performing\
    \ an assessment, an operator or\n   application will need to identify one or more\
    \ security automation\n   data stores that contain the guidance entries necessary\
    \ to perform\n   data collection and evaluation tasks.  The location of a given\n\
    \   guidance entry will either be known a priori or known security\n   automation\
    \ data stores will need to be queried to retrieve applicable\n   guidance.\n \
    \  To query guidance it will be necessary to define a set of search\n   criteria.\
    \  This criteria will often utilize a logical combination of\n   publication metadata\
    \ (e.g., publishing identity, create time,\n   modification time) and criteria\
    \ elements specific to the guidance\n   data.  Once the criteria are defined,\
    \ one or more security automation\n   data stores will need to be queried, thus\
    \ generating a result set.\n   Depending on how the results are used, it may be\
    \ desirable to return\n   the matching guidance directly, a snippet of the guidance\
    \ matching\n   the query, or a resolvable location to retrieve the data at a later\n\
    \   time.  The guidance matching the query will be restricted based on\n   the\
    \ authorized level of access allowed to the requester.\n   If the location of\
    \ guidance is identified in the query result set,\n   the guidance will be retrieved\
    \ when needed using one or more data\n   retrieval requests.  A variation on this\
    \ approach would be to\n   maintain a local cache of previously retrieved data.\
    \  In this case,\n   only guidance that is determined to be stale by some measure\
    \ will be\n   retrieved from the remote data store.\n   Alternately, guidance\
    \ can be discovered by iterating over data\n   published with a given context\
    \ within a security automation data\n   store.  Specific guidance can be selected\
    \ and retrieved as needed.\n   This usage scenario employs the following building\
    \ blocks defined in\n   Section 2.1.1 above:\n   Data Query:  Enables an operator\
    \ or application to query one or more\n         security automation data stores\
    \ for guidance using a set of\n         specified criteria.\n   Data Retrieval:\
    \  If data locations are returned in the query result\n         set, then specific\
    \ guidance entries can be retrieved and\n         possibly cached locally.\n"
- title: 2.2.7.  Guidance Change Detection
  contents:
  - "2.2.7.  Guidance Change Detection\n   An operator or application may need to\
    \ identify new, updated, or\n   deleted guidance in a security automation data\
    \ store for which they\n   have been authorized to access.  This may be achieved\
    \ by querying or\n   iterating over guidance in a security automation data store,\
    \ or\n   through a notification mechanism that generates alerts when changes\n\
    \   are made to a security automation data store.\n   Once guidance changes have\
    \ been determined, data collection and\n   evaluation activities may be triggered.\n\
    \   This usage scenario employs the following building blocks defined in\n   Section\
    \ 2.1.1 above:\n   Data Change Detection:  Allows an operator or application to\
    \ identify\n         guidance changes in a security automation data store for\
    \ which\n         they have been authorized to access.\n   Data Retrieval:  If\
    \ data locations are provided by the change\n         detection mechanism, then\
    \ specific guidance entries can be\n         retrieved and possibly cached locally.\n"
- title: 3.  Security Considerations
  contents:
  - "3.  Security Considerations\n   This memo documents, for informational purposes,\
    \ use cases for\n   security automation.  Specific security and privacy considerations\n\
    \   will be provided in related documents (e.g., requirements,\n   architecture,\
    \ information model, data model, protocol) as appropriate\n   to the function\
    \ described in each related document.\n   One consideration for security automation\
    \ is that a malicious actor\n   could use the security automation infrastructure\
    \ and related\n   collected data to gain access to an item of interest.  This\
    \ may\n   include personal data, private keys, software and configuration state\n\
    \   that can be used to inform an attack against the network and\n   endpoints,\
    \ and other sensitive information.  It is important that\n   security and privacy\
    \ considerations in the related documents indicate\n   methods to both identify\
    \ and prevent such activity.\n   For consideration are means for protecting the\
    \ communications as well\n   as the systems that store the information.  For communications\n\
    \   between the varying SACM components, there should be considerations\n   for\
    \ protecting the confidentiality, data integrity, and peer entity\n   authentication.\
    \  For exchanged information, there should be a means\n   to authenticate the\
    \ origin of the information.  This is important\n   where tracking the provenance\
    \ of data is needed.  Also, for any\n   systems that store information that could\
    \ be used for unauthorized or\n   malicious purposes, methods to identify and\
    \ protect against\n   unauthorized usage, inappropriate usage, and denial of service\
    \ need\n   to be considered.\n"
- title: 4.  Informative References
  contents:
  - "4.  Informative References\n   [RFC3444]  Pras, A. and J. Schoenwaelder, \"On\
    \ the Difference between\n              Information Models and Data Models\",\
    \ RFC 3444,\n              DOI 10.17487/RFC3444, January 2003,\n             \
    \ <http://www.rfc-editor.org/info/rfc3444>.\n   [RFC3954]  Claise, B., Ed., \"\
    Cisco Systems NetFlow Services Export\n              Version 9\", RFC 3954, DOI\
    \ 10.17487/RFC3954, October 2004,\n              <http://www.rfc-editor.org/info/rfc3954>.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   Adam Montville edited early versions of this document.\n\
    \   Kathleen Moriarty and Stephen Hanna contributed text describing the\n   scope\
    \ of the document.\n   Gunnar Engelbach, Steve Hanna, Chris Inacio, Kent Landfield,\
    \ Lisa\n   Lorenzin, Adam Montville, Kathleen Moriarty, Nancy Cam-Winget, and\n\
    \   Aron Woland provided text about the use cases for various revisions\n   of\
    \ this document.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   David Waltermire\n   National Institute of Standards and\
    \ Technology\n   100 Bureau Drive\n   Gaithersburg, Maryland  20877\n   United\
    \ States\n   Email: david.waltermire@nist.gov\n   David Harrington\n   Effective\
    \ Software\n   16 Bayview Drive\n   Westerly, Rhode Island  02891\n   United States\n\
    \   Email: ietfdbh@gmail.com\n"
