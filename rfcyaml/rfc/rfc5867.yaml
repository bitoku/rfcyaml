- title: __initial_text__
  contents:
  - "                Building Automation Routing Requirements\n                  \
    \  in Low-Power and Lossy Networks\n"
- title: Abstract
  contents:
  - "Abstract\n   The Routing Over Low-Power and Lossy (ROLL) networks Working Group\n\
    \   has been chartered to work on routing solutions for Low-Power and\n   Lossy\
    \ Networks (LLNs) in various markets: industrial, commercial\n   (building), home,\
    \ and urban networks.  Pursuant to this effort, this\n   document defines the\
    \ IPv6 routing requirements for building\n   automation.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc5867.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2010 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n   This document\
    \ may contain material from IETF Documents or IETF\n   Contributions published\
    \ or made publicly available before November\n   10, 2008.  The person(s) controlling\
    \ the copyright in some of this\n   material may not have granted the IETF Trust\
    \ the right to allow\n   modifications of such material outside the IETF Standards\
    \ Process.\n   Without obtaining an adequate license from the person(s) controlling\n\
    \   the copyright in such materials, this document may not be modified\n   outside\
    \ the IETF Standards Process, and derivative works of it may\n   not be created\
    \ outside the IETF Standards Process, except to format\n   it for publication\
    \ as an RFC or to translate it into languages other\n   than English.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................4\n\
    \   2. Terminology .....................................................6\n  \
    \    2.1. Requirements Language ......................................6\n   3.\
    \ Overview of Building Automation Networks ........................6\n      3.1.\
    \ Introduction ...............................................6\n      3.2. Building\
    \ Systems Equipment .................................7\n           3.2.1. Sensors/Actuators\
    \ ...................................7\n           3.2.2. Area Controllers ....................................7\n\
    \           3.2.3. Zone Controllers ....................................8\n  \
    \    3.3. Equipment Installation Methods .............................8\n    \
    \  3.4. Device Density .............................................9\n      \
    \     3.4.1. HVAC Device Density .................................9\n        \
    \   3.4.2. Fire Device Density .................................9\n          \
    \ 3.4.3. Lighting Device Density ............................10\n           3.4.4.\
    \ Physical Security Device Density ...................10\n   4. Traffic Pattern\
    \ ................................................10\n   5. Building Automation\
    \ Routing Requirements .......................12\n      5.1. Device and Network\
    \ Commissioning ..........................12\n           5.1.1. Zero-Configuration\
    \ Installation ....................12\n           5.1.2. Local Testing ......................................12\n\
    \           5.1.3. Device Replacement .................................13\n  \
    \    5.2. Scalability ...............................................13\n    \
    \       5.2.1. Network Domain .....................................13\n      \
    \     5.2.2. Peer-to-Peer Communication .........................13\n      5.3.\
    \ Mobility ..................................................13\n           5.3.1.\
    \ Mobile Device Requirements .........................14\n      5.4. Resource\
    \ Constrained Devices ..............................15\n           5.4.1. Limited\
    \ Memory Footprint on Host Devices ...........15\n           5.4.2. Limited Processing\
    \ Power for Routers ...............15\n           5.4.3. Sleeping Devices ...................................15\n\
    \      5.5. Addressing ................................................16\n  \
    \    5.6. Manageability .............................................16\n    \
    \       5.6.1. Diagnostics ........................................17\n      \
    \     5.6.2. Route Tracking .....................................17\n      5.7.\
    \ Route Selection ...........................................17\n           5.7.1.\
    \ Route Cost .........................................17\n           5.7.2. Route\
    \ Adaptation ...................................18\n           5.7.3. Route Redundancy\
    \ ...................................18\n           5.7.4. Route Discovery Time\
    \ ...............................18\n           5.7.5. Route Preference ...................................18\n\
    \           5.7.6. Real-Time Performance Measures .....................18\n  \
    \         5.7.7. Prioritized Routing ................................18\n    \
    \  5.8. Security Requirements .....................................19\n      \
    \     5.8.1. Building Security Use Case .........................19\n        \
    \   5.8.2. Authentication .....................................20\n          \
    \ 5.8.3. Encryption .........................................20\n           5.8.4.\
    \ Disparate Security Policies ........................21\n           5.8.5. Routing\
    \ Security Policies to Sleeping Devices ......21\n   6. Security Considerations\
    \ ........................................21\n   7. Acknowledgments ................................................22\n\
    \   8. References .....................................................22\n  \
    \    8.1. Normative References ......................................22\n    \
    \  8.2. Informative References ....................................22\n   Appendix\
    \ A. Additional Building Requirements ......................23\n      A.1. Additional\
    \ Commercial Product Requirements ................23\n           A.1.1. Wired\
    \ and Wireless Implementations .................23\n           A.1.2. World-Wide\
    \ Applicability ...........................23\n      A.2. Additional Installation\
    \ and Commissioning Requirements ....23\n           A.2.1. Unavailability of an\
    \ IP Network ....................23\n      A.3. Additional Network Requirements\
    \ ...........................23\n           A.3.1. TCP/UDP ............................................23\n\
    \           A.3.2. Interference Mitigation ............................23\n  \
    \         A.3.3. Packet Reliability .................................24\n    \
    \       A.3.4. Merging Commissioned Islands .......................24\n      \
    \     A.3.5. Adjustable Routing Table Sizes .....................24\n        \
    \   A.3.6. Automatic Gain Control .............................24\n          \
    \ A.3.7. Device and Network Integrity .......................24\n      A.4. Additional\
    \ Performance Requirements .......................24\n           A.4.1. Data Rate\
    \ Performance ..............................24\n           A.4.2. Firmware Upgrades\
    \ ..................................25\n           A.4.3. Route Persistence ..................................25\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The Routing Over Low-Power and Lossy (ROLL) networks Working\
    \ Group\n   has been chartered to work on routing solutions for Low-Power and\n\
    \   Lossy Networks (LLNs) in various markets: industrial, commercial\n   (building),\
    \ home, and urban networks.  Pursuant to this effort, this\n   document defines\
    \ the IPv6 routing requirements for building\n   automation.\n   Commercial buildings\
    \ have been fitted with pneumatic, and\n   subsequently electronic, communication\
    \ routes connecting sensors to\n   their controllers for over one hundred years.\
    \  Recent economic and\n   technical advances in wireless communication allow\
    \ facilities to\n   increasingly utilize a wireless solution in lieu of a wired\
    \ solution,\n   thereby reducing installation costs while maintaining highly reliant\n\
    \   communication.\n   The cost benefits and ease of installation of wireless\
    \ sensors allow\n   customers to further instrument their facilities with additional\n\
    \   sensors, providing tighter control while yielding increased energy\n   savings.\n\
    \   Wireless solutions will be adapted from their existing wired\n   counterparts\
    \ in many of the building applications including, but not\n   limited to, heating,\
    \ ventilation, and air conditioning (HVAC);\n   lighting; physical security; fire;\
    \ and elevator/lift systems.  These\n   devices will be developed to reduce installation\
    \ costs while\n   increasing installation and retrofit flexibility, as well as\n\
    \   increasing the sensing fidelity to improve efficiency and building\n   service\
    \ quality.\n   Sensing devices may be battery-less, battery-powered, or mains-\n\
    \   powered.  Actuators and area controllers will be mains-powered.  Due\n   to\
    \ building code and/or device density (e.g., equipment room), it is\n   envisioned\
    \ that a mix of wired and wireless sensors and actuators\n   will be deployed\
    \ within a building.\n   Building management systems (BMSs) are deployed in a\
    \ large set of\n   vertical markets including universities, hospitals, government\n\
    \   facilities, kindergarten through high school (K-12), pharmaceutical\n   manufacturing\
    \ facilities, and single-tenant or multi-tenant office\n   buildings.  These buildings\
    \ range in size from 100K-sq.-ft.\n   structures (5-story office buildings), to\
    \ 1M-sq.-ft. skyscrapers\n   (100-story skyscrapers), to complex government facilities\
    \ such as the\n   Pentagon.  The described topology is meant to be the model to\
    \ be used\n   in all of these types of environments but clearly must be tailored\
    \ to\n   the building class, building tenant, and vertical market being\n   served.\n\
    \   Section 3 describes the necessary background to understand the\n   context\
    \ of building automation including the sensor, actuator, area\n   controller,\
    \ and zone controller layers of the topology; typical\n   device density; and\
    \ installation practices.\n   Section 4 defines the traffic flow of the aforementioned\
    \ sensors,\n   actuators, and controllers in commercial buildings.\n   Section\
    \ 5 defines the full set of IPv6 routing requirements for\n   commercial buildings.\n\
    \   Appendix A documents important commercial building requirements that\n   are\
    \ out of scope for routing yet will be essential to the final\n   acceptance of\
    \ the protocols used within the building.\n   Section 3 and Appendix A are mainly\
    \ included for educational\n   purposes.\n   The expressed aim of this document\
    \ is to provide the set of IPv6\n   routing requirements for LLNs in buildings,\
    \ as described in\n   Section 5.\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   For a description of the terminology used in this specification,\n\
    \   please see [ROLL-TERM].\n"
- title: 2.1.  Requirements Language
  contents:
  - "2.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in [RFC2119].\n"
- title: 3.  Overview of Building Automation Networks
  contents:
  - '3.  Overview of Building Automation Networks

    '
- title: 3.1.  Introduction
  contents:
  - "3.1.  Introduction\n   To understand the network systems requirements of a building\n\
    \   management system in a commercial building, this document uses a\n   framework\
    \ to describe the basic functions and composition of the\n   system.  A BMS is\
    \ a hierarchical system of sensors, actuators,\n   controllers, and user interface\
    \ devices that interoperate to provide\n   a safe and comfortable environment\
    \ while constraining energy costs.\n   A BMS is divided functionally across different\
    \ but interrelated\n   building subsystems such as heating, ventilation, and air\n\
    \   conditioning (HVAC); fire; security; lighting; shutters; and\n   elevator/lift\
    \ control systems, as denoted in Figure 1.\n   Much of the makeup of a BMS is\
    \ optional and installed at the behest\n   of the customer.  Sensors and actuators\
    \ have no standalone\n   functionality.  All other devices support partial or\
    \ complete\n   standalone functionality.  These devices can optionally be tethered\n\
    \   to form a more cohesive system.  The customer requirements dictate\n   the\
    \ level of integration within the facility.  This architecture\n   provides excellent\
    \ fault tolerance since each node is designed to\n   operate in an independent\
    \ mode if the higher layers are unavailable.\n                 +------+ +-----+\
    \ +------+ +------+ +------+ +------+\n   Bldg App'ns   |      | |     | |   \
    \   | |      | |      | |      |\n                 |      | |     | |      | |\
    \      | |      | |      |\n   Building Cntl |      | |     | |   S  | |   L \
    \ | |   S  | |  E   |\n                 |      | |     | |   E  | |   I  | | \
    \  H  | |  L   |\n   Area Control  |  H   | |  F  | |   C  | |   G  | |   U  |\
    \ |  E   |\n                 |  V   | |  I  | |   U  | |   H  | |   T  | |  V\
    \   |\n   Zone Control  |  A   | |  R  | |   R  | |   T  | |   T  | |  A   |\n\
    \                 |  C   | |  E  | |   I  | |   I  | |   E  | |  T   |\n   Actuators\
    \     |      | |     | |   T  | |   N  | |   R  | |  O   |\n                 |\
    \      | |     | |   Y  | |   G  | |   S  | |  R   |\n   Sensors       |     \
    \ | |     | |      | |      | |      | |      |\n                 +------+ +-----+\
    \ +------+ +------+ +------+ +------+\n                  Figure 1: Building Systems\
    \ and Devices\n"
- title: 3.2.  Building Systems Equipment
  contents:
  - '3.2.  Building Systems Equipment

    '
- title: 3.2.1.  Sensors/Actuators
  contents:
  - "3.2.1.  Sensors/Actuators\n   As Figure 1 indicates, a BMS may be composed of\
    \ many functional\n   stacks or silos that are interoperably woven together via\
    \ building\n   applications.  Each silo has an array of sensors that monitor the\n\
    \   environment and actuators that modify the environment, as determined\n   by\
    \ the upper layers of the BMS topology.  The sensors typically are\n   at the\
    \ edge of the network structure, providing environmental data\n   for the system.\
    \  The actuators are the sensors' counterparts,\n   modifying the characteristics\
    \ of the system, based on the sensor data\n   and the applications deployed.\n"
- title: 3.2.2.  Area Controllers
  contents:
  - "3.2.2.  Area Controllers\n   An area describes a small physical locale within\
    \ a building,\n   typically a room.  HVAC (temperature and humidity) and lighting\
    \ (room\n   lighting, shades, solar loads) vendors oftentimes deploy area\n  \
    \ controllers.  Area controllers are fed by sensor inputs that monitor\n   the\
    \ environmental conditions within the room.  Common sensors found\n   in many\
    \ rooms that feed the area controllers include temperature,\n   occupancy, lighting\
    \ load, solar load, and relative humidity.  Sensors\n   found in specialized rooms\
    \ (such as chemistry labs) might include air\n   flow, pressure, and CO2 and CO\
    \ particle sensors.  Room actuation\n   includes temperature setpoint, lights,\
    \ and blinds/curtains.\n"
- title: 3.2.3.  Zone Controllers
  contents:
  - "3.2.3.  Zone Controllers\n   Zone controllers support a similar set of characteristics\
    \ to area\n   controllers, albeit for an extended space.  A zone is normally a\n\
    \   logical grouping or functional division of a commercial building.  A\n   zone\
    \ may also coincidentally map to a physical locale such as a\n   floor.\n   Zone\
    \ controllers may have direct sensor inputs (smoke detectors for\n   fire), controller\
    \ inputs (room controllers for air handlers in HVAC),\n   or both (door controllers\
    \ and tamper sensors for security).  Like\n   area/room controllers, zone controllers\
    \ are standalone devices that\n   operate independently or may be attached to\
    \ the larger network for\n   more synergistic control.\n"
- title: 3.3.  Equipment Installation Methods
  contents:
  - "3.3.  Equipment Installation Methods\n   A BMS is installed very differently\
    \ from most other IT networks.  IT\n   networks are typically installed as an\
    \ overlay onto the existing\n   environment and are installed from the inside\
    \ out.  That is, the\n   network wiring infrastructure is installed; the switches,\
    \ routers,\n   and servers are connected and made operational; and finally, the\n\
    \   endpoints (e.g., PCs, VoIP phones) are added.\n   BMSs, on the other hand,\
    \ are installed from the outside in.  That is,\n   the endpoints (thermostats,\
    \ lights, smoke detectors) are installed in\n   the spaces first; local control\
    \ is established in each room and\n   tested for proper operation.  The individual\
    \ rooms are later lashed\n   together into a subsystem (e.g., lighting).  The\
    \ individual\n   subsystems (e.g., lighting, HVAC) then coalesce.  Later, the\
    \ entire\n   system may be merged onto the enterprise network.\n   The rationale\
    \ for this is partly due to the different construction\n   trades having access\
    \ to a building under construction at different\n   times.  The sheer size of\
    \ a building often dictates that even a\n   single trade may have multiple independent\
    \ teams working\n   simultaneously.  Furthermore, the HVAC, lighting, and fire\
    \ systems\n   must be fully operational before the building can obtain its\n \
    \  occupancy permit.  Hence, the BMS must be in place and configured\n   well\
    \ before any of the IT servers (DHCP; Authentication,\n   Authorization, and Accounting\
    \ (AAA); DNS; etc.) are operational.\n   This implies that the BMS cannot rely\
    \ on the availability of the IT\n   network infrastructure or application servers.\
    \  Rather, the BMS\n   installation should be planned to dovetail into the IT\
    \ system once\n   the IT system is available for easy migration onto the IT network.\n\
    \   Front-end planning of available switch ports, cable runs, access\n   point\
    \ (AP) placement, firewalls, and security policies will\n   facilitate this adoption.\n"
- title: 3.4.  Device Density
  contents:
  - "3.4.  Device Density\n   Device density differs, depending on the application\
    \ and as dictated\n   by the local building code requirements.  The following\
    \ subsections\n   detail typical installation densities for different applications.\n"
- title: 3.4.1.  HVAC Device Density
  contents:
  - "3.4.1.  HVAC Device Density\n   HVAC room applications typically have sensors/actuators\
    \ and\n   controllers spaced about 50 ft. apart.  In most cases, there is a 3:1\n\
    \   ratio of sensors/actuators to controllers.  That is, for each room\n   there\
    \ is an installed temperature sensor, flow sensor, and damper\n   actuator for\
    \ the associated room controller.\n   HVAC equipment room applications are quite\
    \ different.  An air handler\n   system may have a single controller with up to\
    \ 25 sensors and\n   actuators within 50 ft. of the air handler.  A chiller or\
    \ boiler is\n   also controlled with a single equipment controller instrumented\
    \ with\n   25 sensors and actuators.  Each of these devices would be\n   individually\
    \ addressed since the devices are mandated or optional as\n   defined by the specified\
    \ HVAC application.  Air handlers typically\n   serve one or two floors of the\
    \ building.  Chillers and boilers may be\n   installed per floor, but many times\
    \ they service a wing, building, or\n   the entire complex via a central plant.\n\
    \   These numbers are typical.  In special cases, such as clean rooms,\n   operating\
    \ rooms, pharmaceutical facilities, and labs, the ratio of\n   sensors to controllers\
    \ can increase by a factor of three.  Tenant\n   installations such as malls would\
    \ opt for packaged units where much\n   of the sensing and actuation is integrated\
    \ into the unit; here, a\n   single device address would serve the entire unit.\n"
- title: 3.4.2.  Fire Device Density
  contents:
  - "3.4.2.  Fire Device Density\n   Fire systems are much more uniformly installed,\
    \ with smoke detectors\n   installed about every 50 ft.  This is dictated by local\
    \ building\n   codes.  Fire pull boxes are installed uniformly about every 150\
    \ ft.\n   A fire controller will service a floor or wing.  The fireman's fire\n\
    \   panel will service the entire building and typically is installed in\n   the\
    \ atrium.\n"
- title: 3.4.3.  Lighting Device Density
  contents:
  - "3.4.3.  Lighting Device Density\n   Lighting is also very uniformly installed,\
    \ with ballasts installed\n   approximately every 10 ft.  A lighting panel typically\
    \ serves 48 to\n   64 zones.  Wired systems tether many lights together into a\
    \ single\n   zone.  Wireless systems configure each fixture independently to\n\
    \   increase flexibility and reduce installation costs.\n"
- title: 3.4.4.  Physical Security Device Density
  contents:
  - "3.4.4.  Physical Security Device Density\n   Security systems are non-uniformly\
    \ oriented, with heavy density near\n   doors and windows and lighter density\
    \ in the building's interior\n   space.\n   The recent influx of interior and\
    \ perimeter camera systems is\n   increasing the security footprint.  These cameras\
    \ are atypical\n   endpoints requiring up to 1 megabit/second (Mbit/s) data rates\
    \ per\n   camera, as contrasted by the few kbit/s needed by most other BMS\n \
    \  sensing equipment.  Previously, camera systems had been deployed on\n   proprietary\
    \ wired high-speed networks.  More recent implementations\n   utilize wired or\
    \ wireless IP cameras integrated into the enterprise\n   LAN.\n"
- title: 4.  Traffic Pattern
  contents:
  - "4.  Traffic Pattern\n   The independent nature of the automation subsystems within\
    \ a building\n   can significantly affect network traffic patterns.  Much of the\
    \ real-\n   time sensor environmental data and actuator control stays within the\n\
    \   local LLN environment, while alarms and other event data will\n   percolate\
    \ to higher layers.\n   Each sensor in the LLN unicasts point to point (P2P) about\
    \ 200 bytes\n   of sensor data to its associated controller each minute and expects\n\
    \   an application acknowledgment unicast returned from the destination.\n   Each\
    \ controller unicasts messages at a nominal rate of 6 kbit/minute\n   to peer\
    \ or supervisory controllers.  Thirty percent of each node's\n   packets are destined\
    \ for other nodes within the LLN.  Seventy percent\n   of each node's packets\
    \ are destined for an aggregation device\n   (multipoint to point (MP2P)) and\
    \ routed off the LLN.  These messages\n   also require a unicast acknowledgment\
    \ from the destination.  The\n   above values assume direct node-to-node communication;\
    \ meshing and\n   error retransmissions are not considered.\n   Multicasts (point\
    \ to multipoint (P2MP)) to all nodes in the LLN occur\n   for node and object\
    \ discovery when the network is first commissioned.\n   This data is typically\
    \ a one-time bind that is henceforth persisted.\n   Lighting systems will also\
    \ readily use multicasting during normal\n   operations to turn banks of lights\
    \ \"on\" and \"off\" simultaneously.\n   BMSs may be either polled or event-based.\
    \  Polled data systems will\n   generate a uniform and constant packet load on\
    \ the network.  Polled\n   architectures, however, have proven not to be scalable.\
    \  Today, most\n   vendors have developed event-based systems that pass data on\
    \ event.\n   These systems are highly scalable and generate low data on the\n\
    \   network at quiescence.  Unfortunately, the systems will generate a\n   heavy\
    \ load on startup since all initial sensor data must migrate to\n   the controller\
    \ level.  They also will generate a temporary but heavy\n   load during firmware\
    \ upgrades.  This latter load can normally be\n   mitigated by performing these\
    \ downloads during off-peak hours.\n   Devices will also need to reference peers\
    \ periodically for sensor\n   data or to coordinate operation across systems.\
    \  Normally, though,\n   data will migrate from the sensor level upwards through\
    \ the local and\n   area levels, and then to the supervisory level.  Traffic bottlenecks\n\
    \   will typically form at the funnel point from the area controllers to\n   the\
    \ supervisory controllers.\n   Initial system startup after a controlled outage\
    \ or unexpected power\n   failure puts tremendous stress on the network and on\
    \ the routing\n   algorithms.  A BMS is comprised of a myriad of control algorithms\
    \ at\n   the room, area, zone, and enterprise layers.  When these control\n  \
    \ algorithms are at quiescence, the real-time data rate is small, and\n   the\
    \ network will not saturate.  An overall network traffic load of 6\n   kbit/s\
    \ is typical at quiescence.  However, upon any power loss, the\n   control loops\
    \ and real-time data quickly atrophy.  A short power\n   disruption of only 10\
    \ minutes may have a long-term deleterious impact\n   on the building control\
    \ systems, taking many hours to regain proper\n   control.  Control applications\
    \ that cannot handle this level of\n   disruption (e.g., hospital operating rooms)\
    \ must be fitted with a\n   secondary power source.\n   Power disruptions are\
    \ unexpected and in most cases will immediately\n   impact lines-powered devices.\
    \  Power disruptions, however, are\n   transparent to battery-powered devices.\
    \  These devices will continue\n   to attempt to access the LLN during the outage.\
    \  Battery-powered\n   devices designed to buffer data that has not been delivered\
    \ will\n   further stress network operations when power returns.\n   Upon restart,\
    \ lines-powered devices will naturally dither due to\n   primary equipment delays\
    \ or variance in the device self-tests.\n   However, most lines-powered devices\
    \ will be ready to access the LLN\n   network within 10 seconds of power-up. \
    \ Empirical testing indicates\n   that routes acquired during startup will tend\
    \ to be very oblique\n   since the available neighbor lists are incomplete.  This\
    \ demands an\n   adaptive routing protocol to allow for route optimization as\
    \ the\n   network stabilizes.\n"
- title: 5.  Building Automation Routing Requirements
  contents:
  - "5.  Building Automation Routing Requirements\n   Following are the building automation\
    \ routing requirements for\n   networks used to integrate building sensor, actuator,\
    \ and control\n   products.  These requirements are written not presuming any\n\
    \   preordained network topology, physical media (wired), or radio\n   technology\
    \ (wireless).\n"
- title: 5.1.  Device and Network Commissioning
  contents:
  - "5.1.  Device and Network Commissioning\n   Building control systems typically\
    \ are installed and tested by\n   electricians having little computer knowledge\
    \ and no network\n   communication knowledge whatsoever.  These systems are often\n\
    \   installed during the building construction phase, before the drywall\n   and\
    \ ceilings are in place.  For new construction projects, the\n   building enterprise\
    \ IP network is not in place during installation of\n   the building control system.\
    \  For retrofit applications, the\n   installer will still operate independently\
    \ from the IP network so as\n   not to affect network operations during the installation\
    \ phase.\n   In traditional wired systems, correct operation of a light\n   switch/ballast\
    \ pair was as simple as flipping on the light switch.\n   In wireless applications,\
    \ the tradesperson has to assure the same\n   operation, yet be sure the operation\
    \ of the light switch is\n   associated with the proper ballast.\n   System-level\
    \ commissioning will later be deployed using a more\n   computer savvy person\
    \ with access to a commissioning device (e.g., a\n   laptop computer).  The completely\
    \ installed and commissioned\n   enterprise IP network may or may not be in place\
    \ at this time.\n   Following are the installation routing requirements.\n"
- title: 5.1.1.  Zero-Configuration Installation
  contents:
  - "5.1.1.  Zero-Configuration Installation\n   It MUST be possible to fully commission\
    \ network devices without\n   requiring any additional commissioning device (e.g.,\
    \ a laptop).  From\n   the ROLL perspective, \"zero configuration\" means that\
    \ a node can\n   obtain an address and join the network on its own, without human\n\
    \   intervention.\n"
- title: 5.1.2.  Local Testing
  contents:
  - "5.1.2.  Local Testing\n   During installation, the room sensors, actuators, and\
    \ controllers\n   SHOULD be able to route packets amongst themselves and to any\
    \ other\n   device within the LLN, without requiring any additional routing\n\
    \   infrastructure or routing configuration.\n"
- title: 5.1.3.  Device Replacement
  contents:
  - "5.1.3.  Device Replacement\n   To eliminate the need to reconfigure the application\
    \ upon replacing a\n   failed device in the LLN, the replaced device must be able\
    \ to\n   advertise the old IP address of the failed device in addition to its\n\
    \   new IP address.  The routing protocols MUST support hosts and routers\n  \
    \ that advertise multiple IPv6 addresses.\n"
- title: 5.2.  Scalability
  contents:
  - "5.2.  Scalability\n   Building control systems are designed for facilities from\
    \ 50,000 sq.\n   ft. to 1M+ sq. ft.  The networks that support these systems must\n\
    \   cost-effectively scale accordingly.  In larger facilities,\n   installation\
    \ may occur simultaneously on various wings or floors, yet\n   the end system\
    \ must seamlessly merge.  Following are the scalability\n   requirements.\n"
- title: 5.2.1.  Network Domain
  contents:
  - "5.2.1.  Network Domain\n   The routing protocol MUST be able to support networks\
    \ with at least\n   2,000 nodes, where 1,000 nodes would act as routers and the\
    \ other\n   1,000 nodes would be hosts.  Subnetworks (e.g., rooms, primary\n \
    \  equipment) within the network must support up to 255 sensors and/or\n   actuators.\n"
- title: 5.2.2.  Peer-to-Peer Communication
  contents:
  - "5.2.2.  Peer-to-Peer Communication\n   The data domain for commercial BMSs may\
    \ sprawl across a vast portion\n   of the physical domain.  For example, a chiller\
    \ may reside in the\n   facility's basement due to its size, yet the associated\
    \ cooling\n   towers will reside on the roof.  The cold-water supply and return\n\
    \   pipes snake through all of the intervening floors.  The feedback\n   control\
    \ loops for these systems require data from across the\n   facility.\n   A network\
    \ device MUST be able to communicate in an end-to-end manner\n   with any other\
    \ device on the network.  Thus, the routing protocol\n   MUST provide routes between\
    \ arbitrary hosts within the appropriate\n   administrative domain.\n"
- title: 5.3.  Mobility
  contents:
  - "5.3.  Mobility\n   Most devices are affixed to walls or installed on ceilings\
    \ within\n   buildings.  Hence, the mobility requirements for commercial buildings\n\
    \   are few.  However, in wireless environments, location tracking of\n   occupants\
    \ and assets is gaining favor.  Asset-tracking applications,\n   such as tracking\
    \ capital equipment (e.g., wheelchairs) in medical\n   facilities, require monitoring\
    \ movement with granularity of a minute;\n   however, tracking babies in a pediatric\
    \ ward would require latencies\n   less than a few seconds.\n   The following\
    \ subsections document the mobility requirements in the\n   routing layer for\
    \ mobile devices.  Note, however, that mobility can\n   be implemented at various\
    \ layers of the system, and the specific\n   requirements depend on the chosen\
    \ layer.  For instance, some devices\n   may not depend on a static IP address\
    \ and are capable of re-\n   establishing application-level communications when\
    \ given a new IP\n   address.  Alternatively, mobile IP may be used, or the set\
    \ of routers\n   in a building may give an impression of a building-wide network\
    \ and\n   allow devices to retain their addresses regardless of where they are,\n\
    \   handling routing between the devices in the background.\n"
- title: 5.3.1.  Mobile Device Requirements
  contents:
  - "5.3.1.  Mobile Device Requirements\n   To minimize network dynamics, mobile devices\
    \ while in motion should\n   not be allowed to act as forwarding devices (routers)\
    \ for other\n   devices in the LLN.  Network configuration should allow devices\
    \ to be\n   configured as routers or hosts.\n"
- title: 5.3.1.1.  Device Mobility within the LLN
  contents:
  - "5.3.1.1.  Device Mobility within the LLN\n   An LLN typically spans a single\
    \ floor in a commercial building.\n   Mobile devices may move within this LLN.\
    \  For example, a wheelchair\n   may be moved from one room on the floor to another\
    \ room on the same\n   floor.\n   A mobile LLN device that moves within the confines\
    \ of the same LLN\n   SHOULD re-establish end-to-end communication with a fixed\
    \ device also\n   in the LLN within 5 seconds after it ceases movement.  The LLN\n\
    \   network convergence time should be less than 10 seconds once the\n   mobile\
    \ device stops moving.\n"
- title: 5.3.1.2.  Device Mobility across LLNs
  contents:
  - "5.3.1.2.  Device Mobility across LLNs\n   A mobile device may move across LLNs,\
    \ such as a wheelchair being\n   moved to a different floor.\n   A mobile device\
    \ that moves outside of its original LLN SHOULD re-\n   establish end-to-end communication\
    \ with a fixed device also in the\n   new LLN within 10 seconds after the mobile\
    \ device ceases movement.\n   The network convergence time should be less than\
    \ 20 seconds once the\n   mobile device stops moving.\n"
- title: 5.4.  Resource Constrained Devices
  contents:
  - "5.4.  Resource Constrained Devices\n   Sensing and actuator device processing\
    \ power and memory may be 4\n   orders of magnitude less (i.e., 10,000x) than\
    \ many more traditional\n   client devices on an IP network.  The routing mechanisms\
    \ must\n   therefore be tailored to fit these resource constrained devices.\n"
- title: 5.4.1.  Limited Memory Footprint on Host Devices
  contents:
  - "5.4.1.  Limited Memory Footprint on Host Devices\n   The software size requirement\
    \ for non-routing devices (e.g., sleeping\n   sensors and actuators) SHOULD be\
    \ implementable in 8-bit devices with\n   no more than 128 KB of memory.\n"
- title: 5.4.2.  Limited Processing Power for Routers
  contents:
  - "5.4.2.  Limited Processing Power for Routers\n   The software size requirements\
    \ for routing devices (e.g., room\n   controllers) SHOULD be implementable in\
    \ 8-bit devices with no more\n   than 256 KB of flash memory.\n"
- title: 5.4.3.  Sleeping Devices
  contents:
  - "5.4.3.  Sleeping Devices\n   Sensing devices will, in some cases, utilize battery\
    \ power or energy\n   harvesting techniques for power and will operate mostly\
    \ in a sleep\n   mode to maintain power consumption within a modest budget.  The\n\
    \   routing protocol MUST take into account device characteristics such\n   as\
    \ power budget.\n   Typically, sensor battery life (2,000 mAh) needs to extend\
    \ for at\n   least 5 years when the device is transmitting its data (200 octets)\n\
    \   once per minute over a low-power transceiver (25 mA) and expecting an\n  \
    \ application acknowledgment.  In this case, the transmitting device\n   must\
    \ leave its receiver in a high-powered state, awaiting the return\n   of the application\
    \ ACK.  To minimize this latency, a highly efficient\n   routing protocol that\
    \ minimizes hops, and hence end-to-end\n   communication, is required.  The routing\
    \ protocol MUST take into\n   account node properties, such as \"low-powered node\"\
    , that produce\n   efficient low-latency routes that minimize radio \"on\" time\
    \ for these\n   devices.\n   Sleeping devices MUST be able to receive inbound\
    \ data.  Messages sent\n   to battery-powered nodes MUST be buffered by the last-hop\
    \ router for\n   a period of at least 20 seconds when the destination node is\n\
    \   currently in its sleep cycle.\n"
- title: 5.5.  Addressing
  contents:
  - "5.5.  Addressing\n   Building management systems require different communication\
    \ schemes\n   to solicit or post network information.  Multicasts or anycasts\
    \ need\n   to be used to decipher unresolved references within a device when the\n\
    \   device first joins the network.\n   As with any network communication, multicasting\
    \ should be minimized.\n   This is especially a problem for small embedded devices\
    \ with limited\n   network bandwidth.  Multicasts are typically used for network\
    \ joins\n   and application binding in embedded systems.  Routing MUST support\n\
    \   anycast, unicast, and multicast.\n"
- title: 5.6.  Manageability
  contents:
  - "5.6.  Manageability\n   As previously noted in Section 3.3, installation of LLN\
    \ devices\n   within a BMS follows an \"outside-in\" work flow.  Edge devices\
    \ are\n   installed first and tested for communication and application\n   integrity.\
    \  These devices are then aggregated into islands, then\n   LLNs, and later affixed\
    \ onto the enterprise network.\n   The need for diagnostics most often occurs\
    \ during the installation\n   and commissioning phase, although at times diagnostic\
    \ information may\n   be requested during normal operation.  Battery-powered wireless\n\
    \   devices typically will have a self-diagnostic mode that can be\n   initiated\
    \ via a button press on the device.  The device will display\n   its link status\
    \ and/or end-to-end connectivity when the button is\n   pressed.  Lines-powered\
    \ devices will continuously display\n   communication status via a bank of LEDs,\
    \ possibly denoting signal\n   strength and end-to-end application connectivity.\n\
    \   The local diagnostics noted above oftentimes are suitable for\n   defining\
    \ room-level networks.  However, as these devices aggregate,\n   system-level\
    \ diagnostics may need to be executed to ameliorate route\n   vacillation, excessive\
    \ hops, communication retries, and/or network\n   bottlenecks.\n   In operational\
    \ networks, due to the mission-critical nature of the\n   application, the LLN\
    \ devices will be temporally monitored by the\n   higher layers to assure that\
    \ communication integrity is maintained.\n   Failure to maintain this communication\
    \ will result in an alarm being\n   forwarded to the enterprise network from the\
    \ monitoring node for\n   analysis and remediation.\n   In addition to the initial\
    \ installation and commissioning of the\n   system, it is equally important for\
    \ the ongoing maintenance of the\n   system to be simple and inexpensive.  This\
    \ implies a straightforward\n   device swap when a failed device is replaced,\
    \ as noted in Section\n   5.1.3.\n"
- title: 5.6.1.  Diagnostics
  contents:
  - "5.6.1.  Diagnostics\n   To improve diagnostics, the routing protocol SHOULD be\
    \ able to be\n   placed in and out of \"verbose\" mode.  Verbose mode is a temporary\n\
    \   debugging mode that provides additional communication information\n   including,\
    \ at least, the total number of routed packets sent and\n   received, the number\
    \ of routing failures (no route available),\n   neighbor table members, and routing\
    \ table entries.  The data provided\n   in verbose mode should be sufficient that\
    \ a network connection graph\n   could be constructed and maintained by the monitoring\
    \ node.\n   Diagnostic data should be kept by the routers continuously and be\n\
    \   available for solicitation at any time by any other node on the\n   internetwork.\
    \  Verbose mode will be activated/deactivated via\n   unicast, multicast, or other\
    \ means.  Devices having available\n   resources may elect to support verbose\
    \ mode continuously.\n"
- title: 5.6.2.  Route Tracking
  contents:
  - "5.6.2.  Route Tracking\n   Route diagnostics SHOULD be supported, providing information\
    \ such as\n   route quality, number of hops, and available alternate active routes\n\
    \   with associated costs.  Route quality is the relative measure of\n   \"goodness\"\
    \ of the selected source to destination route as compared to\n   alternate routes.\
    \  This composite value may be measured as a function\n   of hop count, signal\
    \ strength, available power, existing active\n   routes, or any other criteria\
    \ deemed by ROLL as the route cost\n   differentiator.\n"
- title: 5.7.  Route Selection
  contents:
  - "5.7.  Route Selection\n   Route selection determines reliability and quality\
    \ of the\n   communication among the devices by optimizing routes over time and\n\
    \   resolving any nuances developed at system startup when nodes are\n   asynchronously\
    \ adding themselves to the network.\n"
- title: 5.7.1.  Route Cost
  contents:
  - "5.7.1.  Route Cost\n   The routing protocol MUST support a metric of route quality\
    \ and\n   optimize selection according to such metrics within constraints\n  \
    \ established for links along the routes.  These metrics SHOULD reflect\n   metrics\
    \ such as signal strength, available bandwidth, hop count,\n   energy availability,\
    \ and communication error rates.\n"
- title: 5.7.2.  Route Adaptation
  contents:
  - "5.7.2.  Route Adaptation\n   Communication routes MUST be adaptive and converge\
    \ toward optimality\n   of the chosen metric (e.g., signal quality, hop count)\
    \ in time.\n"
- title: 5.7.3.  Route Redundancy
  contents:
  - "5.7.3.  Route Redundancy\n   The routing layer SHOULD be configurable to allow\
    \ secondary and\n   tertiary routes to be established and used upon failure of\
    \ the\n   primary route.\n"
- title: 5.7.4.  Route Discovery Time
  contents:
  - "5.7.4.  Route Discovery Time\n   Mission-critical commercial applications (e.g.,\
    \ fire, security)\n   require reliable communication and guaranteed end-to-end\
    \ delivery of\n   all messages in a timely fashion.  Application-layer time-outs\
    \ must\n   be selected judiciously to cover anomalous conditions such as lost\n\
    \   packets and/or route discoveries, yet not be set too large to over-\n   damp\
    \ the network response.  If route discovery occurs during packet\n   transmission\
    \ time (reactive routing), it SHOULD NOT add more than 120\n   ms of latency to\
    \ the packet delivery time.\n"
- title: 5.7.5.  Route Preference
  contents:
  - "5.7.5.  Route Preference\n   The routing protocol SHOULD allow for the support\
    \ of manually\n   configured static preferred routes.\n"
- title: 5.7.6.  Real-Time Performance Measures
  contents:
  - "5.7.6.  Real-Time Performance Measures\n   A node transmitting a \"request with\
    \ expected reply\" to another node\n   must send the message to the destination\
    \ and receive the response in\n   not more than 120 ms.  This response time should\
    \ be achievable with 5\n   or less hops in each direction.  This requirement assumes\
    \ network\n   quiescence and a negligible turnaround time at the destination node.\n"
- title: 5.7.7.  Prioritized Routing
  contents:
  - "5.7.7.  Prioritized Routing\n   Network and application packet routing prioritization\
    \ must be\n   supported to assure that mission-critical applications (e.g., fire\n\
    \   detection) cannot be deferred while less critical applications access\n  \
    \ the network.  The routing protocol MUST be able to provide routes\n   with different\
    \ characteristics, also referred to as Quality of\n   Service (QoS) routing.\n"
- title: 5.8.  Security Requirements
  contents:
  - "5.8.  Security Requirements\n   This section sets forth specific requirements\
    \ that are placed on any\n   protocols developed or used in the ROLL building\
    \ environment, in\n   order to ensure adequate security and retain suitable flexibility\
    \ of\n   use and function of the protocol.\n   Due to the variety of buildings\
    \ and tenants, the BMSs must be\n   completely configurable on-site.\n   Due to\
    \ the quantity of the BMS devices (thousands) and their\n   inaccessibility (oftentimes\
    \ above ceilings), security configuration\n   over the network is preferred over\
    \ local configuration.\n   Wireless encryption and device authentication security\
    \ policies need\n   to be considered in commercial buildings, while keeping in\
    \ mind the\n   impact on the limited processing capabilities and additional latency\n\
    \   incurred on the sensors, actuators, and controllers.\n   BMSs are typically\
    \ highly configurable in the field, and hence the\n   security policy is most\
    \ often dictated by the type of building to\n   which the BMS is being installed.\
    \  Single-tenant owner-occupied\n   office buildings installing lighting or HVAC\
    \ control are candidates\n   for implementing a low level of security on the LLN,\
    \ especially when\n   the LLN is not connected to an external network.  Antithetically,\n\
    \   military or pharmaceutical facilities require strong security\n   policies.\
    \  As noted in the installation procedures described in\n   Sections 3.3 and 5.2,\
    \ security policies MUST support dynamic\n   configuration to allow for a low\
    \ level of security during the\n   installation phase (prior to building occupancy,\
    \ when it may be\n   appropriate to use only diagnostic levels of security), yet\
    \ to make\n   it possible to easily raise the security level network-wide during\n\
    \   the commissioning phase of the system.\n"
- title: 5.8.1.  Building Security Use Case
  contents:
  - "5.8.1.  Building Security Use Case\n   LLNs for commercial building applications\
    \ should always implement and\n   use encrypted packets.  However, depending on\
    \ the state of the LLN,\n   the security keys may either be:\n   1) a key obtained\
    \ from a trust center already operable on the LLN;\n   2) a pre-shared static\
    \ key as defined by the general contractor or\n      its designee; or\n   3) a\
    \ well-known default static key.\n   Unless a node entering the network had previously\
    \ received its\n   credentials from the trust center, the entering node will try\
    \ to\n   solicit the trust center for the network key.  If the trust center is\n\
    \   accessible, the trust center will MAC-authenticate the entering node\n   and\
    \ return the security keys.  If the trust center is not available,\n   the entering\
    \ node will check to determine if it has been given a\n   network key by an off-band\
    \ means and use it to access the network.\n   If no network key has been configured\
    \ in the device, it will revert\n   to the default network key and enter the network.\
    \  If neither of\n   these keys were valid, the device would signal via a fault\
    \ LED.\n   This approach would allow for independent simplified commissioning,\n\
    \   yet centralized authentication.  The building owner or building type\n   would\
    \ then dictate when the trust center would be deployed.  In many\n   cases, the\
    \ trust center need not be deployed until all of the local\n   room commissioning\
    \ is complete.  Yet, at the province of the owner,\n   the trust center may be\
    \ deployed from the onset, thereby trading\n   installation and commissioning\
    \ flexibility for tighter security.\n"
- title: 5.8.2.  Authentication
  contents:
  - "5.8.2.  Authentication\n   Authentication SHOULD be optional on the LLN.  Authentication\
    \ SHOULD\n   be fully configurable on-site.  Authentication policy and updates\n\
    \   MUST be routable over-the-air.  Authentication SHOULD occur upon\n   joining\
    \ or rejoining a network.  However, once authenticated, devices\n   SHOULD NOT\
    \ need to reauthenticate with any other devices in the LLN.\n   Packets may need\
    \ authentication at the source and destination nodes;\n   however, packets routed\
    \ through intermediate hops should not need\n   reauthentication at each hop.\n\
    \   These requirements mean that at least one LLN routing protocol\n   solution\
    \ specification MUST include support for authentication.\n"
- title: 5.8.3.  Encryption
  contents:
  - '5.8.3.  Encryption

    '
- title: 5.8.3.1.  Encryption Types
  contents:
  - "5.8.3.1.  Encryption Types\n   Data encryption of packets MUST be supported by\
    \ all protocol solution\n   specifications.  Support can be provided by use of\
    \ a network-wide key\n   and/or an application key.  The network key would apply\
    \ to all\n   devices in the LLN.  The application key would apply to a subset\
    \ of\n   devices in the LLN.\n   The network key and application key would be\
    \ mutually exclusive.  The\n   routing protocol MUST allow routing a packet encrypted\
    \ with an\n   application key through forwarding devices without requiring each\n\
    \   node in the route to have the application key.\n"
- title: 5.8.3.2.  Packet Encryption
  contents:
  - "5.8.3.2.  Packet Encryption\n   The encryption policy MUST support either encryption\
    \ of the payload\n   only or of the entire packet.  Payload-only encryption would\n\
    \   eliminate the decryption/re-encryption overhead at every hop,\n   providing\
    \ more real-time performance.\n"
- title: 5.8.4.  Disparate Security Policies
  contents:
  - "5.8.4.  Disparate Security Policies\n   Due to the limited resources of an LLN,\
    \ the security policy defined\n   within the LLN MUST be able to differ from that\
    \ of the rest of the IP\n   network within the facility, yet packets MUST still\
    \ be able to route\n   to or through the LLN from/to these networks.\n"
- title: 5.8.5.  Routing Security Policies to Sleeping Devices
  contents:
  - "5.8.5.  Routing Security Policies to Sleeping Devices\n   The routing protocol\
    \ MUST gracefully handle routing temporal security\n   updates (e.g., dynamic\
    \ keys) to sleeping devices on their \"awake\"\n   cycle to assure that sleeping\
    \ devices can readily and efficiently\n   access the network.\n"
- title: 6.  Security Considerations
  contents:
  - "6.  Security Considerations\n   The requirements placed on the LLN routing protocol\
    \ in order to\n   provide the correct level of security support are presented\
    \ in\n   Section 5.8.\n   LLNs deployed in a building environment may be entirely\
    \ isolated from\n   other networks, attached to normal IP networks within the\
    \ building\n   yet physically disjoint from the wider Internet, or connected either\n\
    \   directly or through other IP networks to the Internet.  Additionally,\n  \
    \ even where no wired connectivity exists outside of the building, the\n   use\
    \ of wireless infrastructure within the building means that\n   physical connectivity\
    \ to the LLN is possible for an attacker.\n   Therefore, it is important that\
    \ any routing protocol solution\n   designed to meet the requirements included\
    \ in this document addresses\n   the security features requirements described\
    \ in Section 5.8.\n   Implementations of these protocols will be required in the\
    \ protocol\n   specifications to provide the level of support indicated in Section\n\
    \   5.8, and will be encouraged to make the support flexibly configurable\n  \
    \ to enable an operator to make a judgment of the level of security\n   that they\
    \ want to deploy at any time.\n   As noted in Section 5.8, use/deployment of the\
    \ different security\n   features is intended to be optional.  This means that,\
    \ although the\n   protocols developed must conform to the requirements specified,\
    \ the\n   operator is free to determine the level of risk and the trade-offs\n\
    \   against performance.  An implementation must not make those choices\n   on\
    \ behalf of the operator by avoiding implementing any mandatory-to-\n   implement\
    \ security features.\n   This informational requirements specification introduces\
    \ no new\n   security concerns.\n"
- title: 7.  Acknowledgments
  contents:
  - "7.  Acknowledgments\n   In addition to the authors, JP. Vasseur, David Culler,\
    \ Ted Humpal,\n   and Zach Shelby are gratefully acknowledged for their contributions\n\
    \   to this document.\n"
- title: 8.  References
  contents:
  - '8.  References

    '
- title: 8.1.  Normative References
  contents:
  - "8.1.  Normative References\n   [RFC2119]   Bradner, S., \"Key words for use in\
    \ RFCs to Indicate\n               Requirement Levels\", BCP 14, RFC 2119, March\
    \ 1997.\n"
- title: 8.2.  Informative References
  contents:
  - "8.2.  Informative References\n   [ROLL-TERM] Vasseur, JP., \"Terminology in Low\
    \ power And Lossy\n               Networks\", Work in Progress, March 2010.\n"
- title: Appendix A.  Additional Building Requirements
  contents:
  - "Appendix A.  Additional Building Requirements\n   Appendix A contains additional\
    \ building requirements that were deemed\n   out of scope for ROLL, yet provided\
    \ ancillary substance for the\n   reader.\n"
- title: A.1.  Additional Commercial Product Requirements
  contents:
  - 'A.1.  Additional Commercial Product Requirements

    '
- title: A.1.1.  Wired and Wireless Implementations
  contents:
  - "A.1.1.  Wired and Wireless Implementations\n   Vendors will likely not develop\
    \ a separate product line for both\n   wired and wireless networks.  Hence, the\
    \ solutions set forth must\n   support both wired and wireless implementations.\n"
- title: A.1.2.  World-Wide Applicability
  contents:
  - "A.1.2.  World-Wide Applicability\n   Wireless devices must be supportable unlicensed\
    \ bands.\n"
- title: A.2.  Additional Installation and Commissioning Requirements
  contents:
  - 'A.2.  Additional Installation and Commissioning Requirements

    '
- title: A.2.1.  Unavailability of an IP Network
  contents:
  - "A.2.1.  Unavailability of an IP Network\n   Product commissioning must be performed\
    \ by an application engineer\n   prior to the installation of the IP network (e.g.,\
    \ switches, routers,\n   DHCP, DNS).\n"
- title: A.3.  Additional Network Requirements
  contents:
  - 'A.3.  Additional Network Requirements

    '
- title: A.3.1.  TCP/UDP
  contents:
  - "A.3.1.  TCP/UDP\n   Connection-based and connectionless services must be supported.\n"
- title: A.3.2.  Interference Mitigation
  contents:
  - "A.3.2.  Interference Mitigation\n   The network must automatically detect interference\
    \ and seamlessly\n   switch the channel to improve communication.  Channel changes,\
    \ and\n   the nodes' responses to a given channel change, must occur within 60\n\
    \   seconds.\n"
- title: A.3.3.  Packet Reliability
  contents:
  - "A.3.3.  Packet Reliability\n   In building automation, it is required that the\
    \ network meet the\n   following minimum criteria:\n   <1% MAC-layer errors on\
    \ all messages, after no more than three\n   retries;\n   <0.1% network-layer\
    \ errors on all messages, after no more than three\n   additional retries;\n \
    \  <0.01% application-layer errors on all messages.\n   Therefore, application-layer\
    \ messages will fail no more than once\n   every 100,000 messages.\n"
- title: A.3.4.  Merging Commissioned Islands
  contents:
  - "A.3.4.  Merging Commissioned Islands\n   Subsystems are commissioned by various\
    \ vendors at various times\n   during building construction.  These subnetworks\
    \ must seamlessly\n   merge into networks and networks must seamlessly merge into\n\
    \   internetworks since the end user wants a holistic view of the system.\n"
- title: A.3.5.  Adjustable Routing Table Sizes
  contents:
  - "A.3.5.  Adjustable Routing Table Sizes\n   The routing protocol must allow constrained\
    \ nodes to hold an\n   abbreviated set of routes.  That is, the protocol should\
    \ not mandate\n   that the node routing tables be exhaustive.\n"
- title: A.3.6.  Automatic Gain Control
  contents:
  - "A.3.6.  Automatic Gain Control\n   For wireless implementations, the device radios\
    \ should incorporate\n   automatic transmit power regulation to maximize packet\
    \ transfer and\n   minimize network interference, regardless of network size or\
    \ density.\n"
- title: A.3.7.  Device and Network Integrity
  contents:
  - "A.3.7.  Device and Network Integrity\n   Commercial-building devices must all\
    \ be periodically scanned to\n   assure that each device is viable and can communicate\
    \ data and alarm\n   information as needed.  Routers should maintain previous\
    \ packet flow\n   information temporally to minimize overall network overhead.\n"
- title: A.4.  Additional Performance Requirements
  contents:
  - 'A.4.  Additional Performance Requirements

    '
- title: A.4.1.  Data Rate Performance
  contents:
  - "A.4.1.  Data Rate Performance\n   An effective data rate of 20 kbit/s is the\
    \ lowest acceptable\n   operational data rate on the network.\n"
- title: A.4.2.  Firmware Upgrades
  contents:
  - "A.4.2.  Firmware Upgrades\n   To support high-speed code downloads, routing should\
    \ support\n   transports that provide parallel downloads to targeted devices,\
    \ yet\n   guarantee packet delivery.  In cases where the spatial position of\n\
    \   the devices requires multiple hops, the algorithm should recurse\n   through\
    \ the network until all targeted devices have been serviced.\n   Devices receiving\
    \ a download may cease normal operation, but upon\n   completion of the download\
    \ must automatically resume normal\n   operation.\n"
- title: A.4.3.  Route Persistence
  contents:
  - "A.4.3.  Route Persistence\n   To eliminate high network traffic in power-fail\
    \ or brown-out\n   conditions, previously established routes should be remembered\
    \ and\n   invoked prior to establishing new routes for those devices re-\n   entering\
    \ the network.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Jerry Martocci\n   Johnson Controls Inc.\n   507 E. Michigan\
    \ Street\n   Milwaukee, WI  53202\n   USA\n   Phone: +1 414 524 4010\n   EMail:\
    \ jerald.p.martocci@jci.com\n   Pieter De Mil\n   Ghent University - IBCN\n  \
    \ G. Crommenlaan 8 bus 201\n   Ghent  9050\n   Belgium\n   Phone: +32 9331 4981\n\
    \   Fax:   +32 9331 4899\n   EMail: pieter.demil@intec.ugent.be\n   Nicolas Riou\n\
    \   Schneider Electric\n   Technopole 38TEC T3\n   37 quai Paul Louis Merlin\n\
    \   38050 Grenoble Cedex 9\n   France\n   Phone: +33 4 76 57 66 15\n   EMail:\
    \ nicolas.riou@fr.schneider-electric.com\n   Wouter Vermeylen\n   Arts Centre\
    \ Vooruit\n   Ghent  9000\n   Belgium\n   EMail: wouter@vooruit.be\n"
