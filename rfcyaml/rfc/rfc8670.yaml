- title: __initial_text__
  contents:
  - '             BGP Prefix Segment in Large-Scale Data Centers

    '
- title: Abstract
  contents:
  - "Abstract\n   This document describes the motivation for, and benefits of, applying\n\
    \   Segment Routing (SR) in BGP-based large-scale data centers.  It\n   describes\
    \ the design to deploy SR in those data centers for both the\n   MPLS and IPv6\
    \ data planes.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are candidates for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   https://www.rfc-editor.org/info/rfc8670.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2019 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (https://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction\n   2.  Large-Scale Data-Center Network\
    \ Design Summary\n     2.1.  Reference Design\n   3.  Some Open Problems in Large\
    \ Data-Center Networks\n   4.  Applying Segment Routing in the DC with MPLS Data\
    \ Plane\n     4.1.  BGP Prefix Segment (BGP Prefix-SID)\n     4.2.  EBGP Labeled\
    \ Unicast (RFC 8277)\n       4.2.1.  Control Plane\n       4.2.2.  Data Plane\n\
    \       4.2.3.  Network Design Variation\n       4.2.4.  Global BGP Prefix Segment\
    \ through the Fabric\n       4.2.5.  Incremental Deployments\n     4.3.  IBGP\
    \ Labeled Unicast (RFC 8277)\n   5.  Applying Segment Routing in the DC with IPv6\
    \ Data Plane\n   6.  Communicating Path Information to the Host\n   7.  Additional\
    \ Benefits\n     7.1.  MPLS Data Plane with Operational Simplicity\n     7.2.\
    \  Minimizing the FIB Table\n     7.3.  Egress Peer Engineering\n     7.4.  Anycast\n\
    \   8.  Preferred SRGB Allocation\n   9.  IANA Considerations\n   10. Manageability\
    \ Considerations\n   11. Security Considerations\n   12. References\n     12.1.\
    \  Normative References\n     12.2.  Informative References\n   Acknowledgements\n\
    \   Contributors\n   Authors' Addresses\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Segment Routing (SR), as described in [RFC8402], leverages\
    \ the\n   source-routing paradigm.  A node steers a packet through an ordered\n\
    \   list of instructions called \"segments\".  A segment can represent any\n \
    \  instruction, topological or service based.  A segment can have a\n   local\
    \ semantic to an SR node or a global semantic within an SR\n   domain.  SR allows\
    \ the enforcement of a flow through any topological\n   path while maintaining\
    \ per-flow state only from the ingress node to\n   the SR domain.  SR can be applied\
    \ to the MPLS and IPv6 data planes.\n   The use cases described in this document\
    \ should be considered in the\n   context of the BGP-based large-scale data-center\
    \ (DC) design\n   described in [RFC7938].  This document extends it by applying\
    \ SR both\n   with IPv6 and MPLS data planes.\n"
- title: 2.  Large-Scale Data-Center Network Design Summary
  contents:
  - "2.  Large-Scale Data-Center Network Design Summary\n   This section provides\
    \ a brief summary of the Informational RFC\n   [RFC7938], which outlines a practical\
    \ network design suitable for\n   data centers of various scales:\n   *  Data-center\
    \ networks have highly symmetric topologies with\n      multiple parallel paths\
    \ between two server-attachment points.  The\n      well-known Clos topology is\
    \ most popular among the operators (as\n      described in [RFC7938]).  In a Clos\
    \ topology, the minimum number\n      of parallel paths between two elements is\
    \ determined by the\n      \"width\" of the \"Tier-1\" stage.  See Figure 1 for\
    \ an illustration\n      of the concept.\n   *  Large-scale data centers commonly\
    \ use a routing protocol, such as\n      BGP-4 [RFC4271], in order to provide\
    \ endpoint connectivity.\n      Therefore, recovery after a network failure is\
    \ driven either by\n      local knowledge of directly available backup paths or\
    \ by\n      distributed signaling between the network devices.\n   *  Within data-center\
    \ networks, traffic is load shared using the\n      Equal Cost Multipath (ECMP)\
    \ mechanism.  With ECMP, every network\n      device implements a pseudorandom\
    \ decision, mapping packets to one\n      of the parallel paths by means of a\
    \ hash function calculated over\n      certain parts of the packet, typically\
    \ a combination of various\n      packet header fields.\n   The following is a\
    \ schematic of a five-stage Clos topology with four\n   devices in the \"Tier-1\"\
    \ stage.  Notice that the number of paths\n   between Node1 and Node12 equals\
    \ four; the paths have to cross all of\n   the Tier-1 devices.  At the same time,\
    \ the number of paths between\n   Node1 and Node2 equals two, and the paths only\
    \ cross Tier-2 devices.\n   Other topologies are possible, but for simplicity,\
    \ only the\n   topologies that have a single path from Tier-1 to Tier-3 are\n\
    \   considered below.  The rest could be treated similarly, with a few\n   modifications\
    \ to the logic.\n"
- title: 2.1.  Reference Design
  contents:
  - "2.1.  Reference Design\n                       Tier-2  |           |   Tier-2\n\
    \        +------------>|NODE |--+->|NODE |--+--|NODE |-------------+\n       \
    \ |       +-----|  3  |--+  |  6  |  +--|  9  |-----+       |\n        | +-----+---->|NODE\
    \ |--+  |NODE |  +--|NODE |-----+-----+ |\n        | |     | +---|  4  |--+->|\
    \  7  |--+--|  10 |---+ |     | |\n      |NODE | |NODE | Tier-3   +->|NODE |--+\
    \   Tier-3 |NODE | |NODE |\n      |  1  | |  2  |             |  8  |        \
    \     | 11  | |  12 |\n        A O     B O            <- Servers ->          \
    \  Z O     O O\n                      Figure 1: 5-Stage Clos Topology\n   In the\
    \ reference topology illustrated in Figure 1, it is assumed:\n   *  Each node\
    \ is its own autonomous system (AS) (Node X has AS X).\n      4-byte AS numbers\
    \ are recommended ([RFC6793]).\n      -  For simple and efficient route propagation\
    \ filtering, Node5,\n         Node6, Node7, and Node8 use the same AS; Node3 and\
    \ Node4 use\n         the same AS; and Node9 and Node10 use the same AS.\n   \
    \   -  In the case in which 2-byte autonomous system numbers are used\n      \
    \   for efficient usage of the scarce 2-byte Private Use AS pool,\n         different\
    \ Tier-3 nodes might use the same AS.\n      -  Without loss of generality, these\
    \ details will be simplified in\n         this document.  It is to be assumed\
    \ that each node has its own\n         AS.\n   *  Each node peers with its neighbors\
    \ with a BGP session.  If not\n      specified, external BGP (EBGP) is assumed.\
    \  In a specific use\n      case, internal BGP (IBGP) will be used, but this will\
    \ be called\n      out explicitly in that case.\n   *  Each node originates the\
    \ IPv4 address of its loopback interface\n      into BGP and announces it to its\
    \ neighbors.\n      -  The loopback of Node X is 192.0.2.x/32.\n   In this document,\
    \ the Tier-1, Tier-2, and Tier-3 nodes are referred\n   to as \"Spine\", \"Leaf\"\
    , and \"ToR\" (top of rack) nodes, respectively.\n   When a ToR node acts as a\
    \ gateway to the \"outside world\", it is\n   referred to as a \"border node\"\
    .\n"
- title: 3.  Some Open Problems in Large Data-Center Networks
  contents:
  - "3.  Some Open Problems in Large Data-Center Networks\n   The data-center-network\
    \ design summarized above provides means for\n   moving traffic between hosts\
    \ with reasonable efficiency.  There are\n   few open performance and reliability\
    \ problems that arise in such a\n   design:\n   *  ECMP routing is most commonly\
    \ realized per flow.  This means that\n      large, long-lived \"elephant\" flows\
    \ may affect performance of\n      smaller, short-lived \"mouse\" flows and may\
    \ reduce efficiency of\n      per-flow load sharing.  In other words, per-flow\
    \ ECMP does not\n      perform efficiently when flow-lifetime distribution is\
    \ heavy\n      tailed.  Furthermore, due to hash-function inefficiencies, it is\n\
    \      possible to have frequent flow collisions where more flows get\n      placed\
    \ on one path over the others.\n   *  Shortest-path routing with ECMP implements\
    \ an oblivious routing\n      model that is not aware of the network imbalances.\
    \  If the network\n      symmetry is broken, for example, due to link failures,\
    \ utilization\n      hotspots may appear.  For example, if a link fails between\
    \ Tier-1\n      and Tier-2 devices (e.g., Node5 and Node9), Tier-3 devices Node1\n\
    \      and Node2 will not be aware of that since there are other paths\n     \
    \ available from the perspective of Node3.  They will continue\n      sending\
    \ roughly equal traffic to Node3 and Node4 as if the failure\n      didn't exist,\
    \ which may cause a traffic hotspot.\n   *  Isolating faults in the network with\
    \ multiple parallel paths and\n      ECMP-based routing is nontrivial due to lack\
    \ of determinism.\n      Specifically, the connections from HostA to HostB may\
    \ take a\n      different path every time a new connection is formed, thus making\n\
    \      consistent reproduction of a failure much more difficult.  This\n     \
    \ complexity scales linearly with the number of parallel paths in\n      the network\
    \ and stems from the random nature of path selection by\n      the network devices.\n"
- title: 4.  Applying Segment Routing in the DC with MPLS Data Plane
  contents:
  - '4.  Applying Segment Routing in the DC with MPLS Data Plane

    '
- title: 4.1.  BGP Prefix Segment (BGP Prefix-SID)
  contents:
  - "4.1.  BGP Prefix Segment (BGP Prefix-SID)\n   A BGP Prefix Segment is a segment\
    \ associated with a BGP prefix.  A\n   BGP Prefix Segment is a network-wide instruction\
    \ to forward the\n   packet along the ECMP-aware best path to the related prefix.\n\
    \   The BGP Prefix Segment is defined as the BGP Prefix-SID Attribute in\n   [RFC8669],\
    \ which contains an index.  Throughout this document, the\n   BGP Prefix Segment\
    \ Attribute is referred to as the \"BGP Prefix-SID\"\n   and the encoded index\
    \ as the label index.\n   In this document, the network design decision has been\
    \ made to assume\n   that all the nodes are allocated the same SRGB (Segment Routing\n\
    \   Global Block), e.g., [16000, 23999].  This provides operational\n   simplification\
    \ as explained in Section 8, but this is not a\n   requirement.\n   For illustration\
    \ purposes, when considering an MPLS data plane, it is\n   assumed that the label\
    \ index allocated to prefix 192.0.2.x/32 is X.\n   As a result, a local label\
    \ (16000+x) is allocated for prefix\n   192.0.2.x/32 by each node throughout the\
    \ DC fabric.\n   When the IPv6 data plane is considered, it is assumed that Node\
    \ X is\n   allocated IPv6 address (segment) 2001:DB8::X.\n"
- title: 4.2.  EBGP Labeled Unicast (RFC 8277)
  contents:
  - "4.2.  EBGP Labeled Unicast (RFC 8277)\n   Referring to Figure 1 and [RFC7938],\
    \ the following design\n   modifications are introduced:\n   *  Each node peers\
    \ with its neighbors via an EBGP session with\n      extensions defined in [RFC8277]\
    \ (named \"EBGP8277\" throughout this\n      document) and with the BGP Prefix-SID\
    \ attribute extension as\n      defined in [RFC8669].\n   *  The forwarding plane\
    \ at Tier-2 and Tier-1 is MPLS.\n   *  The forwarding plane at Tier-3 is either\
    \ IP2MPLS (if the host\n      sends IP traffic) or MPLS2MPLS (if the host sends\
    \ MPLS-\n      encapsulated traffic).\n   Figure 2 zooms into a path from ServerA\
    \ to ServerZ within the\n   topology of Figure 1.\n          +---------->|NODE\
    \ |     |NODE |     |NODE |\n          |           |  4  |--+->|  7  |--+--| \
    \ 10 |---+\n      |NODE |                                         |NODE |\n  \
    \    |  1  |                                         | 11  |\n        A      \
    \              <- Servers ->             Z\n          Figure 2: Path from A to\
    \ Z via Nodes 1, 4, 7, 10, and 11\n   Referring to Figures 1 and 2, and assuming\
    \ the IP address with the AS\n   and label-index allocation previously described,\
    \ the following\n   sections detail the control-plane operation and the data-plane\
    \ states\n   for the prefix 192.0.2.11/32 (loopback of Node11).\n"
- title: 4.2.1.  Control Plane
  contents:
  - "4.2.1.  Control Plane\n   Node11 originates 192.0.2.11/32 in BGP and allocates\
    \ to it a BGP\n   Prefix-SID with label-index: index11 [RFC8669].\n   Node11 sends\
    \ the following EBGP8277 update to Node10:\n      IP Prefix:  192.0.2.11/32\n\
    \      Label:  Implicit NULL\n      Next hop:  Node11's interface address on the\
    \ link to Node10\n      AS Path:  {11}\n      BGP Prefix-SID:  Label-Index 11\n\
    \   Node10 receives the above update.  As it is SR capable, Node10 is\n   able\
    \ to interpret the BGP Prefix-SID; therefore, it understands that\n   it should\
    \ allocate the label from its own SRGB block, offset by the\n   label index received\
    \ in the BGP Prefix-SID (16000+11, hence, 16011)\n   to the Network Layer Reachability\
    \ Information (NLRI) instead of\n   allocating a nondeterministic label out of\
    \ a dynamically allocated\n   portion of the local label space.  The implicit\
    \ NULL label in the\n   NLRI tells Node10 that it is the penultimate hop and that\
    \ it must pop\n   the top label on the stack before forwarding traffic for this\
    \ prefix\n   to Node11.\n   Then, Node10 sends the following EBGP8277 update to\
    \ Node7:\n      IP Prefix:  192.0.2.11/32\n      Label:  16011\n      Next hop:\
    \  Node10's interface address on the link to Node7\n      AS Path:  {10, 11}\n\
    \      BGP Prefix-SID:  Label-Index 11\n   Node7 receives the above update.  As\
    \ it is SR capable, Node7 is able\n   to interpret the BGP Prefix-SID; therefore,\
    \ it allocates the local\n   (incoming) label 16011 (16000 + 11) to the NLRI (instead\
    \ of\n   allocating a \"dynamic\" local label from its label manager).  Node7\n\
    \   uses the label in the received EBGP8277 NLRI as the outgoing label\n   (the\
    \ index is only used to derive the local/incoming label).\n   Node7 sends the\
    \ following EBGP8277 update to Node4:\n      IP Prefix:  192.0.2.11/32\n     \
    \ Label:  16011\n      Next hop:  Node7's interface address on the link to Node4\n\
    \      AS Path:  {7, 10, 11}\n      BGP Prefix-SID:  Label-Index 11\n   Node4\
    \ receives the above update.  As it is SR capable, Node4 is able\n   to interpret\
    \ the BGP Prefix-SID; therefore, it allocates the local\n   (incoming) label 16011\
    \ to the NLRI (instead of allocating a \"dynamic\"\n   local label from its label\
    \ manager).  Node4 uses the label in the\n   received EBGP8277 NLRI as an outgoing\
    \ label (the index is only used\n   to derive the local/incoming label).\n   Node4\
    \ sends the following EBGP8277 update to Node1:\n      IP Prefix:  192.0.2.11/32\n\
    \      Label:  16011\n      Next hop:  Node4's interface address on the link to\
    \ Node1\n      AS Path:  {4, 7, 10, 11}\n      BGP Prefix-SID:  Label-Index 11\n\
    \   Node1 receives the above update.  As it is SR capable, Node1 is able\n   to\
    \ interpret the BGP Prefix-SID; therefore, it allocates the local\n   (incoming)\
    \ label 16011 to the NLRI (instead of allocating a \"dynamic\"\n   local label\
    \ from its label manager).  Node1 uses the label in the\n   received EBGP8277\
    \ NLRI as an outgoing label (the index is only used\n   to derive the local/incoming\
    \ label).\n"
- title: 4.2.2.  Data Plane
  contents:
  - "4.2.2.  Data Plane\n   Referring to Figure 1, and assuming all nodes apply the\
    \ same\n   advertisement rules described above and all nodes have the same SRGB\n\
    \   (16000-23999), here are the IP/MPLS forwarding tables for prefix\n   192.0.2.11/32\
    \ at Node1, Node4, Node7, and Node10.\n    | Incoming Label or IP Destination\
    \ | Outgoing Label |  Outgoing  |\n    |              16011               |  \
    \   16011      | ECMP{3, 4} |\n    |          192.0.2.11/32           |     16011\
    \      | ECMP{3, 4} |\n                     Table 1: Node1 Forwarding Table\n\
    \    | Incoming Label or IP Destination | Outgoing Label |  Outgoing  |\n    |\
    \              16011               |     16011      | ECMP{7, 8} |\n    |    \
    \      192.0.2.11/32           |     16011      | ECMP{7, 8} |\n             \
    \        Table 2: Node4 Forwarding Table\n     | Incoming Label or IP Destination\
    \ | Outgoing Label |  Outgoing |\n     |              16011               |  \
    \   16011      |     10    |\n     |          192.0.2.11/32           |     16011\
    \      |     10    |\n                      Table 3: Node7 Forwarding Table\n\
    \     | Incoming Label or IP Destination | Outgoing Label |  Outgoing |\n    \
    \ |              16011               |      POP       |     11    |\n     |  \
    \        192.0.2.11/32           |      N/A       |     11    |\n            \
    \          Table 4: Node10 Forwarding Table\n"
- title: 4.2.3.  Network Design Variation
  contents:
  - "4.2.3.  Network Design Variation\n   A network design choice could consist of\
    \ switching all the traffic\n   through Tier-1 and Tier-2 as MPLS traffic.  In\
    \ this case, one could\n   filter away the IP entries at Node4, Node7, and Node10.\
    \  This might\n   be beneficial in order to optimize the forwarding table size.\n\
    \   A network design choice could consist of allowing the hosts to send\n   MPLS-encapsulated\
    \ traffic based on the Egress Peer Engineering (EPE)\n   use case as defined in\
    \ [SR-CENTRAL-EPE].  For example, applications\n   at HostA would send their Z-destined\
    \ traffic to Node1 with an MPLS\n   label stack where the top label is 16011 and\
    \ the next label is an EPE\n   peer segment ([SR-CENTRAL-EPE]) at Node11 directing\
    \ the traffic to Z.\n"
- title: 4.2.4.  Global BGP Prefix Segment through the Fabric
  contents:
  - "4.2.4.  Global BGP Prefix Segment through the Fabric\n   When the previous design\
    \ is deployed, the operator enjoys global BGP\n   Prefix-SID and label allocation\
    \ throughout the DC fabric.\n   A few examples follow:\n   *  Normal forwarding\
    \ to Node11: A packet with top label 16011\n      received by any node in the\
    \ fabric will be forwarded along the\n      ECMP-aware BGP best path towards Node11,\
    \ and the label 16011 is\n      penultimate popped at Node10 (or at Node 9).\n\
    \   *  Traffic-engineered path to Node11: An application on a host behind\n  \
    \    Node1 might want to restrict its traffic to paths via the Spine\n      node\
    \ Node5.  The application achieves this by sending its packets\n      with a label\
    \ stack of {16005, 16011}. BGP Prefix-SID 16005 directs\n      the packet up to\
    \ Node5 along the path (Node1, Node3, Node5).  BGP\n      Prefix-SID 16011 then\
    \ directs the packet down to Node11 along the\n      path (Node5, Node9, Node11).\n"
- title: 4.2.5.  Incremental Deployments
  contents:
  - "4.2.5.  Incremental Deployments\n   The design previously described can be deployed\
    \ incrementally.  Let\n   us assume that Node7 does not support the BGP Prefix-SID,\
    \ and let us\n   show how the fabric connectivity is preserved.\n   From a signaling\
    \ viewpoint, nothing would change; even though Node7\n   does not support the\
    \ BGP Prefix-SID, it does propagate the attribute\n   unmodified to its neighbors.\n\
    \   From a label-allocation viewpoint, the only difference is that Node7\n   would\
    \ allocate a dynamic (random) label to the prefix 192.0.2.11/32\n   (e.g., 123456)\
    \ instead of the \"hinted\" label as instructed by the BGP\n   Prefix-SID.  The\
    \ neighbors of Node7 adapt automatically as they\n   always use the label in the\
    \ BGP8277 NLRI as an outgoing label.\n   Node4 does understand the BGP Prefix-SID;\
    \ therefore, it allocates the\n   indexed label in the SRGB (16011) for 192.0.2.11/32.\n\
    \   As a result, all the data-plane entries across the network would be\n   unchanged\
    \ except the entries at Node7 and its neighbor Node4 as shown\n   in the figures\
    \ below.\n   The key point is that the end-to-end Label Switched Path (LSP) is\n\
    \   preserved because the outgoing label is always derived from the\n   received\
    \ label within the BGP8277 NLRI.  The index in the BGP Prefix-\n   SID is only\
    \ used as a hint on how to allocate the local label (the\n   incoming label) but\
    \ never for the outgoing label.\n     | Incoming Label or IP Destination | Outgoing\
    \ Label |  Outgoing |\n     |              12345               |     16011   \
    \   |     10    |\n                      Table 5: Node7 Forwarding Table\n   \
    \  | Incoming Label or IP Destination | Outgoing Label |  Outgoing |\n     | \
    \             16011               |     12345      |     7     |\n           \
    \           Table 6: Node4 Forwarding Table\n   The BGP Prefix-SID can thus be\
    \ deployed incrementally, i.e., one node\n   at a time.\n   When deployed together\
    \ with a homogeneous SRGB (the same SRGB across\n   the fabric), the operator\
    \ incrementally enjoys the global prefix\n   segment benefits as the deployment\
    \ progresses through the fabric.\n"
- title: 4.3.  IBGP Labeled Unicast (RFC 8277)
  contents:
  - "4.3.  IBGP Labeled Unicast (RFC 8277)\n   The same exact design as EBGP8277 is\
    \ used with the following\n   modifications:\n   *  All nodes use the same AS\
    \ number.\n   *  Each node peers with its neighbors via an internal BGP session\n\
    \      (IBGP) with extensions defined in [RFC8277] (named \"IBGP8277\"\n     \
    \ throughout this document).\n   *  Each node acts as a route reflector for each\
    \ of its neighbors and\n      with the next-hop-self option.  Next-hop-self is\
    \ a well-known\n      operational feature that consists of rewriting the next\
    \ hop of a\n      BGP update prior to sending it to the neighbor.  Usually, it's\
    \ a\n      common practice to apply next-hop-self behavior towards IBGP peers\n\
    \      for EBGP-learned routes.  In the case outlined in this section, it\n  \
    \    is proposed to use the next-hop-self mechanism also to IBGP-\n      learned\
    \ routes.\n                    Cluster-2  |  +-----+  |  Cluster-3\n         \
    \          | Tier-2  | |           | |  Tier-2 |\n                   | |NODE |\
    \ | |  |NODE |  | | |NODE | |\n                   | |  3  | | |  |  6  |  | |\
    \ |  9  | |\n                   | |NODE | | |  |NODE |  | | |NODE | |\n      \
    \             | |  4  | | |  |  7  |  | | |  10 | |\n             Tier-3     \
    \       |  |  8  |  |         Tier-3\n         |NODE | |NODE |       +-----------+\
    \      |NODE | |NODE |\n         |  1  | |  2  |                          | 11\
    \  | |  12 |\n         Figure 3: IBGP Sessions with Reflection and Next-Hop-Self\n\
    \   *  For simple and efficient route propagation filtering and as\n      illustrated\
    \ in Figure 3:\n      -  Node5, Node6, Node7, and Node8 use the same Cluster ID\n\
    \         (Cluster-1).\n      -  Node3 and Node4 use the same Cluster ID (Cluster-2).\n\
    \      -  Node9 and Node10 use the same Cluster ID (Cluster-3).\n   *  The control-plane\
    \ behavior is mostly the same as described in the\n      previous section; the\
    \ only difference is that the EBGP8277 path\n      propagation is simply replaced\
    \ by an IBGP8277 path reflection with\n      next hop changed to self.\n   * \
    \ The data-plane tables are exactly the same.\n"
- title: 5.  Applying Segment Routing in the DC with IPv6 Data Plane
  contents:
  - "5.  Applying Segment Routing in the DC with IPv6 Data Plane\n   The design described\
    \ in [RFC7938] is reused with one single\n   modification.  It is highlighted\
    \ using the example of the\n   reachability to Node11 via Spine node Node5.\n\
    \   Node5 originates 2001:DB8::5/128 with the attached BGP Prefix-SID for\n  \
    \ IPv6 packets destined to segment 2001:DB8::5 ([RFC8402]).\n   Node11 originates\
    \ 2001:DB8::11/128 with the attached BGP Prefix-SID\n   advertising the support\
    \ of the Segment Routing Header (SRH) for IPv6\n   packets destined to segment\
    \ 2001:DB8::11.\n   The control-plane and data-plane processing of all the other\
    \ nodes in\n   the fabric is unchanged.  Specifically, the routes to 2001:DB8::5\
    \ and\n   2001:DB8::11 are installed in the FIB along the EBGP best path to\n\
    \   Node5 (Spine node) and Node11 (ToR node) respectively.\n   An application\
    \ on HostA that needs to send traffic to HostZ via only\n   Node5 (Spine node)\
    \ can do so by sending IPv6 packets with a Segment\n   Routing Header (SRH, [IPv6-SRH]).\
    \  The destination address and active\n   segment is set to 2001:DB8::5.  The\
    \ next and last segment is set to\n   2001:DB8::11.\n   The application must only\
    \ use IPv6 addresses that have been\n   advertised as capable for SRv6 segment\
    \ processing (e.g., for which\n   the BGP Prefix Segment capability has been advertised).\
    \  How\n   applications learn this (e.g., centralized controller and\n   orchestration)\
    \ is outside the scope of this document.\n"
- title: 6.  Communicating Path Information to the Host
  contents:
  - "6.  Communicating Path Information to the Host\n   There are two general methods\
    \ for communicating path information to\n   the end-hosts: \"proactive\" and \"\
    reactive\", aka \"push\" and \"pull\"\n   models.  There are multiple ways to\
    \ implement either of these\n   methods.  Here, it is noted that one way could\
    \ be using a centralized\n   controller: the controller either tells the hosts\
    \ of the prefix-to-\n   path mappings beforehand and updates them as needed (network\
    \ event\n   driven push) or responds to the hosts making requests for a path to\
    \ a\n   specific destination (host event driven pull).  It is also possible\n\
    \   to use a hybrid model, i.e., pushing some state from the controller\n   in\
    \ response to particular network events, while the host pulls other\n   state\
    \ on demand.\n   Note also that when disseminating network-related data to the\
    \ end-\n   hosts, a trade-off is made to balance the amount of information vs.\n\
    \   the level of visibility in the network state.  This applies to both\n   push\
    \ and pull models.  In the extreme case, the host would request\n   path information\
    \ on every flow and keep no local state at all.  On\n   the other end of the spectrum,\
    \ information for every prefix in the\n   network along with available paths could\
    \ be pushed and continuously\n   updated on all hosts.\n"
- title: 7.  Additional Benefits
  contents:
  - '7.  Additional Benefits

    '
- title: 7.1.  MPLS Data Plane with Operational Simplicity
  contents:
  - "7.1.  MPLS Data Plane with Operational Simplicity\n   As required by [RFC7938],\
    \ no new signaling protocol is introduced.\n   The BGP Prefix-SID is a lightweight\
    \ extension to BGP Labeled Unicast\n   [RFC8277].  It applies either to EBGP-\
    \ or IBGP-based designs.\n   Specifically, LDP and RSVP-TE are not used.  These\
    \ protocols would\n   drastically impact the operational complexity of the data\
    \ center and\n   would not scale.  This is in line with the requirements expressed\
    \ in\n   [RFC7938].\n   Provided the same SRGB is configured on all nodes, all\
    \ nodes use the\n   same MPLS label for a given IP prefix.  This is simpler from\
    \ an\n   operation standpoint, as discussed in Section 8.\n"
- title: 7.2.  Minimizing the FIB Table
  contents:
  - "7.2.  Minimizing the FIB Table\n   The designer may decide to switch all the\
    \ traffic at Tier-1 and\n   Tier-2 based on MPLS, thereby drastically decreasing\
    \ the IP table\n   size at these nodes.\n   This is easily accomplished by encapsulating\
    \ the traffic either\n   directly at the host or at the source ToR node.  The\
    \ encapsulation is\n   done by pushing the BGP Prefix-SID of the destination ToR\
    \ for intra-\n   DC traffic, or by pushing the BGP Prefix-SID for the border node\
    \ for\n   inter-DC or DC-to-outside-world traffic.\n"
- title: 7.3.  Egress Peer Engineering
  contents:
  - "7.3.  Egress Peer Engineering\n   It is straightforward to combine the design\
    \ illustrated in this\n   document with the Egress Peer Engineering (EPE) use\
    \ case described in\n   [SR-CENTRAL-EPE].\n   In such a case, the operator is\
    \ able to engineer its outbound traffic\n   on a per-host-flow basis, without\
    \ incurring any additional state at\n   intermediate points in the DC fabric.\n\
    \   For example, the controller only needs to inject a per-flow state on\n   the\
    \ HostA to force it to send its traffic destined to a specific\n   Internet destination\
    \ D via a selected border node (say Node12 in\n   Figure 1 instead of another\
    \ border node, Node11) and a specific\n   egress peer of Node12 (say peer AS 9999\
    \ of local PeerNode segment\n   9999 at Node12 instead of any other peer that\
    \ provides a path to the\n   destination D).  Any packet matching this state at\
    \ HostA would be\n   encapsulated with SR segment list (label stack) {16012, 9999}.\
    \  16012\n   would steer the flow through the DC fabric, leveraging any ECMP,\n\
    \   along the best path to border node Node12.  Once the flow gets to\n   border\
    \ node Node12, the active segment is 9999 (because of\n   Penultimate Hop Popping\
    \ (PHP) on the upstream neighbor of Node12).\n   This EPE PeerNode segment forces\
    \ border node Node12 to forward the\n   packet to peer AS 9999 without any IP\
    \ lookup at the border node.\n   There is no per-flow state for this engineered\
    \ flow in the DC fabric.\n   A benefit of SR is that the per-flow state is only\
    \ required at the\n   source.\n   As well as allowing full traffic-engineering\
    \ control, such a design\n   also offers FIB table-minimization benefits as the\
    \ Internet-scale FIB\n   at border node Node12 is not required if all FIB lookups\
    \ are avoided\n   there by using EPE.\n"
- title: 7.4.  Anycast
  contents:
  - "7.4.  Anycast\n   The design presented in this document preserves the availability\
    \ and\n   load-balancing properties of the base design presented in [RFC8402].\n\
    \   For example, one could assign an anycast loopback 192.0.2.20/32 and\n   associate\
    \ segment index 20 to it on the border nodes Node11 and\n   Node12 (in addition\
    \ to their node-specific loopbacks).  Doing so, the\n   EPE controller could express\
    \ a default \"go-to-the-Internet via any\n   border node\" policy as segment list\
    \ {16020}. Indeed, from any host in\n   the DC fabric or from any ToR node, 16020\
    \ steers the packet towards\n   the border nodes Node11 or Node12 leveraging ECMP\
    \ where available\n   along the best paths to these nodes.\n"
- title: 8.  Preferred SRGB Allocation
  contents:
  - "8.  Preferred SRGB Allocation\n   In the MPLS case, it is recommended to use\
    \ the same SRGBs at each\n   node.\n   Different SRGBs in each node likely increase\
    \ the complexity of the\n   solution both from an operational viewpoint and from\
    \ a controller\n   viewpoint.\n   From an operational viewpoint, it is much simpler\
    \ to have the same\n   global label at every node for the same destination (the\
    \ MPLS\n   troubleshooting is then similar to the IPv6 troubleshooting where\n\
    \   this global property is a given).\n   From a controller viewpoint, this allows\
    \ us to construct simple\n   policies applicable across the fabric.\n   Let us\
    \ consider two applications, A and B, respectively connected to\n   Node1 and\
    \ Node2 (ToR nodes).  Application A has two flows, FA1 and\n   FA2, destined to\
    \ Z.  B has two flows, FB1 and FB2, destined to Z.\n   The controller wants FA1\
    \ and FB1 to be load shared across the fabric\n   while FA2 and FB2 must be respectively\
    \ steered via Node5 and Node8.\n   Assuming a consistent unique SRGB across the\
    \ fabric as described in\n   this document, the controller can simply do it by\
    \ instructing A and B\n   to use {16011} respectively for FA1 and FB1 and by instructing\
    \ A and\n   B to use {16005 16011} and {16008 16011} respectively for FA2 and\n\
    \   FB2.\n   Let us assume a design where the SRGB is different at every node\
    \ and\n   where the SRGB of each node is advertised using the Originator SRGB\n\
    \   TLV of the BGP Prefix-SID as defined in [RFC8669]: SRGB of Node K\n   starts\
    \ at value K*1000, and the SRGB length is 1000 (e.g., Node1's\n   SRGB is [1000,\
    \ 1999], Node2's SRGB is [2000, 2999], ...).\n   In this case, the controller\
    \ would need to collect and store all of\n   these different SRGBs (e.g., through\
    \ the Originator SRGB TLV of the\n   BGP Prefix-SID); furthermore, it would also\
    \ need to adapt the policy\n   for each host.  Indeed, the controller would instruct\
    \ A to use {1011}\n   for FA1 while it would have to instruct B to use {2011}\
    \ for FB1\n   (while with the same SRGB, both policies are the same {16011}).\n\
    \   Even worse, the controller would instruct A to use {1005, 5011} for\n   FA1\
    \ while it would instruct B to use {2011, 8011} for FB1 (while with\n   the same\
    \ SRGB, the second segment is the same across both policies:\n   16011).  When\
    \ combining segments to create a policy, one needs to\n   carefully update the\
    \ label of each segment.  This is obviously more\n   error prone, more complex,\
    \ and more difficult to troubleshoot.\n"
- title: 9.  IANA Considerations
  contents:
  - "9.  IANA Considerations\n   This document has no IANA actions.\n"
- title: 10.  Manageability Considerations
  contents:
  - "10.  Manageability Considerations\n   The design and deployment guidelines described\
    \ in this document are\n   based on the network design described in [RFC7938].\n\
    \   The deployment model assumed in this document is based on a single\n   domain\
    \ where the interconnected DCs are part of the same\n   administrative domain\
    \ (which, of course, is split into different\n   autonomous systems).  The operator\
    \ has full control of the whole\n   domain, and the usual operational and management\
    \ mechanisms and\n   procedures are used in order to prevent any information related\
    \ to\n   internal prefixes and topology to be leaked outside the domain.\n   As\
    \ recommended in [RFC8402], the same SRGB should be allocated in all\n   nodes\
    \ in order to facilitate the design, deployment, and operations\n   of the domain.\n\
    \   When EPE ([SR-CENTRAL-EPE]) is used (as explained in Section 7.3),\n   the\
    \ same operational model is assumed.  EPE information is originated\n   and propagated\
    \ throughout the domain towards an internal server, and\n   unless explicitly\
    \ configured by the operator, no EPE information is\n   leaked outside the domain\
    \ boundaries.\n"
- title: 11.  Security Considerations
  contents:
  - "11.  Security Considerations\n   This document proposes to apply SR to a well-known\
    \ scalability\n   requirement expressed in [RFC7938] using the BGP Prefix-SID\
    \ as\n   defined in [RFC8669].\n   It has to be noted, as described in Section\
    \ 10, that the design\n   illustrated in [RFC7938] and in this document refer\
    \ to a deployment\n   model where all nodes are under the same administration.\
    \  In this\n   context, it is assumed that the operator doesn't want to leak outside\n\
    \   of the domain any information related to internal prefixes and\n   topology.\
    \  The internal information includes Prefix-SID and EPE\n   information.  In order\
    \ to prevent such leaking, the standard BGP\n   mechanisms (filters) are applied\
    \ on the boundary of the domain.\n   Therefore, the solution proposed in this\
    \ document does not introduce\n   any additional security concerns from what is\
    \ expressed in [RFC7938]\n   and [RFC8669].  It is assumed that the security and\
    \ confidentiality\n   of the prefix and topology information is preserved by outbound\n\
    \   filters at each peering point of the domain as described in\n   Section 10.\n"
- title: 12.  References
  contents:
  - '12.  References

    '
- title: 12.1.  Normative References
  contents:
  - "12.1.  Normative References\n   [RFC4271]  Rekhter, Y., Ed., Li, T., Ed., and\
    \ S. Hares, Ed., \"A\n              Border Gateway Protocol 4 (BGP-4)\", RFC 4271,\n\
    \              DOI 10.17487/RFC4271, January 2006,\n              <https://www.rfc-editor.org/info/rfc4271>.\n\
    \   [RFC7938]  Lapukhov, P., Premji, A., and J. Mitchell, Ed., \"Use of\n    \
    \          BGP for Routing in Large-Scale Data Centers\", RFC 7938,\n        \
    \      DOI 10.17487/RFC7938, August 2016,\n              <https://www.rfc-editor.org/info/rfc7938>.\n\
    \   [RFC8277]  Rosen, E., \"Using BGP to Bind MPLS Labels to Address\n       \
    \       Prefixes\", RFC 8277, DOI 10.17487/RFC8277, October 2017,\n          \
    \    <https://www.rfc-editor.org/info/rfc8277>.\n   [RFC8402]  Filsfils, C., Ed.,\
    \ Previdi, S., Ed., Ginsberg, L.,\n              Decraene, B., Litkowski, S.,\
    \ and R. Shakir, \"Segment\n              Routing Architecture\", RFC 8402, DOI\
    \ 10.17487/RFC8402,\n              July 2018, <https://www.rfc-editor.org/info/rfc8402>.\n\
    \   [RFC8669]  Previdi, S., Filsfils, C., Lindem, A., Ed., Sreekantiah,\n    \
    \          A., and H. Gredler, \"Segment Routing Prefix Segment\n            \
    \  Identifier Extensions for BGP\", RFC 8669,\n              DOI 10.17487/RFC8669,\
    \ December 2019,\n              <https://www.rfc-editor.org/info/rfc8669>.\n"
- title: 12.2.  Informative References
  contents:
  - "12.2.  Informative References\n   [IPv6-SRH] Filsfils, C., Dukes, D., Previdi,\
    \ S., Leddy, J.,\n              Matsushima, S., and D. Voyer, \"IPv6 Segment Routing\
    \ Header\n              (SRH)\", Work in Progress, Internet-Draft, draft-ietf-6man-\n\
    \              segment-routing-header-26, 22 October 2019,\n              <https://tools.ietf.org/html/draft-ietf-6man-segment-\n\
    \              routing-header-26>.\n   [RFC6793]  Vohra, Q. and E. Chen, \"BGP\
    \ Support for Four-Octet\n              Autonomous System (AS) Number Space\"\
    , RFC 6793,\n              DOI 10.17487/RFC6793, December 2012,\n            \
    \  <https://www.rfc-editor.org/info/rfc6793>.\n   [SR-CENTRAL-EPE]\n         \
    \     Filsfils, C., Previdi, S., Dawra, G., Aries, E., and D.\n              Afanasiev,\
    \ \"Segment Routing Centralized BGP Egress Peer\n              Engineering\",\
    \ Work in Progress, Internet-Draft, draft-\n              ietf-spring-segment-routing-central-epe-10,\
    \ 21 December\n              2017, <https://tools.ietf.org/html/draft-ietf-spring-\n\
    \              segment-routing-central-epe-10>.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   The authors would like to thank Benjamin Black, Arjun Sreekantiah,\n\
    \   Keyur Patel, Acee Lindem, and Anoop Ghanwani for their comments and\n   review\
    \ of this document.\n"
- title: Contributors
  contents:
  - "Contributors\n   Gaya Nagarajan\n   Facebook\n   United States of America\n \
    \  Email: gaya@fb.com\n   Gaurav Dawra\n   Cisco Systems\n   United States of\
    \ America\n   Email: gdawra.ietf@gmail.com\n   Dmitry Afanasiev\n   Yandex\n \
    \  Russian Federation\n   Email: fl0w@yandex-team.ru\n   Tim Laberge\n   Cisco\n\
    \   United States of America\n   Email: tlaberge@cisco.com\n   Edet Nkposong\n\
    \   Salesforce.com Inc.\n   United States of America\n   Email: enkposong@salesforce.com\n\
    \   Mohan Nanduri\n   Microsoft\n   United States of America\n   Email: mohan.nanduri@oracle.com\n\
    \   James Uttaro\n   ATT\n   United States of America\n   Email: ju1738@att.com\n\
    \   Saikat Ray\n   Unaffiliated\n   United States of America\n   Email: raysaikat@gmail.com\n\
    \   Jon Mitchell\n   Unaffiliated\n   United States of America\n   Email: jrmitche@puck.nether.net\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Clarence Filsfils (editor)\n   Cisco Systems, Inc.\n \
    \  Brussels\n   Belgium\n   Email: cfilsfil@cisco.com\n   Stefano Previdi\n  \
    \ Cisco Systems, Inc.\n   Italy\n   Email: stefano@previdi.net\n   Gaurav Dawra\n\
    \   LinkedIn\n   United States of America\n   Email: gdawra.ietf@gmail.com\n \
    \  Ebben Aries\n   Arrcus, Inc.\n   2077 Gateway Place, Suite #400\n   San Jose,\
    \  CA 95119\n   United States of America\n   Email: exa@arrcus.com\n   Petr Lapukhov\n\
    \   Facebook\n"
