Abstract An increase in the permissible initial window size of a TCP connection, from one segment to three or four segments, has been under discussion in the tcp impl working group.
This document covers some simulation studies of the effects of increasing the initial window size of TCP.
Both long lived TCP connections (file transfers) and short lived web browsing style connections were modeled.
The simulations were performed using the publicly available ns 2 simulator and our custom models and files are also available.
We present results from a set of simulations with increased TCP initial window (IW).
The main objectives were to explore the conditions under which the larger IW was a "win" and to determine the effects, if any, the larger IW might have on other traffic flows using an IW of one segment.
This study was inspired by discussions at the Munich IETF tcp impl and tcp sat meetings.
A proposal to increase the IW size to about 4K bytes (4380 bytes in the case of 1460 byte segments) was discussed.
Concerns about both the utility of the increase and its effect on other traffic were raised.
Some studies were presented showing the positive effects of increased IW on individual connections, but no studies were shown with a wide variety of simultaneous traffic flows.
It appeared that some of the questions being raised could be addressed in an ns 2 simulation.
Early results from our simulations were previously posted to the tcp impl mailing list and presented at the tcp impl WG meeting at the December 1997 IETF.
We simulated a network topology with a bottleneck link as shown:
These clients are served by the FTP and Web servers attached to the nodes (n6 n9) on the right hand side.
The links to and from those nodes are at 10 Mbps.
The bottleneck link is between n1 and n0.
All links are bi directional, but only ACKs, SYNs, FINs, and URLs are flowing from left to right.
Some simulations were also performed with data traffic flowing from right to left simultaneously, but it had no effect on the results.
In the simulations we assumed that all ftps transferred 1 MB files and that all web pages had exactly three embedded URLs.
The web clients are browsing quite aggressively, requesting a new page after a random delay uniformly distributed between 1 and 5 seconds.
This is not meant to realistically model a single user's web browsing pattern, but to create a reasonably heavy traffic load whose individual tcp connections accurately reflect real web traffic.
Some discussion of these models as used in earlier studies is available in references [3] and [4].
The maximum tcp window was set to 11 packets, maximum packet (or segment) size to 1460 bytes, and buffer sizes were set at 25 packets.
(The ns 2 TCPs require setting window sizes and buffer sizes in number of packets.
In our tcp full code some of the internal parameters have been set to be byte oriented, but external values must still be set in number of packets.)
In our simulations, we varied the number of data segments sent into a new TCP connection (or initial window) from one to four, keeping all segments at 1460 bytes.
A dropped packet causes a restart window of one segment to be used, just as in current practice.
For ns 2 users: The tcp full code was modified to use an "application" class and three application client server pairs were written: a simple file transfer (ftp), a model of http1.0 style web connection and a very rough model of http1.1 style web connection.
The required files and scripts for these simulations are available under the contributed code section on the ns simulator web page at the sites ftp://ftp.ee.lbl.gov/IW.{tar, tar.
Simulations were run with 8, 16, 32 web clients and a number of ftp clients ranging from 0 to 3.
The IW was varied from 1 to 4, though the 4 packet case lies beyond what is currently recommended.
The figures of merit used were goodput, the median page delay seen by the web clients and the median file transfer delay seen by the ftp clients.
The simulated run time was rather large, 360 seconds, to ensure an adequate sample.
(Median values remained the same for simulations with larger run times and can be considered stable) 3.
In our simulations, we varied the number of file transfer clients in order to change the congestion of the link.
Recall that our ftp clients continuously request 1 Mbyte transfers, so the link utilization is over 90% when even a single ftp client is present.
When three file transfer clients are running simultaneously, the resultant congestion is somewhat pathological, making the values recorded stable.
Though all connections use the same initial window, the effect of increasing the IW on a 1 Mbyte file transfer is not detectable, thus we focus on the web browsing connections.
(In the tables, we use "webs" to indicate number of web clients and "ftps" to indicate the number of file transfer clients attached.)
Table 1 shows the median delays experienced by the web transfers with an increase in the TCP IW.
There is clearly an improvement in transfer delays for the web connections with increase in the IW, in many cases on the order of 30%.
The steepness of the performance improvement going from an IW of 1 to an IW of 2 is mainly due to the distribution of files fetched by each URL (see references [1] and [2]); the median size of both primary and in line URLs fits completely into two packets.
If file distributions change, the shape of this curve may also change.
Median web page delay #Webs
IW 1    IW 2    IW 3    IW 4
Packet drop rates did increase with IW, but in all cases except that of the single most pathological overload, the increase in drop percentage was less than 1%.
A decrease in packet drop percentage is observed in some overloaded situations, specifically when ftp transfers consumed most of the link bandwidth and a large number of web transfers
shared the remaining bandwidth of the link.
In this case, the web transfers experience severe packet loss and some of the IW 4 web clients suffer multiple packet losses from the same window, resulting in longer recovery times than when there is a single packet loss in a window.
During the recovery time, the connections are inactive which alleviates congestion and thus results in a decrease in the packet drop percentage.
It should be noted that such observations were made only in extremely overloaded scenarios.
Link utilization and packet drop rates
To get a more complete picture of performance, we computed the network power, goodput divided by median delay (in Mbytes/ms), and plotted it against IW for all scenarios.
(Each scenario is uniquely identified by its number of webs and number of file transfers.)
We plot these values in Figure 1 (in the pdf version), illustrating a general advantage to increasing IW.
When a large number of web clients is combined with ftps, particularly multiple ftps, pathological cases result from the extreme congestion.
In these cases, there appears to be no particular trend to the results of increasing the IW, in fact simulation results are not particularly stable.
To get a clearer picture of what is happening across all the tested scenarios, we normalized the network power values for the non  pathological scenario by the network power for that scenario at IW of one.
These results are plotted in Figure 2.
As IW is increased from one to four, network power increased by at least 15%, even in a congested scenario dominated by bulk transfer traffic.
In simulations where web traffic has a dominant share of the available bandwidth, the increase in network power was up to 60%.
The increase in network power at higher initial window sizes is due to an increase in throughput and a decrease in the delay.
Since the (slightly) increased drop rates were accompanied by better performance, drop rate is clearly not an indicator of user level performance.
The gains in performance seen by the web clients need to be balanced against the performance the file transfers are seeing.
We computed ftp network power and show this in Table 3.
It appears that the improvement in network power seen by the web connections has negligible effect on the concurrent file transfers.
It can be observed from the table that there is a small variation in the network power of file transfers with an increase in the size of IW but no particular trend can be seen.
It can be concluded that the network power of file transfers essentially remained the same.
However, it should be noted that a larger IW does allow web transfers to gain slightly more bandwidth than with a smaller IW.
This could mean fewer bytes transferred for FTP applications or a slight decrease in network power as computed by us.
Network power of file transfers with an increase in the TCP IW size #Webs
IW 1    IW 2    IW 3    IW 4   8
The above simulations all used http1.0 style web connections, thus, a natural question is to ask how results are affected by migration to http1.1.
A rough model of this behavior was simulated by using one connection to send all of the information from both the primary URL and the three embedded, or in line, URLs.
Since the transfer size is now made up of four web files, the steep improvement in performance between an IW of 1 and an IW of two, noted in the previous results, has been smoothed.
Results are shown in Tables 4 & 5 and Figs.
Occasionally an increase in IW from 3 to 4 decreases the network power owing to a non increase or a slight decrease in the throughput.
TCP connections opening up with a higher window size into a very congested network might experience some packet drops and consequently a slight decrease in the throughput.
This indicates that increase of the initial window sizes to further higher values (>4) may not always result in a favorable network performance.
This can be seen clearly in Figure 4 where the network power shows a decrease for the two highly congested cases.
Median web page delay for http1.1 #Webs
IW 1    IW 2    IW 3    IW 4
Network power of file transfers with an increase in the TCP IW size #Webs
For further insight, we returned to the http1.0 model and mixed some web browsing connections with IWs of one with those using IWs of three.
In this experiment, we first simulated a total of 16 web  browsing connections, all using IW of one.
Then the clients were split into two groups of 8 each, one of which uses IW 1 and the other used IW 3.
We repeated the simulations for a total of 32 and 64 web browsing clients, splitting those into groups of 16 and 32 respectively.
Table 6 shows these results.
We report the goodput (in Mbytes), the web page delays (in milli seconds), the percent utilization of the link and the percent of packets dropped.
Unsurprisingly, the non split experiments are consistent with our earlier results, clients with IW 3 outperform clients with IW 1.
The results of running the 8/8 and 16/16 splits show that running a mixture of IW 3 and IW 1 has no negative effect on the IW 1 conversations, while IW 3 conversations maintain their performance.
However, the 32/32 split shows that web browsing connections with IW 3 are adversely affected.
We believe this is due to the pathological dynamics of this extremely congested scenario.
Since embedded URLs open their connections simultaneously, very large number of TCP connections are arriving at the bottleneck link resulting in multiple packet losses for the IW 3 conversations.
The myriad problems of this simultaneous opening strategy is, of course, part of the motivation for the development of http1.1. 4.
The indications from these results are that increasing the initial window size to 3 packets (or 4380 bytes) helps to improve perceived performance.
Many further variations on these simulation scenarios are possible and we've made our simulation models and scripts available in order to facilitate others' experiments.
We also used the RED queue management included with ns 2 to perform some other simulation studies.
We have not reported on those results here since we don't consider the studies complete.
We found that by adding RED to the bottleneck link, we achieved similar performance gains (with an IW of 1) to those we found with increased IWs without RED.
Others may wish to investigate this further.
Although the simulation sets were run for a T1 link, several scenarios with varying levels of congestion and varying number of web and ftp clients were analyzed.
It is reasonable to expect that the results would scale for links with higher bandwidth.
However, interested readers could investigate this aspect further.
We also used the RED queue management included with ns 2 to perform some other simulation studies.
We have not reported on those results here since we don't consider the studies complete.
We found that by adding RED to the bottleneck link, we achieved similar performance gains (with an IW of 1) to those we found with increased IWs without RED.
Others may wish to investigate this further.
This document discusses a simulation study of the effects of a proposed change to TCP.
Consequently, there are no security considerations directly related to the document.
There are also no known security considerations associated with the proposed change.
