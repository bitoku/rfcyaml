- title: __initial_text__
  contents:
  - "       PERFORMANCE IMPROVEMENT IN ARPANET FILE TRANSFERS FROM MULTICS\nWith the\
    \ growth of Multics usage in the ARPA Network community, users often \nneed to\
    \ transfer large amounts of data from Multics to other systems. However,\nMultics\
    \ performance in this area has been less than optimal and there clearly\nexists\
    \ a need to improve the situation. Even within Project MAC there are users\nwho\
    \ regularly ship files back and forth between Multics and other Project MAC\n\
    computer systems. Recently the Cambridge Information Systems Laboratory (CISL)\n\
    development Multics system was connected to the ARPA Network. This has\ngenerated\
    \ considerable interest among the members of the CSR (Computer Systems\nResearch)\
    \ and CISL groups in using the Multics file transfer facility for tran-\nsmission\
    \ of Multics system tapes (MST) from one Multics system to the other.\nAt currently\
    \ realizable data transfer rates, transmission of a typical MST,\n(approximately\
    \ 12 million bits), takes about half an hour. The usefulness of\nthe present file\
    \ transfer system is severely limited by its low bandwidth.\nOne of the reasons\
    \ for such a poor performance is the output buffering strategy\nin the Multics\
    \ IMP DIM (IMP Device Interface Manager.) With the hope of making\na significant\
    \ performance improvement, we recently changed the IMP DIM buffer-\ning strategy.\
    \ To assess the effects of this change an experiment was conducted\nbetween the\
    \ service machine (MIT Multics) and the development machine (CISL\nMultics.) This\
    \ memo reports the experiment and its results.\nPROBLEM\nDue to reasons that are\
    \ of historical interest only, the output buffers in the\nIMP DIM have been very\
    \ small; each buffer can accommodate up to 940 bits only.\nWith the addition of\
    \ a 72 bit long header, the resulting message has a maximum\nlength of 1012 bits,\
    \ which in the Network terminology is a single packet\nmessage. Due to this buffer\
    \ size limit, Multics can transmit single packet\nmessages only, even though the\
    \ network can accept up to 8096 bit long messages.\nThis results in increased\
    \ overhead in the set up time for transmission of large\nnumber of bits.\nAn old\
    \ version of the IMP-to-Host protocol requires that a host may not transmit\n\
    another message on a network connection unless a Request-for-Next-Message (RFNM)\n\
    has been received in response to the previous message. Even though this\nrestriction\
    \ has now been relaxed, the protocol does not specify any way to\nrecover from\
    \ transmission errors that occur while more than one RFNM is pending\non the same\
    \ connection. If the constraint of transmitting only one message at a\ntime is\
    \ observed there is at least some potential for recovery in case of an\nerror.\
    \ 8eing reliability conscious, Multics observes the constraint imposed by\nthe\
    \ old protocol. Following the old protocol introduces delays in the\ntransmission\
    \ of successive messages on the same link and therefore the overall\nbandwidth\
    \ per connection is reduced.\nSOLUTION\nAt least one method of improving the file\
    \ transfer bandwidth is obvious.\nIncreasing the size of IMP DIM output buffers\
    \ will allow us to transmit\nsignificantly larger messages<s1>s This will result\
    \ in fewer RFNMs and delays and\nimproved bandwidth. We have made two assumptions\
    \ here. The first assumption is\nthat the delay introduced by RFNMs does not increase\
    \ with the size of the\nmessage.<s2>s Our experience with the network tends to\
    \ support this assumption.\nThe second assumption is that actual time taken to\
    \ transmit a message increases,\nat best, linearly with the size of message. This\
    \ second assumption does not\nhold for a heavily loaded network. But for our purpose\
    \ we may assume it to be\nessentially correct.\nThere is another advantage in\
    \ transmitting large messages. For a given amount\nof data, fewer messages will\
    \ be transmitted, fewer RFNMs will be read, and\ntherefore time spent in the channel\
    \ driver programs will be reduced. Since the\nchannel sits idle during at least\
    \ some of this time, the idle time for the\nchannel will be reduced, resulting\
    \ in improved channel bandwidth.\nEXPERIMENT\nWe changed the IMP DIM to implement\
    \ two kinds of output buffers. One kind of\noutput buffer is short and can hold\
    \ single packet messages only. The other kind\nof buffer is, naturally enough,\
    \ large and can hold the largest message allowed\nby the network. If the number\
    \ of bits to be transmitted is low (this is mostly\nthe situation with interactive\
    \ users,) a small buffer is chosen and if it is\nlarge (for example, file transfers,)\
    \ a large buffer is chosen.\nThe new IMP DIM was used in an experimental Multics\
    \ system on the development\nmachine at CISL. The service machine at MIT had the\
    \ old version. Large files\nwere transmitted from the development system to the\
    \ service system and the file\ntransfer rate was measured in bits per second (\
    \ of real time.) To get an\nestimate of gain in the performance, files were transmitted\
    \ from the service\n<s1>s In one situation it may not be possible to transmit\
    \ large messages. The\nnumber of messages and bits that a host can transmit on\
    \ a particular connection\nmust not exceed the message and bit allocation provided\
    \ by the receiving host.\nIf a receiving host gives very small bit allocation,\
    \ then the sending host can\nnot transmit very large messages. Since Multics always\
    \ gives out very large\nallocations, there should be no problem in Multics to\
    \ Multics file transfers.\n<s2>s It should be noted that the size of RFNM is fixed\
    \ and does not change \nwith the size of message.\nsystem to the development system\
    \ and the old file transfer rate was measured.\nThe same file, approximately 2.5\
    \ million bits long, was used in both\nexperiments. The BYTE-SIZE and MODE parameters\
    \ of the ARPANET File Transfer\nprotocol were set to 36 and \"image\" mode respectively.\
    \ This implies that no\ncharacter conversion was performed in the file transfer.\
    \ The following table\nshows the results obtained:\nService to Development Development\
    \ to Service\n(old system) (new system)\naverage file-\ntransfer rate 6,837 27,578\n\
    (bits per second)\nThe following information regarding the environment of the\
    \ experiment is\nsupplied for the sake of completeness. At the time of this experiment,\
    \ the\nservice system was lightly loaded (the system load varied between 30.0\
    \ and\n35.0). The service system configuration was 2 cpu's and 384 K primary memory.\n\
    The development machine configuration was 1 cpu and 256 K of primary memory.\n\
    The development system load was limited to the file transfer processes and the\n\
    initializer process. The MIT Multics is connected to the IMP number 44 (port 0)\n\
    by a rather short cable (approximately 100 feet long.) The CISL Multics is\nconnected\
    \ to the IMP number 6 (port 0) by an approximately l5OO feet long cable.\n8oth\
    \ IMPs are in close physical proximity (approximately 2000 feet,) and are\nconnected\
    \ to each other by a 5O kilobits per second line. The results given\nabove show\
    \ considerable improvement in the performance with the new IMP DIM.\nSince there\
    \ is considerable interest in the Multics development community in\nusing the\
    \ file transfer facility for transmitting Multics System Tapes between\nthe two\
    \ systems, we are providing here an estimate of time that would be taken\nto transmit\
    \ the current MST. MST 23.4-0A contains 334,231 words. When\nmultiplied by 36\
    \ (the word length in bits) this yields the total number of bits\nto be approximately\
    \ 12.5 million. Assuming a file transfer rate of 27,500 bits\nper second, it will\
    \ take approximately 7 minutes and 30 seconds to transmit the\nsystem 23.4-0A.\n\
    In the experiment outlined above, there was only one file being transferred at\n\
    any given tine between the two systems. We conducted another experiment to get\n\
    an estimate of performance in the situation of multiple file transfers occurring\n\
    simultaneously. This experiment consisted of first two, and then three,\nsimultaneous\
    \ file-transfers from the development system to the service system.\nThese file\
    \ transfers were started by different processes logged in from separate\nconsoles.\
    \ Because these file-transfers were started manually, we could not\nobtain perfect\
    \ simultaneity and results obtained for the total bandwidth are\nessentially approximate.\
    \ For two simultaneous file transfers the total bit rate\nwas approximately 40,000\
    \ bits per second and bit rate for each file transfer was\napproximately 20,000\
    \ bits per second. For three simultaneous file transfers,\ntotal bit rate remained\
    \ at approximately 40,000 bits per second and bit rate for\nindividual transfers\
    \ was approximately 13,500 bits per second.\n"
