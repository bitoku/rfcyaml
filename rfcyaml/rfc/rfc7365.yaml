- title: __initial_text__
  contents:
  - '         Framework for Data Center (DC) Network Virtualization

    '
- title: Abstract
  contents:
  - "Abstract\n   This document provides a framework for Data Center (DC) Network\n\
    \   Virtualization over Layer 3 (NVO3) and defines a reference model\n   along\
    \ with logical components required to design a solution.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc7365.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2014 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................4\n\
    \      1.1. General Terminology ........................................4\n  \
    \    1.2. DC Network Architecture ....................................7\n   2.\
    \ Reference Models ................................................8\n      2.1.\
    \ Generic Reference Model ....................................8\n      2.2. NVE\
    \ Reference Model .......................................10\n      2.3. NVE Service\
    \ Types .........................................11\n           2.3.1. L2 NVE\
    \ Providing Ethernet LAN-Like Service .........11\n           2.3.2. L3 NVE Providing\
    \ IP/VRF-Like Service ...............11\n      2.4. Operational Management Considerations\
    \ .....................12\n   3. Functional Components ..........................................12\n\
    \      3.1. Service Virtualization Components .........................12\n  \
    \         3.1.1. Virtual Access Points (VAPs) .......................12\n    \
    \       3.1.2. Virtual Network Instance (VNI) .....................12\n      \
    \     3.1.3. Overlay Modules and VN Context .....................14\n        \
    \   3.1.4. Tunnel Overlays and Encapsulation Options ..........14\n          \
    \ 3.1.5. Control-Plane Components ...........................14\n            \
    \      3.1.5.1. Distributed vs. Centralized\n                           Control\
    \ Plane .............................14\n                  3.1.5.2. Auto-provisioning\
    \ and Service Discovery ...15\n                  3.1.5.3. Address Advertisement\
    \ and Tunnel Mapping ..15\n                  3.1.5.4. Overlay Tunneling .........................16\n\
    \      3.2. Multihoming ...............................................16\n  \
    \    3.3. VM Mobility ...............................................17\n   4.\
    \ Key Aspects of Overlay Networks ................................17\n      4.1.\
    \ Pros and Cons .............................................18\n      4.2. Overlay\
    \ Issues to Consider ................................19\n           4.2.1. Data\
    \ Plane vs. Control Plane Driven ................19\n           4.2.2. Coordination\
    \ between Data Plane and Control Plane ..19\n           4.2.3. Handling Broadcast,\
    \ Unknown Unicast, and\n                  Multicast (BUM) Traffic ............................20\n\
    \           4.2.4. Path MTU ...........................................20\n  \
    \         4.2.5. NVE Location Trade-Offs ............................21\n    \
    \       4.2.6. Interaction between Network Overlays and\n                  Underlays\
    \ ..........................................22\n   5. Security Considerations\
    \ ........................................22\n   6. Informative References .........................................24\n\
    \   Acknowledgments ...................................................26\n  \
    \ Authors' Addresses ................................................26\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document provides a framework for Data Center (DC)\
    \ Network\n   Virtualization over Layer 3 (NVO3) tunnels.  This framework is\n\
    \   intended to aid in standardizing protocols and mechanisms to support\n   large-scale\
    \ network virtualization for data centers.\n   [RFC7364] defines the rationale\
    \ for using overlay networks in order\n   to build large multi-tenant data center\
    \ networks.  Compute, storage\n   and network virtualization are often used in\
    \ these large data centers\n   to support a large number of communication domains\
    \ and end systems.\n   This document provides reference models and functional\
    \ components of\n   data center overlay networks as well as a discussion of technical\n\
    \   issues that have to be addressed.\n"
- title: 1.1.  General Terminology
  contents:
  - "1.1.  General Terminology\n   This document uses the following terminology:\n\
    \   NVO3 Network: An overlay network that provides a Layer 2 (L2) or\n   Layer\
    \ 3 (L3) service to Tenant Systems over an L3 underlay network\n   using the architecture\
    \ and protocols as defined by the NVO3 Working\n   Group.\n   Network Virtualization\
    \ Edge (NVE): An NVE is the network entity that\n   sits at the edge of an underlay\
    \ network and implements L2 and/or L3\n   network virtualization functions.  The\
    \ network-facing side of the NVE\n   uses the underlying L3 network to tunnel\
    \ tenant frames to and from\n   other NVEs.  The tenant-facing side of the NVE\
    \ sends and receives\n   Ethernet frames to and from individual Tenant Systems.\
    \  An NVE could\n   be implemented as part of a virtual switch within a hypervisor,\
    \ a\n   physical switch or router, or a Network Service Appliance, or it\n   could\
    \ be split across multiple devices.\n   Virtual Network (VN): A VN is a logical\
    \ abstraction of a physical\n   network that provides L2 or L3 network services\
    \ to a set of Tenant\n   Systems.  A VN is also known as a Closed User Group (CUG).\n\
    \   Virtual Network Instance (VNI): A specific instance of a VN from the\n   perspective\
    \ of an NVE.\n   Virtual Network Context (VN Context) Identifier: Field in an\
    \ overlay\n   encapsulation header that identifies the specific VN the packet\n\
    \   belongs to.  The egress NVE uses the VN Context identifier to deliver\n  \
    \ the packet to the correct Tenant System.  The VN Context identifier\n   can\
    \ be a locally significant identifier or a globally unique\n   identifier.\n \
    \  Underlay or Underlying Network: The network that provides the\n   connectivity\
    \ among NVEs and that NVO3 packets are tunneled over,\n   where an NVO3 packet\
    \ carries an NVO3 overlay header followed by a\n   tenant packet.  The underlay\
    \ network does not need to be aware that\n   it is carrying NVO3 packets.  Addresses\
    \ on the underlay network\n   appear as \"outer addresses\" in encapsulated NVO3\
    \ packets.  In\n   general, the underlay network can use a completely different\
    \ protocol\n   (and address family) from that of the overlay.  In the case of\
    \ NVO3,\n   the underlay network is IP.\n   Data Center (DC): A physical complex\
    \ housing physical servers,\n   network switches and routers, network service\
    \ appliances, and\n   networked storage.  The purpose of a data center is to provide\n\
    \   application, compute, and/or storage services.  One such service is\n   virtualized\
    \ infrastructure data center services, also known as\n   \"Infrastructure as a\
    \ Service\".\n   Virtual Data Center (Virtual DC): A container for virtualized\n\
    \   compute, storage, and network services.  A virtual DC is associated\n   with\
    \ a single tenant and can contain multiple VNs and Tenant Systems\n   connected\
    \ to one or more of these VNs.\n   Virtual Machine (VM): A software implementation\
    \ of a physical machine\n   that runs programs as if they were executing on a\
    \ physical, non-\n   virtualized machine.  Applications (generally) do not know\
    \ they are\n   running on a VM as opposed to running on a \"bare metal\" host\
    \ or\n   server, though some systems provide a para-virtualization environment\n\
    \   that allows an operating system or application to be aware of the\n   presence\
    \ of virtualization for optimization purposes.\n   Hypervisor: Software running\
    \ on a server that allows multiple VMs to\n   run on the same physical server.\
    \  The hypervisor manages and provides\n   shared computation, memory, and storage\
    \ services and network\n   connectivity to the VMs that it hosts.  Hypervisors\
    \ often embed a\n   virtual switch (see below).\n   Server: A physical end-host\
    \ machine that runs user applications.  A\n   standalone (or \"bare metal\") server\
    \ runs a conventional operating\n   system hosting a single-tenant application.\
    \  A virtualized server\n   runs a hypervisor supporting one or more VMs.\n  \
    \ Virtual Switch (vSwitch): A function within a hypervisor (typically\n   implemented\
    \ in software) that provides similar forwarding services to\n   a physical Ethernet\
    \ switch.  A vSwitch forwards Ethernet frames\n   between VMs running on the same\
    \ server or between a VM and a physical\n   Network Interface Card (NIC) connecting\
    \ the server to a physical\n   Ethernet switch or router.  A vSwitch also enforces\
    \ network isolation\n   between VMs that by policy are not permitted to communicate\
    \ with each\n   other (e.g., by honoring VLANs).  A vSwitch may be bypassed when\
    \ an\n   NVE is enabled on the host server.\n   Tenant: The customer using a virtual\
    \ network and any associated\n   resources (e.g., compute, storage, and network).\
    \  A tenant could be\n   an enterprise or a department/organization within an\
    \ enterprise.\n   Tenant System: A physical or virtual system that can play the\
    \ role of\n   a host or a forwarding element such as a router, switch, firewall,\n\
    \   etc.  It belongs to a single tenant and connects to one or more VNs\n   of\
    \ that tenant.\n   Tenant Separation: Refers to isolating traffic of different\
    \ tenants\n   such that traffic from one tenant is not visible to or delivered\
    \ to\n   another tenant, except when allowed by policy.  Tenant separation\n \
    \  also refers to address space separation, whereby different tenants\n   can\
    \ use the same address space without conflict.\n   Virtual Access Points (VAPs):\
    \ A logical connection point on the NVE\n   for connecting a Tenant System to\
    \ a virtual network.  Tenant Systems\n   connect to VNIs at an NVE through VAPs.\
    \  VAPs can be physical ports\n   or virtual ports identified through logical\
    \ interface identifiers\n   (e.g., VLAN ID or internal vSwitch Interface ID connected\
    \ to a VM).\n   End Device: A physical device that connects directly to the DC\n\
    \   underlay network.  This is in contrast to a Tenant System, which\n   connects\
    \ to a corresponding tenant VN.  An End Device is administered\n   by the DC operator\
    \ rather than a tenant and is part of the DC\n   infrastructure.  An End Device\
    \ may implement NVO3 technology in\n   support of NVO3 functions.  Examples of\
    \ an End Device include hosts\n   (e.g., server or server blade), storage systems\
    \ (e.g., file servers\n   and iSCSI storage systems), and network devices (e.g.,\
    \ firewall,\n   load-balancer, and IPsec gateway).\n   Network Virtualization\
    \ Authority (NVA): Entity that provides\n   reachability and forwarding information\
    \ to NVEs.\n"
- title: 1.2.  DC Network Architecture
  contents:
  - "1.2.  DC Network Architecture\n   A generic architecture for data centers is\
    \ depicted in Figure 1:\n                                ,---------.\n       \
    \                       ,'           `.\n                             (  IP/MPLS\
    \ WAN )\n                              `.           ,'\n                     \
    \           `-+------+'\n                                 \\      /\n        \
    \                  +--------+   +--------+\n                          |   DC \
    \  |+-+|   DC   |\n                          |gateway |+-+|gateway |\n       \
    \                   +--------+   +--------+\n                                |\
    \       /\n                                .--. .--.\n                       \
    \       (    '    '.--.\n                            .-.' Intra-DC     '\n   \
    \                        (     network      )\n                            ( \
    \            .'-'\n                             '--'._.'.    )\\ \\\n        \
    \                     / /     '--'  \\ \\\n                            / /   \
    \   | |    \\ \\\n                   +--------+   +--------+   +--------+\n  \
    \                 | access |   | access |   | access |\n                   | switch\
    \ |   | switch |   | switch |\n                   +--------+   +--------+   +--------+\n\
    \                      /     \\    /    \\     /      \\\n                   __/_\
    \      \\  /      \\   /_      _\\__\n             '--------'   '--------'   '--------'\
    \   '--------'\n             :  End   :   :  End   :   :  End   :   :  End   :\n\
    \             : Device :   : Device :   : Device :   : Device :\n            \
    \ '--------'   '--------'   '--------'   '--------'\n             Figure 1: A\
    \ Generic Architecture for Data Centers\n   An example of multi-tier DC network\
    \ architecture is presented in\n   Figure 1.  It provides a view of the physical\
    \ components inside a DC.\n   A DC network is usually composed of intra-DC networks\
    \ and network\n   services, and inter-DC network and network connectivity services.\n\
    \   DC networking elements can act as strict L2 switches and/or provide\n   IP\
    \ routing capabilities, including network service virtualization.\n   In some\
    \ DC architectures, some tier layers could provide L2 and/or L3\n   services.\
    \  In addition, some tier layers may be collapsed, and\n   Internet connectivity,\
    \ inter-DC connectivity, and VPN support may be\n   handled by a smaller number\
    \ of nodes.  Nevertheless, one can assume\n   that the network functional blocks\
    \ in a DC fit in the architecture\n   depicted in Figure 1.\n   The following\
    \ components can be present in a DC:\n   -  Access switch: Hardware-based Ethernet\
    \ switch aggregating all\n      Ethernet links from the End Devices in a rack\
    \ representing the\n      entry point in the physical DC network for the hosts.\
    \  It may also\n      provide routing functionality, virtual IP network connectivity,\
    \ or\n      Layer 2 tunneling over IP, for instance.  Access switches are\n  \
    \    usually multihomed to aggregation switches in the Intra-DC\n      network.\
    \  A typical example of an access switch is a Top-of-Rack\n      (ToR) switch.\
    \  Other deployment scenarios may use an intermediate\n      Blade Switch before\
    \ the ToR, or an End-of-Row (EoR) switch, to\n      provide similar functions\
    \ to a ToR.\n   -  Intra-DC Network: Network composed of high-capacity core nodes\n\
    \      (Ethernet switches/routers).  Core nodes may provide virtual\n      Ethernet\
    \ bridging and/or IP routing services.\n   -  DC Gateway (DC GW): Gateway to the\
    \ outside world providing DC\n      interconnect and connectivity to Internet\
    \ and VPN customers.  In\n      the current DC network model, this may be simply\
    \ a router\n      connected to the Internet and/or an IP VPN/L2VPN PE.  Some network\n\
    \      implementations may dedicate DC GWs for different connectivity\n      types\
    \ (e.g., a DC GW for Internet and another for VPN).\n   Note that End Devices\
    \ may be single-homed or multihomed to access\n   switches.\n"
- title: 2.  Reference Models
  contents:
  - '2.  Reference Models

    '
- title: 2.1.  Generic Reference Model
  contents:
  - "2.1.  Generic Reference Model\n   Figure 2 depicts a DC reference model for network\
    \ virtualization\n   overlays where NVEs provide a logical interconnect between\
    \ Tenant\n   Systems that belong to a specific VN.\n         +--------+      \
    \                              +--------+\n         | Tenant +--+            \
    \                +----| Tenant |\n         | System |  |                     \
    \      (')   | System |\n         +--------+  |    .................     (   )\
    \  +--------+\n                     |  +---+           +---+    (_)\n        \
    \             +--|NVE|---+   +---|NVE|-----+\n                        +---+  \
    \ |   |   +---+\n                        / .    +-----+      .\n             \
    \          /  . +--| NVA |--+   .\n                      /   . |  +-----+   \\\
    \  .\n                     |    . |             \\ .\n                     | \
    \   . |   Overlay   +--+--++--------+\n         +--------+  |    . |   Network\
    \   | NVE || Tenant |\n         | Tenant +--+    . |             |     || System\
    \ |\n         | System |       .  \\ +---+      +--+--++--------+\n         +--------+\
    \       .....|NVE|.........\n                               +---+\n          \
    \                       |\n                                 |\n              \
    \         =====================\n                         |               |\n\
    \                     +--------+      +--------+\n                     | Tenant\
    \ |      | Tenant |\n                     | System |      | System |\n       \
    \              +--------+      +--------+\n      Figure 2: Generic Reference Model\
    \ for DC Network Virtualization\n                                 Overlays\n \
    \  In order to obtain reachability information, NVEs may exchange\n   information\
    \ directly between themselves via a control-plane protocol.\n   In this case,\
    \ a control-plane module resides in every NVE.\n   It is also possible for NVEs\
    \ to communicate with an external Network\n   Virtualization Authority (NVA) to\
    \ obtain reachability and forwarding\n   information.  In this case, a protocol\
    \ is used between NVEs and\n   NVA(s) to exchange information.\n   It should be\
    \ noted that NVAs may be organized in clusters for\n   redundancy and scalability\
    \ and can appear as one logically\n   centralized controller.  In this case, inter-NVA\
    \ communication is\n   necessary to synchronize state among nodes within a cluster\
    \ or share\n   information across clusters.  The information exchanged between\
    \ NVAs\n   of the same cluster could be different from the information exchanged\n\
    \   across clusters.\n   A Tenant System can be attached to an NVE in several\
    \ ways:\n   -  locally, by being co-located in the same End Device\n   -  remotely,\
    \ via a point-to-point connection or a switched network\n   When an NVE is co-located\
    \ with a Tenant System, the state of the\n   Tenant System can be determined without\
    \ protocol assistance.  For\n   instance, the operational status of a VM can be\
    \ communicated via a\n   local API.  When an NVE is remotely connected to a Tenant\
    \ System, the\n   state of the Tenant System or NVE needs to be exchanged directly\
    \ or\n   via a management entity, using a control-plane protocol or API, or\n\
    \   directly via a data-plane protocol.\n   The functional components in Figure\
    \ 2 do not necessarily map directly\n   to the physical components described in\
    \ Figure 1.  For example, an\n   End Device can be a server blade with VMs and\
    \ a virtual switch.  A VM\n   can be a Tenant System, and the NVE functions may\
    \ be performed by the\n   host server.  In this case, the Tenant System and NVE\
    \ function are\n   co-located.  Another example is the case where the End Device\
    \ is the\n   Tenant System and the NVE function can be implemented by the\n  \
    \ connected ToR.  In this case, the Tenant System and NVE function are\n   not\
    \ co-located.\n   Underlay nodes utilize L3 technologies to interconnect NVE nodes.\n\
    \   These nodes perform forwarding based on outer L3 header information,\n   and\
    \ generally do not maintain state for each tenant service, albeit\n   some applications\
    \ (e.g., multicast) may require control-plane or\n   forwarding-plane information\
    \ that pertains to a tenant, group of\n   tenants, tenant service, or a set of\
    \ services that belong to one or\n   more tenants.  Mechanisms to control the\
    \ amount of state maintained\n   in the underlay may be needed.\n"
- title: 2.2.  NVE Reference Model
  contents:
  - "2.2.  NVE Reference Model\n   Figure 3 depicts the NVE reference model.  One\
    \ or more VNIs can be\n   instantiated on an NVE.  A Tenant System interfaces\
    \ with a\n   corresponding VNI via a VAP.  An overlay module provides tunneling\n\
    \   overlay functions (e.g., encapsulation and decapsulation of tenant\n   traffic,\
    \ tenant identification, and mapping, etc.).\n                     +-------- L3\
    \ Network -------+\n                     |                           |\n     \
    \                |        Tunnel Overlay     |\n         +------------+---------+\
    \       +---------+------------+\n         | +----------+-------+ |       | +---------+--------+\
    \ |\n         | |  Overlay Module  | |       | |  Overlay Module  | |\n      \
    \   | +---------+--------+ |       | +---------+--------+ |\n         |      \
    \     |VN Context|       | VN Context|          |\n         |           |    \
    \      |       |           |          |\n         |  +--------+-------+  |   \
    \    |  +--------+-------+  |\n         |  | |VNI|   .  |VNI|  |       |  | |VNI|\
    \   .  |VNI|  |\n    NVE1 |  +-+------------+-+  |       |  +-+-----------+--+\
    \  | NVE2\n         |    |   VAPs     |    |       |    |    VAPs   |     |\n\
    \         +----+------------+----+       +----+-----------+-----+\n          \
    \    |            |                 |           |\n              |           \
    \ |                 |           |\n             Tenant Systems               \
    \  Tenant Systems\n                  Figure 3: Generic NVE Reference Model\n \
    \  Note that some NVE functions (e.g., data-plane and control-plane\n   functions)\
    \ may reside in one device or may be implemented separately\n   in different devices.\n"
- title: 2.3.  NVE Service Types
  contents:
  - "2.3.  NVE Service Types\n   An NVE provides different types of virtualized network\
    \ services to\n   multiple tenants, i.e., an L2 service or an L3 service.  Note\
    \ that an\n   NVE may be capable of providing both L2 and L3 services for a tenant.\n\
    \   This section defines the service types and associated attributes.\n"
- title: 2.3.1.  L2 NVE Providing Ethernet LAN-Like Service
  contents:
  - "2.3.1.  L2 NVE Providing Ethernet LAN-Like Service\n   An L2 NVE implements Ethernet\
    \ LAN emulation, an Ethernet-based\n   multipoint service similar to an IETF Virtual\
    \ Private LAN Service\n   (VPLS) [RFC4761][RFC4762] or Ethernet VPN [EVPN] service,\
    \ where the\n   Tenant Systems appear to be interconnected by a LAN environment\
    \ over\n   an L3 overlay.  As such, an L2 NVE provides per-tenant virtual\n  \
    \ switching instance (L2 VNI) and L3 (IP/MPLS) tunneling encapsulation\n   of\
    \ tenant Media Access Control (MAC) frames across the underlay.\n   Note that\
    \ the control plane for an L2 NVE could be implemented\n   locally on the NVE\
    \ or in a separate control entity.\n"
- title: 2.3.2.  L3 NVE Providing IP/VRF-Like Service
  contents:
  - "2.3.2.  L3 NVE Providing IP/VRF-Like Service\n   An L3 NVE provides virtualized\
    \ IP forwarding service, similar to IETF\n   IP VPN (e.g., BGP/MPLS IP VPN [RFC4364])\
    \ from a service definition\n   perspective.  That is, an L3 NVE provides per-tenant\
    \ forwarding and\n   routing instance (L3 VNI) and L3 (IP/MPLS) tunneling encapsulation\
    \ of\n   tenant IP packets across the underlay.  Note that routing could be\n\
    \   performed locally on the NVE or in a separate control entity.\n"
- title: 2.4.  Operational Management Considerations
  contents:
  - "2.4.  Operational Management Considerations\n   NVO3 services are overlay services\
    \ over an IP underlay.\n   As far as the IP underlay is concerned, existing IP\
    \ Operations,\n   Administration, and Maintenance (OAM) facilities are used.\n\
    \   With regard to the NVO3 overlay, both L2 and L3 services can be\n   offered.\
    \  It is expected that existing fault and performance OAM\n   facilities will\
    \ be used.  Sections 4.1 and 4.2.6 provide further\n   discussion of additional\
    \ fault and performance management issues to\n   consider.\n   As far as configuration\
    \ is concerned, the DC environment is driven by\n   the need to bring new services\
    \ up rapidly and is typically very\n   dynamic, specifically in the context of\
    \ virtualized services.  It is\n   therefore critical to automate the configuration\
    \ of NVO3 services.\n"
- title: 3. Functional Components
  contents:
  - "3. Functional Components\n   This section decomposes the network virtualization\
    \ architecture into\n   the functional components described in Figure 3 to make\
    \ it easier to\n   discuss solution options for these components.\n"
- title: 3.1.  Service Virtualization Components
  contents:
  - '3.1.  Service Virtualization Components

    '
- title: 3.1.1.  Virtual Access Points (VAPs)
  contents:
  - "3.1.1.  Virtual Access Points (VAPs)\n   Tenant Systems are connected to VNIs\
    \ through Virtual Access Points\n   (VAPs).\n   VAPs can be physical ports or\
    \ virtual ports identified through\n   logical interface identifiers (e.g., VLAN\
    \ ID and internal vSwitch\n   Interface ID connected to a VM).\n"
- title: 3.1.2.  Virtual Network Instance (VNI)
  contents:
  - "3.1.2.  Virtual Network Instance (VNI)\n   A VNI is a specific VN instance on\
    \ an NVE.  Each VNI defines a\n   forwarding context that contains reachability\
    \ information and\n   policies.\n"
- title: 3.1.3.  Overlay Modules and VN Context
  contents:
  - "3.1.3.  Overlay Modules and VN Context\n   Mechanisms for identifying each tenant\
    \ service are required to allow\n   the simultaneous overlay of multiple tenant\
    \ services over the same\n   underlay L3 network topology.  In the data plane,\
    \ each NVE, upon\n   sending a tenant packet, must be able to encode the VN Context\
    \ for\n   the destination NVE in addition to the L3 tunneling information\n  \
    \ (e.g., source IP address identifying the source NVE and the\n   destination\
    \ IP address identifying the destination NVE, or MPLS\n   label).  This allows\
    \ the destination NVE to identify the tenant\n   service instance and therefore\
    \ appropriately process and forward the\n   tenant packet.\n   The overlay module\
    \ provides tunneling overlay functions: tunnel\n   initiation/termination as in\
    \ the case of stateful tunnels (see\n   Section 3.1.4) and/or encapsulation/decapsulation\
    \ of frames from the\n   VAPs/L3 underlay.\n   In a multi-tenant context, tunneling\
    \ aggregates frames from/to\n   different VNIs.  Tenant identification and traffic\
    \ demultiplexing are\n   based on the VN Context identifier.\n   The following\
    \ approaches can be considered:\n   -  VN Context identifier per Tenant: This\
    \ is a globally unique (on a\n      per-DC administrative domain) VN identifier\
    \ used to identify the\n      corresponding VNI.  Examples of such identifiers\
    \ in existing\n      technologies are IEEE VLAN IDs and Service Instance IDs (I-SIDs)\n\
    \      that identify virtual L2 domains when using IEEE 802.1Q and IEEE\n    \
    \  802.1ah, respectively.  Note that multiple VN identifiers can\n      belong\
    \ to a tenant.\n   -  One VN Context identifier per VNI: Each VNI value is automatically\n\
    \      generated by the egress NVE, or a control plane associated with\n     \
    \ that NVE, and usually distributed by a control-plane protocol to\n      all\
    \ the related NVEs.  An example of this approach is the use of\n      per-VRF\
    \ MPLS labels in IP VPN [RFC4364].  The VNI value is\n      therefore locally\
    \ significant to the egress NVE.\n   -  One VN Context identifier per VAP: A value\
    \ locally significant to\n      an NVE is assigned and usually distributed by\
    \ a control-plane\n      protocol to identify a VAP.  An example of this approach\
    \ is the\n      use of per-CE MPLS labels in IP VPN [RFC4364].\n   Note that when\
    \ using one VN Context per VNI or per VAP, an additional\n   global identifier\
    \ (e.g., a VN identifier or name) may be used by the\n   control plane to identify\
    \ the tenant context.\n"
- title: 3.1.4.  Tunnel Overlays and Encapsulation Options
  contents:
  - "3.1.4.  Tunnel Overlays and Encapsulation Options\n   Once the VN Context identifier\
    \ is added to the frame, an L3 tunnel\n   encapsulation is used to transport the\
    \ frame to the destination NVE.\n   Different IP tunneling options (e.g., Generic\
    \ Routing Encapsulation\n   (GRE), the Layer 2 Tunneling Protocol (L2TP), and\
    \ IPsec) and MPLS\n   tunneling can be used.  Tunneling could be stateless or\
    \ stateful.\n   Stateless tunneling simply entails the encapsulation of a tenant\n\
    \   packet with another header necessary for forwarding the packet across\n  \
    \ the underlay (e.g., IP tunneling over an IP underlay).  Stateful\n   tunneling,\
    \ on the other hand, entails maintaining tunneling state at\n   the tunnel endpoints\
    \ (i.e., NVEs).  Tenant packets on an ingress NVE\n   can then be transmitted\
    \ over such tunnels to a destination (egress)\n   NVE by encapsulating the packets\
    \ with a corresponding tunneling\n   header.  The tunneling state at the endpoints\
    \ may be configured or\n   dynamically established.  Solutions should specify\
    \ the tunneling\n   technology used and whether it is stateful or stateless. \
    \ In this\n   document, however, tunneling and tunneling encapsulation are used\n\
    \   interchangeably to simply mean the encapsulation of a tenant packet\n   with\
    \ a tunneling header necessary to carry the packet between an\n   ingress NVE\
    \ and an egress NVE across the underlay.  It should be\n   noted that stateful\
    \ tunneling, especially when configuration is\n   involved, does impose management\
    \ overhead and scale constraints.\n   When confidentiality is required, the use\
    \ of opportunistic security\n   [OPPSEC] can be used as a stateless tunneling\
    \ solution.\n"
- title: 3.1.5.  Control-Plane Components
  contents:
  - '3.1.5.  Control-Plane Components

    '
- title: 3.1.5.1.  Distributed vs. Centralized Control Plane
  contents:
  - "3.1.5.1.  Distributed vs. Centralized Control Plane\n   Control- and management-plane\
    \ entities can be centralized or\n   distributed.  Both approaches have been used\
    \ extensively in the past.\n   The routing model of the Internet is a good example\
    \ of a distributed\n   approach.  Transport networks have usually used a centralized\n\
    \   approach to manage transport paths.\n   It is also possible to combine the\
    \ two approaches, i.e., using a\n   hybrid model.  A global view of network state\
    \ can have many benefits,\n   but it does not preclude the use of distributed\
    \ protocols within the\n   network.  Centralized models provide a facility to\
    \ maintain global\n   state and distribute that state to the network.  When used\
    \ in\n   combination with distributed protocols, greater network efficiencies,\n\
    \   improved reliability, and robustness can be achieved.  Domain- and/or\n  \
    \ deployment-specific constraints define the balance between\n   centralized and\
    \ distributed approaches.\n"
- title: 3.1.5.2.  Auto-provisioning and Service Discovery
  contents:
  - "3.1.5.2.  Auto-provisioning and Service Discovery\n   NVEs must be able to identify\
    \ the appropriate VNI for each Tenant\n   System.  This is based on state information\
    \ that is often provided by\n   external entities.  For example, in an environment\
    \ where a VM is a\n   Tenant System, this information is provided by VM orchestration\n\
    \   systems, since these are the only entities that have visibility of\n   which\
    \ VM belongs to which tenant.\n   A mechanism for communicating this information\
    \ to the NVE is\n   required.  VAPs have to be created and mapped to the appropriate\
    \ VNI.\n   Depending upon the implementation, this control interface can be\n\
    \   implemented using an auto-discovery protocol between Tenant Systems\n   and\
    \ their local NVE or through management entities.  In either case,\n   appropriate\
    \ security and authentication mechanisms to verify that\n   Tenant System information\
    \ is not spoofed or altered are required.\n   This is one critical aspect for\
    \ providing integrity and tenant\n   isolation in the system.\n   NVEs may learn\
    \ reachability information for VNIs on other NVEs via a\n   control protocol that\
    \ exchanges such information among NVEs or via a\n   management-control entity.\n"
- title: 3.1.5.3.  Address Advertisement and Tunnel Mapping
  contents:
  - "3.1.5.3.  Address Advertisement and Tunnel Mapping\n   As traffic reaches an\
    \ ingress NVE on a VAP, a lookup is performed to\n   determine which NVE or local\
    \ VAP the packet needs to be sent to.  If\n   the packet is to be sent to another\
    \ NVE, the packet is encapsulated\n   with a tunnel header containing the destination\
    \ information\n   (destination IP address or MPLS label) of the egress NVE.\n\
    \   Intermediate nodes (between the ingress and egress NVEs) switch or\n   route\
    \ traffic based upon the tunnel destination information.\n   A key step in the\
    \ above process consists of identifying the\n   destination NVE the packet is\
    \ to be tunneled to.  NVEs are\n   responsible for maintaining a set of forwarding\
    \ or mapping tables\n   that hold the bindings between destination VM and egress\
    \ NVE\n   addresses.  Several ways of populating these tables are possible:\n\
    \   control plane driven, management plane driven, or data plane driven.\n   When\
    \ a control-plane protocol is used to distribute address\n   reachability and\
    \ tunneling information, the auto-provisioning and\n   service discovery could\
    \ be accomplished by the same protocol.  In\n   this scenario, the auto-provisioning\
    \ and service discovery could be\n   combined with (be inferred from) the address\
    \ advertisement and\n   associated tunnel mapping.  Furthermore, a control-plane\
    \ protocol\n   that carries both MAC and IP addresses eliminates the need for\
    \ the\n   Address Resolution Protocol (ARP) and hence addresses one of the\n \
    \  issues with explosive ARP handling as discussed in [RFC6820].\n"
- title: 3.1.5.4.  Overlay Tunneling
  contents:
  - "3.1.5.4.  Overlay Tunneling\n   For overlay tunneling, and dependent upon the\
    \ tunneling technology\n   used for encapsulating the Tenant System packets, it\
    \ may be\n   sufficient to have one or more local NVE addresses assigned and used\n\
    \   in the source and destination fields of a tunneling encapsulation\n   header.\
    \  Other information that is part of the tunneling\n   encapsulation header may\
    \ also need to be configured.  In certain\n   cases, local NVE configuration may\
    \ be sufficient while in other\n   cases, some tunneling-related information may\
    \ need to be shared among\n   NVEs.  The information that needs to be shared will\
    \ be technology\n   dependent.  For instance, potential information could include\
    \ tunnel\n   identity, encapsulation type, and/or tunnel resources.  In certain\n\
    \   cases, such as when using IP multicast in the underlay, tunnels that\n   interconnect\
    \ NVEs may need to be established.  When tunneling\n   information needs to be\
    \ exchanged or shared among NVEs, a control-\n   plane protocol may be required.\
    \  For instance, it may be necessary to\n   provide active/standby status information\
    \ between NVEs, up/down\n   status information, pruning/grafting information for\
    \ multicast\n   tunnels, etc.\n   In addition, a control plane may be required\
    \ to set up the tunnel\n   path for some tunneling technologies.  This applies\
    \ to both unicast\n   and multicast tunneling.\n"
- title: 3.2.  Multihoming
  contents:
  - "3.2.  Multihoming\n   Multihoming techniques can be used to increase the reliability\
    \ of an\n   NVO3 network.  It is also important to ensure that the physical\n\
    \   diversity in an NVO3 network is taken into account to avoid single\n   points\
    \ of failure.\n   Multihoming can be enabled in various nodes, from Tenant Systems\
    \ into\n   ToRs, ToRs into core switches/routers, and core nodes into DC GWs.\n\
    \   The NVO3 underlay nodes (i.e., from NVEs to DC GWs) rely on IP\n   routing\
    \ techniques or MPLS re-rerouting capabilities as the means to\n   re-route traffic\
    \ upon failures.\n   When a Tenant System is co-located with the NVE, the Tenant\
    \ System is\n   effectively single-homed to the NVE via a virtual port.  When\
    \ the\n   Tenant System and the NVE are separated, the Tenant System is\n   connected\
    \ to the NVE via a logical L2 construct such as a VLAN, and\n   it can be multihomed\
    \ to various NVEs.  An NVE may provide an L2\n   service to the end system or\
    \ an l3 service.  An NVE may be multihomed\n   to a next layer in the DC at L2\
    \ or L3.  When an NVE provides an L2\n   service and is not co-located with the\
    \ end system, loop-avoidance\n   techniques must be used.  Similarly, when the\
    \ NVE provides L3\n   service, similar dual-homing techniques can be used.  When\
    \ the NVE\n   provides an L3 service to the end system, it is possible that no\n\
    \   dynamic routing protocol is enabled between the end system and the\n   NVE.\
    \  The end system can be multihomed to multiple physically\n   separated L3 NVEs\
    \ over multiple interfaces.  When one of the links\n   connected to an NVE fails,\
    \ the other interfaces can be used to reach\n   the end system.\n   External connectivity\
    \ from a DC can be handled by two or more DC\n   gateways.  Each gateway provides\
    \ access to external networks such as\n   VPNs or the Internet.  A gateway may\
    \ be connected to two or more edge\n   nodes in the external network for redundancy.\
    \  When a connection to\n   an upstream node is lost, the alternative connection\
    \ is used, and the\n   failed route withdrawn.\n"
- title: 3.3.  VM Mobility
  contents:
  - "3.3.  VM Mobility\n   In DC environments utilizing VM technologies, an important\
    \ feature is\n   that VMs can move from one server to another server in the same\
    \ or\n   different L2 physical domains (within or across DCs) in a seamless\n\
    \   manner.\n   A VM can be moved from one server to another in stopped or suspended\n\
    \   state (\"cold\" VM mobility) or in running/active state (\"hot\" VM\n   mobility).\
    \  With \"hot\" mobility, VM L2 and L3 addresses need to be\n   preserved.  With\
    \ \"cold\" mobility, it may be desired to preserve at\n   least VM L3 addresses.\n\
    \   Solutions to maintain connectivity while a VM is moved are necessary\n   in\
    \ the case of \"hot\" mobility.  This implies that connectivity among\n   VMs\
    \ is preserved.  For instance, for L2 VNs, ARP caches are updated\n   accordingly.\n\
    \   Upon VM mobility, NVE policies that define connectivity among VMs\n   must\
    \ be maintained.\n   During VM mobility, it is expected that the path to the VM's\
    \ default\n   gateway assures adequate QoS to VM applications, i.e., QoS that\n\
    \   matches the expected service-level agreement for these applications.\n"
- title: 4.  Key Aspects of Overlay Networks
  contents:
  - "4.  Key Aspects of Overlay Networks\n   The intent of this section is to highlight\
    \ specific issues that\n   proposed overlay solutions need to address.\n"
- title: 4.1.  Pros and Cons
  contents:
  - "4.1.  Pros and Cons\n   An overlay network is a layer of virtual network topology\
    \ on top of\n   the physical network.\n   Overlay networks offer the following\
    \ key advantages:\n   -  Unicast tunneling state management and association of\
    \ Tenant\n      Systems reachability are handled at the edge of the network (at\n\
    \      the NVE).  Intermediate transport nodes are unaware of such state.\n  \
    \    Note that when multicast is enabled in the underlay network to\n      build\
    \ multicast trees for tenant VNs, there would be more state\n      related to\
    \ tenants in the underlay core network.\n   -  Tunneling is used to aggregate\
    \ traffic and hide tenant addresses\n      from the underlay network and hence\
    \ offers the advantage of\n      minimizing the amount of forwarding state required\
    \ within the\n      underlay network.\n   -  Decoupling of the overlay addresses\
    \ (MAC and IP) used by VMs from\n      the underlay network provides tenant separation\
    \ and separation of\n      the tenant address spaces from the underlay address\
    \ space.\n   -  Overlay networks support of a large number of virtual network\n\
    \      identifiers.\n   Overlay networks also create several challenges:\n   -\
    \  Overlay networks typically have no control of underlay networks\n      and\
    \ lack underlay network information (e.g., underlay\n      utilization):\n   \
    \   o  Overlay networks and/or their associated management entities\n        \
    \ typically probe the network to measure link or path properties,\n         such\
    \ as available bandwidth or packet loss rate.  It is\n         difficult to accurately\
    \ evaluate network properties.  It might\n         be preferable for the underlay\
    \ network to expose usage and\n         performance information.\n      o  Miscommunication\
    \ or lack of coordination between overlay and\n         underlay networks can\
    \ lead to an inefficient usage of network\n         resources.\n      o  When\
    \ multiple overlays co-exist on top of a common underlay\n         network, the\
    \ lack of coordination between overlays can lead to\n         performance issues\
    \ and/or resource usage inefficiencies.\n   -  Traffic carried over an overlay\
    \ might fail to traverse firewalls\n      and NAT devices.\n   -  Multicast service\
    \ scalability: Multicast support may be required\n      in the underlay network\
    \ to address tenant flood containment or\n      efficient multicast handling.\
    \  The underlay may also be required\n      to maintain multicast state on a per-tenant\
    \ basis or even on a\n      per-individual multicast flow of a given tenant. \
    \ Ingress\n      replication at the NVE eliminates that additional multicast state\n\
    \      in the underlay core, but depending on the multicast traffic\n      volume,\
    \ it may cause inefficient use of bandwidth.\n"
- title: 4.2.  Overlay Issues to Consider
  contents:
  - '4.2.  Overlay Issues to Consider

    '
- title: 4.2.1.  Data Plane vs. Control Plane Driven
  contents:
  - "4.2.1.  Data Plane vs. Control Plane Driven\n   In the case of an L2 NVE, it\
    \ is possible to dynamically learn MAC\n   addresses against VAPs.  It is also\
    \ possible that such addresses be\n   known and controlled via management or a\
    \ control protocol for both L2\n   NVEs and L3 NVEs.  Dynamic data-plane learning\
    \ implies that flooding\n   of unknown destinations be supported and hence implies\
    \ that broadcast\n   and/or multicast be supported or that ingress replication\
    \ be used as\n   described in Section 4.2.3.  Multicasting in the underlay network\
    \ for\n   dynamic learning may lead to significant scalability limitations.\n\
    \   Specific forwarding rules must be enforced to prevent loops from\n   happening.\
    \  This can be achieved using a spanning tree, a shortest\n   path tree, or a\
    \ split-horizon mesh.\n   It should be noted that the amount of state to be distributed\
    \ is\n   dependent upon network topology and the number of virtual machines.\n\
    \   Different forms of caching can also be utilized to minimize state\n   distribution\
    \ between the various elements.  The control plane should\n   not require an NVE\
    \ to maintain the locations of all the Tenant\n   Systems whose VNs are not present\
    \ on the NVE.  The use of a control\n   plane does not imply that the data plane\
    \ on NVEs has to maintain all\n   the forwarding state in the control plane.\n"
- title: 4.2.2.  Coordination between Data Plane and Control Plane
  contents:
  - "4.2.2.  Coordination between Data Plane and Control Plane\n   For an L2 NVE,\
    \ the NVE needs to be able to determine MAC addresses of\n   the Tenant Systems\
    \ connected via a VAP.  This can be achieved via\n   data-plane learning or a\
    \ control plane.  For an L3 NVE, the NVE needs\n   to be able to determine the\
    \ IP addresses of the Tenant Systems\n   connected via a VAP.\n   In both cases,\
    \ coordination with the NVE control protocol is needed\n   such that when the\
    \ NVE determines that the set of addresses behind a\n   VAP has changed, it triggers\
    \ the NVE control plane to distribute this\n   information to its peers.\n"
- title: 4.2.3.  Handling Broadcast, Unknown Unicast, and Multicast (BUM) Traffic
  contents:
  - "4.2.3.  Handling Broadcast, Unknown Unicast, and Multicast (BUM) Traffic\n  \
    \ There are several options to support packet replication needed for\n   broadcast,\
    \ unknown unicast, and multicast.  Typical methods include:\n   - Ingress replication\n\
    \   - Use of underlay multicast trees\n   There is a bandwidth vs. state trade-off\
    \ between the two approaches.\n   Depending upon the degree of replication required\
    \ (i.e., the number\n   of hosts per group) and the amount of multicast state\
    \ to maintain,\n   trading bandwidth for state should be considered.\n   When\
    \ the number of hosts per group is large, the use of underlay\n   multicast trees\
    \ may be more appropriate.  When the number of hosts is\n   small (e.g., 2-3)\
    \ and/or the amount of multicast traffic is small,\n   ingress replication may\
    \ not be an issue.\n   Depending upon the size of the data center network and\
    \ hence the\n   number of (S,G) entries, and also the duration of multicast flows,\n\
    \   the use of underlay multicast trees can be a challenge.\n   When flows are\
    \ well known, it is possible to pre-provision such\n   multicast trees.  However,\
    \ it is often difficult to predict\n   application flows ahead of time; hence,\
    \ programming of (S,G) entries\n   for short-lived flows could be impractical.\n\
    \   A possible trade-off is to use in the underlay shared multicast trees\n  \
    \ as opposed to dedicated multicast trees.\n"
- title: 4.2.4. Path MTU
  contents:
  - "4.2.4. Path MTU\n   When using overlay tunneling, an outer header is added to\
    \ the\n   original frame.  This can cause the MTU of the path to the egress\n\
    \   tunnel endpoint to be exceeded.\n   It is usually not desirable to rely on\
    \ IP fragmentation for\n   performance reasons.  Ideally, the interface MTU as\
    \ seen by a Tenant\n   System is adjusted such that no fragmentation is needed.\n\
    \   It is possible for the MTU to be configured manually or to be\n   discovered\
    \ dynamically.  Various Path MTU discovery techniques exist\n   in order to determine\
    \ the proper MTU size to use:\n   -  Classical ICMP-based Path MTU Discovery [RFC1191]\
    \ [RFC1981]\n      Tenant Systems rely on ICMP messages to discover the MTU of\
    \ the\n      end-to-end path to its destination.  This method is not always\n\
    \      possible, such as when traversing middleboxes (e.g., firewalls)\n     \
    \ that disable ICMP for security reasons.\n   -  Extended Path MTU Discovery techniques\
    \ such as those defined in\n      [RFC4821]\n      Tenant Systems send probe packets\
    \ of different sizes and rely on\n      confirmation of receipt or lack thereof\
    \ from receivers to allow a\n      sender to discover the MTU of the end-to-end\
    \ paths.\n   While it could also be possible to rely on the NVE to perform\n \
    \  segmentation and reassembly operations without relying on the Tenant\n   Systems\
    \ to know about the end-to-end MTU, this would lead to\n   undesired performance\
    \ and congestion issues as well as significantly\n   increase the complexity of\
    \ hardware NVEs required for buffering and\n   reassembly logic.\n   Preferably,\
    \ the underlay network should be designed in such a way\n   that the MTU can accommodate\
    \ the extra tunneling and possibly\n   additional NVO3 header encapsulation overhead.\n"
- title: 4.2.5.  NVE Location Trade-Offs
  contents:
  - "4.2.5.  NVE Location Trade-Offs\n   In the case of DC traffic, traffic originated\
    \ from a VM is native\n   Ethernet traffic.  This traffic can be switched by a\
    \ local virtual\n   switch or ToR switch and then by a DC gateway.  The NVE function\
    \ can\n   be embedded within any of these elements.\n   There are several criteria\
    \ to consider when deciding where the NVE\n   function should happen:\n   -  Processing\
    \ and memory requirements\n      o  Datapath (e.g., lookups, filtering, and\n\
    \         encapsulation/decapsulation)\n      o  Control-plane processing (e.g.,\
    \ routing, signaling, and OAM)\n         and where specific control-plane functions\
    \ should be enabled\n   -  FIB/RIB size\n   -  Multicast support\n      o  Routing/signaling\
    \ protocols\n      o  Packet replication capability\n      o  Multicast FIB\n\
    \   -  Fragmentation support\n   -  QoS support (e.g., marking, policing, and\
    \ queuing)\n   -  Resiliency\n"
- title: 4.2.6.  Interaction between Network Overlays and Underlays
  contents:
  - "4.2.6.  Interaction between Network Overlays and Underlays\n   When multiple\
    \ overlays co-exist on top of a common underlay network,\n   resources (e.g.,\
    \ bandwidth) should be provisioned to ensure that\n   traffic from overlays can\
    \ be accommodated and QoS objectives can be\n   met.  Overlays can have partially\
    \ overlapping paths (nodes and\n   links).\n   Each overlay is selfish by nature.\
    \  It sends traffic so as to\n   optimize its own performance without considering\
    \ the impact on other\n   overlays, unless the underlay paths are traffic engineered\
    \ on a per-\n   overlay basis to avoid congestion of underlay resources.\n   Better\
    \ visibility between overlays and underlays, or general\n   coordination in placing\
    \ overlay demands on an underlay network, may\n   be achieved by providing mechanisms\
    \ to exchange performance and\n   liveliness information between the underlay\
    \ and overlay(s) or by the\n   use of such information by a coordination system.\
    \  Such information\n   may include:\n   -  Performance metrics (throughput, delay,\
    \ loss, jitter) such as\n      defined in [RFC3148], [RFC2679], [RFC2680], and\
    \ [RFC3393].\n   -  Cost metrics\n"
- title: 5.  Security Considerations
  contents:
  - "5.  Security Considerations\n   There are three points of view when considering\
    \ security for NVO3.\n   First, the service offered by a service provider via\
    \ NVO3 technology\n   to a tenant must meet the mutually agreed security requirements.\n\
    \   Second, a network implementing NVO3 must be able to trust the virtual\n  \
    \ network identity associated with packets received from a tenant.\n   Third,\
    \ an NVO3 network must consider the security associated with\n   running as an\
    \ overlay across the underlay network.\n   To meet a tenant's security requirements,\
    \ the NVO3 service must\n   deliver packets from the tenant to the indicated destination(s)\
    \ in\n   the overlay network and external networks.  The NVO3 service provides\n\
    \   data confidentiality through data separation.  The use of both VNIs\n   and\
    \ tunneling of tenant traffic by NVEs ensures that NVO3 data is\n   kept in a\
    \ separate context and thus separated from other tenant\n   traffic.  The infrastructure\
    \ supporting an NVO3 service (e.g.,\n   management systems, NVEs, NVAs, and intermediate\
    \ underlay networks)\n   should be limited to authorized access so that data integrity\
    \ can be\n   expected.  If a tenant requires that its data be confidential, then\n\
    \   the Tenant System may choose to encrypt its data before transmission\n   into\
    \ the NVO3 service.\n   An NVO3 service must be able to verify the VNI received\
    \ on a packet\n   from the tenant.  To ensure this, not only tenant data but also\
    \ NVO3\n   control data must be secured (e.g., control traffic between NVAs and\n\
    \   NVEs, between NVAs, and between NVEs).  Since NVEs and NVAs play a\n   central\
    \ role in NVO3, it is critical that secure access to NVEs and\n   NVAs be ensured\
    \ such that no unauthorized access is possible.  As\n   discussed in Section 3.1.5.2,\
    \ identification of Tenant Systems is\n   based upon state that is often provided\
    \ by management systems (e.g.,\n   a VM orchestration system in a virtualized\
    \ environment).  Secure\n   access to such management systems must also be ensured.\
    \  When an NVE\n   receives data from a Tenant System, the tenant identity needs\
    \ to be\n   verified in order to guarantee that it is authorized to access the\n\
    \   corresponding VN.  This can be achieved by identifying incoming\n   packets\
    \ against specific VAPs in some cases.  In other circumstances,\n   authentication\
    \ may be necessary.  Once this verification is done, the\n   packet is allowed\
    \ into the NVO3 overlay, and no integrity protection\n   is provided on the overlay\
    \ packet encapsulation (e.g., the VNI,\n   destination NVE, etc.).\n   Since an\
    \ NVO3 service can run across diverse underlay networks, when\n   the underlay\
    \ network is not trusted to provide at least data\n   integrity, data encryption\
    \ is needed to assure correct packet\n   delivery.\n   It is also desirable to\
    \ restrict the types of information (e.g.,\n   topology information as discussed\
    \ in Section 4.2.6) that can be\n   exchanged between an NVO3 service and underlay\
    \ networks based upon\n   their agreed security requirements.\n"
- title: 6.  Informative References
  contents:
  - "6.  Informative References\n   [EVPN]     Sajassi, A., Aggarwal, R., Bitar, N.,\
    \ Isaac, A., and J.\n              Uttaro, \"BGP MPLS Based Ethernet VPN\", Work\
    \ in Progress,\n              draft-ietf-l2vpn-evpn-10, October 2014.\n   [OPPSEC]\
    \   Dukhovni, V. \"Opportunistic Security: Some Protection Most\n            \
    \  of the Time\", Work in Progress, draft-dukhovni-\n              opportunistic-security-04,\
    \ August 2014.\n   [RFC1191]  Mogul, J. and S. Deering, \"Path MTU discovery\"\
    , RFC 1191,\n              November 1990, <http://www.rfc-editor.org/info/rfc1191>.\n\
    \   [RFC1981]  McCann, J., Deering, S., and J. Mogul, \"Path MTU Discovery\n \
    \             for IP version 6\", RFC 1981, August 1996,\n              <http://www.rfc-editor.org/info/rfc1981>.\n\
    \   [RFC2679]  Almes, G., Kalidindi, S., and M. Zekauskas, \"A One-way\n     \
    \         Delay Metric for IPPM\", RFC 2679, September 1999,\n              <http://www.rfc-editor.org/info/rfc2679>.\n\
    \   [RFC2680]  Almes, G., Kalidindi, S., and M. Zekauskas, \"A One-way\n     \
    \         Packet Loss Metric for IPPM\", RFC 2680, September 1999,\n         \
    \     <http://www.rfc-editor.org/info/rfc2680>.\n   [RFC3148]  Mathis, M. and\
    \ M. Allman, \"A Framework for Defining\n              Empirical Bulk Transfer\
    \ Capacity Metrics\", RFC 3148, July\n              2001, <http://www.rfc-editor.org/info/rfc3148>.\n\
    \   [RFC3393]  Demichelis, C. and P. Chimento, \"IP Packet Delay Variation\n \
    \             Metric for IP Performance Metrics (IPPM)\", RFC 3393,\n        \
    \      November 2002, <http://www.rfc-editor.org/info/rfc3393>.\n   [RFC4364]\
    \  Rosen, E. and Y. Rekhter, \"BGP/MPLS IP Virtual Private\n              Networks\
    \ (VPNs)\", RFC 4364, February 2006,\n              <http://www.rfc-editor.org/info/rfc4364>.\n\
    \   [RFC4761]  Kompella, K., Ed., and Y. Rekhter, Ed., \"Virtual Private\n   \
    \           LAN Service (VPLS) Using BGP for Auto-Discovery and\n            \
    \  Signaling\", RFC 4761, January 2007,\n              <http://www.rfc-editor.org/info/rfc4761>.\n\
    \   [RFC4762]  Lasserre, M., Ed., and V. Kompella, Ed., \"Virtual Private\n  \
    \            LAN Service (VPLS) Using Label Distribution Protocol (LDP)\n    \
    \          Signaling\", RFC 4762, January 2007,\n              <http://www.rfc-editor.org/info/rfc4762>.\n\
    \   [RFC4821]  Mathis, M. and J. Heffner, \"Packetization Layer Path MTU\n   \
    \           Discovery\", RFC 4821, March 2007,\n              <http://www.rfc-editor.org/info/rfc4821>.\n\
    \   [RFC6820]  Narten, T., Karir, M., and I. Foo, \"Address Resolution\n     \
    \         Problems in Large Data Center Networks\", RFC 6820, January\n      \
    \        2013, <http://www.rfc-editor.org/info/rfc6820>.\n   [RFC7364]  Narten,\
    \ T., Ed., Gray, E., Ed., Black, D., Fang, L.,\n              Kreeger, L., and\
    \ M. Napierala, \"Problem Statement:\n              Overlays for Network Virtualization\"\
    , RFC 7364, October\n              2014, <http://www.rfc-editor.org/info/rfc7364>.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   In addition to the authors, the following people contributed\
    \ to this\n   document: Dimitrios Stiliadis, Rotem Salomonovitch, Lucy Yong, Thomas\n\
    \   Narten, Larry Kreeger, and David Black.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Marc Lasserre\n   Alcatel-Lucent\n   EMail: marc.lasserre@alcatel-lucent.com\n\
    \   Florin Balus\n   Alcatel-Lucent\n   777 E. Middlefield Road\n   Mountain View,\
    \ CA 94043\n   United States\n   EMail: florin.balus@alcatel-lucent.com\n   Thomas\
    \ Morin\n   Orange\n   EMail: thomas.morin@orange.com\n   Nabil Bitar\n   Verizon\n\
    \   50 Sylvan Road\n   Waltham, MA 02145\n   United States\n   EMail: nabil.n.bitar@verizon.com\n\
    \   Yakov Rekhter\n   Juniper\n   EMail: yakov@juniper.net\n"
