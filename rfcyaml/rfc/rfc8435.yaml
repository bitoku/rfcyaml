- contents:
  - '                Parallel NFS (pNFS) Flexible File Layout

    '
  title: __initial_text__
- contents:
  - "Abstract\n   Parallel NFS (pNFS) allows a separation between the metadata (onto
    a\n   metadata server) and data (onto a storage device) for a file.  The\n   flexible
    file layout type is defined in this document as an extension\n   to pNFS that
    allows the use of storage devices that require only a\n   limited degree of interaction
    with the metadata server and use\n   already-existing protocols.  Client-side
    mirroring is also added to\n   provide replication of files.\n"
  title: Abstract
- contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This
    document is a product of the Internet Engineering Task Force\n   (IETF).  It represents
    the consensus of the IETF community.  It has\n   received public review and has
    been approved for publication by the\n   Internet Engineering Steering Group (IESG).
    \ Further information on\n   Internet Standards is available in Section 2 of RFC
    7841.\n   Information about the current status of this document, any errata,\n
    \  and how to provide feedback on it may be obtained at\n   https://www.rfc-editor.org/info/rfc8435.\n"
  title: Status of This Memo
- contents:
  - "Copyright Notice\n   Copyright (c) 2018 IETF Trust and the persons identified
    as the\n   document authors.  All rights reserved.\n   This document is subject
    to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n
    \  (https://trustee.ietf.org/license-info) in effect on the date of\n   publication
    of this document.  Please review these documents\n   carefully, as they describe
    your rights and restrictions with respect\n   to this document.  Code Components
    extracted from this document must\n   include Simplified BSD License text as described
    in Section 4.e of\n   the Trust Legal Provisions and are provided without warranty
    as\n   described in the Simplified BSD License.\n"
  title: Copyright Notice
- contents:
  - "Table of Contents\n   1. Introduction ....................................................3\n
    \     1.1. Definitions ................................................4\n      1.2.
    Requirements Language ......................................6\n   2. Coupling
    of Storage Devices .....................................6\n      2.1. LAYOUTCOMMIT
    ...............................................7\n      2.2. Fencing Clients from
    the Storage Device ....................7\n           2.2.1. Implementation Notes
    for Synthetic uids/gids ........8\n           2.2.2. Example of Using Synthetic
    uids/gids ................9\n      2.3. State and Locking Models ..................................10\n
    \          2.3.1. Loosely Coupled Locking Model ......................11\n           2.3.2.
    Tightly Coupled Locking Model ......................12\n   3. XDR Description
    of the Flexible File Layout Type ...............13\n      3.1. Code Components
    Licensing Notice ..........................14\n   4. Device Addressing and Discovery
    ................................16\n      4.1. ff_device_addr4 ...........................................16\n
    \     4.2. Storage Device Multipathing ...............................17\n   5.
    Flexible File Layout Type ......................................18\n      5.1.
    ff_layout4 ................................................19\n           5.1.1.
    Error Codes from LAYOUTGET .........................23\n           5.1.2. Client
    Interactions with FF_FLAGS_NO_IO_THRU_MDS ...23\n      5.2. LAYOUTCOMMIT ..............................................24\n
    \     5.3. Interactions between Devices and Layouts ..................24\n      5.4.
    Handling Version Errors ...................................24\n   6. Striping
    via Sparse Mapping ....................................25\n   7. Recovering from
    Client I/O Errors ..............................25\n   8. Mirroring ......................................................26\n
    \     8.1. Selecting a Mirror ........................................26\n      8.2.
    Writing to Mirrors ........................................27\n           8.2.1.
    Single Storage Device Updates Mirrors ..............27\n           8.2.2. Client
    Updates All Mirrors .........................27\n           8.2.3. Handling Write
    Errors ..............................28\n           8.2.4. Handling Write COMMITs
    .............................28\n      8.3. Metadata Server Resilvering of the
    File ...................29\n   9. Flexible File Layout Type Return ...............................29\n
    \     9.1. I/O Error Reporting .......................................30\n           9.1.1.
    ff_ioerr4 ..........................................30\n      9.2. Layout Usage
    Statistics ...................................31\n           9.2.1. ff_io_latency4
    .....................................31\n           9.2.2. ff_layoutupdate4 ...................................32\n
    \          9.2.3. ff_iostats4 ........................................33\n      9.3.
    ff_layoutreturn4 ..........................................34\n   10. Flexible
    File Layout Type LAYOUTERROR .........................35\n   11. Flexible File
    Layout Type LAYOUTSTATS .........................35\n   12. Flexible File Layout
    Type Creation Hint .......................35\n      12.1. ff_layouthint4 ...........................................35\n
    \  13. Recalling a Layout ............................................36\n      13.1.
    CB_RECALL_ANY ............................................36\n   14. Client Fencing
    ................................................37\n   15. Security Considerations
    .......................................37\n      15.1. RPCSEC_GSS and Security
    Services .........................39\n           15.1.1. Loosely Coupled ...................................39\n
    \          15.1.2. Tightly Coupled ...................................39\n   16.
    IANA Considerations ...........................................39\n   17. References
    ....................................................40\n      17.1. Normative
    References .....................................40\n      17.2. Informative References
    ...................................41\n   Acknowledgments ...................................................42\n
    \  Authors' Addresses ................................................42\n"
  title: Table of Contents
- contents:
  - "1.  Introduction\n   In Parallel NFS (pNFS), the metadata server returns layout
    type\n   structures that describe where file data is located.  There are\n   different
    layout types for different storage systems and methods of\n   arranging data on
    storage devices.  This document defines the\n   flexible file layout type used
    with file-based data servers that are\n   accessed using the NFS protocols: NFSv3
    [RFC1813], NFSv4.0 [RFC7530],\n   NFSv4.1 [RFC5661], and NFSv4.2 [RFC7862].\n
    \  To provide a global state model equivalent to that of the files\n   layout
    type, a back-end control protocol might be implemented between\n   the metadata
    server and NFSv4.1+ storage devices.  An implementation\n   can either define
    its own proprietary mechanism or it could define a\n   control protocol in a Standards
    Track document.  The requirements for\n   a control protocol are specified in
    [RFC5661] and clarified in\n   [RFC8434].\n   The control protocol described in
    this document is based on NFS.  It\n   does not provide for knowledge of stateids
    to be passed between the\n   metadata server and the storage devices.  Instead,
    the storage\n   devices are configured such that the metadata server has full
    access\n   rights to the data file system and then the metadata server uses\n
    \  synthetic ids to control client access to individual files.\n   In traditional
    mirroring of data, the server is responsible for\n   replicating, validating,
    and repairing copies of the data file.  With\n   client-side mirroring, the metadata
    server provides a layout that\n   presents the available mirrors to the client.
    \ The client then picks\n   a mirror to read from and ensures that all writes
    go to all mirrors.\n   The client only considers the write transaction to have
    succeeded if\n   all mirrors are successfully updated.  In case of error, the
    client\n   can use the LAYOUTERROR operation to inform the metadata server,\n
    \  which is then responsible for the repairing of the mirrored copies of\n   the
    file.\n"
  - contents:
    - "1.1.  Definitions\n   control communication requirements:  the specification
      for\n      information on layouts, stateids, file metadata, and file data\n
      \     that must be communicated between the metadata server and the\n      storage
      devices.  There is a separate set of requirements for each\n      layout type.\n
      \  control protocol:  the particular mechanism that an implementation of\n      a
      layout type would use to meet the control communication\n      requirement for
      that layout type.  This need not be a protocol as\n      normally understood.
      \ In some cases, the same protocol may be used\n      as a control protocol
      and storage protocol.\n   client-side mirroring:  a feature in which the client,
      not the\n      server, is responsible for updating all of the mirrored copies
      of\n      a layout segment.\n   (file) data:  that part of the file system object
      that contains the\n      data to be read or written.  It is the contents of
      the object\n      rather than the attributes of the object.\n   data server
      (DS):  a pNFS server that provides the file's data when\n      the file system
      object is accessed over a file-based protocol.\n   fencing:  the process by
      which the metadata server prevents the\n      storage devices from processing
      I/O from a specific client to a\n      specific file.\n   file layout type:
      \ a layout type in which the storage devices are\n      accessed via the NFS
      protocol (see Section 13 of [RFC5661]).\n   gid:  the group id, a numeric value
      that identifies to which group a\n      file belongs.\n   layout:  the information
      a client uses to access file data on a\n      storage device.  This information
      includes specification of the\n      protocol (layout type) and the identity
      of the storage devices to\n      be used.\n   layout iomode:  a grant of either
      read-only or read/write I/O to the\n      client.\n   layout segment:  a sub-division
      of a layout.  That sub-division might\n      be by the layout iomode (see Sections
      3.3.20 and 12.2.9 of\n      [RFC5661]), a striping pattern (see Section 13.3
      of [RFC5661]), or\n      requested byte range.\n   layout stateid:  a 128-bit
      quantity returned by a server that\n      uniquely defines the layout state
      provided by the server for a\n      specific layout that describes a layout
      type and file (see\n      Section 12.5.2 of [RFC5661]).  Further, Section 12.5.3
      of\n      [RFC5661] describes differences in handling between layout\n      stateids
      and other stateid types.\n   layout type:  a specification of both the storage
      protocol used to\n      access the data and the aggregation scheme used to lay
      out the\n      file data on the underlying storage devices.\n   loose coupling:
      \ when the control protocol is a storage protocol.\n   (file) metadata:  the
      part of the file system object that contains\n      various descriptive data
      relevant to the file object, as opposed\n      to the file data itself.  This
      could include the time of last\n      modification, access time, EOF position,
      etc.\n   metadata server (MDS):  the pNFS server that provides metadata\n      information
      for a file system object.  It is also responsible for\n      generating, recalling,
      and revoking layouts for file system\n      objects, for performing directory
      operations, and for performing\n      I/O operations to regular files when the
      clients direct these to\n      the metadata server itself.\n   mirror:  a copy
      of a layout segment.  Note that if one copy of the\n      mirror is updated,
      then all copies must be updated.\n   recalling a layout:  a graceful recall,
      via a callback, of a specific\n      layout by the metadata server to the client.
      \ Graceful here means\n      that the client would have the opportunity to flush
      any WRITEs,\n      etc., before returning the layout to the metadata server.\n
      \  revoking a layout:  an invalidation of a specific layout by the\n      metadata
      server.  Once revocation occurs, the metadata server will\n      not accept
      as valid any reference to the revoked layout, and a\n      storage device will
      not accept any client access based on the\n      layout.\n   resilvering:  the
      act of rebuilding a mirrored copy of a layout\n      segment from a known good
      copy of the layout segment.  Note that\n      this can also be done to create
      a new mirrored copy of the layout\n      segment.\n   rsize:  the data transfer
      buffer size used for READs.\n   stateid:  a 128-bit quantity returned by a server
      that uniquely\n      defines the set of locking-related state provided by the
      server.\n      Stateids may designate state related to open files, byte-range\n
      \     locks, delegations, or layouts.\n   storage device:  the target to which
      clients may direct I/O requests\n      when they hold an appropriate layout.
      \ See Section 2.1 of\n      [RFC8434] for further discussion of the difference
      between a data\n      server and a storage device.\n   storage protocol:  the
      protocol used by clients to do I/O operations\n      to the storage device.
      \ Each layout type specifies the set of\n      storage protocols.\n   tight
      coupling:  an arrangement in which the control protocol is one\n      designed
      specifically for control communication.  It may be either\n      a proprietary
      protocol adapted specifically to a particular\n      metadata server or a protocol
      based on a Standards Track document.\n   uid:  the user id, a numeric value
      that identifies which user owns a\n      file.\n   wsize:  the data transfer
      buffer size used for WRITEs.\n"
    title: 1.1.  Definitions
  - contents:
    - "1.2.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\",
      \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT
      RECOMMENDED\", \"MAY\", and\n   \"OPTIONAL\" in this document are to be interpreted
      as described in\n   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear
      in all\n   capitals, as shown here.\n"
    title: 1.2.  Requirements Language
  title: 1.  Introduction
- contents:
  - "2.  Coupling of Storage Devices\n   A server implementation may choose either
    a loosely coupled model or\n   a tightly coupled model between the metadata server
    and the storage\n   devices.  [RFC8434] describes the general problems facing
    pNFS\n   implementations.  This document details how the new flexible file\n   layout
    type addresses these issues.  To implement the tightly coupled\n   model, a control
    protocol has to be defined.  As the flexible file\n   layout imposes no special
    requirements on the client, the control\n   protocol will need to provide:\n   (1)
    \ management of both security and LAYOUTCOMMITs and\n   (2)  a global stateid
    model and management of these stateids.\n   When implementing the loosely coupled
    model, the only control\n   protocol will be a version of NFS, with no ability
    to provide a\n   global stateid model or to prevent clients from using layouts\n
    \  inappropriately.  To enable client use in that environment, this\n   document
    will specify how security, state, and locking are to be\n   managed.\n"
  - contents:
    - "2.1.  LAYOUTCOMMIT\n   Regardless of the coupling model, the metadata server
      has the\n   responsibility, upon receiving a LAYOUTCOMMIT (see Section 18.42
      of\n   [RFC5661]) to ensure that the semantics of pNFS are respected (see\n
      \  Section 3.1 of [RFC8434]).  These do include a requirement that data\n   written
      to a data storage device be stable before the occurrence of\n   the LAYOUTCOMMIT.\n
      \  It is the responsibility of the client to make sure the data file is\n   stable
      before the metadata server begins to query the storage devices\n   about the
      changes to the file.  If any WRITE to a storage device did\n   not result with
      stable_how equal to FILE_SYNC, a LAYOUTCOMMIT to the\n   metadata server MUST
      be preceded by a COMMIT to the storage devices\n   written to.  Note that if
      the client has not done a COMMIT to the\n   storage device, then the LAYOUTCOMMIT
      might not be synchronized to\n   the last WRITE operation to the storage device.\n"
    title: 2.1.  LAYOUTCOMMIT
  - contents:
    - "2.2.  Fencing Clients from the Storage Device\n   With loosely coupled storage
      devices, the metadata server uses\n   synthetic uids (user ids) and gids (group
      ids) for the data file,\n   where the uid owner of the data file is allowed
      read/write access and\n   the gid owner is allowed read-only access.  As part
      of the layout\n   (see ffds_user and ffds_group in Section 5.1), the client
      is provided\n   with the user and group to be used in the Remote Procedure Call
      (RPC)\n   [RFC5531] credentials needed to access the data file.  Fencing off
      of\n   clients is achieved by the metadata server changing the synthetic uid\n
      \  and/or gid owners of the data file on the storage device to\n   implicitly
      revoke the outstanding RPC credentials.  A client\n   presenting the wrong credential
      for the desired access will get an\n   NFS4ERR_ACCESS error.\n   With this loosely
      coupled model, the metadata server is not able to\n   fence off a single client;
      it is forced to fence off all clients.\n   However, as the other clients react
      to the fencing, returning their\n   layouts and trying to get new ones, the
      metadata server can hand out\n   a new uid and gid to allow access.\n   It is
      RECOMMENDED to implement common access control methods at the\n   storage device
      file system to allow only the metadata server root\n   (super user) access to
      the storage device and to set the owner of all\n   directories holding data
      files to the root user.  This approach\n   provides a practical model to enforce
      access control and fence off\n   cooperative clients, but it cannot protect
      against malicious clients;\n   hence, it provides a level of security equivalent
      to AUTH_SYS.  It is\n   RECOMMENDED that the communication between the metadata
      server and\n   storage device be secure from eavesdroppers and man-in-the-middle\n
      \  protocol tampering.  The security measure could be physical security\n   (e.g.,
      the servers are co-located in a physically secure area),\n   encrypted communications,
      or some other technique.\n   With tightly coupled storage devices, the metadata
      server sets the\n   user and group owners, mode bits, and Access Control List
      (ACL) of\n   the data file to be the same as the metadata file.  And the client\n
      \  must authenticate with the storage device and go through the same\n   authorization
      process it would go through via the metadata server.\n   In the case of tight
      coupling, fencing is the responsibility of the\n   control protocol and is not
      described in detail in this document.\n   However, implementations of the tightly
      coupled locking model (see\n   Section 2.3) will need a way to prevent access
      by certain clients to\n   specific files by invalidating the corresponding stateids
      on the\n   storage device.  In such a scenario, the client will be given an\n
      \  error of NFS4ERR_BAD_STATEID.\n   The client need not know the model used
      between the metadata server\n   and the storage device.  It need only react
      consistently to any\n   errors in interacting with the storage device.  It should
      both return\n   the layout and error to the metadata server and ask for a new
      layout.\n   At that point, the metadata server can either hand out a new layout,\n
      \  hand out no layout (forcing the I/O through it), or deny the client\n   further
      access to the file.\n"
    - contents:
      - "2.2.1.  Implementation Notes for Synthetic uids/gids\n   The selection method
        for the synthetic uids and gids to be used for\n   fencing in loosely coupled
        storage devices is strictly an\n   implementation issue.  That is, an administrator
        might restrict a\n   range of such ids available to the Lightweight Directory
        Access\n   Protocol (LDAP) 'uid' field [RFC4519].  The administrator might
        also\n   be able to choose an id that would never be used to grant access.\n
        \  Then, when the metadata server had a request to access a file, a\n   SETATTR
        would be sent to the storage device to set the owner and\n   group of the
        data file.  The user and group might be selected in a\n   round-robin fashion
        from the range of available ids.\n   Those ids would be sent back as ffds_user
        and ffds_group to the\n   client, who would present them as the RPC credentials
        to the storage\n   device.  When the client is done accessing the file and
        the metadata\n   server knows that no other client is accessing the file,
        it can reset\n   the owner and group to restrict access to the data file.\n
        \  When the metadata server wants to fence off a client, it changes the\n
        \  synthetic uid and/or gid to the restricted ids.  Note that using a\n   restricted
        id ensures that there is a change of owner and at least\n   one id available
        that never gets allowed access.\n   Under an AUTH_SYS security model, synthetic
        uids and gids of 0 SHOULD\n   be avoided.  These typically either grant super
        access to files on a\n   storage device or are mapped to an anonymous id.
        \ In the first case,\n   even if the data file is fenced, the client might
        still be able to\n   access the file.  In the second case, multiple ids might
        be mapped to\n   the anonymous ids.\n"
      title: 2.2.1.  Implementation Notes for Synthetic uids/gids
    - contents:
      - "2.2.2.  Example of Using Synthetic uids/gids\n   The user loghyr creates
        a file \"ompha.c\" on the metadata server,\n   which then creates a corresponding
        data file on the storage device.\n   The metadata server entry may look like:\n
        \  -rw-r--r--    1 loghyr  staff    1697 Dec  4 11:31 ompha.c\n   On the storage
        device, the file may be assigned some unpredictable\n   synthetic uid/gid
        to deny access:\n   -rw-r-----    1 19452   28418    1697 Dec  4 11:31 data_ompha.c\n
        \  When the file is opened on a client and accessed, the user will try\n   to
        get a layout for the data file.  Since the layout knows nothing\n   about
        the user (and does not care), it does not matter whether the\n   user loghyr
        or garbo opens the file.  The client has to present an\n   uid of 19452 to
        get write permission.  If it presents any other value\n   for the uid, then
        it must give a gid of 28418 to get read access.\n   Further, if the metadata
        server decides to fence the file, it should\n   change the uid and/or gid
        such that these values neither match\n   earlier values for that file nor
        match a predictable change based on\n   an earlier fencing.\n   -rw-r-----
        \   1 19453   28419    1697 Dec  4 11:31 data_ompha.c\n   The set of synthetic
        gids on the storage device should be selected\n   such that there is no mapping
        in any of the name services used by the\n   storage device, i.e., each group
        should have no members.\n   If the layout segment has an iomode of LAYOUTIOMODE4_READ,
        then the\n   metadata server should return a synthetic uid that is not set
        on the\n   storage device.  Only the synthetic gid would be valid.\n   The
        client is thus solely responsible for enforcing file permissions\n   in a
        loosely coupled model.  To allow loghyr write access, it will\n   send an
        RPC to the storage device with a credential of 1066:1067.  To\n   allow garbo
        read access, it will send an RPC to the storage device\n   with a credential
        of 1067:1067.  The value of the uid does not matter\n   as long as it is not
        the synthetic uid granted when getting the\n   layout.\n   While pushing the
        enforcement of permission checking onto the client\n   may seem to weaken
        security, the client may already be responsible\n   for enforcing permissions
        before modifications are sent to a server.\n   With cached writes, the client
        is always responsible for tracking who\n   is modifying a file and making
        sure to not coalesce requests from\n   multiple users into one request.\n"
      title: 2.2.2.  Example of Using Synthetic uids/gids
    title: 2.2.  Fencing Clients from the Storage Device
  - contents:
    - "2.3.  State and Locking Models\n   An implementation can always be deployed
      as a loosely coupled model.\n   There is, however, no way for a storage device
      to indicate over an\n   NFS protocol that it can definitively participate in
      a tightly\n   coupled model:\n   o  Storage devices implementing the NFSv3 and
      NFSv4.0 protocols are\n      always treated as loosely coupled.\n   o  NFSv4.1+
      storage devices that do not return the\n      EXCHGID4_FLAG_USE_PNFS_DS flag
      set to EXCHANGE_ID are indicating\n      that they are to be treated as loosely
      coupled.  From the locking\n      viewpoint, they are treated in the same way
      as NFSv4.0 storage\n      devices.\n   o  NFSv4.1+ storage devices that do identify
      themselves with the\n      EXCHGID4_FLAG_USE_PNFS_DS flag set to EXCHANGE_ID
      can potentially\n      be tightly coupled.  They would use a back-end control
      protocol to\n      implement the global stateid model as described in [RFC5661].\n
      \  A storage device would have to be either discovered or advertised\n   over
      the control protocol to enable a tightly coupled model.\n"
    - contents:
      - "2.3.1.  Loosely Coupled Locking Model\n   When locking-related operations
        are requested, they are primarily\n   dealt with by the metadata server, which
        generates the appropriate\n   stateids.  When an NFSv4 version is used as
        the data access protocol,\n   the metadata server may make stateid-related
        requests of the storage\n   devices.  However, it is not required to do so,
        and the resulting\n   stateids are known only to the metadata server and the
        storage\n   device.\n   Given this basic structure, locking-related operations
        are handled as\n   follows:\n   o  OPENs are dealt with by the metadata server.
        \ Stateids are\n      selected by the metadata server and associated with
        the client ID\n      describing the client's connection to the metadata server.
        \ The\n      metadata server may need to interact with the storage device
        to\n      locate the file to be opened, but no locking-related functionality\n
        \     need be used on the storage device.\n      OPEN_DOWNGRADE and CLOSE
        only require local execution on the\n      metadata server.\n   o  Advisory
        byte-range locks can be implemented locally on the\n      metadata server.
        \ As in the case of OPENs, the stateids associated\n      with byte-range
        locks are assigned by the metadata server and only\n      used on the metadata
        server.\n   o  Delegations are assigned by the metadata server that initiates\n
        \     recalls when conflicting OPENs are processed.  No storage device\n      involvement
        is required.\n   o  TEST_STATEID and FREE_STATEID are processed locally on
        the\n      metadata server, without storage device involvement.\n   All I/O
        operations to the storage device are done using the anonymous\n   stateid.
        \ Thus, the storage device has no information about the\n   openowner and
        lockowner responsible for issuing a particular I/O\n   operation.  As a result:\n
        \  o  Mandatory byte-range locking cannot be supported because the\n      storage
        device has no way of distinguishing I/O done on behalf of\n      the lock
        owner from those done by others.\n   o  Enforcement of share reservations
        is the responsibility of the\n      client.  Even though I/O is done using
        the anonymous stateid, the\n      client must ensure that it has a valid stateid
        associated with the\n      openowner.\n   In the event that a stateid is revoked,
        the metadata server is\n   responsible for preventing client access, since
        it has no way of\n   being sure that the client is aware that the stateid
        in question has\n   been revoked.\n   As the client never receives a stateid
        generated by a storage device,\n   there is no client lease on the storage
        device and no prospect of\n   lease expiration, even when access is via NFSv4
        protocols.  Clients\n   will have leases on the metadata server.  In dealing
        with lease\n   expiration, the metadata server may need to use fencing to
        prevent\n   revoked stateids from being relied upon by a client unaware of
        the\n   fact that they have been revoked.\n"
      title: 2.3.1.  Loosely Coupled Locking Model
    - contents:
      - "2.3.2.  Tightly Coupled Locking Model\n   When locking-related operations
        are requested, they are primarily\n   dealt with by the metadata server, which
        generates the appropriate\n   stateids.  These stateids must be made known
        to the storage device\n   using control protocol facilities, the details of
        which are not\n   discussed in this document.\n   Given this basic structure,
        locking-related operations are handled as\n   follows:\n   o  OPENs are dealt
        with primarily on the metadata server.  Stateids\n      are selected by the
        metadata server and associated with the client\n      ID describing the client's
        connection to the metadata server.  The\n      metadata server needs to interact
        with the storage device to\n      locate the file to be opened and to make
        the storage device aware\n      of the association between the metadata-server-chosen
        stateid and\n      the client and openowner that it represents.\n      OPEN_DOWNGRADE
        and CLOSE are executed initially on the metadata\n      server, but the state
        change made must be propagated to the\n      storage device.\n   o  Advisory
        byte-range locks can be implemented locally on the\n      metadata server.
        \ As in the case of OPENs, the stateids associated\n      with byte-range
        locks are assigned by the metadata server and are\n      available for use
        on the metadata server.  Because I/O operations\n      are allowed to present
        lock stateids, the metadata server needs\n      the ability to make the storage
        device aware of the association\n      between the metadata-server-chosen
        stateid and the corresponding\n      open stateid it is associated with.\n
        \  o  Mandatory byte-range locks can be supported when both the metadata\n
        \     server and the storage devices have the appropriate support.  As\n      in
        the case of advisory byte-range locks, these are assigned by\n      the metadata
        server and are available for use on the metadata\n      server.  To enable
        mandatory lock enforcement on the storage\n      device, the metadata server
        needs the ability to make the storage\n      device aware of the association
        between the metadata-server-chosen\n      stateid and the client, openowner,
        and lock (i.e., lockowner,\n      byte-range, and lock-type) that it represents.
        \ Because I/O\n      operations are allowed to present lock stateids, this
        information\n      needs to be propagated to all storage devices to which
        I/O might\n      be directed rather than only to storage device that contain
        the\n      locked region.\n   o  Delegations are assigned by the metadata
        server that initiates\n      recalls when conflicting OPENs are processed.
        \ Because I/O\n      operations are allowed to present delegation stateids,
        the\n      metadata server requires the ability (1) to make the storage\n
        \     device aware of the association between the metadata-server-chosen\n
        \     stateid and the filehandle and delegation type it represents and\n      (2)
        to break such an association.\n   o  TEST_STATEID is processed locally on
        the metadata server, without\n      storage device involvement.\n   o  FREE_STATEID
        is processed on the metadata server, but the metadata\n      server requires
        the ability to propagate the request to the\n      corresponding storage devices.\n
        \  Because the client will possess and use stateids valid on the storage\n
        \  device, there will be a client lease on the storage device, and the\n   possibility
        of lease expiration does exist.  The best approach for\n   the storage device
        is to retain these locks as a courtesy.  However,\n   if it does not do so,
        control protocol facilities need to provide the\n   means to synchronize lock
        state between the metadata server and\n   storage device.\n   Clients will
        also have leases on the metadata server that are subject\n   to expiration.
        \ In dealing with lease expiration, the metadata server\n   would be expected
        to use control protocol facilities enabling it to\n   invalidate revoked stateids
        on the storage device.  In the event the\n   client is not responsive, the
        metadata server may need to use fencing\n   to prevent revoked stateids from
        being acted upon by the storage\n   device.\n"
      title: 2.3.2.  Tightly Coupled Locking Model
    title: 2.3.  State and Locking Models
  title: 2.  Coupling of Storage Devices
- contents:
  - "3.  XDR Description of the Flexible File Layout Type\n   This document contains
    the External Data Representation (XDR)\n   [RFC4506] description of the flexible
    file layout type.  The XDR\n   description is embedded in this document in a way
    that makes it\n   simple for the reader to extract into a ready-to-compile form.
    \ The\n   reader can feed this document into the following shell script to\n   produce
    the machine-readable XDR description of the flexible file\n   layout type:\n   <CODE
    BEGINS>\n   #!/bin/sh\n   grep '^ *///' $* | sed 's?^ */// ??' | sed 's?^ *///$??'\n
    \  <CODE ENDS>\n   That is, if the above script is stored in a file called \"extract.sh\"\n
    \  and this document is in a file called \"spec.txt\", then the reader can\n   do:\n
    \  sh extract.sh < spec.txt > flex_files_prot.x\n   The effect of the script is
    to remove leading white space from each\n   line, plus a sentinel sequence of
    \"///\".\n   The embedded XDR file header follows.  Subsequent XDR descriptions\n
    \  with the sentinel sequence are embedded throughout the document.\n   Note that
    the XDR code contained in this document depends on types\n   from the NFSv4.1
    nfs4_prot.x file [RFC5662].  This includes both nfs\n   types that end with a
    4, such as offset4, length4, etc., as well as\n   more generic types such as uint32_t
    and uint64_t.\n"
  - contents:
    - "3.1.  Code Components Licensing Notice\n   Both the XDR description and the
      scripts used for extracting the XDR\n   description are Code Components as described
      in Section 4 of \"Trust\n   Legal Provisions (TLP)\" [LEGAL].  These Code Components
      are licensed\n   according to the terms of that document.\n   <CODE BEGINS>\n
      \  /// /*\n   ///  * Copyright (c) 2018 IETF Trust and the persons identified\n
      \  ///  * as authors of the code.  All rights reserved.\n   ///  *\n   ///  *
      Redistribution and use in source and binary forms, with\n   ///  * or without
      modification, are permitted provided that the\n   ///  * following conditions
      are met:\n   ///  *\n   ///  * - Redistributions of source code must retain
      the above\n   ///  *   copyright notice, this list of conditions and the\n   ///
      \ *   following disclaimer.\n   ///  *\n   ///  * - Redistributions in binary
      form must reproduce the above\n   ///  *   copyright notice, this list of conditions
      and the\n   ///  *   following disclaimer in the documentation and/or other\n
      \  ///  *   materials provided with the distribution.\n   ///  *\n   ///  *
      - Neither the name of Internet Society, IETF or IETF\n   ///  *   Trust, nor
      the names of specific contributors, may be\n   ///  *   used to endorse or promote
      products derived from this\n   ///  *   software without specific prior written
      permission.\n   ///  *\n   ///  *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT
      HOLDERS\n   ///  *   AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED\n
      \  ///  *   WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n   ///  *   IMPLIED
      WARRANTIES OF MERCHANTABILITY AND FITNESS\n   ///  *   FOR A PARTICULAR PURPOSE
      ARE DISCLAIMED.  IN NO\n   ///  *   EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
      BE\n   ///  *   LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n   ///
      \ *   EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n   ///  *   NOT LIMITED
      TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n   ///  *   SERVICES; LOSS OF USE, DATA,
      OR PROFITS; OR BUSINESS\n   ///  *   INTERRUPTION) HOWEVER CAUSED AND ON ANY
      THEORY OF\n   ///  *   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n   ///
      \ *   OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING\n   ///  *   IN ANY
      WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n   ///  *   ADVISED OF THE POSSIBILITY
      OF SUCH DAMAGE.\n   ///  *\n   ///  * This code was derived from RFC 8435.\n
      \  ///  * Please reproduce this note if possible.\n   ///  */\n   ///\n   ///
      /*\n   ///  * flex_files_prot.x\n   ///  */\n   ///\n   /// /*\n   ///  * The
      following include statements are for example only.\n   ///  * The actual XDR
      definition files are generated separately\n   ///  * and independently and are
      likely to have a different name.\n   ///  * %#include <nfsv42.x>\n   ///  *
      %#include <rpc_prot.x>\n   ///  */\n   ///\n   <CODE ENDS>\n"
    title: 3.1.  Code Components Licensing Notice
  title: 3.  XDR Description of the Flexible File Layout Type
- contents:
  - "4.  Device Addressing and Discovery\n   Data operations to a storage device require
    the client to know the\n   network address of the storage device.  The NFSv4.1+
    GETDEVICEINFO\n   operation (Section 18.40 of [RFC5661]) is used by the client
    to\n   retrieve that information.\n"
  - contents:
    - "4.1.  ff_device_addr4\n   The ff_device_addr4 data structure is returned by
      the server as the\n   layout-type-specific opaque field da_addr_body in the
      device_addr4\n   structure by a successful GETDEVICEINFO operation.\n   <CODE
      BEGINS>\n   /// struct ff_device_versions4 {\n   ///         uint32_t        ffdv_version;\n
      \  ///         uint32_t        ffdv_minorversion;\n   ///         uint32_t        ffdv_rsize;\n
      \  ///         uint32_t        ffdv_wsize;\n   ///         bool            ffdv_tightly_coupled;\n
      \  /// };\n   ///\n   /// struct ff_device_addr4 {\n   ///         multipath_list4
      \    ffda_netaddrs;\n   ///         ff_device_versions4 ffda_versions<>;\n   ///
      };\n   ///\n   <CODE ENDS>\n   The ffda_netaddrs field is used to locate the
      storage device.  It\n   MUST be set by the server to a list holding one or more
      of the device\n   network addresses.\n   The ffda_versions array allows the
      metadata server to present choices\n   as to NFS version, minor version, and
      coupling strength to the\n   client.  The ffdv_version and ffdv_minorversion
      represent the NFS\n   protocol to be used to access the storage device.  This
      layout\n   specification defines the semantics for ffdv_versions 3 and 4.  If\n
      \  ffdv_version equals 3, then the server MUST set ffdv_minorversion to\n   0
      and ffdv_tightly_coupled to false.  The client MUST then access the\n   storage
      device using the NFSv3 protocol [RFC1813].  If ffdv_version\n   equals 4, then
      the server MUST set ffdv_minorversion to one of the\n   NFSv4 minor version
      numbers, and the client MUST access the storage\n   device using NFSv4 with
      the specified minor version.\n   Note that while the client might determine
      that it cannot use any of\n   the configured combinations of ffdv_version, ffdv_minorversion,
      and\n   ffdv_tightly_coupled, when it gets the device list from the metadata\n
      \  server, there is no way to indicate to the metadata server as to\n   which
      device it is version incompatible.  However, if the client\n   waits until it
      retrieves the layout from the metadata server, it can\n   at that time clearly
      identify the storage device in question (see\n   Section 5.4).\n   The ffdv_rsize
      and ffdv_wsize are used to communicate the maximum\n   rsize and wsize supported
      by the storage device.  As the storage\n   device can have a different rsize
      or wsize than the metadata server,\n   the ffdv_rsize and ffdv_wsize allow the
      metadata server to\n   communicate that information on behalf of the storage
      device.\n   ffdv_tightly_coupled informs the client as to whether or not the\n
      \  metadata server is tightly coupled with the storage devices.  Note\n   that
      even if the data protocol is at least NFSv4.1, it may still be\n   the case
      that there is loose coupling in effect.  If\n   ffdv_tightly_coupled is not
      set, then the client MUST commit writes\n   to the storage devices for the file
      before sending a LAYOUTCOMMIT to\n   the metadata server.  That is, the writes
      MUST be committed by the\n   client to stable storage via issuing WRITEs with
      stable_how ==\n   FILE_SYNC or by issuing a COMMIT after WRITEs with stable_how
      !=\n   FILE_SYNC (see Section 3.3.7 of [RFC1813]).\n"
    title: 4.1.  ff_device_addr4
  - contents:
    - "4.2.  Storage Device Multipathing\n   The flexible file layout type supports
      multipathing to multiple\n   storage device addresses.  Storage-device-level
      multipathing is used\n   for bandwidth scaling via trunking and for higher availability
      of use\n   in the event of a storage device failure.  Multipathing allows the\n
      \  client to switch to another storage device address that may be that\n   of
      another storage device that is exporting the same data stripe\n   unit, without
      having to contact the metadata server for a new layout.\n   To support storage
      device multipathing, ffda_netaddrs contains an\n   array of one or more storage
      device network addresses.  This array\n   (data type multipath_list4) represents
      a list of storage devices\n   (each identified by a network address), with the
      possibility that\n   some storage device will appear in the list multiple times.\n
      \  The client is free to use any of the network addresses as a\n   destination
      to send storage device requests.  If some network\n   addresses are less desirable
      paths to the data than others, then the\n   metadata server SHOULD NOT include
      those network addresses in\n   ffda_netaddrs.  If less desirable network addresses
      exist to provide\n   failover, the RECOMMENDED method to offer the addresses
      is to provide\n   them in a replacement device-ID-to-device-address mapping
      or a\n   replacement device ID.  When a client finds no response from the\n
      \  storage device using all addresses available in ffda_netaddrs, it\n   SHOULD
      send a GETDEVICEINFO to attempt to replace the existing\n   device-ID-to-device-address
      mappings.  If the metadata server detects\n   that all network paths represented
      by ffda_netaddrs are unavailable,\n   the metadata server SHOULD send a CB_NOTIFY_DEVICEID
      (if the client\n   has indicated it wants device ID notifications for changed
      device\n   IDs) to change the device-ID-to-device-address mappings to the\n
      \  available addresses.  If the device ID itself will be replaced, the\n   metadata
      server SHOULD recall all layouts with the device ID and thus\n   force the client
      to get new layouts and device ID mappings via\n   LAYOUTGET and GETDEVICEINFO.\n
      \  Generally, if two network addresses appear in ffda_netaddrs, they\n   will
      designate the same storage device.  When the storage device is\n   accessed
      over NFSv4.1 or a higher minor version, the two storage\n   device addresses
      will support the implementation of client ID or\n   session trunking (the latter
      is RECOMMENDED) as defined in [RFC5661].\n   The two storage device addresses
      will share the same server owner or\n   major ID of the server owner.  It is
      not always necessary for the two\n   storage device addresses to designate the
      same storage device with\n   trunking being used.  For example, the data could
      be read-only, and\n   the data consist of exact replicas.\n"
    title: 4.2.  Storage Device Multipathing
  title: 4.  Device Addressing and Discovery
- contents:
  - "5.  Flexible File Layout Type\n   The original layouttype4 introduced in [RFC5662]
    is modified to be:\n   <CODE BEGINS>\n       enum layouttype4 {\n           LAYOUT4_NFSV4_1_FILES
    \  = 1,\n           LAYOUT4_OSD2_OBJECTS    = 2,\n           LAYOUT4_BLOCK_VOLUME
    \   = 3,\n           LAYOUT4_FLEX_FILES      = 4\n       };\n       struct layout_content4
    {\n           layouttype4             loc_type;\n           opaque                  loc_body<>;\n
    \      };\n       struct layout4 {\n           offset4                 lo_offset;\n
    \          length4                 lo_length;\n           layoutiomode4           lo_iomode;\n
    \          layout_content4         lo_content;\n       };\n   <CODE ENDS>\n   This
    document defines structures associated with the layouttype4\n   value LAYOUT4_FLEX_FILES.
    \ [RFC5661] specifies the loc_body structure\n   as an XDR type \"opaque\".  The
    opaque layout is uninterpreted by the\n   generic pNFS client layers but is interpreted
    by the flexible file\n   layout type implementation.  This section defines the
    structure of\n   this otherwise opaque value, ff_layout4.\n"
  - contents:
    - "5.1.  ff_layout4\n   <CODE BEGINS>\n   /// const FF_FLAGS_NO_LAYOUTCOMMIT   =
      0x00000001;\n   /// const FF_FLAGS_NO_IO_THRU_MDS    = 0x00000002;\n   /// const
      FF_FLAGS_NO_READ_IO        = 0x00000004;\n   /// const FF_FLAGS_WRITE_ONE_MIRROR
      \ = 0x00000008;\n   /// typedef uint32_t            ff_flags4;\n   ///\n   ///
      struct ff_data_server4 {\n   ///     deviceid4               ffds_deviceid;\n
      \  ///     uint32_t                ffds_efficiency;\n   ///     stateid4                ffds_stateid;\n
      \  ///     nfs_fh4                 ffds_fh_vers<>;\n   ///     fattr4_owner
      \           ffds_user;\n   ///     fattr4_owner_group      ffds_group;\n   ///
      };\n   ///\n   /// struct ff_mirror4 {\n   ///     ff_data_server4         ffm_data_servers<>;\n
      \  /// };\n   ///\n   /// struct ff_layout4 {\n   ///     length4                 ffl_stripe_unit;\n
      \  ///     ff_mirror4              ffl_mirrors<>;\n   ///     ff_flags4               ffl_flags;\n
      \  ///     uint32_t                ffl_stats_collect_hint;\n   /// };\n   ///\n
      \  <CODE ENDS>\n   The ff_layout4 structure specifies a layout in that portion
      of the\n   data file described in the current layout segment.  It is either
      a\n   single instance or a set of mirrored copies of that portion of the\n   data
      file.  When mirroring is in effect, it protects against loss of\n   data in
      layout segments.\n   While not explicitly shown in the above XDR, each layout4
      element\n   returned in the logr_layout array of LAYOUTGET4res (see\n   Section
      18.43.2 of [RFC5661]) describes a layout segment.  Hence,\n   each ff_layout4
      also describes a layout segment.  It is possible that\n   the file is concatenated
      from more than one layout segment.  Each\n   layout segment MAY represent different
      striping parameters.\n   The ffl_stripe_unit field is the stripe unit size in
      use for the\n   current layout segment.  The number of stripes is given inside
      each\n   mirror by the number of elements in ffm_data_servers.  If the number\n
      \  of stripes is one, then the value for ffl_stripe_unit MUST default to\n   zero.
      \ The only supported mapping scheme is sparse and is detailed in\n   Section
      6.  Note that there is an assumption here that both the\n   stripe unit size
      and the number of stripes are the same across all\n   mirrors.\n   The ffl_mirrors
      field is the array of mirrored storage devices that\n   provide the storage
      for the current stripe; see Figure 1.\n   The ffl_stats_collect_hint field provides
      a hint to the client on how\n   often the server wants it to report LAYOUTSTATS
      for a file.  The time\n   is in seconds.\n                      +-----------+\n
      \                     |           |\n                      |           |\n                      |
      \  File    |\n                      |           |\n                      |           |\n
      \                     +-----+-----+\n                            |\n               +------------+------------+\n
      \              |                         |\n          +----+-----+             +-----+----+\n
      \         | Mirror 1 |             | Mirror 2 |\n          +----+-----+             +-----+----+\n
      \              |                         |\n          +-----------+            +-----------+\n
      \         |+-----------+           |+-----------+\n          ||+-----------+
      \         ||+-----------+\n          +||  Storage  |          +||  Storage  |\n
      \          +|  Devices  |           +|  Devices  |\n            +-----------+
      \           +-----------+\n                           Figure 1\n   The ffs_mirrors
      field represents an array of state information for\n   each mirrored copy of
      the current layout segment.  Each element is\n   described by a ff_mirror4 type.\n
      \  ffds_deviceid provides the deviceid of the storage device holding the\n   data
      file.\n   ffds_fh_vers is an array of filehandles of the data file matching
      the\n   available NFS versions on the given storage device.  There MUST be\n
      \  exactly as many elements in ffds_fh_vers as there are in\n   ffda_versions.
      \ Each element of the array corresponds to a particular\n   combination of ffdv_version,
      ffdv_minorversion, and\n   ffdv_tightly_coupled provided for the device.  The
      array allows for\n   server implementations that have different filehandles
      for different\n   combinations of version, minor version, and coupling strength.
      \ See\n   Section 5.4 for how to handle versioning issues between the client\n
      \  and storage devices.\n   For tight coupling, ffds_stateid provides the stateid
      to be used by\n   the client to access the file.  For loose coupling and an
      NFSv4\n   storage device, the client will have to use an anonymous stateid to\n
      \  perform I/O on the storage device.  With no control protocol, the\n   metadata
      server stateid cannot be used to provide a global stateid\n   model.  Thus,
      the server MUST set the ffds_stateid to be the\n   anonymous stateid.\n   This
      specification of the ffds_stateid restricts both models for\n   NFSv4.x storage
      protocols:\n   loosely coupled model:  the stateid has to be an anonymous stateid\n
      \  tightly coupled model:  the stateid has to be a global stateid\n   A number
      of issues stem from a mismatch between the fact that\n   ffds_stateid is defined
      as a single item while ffds_fh_vers is\n   defined as an array.  It is possible
      for each open file on the\n   storage device to require its own open stateid.
      \ Because there are\n   established loosely coupled implementations of the version
      of the\n   protocol described in this document, such potential issues have not\n
      \  been addressed here.  It is possible for future layout types to be\n   defined
      that address these issues, should it become important to\n   provide multiple
      stateids for the same underlying file.\n   For loosely coupled storage devices,
      ffds_user and ffds_group provide\n   the synthetic user and group to be used
      in the RPC credentials that\n   the client presents to the storage device to
      access the data files.\n   For tightly coupled storage devices, the user and
      group on the\n   storage device will be the same as on the metadata server;
      that is,\n   if ffdv_tightly_coupled (see Section 4.1) is set, then the client\n
      \  MUST ignore both ffds_user and ffds_group.\n   The allowed values for both
      ffds_user and ffds_group are specified as\n   owner and owner_group, respectively,
      in Section 5.9 of [RFC5661].\n   For NFSv3 compatibility, user and group strings
      that consist of\n   decimal numeric values with no leading zeros can be given
      a special\n   interpretation by clients and servers that choose to provide such\n
      \  support.  The receiver may treat such a user or group string as\n   representing
      the same user as would be represented by an NFSv3 uid or\n   gid having the
      corresponding numeric value.  Note that if using\n   Kerberos for security,
      the expectation is that these values will be a\n   name@domain string.\n   ffds_efficiency
      describes the metadata server's evaluation as to the\n   effectiveness of each
      mirror.  Note that this is per layout and not\n   per device as the metric may
      change due to perceived load,\n   availability to the metadata server, etc.
      \ Higher values denote\n   higher perceived utility.  The way the client can
      select the best\n   mirror to access is discussed in Section 8.1.\n   ffl_flags
      is a bitmap that allows the metadata server to inform the\n   client of particular
      conditions that may result from more or less\n   tight coupling of the storage
      devices.\n   FF_FLAGS_NO_LAYOUTCOMMIT:  can be set to indicate that the client
      is\n      not required to send LAYOUTCOMMIT to the metadata server.\n   FF_FLAGS_NO_IO_THRU_MDS:
      \ can be set to indicate that the client\n      should not send I/O operations
      to the metadata server.  That is,\n      even if the client could determine
      that there was a network\n      disconnect to a storage device, the client should
      not try to proxy\n      the I/O through the metadata server.\n   FF_FLAGS_NO_READ_IO:
      \ can be set to indicate that the client should\n      not send READ requests
      with the layouts of iomode\n      LAYOUTIOMODE4_RW.  Instead, it should request
      a layout of iomode\n      LAYOUTIOMODE4_READ from the metadata server.\n   FF_FLAGS_WRITE_ONE_MIRROR:
      \ can be set to indicate that the client\n      only needs to update one of
      the mirrors (see Section 8.2).\n"
    - contents:
      - "5.1.1.  Error Codes from LAYOUTGET\n   [RFC5661] provides little guidance
        as to how the client is to proceed\n   with a LAYOUTGET that returns an error
        of either\n   NFS4ERR_LAYOUTTRYLATER, NFS4ERR_LAYOUTUNAVAILABLE, and NFS4ERR_DELAY.\n
        \  Within the context of this document:\n   NFS4ERR_LAYOUTUNAVAILABLE:  there
        is no layout available and the I/O\n      is to go to the metadata server.
        \ Note that it is possible to have\n      had a layout before a recall and
        not after.\n   NFS4ERR_LAYOUTTRYLATER:  there is some issue preventing the
        layout\n      from being granted.  If the client already has an appropriate\n
        \     layout, it should continue with I/O to the storage devices.\n   NFS4ERR_DELAY:
        \ there is some issue preventing the layout from being\n      granted.  If
        the client already has an appropriate layout, it\n      should not continue
        with I/O to the storage devices.\n"
      title: 5.1.1.  Error Codes from LAYOUTGET
    - contents:
      - "5.1.2.  Client Interactions with FF_FLAGS_NO_IO_THRU_MDS\n   Even if the
        metadata server provides the FF_FLAGS_NO_IO_THRU_MDS\n   flag, the client
        can still perform I/O to the metadata server.  The\n   flag functions as a
        hint.  The flag indicates to the client that the\n   metadata server prefers
        to separate the metadata I/O from the data I/\n   O, most likely for performance
        reasons.\n"
      title: 5.1.2.  Client Interactions with FF_FLAGS_NO_IO_THRU_MDS
    title: 5.1.  ff_layout4
  - contents:
    - "5.2.  LAYOUTCOMMIT\n   The flexible file layout does not use lou_body inside
      the\n   loca_layoutupdate argument to LAYOUTCOMMIT.  If lou_type is\n   LAYOUT4_FLEX_FILES,
      the lou_body field MUST have a zero length (see\n   Section 18.42.1 of [RFC5661]).\n"
    title: 5.2.  LAYOUTCOMMIT
  - contents:
    - "5.3.  Interactions between Devices and Layouts\n   In [RFC5661], the file layout
      type is defined such that the\n   relationship between multipathing and filehandles
      can result in\n   either 0, 1, or N filehandles (see Section 13.3).  Some rationales\n
      \  for this are clustered servers that share the same filehandle or\n   allow
      for multiple read-only copies of the file on the same storage\n   device.  In
      the flexible file layout type, while there is an array of\n   filehandles, they
      are independent of the multipathing being used.  If\n   the metadata server
      wants to provide multiple read-only copies of the\n   same file on the same
      storage device, then it should provide multiple\n   mirrored instances, each
      with a different ff_device_addr4.  The\n   client can then determine that, since
      the each of the ffds_fh_vers\n   are different, there are multiple copies of
      the file for the current\n   layout segment available.\n"
    title: 5.3.  Interactions between Devices and Layouts
  - contents:
    - "5.4.  Handling Version Errors\n   When the metadata server provides the ffda_versions
      array in the\n   ff_device_addr4 (see Section 4.1), the client is able to determine\n
      \  whether or not it can access a storage device with any of the\n   supplied
      combinations of ffdv_version, ffdv_minorversion, and\n   ffdv_tightly_coupled.
      \ However, due to the limitations of reporting\n   errors in GETDEVICEINFO (see
      Section 18.40 in [RFC5661]), the client\n   is not able to specify which specific
      device it cannot communicate\n   with over one of the provided ffdv_version
      and ffdv_minorversion\n   combinations.  Using ff_ioerr4 (see Section 9.1.1)
      inside either the\n   LAYOUTRETURN (see Section 18.44 of [RFC5661]) or the LAYOUTERROR
      (see\n   Section 15.6 of [RFC7862] and Section 10 of this document), the\n   client
      can isolate the problematic storage device.\n   The error code to return for
      LAYOUTRETURN and/or LAYOUTERROR is\n   NFS4ERR_MINOR_VERS_MISMATCH.  It does
      not matter whether the mismatch\n   is a major version (e.g., client can use
      NFSv3 but not NFSv4) or\n   minor version (e.g., client can use NFSv4.1 but
      not NFSv4.2), the\n   error indicates that for all the supplied combinations
      for\n   ffdv_version and ffdv_minorversion, the client cannot communicate\n
      \  with the storage device.  The client can retry the GETDEVICEINFO to\n   see
      if the metadata server can provide a different combination, or it\n   can fall
      back to doing the I/O through the metadata server.\n"
    title: 5.4.  Handling Version Errors
  title: 5.  Flexible File Layout Type
- contents:
  - "6.  Striping via Sparse Mapping\n   While other layout types support both dense
    and sparse mapping of\n   logical offsets to physical offsets within a file (see,
    for example,\n   Section 13.4 of [RFC5661]), the flexible file layout type only\n
    \  supports a sparse mapping.\n   With sparse mappings, the logical offset within
    a file (L) is also\n   the physical offset on the storage device.  As detailed
    in\n   Section 13.4.4 of [RFC5661], this results in holes across each\n   storage
    device that does not contain the current stripe index.\n   L: logical offset within
    the file\n   W: stripe width\n       W = number of elements in ffm_data_servers\n
    \  S: number of bytes in a stripe\n       S = W * ffl_stripe_unit\n   N: stripe
    number\n       N = L / S\n"
  title: 6.  Striping via Sparse Mapping
- contents:
  - "7.  Recovering from Client I/O Errors\n   The pNFS client may encounter errors
    when directly accessing the\n   storage devices.  However, it is the responsibility
    of the metadata\n   server to recover from the I/O errors.  When the LAYOUT4_FLEX_FILES\n
    \  layout type is used, the client MUST report the I/O errors to the\n   server
    at LAYOUTRETURN time using the ff_ioerr4 structure (see\n   Section 9.1.1).\n
    \  The metadata server analyzes the error and determines the required\n   recovery
    operations such as recovering media failures or\n   reconstructing missing data
    files.\n   The metadata server MUST recall any outstanding layouts to allow it\n
    \  exclusive write access to the stripes being recovered and to prevent\n   other
    clients from hitting the same error condition.  In these cases,\n   the server
    MUST complete recovery before handing out any new layouts\n   to the affected
    byte ranges.\n   Although the client implementation has the option to propagate
    a\n   corresponding error to the application that initiated the I/O\n   operation
    and drop any unwritten data, the client should attempt to\n   retry the original
    I/O operation by either requesting a new layout or\n   sending the I/O via regular
    NFSv4.1+ READ or WRITE operations to the\n   metadata server.  The client SHOULD
    attempt to retrieve a new layout\n   and retry the I/O operation using the storage
    device first and only\n   retry the I/O operation via the metadata server if the
    error\n   persists.\n"
  title: 7.  Recovering from Client I/O Errors
- contents:
  - "8.  Mirroring\n   The flexible file layout type has a simple model in place for
    the\n   mirroring of the file data constrained by a layout segment.  There is\n
    \  no assumption that each copy of the mirror is stored identically on\n   the
    storage devices.  For example, one device might employ\n   compression or deduplication
    on the data.  However, the over-the-wire\n   transfer of the file contents MUST
    appear identical.  Note, this is a\n   constraint of the selected XDR representation
    in which each mirrored\n   copy of the layout segment has the same striping pattern
    (see\n   Figure 1).\n   The metadata server is responsible for determining the
    number of\n   mirrored copies and the location of each mirror.  While the client\n
    \  may provide a hint to how many copies it wants (see Section 12), the\n   metadata
    server can ignore that hint; in any event, the client has no\n   means to dictate
    either the storage device (which also means the\n   coupling and/or protocol levels
    to access the layout segments) or the\n   location of said storage device.\n   The
    updating of mirrored layout segments is done via client-side\n   mirroring.  With
    this approach, the client is responsible for making\n   sure modifications are
    made on all copies of the layout segments it\n   is informed of via the layout.
    \ If a layout segment is being\n   resilvered to a storage device, that mirrored
    copy will not be in the\n   layout.  Thus, the metadata server MUST update that
    copy until the\n   client is presented it in a layout.  If the FF_FLAGS_WRITE_ONE_MIRROR\n
    \  is set in ffl_flags, the client need only update one of the mirrors\n   (see
    Section 8.2).  If the client is writing to the layout segments\n   via the metadata
    server, then the metadata server MUST update all\n   copies of the mirror.  As
    seen in Section 8.3, during the\n   resilvering, the layout is recalled, and the
    client has to make\n   modifications via the metadata server.\n"
  - contents:
    - "8.1.  Selecting a Mirror\n   When the metadata server grants a layout to a
      client, it MAY let the\n   client know how fast it expects each mirror to be
      once the request\n   arrives at the storage devices via the ffds_efficiency
      member.  While\n   the algorithms to calculate that value are left to the metadata\n
      \  server implementations, factors that could contribute to that\n   calculation
      include speed of the storage device, physical memory\n   available to the device,
      operating system version, current load, etc.\n   However, what should not be
      involved in that calculation is a\n   perceived network distance between the
      client and the storage device.\n   The client is better situated for making
      that determination based on\n   past interaction with the storage device over
      the different available\n   network interfaces between the two; that is, the
      metadata server\n   might not know about a transient outage between the client
      and\n   storage device because it has no presence on the given subnet.\n   As
      such, it is the client that decides which mirror to access for\n   reading the
      file.  The requirements for writing to mirrored layout\n   segments are presented
      below.\n"
    title: 8.1.  Selecting a Mirror
  - contents:
    - '8.2.  Writing to Mirrors

      '
    - contents:
      - "8.2.1.  Single Storage Device Updates Mirrors\n   If the FF_FLAGS_WRITE_ONE_MIRROR
        flag in ffl_flags is set, the client\n   only needs to update one of the copies
        of the layout segment.  For\n   this case, the storage device MUST ensure
        that all copies of the\n   mirror are updated when any one of the mirrors
        is updated.  If the\n   storage device gets an error when updating one of
        the mirrors, then\n   it MUST inform the client that the original WRITE had
        an error.  The\n   client then MUST inform the metadata server (see Section
        8.2.3).  The\n   client's responsibility with respect to COMMIT is explained
        in\n   Section 8.2.4.  The client may choose any one of the mirrors and may\n
        \  use ffds_efficiency as described in Section 8.1 when making this\n   choice.\n"
      title: 8.2.1.  Single Storage Device Updates Mirrors
    - contents:
      - "8.2.2.  Client Updates All Mirrors\n   If the FF_FLAGS_WRITE_ONE_MIRROR flag
        in ffl_flags is not set, the\n   client is responsible for updating all mirrored
        copies of the layout\n   segments that it is given in the layout.  A single
        failed update is\n   sufficient to fail the entire operation.  If all but
        one copy is\n   updated successfully and the last one provides an error, then
        the\n   client needs to inform the metadata server about the error.  The\n
        \  client can use either LAYOUTRETURN or LAYOUTERROR to inform the\n   metadata
        server that the update failed to that storage device.  If\n   the client is
        updating the mirrors serially, then it SHOULD stop at\n   the first error
        encountered and report that to the metadata server.\n   If the client is updating
        the mirrors in parallel, then it SHOULD\n   wait until all storage devices
        respond so that it can report all\n   errors encountered during the update.\n"
      title: 8.2.2.  Client Updates All Mirrors
    - contents:
      - "8.2.3.  Handling Write Errors\n   When the client reports a write error to
        the metadata server, the\n   metadata server is responsible for determining
        if it wants to remove\n   the errant mirror from the layout, if the mirror
        has recovered from\n   some transient error, etc.  When the client tries to
        get a new\n   layout, the metadata server informs it of the decision by the\n
        \  contents of the layout.  The client MUST NOT assume that the contents\n
        \  of the previous layout will match those of the new one.  If it has\n   updates
        that were not committed to all mirrors, then it MUST resend\n   those updates
        to all mirrors.\n   There is no provision in the protocol for the metadata
        server to\n   directly determine that the client has or has not recovered
        from an\n   error.  For example, if a storage device was network partitioned
        from\n   the client and the client reported the error to the metadata server,\n
        \  then the network partition would be repaired, and all of the copies\n   would
        be successfully updated.  There is no mechanism for the client\n   to report
        that fact, and the metadata server is forced to repair the\n   file across
        the mirror.\n   If the client supports NFSv4.2, it can use LAYOUTERROR and\n
        \  LAYOUTRETURN to provide hints to the metadata server about the\n   recovery
        efforts.  A LAYOUTERROR on a file is for a non-fatal error.\n   A subsequent
        LAYOUTRETURN without a ff_ioerr4 indicates that the\n   client successfully
        replayed the I/O to all mirrors.  Any\n   LAYOUTRETURN with a ff_ioerr4 is
        an error that the metadata server\n   needs to repair.  The client MUST be
        prepared for the LAYOUTERROR to\n   trigger a CB_LAYOUTRECALL if the metadata
        server determines it needs\n   to start repairing the file.\n"
      title: 8.2.3.  Handling Write Errors
    - contents:
      - "8.2.4.  Handling Write COMMITs\n   When stable writes are done to the metadata
        server or to a single\n   replica (if allowed by the use of FF_FLAGS_WRITE_ONE_MIRROR),
        it is\n   the responsibility of the receiving node to propagate the written\n
        \  data stably, before replying to the client.\n   In the corresponding cases
        in which unstable writes are done, the\n   receiving node does not have any
        such obligation, although it may\n   choose to asynchronously propagate the
        updates.  However, once a\n   COMMIT is replied to, all replicas must reflect
        the writes that have\n   been done, and this data must have been committed
        to stable storage\n   on all replicas.\n   In order to avoid situations in
        which stale data is read from\n   replicas to which writes have not been propagated:\n
        \  o  A client that has outstanding unstable writes made to single node\n
        \     (metadata server or storage device) MUST do all reads from that\n      same
        node.\n   o  When writes are flushed to the server (for example, to implement\n
        \     close-to-open semantics), a COMMIT must be done by the client to\n      ensure
        that up-to-date written data will be available irrespective\n      of the
        particular replica read.\n"
      title: 8.2.4.  Handling Write COMMITs
    title: 8.2.  Writing to Mirrors
  - contents:
    - "8.3.  Metadata Server Resilvering of the File\n   The metadata server may elect
      to create a new mirror of the layout\n   segments at any time.  This might be
      to resilver a copy on a storage\n   device that was down for servicing, to provide
      a copy of the layout\n   segments on storage with different storage performance\n
      \  characteristics, etc.  As the client will not be aware of the new\n   mirror
      and the metadata server will not be aware of updates that the\n   client is
      making to the layout segments, the metadata server MUST\n   recall the writable
      layout segment(s) that it is resilvering.  If the\n   client issues a LAYOUTGET
      for a writable layout segment that is in\n   the process of being resilvered,
      then the metadata server can deny\n   that request with an NFS4ERR_LAYOUTUNAVAILABLE.
      \ The client would\n   then have to perform the I/O through the metadata server.\n"
    title: 8.3.  Metadata Server Resilvering of the File
  title: 8.  Mirroring
- contents:
  - "9.  Flexible File Layout Type Return\n   layoutreturn_file4 is used in the LAYOUTRETURN
    operation to convey\n   layout-type-specific information to the server.  It is
    defined in\n   Section 18.44.1 of [RFC5661] as follows:\n   <CODE BEGINS>\n      /*
    Constants used for LAYOUTRETURN and CB_LAYOUTRECALL */\n      const LAYOUT4_RET_REC_FILE
    \     = 1;\n      const LAYOUT4_RET_REC_FSID      = 2;\n      const LAYOUT4_RET_REC_ALL
    \      = 3;\n      enum layoutreturn_type4 {\n              LAYOUTRETURN4_FILE
    = LAYOUT4_RET_REC_FILE,\n              LAYOUTRETURN4_FSID = LAYOUT4_RET_REC_FSID,\n
    \             LAYOUTRETURN4_ALL  = LAYOUT4_RET_REC_ALL\n      };\n   struct layoutreturn_file4
    {\n           offset4         lrf_offset;\n           length4         lrf_length;\n
    \          stateid4        lrf_stateid;\n           /* layouttype4 specific data
    */\n           opaque          lrf_body<>;\n   };\n   union layoutreturn4 switch(layoutreturn_type4
    lr_returntype) {\n           case LAYOUTRETURN4_FILE:\n                   layoutreturn_file4
    \     lr_layout;\n           default:\n                   void;\n   };\n   struct
    LAYOUTRETURN4args {\n           /* CURRENT_FH: file */\n           bool                    lora_reclaim;\n
    \          layouttype4             lora_layout_type;\n           layoutiomode4
    \          lora_iomode;\n           layoutreturn4           lora_layoutreturn;\n
    \  };\n   <CODE ENDS>\n   If the lora_layout_type layout type is LAYOUT4_FLEX_FILES
    and the\n   lr_returntype is LAYOUTRETURN4_FILE, then the lrf_body opaque value\n
    \  is defined by ff_layoutreturn4 (see Section 9.3).  This allows the\n   client
    to report I/O error information or layout usage statistics\n   back to the metadata
    server as defined below.  Note that while the\n   data structures are built on
    concepts introduced in NFSv4.2, the\n   effective discriminated union (lora_layout_type
    combined with\n   ff_layoutreturn4) allows for an NFSv4.1 metadata server to utilize\n
    \  the data.\n"
  - contents:
    - '9.1.  I/O Error Reporting

      '
    - contents:
      - "9.1.1.  ff_ioerr4\n   <CODE BEGINS>\n   /// struct ff_ioerr4 {\n   ///         offset4
        \       ffie_offset;\n   ///         length4        ffie_length;\n   ///         stateid4
        \      ffie_stateid;\n   ///         device_error4  ffie_errors<>;\n   ///
        };\n   ///\n   <CODE ENDS>\n   Recall that [RFC7862] defines device_error4
        as:\n   <CODE BEGINS>\n   struct device_error4 {\n           deviceid4       de_deviceid;\n
        \          nfsstat4        de_status;\n           nfs_opnum4      de_opnum;\n
        \  };\n   <CODE ENDS>\n   The ff_ioerr4 structure is used to return error
        indications for data\n   files that generated errors during data transfers.
        \ These are hints\n   to the metadata server that there are problems with
        that file.  For\n   each error, ffie_errors.de_deviceid, ffie_offset, and
        ffie_length\n   represent the storage device and byte range within the file
        in which\n   the error occurred; ffie_errors represents the operation and
        type of\n   error.  The use of device_error4 is described in Section 15.6
        of\n   [RFC7862].\n   Even though the storage device might be accessed via
        NFSv3 and\n   reports back NFSv3 errors to the client, the client is responsible\n
        \  for mapping these to appropriate NFSv4 status codes as de_status.\n   Likewise,
        the NFSv3 operations need to be mapped to equivalent NFSv4\n   operations.\n"
      title: 9.1.1.  ff_ioerr4
    title: 9.1.  I/O Error Reporting
  - contents:
    - '9.2.  Layout Usage Statistics

      '
    - contents:
      - "9.2.1.  ff_io_latency4\n   <CODE BEGINS>\n   /// struct ff_io_latency4 {\n
        \  ///         uint64_t       ffil_ops_requested;\n   ///         uint64_t
        \      ffil_bytes_requested;\n   ///         uint64_t       ffil_ops_completed;\n
        \  ///         uint64_t       ffil_bytes_completed;\n   ///         uint64_t
        \      ffil_bytes_not_delivered;\n   ///         nfstime4       ffil_total_busy_time;\n
        \  ///         nfstime4       ffil_aggregate_completion_time;\n   /// };\n
        \  ///\n   <CODE ENDS>\n   Both operation counts and bytes transferred are
        kept in the\n   ff_io_latency4.  As seen in ff_layoutupdate4 (see Section
        9.2.2),\n   READ and WRITE operations are aggregated separately.  READ operations\n
        \  are used for the ff_io_latency4 ffl_read.  Both WRITE and COMMIT\n   operations
        are used for the ff_io_latency4 ffl_write.  \"Requested\"\n   counters track
        what the client is attempting to do, and \"completed\"\n   counters track
        what was done.  There is no requirement that the\n   client only report completed
        results that have matching requested\n   results from the reported period.\n
        \  ffil_bytes_not_delivered is used to track the aggregate number of\n   bytes
        requested but not fulfilled due to error conditions.\n   ffil_total_busy_time
        is the aggregate time spent with outstanding RPC\n   calls. ffil_aggregate_completion_time
        is the sum of all round-trip\n   times for completed RPC calls.\n   In Section
        3.3.1 of [RFC5661], the nfstime4 is defined as the number\n   of seconds and
        nanoseconds since midnight or zero hour January 1,\n   1970 Coordinated Universal
        Time (UTC).  The use of nfstime4 in\n   ff_io_latency4 is to store time since
        the start of the first I/O from\n   the client after receiving the layout.
        \ In other words, these are to\n   be decoded as duration and not as a date
        and time.\n   Note that LAYOUTSTATS are cumulative, i.e., not reset each time
        the\n   operation is sent.  If two LAYOUTSTATS operations for the same file\n
        \  and layout stateid originate from the same NFS client and are\n   processed
        at the same time by the metadata server, then the one\n   containing the larger
        values contains the most recent time series\n   data.\n"
      title: 9.2.1.  ff_io_latency4
    - contents:
      - "9.2.2.  ff_layoutupdate4\n   <CODE BEGINS>\n   /// struct ff_layoutupdate4
        {\n   ///         netaddr4       ffl_addr;\n   ///         nfs_fh4        ffl_fhandle;\n
        \  ///         ff_io_latency4 ffl_read;\n   ///         ff_io_latency4 ffl_write;\n
        \  ///         nfstime4       ffl_duration;\n   ///         bool           ffl_local;\n
        \  /// };\n   ///\n   <CODE ENDS>\n   ffl_addr differentiates which network
        address the client is connected\n   to on the storage device.  In the case
        of multipathing, ffl_fhandle\n   indicates which read-only copy was selected.
        ffl_read and ffl_write\n   convey the latencies for both READ and WRITE operations,\n
        \  respectively.  ffl_duration is used to indicate the time period over\n
        \  which the statistics were collected.  If true, ffl_local indicates\n   that
        the I/O was serviced by the client's cache.  This flag allows\n   the client
        to inform the metadata server about \"hot\" access to a file\n   it would
        not normally be allowed to report on.\n"
      title: 9.2.2.  ff_layoutupdate4
    - contents:
      - "9.2.3.  ff_iostats4\n   <CODE BEGINS>\n   /// struct ff_iostats4 {\n   ///
        \        offset4           ffis_offset;\n   ///         length4           ffis_length;\n
        \  ///         stateid4          ffis_stateid;\n   ///         io_info4          ffis_read;\n
        \  ///         io_info4          ffis_write;\n   ///         deviceid4         ffis_deviceid;\n
        \  ///         ff_layoutupdate4  ffis_layoutupdate;\n   /// };\n   ///\n   <CODE
        ENDS>\n   [RFC7862] defines io_info4 as:\n   <CODE BEGINS>\n   struct io_info4
        {\n           uint64_t        ii_count;\n           uint64_t        ii_bytes;\n
        \  };\n   <CODE ENDS>\n   With pNFS, data transfers are performed directly
        between the pNFS\n   client and the storage devices.  Therefore, the metadata
        server has\n   no direct knowledge of the I/O operations being done and thus
        cannot\n   create on its own statistical information about client I/O to\n
        \  optimize the data storage location.  ff_iostats4 MAY be used by the\n   client
        to report I/O statistics back to the metadata server upon\n   returning the
        layout.\n   Since it is not feasible for the client to report every I/O that
        used\n   the layout, the client MAY identify \"hot\" byte ranges for which
        to\n   report I/O statistics.  The definition and/or configuration mechanism\n
        \  of what is considered \"hot\" and the size of the reported byte range\n
        \  are out of the scope of this document.  For client implementation,\n   providing
        reasonable default values and an optional run-time\n   management interface
        to control these parameters is suggested.  For\n   example, a client can define
        the default byte-range resolution to be\n   1 MB in size and the thresholds
        for reporting to be 1 MB/second or 10\n   I/O operations per second.\n   For
        each byte range, ffis_offset and ffis_length represent the\n   starting offset
        of the range and the range length in bytes.\n   ffis_read.ii_count, ffis_read.ii_bytes,
        ffis_write.ii_count, and\n   ffis_write.ii_bytes represent the number of contiguous
        READ and WRITE\n   I/Os and the respective aggregate number of bytes transferred
        within\n   the reported byte range.\n   The combination of ffis_deviceid and
        ffl_addr uniquely identifies\n   both the storage path and the network route
        to it.  Finally,\n   ffl_fhandle allows the metadata server to differentiate
        between\n   multiple read-only copies of the file on the same storage device.\n"
      title: 9.2.3.  ff_iostats4
    title: 9.2.  Layout Usage Statistics
  - contents:
    - "9.3.  ff_layoutreturn4\n   <CODE BEGINS>\n   /// struct ff_layoutreturn4 {\n
      \  ///         ff_ioerr4     fflr_ioerr_report<>;\n   ///         ff_iostats4
      \  fflr_iostats_report<>;\n   /// };\n   ///\n   <CODE ENDS>\n   When data file
      I/O operations fail, fflr_ioerr_report<> is used to\n   report these errors
      to the metadata server as an array of elements of\n   type ff_ioerr4.  Each
      element in the array represents an error that\n   occurred on the data file
      identified by ffie_errors.de_deviceid.  If\n   no errors are to be reported,
      the size of the fflr_ioerr_report<>\n   array is set to zero.  The client MAY
      also use fflr_iostats_report<>\n   to report a list of I/O statistics as an
      array of elements of type\n   ff_iostats4.  Each element in the array represents
      statistics for a\n   particular byte range.  Byte ranges are not guaranteed
      to be disjoint\n   and MAY repeat or intersect.\n"
    title: 9.3.  ff_layoutreturn4
  title: 9.  Flexible File Layout Type Return
- contents:
  - "10.  Flexible File Layout Type LAYOUTERROR\n   If the client is using NFSv4.2
    to communicate with the metadata\n   server, then instead of waiting for a LAYOUTRETURN
    to send error\n   information to the metadata server (see Section 9.1), it MAY
    use\n   LAYOUTERROR (see Section 15.6 of [RFC7862]) to communicate that\n   information.
    \ For the flexible file layout type, this means that\n   LAYOUTERROR4args is treated
    the same as ff_ioerr4.\n"
  title: 10.  Flexible File Layout Type LAYOUTERROR
- contents:
  - "11.  Flexible File Layout Type LAYOUTSTATS\n   If the client is using NFSv4.2
    to communicate with the metadata\n   server, then instead of waiting for a LAYOUTRETURN
    to send I/O\n   statistics to the metadata server (see Section 9.2), it MAY use\n
    \  LAYOUTSTATS (see Section 15.7 of [RFC7862]) to communicate that\n   information.
    \ For the flexible file layout type, this means that\n   LAYOUTSTATS4args.lsa_layoutupdate
    is overloaded with the same\n   contents as in ffis_layoutupdate.\n"
  title: 11.  Flexible File Layout Type LAYOUTSTATS
- contents:
  - "12.  Flexible File Layout Type Creation Hint\n   The layouthint4 type is defined
    in the [RFC5661] as follows:\n   <CODE BEGINS>\n   struct layouthint4 {\n       layouttype4
    \       loh_type;\n       opaque             loh_body<>;\n   };\n   <CODE ENDS>\n
    \  The layouthint4 structure is used by the client to pass a hint about\n   the
    type of layout it would like created for a particular file.  If\n   the loh_type
    layout type is LAYOUT4_FLEX_FILES, then the loh_body\n   opaque value is defined
    by the ff_layouthint4 type.\n"
  - contents:
    - "12.1.  ff_layouthint4\n   <CODE BEGINS>\n   /// union ff_mirrors_hint switch
      (bool ffmc_valid) {\n   ///     case TRUE:\n   ///         uint32_t    ffmc_mirrors;\n
      \  ///     case FALSE:\n   ///         void;\n   /// };\n   ///\n   /// struct
      ff_layouthint4 {\n   ///     ff_mirrors_hint    fflh_mirrors_hint;\n   /// };\n
      \  ///\n   <CODE ENDS>\n   This type conveys hints for the desired data map.
      \ All parameters are\n   optional so the client can give values for only the
      parameter it\n   cares about.\n"
    title: 12.1.  ff_layouthint4
  title: 12.  Flexible File Layout Type Creation Hint
- contents:
  - "13.  Recalling a Layout\n   While Section 12.5.5 of [RFC5661] discusses reasons
    independent of\n   layout type for recalling a layout, the flexible file layout
    type\n   metadata server should recall outstanding layouts in the following\n
    \  cases:\n   o  When the file's security policy changes, i.e., ACLs or permission\n
    \     mode bits are set.\n   o  When the file's layout changes, rendering outstanding
    layouts\n      invalid.\n   o  When existing layouts are inconsistent with the
    need to enforce\n      locking constraints.\n   o  When existing layouts are inconsistent
    with the requirements\n      regarding resilvering as described in Section 8.3.\n"
  - contents:
    - "13.1.  CB_RECALL_ANY\n   The metadata server can use the CB_RECALL_ANY callback
      operation to\n   notify the client to return some or all of its layouts.  Section
      22.3\n   of [RFC5661] defines the allowed types of the \"NFSv4 Recallable\n
      \  Object Types Registry\".\n   <CODE BEGINS>\n   /// const RCA4_TYPE_MASK_FF_LAYOUT_MIN
      \    = 16;\n   /// const RCA4_TYPE_MASK_FF_LAYOUT_MAX     = 17;\n   ///\n   struct
      \ CB_RECALL_ANY4args      {\n       uint32_t        craa_layouts_to_keep;\n
      \      bitmap4         craa_type_mask;\n   };\n   <CODE ENDS>\n   Typically,
      CB_RECALL_ANY will be used to recall client state when the\n   server needs
      to reclaim resources.  The craa_type_mask bitmap\n   specifies the type of resources
      that are recalled, and the\n   craa_layouts_to_keep value specifies how many
      of the recalled\n   flexible file layouts the client is allowed to keep.  The
      mask flags\n   for the flexible file layout type are defined as follows:\n   <CODE
      BEGINS>\n   /// enum ff_cb_recall_any_mask {\n   ///     PNFS_FF_RCA4_TYPE_MASK_READ
      = 16,\n   ///     PNFS_FF_RCA4_TYPE_MASK_RW   = 17\n   /// };\n   ///\n   <CODE
      ENDS>\n   The flags represent the iomode of the recalled layouts.  In response,\n
      \  the client SHOULD return layouts of the recalled iomode that it needs\n   the
      least, keeping at most craa_layouts_to_keep flexible file\n   layouts.\n   The
      PNFS_FF_RCA4_TYPE_MASK_READ flag notifies the client to return\n   layouts of
      iomode LAYOUTIOMODE4_READ.  Similarly, the\n   PNFS_FF_RCA4_TYPE_MASK_RW flag
      notifies the client to return layouts\n   of iomode LAYOUTIOMODE4_RW.  When
      both mask flags are set, the client\n   is notified to return layouts of either
      iomode.\n"
    title: 13.1.  CB_RECALL_ANY
  title: 13.  Recalling a Layout
- contents:
  - "14.  Client Fencing\n   In cases where clients are uncommunicative and their
    lease has\n   expired or when clients fail to return recalled layouts within a\n
    \  lease period, the server MAY revoke client layouts and reassign these\n   resources
    to other clients (see Section 12.5.5 of [RFC5661]).  To\n   avoid data corruption,
    the metadata server MUST fence off the revoked\n   clients from the respective
    data files as described in Section 2.2.\n"
  title: 14.  Client Fencing
- contents:
  - "15.  Security Considerations\n   The combination of components in a pNFS system
    is required to\n   preserve the security properties of NFSv4.1+ with respect to
    an\n   entity accessing data via a client.  The pNFS feature partitions the\n
    \  NFSv4.1+ file system protocol into two parts: the control protocol\n   and
    the data protocol.  As the control protocol in this document is\n   NFS, the security
    properties are equivalent to the version of NFS\n   being used.  The flexible
    file layout further divides the data\n   protocol into metadata and data paths.
    \ The security properties of\n   the metadata path are equivalent to those of
    NFSv4.1x (see Sections\n   1.7.1 and 2.2.1 of [RFC5661]).  And the security properties
    of the\n   data path are equivalent to those of the version of NFS used to\n   access
    the storage device, with the provision that the metadata\n   server is responsible
    for authenticating client access to the data\n   file.  The metadata server provides
    appropriate credentials to the\n   client to access data files on the storage
    device.  It is also\n   responsible for revoking access for a client to the storage
    device.\n   The metadata server enforces the file access control policy at\n   LAYOUTGET
    time.  The client should use RPC authorization credentials\n   for getting the
    layout for the requested iomode ((LAYOUTIOMODE4_READ\n   or LAYOUTIOMODE4_RW),
    and the server verifies the permissions and ACL\n   for these credentials, possibly
    returning NFS4ERR_ACCESS if the\n   client is not allowed the requested iomode.
    \ If the LAYOUTGET\n   operation succeeds, the client receives, as part of the
    layout, a set\n   of credentials allowing it I/O access to the specified data
    files\n   corresponding to the requested iomode.  When the client acts on I/O\n
    \  operations on behalf of its local users, it MUST authenticate and\n   authorize
    the user by issuing respective OPEN and ACCESS calls to the\n   metadata server,
    similar to having NFSv4 data delegations.\n   The combination of filehandle, synthetic
    uid, and gid in the layout\n   is the way that the metadata server enforces access
    control to the\n   data server.  The client only has access to filehandles of
    file\n   objects and not directory objects.  Thus, given a filehandle in a\n   layout,
    it is not possible to guess the parent directory filehandle.\n   Further, as the
    data file permissions only allow the given synthetic\n   uid read/write permission
    and the given synthetic gid read\n   permission, knowing the synthetic ids of
    one file does not\n   necessarily allow access to any other data file on the storage\n
    \  device.\n   The metadata server can also deny access at any time by fencing
    the\n   data file, which means changing the synthetic ids.  In turn, that\n   forces
    the client to return its current layout and get a new layout\n   if it wants to
    continue I/O to the data file.\n   If access is allowed, the client uses the corresponding
    (read-only or\n   read/write) credentials to perform the I/O operations at the
    data\n   file's storage devices.  When the metadata server receives a request\n
    \  to change a file's permissions or ACL, it SHOULD recall all layouts\n   for
    that file and then MUST fence off any clients still holding\n   outstanding layouts
    for the respective files by implicitly\n   invalidating the previously distributed
    credential on all data file\n   comprising the file in question.  It is REQUIRED
    that this be done\n   before committing to the new permissions and/or ACL.  By
    requesting\n   new layouts, the clients will reauthorize access against the modified\n
    \  access control metadata.  Recalling the layouts in this case is\n   intended
    to prevent clients from getting an error on I/Os done after\n   the client was
    fenced off.\n"
  - contents:
    - "15.1.  RPCSEC_GSS and Security Services\n   Because of the special use of principals
      within the loosely coupled\n   model, the issues are different depending on
      the coupling model.\n"
    - contents:
      - "15.1.1.  Loosely Coupled\n   RPCSEC_GSS version 3 (RPCSEC_GSSv3) [RFC7861]
        contains facilities\n   that would allow it to be used to authorize the client
        to the storage\n   device on behalf of the metadata server.  Doing so would
        require that\n   each of the metadata server, storage device, and client would
        need to\n   implement RPCSEC_GSSv3 using an RPC-application-defined structured\n
        \  privilege assertion in a manner described in Section 4.9.1 of\n   [RFC7862].
        \ The specifics necessary to do so are not described in\n   this document.
        \ This is principally because any such specification\n   would require extensive
        implementation work on a wide range of\n   storage devices, which would be
        unlikely to result in a widely usable\n   specification for a considerable
        time.\n   As a result, the layout type described in this document will not\n
        \  provide support for use of RPCSEC_GSS together with the loosely\n   coupled
        model.  However, future layout types could be specified,\n   which would allow
        such support, either through the use of\n   RPCSEC_GSSv3 or in other ways.\n"
      title: 15.1.1.  Loosely Coupled
    - contents:
      - "15.1.2.  Tightly Coupled\n   With tight coupling, the principal used to access
        the metadata file\n   is exactly the same as used to access the data file.
        \ The storage\n   device can use the control protocol to validate any RPC
        credentials.\n   As a result, there are no security issues related to using
        RPCSEC_GSS\n   with a tightly coupled system.  For example, if Kerberos V5
        Generic\n   Security Service Application Program Interface (GSS-API) [RFC4121]
        is\n   used as the security mechanism, then the storage device could use a\n
        \  control protocol to validate the RPC credentials to the metadata\n   server.\n"
      title: 15.1.2.  Tightly Coupled
    title: 15.1.  RPCSEC_GSS and Security Services
  title: 15.  Security Considerations
- contents:
  - "16.  IANA Considerations\n   [RFC5661] introduced the \"pNFS Layout Types Registry\";
    new layout\n   type numbers in this registry need to be assigned by IANA.  This\n
    \  document defines the protocol associated with an existing layout type\n   number:
    LAYOUT4_FLEX_FILES.  See Table 1.\n   +--------------------+------------+----------+-----+----------------+\n
    \  | Layout Type Name   | Value      | RFC      | How | Minor Versions |\n   +--------------------+------------+----------+-----+----------------+\n
    \  | LAYOUT4_FLEX_FILES | 0x00000004 | RFC 8435 | L   | 1              |\n   +--------------------+------------+----------+-----+----------------+\n
    \                    Table 1: Layout Type Assignments\n   [RFC5661] also introduced
    the \"NFSv4 Recallable Object Types\n   Registry\".  This document defines new
    recallable objects for\n   RCA4_TYPE_MASK_FF_LAYOUT_MIN and RCA4_TYPE_MASK_FF_LAYOUT_MAX
    (see\n   Table 2).\n   +------------------------------+-------+--------+-----+-------------+\n
    \  | Recallable Object Type Name  | Value | RFC    | How | Minor       |\n   |
    \                             |       |        |     | Versions    |\n   +------------------------------+-------+--------+-----+-------------+\n
    \  | RCA4_TYPE_MASK_FF_LAYOUT_MIN | 16    | RFC    | L   | 1           |\n   |
    \                             |       | 8435   |     |             |\n   | RCA4_TYPE_MASK_FF_LAYOUT_MAX
    | 17    | RFC    | L   | 1           |\n   |                              |       |
    8435   |     |             |\n   +------------------------------+-------+--------+-----+-------------+\n
    \               Table 2: Recallable Object Type Assignments\n"
  title: 16.  IANA Considerations
- contents:
  - '17.  References

    '
  - contents:
    - "17.1.  Normative References\n   [LEGAL]    IETF Trust, \"Trust Legal Provisions
      (TLP)\",\n              <https://trustee.ietf.org/trust-legal-provisions.html>.\n
      \  [RFC1813]  Callaghan, B., Pawlowski, B., and P. Staubach, \"NFS\n              Version
      3 Protocol Specification\", RFC 1813,\n              DOI 10.17487/RFC1813, June
      1995,\n              <https://www.rfc-editor.org/info/rfc1813>.\n   [RFC2119]
      \ Bradner, S., \"Key words for use in RFCs to Indicate\n              Requirement
      Levels\", BCP 14, RFC 2119,\n              DOI 10.17487/RFC2119, March 1997,\n
      \             <https://www.rfc-editor.org/info/rfc2119>.\n   [RFC4121]  Zhu,
      L., Jaganathan, K., and S. Hartman, \"The Kerberos\n              Version 5
      Generic Security Service Application Program\n              Interface (GSS-API)
      Mechanism: Version 2\", RFC 4121,\n              DOI 10.17487/RFC4121, July
      2005,\n              <https://www.rfc-editor.org/info/rfc4121>.\n   [RFC4506]
      \ Eisler, M., Ed., \"XDR: External Data Representation\n              Standard\",
      STD 67, RFC 4506, DOI 10.17487/RFC4506, May\n              2006, <https://www.rfc-editor.org/info/rfc4506>.\n
      \  [RFC5531]  Thurlow, R., \"RPC: Remote Procedure Call Protocol\n              Specification
      Version 2\", RFC 5531, DOI 10.17487/RFC5531,\n              May 2009, <https://www.rfc-editor.org/info/rfc5531>.\n
      \  [RFC5661]  Shepler, S., Ed., Eisler, M., Ed., and D. Noveck, Ed.,\n              \"Network
      File System (NFS) Version 4 Minor Version 1\n              Protocol\", RFC 5661,
      DOI 10.17487/RFC5661, January 2010,\n              <https://www.rfc-editor.org/info/rfc5661>.\n
      \  [RFC5662]  Shepler, S., Ed., Eisler, M., Ed., and D. Noveck, Ed.,\n              \"Network
      File System (NFS) Version 4 Minor Version 1\n              External Data Representation
      Standard (XDR) Description\",\n              RFC 5662, DOI 10.17487/RFC5662,
      January 2010,\n              <https://www.rfc-editor.org/info/rfc5662>.\n   [RFC7530]
      \ Haynes, T., Ed. and D. Noveck, Ed., \"Network File System\n              (NFS)
      Version 4 Protocol\", RFC 7530, DOI 10.17487/RFC7530,\n              March 2015,
      <https://www.rfc-editor.org/info/rfc7530>.\n   [RFC7861]  Adamson, A. and N.
      Williams, \"Remote Procedure Call (RPC)\n              Security Version 3\",
      RFC 7861, DOI 10.17487/RFC7861,\n              November 2016, <https://www.rfc-editor.org/info/rfc7861>.\n
      \  [RFC7862]  Haynes, T., \"Network File System (NFS) Version 4 Minor\n              Version
      2 Protocol\", RFC 7862, DOI 10.17487/RFC7862,\n              November 2016,
      <https://www.rfc-editor.org/info/rfc7862>.\n   [RFC8174]  Leiba, B., \"Ambiguity
      of Uppercase vs Lowercase in RFC\n              2119 Key Words\", BCP 14, RFC
      8174, DOI 10.17487/RFC8174,\n              May 2017, <https://www.rfc-editor.org/info/rfc8174>.\n
      \  [RFC8434]  Haynes, T., \"Requirements for Parallel NFS (pNFS) Layout\n              Types\",
      RFC 8434, DOI 10.17487/RFC8434, August 2018,\n              <https://www.rfc-editor.org/info/rfc8434>.\n"
    title: 17.1.  Normative References
  - contents:
    - "17.2.  Informative References\n   [RFC4519]  Sciberras, A., Ed., \"Lightweight
      Directory Access Protocol\n              (LDAP): Schema for User Applications\",
      RFC 4519,\n              DOI 10.17487/RFC4519, June 2006,\n              <https://www.rfc-editor.org/info/rfc4519>.\n"
    title: 17.2.  Informative References
  title: 17.  References
- contents:
  - "Acknowledgments\n   The following individuals provided miscellaneous comments
    to early\n   draft versions of this document: Matt W. Benjamin, Adam Emerson,\n
    \  J. Bruce Fields, and Lev Solomonov.\n   The following individuals provided
    miscellaneous comments to the\n   final draft versions of this document: Anand
    Ganesh, Robert Wipfel,\n   Gobikrishnan Sundharraj, Trond Myklebust, Rick Macklem,
    and Jim\n   Sermersheim.\n   Idan Kedar caught a nasty bug in the interaction
    of client-side\n   mirroring and the minor versioning of devices.\n   Dave Noveck
    provided comprehensive reviews of the document during the\n   working group last
    calls.  He also rewrote Section 2.3.\n   Olga Kornievskaia made a convincing case
    against the use of a\n   credential versus a principal in the fencing approach.
    \ Andy Adamson\n   and Benjamin Kaduk helped to sharpen the focus.\n   Benjamin
    Kaduk and Olga Kornievskaia also helped provide concrete\n   scenarios for loosely
    coupled security mechanisms.  In the end, Olga\n   proved that as defined, the
    loosely coupled model would not work with\n   RPCSEC_GSS.\n   Tigran Mkrtchyan
    provided the use case for not allowing the client to\n   proxy the I/O through
    the data server.\n   Rick Macklem provided the use case for only writing to a
    single\n   mirror.\n"
  title: Acknowledgments
- contents:
  - "Authors' Addresses\n   Benny Halevy\n   Email: bhalevy@gmail.com\n   Thomas Haynes\n
    \  Hammerspace\n   4300 El Camino Real Ste 105\n   Los Altos, CA  94022\n   United
    States of America\n   Email: loghyr@gmail.com\n"
  title: Authors' Addresses
