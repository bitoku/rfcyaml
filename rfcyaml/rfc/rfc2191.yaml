- title: __initial_text__
  contents:
  - '               VENUS - Very Extensive Non-Unicast Service

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  This memo\n   does not specify an Internet standard of any kind.  Distribution\
    \ of\n   this memo is unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   The MARS model (RFC2022) provides a solution to intra-LIS IP\n \
    \  multicasting over ATM, establishing and managing the use of ATM pt-\n   mpt\
    \ SVCs for IP multicast packet forwarding. Inter-LIS multicast\n   forwarding\
    \ is achieved using Mrouters, in a similar manner to which\n   the \"Classical\
    \ IP over ATM\" model uses Routers to inter-connect LISes\n   for unicast traffic.\
    \ The development of unicast IP shortcut\n   mechanisms (e.g.  NHRP) has led some\
    \ people to request the\n   development of a Multicast equivalent. There are a\
    \ number of\n   different approaches. This document focuses exclusively on the\n\
    \   problems associated with extending the MARS model to cover multiple\n   clusters\
    \ or clusters spanning more than one subnet. It describes a\n   hypothetical solution,\
    \ dubbed \"Very Extensive NonUnicast Service\"\n   (VENUS), and shows how complex\
    \ such a service would be. It is also\n   noted that VENUS ultimately has the\
    \ look and feel of a single, large\n   cluster using a distributed MARS.  This\
    \ document is being issued to\n   help focus ION efforts towards alternative solutions\
    \ for establishing\n   ATM level multicast connections between LISes.\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   The classical model of the Internet running over an ATM cloud\n\
    \   consists of multiple Logical IP Subnets (LISs) interconnected by IP\n   Routers\
    \ [1].  The evolving IP Multicast over ATM solution (the \"MARS\n   model\" [2])\
    \ retains the classical model. The LIS becomes a \"MARS\n   Cluster\", and Clusters\
    \ are interconnected by conventional IP\n   Multicast routers (Mrouters).\n  \
    \ The development of NHRP [3], a protocol for discovering and managing\n   unicast\
    \ forwarding paths that bypass IP routers, has led to some\n   calls for an IP\
    \ multicast equivalent.  Unfortunately, the IP\n   multicast service is a rather\
    \ different beast to the IP unicast\n   service. This document aims to explain\
    \ how much of what has been\n   learned during the development of NHRP must be\
    \ carefully scrutinized\n   before being re-applied to the multicast scenario.\
    \ Indeed, the\n   service provided by the MARS and MARS Clients in [2] are almost\n\
    \   orthogonal to the IP unicast service over ATM.\n   For the sake of discussion,\
    \ let's call this hypothetical multicast\n   shortcut discovery protocol the \"\
    Very Extensive Non-Unicast Service\"\n   (VENUS). A \"VENUS Domain\" is defined\
    \ as the set of hosts from two or\n   more participating Logical IP Subnets (LISs).\
    \ A multicast shortcut\n   connection is a point to multipoint SVC whose leaf\
    \ nodes are\n   scattered around the VENUS Domain. (It will be noted in section\
    \ 2\n   that a VENUS Domain might consist of a single MARS Cluster spanning\n\
    \   multiple LISs, or multiple MARS Clusters.)\n   VENUS faces a number of fundamental\
    \ problems. The first is exploding\n   the scope over which individual IP/ATM\
    \ interfaces must track and\n   react to IP multicast group membership changes.\
    \ Under the classical\n   IP routing model Mrouters act as aggregation points\
    \ for multicast\n   traffic flows in and out of Clusters [4]. They also act as\n\
    \   aggregators of group membership change information - only the IP/ATM\n   interfaces\
    \ within each Cluster need to know the specific identities\n   of their local\
    \ (intra-cluster) group members at any given time.\n   However, once you have\
    \ sources within a VENUS Domain establishing\n   shortcut connections the data\
    \ and signaling plane aggregation of\n   Mrouters is lost. In order for all possible\
    \ sources throughout a\n   VENUS Domain to manage their outgoing pt-mpt SVCs they\
    \ must be kept\n   aware of MARS_JOINs and MARS_LEAVEs occuring in every MARS\
    \ Cluster\n   that makes up a VENUS Domain. The nett effect is that a VENUS domain\n\
    \   looks very similar to a single, large distributed MARS Cluster.\n   A second\
    \ problem is the impact that shortcut connections will have on\n   IP level Inter\
    \ Domain Multicast Routing (IDMR) protocols. Multicast\n   groups have many sources\
    \ and many destinations scattered amongst the\n   participating Clusters. IDMR\
    \ protocols assume that they can calculate\n   efficient inter-Cluster multicast\
    \ trees by aggregating individual\n   sources or group members in any given Cluster\
    \ (subnet) behind the\n   Mrouter serving that Cluster. If sources are able to\
    \ simply bypass an\n   Mrouter we introduce a requirement that the existence of\
    \ each and\n   every shortcut connection be propagated into the IDMR decision\
    \ making\n   processes. The IDMR protocols may need to adapt when a source's\n\
    \   traffic bypasses its local Mrouter(s) and is injected into Mrouters\n   at\
    \ more distant points on the IP-level multicast distribution tree.\n   (This issue\
    \ has been looked at in [7], focussing on building\n   forwarding trees within\
    \ networks where the termination points are\n   small in number and sparsely distributed.\
    \ VENUS introduces tougher\n   requirements by assuming that multicast group membership\
    \ may be dense\n   across the region of interest.)\n   This document will focus\
    \ primarily on the internal problems of a\n   VENUS Domain, and leave the IDMR\
    \ interactions for future analysis.\n"
- title: 2. What does it mean to "shortcut" ?
  contents:
  - "2. What does it mean to \"shortcut\" ?\n   Before going further it is worth considering\
    \ both the definition of\n   the Cluster, and two possible definitions of \"shortcut\"\
    .\n"
- title: 2.1 What is a Cluster?
  contents:
  - "2.1 What is a Cluster?\n   In [2] a MARS Cluster is defined as the set of IP/ATM\
    \ interfaces that\n   are willing to engage in direct, ATM level pt-mpt SVCs to\
    \ perform IP\n   multicast packet forwarding. Each IP/ATM interface (a MARS Client)\n\
    \   must keep state information regarding the ATM addresses of each leaf\n   node\
    \ (recipient) of each pt-mpt SVC it has open. In addition, each\n   MARS Client\
    \ receives MARS_JOIN and MARS_LEAVE messages from the MARS\n   whenever there\
    \ is a requirement that Clients around the Cluster need\n   to update their pt-mpt\
    \ SVCs for a given IP multicast group.\n   It is worth noting that no MARS Client\
    \ has any concept of how big its\n   local cluster is - this knowledge is kept\
    \ only by the MARS that a\n   given Client is registered with.\n   Fundamentally\
    \ the Cluster (and the MARS model as a whole) is a\n   response to the requirement\
    \ that any multicast IP/ATM interface using\n   pt-mpt SVCs must, as group membership\
    \ changes, add and drop leaf\n   nodes itself. This means that some mechanism,\
    \ spanning all possible\n   group members within the scopes of these pt-mpt SVCs,\
    \ is required to\n   collect group membership information and distribute it in\
    \ a timely\n   fashion to those interfaces.  This is the MARS Cluster, with certain\n\
    \   scaling limits described in [4].\n"
- title: 2.2 LIS/Cluster boundary "shortcut"
  contents:
  - "2.2 LIS/Cluster boundary \"shortcut\"\n   The currently popular definition of\
    \ \"shortcut\" is based on the\n   existence of unicast LIS boundaries. It is\
    \ tied to the notion that\n   LIS boundaries have physical routers, and cutting\
    \ through a LIS\n   boundary means bypassing a router. Intelligently bypassing\
    \ routers\n   that sit at the edges of LISs has been the goal of NHRP. Discovering\n\
    \   the ATM level identity of an IP endpoint in a different LIS allows a\n   direct\
    \ SVC to be established, thus shortcutting the logical IP\n   topology (and very\
    \ real routers) along the unicast path from source\n   to destination.\n   For\
    \ simplicity of early adoption RFC2022 recommends that a Cluster's\n   scope be\
    \ made equivalent to that of a LIS. Under these circumstances\n   the \"Classical\
    \ IP\" routing model places Mrouters at LIS/Cluster\n   boundaries, and multicast\
    \ shortcutting must involve bypassing the\n   same physical routing entities as\
    \ unicast shortcutting. Each MARS\n   Cluster would be independent and contain\
    \ only those IP/ATM interfaces\n   that had been assigned to the same LIS.\n \
    \  As a consequence, a VENUS Domain covering the hosts in a number of\n   LIS/Clusters\
    \ would have to co-ordinate each individual MARS from each\n   LIS/Cluster (to\
    \ ensure group membership updates from around the VENUS\n   Domain were propagated\
    \ correctly).\n"
- title: 2.3 Big Cluster, LIS boundary "shortcut"
  contents:
  - "2.3 Big Cluster, LIS boundary \"shortcut\"\n   The MARS model's fundamental definition\
    \ of a Cluster was deliberately\n   created to be independent of unicast terminology.\
    \ Although not\n   currently well understood, it is possible to build a single\
    \ MARS\n   Cluster that encompasses the members of multiple LISs. As expected,\n\
    \   inter-LIS unicast traffic would pass through (or bypass, if using\n   NHRP)\
    \ routers on the LIS boundaries. Also as expected, each IP/ATM\n   interface,\
    \ acting as a MARS Client, would forward their IP multicast\n   packets directly\
    \ to intra-cluster group members. However, because the\n   direct intra-cluster\
    \ SVCs would exist between hosts from the\n   different LISs making up the cluster,\
    \ this could be considered a\n   \"shortcut\" of the unicast LIS boundaries.\n\
    \   This approach immediately brings up the problem of how the IDMR\n   protocols\
    \ will react. Mrouters only need to exist at the edges of\n   Clusters. In the\
    \ case of a single Cluster spanning multiple LISs,\n   each LIS becomes hidden\
    \ behind the Mrouter at the Cluster's edge.\n   This is arguably not a big problem\
    \ if the Cluster is a stub on an\n   IDMR protocol's multicast distribution tree,\
    \ and if there is only a\n   single Mrouter in or out of the Cluster. Problems\
    \ arise when two or\n   more Mrouters are attached to the edges of the Cluster,\
    \ and the\n   Cluster is used for transit multicast traffic. Each Mrouter's\n\
    \   interface is assigned a unicast identity (e.g. that of the unicast\n   router\
    \ containing the Mrouter). IDMR protocols that filter packets\n   based on the\
    \ correctness of the upstream source may be confused at\n   receiving IP multicast\
    \ packets directly from another Mrouter in the\n   same cluster but notionally\
    \ \"belonging\" to an LIS multiple unicast IP\n   hops away.\n   Adjusting the\
    \ packet filtering algorithms of Mrouters is something\n   that needs to be addressed\
    \ by any multicast shortcut scheme. It has\n   been noted before and a solution\
    \ proposed in [7]. For the sake of\n   argument this document will assume the\
    \ problem solvable. (However, it\n   is important that any solution scales well\
    \ under general topologies\n   and group membership densities.)\n   A multi-LIS\
    \ MARS Cluster can be considered a simple VENUS Domain.\n   Since it is a single\
    \ Cluster it can be scaled using the distributed\n   MARS solutions currently\
    \ being developed within the IETF [5,6].\n"
- title: 3. So what must VENUS look like?
  contents:
  - "3. So what must VENUS look like?\n   A number of functions that occur in the\
    \ MARS model are fundamental to\n   the problem of managing root controlled, pt-mpt\
    \ SVCs. The initial\n   setup of the forwarding SVC by any one MARS Client requires\
    \ a\n   query/response exchange with the Client's local MARS, establishing\n \
    \  who the current group members are (i.e. what leaf nodes should be on\n   the\
    \ SVC). Following SVC establishment comes the management phase -\n   MARS Clients\
    \ need to be kept informed of group membership changes\n   within the scopes of\
    \ their SVCs, so that leaf nodes may be added or\n   dropped as appropriate.\n\
    \   For intra-cluster multicasting the current MARS approach is our\n   solution\
    \ for these two phases.\n   For the rest of this document we will focus on what\
    \ VENUS would look\n   like when a VENUS Domain spans multiple MARS Clusters.\
    \ Under such\n   circumstances VENUS is a mechanism co-ordinating the MARS entities\
    \ of\n   each participating cluster. Each MARS is kept up to date with\n   sufficient\
    \ domain-wide information to support both phases of client\n   operation (SVC\
    \ establishment and SVC management) when the SVC's\n   endpoints are outside the\
    \ immediate scope of a client's local MARS.\n   Inside a VENUS Domain a MARS Client\
    \ is supplied information on group\n   members from all participating clusters.\n\
    \   The following subsections look at the problems associated with both\n   of\
    \ these phases independently. To a first approximation the problems\n   identified\
    \ are independent of the possible inter-MARS mechanisms. The\n   reader may assume\
    \ the MARS in any cluster has some undefined\n   mechanism for communicating with\
    \ the MARSs of clusters immediately\n   adjacent to its own cluster (i.e. connected\
    \ by a single Mrouter hop).\n"
- title: 3.1 SVC establishment - answering a MARS_REQUEST.
  contents:
  - "3.1 SVC establishment - answering a MARS_REQUEST.\n   The SVC establishment phase\
    \ contains a number of inter-related\n   problems.\n   First, the target of a\
    \ MARS_REQUEST (an IP multicast group) is an\n   abstract entity. Let us assume\
    \ that VENUS does not require every MARS\n   to know the entire list of group\
    \ members across the participating\n   clusters.  In this case each time a MARS_REQUEST\
    \ is received by a\n   MARS from a local client, the MARS must construct a sequence\
    \ of\n   MARS_MULTIs based on locally held information (on intra-cluster\n   members)\
    \ and remotely solicited information.\n   So how does it solicit this information?\
    \ Unlike the unicast\n   situation, there is no definite, single direction to\
    \ route a\n   MARS_REQUEST across the participating clusters. The only \"right\"\
    \n   approach is to send the MARS_REQUEST to all clusters, since group\n   members\
    \ may exist anywhere and everywhere. Let us allow one obvious\n   optimization\
    \ - the MARS_REQUEST is propagated along the IP multicast\n   forwarding tree\
    \ that has been established for the target group by\n   whatever IDMR protocol\
    \ is running at the time.\n   As noted in [4] there are various reasons why a\
    \ Cluster's scope be\n   kept limited. Some of these (MARS Client or ATM NIC limitations)\n\
    \   imply that the VENUS discovery process not return more group members\n   in\
    \ the MARS_MULTIs that the requesting MARS Client can handle. This\n   provides\
    \ VENUS with an interesting problem of propagating out the\n   original MARS_REQUEST,\
    \ but curtailing the MARS_REQUESTs propagation\n   when a sufficient number of\
    \ group members have been identified.\n   Viewed from a different perspective,\
    \ this means that the scope of\n   shortcut achievable by any given MARS Client\
    \ may depend greatly on\n   the shape of the IP forwarding tree away from its\
    \ location (and the\n   density of group members within clusters along the tree)\
    \ at the time\n   the request was issued.\n   How might we limit the number of\
    \ group members returned to a given\n   MARS Client? Adding a limit TLV to the\
    \ MARS_REQUEST itself is\n   trivial. At first glance it might appear that when\
    \ the limit is being\n   reached we could summarize the next cluster along the\
    \ tree by the ATM\n   address of the Mrouter into that cluster. The nett effect\
    \ would be\n   that the MARS Client establishes a shortcut to many hosts that\
    \ are\n   inside closer clusters, and passes its traffic to more distant\n   clusters\
    \ through the distant Mrouter. However, this approach only\n   works passably\
    \ well for a very simplistic multicast topology (e.g. a\n   linear concatenation\
    \ of clusters).\n   In a more general topology the IP multicast forwarding tree\
    \ away from\n   the requesting MARS Client will branch a number of times, requiring\n\
    \   the MARS_REQUEST to be replicated along each branch. Ensuring that\n   the\
    \ total number of returned group members does not exceed the\n   client's limit\
    \ becomes rather more difficult to do efficiently.\n   (VENUS could simply halve\
    \ the limit value each time it split a\n   MARS_REQUEST, but this might cause\
    \ group member discovery on one\n   branch to end prematurely while all the group\
    \ members along another\n   branch are discovered without reaching the subdivided\
    \ limit.)\n   Now consider this decision making process scattered across all the\n\
    \   clients in all participating clusters. Clients may have different\n   limits\
    \ on how many group members they can handle - leading to\n   situations where\
    \ different sources can shortcut to different\n   (sub)sets of the group members\
    \ scattered across the participating\n   clusters (because the IP multicast forwarding\
    \ trees from senders in\n   different clusters may result in different discovery\
    \ paths being\n   taken by their MARS_REQUESTs.)\n   Finally, when the MARS_REQUEST\
    \ passes a cluster where the target\n   group is MCS supported, VENUS must ensure\
    \ the ATM address of the MCS\n   is collected rather than the addresses of the\
    \ actual group members.\n   (To do otherwise would violate the remote cluster's\
    \ intra-cluster\n   decision to use an MCS. The shortcut in this case must be\
    \ content to\n   directly reach the remote cluster's MCS.)\n   (A solution to\
    \ part of this problem would be to ensure that a VENUS\n   Domain never has more\
    \ MARS Clients throughout than the clients are\n   capable of adding as leaf nodes.\
    \ This may or may not appeal to\n   people's desire for generality of a VENUS\
    \ solution. It also would\n   appear to beg the question of why the problem of\
    \ multiple-LIS\n   multicasting isn't solved simply by creating a single big MARS\n\
    \   Cluster.)\n"
- title: 3.2 SVC management - tracking group membership changes.
  contents:
  - "3.2 SVC management - tracking group membership changes.\n   Once a client's pt-mpt\
    \ SVC is established, it must be kept up to\n   date.  The consequence of this\
    \ is simple, and potentially\n   devastating: The MARS_JOINs and MARS_LEAVEs from\
    \ every MARS Client in\n   every participating cluster must be propagated to every\
    \ possible\n   sender in every participating cluster (this applies to groups that\n\
    \   are VC Mesh supported - groups that are MCS supported in some or all\n   participating\
    \ clusters introduce complications described below).\n   Unfortunately, the consequential\
    \ signaling load (as all the\n   participating MARSs start broadcasting their\
    \ MARS_JOIN/LEAVE\n   activity) is not localized to clusters containing MARS Clients\
    \ who\n   have established shortcut SVCs.  Since the IP multicast model is Any\n\
    \   to Multipoint, and you can never know where there may be source MARS\n   Clients,\
    \ the JOINs and LEAVEs must be propagated everywhere, always,\n   just in case.\
    \ (This is simply a larger scale version of sending JOINs\n   and LEAVEs to every\
    \ cluster member over ClusterControlVC, and for\n   exactly the same reason.)\n\
    \   The use of MCSs in some clusters instead of VC Meshes significantly\n   complicates\
    \ the situation, as does the initial scoping of a client's\n   shortcut during\
    \ the SVC establishment phase (described in the\n   preceding section).\n   In\
    \ Clusters where MCSs are supporting certain groups, MARS_JOINs or\n   MARS_LEAVEs\
    \ are only propagated to MARS Clients when an MCS comes or\n   goes. However,\
    \ it is not clear how to effectively accommodate the\n   current MARS_MIGRATE\
    \ functionality (that allows a previously VC Mesh\n   based group to be shifted\
    \ to an MCS within the scope of a single\n   cluster). If an MCS starts up within\
    \ a single Cluster, it is possible\n   to shift all the intra-cluster senders\
    \ to the MCS using MARS_MIGRATE\n   as currently described in the MARS model.\
    \ However, MARS Clients in\n   remote clusters that have shortcut SVCs into the\
    \ local cluster also\n   need some signal to shift (otherwise they will continue\
    \ to send their\n   packets directly to the group members in the local cluster).\n\
    \   This is a non-trivial requirement, since we only want to force the\n   remote\
    \ MARS Clients to drop some of their leaf nodes (the ones to\n   clients within\
    \ the Cluster that now has an MCS), add the new MCS as a\n   leaf node, and leave\
    \ all their other leaf nodes untouched (the cut-\n   through connections to other\
    \ clusters). Simply broadcasting the\n   MARS_MIGRATE around all participating\
    \ clusters would certainly not\n   work.  VENUS needs a new control message with\
    \ semantics of \"replaced\n   leaf nodes {x, y, z} with leaf node {a}, and leave\
    \ the rest alone\".\n   Such a message is easy to define, but harder to use.\n\
    \   Another issue for SVC management is that the scope over which a MARS\n   Client\
    \ needs to receive JOINs and LEAVEs needs to respect the\n   Client's limited\
    \ capacity for handling leaf nodes on its SVC. If the\n   MARS Client initially\
    \ issued a MARS_REQUEST and indicated it could\n   handle 1000 leaf nodes, it\
    \ is not clear how to ensure that subsequent\n   joins of new members wont exceed\
    \ that limit. Furthermore, if the SVC\n   establishment phase decided that the\
    \ SVC would stop at a particular\n   Mrouter (due to leaf node limits being reached),\
    \ the Client probably\n   should not be receiving direct MARS_JOIN or MARS_LEAVE\
    \ messages\n   pertaining to activity in the cluster \"behind\" this Mrouter.\
    \ (To do\n   otherwise could lead to multiple copies of the source client's\n\
    \   packets reaching group members inside the remote cluster - one\n   version\
    \ through the Mrouter, and another on the direct SVC connection\n   that the source\
    \ client would establish after receiving a subsequent,\n   global MARS_JOIN regarding\
    \ a host inside the remote cluster.)\n   Another scenario involves the density\
    \ of group members along the IDMR\n   multicast tree increasing with time after\
    \ the initial MARS_REQUEST is\n   answered. Subsequent JOINs from Cluster members\
    \ may dictate that a\n   \"closer\" Mrouter be used to aggregate the source's\
    \ outbound traffic\n   (so as not to exceed the source's leaf node limitations).\
    \ How to\n   dynamically shift between terminating on hosts within a Cluster,\
    \ and\n   terminating on a cluster's edge Mrouter, is an open question.\n   To\
    \ complicate matters further, this scoping of the VENUS domain-wide\n   propagation\
    \ of MARS_JOINs and MARS_LEAVEs needs to be on a per-\n   source- cluster basis,\
    \ at least. If MARS Clients within the same\n   cluster have different leaf node\
    \ limits, the problem worsens. Under\n   such circumstances, one client may have\
    \ been able to establish a\n   shortcut SVC directly into a remote cluster while\
    \ a second client -\n   in the same source cluster - may have been forced to terminate\
    \ its\n   shortcut on the remote cluster's Mrouter. The first client obviously\n\
    \   needs to know about group membership changes in the remote cluster,\n   whilst\
    \ the second client does not. Propagating these JOIN/LEAVE\n   messages on ClusterControlVC\
    \ in the source cluster will not work -\n   the MARS for the source cluster will\
    \ need to explicitly send copies\n   of the JOIN/LEAVE messages only to those\
    \ MARS Clients whose prior SVC\n   establishment phase indicates they need them.\
    \ Propagation of messages\n   to indicate a VC Mesh to MCS transition within clusters\
    \ may also need\n   to take account of the leaf node limitations of MARS Clients.\
    \ The\n   scaling characteristics of this problem are left to the readers\n  \
    \ imagination.\n   It was noted in the previous section that a VENUS domain could\
    \ be\n   limited to ensure there are never more MARS Clients than any one\n  \
    \ client's leaf node limit. This would certainly avoid the need to for\n   complicated\
    \ MARS_JOIN/LEAVE propagation mechanisms. However, it begs\n   the question of\
    \ how different the VENUS domain then becomes from a\n   single, large MARS Cluster.\n"
- title: 4. What is the value in bypassing Mrouters?
  contents:
  - "4. What is the value in bypassing Mrouters?\n   This is a good question, since\
    \ the whole aim of developing a shortcut\n   connection mechanism is predicated\
    \ on the assumption that bypassing\n   IP level entities is always a \"win\".\
    \ However, this is arguably not\n   true for multicast.\n   The most important\
    \ observation that should be made about shortcut\n   connection scenarios is that\
    \ they increase the exposure of any given\n   IP/ATM interface to externally generated\
    \ SVCs. If there are a\n   potential 1000 senders in a VENUS Domain, then you\
    \ (as a group\n   member) open yourself up to a potential demand for 1000 instances\
    \ of\n   your re-assembly engine (and 1000 distinct incoming SVCs, when you\n\
    \   get added as a leaf node to each sender's pt-mpt SVC, which your\n   local\
    \ switch port must be able to support).\n   It should be no surprise that the\
    \ ATM level scaling limits applicable\n   to a single MARS Cluster [4] will also\
    \ apply to a VENUS Domain. Again\n   we're up against the question of why you'd\
    \ bypass an Mrouter. As\n   noted in [4] Mrouters perform a useful function of\
    \ data path\n   aggregation - 100 senders in one cluster become 1 pt-mpt SVC out\
    \ of\n   the Mrouter into the next cluster along the tree. They also hide MARS\n\
    \   signaling activity - individual group membership changes in one\n   cluster\
    \ are hidden from IP/ATM interfaces in surrounding clusters.\n   The loss of these\
    \ benefits must be factored into any network designed\n   to utilize multicast\
    \ shortcut connections.\n   (For the sake of completeness, it must be noted that\
    \ extremely poor\n   mismatches of IP and ATM topologies may make Mrouter bypass\n\
    \   attractive if it improves the use of the underlying ATM cloud. There\n   may\
    \ also be benefits in removing the additional re-\n   assembly/segmentation latencies\
    \ of having packets pass through an\n   Mrouter. However, a VENUS Domain ascertained\
    \ to be small enough to\n   avoid the scaling limits in [4] might just as well\
    \ be constructed as\n   a single large MARS Cluster. A large cluster also avoids\
    \ a\n   topological mismatch between IP Mrouters and ATM switches.)\n"
- title: 5. Relationship to Distributed MARS protocols.
  contents:
  - "5. Relationship to Distributed MARS protocols.\n   The ION working group is looking\
    \ closely at the development of\n   distributed MARS architectures. An outline\
    \ of some issues is provided\n   in [5,6]. As noted earlier in this document the\
    \ problem space looks\n   very similar that faced by our hypothetical VENUS Domain.\
    \ For\n   example, in the load-sharing distributed MARS model:\n      - The Cluster\
    \ is partitioned into sub-clusters.\n      - Each Active MARS is assigned a particular\
    \ sub-cluster, and uses\n      its own sub-ClusterControlVC to propagate JOIN/LEAVE\
    \ messages to\n      members of its sub-cluster.\n      - The MARS_REQUEST from\
    \ any sub-cluster member must return\n      information from all the sub-clusters,\
    \ so as to ensure that all a\n      group's members across the cluster are identified.\n\
    \      - Group membership changes in any one sub-cluster must be\n      immediately\
    \ propagated to all the other sub-clusters.\n   There is a clear analogy to be\
    \ made between a distributed MARS\n   Cluster, and a VENUS Domain made up of multiple\
    \ single-MARS Clusters.\n   The information that must be shared between sub-clusters\
    \ in a\n   distributed MARS scenario is similar to the information that must be\n\
    \   shared between Clusters in a VENUS Domain.\n   The distributed MARS problem\
    \ is slightly simpler than that faced by\n   VENUS:\n      - There are no Mrouters\
    \ (IDMR nodes) within the scope of the\n      distributed Cluster.\n      - In\
    \ a distributed MARS Cluster an MCS supported group uses the\n      same MCS across\
    \ all the sub-clusters (unlike the VENUS Domain,\n      where complete generality\
    \ makes it necessary to cope with mixtures\n      of MCS and VC Mesh based Clusters).\n"
- title: 6. Conclusion.
  contents:
  - "6. Conclusion.\n   This document has described a hypothetical multicast shortcut\n\
    \   connection scheme, dubbed \"Very Extensive NonUnicast Service\"\n   (VENUS).\
    \  The two phases of multicast support - SVC establishment,\n   and SVC management\
    \ - are shown to be essential whether the scope is a\n   Cluster or a wider VENUS\
    \ Domain. It has been shown that once the\n   potential scope of a pt-mpt SVC\
    \ at establishment phase has been\n   expanded, the scope of the SVC management\
    \ mechanism must similarly be\n   expanded. This means timely tracking and propagation\
    \ of group\n   membership changes across the entire scope of a VENUS Domain.\n\
    \   It has also been noted that there is little difference in result\n   between\
    \ a VENUS Domain and a large MARS Cluster. Both suffer from the\n   same fundamental\
    \ scaling limitations, and both can be arranged to\n   provide shortcut of unicast\
    \ routing boundaries. However, a completely\n   general multi-cluster VENUS solution\
    \ ends up being more complex. It\n   needs to deal with bypassed Mrouter boundaries,\
    \ and dynamically\n   changing group membership densities along multicast distribution\n\
    \   trees established by the IDMR protocols in use.\n   No solutions have been\
    \ presented. This document's role is to provide\n   context for future developments.\n"
- title: Acknowledgment
  contents:
  - "Acknowledgment\n   This document was prepared while the author was with the\n\
    \   Internetworking Research group at Bellcore.\n"
- title: Security Considerations
  contents:
  - "Security Considerations\n   This memo addresses specific scaling issues associated\
    \ with the\n   extension of the MARS architecture beyond that described in RFC\
    \ 2022.\n   It is an Informational memo, and does not mandate any additional\n\
    \   protocol behaviors beyond those described in RFC 2022.  As such, the\n   security\
    \ implications are no greater or less than the implications\n   inherent in RFC\
    \ 2022.  Should enhancements to security be required,\n   they would need to be\
    \ added as an extension to the base architecture\n   in RFC 2022.\n"
- title: Author's Address
  contents:
  - "Author's Address\n   Grenville Armitage\n   Bell Labs, Lucent Technologies.\n\
    \   101 Crawfords Corner Rd,\n   Holmdel, NJ, 07733\n   USA\n   EMail: gja@dnrc.bell-labs.com\n"
- title: References
  contents:
  - "References\n   [1] Laubach, M., \"Classical IP and ARP over ATM\", RFC 1577,\
    \ Hewlett-\n   Packard Laboratories, December 1993.\n   [2] Armitage, G., \"Support\
    \ for Multicast over UNI 3.0/3.1 based ATM\n   Networks.\", Bellcore, RFC 2022,\
    \ November 1996.\n   [3] Luciani, J., et al, \"NBMA Next Hop Resolution Protocol\
    \ (NHRP)\",\n   Work in Progress, February 1997.\n   [4] Armitage, G., \"Issues\
    \ affecting MARS Cluster Size\", Bellcore, RFC\n   2121, March 1997.\n   [5] Armitage,\
    \ G., \"Redundant MARS architectures and SCSP\", Bellcore,\n   Work in Progress,\
    \ November 1996.\n   [6] Luciani, J., G. Armitage, J. Jalpern, \"Server Cache\n\
    \   Synchronization Protocol (SCSP) - NBMA\", Work in Progress, March 1997.\n\
    \   [7] Rekhter, Y., D. Farinacci, \" Support for Sparse Mode PIM over\n   ATM\"\
    , Cisco Systems, Work in Progress, April 1996.\n"
