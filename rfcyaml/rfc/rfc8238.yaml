- title: __initial_text__
  contents:
  - '                  Data Center Benchmarking Terminology

    '
- title: Abstract
  contents:
  - "Abstract\n   The purposes of this informational document are to establish\n \
    \  definitions and describe measurement techniques for data center\n   benchmarking,\
    \ as well as to introduce new terminology applicable to\n   performance evaluations\
    \ of data center network equipment.  This\n   document establishes the important\
    \ concepts for benchmarking network\n   switches and routers in the data center\
    \ and is a prerequisite for the\n   test methodology document (RFC 8239).  Many\
    \ of these terms and\n   methods may be applicable to network equipment beyond\
    \ the scope of\n   this document as the technologies originally applied in the\
    \ data\n   center are deployed elsewhere.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc8238.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2017 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................4\n\
    \      1.1. Requirements Language ......................................5\n  \
    \    1.2. Definition Format ..........................................5\n   2.\
    \ Latency .........................................................5\n      2.1.\
    \ Definition .................................................5\n      2.2. Discussion\
    \ .................................................7\n      2.3. Measurement Units\
    \ ..........................................7\n   3. Jitter ..........................................................8\n\
    \      3.1. Definition .................................................8\n  \
    \    3.2. Discussion .................................................8\n    \
    \  3.3. Measurement Units ..........................................8\n   4. Calibration\
    \ of the Physical Layer ...............................9\n      4.1. Definition\
    \ .................................................9\n      4.2. Discussion .................................................9\n\
    \      4.3. Measurement Units ..........................................9\n  \
    \ 5. Line Rate ......................................................10\n    \
    \  5.1. Definition ................................................10\n      5.2.\
    \ Discussion ................................................10\n      5.3. Measurement\
    \ Units .........................................11\n   6. Buffering ......................................................12\n\
    \      6.1. Buffer ....................................................12\n  \
    \         6.1.1. Definition .........................................12\n    \
    \       6.1.2. Discussion .........................................14\n      \
    \     6.1.3. Measurement Units ..................................14\n      6.2.\
    \ Incast ....................................................15\n           6.2.1.\
    \ Definition .........................................15\n           6.2.2. Discussion\
    \ .........................................15\n           6.2.3. Measurement Units\
    \ ..................................16\n   7. Application Throughput: Data Center\
    \ Goodput ....................16\n      7.1. Definition ................................................16\n\
    \      7.2. Discussion ................................................16\n  \
    \    7.3. Measurement Units .........................................16\n   8.\
    \ Security Considerations ........................................17\n   9. IANA\
    \ Considerations ............................................18\n   10. References\
    \ ....................................................18\n      10.1. Normative\
    \ References .....................................18\n      10.2. Informative\
    \ References ...................................19\n   Acknowledgments ...................................................20\n\
    \   Authors' Addresses ................................................20\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Traffic patterns in the data center are not uniform and\
    \ are\n   constantly changing.  They are dictated by the nature and variety of\n\
    \   applications utilized in the data center.  They can be largely\n   east-west\
    \ traffic flows (server to server inside the data center) in\n   one data center\
    \ and north-south (from the outside of the data center\n   to the server) in another,\
    \ while some may combine both.  Traffic\n   patterns can be bursty in nature and\
    \ contain many-to-one,\n   many-to-many, or one-to-many flows.  Each flow may\
    \ also be small and\n   latency sensitive or large and throughput sensitive while\
    \ containing\n   a mix of UDP and TCP traffic.  All of these may coexist in a\
    \ single\n   cluster and flow through a single network device simultaneously.\n\
    \   Benchmarking tests for network devices have long used [RFC1242],\n   [RFC2432],\
    \ [RFC2544], [RFC2889], and [RFC3918].  These benchmarks\n   have largely been\
    \ focused around various latency attributes and max\n   throughput of the Device\
    \ Under Test (DUT) being benchmarked.  These\n   standards are good at measuring\
    \ theoretical max throughput,\n   forwarding rates, and latency under testing\
    \ conditions, but they do\n   not represent real traffic patterns that may affect\
    \ these networking\n   devices.  The data center networking devices covered are\
    \ switches and\n   routers.\n   Currently, typical data center networking devices\
    \ are\n   characterized by:\n   -  High port density (48 ports or more).\n   -\
    \  High speed (currently, up to 100 GB/s per port).\n   -  High throughput (line\
    \ rate on all ports for Layer 2 and/or\n      Layer 3).\n   -  Low latency (in\
    \ the microsecond or nanosecond range).\n   -  Low amount of buffer (in the MB\
    \ range per networking device).\n   -  Layer 2 and Layer 3 forwarding capability\
    \ (Layer 3 not mandatory).\n   This document defines a set of definitions, metrics,\
    \ and new\n   terminology, including congestion scenarios and switch buffer\n\
    \   analysis, and redefines basic definitions in order to represent a\n   wide\
    \ mix of traffic conditions.  The test methodologies are defined\n   in [RFC8239].\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    NOT RECOMMENDED\", \"MAY\", and\n   \"OPTIONAL\" in this document are to be interpreted\
    \ as described in\n   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear\
    \ in all\n   capitals, as shown here.\n"
- title: 1.2.  Definition Format
  contents:
  - "1.2.  Definition Format\n   -  Term to be defined (e.g., \"latency\").\n   -\
    \  Definition: The specific definition for the term.\n   -  Discussion: A brief\
    \ discussion about the term, its application,\n      and any restrictions on measurement\
    \ procedures.\n   -  Measurement Units: Methodology for measurements and units\
    \ used to\n      report measurements of the term in question, if applicable.\n"
- title: 2.  Latency
  contents:
  - '2.  Latency

    '
- title: 2.1.  Definition
  contents:
  - "2.1.  Definition\n   Latency is the amount of time it takes a frame to transit\
    \ the DUT.\n   Latency is measured in units of time (seconds, milliseconds,\n\
    \   microseconds, and so on).  The purpose of measuring latency is to\n   understand\
    \ the impact of adding a device in the communication path.\n   The latency interval\
    \ can be assessed between different combinations\n   of events, regardless of\
    \ the type of switching device\n   (bit forwarding, aka cut-through; or a store-and-forward\
    \ device).\n   [RFC1242] defined latency differently for each of these types of\n\
    \   devices.\n   Traditionally, the latency measurement definitions are:\n   -\
    \  FILO (First In Last Out):\n      The time interval starting when the end of\
    \ the first bit of the\n      input frame reaches the input port and ending when\
    \ the last bit of\n      the output frame is seen on the output port.\n   -  FIFO\
    \ (First In First Out):\n      The time interval starting when the end of the\
    \ first bit of the\n      input frame reaches the input port and ending when the\
    \ start of\n      the first bit of the output frame is seen on the output port.\n\
    \      Latency (as defined in [RFC1242]) for bit-forwarding devices uses\n   \
    \   these events.\n   -  LILO (Last In Last Out):\n      The time interval starting\
    \ when the last bit of the input frame\n      reaches the input port and the last\
    \ bit of the output frame is\n      seen on the output port.\n   -  LIFO (Last\
    \ In First Out):\n      The time interval starting when the last bit of the input\
    \ frame\n      reaches the input port and ending when the first bit of the output\n\
    \      frame is seen on the output port.  Latency (as defined in\n      [RFC1242])\
    \ for store-and-forward devices uses these events.\n   Another possible way to\
    \ summarize the four definitions above is to\n   refer to the bit positions as\
    \ they normally occur: input to output.\n   -  FILO is FL (First bit Last bit).\n\
    \   -  FIFO is FF (First bit First bit).\n   -  LILO is LL (Last bit Last bit).\n\
    \   -  LIFO is LF (Last bit First bit).\n   This definition, as explained in this\
    \ section in the context of\n   data center switch benchmarking, is in lieu of\
    \ the previous\n   definition of \"latency\" as provided in RFC 1242, Section\
    \ 3.8 and\n   quoted here:\n      For store and forward devices: The time interval\
    \ starting when the\n      last bit of the input frame reaches the input port\
    \ and ending when\n      the first bit of the output frame is seen on the output\
    \ port.\n      For bit forwarding devices: The time interval starting when the\n\
    \      end of the first bit of the input frame reaches the input port and\n  \
    \    ending when the start of the first bit of the output frame is seen\n    \
    \  on the output port.\n   To accommodate both types of network devices and hybrids\
    \ of the two\n   types that have emerged, switch latency measurements made according\n\
    \   to this document MUST be measured with the FILO events.  FILO will\n   include\
    \ the latency of the switch and the latency of the frame as\n   well as the serialization\
    \ delay.  It is a picture of the \"whole\"\n   latency going through the DUT.\
    \  For applications that are latency\n   sensitive and can function with initial\
    \ bytes of the frame, FIFO\n   (or, for bit-forwarding devices, latency per RFC\
    \ 1242) MAY be used.\n   In all cases, the event combinations used in latency\
    \ measurements\n   MUST be reported.\n"
- title: 2.2.  Discussion
  contents:
  - "2.2.  Discussion\n   As mentioned in Section 2.1, FILO is the most important\
    \ measuring\n   definition.\n   Not all DUTs are exclusively cut-through or store-and-forward.\n\
    \   Data center DUTs are frequently store-and-forward for smaller packet\n   sizes\
    \ and then change to cut-through behavior at specific larger\n   packet sizes.\
    \  The value of the packet size at which the behavior\n   changes MAY be configurable,\
    \ depending on the DUT manufacturer.  FILO\n   covers both scenarios: store-and-forward\
    \ and cut-through.  The\n   threshold for the change in behavior does not matter\
    \ for\n   benchmarking, since FILO covers both possible scenarios.\n   The LIFO\
    \ mechanism can be used with store-and-forward switches\n   but not with cut-through\
    \ switches, as it will provide negative\n   latency values for larger packet sizes\
    \ because LIFO removes the\n   serialization delay.  Therefore, this mechanism\
    \ MUST NOT be used when\n   comparing the latencies of two different DUTs.\n"
- title: 2.3.  Measurement Units
  contents:
  - "2.3.  Measurement Units\n   The measuring methods to use for benchmarking purposes\
    \ are as\n   follows:\n   1) FILO MUST be used as a measuring method, as this\
    \ will include the\n      latency of the packet; today, the application commonly\
    \ needs to\n      read the whole packet to process the information and take an\n\
    \      action.\n   2) FIFO MAY be used for certain applications able to process\
    \ the data\n      as the first bits arrive -- for example, with a Field-Programmable\n\
    \      Gate Array (FPGA).\n   3) LIFO MUST NOT be used because, unlike all the\
    \ other methods, it\n      subtracts the latency of the packet.\n"
- title: 3.  Jitter
  contents:
  - '3.  Jitter

    '
- title: 3.1.  Definition
  contents:
  - "3.1.  Definition\n   In the context of the data center, jitter is synonymous\
    \ with the\n   common term \"delay variation\".  It is derived from multiple\n\
    \   measurements of one-way delay, as described in RFC 3393.  The\n   mandatory\
    \ definition of \"delay variation\" is the Packet Delay\n   Variation (PDV) as\
    \ defined in Section 4.2 of [RFC5481].  When\n   considering a stream of packets,\
    \ the delays of all packets are\n   subtracted from the minimum delay over all\
    \ packets in the stream.\n   This facilitates the assessment of the range of delay\
    \ variation\n   (Max - Min) or a high percentile of PDV (99th percentile, for\n\
    \   robustness against outliers).\n   When First-bit to Last-bit timestamps are\
    \ used for delay measurement,\n   then delay variation MUST be measured using\
    \ packets or frames of the\n   same size, since the definition of latency includes\
    \ the serialization\n   time for each packet.  Otherwise, if using First-bit to\
    \ First-bit,\n   the size restriction does not apply.\n"
- title: 3.2.  Discussion
  contents:
  - "3.2.  Discussion\n   In addition to a PDV range and/or a high percentile of PDV,\n\
    \   Inter-Packet Delay Variation (IPDV) as defined in Section 4.1 of\n   [RFC5481]\
    \ (differences between two consecutive packets) MAY be used\n   for the purpose\
    \ of determining how packet spacing has changed during\n   transfer -- for example,\
    \ to see if a packet stream has become closely\n   spaced or \"bursty\".  However,\
    \ the absolute value of IPDV SHOULD NOT\n   be used, as this \"collapses\" the\
    \ \"bursty\" and \"dispersed\" sides of\n   the IPDV distribution together.\n"
- title: 3.3.  Measurement Units
  contents:
  - "3.3.  Measurement Units\n   The measurement of delay variation is expressed in\
    \ units of seconds.\n   A PDV histogram MAY be provided for the population of\
    \ packets\n   measured.\n"
- title: 4.  Calibration of the Physical Layer
  contents:
  - '4.  Calibration of the Physical Layer

    '
- title: 4.1.  Definition
  contents:
  - "4.1.  Definition\n   Calibration of the physical layer consists of defining and\
    \ measuring\n   the latency of the physical devices used to perform tests on the\
    \ DUT.\n   It includes the list of all physical-layer components used, as\n  \
    \ specified here:\n   -  Type of device used to generate traffic / measure traffic.\n\
    \   -  Type of line cards used on the traffic generator.\n   -  Type of transceivers\
    \ on the traffic generator.\n   -  Type of transceivers on the DUT.\n   -  Type\
    \ of cables.\n   -  Length of cables.\n   -  Software name and version of the\
    \ traffic generator and DUT.\n   -  A list of enabled features on the DUT MAY\
    \ be provided and is\n      recommended (especially in the case of control-plane\
    \ protocols,\n      such as the Link Layer Discovery Protocol and Spanning Tree).\
    \  A\n      comprehensive configuration file MAY be provided to this effect.\n"
- title: 4.2.  Discussion
  contents:
  - "4.2.  Discussion\n   Calibration of the physical layer contributes to end-to-end\
    \ latency\n   and should be taken into account when evaluating the DUT.  Small\n\
    \   variations in the physical components of the test may impact the\n   latency\
    \ being measured; therefore, they MUST be described when\n   presenting results.\n"
- title: 4.3.  Measurement Units
  contents:
  - "4.3.  Measurement Units\n   It is RECOMMENDED that all cables used for testing\
    \ (1) be of the same\n   type and length and (2) come from the same vendor whenever\
    \ possible.\n   It is a MUST to document the cable specifications listed in\n\
    \   Section 4.1, along with the test results.  The test report MUST\n   specify\
    \ whether or not the cable latency has been subtracted from the\n   test measurements.\
    \  The accuracy of the traffic-generator\n   measurements MUST be provided (for\
    \ current test equipment, this is\n   usually a value within a range of 20 ns).\n"
- title: 5.  Line Rate
  contents:
  - '5.  Line Rate

    '
- title: 5.1.  Definition
  contents:
  - "5.1.  Definition\n   The transmit timing, or maximum transmitted data rate, is\
    \ controlled\n   by the \"transmit clock\" in the DUT.  The receive timing (maximum\n\
    \   ingress data rate) is derived from the transmit clock of the\n   connected\
    \ interface.\n   The line rate or physical-layer frame rate is the maximum capacity\
    \ to\n   send frames of a specific size at the transmit clock frequency of\n \
    \  the DUT.\n   The term \"nominal value of line rate\" defines the maximum speed\n\
    \   capability for the given port -- for example (expressed as Gigabit\n   Ethernet),\
    \ 1 GE, 10 GE, 40 GE, 100 GE.\n   The frequency (\"clock rate\") of the transmit\
    \ clock in any two\n   connected interfaces will never be precisely the same;\
    \ therefore, a\n   tolerance is needed.  This will be expressed by a Parts Per\
    \ Million\n   (PPM) value.  The IEEE standards allow a specific +/- variance in\
    \ the\n   transmit clock rate, and Ethernet is designed to allow for small,\n\
    \   normal variations between the two clock rates.  This results in a\n   tolerance\
    \ of the line-rate value when traffic is generated from test\n   equipment to\
    \ a DUT.\n   Line rate SHOULD be measured in frames per second (FPS).\n"
- title: 5.2.  Discussion
  contents:
  - "5.2.  Discussion\n   For a transmit clock source, most Ethernet switches use\
    \ \"clock\n   modules\" (also called \"oscillator modules\") that are sealed,\n\
    \   internally temperature-compensated, and very accurate.  The output\n   frequency\
    \ of these modules is not adjustable because it is not\n   necessary.  Many test\
    \ sets, however, offer a software-controlled\n   adjustment of the transmit clock\
    \ rate.  These adjustments SHOULD be\n   used to \"compensate\" the test equipment\
    \ in order to not send more\n   than the line rate of the DUT.\n   To allow for\
    \ the minor variations typically found in the clock rate\n   of commercially available\
    \ clock modules and other crystal-based\n   oscillators, Ethernet standards specify\
    \ the maximum transmit\n   clock-rate variation to be not more than +/- 100 PPM\
    \ from a\n   calculated center frequency.  Therefore, a DUT must be able to accept\n\
    \   frames at a rate within +/- 100 PPM to comply with the standards.\n   Very\
    \ few clock circuits are precisely +/- 0.0 PPM because:\n   1. The Ethernet standards\
    \ allow a maximum variance of +/- 100 PPM\n      over time.  Therefore, it is\
    \ normal for the frequency of the\n      oscillator circuits to experience variation\
    \ over time and over a\n      wide temperature range, among other external factors.\n\
    \   2. The crystals, or clock modules, usually have a specific +/- PPM\n     \
    \ variance that is significantly better than +/- 100 PPM.\n      Oftentimes, this\
    \ is +/- 30 PPM or better in order to be considered\n      a \"certification instrument\"\
    .\n   When testing an Ethernet switch throughput at \"line rate\", any\n   specific\
    \ switch will have a clock-rate variance.  If a test set is\n   running +1 PPM\
    \ faster than a switch under test and a sustained\n   line-rate test is performed,\
    \ a gradual increase in latency and,\n   eventually, packet drops as buffers fill\
    \ and overflow in the switch,\n   can be observed.  Depending on how much clock\
    \ variance there is\n   between the two connected systems, the effect may be seen\
    \ after the\n   traffic stream has been running for a few hundred microseconds,\
    \ a few\n   milliseconds, or seconds.  The same low latency, and no packet loss,\n\
    \   can be demonstrated by setting the test set's link occupancy to\n   slightly\
    \ less than 100 percent link occupancy.  Typically, 99 percent\n   link occupancy\
    \ produces excellent low latency and no packet loss.  No\n   Ethernet switch or\
    \ router will have a transmit clock rate of exactly\n   +/- 0.0 PPM.  Very few\
    \ (if any) test sets have a clock rate that is\n   precisely +/- 0.0 PPM.\n  \
    \ Test-set equipment manufacturers are well aware of the standards and\n   allow\
    \ a software-controlled +/- 100 PPM \"offset\" (clock-rate\n   adjustment) to\
    \ compensate for normal variations in the clock speed of\n   DUTs.  This offset\
    \ adjustment allows engineers to determine the\n   approximate speed at which\
    \ the connected device is operating and\n   verify that it is within parameters\
    \ allowed by standards.\n"
- title: 5.3.  Measurement Units
  contents:
  - "5.3.  Measurement Units\n   \"Line rate\" can be measured in terms of \"frame\
    \ rate\":\n   Frame Rate = Transmit-Clock-Frequency /\n      (Frame-Length*8 +\
    \ Minimum_Gap + Preamble + Start-Frame Delimiter)\n   Minimum_Gap represents the\
    \ interframe gap.  This formula \"scales up\"\n   or \"scales down\" to represent\
    \ 1 GB Ethernet, 10 GB Ethernet, and\n   so on.\n   Example for 1 GB Ethernet\
    \ speed with 64-byte frames:\n      Frame Rate = 1,000,000,000 / (64*8 + 96 +\
    \ 56 + 8)\n                 = 1,000,000,000 / 672\n                 = 1,488,095.2\
    \ FPS\n   Considering the allowance of +/- 100 PPM, a switch may \"legally\"\n\
    \   transmit traffic at a frame rate between 1,487,946.4 FPS and\n   1,488,244\
    \ FPS.  Each 1 PPM variation in clock rate will translate to\n   a frame-rate\
    \ increase or decrease of 1.488 FPS.\n   In a production network, it is very unlikely\
    \ that one would see\n   precise line rate over a very brief period.  There is\
    \ no observable\n   difference between dropping packets at 99% of line rate and\
    \ 100% of\n   line rate.\n   Line rate can be measured at 100% of line rate with\
    \ a -100 PPM\n   adjustment.\n   Line rate SHOULD be measured at 99.98% with a\
    \ 0 PPM adjustment.\n   The PPM adjustment SHOULD only be used for a line-rate\
    \ measurement.\n"
- title: 6.  Buffering
  contents:
  - '6.  Buffering

    '
- title: 6.1.  Buffer
  contents:
  - '6.1.  Buffer

    '
- title: 6.1.1.  Definition
  contents:
  - "6.1.1.  Definition\n   Buffer Size: The term \"buffer size\" represents the total\
    \ amount of\n      frame-buffering memory available on a DUT.  This size is expressed\n\
    \      in B (bytes), KB (kilobytes), MB (megabytes), or GB (gigabytes).\n    \
    \  When the buffer size is expressed, an indication of the frame MTU\n      (Maximum\
    \ Transmission Unit) used for that measurement is also\n      necessary, as well\
    \ as the CoS (Class of Service) or DSCP\n      (Differentiated Services Code Point)\
    \ value set, as oftentimes the\n      buffers are carved by a quality-of-service\
    \ implementation.  Please\n      refer to Section 3 of [RFC8239] for further details.\n\
    \      Example: The Buffer Size of the DUT when sending 1518-byte frames\n   \
    \   is 18 MB.\n   Port Buffer Size: The port buffer size is the amount of buffer\
    \ for\n      a single ingress port, a single egress port, or a combination of\n\
    \      ingress and egress buffering locations for a single port.  We\n      mention\
    \ the three locations for the port buffer because the DUT's\n      buffering scheme\
    \ can be unknown or untested, so knowing the buffer\n      location helps clarify\
    \ the buffer architecture and, consequently,\n      the total buffer size.  The\
    \ Port Buffer Size is an informational\n      value that MAY be provided by the\
    \ DUT vendor.  It is not a value\n      that is tested by benchmarking.  Benchmarking\
    \ will be done using\n      the Maximum Port Buffer Size or Maximum Buffer Size\
    \ methodology.\n   Maximum Port Buffer Size: In most cases, this is the same as\
    \ the Port\n      Buffer Size.  In a certain type of switch architecture called\n\
    \      \"SoC\" (switch on chip), there is a port buffer and a shared buffer\n\
    \      pool available for all ports.  The Maximum Port Buffer Size, in\n     \
    \ terms of an SoC buffer, represents the sum of the port buffer and\n      the\
    \ maximum value of shared buffer allowed for this port, defined\n      in terms\
    \ of B (bytes), KB (kilobytes), MB (megabytes), or GB\n      (gigabytes).  The\
    \ Maximum Port Buffer Size needs to be expressed\n      along with the frame MTU\
    \ used for the measurement and the CoS or\n      DSCP bit value set for the test.\n\
    \      Example: A DUT has been measured to have 3 KB of port buffer for\n    \
    \  1518-byte frames, and a total of 4.7 MB of maximum port buffer for\n      1518-byte\
    \ frames and a CoS of 0.\n   Maximum DUT Buffer Size: This is the total buffer\
    \ size that a DUT can\n      be measured to have.  It is most likely different\
    \ than the Maximum\n      Port Buffer Size.  It can also be different from the\
    \ sum of\n      Maximum Port Buffer Size.  The Maximum Buffer Size needs to be\n\
    \      expressed along with the frame MTU used for the measurement and\n     \
    \ along with the CoS or DSCP value set during the test.\n      Example: A DUT\
    \ has been measured to have 3 KB of port buffer for\n      1518-byte frames and\
    \ a total of 4.7 MB of maximum port buffer for\n      1518-byte frames.  The DUT\
    \ has a Maximum Buffer Size of 18 MB at\n      1500 B and a CoS of 0.\n   Burst:\
    \ A burst is a fixed number of packets sent over a percentage of\n      line rate\
    \ for a defined port speed.  The amount of frames sent is\n      evenly distributed\
    \ across the interval T.  A constant, C, can be\n      defined to provide the\
    \ average time between two evenly spaced\n      consecutive packets.\n   Microburst:\
    \ A microburst is a type of burst where packet drops occur\n      when there is\
    \ not sustained or noticeable congestion on a link or\n      device.  One characteristic\
    \ of a microburst is when the burst\n      is not evenly distributed over T and\
    \ is less than the constant C\n      (C = the average time between two evenly\
    \ spaced consecutive\n      packets).\n   Intensity of Microburst: This is a percentage\
    \ and represents the\n      level, between 1 and 100%, of the microburst.  The\
    \ higher the\n      number, the higher the microburst is.\n      I=[1-[ (Tp2-Tp1)+(Tp3-Tp2)+....(TpN-Tp(n-1)\
    \ ] / Sum(packets)]]*100\n   The above definitions are not meant to comment on\
    \ the ideal sizing of\n   a buffer but rather on how to measure it.  A larger\
    \ buffer is not\n   necessarily better and can cause issues with bufferbloat.\n"
- title: 6.1.2.  Discussion
  contents:
  - "6.1.2.  Discussion\n   When measuring buffering on a DUT, it is important to\
    \ understand the\n   behavior of each and every port.  This provides data for\
    \ the total\n   amount of buffering available on the switch.  The terms of buffer\n\
    \   efficiency help one understand the optimum packet size for the buffer\n  \
    \ or the real volume of the buffer available for a specific packet\n   size. \
    \ This section does not discuss how to conduct the test\n   methodology; instead,\
    \ it explains the buffer definitions and what\n   metrics should be provided for\
    \ comprehensive data center\n   device-buffering benchmarking.\n"
- title: 6.1.3.  Measurement Units
  contents:
  - "6.1.3.  Measurement Units\n   When the DUT buffer is measured:\n   -  The buffer\
    \ size MUST be measured.\n   -  The port buffer size MAY be provided for each\
    \ port.\n   -  The maximum port buffer size MUST be measured.\n   -  The maximum\
    \ DUT buffer size MUST be measured.\n   -  The intensity of the microburst MAY\
    \ be mentioned when a microburst\n      test is performed.\n   -  The CoS or DSCP\
    \ value set during the test SHOULD be provided.\n"
- title: 6.2.  Incast
  contents:
  - '6.2.  Incast

    '
- title: 6.2.1.  Definition
  contents:
  - "6.2.1.  Definition\n   The term \"Incast\", very commonly utilized in the data\
    \ center, refers\n   to the many-to-one or many-to-many traffic patterns.  As\
    \ defined in\n   this section, it measures the number of ingress and egress ports\
    \ and\n   the percentage of synchronization attributed to them.  Typically, in\n\
    \   the data center, it would refer to many different ingress server\n   ports\
    \ (many), sending traffic to a common uplink (many-to-one), or\n   multiple uplinks\
    \ (many-to-many).  This pattern is generalized for any\n   network as many incoming\
    \ ports sending traffic to one or a few\n   uplinks.\n   Synchronous arrival time:\
    \ When two or more frames of sizes L1 and L2\n      arrive at their respective\
    \ ingress port or multiple ingress ports\n      and there is an overlap of arrival\
    \ times for any of the bits on\n      the DUT, then the L1 and L2 frames have\
    \ synchronous arrival times.\n      This is called \"Incast\", regardless of whether\
    \ the pattern is\n      many-to-one (simpler) or many-to-many.\n   Asynchronous\
    \ arrival time: This is any condition not defined by\n      \"synchronous arrival\
    \ time\".\n   Percentage of synchronization: This defines the level of overlap\n\
    \      (amount of bits) between frames of sizes L1,L2..Ln.\n      Example: Two\
    \ 64-byte frames of length L1 and L2 arrive at ingress\n      port 1 and port\
    \ 2 of the DUT.  There is an overlap of 6.4 bytes\n      between the two, where\
    \ the L1 and L2 frames were on their\n      respective ingress ports at the same\
    \ time.  Therefore, the\n      percentage of synchronization is 10%.\n   Stateful\
    \ traffic: Stateful traffic is packets exchanged with a\n      stateful protocol,\
    \ such as TCP.\n   Stateless traffic: Stateless traffic is packets exchanged with\
    \ a\n      stateless protocol, such as UDP.\n"
- title: 6.2.2.  Discussion
  contents:
  - "6.2.2.  Discussion\n   In this scenario, buffers are used on the DUT.  In an\
    \ ingress\n   buffering mechanism, the ingress port buffers would be used along\n\
    \   with virtual output queues, when available, whereas in an egress\n   buffering\
    \ mechanism, the egress buffer of the one outgoing port would\n   be used.\n \
    \  In either case, regardless of where the buffer memory is located in\n   the\
    \ switch architecture, the Incast creates buffer utilization.\n   When one or\
    \ more frames have synchronous arrival times at the DUT,\n   they are considered\
    \ to be forming an Incast.\n"
- title: 6.2.3.  Measurement Units
  contents:
  - "6.2.3.  Measurement Units\n   It is a MUST to measure the number of ingress and\
    \ egress ports.\n   It is a MUST to have a non-null percentage of synchronization,\
    \ which\n   MUST be specified.\n"
- title: '7.  Application Throughput: Data Center Goodput'
  contents:
  - '7.  Application Throughput: Data Center Goodput

    '
- title: 7.1.  Definition
  contents:
  - "7.1.  Definition\n   In data center networking, a balanced network is a function\
    \ of\n   maximal throughput and minimal loss at any given time.  This is\n   captured\
    \ by the Goodput [TCP-INCAST].  Goodput is the\n   application-level throughput.\
    \  For standard TCP applications, a very\n   small loss can have a dramatic effect\
    \ on application throughput.\n   [RFC2647] provides a definition of Goodput; the\
    \ definition in this\n   document is a variant of that definition.\n   Goodput\
    \ is the number of bits per unit of time forwarded to the\n   correct destination\
    \ interface of the DUT, minus any bits\n   retransmitted.\n"
- title: 7.2.  Discussion
  contents:
  - "7.2.  Discussion\n   In data center benchmarking, the goodput is a value that\
    \ SHOULD be\n   measured.  It provides a realistic idea of the usage of the available\n\
    \   bandwidth.  A goal in data center environments is to maximize the\n   goodput\
    \ while minimizing loss.\n"
- title: 7.3.  Measurement Units
  contents:
  - "7.3.  Measurement Units\n   The Goodput, G, is then measured by the following\
    \ formula:\n      G = (S/F) x V bytes per second\n      -  S represents the payload\
    \ bytes, not including packet or\n         TCP headers.\n      -  F is the frame\
    \ size.\n      -  V is the speed of the media in bytes per second.\n      Example:\
    \ A TCP file transfer over HTTP on 10 GB/s media.\n      The file cannot be transferred\
    \ over Ethernet as a single\n      continuous stream.  It must be broken down\
    \ into individual frames\n      of 1500 B when the standard MTU is used.  Each\
    \ packet requires\n      20 B of IP header information and 20 B of TCP header\
    \ information;\n      therefore, 1460 B are available per packet for the file\
    \ transfer.\n      Linux-based systems are further limited to 1448 B, as they\
    \ also\n      carry a 12 B timestamp.  Finally, in this example the date is\n\
    \      transmitted over Ethernet, which adds 26 B of overhead per packet\n   \
    \   to 1500 B, increasing it to 1526 B.\n      G = 1460/1526 x 10 Gbit/s, which\
    \ is 9.567 Gbit/s or 1.196 GB/s.\n      Please note: This example does not take\
    \ into consideration the\n      additional Ethernet overhead, such as the interframe\
    \ gap (a\n      minimum of 96 bit times), nor does it account for collisions\n\
    \      (which have a variable impact, depending on the network load).\n   When\
    \ conducting Goodput measurements, please document, in addition to\n   the items\
    \ listed in Section 4.1, the following information:\n   -  The TCP stack used.\n\
    \   -  OS versions.\n   -  Network Interface Card (NIC) firmware version and model.\n\
    \   For example, Windows TCP stacks and different Linux versions can\n   influence\
    \ TCP-based test results.\n"
- title: 8.  Security Considerations
  contents:
  - "8.  Security Considerations\n   Benchmarking activities as described in this\
    \ memo are limited to\n   technology characterization using controlled stimuli\
    \ in a laboratory\n   environment, with dedicated address space and the constraints\n\
    \   specified in the sections above.\n   The benchmarking network topology will\
    \ be an independent test setup\n   and MUST NOT be connected to devices that may\
    \ forward the test\n   traffic into a production network or misroute traffic to\
    \ the test\n   management network.\n   Further, benchmarking is performed on a\
    \ \"black-box\" basis, relying\n   solely on measurements observable external\
    \ to the DUT.\n   Special capabilities SHOULD NOT exist in the DUT specifically\
    \ for\n   benchmarking purposes.  Any implications for network security arising\n\
    \   from the DUT SHOULD be identical in the lab and in production\n   networks.\n"
- title: 9.  IANA Considerations
  contents:
  - "9.  IANA Considerations\n   This document does not require any IANA actions.\n"
- title: 10.  References
  contents:
  - '10.  References

    '
- title: 10.1.  Normative References
  contents:
  - "10.1.  Normative References\n   [RFC1242]  Bradner, S., \"Benchmarking Terminology\
    \ for Network\n              Interconnection Devices\", RFC 1242, DOI 10.17487/RFC1242,\n\
    \              July 1991, <https://www.rfc-editor.org/info/rfc1242>.\n   [RFC2119]\
    \  Bradner, S., \"Key words for use in RFCs to Indicate\n              Requirement\
    \ Levels\", BCP 14, RFC 2119,\n              DOI 10.17487/RFC2119, March 1997,\n\
    \              <https://www.rfc-editor.org/info/rfc2119>.\n   [RFC2544]  Bradner,\
    \ S. and J. McQuaid, \"Benchmarking Methodology for\n              Network Interconnect\
    \ Devices\", RFC 2544,\n              DOI 10.17487/RFC2544, March 1999,\n    \
    \          <https://www.rfc-editor.org/info/rfc2544>.\n   [RFC5481]  Morton, A.\
    \ and B. Claise, \"Packet Delay Variation\n              Applicability Statement\"\
    , RFC 5481, DOI 10.17487/RFC5481,\n              March 2009, <https://www.rfc-editor.org/info/rfc5481>.\n\
    \   [RFC8174]  Leiba, B., \"Ambiguity of Uppercase vs Lowercase in\n         \
    \     RFC 2119 Key Words\", BCP 14, RFC 8174,\n              DOI 10.17487/RFC8174,\
    \ May 2017,\n              <https://www.rfc-editor.org/info/rfc8174>.\n   [RFC8239]\
    \  Avramov, L. and J. Rapp, \"Data Center Benchmarking\n              Methodology\"\
    , RFC 8239, DOI 10.17487/RFC8239, August 2017,\n              <https://www.rfc-editor.org/info/rfc8239>.\n"
- title: 10.2.  Informative References
  contents:
  - "10.2.  Informative References\n   [RFC2432]  Dubray, K., \"Terminology for IP\
    \ Multicast Benchmarking\",\n              RFC 2432, DOI 10.17487/RFC2432, October\
    \ 1998,\n              <https://www.rfc-editor.org/info/rfc2432>.\n   [RFC2647]\
    \  Newman, D., \"Benchmarking Terminology for Firewall\n              Performance\"\
    , RFC 2647, DOI 10.17487/RFC2647, August 1999,\n              <https://www.rfc-editor.org/info/rfc2647>.\n\
    \   [RFC2889]  Mandeville, R. and J. Perser, \"Benchmarking Methodology\n    \
    \          for LAN Switching Devices\", RFC 2889,\n              DOI 10.17487/RFC2889,\
    \ August 2000,\n              <https://www.rfc-editor.org/info/rfc2889>.\n   [RFC3918]\
    \  Stopp, D. and B. Hickman, \"Methodology for IP Multicast\n              Benchmarking\"\
    , RFC 3918, DOI 10.17487/RFC3918,\n              October 2004, <https://www.rfc-editor.org/info/rfc3918>.\n\
    \   [TCP-INCAST]\n              Chen, Y., Griffith, R., Zats, D., Joseph, A.,\
    \ and R. Katz,\n              \"Understanding TCP Incast and Its Implications\
    \ for Big\n              Data Workloads\", April 2012, <http://yanpeichen.com/\n\
    \              professional/usenixLoginIncastReady.pdf>.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   The authors would like to thank Al Morton, Scott Bradner,\
    \ Ian Cox,\n   and Tim Stevenson for their reviews and feedback.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Lucien Avramov\n   Google\n   1600 Amphitheatre Parkway\n\
    \   Mountain View, CA  94043\n   United States of America\n   Email: lucien.avramov@gmail.com\n\
    \   Jacob Rapp\n   VMware\n   3401 Hillview Ave.\n   Palo Alto, CA  94304\n  \
    \ United States of America\n   Email: jhrapp@gmail.com\n"
