- title: __initial_text__
  contents:
  - ''
- title: Benchmarking Methodology for Link-State IGP Data-Plane Route Convergence
  contents:
  - 'Benchmarking Methodology for Link-State IGP Data-Plane Route Convergence

    '
- title: Abstract
  contents:
  - "Abstract\n   This document describes the methodology for benchmarking Link-State\n\
    \   Interior Gateway Protocol (IGP) Route Convergence.  The methodology\n   is\
    \ to be used for benchmarking IGP convergence time through\n   externally observable\
    \ (black-box) data-plane measurements.  The\n   methodology can be applied to\
    \ any link-state IGP, such as IS-IS and\n   OSPF.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc6413.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2011 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n   This document\
    \ may contain material from IETF Documents or IETF\n   Contributions published\
    \ or made publicly available before November\n   10, 2008.  The person(s) controlling\
    \ the copyright in some of this\n   material may not have granted the IETF Trust\
    \ the right to allow\n   modifications of such material outside the IETF Standards\
    \ Process.\n   Without obtaining an adequate license from the person(s) controlling\n\
    \   the copyright in such materials, this document may not be modified\n   outside\
    \ the IETF Standards Process, and derivative works of it may\n   not be created\
    \ outside the IETF Standards Process, except to format\n   it for publication\
    \ as an RFC or to translate it into languages other\n   than English.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction . . . . . . . . . . . . . . . . . . .\
    \ . . . . . .  4\n     1.1.  Motivation . . . . . . . . . . . . . . . . . . .\
    \ . . . . .  4\n     1.2.  Factors for IGP Route Convergence Time . . . . . .\
    \ . . . .  4\n     1.3.  Use of Data Plane for IGP Route Convergence\n       \
    \    Benchmarking . . . . . . . . . . . . . . . . . . . . . . .  5\n     1.4.\
    \  Applicability and Scope  . . . . . . . . . . . . . . . . .  6\n   2.  Existing\
    \ Definitions . . . . . . . . . . . . . . . . . . . . .  6\n   3.  Test Topologies\
    \  . . . . . . . . . . . . . . . . . . . . . . .  7\n     3.1.  Test Topology\
    \ for Local Changes  . . . . . . . . . . . . .  7\n     3.2.  Test Topology for\
    \ Remote Changes . . . . . . . . . . . . .  8\n     3.3.  Test Topology for Local\
    \ ECMP Changes . . . . . . . . . . . 10\n     3.4.  Test Topology for Remote ECMP\
    \ Changes  . . . . . . . . . . 11\n     3.5.  Test topology for Parallel Link\
    \ Changes  . . . . . . . . . 11\n   4.  Convergence Time and Loss of Connectivity\
    \ Period . . . . . . . 12\n     4.1.  Convergence Events without Instant Traffic\
    \ Loss  . . . . . 13\n     4.2.  Loss of Connectivity (LoC) . . . . . . . . .\
    \ . . . . . . . 16\n   5.  Test Considerations  . . . . . . . . . . . . . . .\
    \ . . . . . . 17\n     5.1.  IGP Selection  . . . . . . . . . . . . . . . . .\
    \ . . . . . 17\n     5.2.  Routing Protocol Configuration . . . . . . . . . .\
    \ . . . . 17\n     5.3.  IGP Topology . . . . . . . . . . . . . . . . . . . .\
    \ . . . 17\n     5.4.  Timers . . . . . . . . . . . . . . . . . . . . . . . .\
    \ . . 18\n     5.5.  Interface Types  . . . . . . . . . . . . . . . . . . . .\
    \ . 18\n     5.6.  Offered Load . . . . . . . . . . . . . . . . . . . . . . .\
    \ 18\n     5.7.  Measurement Accuracy . . . . . . . . . . . . . . . . . . . 19\n\
    \     5.8.  Measurement Statistics . . . . . . . . . . . . . . . . . . 20\n  \
    \   5.9.  Tester Capabilities  . . . . . . . . . . . . . . . . . . . 20\n   6.\
    \  Selection of Convergence Time Benchmark Metrics and Methods  . 20\n     6.1.\
    \  Loss-Derived Method  . . . . . . . . . . . . . . . . . . . 21\n       6.1.1.\
    \  Tester Capabilities  . . . . . . . . . . . . . . . . . 21\n       6.1.2.  Benchmark\
    \ Metrics  . . . . . . . . . . . . . . . . . . 21\n       6.1.3.  Measurement\
    \ Accuracy . . . . . . . . . . . . . . . . . 21\n     6.2.  Rate-Derived Method\
    \  . . . . . . . . . . . . . . . . . . . 22\n       6.2.1.  Tester Capabilities\
    \  . . . . . . . . . . . . . . . . . 22\n       6.2.2.  Benchmark Metrics  . .\
    \ . . . . . . . . . . . . . . . . 23\n       6.2.3.  Measurement Accuracy . .\
    \ . . . . . . . . . . . . . . . 23\n     6.3.  Route-Specific Loss-Derived Method\
    \ . . . . . . . . . . . . 24\n       6.3.1.  Tester Capabilities  . . . . . .\
    \ . . . . . . . . . . . 24\n       6.3.2.  Benchmark Metrics  . . . . . . . .\
    \ . . . . . . . . . . 24\n       6.3.3.  Measurement Accuracy . . . . . . . .\
    \ . . . . . . . . . 24\n   7.  Reporting Format . . . . . . . . . . . . . . .\
    \ . . . . . . . . 25\n   8.  Test Cases . . . . . . . . . . . . . . . . . . .\
    \ . . . . . . . 26\n     8.1.  Interface Failure and Recovery . . . . . . . .\
    \ . . . . . . 27\n       8.1.1.  Convergence Due to Local Interface Failure and\n\
    \               Recovery . . . . . . . . . . . . . . . . . . . . . . . 27\n  \
    \     8.1.2.  Convergence Due to Remote Interface Failure and\n              \
    \ Recovery . . . . . . . . . . . . . . . . . . . . . . . 28\n       8.1.3.  Convergence\
    \ Due to ECMP Member Local Interface\n               Failure and Recovery . .\
    \ . . . . . . . . . . . . . . . 30\n       8.1.4.  Convergence Due to ECMP Member\
    \ Remote Interface\n               Failure and Recovery . . . . . . . . . . .\
    \ . . . . . . 31\n       8.1.5.  Convergence Due to Parallel Link Interface Failure\n\
    \               and Recovery . . . . . . . . . . . . . . . . . . . . . 32\n  \
    \   8.2.  Other Failures and Recoveries  . . . . . . . . . . . . . . 33\n    \
    \   8.2.1.  Convergence Due to Layer 2 Session Loss and\n               Recovery\
    \ . . . . . . . . . . . . . . . . . . . . . . . 33\n       8.2.2.  Convergence\
    \ Due to Loss and Recovery of IGP\n               Adjacency  . . . . . . . . .\
    \ . . . . . . . . . . . . . 34\n       8.2.3.  Convergence Due to Route Withdrawal\
    \ and\n               Re-Advertisement . . . . . . . . . . . . . . . . . . . 35\n\
    \     8.3.  Administrative Changes . . . . . . . . . . . . . . . . . . 37\n  \
    \     8.3.1.  Convergence Due to Local Interface Administrative\n            \
    \   Changes  . . . . . . . . . . . . . . . . . . . . . . . 37\n       8.3.2. \
    \ Convergence Due to Cost Change . . . . . . . . . . . . 38\n   9.  Security Considerations\
    \  . . . . . . . . . . . . . . . . . . . 39\n   10. Acknowledgements . . . . .\
    \ . . . . . . . . . . . . . . . . . . 40\n   11. References . . . . . . . . .\
    \ . . . . . . . . . . . . . . . . . 40\n     11.1. Normative References . . .\
    \ . . . . . . . . . . . . . . . . 40\n     11.2. Informative References . . .\
    \ . . . . . . . . . . . . . . . 41\n"
- title: 1.  Introduction
  contents:
  - '1.  Introduction

    '
- title: 1.1.  Motivation
  contents:
  - "1.1.  Motivation\n   Convergence time is a critical performance parameter.  Service\n\
    \   Providers use IGP convergence time as a key metric of router design\n   and\
    \ architecture.  Fast network convergence can be optimally achieved\n   through\
    \ deployment of fast converging routers.  Customers of Service\n   Providers use\
    \ packet loss due to Interior Gateway Protocol (IGP)\n   convergence as a key\
    \ metric of their network service quality.  IGP\n   route convergence is a Direct\
    \ Measure of Quality (DMOQ) when\n   benchmarking the data plane.  The fundamental\
    \ basis by which network\n   users and operators benchmark convergence is packet\
    \ loss and other\n   packet impairments, which are externally observable events\
    \ having\n   direct impact on their application performance.  For this reason,\
    \ it\n   is important to develop a standard methodology for benchmarking link-\n\
    \   state IGP convergence time through externally observable (black-box)\n   data-plane\
    \ measurements.  All factors contributing to convergence\n   time are accounted\
    \ for by measuring on the data plane.\n"
- title: 1.2.  Factors for IGP Route Convergence Time
  contents:
  - "1.2.  Factors for IGP Route Convergence Time\n   There are four major categories\
    \ of factors contributing to the\n   measured IGP convergence time.  As discussed\
    \ in [Vi02], [Ka02],\n   [Fi02], [Al00], [Al02], and [Fr05], these categories\
    \ are Event\n   Detection, Shortest Path First (SPF) Processing, Link State\n\
    \   Advertisement (LSA) / Link State Packet (LSP) Advertisement, and\n   Forwarding\
    \ Information Base (FIB) Update.  These have numerous\n   components that influence\
    \ the convergence time, including but not\n   limited to the list below:\n   o\
    \  Event Detection\n      *  Physical-Layer Failure/Recovery Indication Time\n\
    \      *  Layer 2 Failure/Recovery Indication Time\n      *  IGP Hello Dead Interval\n\
    \   o  SPF Processing\n      *  SPF Delay Time\n      *  SPF Hold Time\n     \
    \ *  SPF Execution Time\n   o  LSA/LSP Advertisement\n      *  LSA/LSP Generation\
    \ Time\n      *  LSA/LSP Flood Packet Pacing\n      *  LSA/LSP Retransmission\
    \ Packet Pacing\n   o  FIB Update\n      *  Tree Build Time\n      *  Hardware\
    \ Update Time\n   o  Increased Forwarding Delay due to Queueing\n   The contribution\
    \ of each of the factors listed above will vary with\n   each router vendor's\
    \ architecture and IGP implementation.  Routers\n   may have a centralized forwarding\
    \ architecture, in which one\n   forwarding table is calculated and referenced\
    \ for all arriving\n   packets, or a distributed forwarding architecture, in which\
    \ the\n   central forwarding table is calculated and distributed to the\n   interfaces\
    \ for local look-up as packets arrive.  The distributed\n   forwarding tables\
    \ are typically maintained (loaded and changed) in\n   software.\n   The variation\
    \ in router architecture and implementation necessitates\n   the design of a convergence\
    \ test that considers all of these\n   components contributing to convergence\
    \ time and is independent of the\n   Device Under Test (DUT) architecture and\
    \ implementation.  The benefit\n   of designing a test for these considerations\
    \ is that it enables\n   black-box testing in which knowledge of the routers'\
    \ internal\n   implementation is not required.  It is then possible to make valid\n\
    \   use of the convergence benchmarking metrics when comparing routers\n   from\
    \ different vendors.\n   Convergence performance is tightly linked to the number\
    \ of tasks a\n   router has to deal with.  As the most important tasks are mainly\n\
    \   related to the control plane and the data plane, the more the DUT is\n   stressed\
    \ as in a live production environment, the closer performance\n   measurement\
    \ results match the ones that would be observed in a live\n   production environment.\n"
- title: 1.3.  Use of Data Plane for IGP Route Convergence Benchmarking
  contents:
  - "1.3.  Use of Data Plane for IGP Route Convergence Benchmarking\n   Customers\
    \ of Service Providers use packet loss and other packet\n   impairments as metrics\
    \ to calculate convergence time.  Packet loss\n   and other packet impairments\
    \ are externally observable events having\n   direct impact on customers' application\
    \ performance.  For this\n   reason, it is important to develop a standard router\
    \ benchmarking\n   methodology that is a Direct Measure of Quality (DMOQ) for\
    \ measuring\n   IGP convergence.  An additional benefit of using packet loss for\n\
    \   calculation of IGP Route Convergence time is that it enables black-\n   box\
    \ tests to be designed.  Data traffic can be offered to the Device\n   Under Test\
    \ (DUT), an emulated network event can be forced to occur,\n   and packet loss\
    \ and other impaired packets can be externally measured\n   to calculate the convergence\
    \ time.  Knowledge of the DUT architecture\n   and IGP implementation is not required.\
    \  There is no need to rely on\n   the DUT to produce the test results.  There\
    \ is no need to build\n   intrusive test harnesses for the DUT.  All factors contributing\
    \ to\n   convergence time are accounted for by measuring on the data plane.\n\
    \   Other work of the Benchmarking Methodology Working Group (BMWG)\n   focuses\
    \ on characterizing single router control-plane convergence.\n   See [Ma05], [Ma05t],\
    \ and [Ma05c].\n"
- title: 1.4.  Applicability and Scope
  contents:
  - "1.4.  Applicability and Scope\n   The methodology described in this document\
    \ can be applied to IPv4 and\n   IPv6 traffic and link-state IGPs such as IS-IS\
    \ [Ca90][Ho08], OSPF\n   [Mo98][Co08], and others.  IGP adjacencies established\
    \ over any kind\n   of tunnel (such as Traffic Engineering tunnels) are outside\
    \ the scope\n   of this document.  Convergence time benchmarking in topologies\
    \ with\n   IGP adjacencies that are not point-to-point will be covered in a\n\
    \   later document.  Convergence from Bidirectional Forwarding Detection\n   (BFD)\
    \ is outside the scope of this document.  Non-Stop Forwarding\n   (NSF), Non-Stop\
    \ Routing (NSR), Graceful Restart (GR), and any other\n   High Availability mechanism\
    \ are outside the scope of this document.\n   Fast reroute mechanisms such as\
    \ IP Fast-Reroute [Sh10i] or MPLS Fast-\n   Reroute [Pa05] are outside the scope\
    \ of this document.\n"
- title: 2.  Existing Definitions
  contents:
  - "2.  Existing Definitions\n   The keywords \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described\
    \ in BCP 14, RFC 2119\n   [Br97].  RFC 2119 defines the use of these keywords\
    \ to help make the\n   intent of Standards Track documents as clear as possible.\
    \  While this\n   document uses these keywords, this document is not a Standards\
    \ Track\n   document.\n   This document uses much of the terminology defined in\
    \ [Po11t].  For\n   any conflicting content, this document supersedes [Po11t].\
    \  This\n   document uses existing terminology defined in other documents issued\n\
    \   by the Benchmarking Methodology Working Group (BMWG).  Examples\n   include,\
    \ but are not limited to:\n         Throughput                         [Br91],\
    \ Section 3.17\n         Offered Load                       [Ma98], Section 3.5.2\n\
    \         Forwarding Rate                    [Ma98], Section 3.6.1\n         Device\
    \ Under Test (DUT)            [Ma98], Section 3.1.1\n         System Under Test\
    \ (SUT)            [Ma98], Section 3.1.2\n         Out-of-Order Packet       \
    \         [Po06], Section 3.3.4\n         Duplicate Packet                   [Po06],\
    \ Section 3.3.5\n         Stream                             [Po06], Section 3.3.2\n\
    \         Forwarding Delay                   [Po06], Section 3.2.4\n         IP\
    \ Packet Delay Variation (IPDV)   [De02], Section 1.2\n         Loss Period  \
    \                      [Ko02], Section 4\n"
- title: 3.  Test Topologies
  contents:
  - '3.  Test Topologies

    '
- title: 3.1.  Test Topology for Local Changes
  contents:
  - "3.1.  Test Topology for Local Changes\n   Figure 1 shows the test topology to\
    \ measure IGP convergence time due\n   to local Convergence Events such as Local\
    \ Interface failure and\n   recovery (Section 8.1.1), Layer 2 session failure\
    \ and recovery\n   (Section 8.2.1), and IGP adjacency failure and recovery\n \
    \  (Section 8.2.2).  This topology is also used to measure IGP\n   convergence\
    \ time due to route withdrawal and re-advertisement\n   (Section 8.2.3) and to\
    \ measure IGP convergence time due to route cost\n   change (Section 8.3.2) Convergence\
    \ Events.  IGP adjacencies MUST be\n   established between Tester and DUT: one\
    \ on the Ingress Interface, one\n   on the Preferred Egress Interface, and one\
    \ on the Next-Best Egress\n   Interface.  For this purpose, the Tester emulates\
    \ three routers (RTa,\n   RTb, and RTc), each establishing one adjacency with\
    \ the DUT.\n                               -------\n                         \
    \      |     | Preferred        .......\n                               |    \
    \ |------------------. RTb .\n            .......    Ingress |     | Egress Interface\
    \ .......\n            . RTa .------------| DUT |\n            .......  Interface\
    \ |     | Next-Best        .......\n                               |     |------------------.\
    \ RTc .\n                               |     | Egress Interface .......\n   \
    \                            -------\n         Figure 1: IGP convergence test\
    \ topology for local changes\n   Figure 2 shows the test topology to measure IGP\
    \ convergence time due\n   to local Convergence Events with a non-Equal Cost Multipath\
    \ (ECMP)\n   Preferred Egress Interface and ECMP Next-Best Egress Interfaces\n\
    \   (Section 8.1.1).  In this topology, the DUT is configured with each\n   Next-Best\
    \ Egress Interface as a member of a single ECMP set.  The\n   Preferred Egress\
    \ Interface is not a member of an ECMP set.  The\n   Tester emulates N+2 neighbor\
    \ routers (N>0): one router for the\n   Ingress Interface (RTa), one router for\
    \ the Preferred Egress\n   Interface (RTb), and N routers for the members of the\
    \ ECMP set\n   (RTc1...RTcN).  IGP adjacencies MUST be established between Tester\n\
    \   and DUT: one on the Ingress Interface, one on the Preferred Egress\n   Interface,\
    \ and one on each member of the ECMP set.  When the test\n   specifies to observe\
    \ the Next-Best Egress Interface statistics, the\n   combined statistics for all\
    \ ECMP members should be observed.\n                               -------\n \
    \                              |     | Preferred        .......\n            \
    \                   |     |------------------. RTb .\n                       \
    \        |     | Egress Interface .......\n                               |  \
    \   |\n                               |     | ECMP Set         ........\n    \
    \        .......    Ingress |     |------------------. RTc1 .\n            . RTa\
    \ .------------| DUT | Interface 1      ........\n            .......  Interface\
    \ |     |       .\n                               |     |       .\n          \
    \                     |     |       .\n                               |     |\
    \ ECMP Set         ........\n                               |     |------------------.\
    \ RTcN .\n                               |     | Interface N      ........\n \
    \                              -------\n    Figure 2: IGP convergence test topology\
    \ for local changes with non-\n                         ECMP to ECMP convergence\n"
- title: 3.2.  Test Topology for Remote Changes
  contents:
  - "3.2.  Test Topology for Remote Changes\n   Figure 3 shows the test topology to\
    \ measure IGP convergence time due\n   to Remote Interface failure and recovery\
    \ (Section 8.1.2).  In this\n   topology, the two routers DUT1 and DUT2 are considered\
    \ the System\n   Under Test (SUT) and SHOULD be identically configured devices\
    \ of the\n   same model.  IGP adjacencies MUST be established between Tester and\n\
    \   SUT, one on the Ingress Interface, one on the Preferred Egress\n   Interface,\
    \ and one on the Next-Best Egress Interface.  For this\n   purpose, the Tester\
    \ emulates three routers (RTa, RTb, and RTc).  In\n   this topology, a packet\
    \ forwarding loop, also known as micro-loop\n   (see [Sh10]), may occur transiently\
    \ between DUT1 and DUT2 during\n   convergence.\n                          --------\n\
    \                          |      |  -------- Preferred        .......\n     \
    \                     |      |--| DUT2 |------------------. RTb .\n       .......\
    \    Ingress |      |  -------- Egress Interface .......\n       . RTa .------------|\
    \ DUT1 |\n       .......  Interface |      | Next-Best                  .......\n\
    \                          |      |----------------------------. RTc .\n     \
    \                     |      | Egress Interface           .......\n          \
    \                --------\n        Figure 3: IGP convergence test topology for\
    \ remote changes\n   Figure 4 shows the test topology to measure IGP convergence\
    \ time due\n   to remote Convergence Events with a non-ECMP Preferred Egress\n\
    \   Interface and ECMP Next-Best Egress Interfaces (Section 8.1.2).  In\n   this\
    \ topology the two routers DUT1 and DUT2 are considered System\n   Under Test\
    \ (SUT) and MUST be identically configured devices of the\n   same model.  Router\
    \ DUT1 is configured with the Next-Best Egress\n   Interface an ECMP set of interfaces.\
    \  The Preferred Egress Interface\n   of DUT1 is not a member of an ECMP set.\
    \  The Tester emulates N+2\n   neighbor routers (N>0), one for the Ingress Interface\
    \ (RTa), one for\n   DUT2 (RTb) and one for each member of the ECMP set (RTc1...RTcN).\n\
    \   IGP adjacencies MUST be established between Tester and SUT, one on\n   each\
    \ interface of the SUT.  For this purpose each of the N+2 routers\n   emulated\
    \ by the Tester establishes one adjacency with the SUT.  In\n   this topology,\
    \ there is a possibility of a packet-forwarding loop\n   that may occur transiently\
    \ between DUT1 and DUT2 during convergence\n   (micro-loop, see [Sh10]).  When\
    \ the test specifies to observe the\n   Next-Best Egress Interface statistics,\
    \ the combined statistics for\n   all members of the ECMP set should be observed.\n\
    \                         --------\n                         |      |  --------\
    \ Preferred        .......\n                         |      |--| DUT2 |------------------.\
    \ RTb .\n                         |      |  -------- Egress Interface .......\n\
    \                         |      |\n                         |      | ECMP Set\
    \                   ........\n      .......    Ingress |      |----------------------------.\
    \ RTc1 .\n      . RTa .------------| DUT1 | Interface 1                ........\n\
    \      .......  Interface |      |       .\n                         |      |\
    \       .\n                         |      |       .\n                       \
    \  |      | ECMP Set                   ........\n                         |  \
    \    |----------------------------. RTcN .\n                         |      |\
    \ Interface N                ........\n                         --------\n   \
    \   Figure 4: IGP convergence test topology for remote changes with\n        \
    \               non-ECMP to ECMP convergence\n"
- title: 3.3.  Test Topology for Local ECMP Changes
  contents:
  - "3.3.  Test Topology for Local ECMP Changes\n   Figure 5 shows the test topology\
    \ to measure IGP convergence time due\n   to local Convergence Events of a member\
    \ of an Equal Cost Multipath\n   (ECMP) set (Section 8.1.3).  In this topology,\
    \ the DUT is configured\n   with each egress interface as a member of a single\
    \ ECMP set and the\n   Tester emulates N+1 next-hop routers, one for the Ingress\
    \ Interface\n   (RTa) and one for each member of the ECMP set (RTb1...RTbN). \
    \ IGP\n   adjacencies MUST be established between Tester and DUT, one on the\n\
    \   Ingress Interface and one on each member of the ECMP set.  For this\n   purpose,\
    \ each of the N+1 routers emulated by the Tester establishes\n   one adjacency\
    \ with the DUT.  When the test specifies to observe the\n   Next-Best Egress Interface\
    \ statistics, the combined statistics for\n   all ECMP members except the one\
    \ affected by the Convergence Event\n   should be observed.\n                \
    \                 -------\n                                 |     | ECMP Set \
    \   ........\n                                 |     |-------------. RTb1 .\n\
    \                                 |     | Interface 1 ........\n             \
    \ .......    Ingress |     |       .\n              . RTa .------------| DUT |\
    \       .\n              .......  Interface |     |       .\n                \
    \                 |     | ECMP Set    ........\n                             \
    \    |     |-------------. RTbN .\n                                 |     | Interface\
    \ N ........\n                                 -------\n      Figure 5: IGP convergence\
    \ test topology for local ECMP changes\n"
- title: 3.4.  Test Topology for Remote ECMP Changes
  contents:
  - "3.4.  Test Topology for Remote ECMP Changes\n   Figure 6 shows the test topology\
    \ to measure IGP convergence time due\n   to remote Convergence Events of a member\
    \ of an Equal Cost Multipath\n   (ECMP) set (Section 8.1.4).  In this topology,\
    \ the two routers DUT1\n   and DUT2 are considered the System Under Test (SUT)\
    \ and MUST be\n   identically configured devices of the same model.  Router DUT1\
    \ is\n   configured with each egress interface as a member of a single ECMP\n\
    \   set, and the Tester emulates N+1 neighbor routers (N>0), one for the\n   Ingress\
    \ Interface (RTa) and one for each member of the ECMP set\n   (RTb1...RTbN). \
    \ IGP adjacencies MUST be established between Tester\n   and SUT, one on each\
    \ interface of the SUT.  For this purpose, each of\n   the N+1 routers emulated\
    \ by the Tester establishes one adjacency with\n   the SUT (N-1 emulated routers\
    \ are adjacent to DUT1 egress interfaces,\n   one emulated router is adjacent\
    \ to DUT1 Ingress Interface, and one\n   emulated router is adjacent to DUT2).\
    \  In this topology, there is a\n   possibility of a packet-forwarding loop that\
    \ may occur transiently\n   between DUT1 and DUT2 during convergence (micro-loop,\
    \ see [Sh10]).\n   When the test specifies to observe the Next-Best Egress Interface\n\
    \   statistics, the combined statistics for all ECMP members except the\n   one\
    \ affected by the Convergence Event should be observed.\n                    \
    \       --------\n                           |      | ECMP Set    --------   ........\n\
    \                           |      |-------------| DUT2 |---. RTb1 .\n       \
    \                    |      | Interface 1 --------   ........\n              \
    \             |      |\n                           |      | ECMP Set         \
    \      ........\n        .......    Ingress |      |------------------------.\
    \ RTb2 .\n        . RTa .------------| DUT1 | Interface 2            ........\n\
    \        .......  Interface |      |       .\n                           |   \
    \   |       .\n                           |      |       .\n                 \
    \          |      | ECMP Set               ........\n                        \
    \   |      |------------------------. RTbN .\n                           |   \
    \   | Interface N            ........\n                           --------\n \
    \     Figure 6: IGP convergence test topology for remote ECMP changes\n"
- title: 3.5.  Test topology for Parallel Link Changes
  contents:
  - "3.5.  Test topology for Parallel Link Changes\n   Figure 7 shows the test topology\
    \ to measure IGP convergence time due\n   to local Convergence Events with members\
    \ of a Parallel Link\n   (Section 8.1.5).  In this topology, the DUT is configured\
    \ with each\n   egress interface as a member of a Parallel Link and the Tester\n\
    \   emulates two neighbor routers, one for the Ingress Interface (RTa)\n   and\
    \ one for the Parallel Link members (RTb).  IGP adjacencies MUST be\n   established\
    \ on the Ingress Interface and on all N members of the\n   Parallel Link between\
    \ Tester and DUT (N>0).  For this purpose, the\n   routers emulated by the Tester\
    \ establishes N+1 adjacencies with the\n   DUT.  When the test specifies to observe\
    \ the Next-Best Egress\n   Interface statistics, the combined statistics for all\
    \ Parallel Link\n   members except the one affected by the Convergence Event should\
    \ be\n   observed.\n                                -------                .......\n\
    \                                |     | Parallel Link  .     .\n            \
    \                    |     |----------------.     .\n                        \
    \        |     | Interface 1    .     .\n             .......    Ingress |   \
    \  |       .        .     .\n             . RTa .------------| DUT |       . \
    \       . RTb .\n             .......  Interface |     |       .        .    \
    \ .\n                                |     | Parallel Link  .     .\n        \
    \                        |     |----------------.     .\n                    \
    \            |     | Interface N    .     .\n                                -------\
    \                .......\n     Figure 7: IGP convergence test topology for Parallel\
    \ Link changes\n"
- title: 4.  Convergence Time and Loss of Connectivity Period
  contents:
  - "4.  Convergence Time and Loss of Connectivity Period\n   Two concepts will be\
    \ highlighted in this section: convergence time\n   and loss of connectivity period.\n\
    \   The Route Convergence [Po11t] time indicates the period in time\n   between\
    \ the Convergence Event Instant [Po11t] and the instant in time\n   the DUT is\
    \ ready to forward traffic for a specific route on its Next-\n   Best Egress Interface\
    \ and maintains this state for the duration of\n   the Sustained Convergence Validation\
    \ Time [Po11t].  To measure Route\n   Convergence time, the Convergence Event\
    \ Instant and the traffic\n   received from the Next-Best Egress Interface need\
    \ to be observed.\n   The Route Loss of Connectivity Period [Po11t] indicates\
    \ the time\n   during which traffic to a specific route is lost following a\n\
    \   Convergence Event until Full Convergence [Po11t] completes.  This\n   Route\
    \ Loss of Connectivity Period can consist of one or more Loss\n   Periods [Ko02].\
    \  For the test cases described in this document, it is\n   expected to have a\
    \ single Loss Period.  To measure the Route Loss of\n   Connectivity Period, the\
    \ traffic received from the Preferred Egress\n   Interface and the traffic received\
    \ from the Next-Best Egress\n   Interface need to be observed.\n   The Route Loss\
    \ of Connectivity Period is most important since that\n   has a direct impact\
    \ on the network user's application performance.\n   In general, the Route Convergence\
    \ time is larger than or equal to the\n   Route Loss of Connectivity Period. \
    \ Depending on which Convergence\n   Event occurs and how this Convergence Event\
    \ is applied, traffic for a\n   route may still be forwarded over the Preferred\
    \ Egress Interface\n   after the Convergence Event Instant, before converging\
    \ to the Next-\n   Best Egress Interface.  In that case, the Route Loss of Connectivity\n\
    \   Period is shorter than the Route Convergence time.\n   At least one condition\
    \ needs to be fulfilled for Route Convergence\n   time to be equal to Route Loss\
    \ of Connectivity Period.  The condition\n   is that the Convergence Event causes\
    \ an instantaneous traffic loss\n   for the measured route.  A fiber cut on the\
    \ Preferred Egress\n   Interface is an example of such a Convergence Event.\n\
    \   A second condition applies to Route Convergence time measurements\n   based\
    \ on Connectivity Packet Loss [Po11t].  This second condition is\n   that there\
    \ is only a single Loss Period during Route Convergence.\n   For the test cases\
    \ described in this document, the second condition\n   is expected to apply.\n"
- title: 4.1.  Convergence Events without Instant Traffic Loss
  contents:
  - "4.1.  Convergence Events without Instant Traffic Loss\n   To measure convergence\
    \ time benchmarks for Convergence Events caused\n   by a Tester, such as an IGP\
    \ cost change, the Tester MAY start to\n   discard all traffic received from the\
    \ Preferred Egress Interface at\n   the Convergence Event Instant, or MAY separately\
    \ observe packets\n   received from the Preferred Egress Interface prior to the\
    \ Convergence\n   Event Instant.  This way, these Convergence Events can be treated\
    \ the\n   same as Convergence Events that cause instantaneous traffic loss.\n\
    \   To measure convergence time benchmarks without instantaneous traffic\n   loss\
    \ (either real or induced by the Tester) at the Convergence Event\n   Instant,\
    \ such as a reversion of a link failure Convergence Event, the\n   Tester SHALL\
    \ only observe packet statistics on the Next-Best Egress\n   Interface.  If using\
    \ the Rate-Derived method to benchmark convergence\n   times for such Convergence\
    \ Events, the Tester MUST collect a\n   timestamp at the Convergence Event Instant.\
    \  If using a loss-derived\n   method to benchmark convergence times for such\
    \ Convergence Events,\n   the Tester MUST measure the period in time between the\
    \ Start Traffic\n   Instant and the Convergence Event Instant.  To measure this\
    \ period in\n   time, the Tester can collect timestamps at the Start Traffic Instant\n\
    \   and the Convergence Event Instant.\n   The Convergence Event Instant together\
    \ with the receive rate\n   observations on the Next-Best Egress Interface allow\
    \ the derivation\n   of the convergence time benchmarks using the Rate-Derived\
    \ Method\n   [Po11t].\n   By observing packets on the Next-Best Egress Interface\
    \ only, the\n   observed Impaired Packet count is the number of Impaired Packets\n\
    \   between Traffic Start Instant and Convergence Recovery Instant.  To\n   measure\
    \ convergence times using a loss-derived method, the Impaired\n   Packet count\
    \ between the Convergence Event Instant and the\n   Convergence Recovery Instant\
    \ is needed.  The time between Traffic\n   Start Instant and Convergence Event\
    \ Instant must be accounted for.\n   An example may clarify this.\n   Figure 8\
    \ illustrates a Convergence Event without instantaneous\n   traffic loss for all\
    \ routes.  The top graph shows the Forwarding Rate\n   over all routes, the bottom\
    \ graph shows the Forwarding Rate for a\n   single route Rta.  Some time after\
    \ the Convergence Event Instant, the\n   Forwarding Rate observed on the Preferred\
    \ Egress Interface starts to\n   decrease.  In the example, route Rta is the first\
    \ route to experience\n   packet loss at time Ta.  Some time later, the Forwarding\
    \ Rate\n   observed on the Next-Best Egress Interface starts to increase.  In\n\
    \   the example, route Rta is the first route to complete convergence at\n   time\
    \ Ta'.\n           ^\n      Fwd  |\n      Rate |-------------                \
    \    ............\n           |             \\                  .\n          \
    \ |              \\                .\n           |               \\          \
    \    .\n           |                \\            .\n           |.................-.-.-.-.-.-.----------------\n\
    \           +----+-------+---------------+----------------->\n           ^   \
    \ ^       ^               ^             time\n          T0   CEI      Ta     \
    \         Ta'\n           ^\n      Fwd  |\n      Rate |-------------         \
    \      .................\n      Rta  |            |               .\n        \
    \   |            |               .\n           |.............-.-.-.-.-.-.-.-.----------------\n\
    \           +----+-------+---------------+----------------->\n           ^   \
    \ ^       ^               ^             time\n          T0   CEI      Ta     \
    \         Ta'\n           Preferred Egress Interface: ---\n           Next-Best\
    \ Egress Interface: ...\n           T0  : Start Traffic Instant\n           CEI\
    \ : Convergence Event Instant\n           Ta  : the time instant packet loss for\
    \ route Rta starts\n           Ta' : the time instant packet impairment for route\
    \ Rta ends\n                                 Figure 8\n   If only packets received\
    \ on the Next-Best Egress Interface are\n   observed, the duration of the loss\
    \ period for route Rta can be\n   calculated from the received packets as in Equation\
    \ 1.  Since the\n   Convergence Event Instant is the start time for convergence\
    \ time\n   measurement, the period in time between T0 and CEI needs to be\n  \
    \ subtracted from the calculated result to become the convergence time,\n   as\
    \ in Equation 2.\n   Next-Best Egress Interface loss period\n       = (packets\
    \ transmitted\n           - packets received from Next-Best Egress Interface)\
    \ / tx rate\n       = Ta' - T0\n                                Equation 1\n \
    \        convergence time\n             = Next-Best Egress Interface loss period\
    \ - (CEI - T0)\n             = Ta' - CEI\n                                Equation\
    \ 2\n"
- title: 4.2.  Loss of Connectivity (LoC)
  contents:
  - "4.2.  Loss of Connectivity (LoC)\n   Route Loss of Connectivity Period SHOULD\
    \ be measured using the Route-\n   Specific Loss-Derived Method.  Since the start\
    \ instant and end\n   instant of the Route Loss of Connectivity Period can be\
    \ different for\n   each route, these cannot be accurately derived by only observing\n\
    \   global statistics over all routes.  An example may clarify this.\n   Following\
    \ a Convergence Event, route Rta is the first route for which\n   packet impairment\
    \ starts; the Route Loss of Connectivity Period for\n   route Rta starts at time\
    \ Ta.  Route Rtb is the last route for which\n   packet impairment starts; the\
    \ Route Loss of Connectivity Period for\n   route Rtb starts at time Tb with Tb>Ta.\n\
    \                  ^\n             Fwd  |\n             Rate |--------       \
    \                -----------\n                  |        \\                  \
    \   /\n                  |         \\                   /\n                  |\
    \          \\                 /\n                  |           \\            \
    \   /\n                  |            ---------------\n                  +------------------------------------------>\n\
    \                           ^   ^             ^    ^      time\n             \
    \             Ta   Tb           Ta'   Tb'\n                                  \
    \          Tb''  Ta''\n            Figure 9: Example Route Loss Of Connectivity\
    \ Period\n   If the DUT implementation were such that route Rta would be the first\n\
    \   route for which traffic loss ends at time Ta' (with Ta'>Tb), and\n   route\
    \ Rtb would be the last route for which traffic loss ends at time\n   Tb' (with\
    \ Tb'>Ta').  By only observing global traffic statistics over\n   all routes,\
    \ the minimum Route Loss of Connectivity Period would be\n   measured as Ta'-Ta.\
    \  The maximum calculated Route Loss of\n   Connectivity Period would be Tb'-Ta.\
    \  The real minimum and maximum\n   Route Loss of Connectivity Periods are Ta'-Ta\
    \ and Tb'-Tb.\n   Illustrating this with the numbers Ta=0, Tb=1, Ta'=3, and Tb'=5\
    \ would\n   give a Loss of Connectivity Period between 3 and 5 derived from the\n\
    \   global traffic statistics, versus the real Loss of Connectivity\n   Period\
    \ between 3 and 4.\n   If the DUT implementation were such that route Rtb would\
    \ be the first\n   for which packet loss ends at time Tb'' and route Rta would\
    \ be the\n   last for which packet impairment ends at time Ta'', then the minimum\n\
    \   and maximum Route Loss of Connectivity Periods derived by observing\n   only\
    \ global traffic statistics would be Tb''-Ta and Ta''-Ta.  The\n   real minimum\
    \ and maximum Route Loss of Connectivity Periods are\n   Tb''-Tb and Ta''-Ta.\
    \  Illustrating this with the numbers Ta=0, Tb=1,\n   Ta''=5, Tb''=3 would give\
    \ a Loss of Connectivity Period between 3 and\n   5 derived from the global traffic\
    \ statistics, versus the real Loss of\n   Connectivity Period between 2 and 5.\n\
    \   The two implementation variations in the above example would result\n   in\
    \ the same derived minimum and maximum Route Loss of Connectivity\n   Periods\
    \ when only observing the global packet statistics, while the\n   real Route Loss\
    \ of Connectivity Periods are different.\n"
- title: 5.  Test Considerations
  contents:
  - '5.  Test Considerations

    '
- title: 5.1.  IGP Selection
  contents:
  - "5.1.  IGP Selection\n   The test cases described in Section 8 can be used for\
    \ link-state\n   IGPs, such as IS-IS or OSPF.  The IGP convergence time test\n\
    \   methodology is identical.\n"
- title: 5.2.  Routing Protocol Configuration
  contents:
  - "5.2.  Routing Protocol Configuration\n   The obtained results for IGP convergence\
    \ time may vary if other\n   routing protocols are enabled and routes learned\
    \ via those protocols\n   are installed.  IGP convergence times SHOULD be benchmarked\
    \ without\n   routes installed from other protocols.  Any enabled IGP routing\n\
    \   protocol extension (such as extensions for Traffic Engineering) and\n   any\
    \ enabled IGP routing protocol security mechanism must be reported\n   with the\
    \ results.\n"
- title: 5.3.  IGP Topology
  contents:
  - "5.3.  IGP Topology\n   The Tester emulates a single IGP topology.  The DUT establishes\
    \ IGP\n   adjacencies with one or more of the emulated routers in this single\n\
    \   IGP topology emulated by the Tester.  See test topology details in\n   Section\
    \ 3.  The emulated topology SHOULD only be advertised on the\n   DUT egress interfaces.\n\
    \   The number of IGP routes and number of nodes in the topology, and the\n  \
    \ type of topology will impact the measured IGP convergence time.  To\n   obtain\
    \ results similar to those that would be observed in an\n   operational network,\
    \ it is RECOMMENDED that the number of installed\n   routes and nodes closely\
    \ approximate that of the network (e.g.,\n   thousands of routes with tens or\
    \ hundreds of nodes).\n   The number of areas (for OSPF) and levels (for IS-IS)\
    \ can impact the\n   benchmark results.\n"
- title: 5.4.  Timers
  contents:
  - "5.4.  Timers\n   There are timers that may impact the measured IGP convergence\
    \ times.\n   The benchmark metrics MAY be measured at any fixed values for these\n\
    \   timers.  To obtain results similar to those that would be observed in\n  \
    \ an operational network, it is RECOMMENDED to configure the timers\n   with the\
    \ values as configured in the operational network.\n   Examples of timers that\
    \ may impact measured IGP convergence time\n   include, but are not limited to:\n\
    \      Interface failure indication\n      IGP hello timer\n      IGP dead-interval\
    \ or hold-timer\n      Link State Advertisement (LSA) or Link State Packet (LSP)\n\
    \      generation delay\n      LSA or LSP flood packet pacing\n      Route calculation\
    \ delay\n"
- title: 5.5.  Interface Types
  contents:
  - "5.5.  Interface Types\n   All test cases in this methodology document can be\
    \ executed with any\n   interface type.  The type of media may dictate which test\
    \ cases may\n   be executed.  Each interface type has a unique mechanism for\n\
    \   detecting link failures, and the speed at which that mechanism\n   operates\
    \ will influence the measurement results.  All interfaces MUST\n   be the same\
    \ media and Throughput [Br91][Br99] for each test case.\n   All interfaces SHOULD\
    \ be configured as point-to-point.\n"
- title: 5.6.  Offered Load
  contents:
  - "5.6.  Offered Load\n   The Throughput of the device, as defined in [Br91] and\
    \ benchmarked in\n   [Br99] at a fixed packet size, needs to be determined over\
    \ the\n   preferred path and over the next-best path.  The Offered Load SHOULD\n\
    \   be the minimum of the measured Throughput of the device over the\n   primary\
    \ path and over the backup path.  The packet size is selectable\n   and MUST be\
    \ recorded.  Packet size is measured in bytes and includes\n   the IP header and\
    \ payload.\n   The destination addresses for the Offered Load MUST be distributed\n\
    \   such that all routes or a statistically representative subset of all\n   routes\
    \ are matched and each of these routes is offered an equal share\n   of the Offered\
    \ Load.  It is RECOMMENDED to send traffic matching all\n   routes, but a statistically\
    \ representative subset of all routes can\n   be used if required.\n   Splitting\
    \ traffic flows across multiple paths (as with ECMP or\n   Parallel Link sets)\
    \ is in general done by hashing on various fields\n   on the IP or contained headers.\
    \  The hashing is typically based on\n   the IP source and destination addresses,\
    \ the protocol ID, and higher-\n   layer flow-dependent fields such as TCP/UDP\
    \ ports.  In practice,\n   within a network core, the hashing is based mainly\
    \ or exclusively on\n   the IP source and destination addresses.  Knowledge of\
    \ the hashing\n   algorithm used by the DUT is not always possible beforehand\
    \ and would\n   violate the black-box spirit of this document.  Therefore, it\
    \ is\n   RECOMMENDED to use a randomly distributed range of source and\n   destination\
    \ IP addresses, protocol IDs, and higher-layer flow-\n   dependent fields for\
    \ the packets of the Offered Load (see also\n   [Ne07]).  The content of the Offered\
    \ Load MUST remain the same during\n   the test.  It is RECOMMENDED to repeat\
    \ a test multiple times with\n   different random ranges of the header fields\
    \ such that convergence\n   time benchmarks are measured for different distributions\
    \ of traffic\n   over the available paths.\n   In the Remote Interface failure\
    \ test cases using topologies 3, 4, and\n   6, there is a possibility of a packet-forwarding\
    \ loop that may occur\n   transiently between DUT1 and DUT2 during convergence\
    \ (micro-loop, see\n   [Sh10]).  The Time To Live (TTL) or Hop Limit value of\
    \ the packets\n   sent by the Tester may influence the benchmark measurements\
    \ since it\n   determines which device in the topology may send an ICMP Time\n\
    \   Exceeded Message for looped packets.\n   The duration of the Offered Load\
    \ MUST be greater than the convergence\n   time plus the Sustained Convergence\
    \ Validation Time.\n   Offered load should send a packet to each destination before\
    \ sending\n   another packet to the same destination.  It is RECOMMENDED that\
    \ the\n   packets be transmitted in a round-robin fashion with a uniform\n   interpacket\
    \ delay.\n"
- title: 5.7.  Measurement Accuracy
  contents:
  - "5.7.  Measurement Accuracy\n   Since Impaired Packet count is observed to measure\
    \ the Route\n   Convergence Time, the time between two successive packets offered\
    \ to\n   each individual route is the highest possible accuracy of any\n   Impaired-Packet-based\
    \ measurement.  The higher the traffic rate\n   offered to each route, the higher\
    \ the possible measurement accuracy.\n   Also see Section 6 for method-specific\
    \ measurement accuracy.\n"
- title: 5.8.  Measurement Statistics
  contents:
  - "5.8.  Measurement Statistics\n   The benchmark measurements may vary for each\
    \ trial, due to the\n   statistical nature of timer expirations, CPU scheduling,\
    \ etc.\n   Evaluation of the test data must be done with an understanding of\n\
    \   generally accepted testing practices regarding repeatability,\n   variance,\
    \ and statistical significance of a small number of trials.\n"
- title: 5.9.  Tester Capabilities
  contents:
  - "5.9.  Tester Capabilities\n   It is RECOMMENDED that the Tester used to execute\
    \ each test case have\n   the following capabilities:\n   1.  Ability to establish\
    \ IGP adjacencies and advertise a single IGP\n       topology to one or more peers.\n\
    \   2.  Ability to measure Forwarding Delay, Duplicate Packets, and Out-\n   \
    \    of-Order Packets.\n   3.  An internal time clock to control timestamping,\
    \ time\n       measurements, and time calculations.\n   4.  Ability to distinguish\
    \ traffic load received on the Preferred and\n       Next-Best Interfaces [Po11t].\n\
    \   5.  Ability to disable or tune specific Layer 2 and Layer 3 protocol\n   \
    \    functions on any interface(s).\n   The Tester MAY be capable of making non-data-plane\
    \ convergence\n   observations and using those observations for measurements.\
    \  The\n   Tester MAY be capable of sending and receiving multiple traffic\n \
    \  Streams [Po06].\n   Also see Section 6 for method-specific capabilities.\n"
- title: 6.  Selection of Convergence Time Benchmark Metrics and Methods
  contents:
  - "6.  Selection of Convergence Time Benchmark Metrics and Methods\n   Different\
    \ convergence time benchmark methods MAY be used to measure\n   convergence time\
    \ benchmark metrics.  The Tester capabilities are\n   important criteria to select\
    \ a specific convergence time benchmark\n   method.  The criteria to select a\
    \ specific benchmark method include,\n   but are not limited to:\n   Tester capabilities:\
    \               Sampling Interval, number of\n                               \
    \       Stream statistics to collect\n   Measurement accuracy:              Sampling\
    \ Interval, Offered Load,\n                                      number of routes\n\
    \   Test specification:                number of routes\n   DUT capabilities:\
    \                  Throughput, IP Packet Delay\n                             \
    \         Variation\n"
- title: 6.1.  Loss-Derived Method
  contents:
  - '6.1.  Loss-Derived Method

    '
- title: 6.1.1.  Tester Capabilities
  contents:
  - "6.1.1.  Tester Capabilities\n   To enable collecting statistics of Out-of-Order\
    \ Packets per flow (see\n   [Th00], Section 3), the Offered Load SHOULD consist\
    \ of multiple\n   Streams [Po06], and each Stream SHOULD consist of a single flow.\
    \  If\n   sending multiple Streams, the measured traffic statistics for all\n\
    \   Streams MUST be added together.\n   In order to verify Full Convergence completion\
    \ and the Sustained\n   Convergence Validation Time, the Tester MUST measure Forwarding\
    \ Rate\n   each Packet Sampling Interval.\n   The total number of Impaired Packets\
    \ between the start of the traffic\n   and the end of the Sustained Convergence\
    \ Validation Time is used to\n   calculate the Loss-Derived Convergence Time.\n"
- title: 6.1.2.  Benchmark Metrics
  contents:
  - "6.1.2.  Benchmark Metrics\n   The Loss-Derived Method can be used to measure\
    \ the Loss-Derived\n   Convergence Time, which is the average convergence time\
    \ over all\n   routes, and to measure the Loss-Derived Loss of Connectivity Period,\n\
    \   which is the average Route Loss of Connectivity Period over all\n   routes.\n"
- title: 6.1.3.  Measurement Accuracy
  contents:
  - "6.1.3.  Measurement Accuracy\n   The actual value falls within the accuracy interval\
    \ [-(number of\n   destinations/Offered Load), +(number of destinations/Offered\
    \ Load)]\n   around the value as measured using the Loss-Derived Method.\n"
- title: 6.2.  Rate-Derived Method
  contents:
  - '6.2.  Rate-Derived Method

    '
- title: 6.2.1.  Tester Capabilities
  contents:
  - "6.2.1.  Tester Capabilities\n   To enable collecting statistics of Out-of-Order\
    \ Packets per flow (see\n   [Th00], Section 3), the Offered Load SHOULD consist\
    \ of multiple\n   Streams [Po06], and each Stream SHOULD consist of a single flow.\
    \  If\n   sending multiple Streams, the measured traffic statistics for all\n\
    \   Streams MUST be added together.\n   The Tester measures Forwarding Rate each\
    \ Sampling Interval.  The\n   Packet Sampling Interval influences the observation\
    \ of the different\n   convergence time instants.  If the Packet Sampling Interval\
    \ is large\n   compared to the time between the convergence time instants, then\
    \ the\n   different time instants may not be easily identifiable from the\n  \
    \ Forwarding Rate observation.  The presence of IP Packet Delay\n   Variation\
    \ (IPDV) [De02] may cause fluctuations of the Forwarding Rate\n   observation\
    \ and can prevent correct observation of the different\n   convergence time instants.\n\
    \   The Packet Sampling Interval MUST be larger than or equal to the time\n  \
    \ between two consecutive packets to the same destination.  For maximum\n   accuracy,\
    \ the value for the Packet Sampling Interval SHOULD be as\n   small as possible,\
    \ but the presence of IPDV may require the use of a\n   larger Packet Sampling\
    \ Interval.  The Packet Sampling Interval MUST\n   be reported.\n   IPDV causes\
    \ fluctuations in the number of received packets during\n   each Packet Sampling\
    \ Interval.  To account for the presence of IPDV\n   in determining if a convergence\
    \ instant has been reached, Forwarding\n   Delay SHOULD be observed during each\
    \ Packet Sampling Interval.  The\n   minimum and maximum number of packets expected\
    \ in a Packet Sampling\n   Interval in presence of IPDV can be calculated with\
    \ Equation 3.\n    number of packets expected in a Packet Sampling Interval\n\
    \      in presence of IP Packet Delay Variation\n        = expected number of\
    \ packets without IP Packet Delay Variation\n          +/-( (maxDelay - minDelay)\
    \ * Offered Load)\n    where minDelay and maxDelay indicate (respectively) the\
    \ minimum and\n      maximum Forwarding Delay of packets received during the Packet\n\
    \      Sampling Interval\n                                Equation 3\n   To determine\
    \ if a convergence instant has been reached, the number of\n   packets received\
    \ in a Packet Sampling Interval is compared with the\n   range of expected number\
    \ of packets calculated in Equation 3.\n"
- title: 6.2.2.  Benchmark Metrics
  contents:
  - "6.2.2.  Benchmark Metrics\n   The Rate-Derived Method SHOULD be used to measure\
    \ First Route\n   Convergence Time and Full Convergence Time.  It SHOULD NOT be\
    \ used to\n   measure Loss of Connectivity Period (see Section 4).\n"
- title: 6.2.3.  Measurement Accuracy
  contents:
  - "6.2.3.  Measurement Accuracy\n   The measurement accuracy interval of the Rate-Derived\
    \ Method depends\n   on the metric being measured or calculated and the characteristics\
    \ of\n   the related transition.  IP Packet Delay Variation (IPDV) [De02] adds\n\
    \   uncertainty to the amount of packets received in a Packet Sampling\n   Interval,\
    \ and this uncertainty adds to the measurement error.  The\n   effect of IPDV\
    \ is not accounted for in the calculation of the\n   accuracy intervals below.\
    \  IPDV is of importance for the convergence\n   instants where a variation in\
    \ Forwarding Rate needs to be observed.\n   This is applicable to the Convergence\
    \ Recovery Instant for all\n   topologies, and for topologies with ECMP it also\
    \ applies to the\n   Convergence Event Instant and the First Route Convergence\
    \ Instant.\n   and for topologies with ECMP also Convergence Event Instant and\
    \ First\n   Route Convergence Instant).\n   If the Convergence Event Instant is\
    \ observed on the data plane using\n   the Rate Derived Method, it needs to be\
    \ instantaneous for all routes\n   (see Section 4.1).  The actual value of the\
    \ Convergence Event Instant\n   falls within the accuracy interval [-(Packet Sampling\
    \ Interval +\n   1/Offered Load), +0] around the value as measured using the Rate-\n\
    \   Derived Method.\n   If the Convergence Recovery Transition is non-instantaneous\
    \ for all\n   routes, then the actual value of the First Route Convergence Instant\n\
    \   falls within the accuracy interval [-(Packet Sampling Interval + time\n  \
    \ between two consecutive packets to the same destination), +0] around\n   the\
    \ value as measured using the Rate-Derived Method, and the actual\n   value of\
    \ the Convergence Recovery Instant falls within the accuracy\n   interval [-(2\
    \ * Packet Sampling Interval), -(Packet Sampling Interval\n   - time between two\
    \ consecutive packets to the same destination)]\n   around the value as measured\
    \ using the Rate-Derived Method.\n   The term \"time between two consecutive packets\
    \ to the same\n   destination\" is added in the above accuracy intervals since\
    \ packets\n   are sent in a particular order to all destinations in a stream,\
    \ and\n   when part of the routes experience packet loss, it is unknown where\n\
    \   in the transmit cycle packets to these routes are sent.  This\n   uncertainty\
    \ adds to the error.\n   The accuracy intervals of the derived metrics First Route\
    \ Convergence\n   Time and Rate-Derived Convergence Time are calculated from the\
    \ above\n   convergence instants accuracy intervals.  The actual value of First\n\
    \   Route Convergence Time falls within the accuracy interval [-(Packet\n   Sampling\
    \ Interval + time between two consecutive packets to the same\n   destination),\
    \ +(Packet Sampling Interval + 1/Offered Load)] around\n   the calculated value.\
    \  The actual value of Rate-Derived Convergence\n   Time falls within the accuracy\
    \ interval [-(2 * Packet Sampling\n   Interval), +(time between two consecutive\
    \ packets to the same\n   destination + 1/Offered Load)] around the calculated\
    \ value.\n"
- title: 6.3.  Route-Specific Loss-Derived Method
  contents:
  - '6.3.  Route-Specific Loss-Derived Method

    '
- title: 6.3.1.  Tester Capabilities
  contents:
  - "6.3.1.  Tester Capabilities\n   The Offered Load consists of multiple Streams.\
    \  The Tester MUST\n   measure Impaired Packet count for each Stream separately.\n\
    \   In order to verify Full Convergence completion and the Sustained\n   Convergence\
    \ Validation Time, the Tester MUST measure Forwarding Rate\n   each Packet Sampling\
    \ Interval.  This measurement at each Packet\n   Sampling Interval MAY be per\
    \ Stream.\n   Only the total number of Impaired Packets measured per Stream at\
    \ the\n   end of the Sustained Convergence Validation Time is used to calculate\n\
    \   the benchmark metrics with this method.\n"
- title: 6.3.2.  Benchmark Metrics
  contents:
  - "6.3.2.  Benchmark Metrics\n   The Route-Specific Loss-Derived Method SHOULD be\
    \ used to measure\n   Route-Specific Convergence Times.  It is the RECOMMENDED\
    \ method to\n   measure Route Loss of Connectivity Period.\n   Under the conditions\
    \ explained in Section 4, First Route Convergence\n   Time and Full Convergence\
    \ Time, as benchmarked using Rate-Derived\n   Method, may be equal to the minimum\
    \ and maximum (respectively) of the\n   Route-Specific Convergence Times.\n"
- title: 6.3.3.  Measurement Accuracy
  contents:
  - "6.3.3.  Measurement Accuracy\n   The actual value falls within the accuracy interval\
    \ [-(number of\n   destinations/Offered Load), +(number of destinations/Offered\
    \ Load)]\n   around the value as measured using the Route-Specific Loss-Derived\n\
    \   Method.\n"
- title: 7.  Reporting Format
  contents:
  - "7.  Reporting Format\n   For each test case, it is RECOMMENDED that the reporting\
    \ tables below\n   be completed.  All time values SHOULD be reported with a sufficiently\n\
    \   high resolution (fractions of a second sufficient to distinguish\n   significant\
    \ differences between measured values).\n     Parameter                      \
    \       Units\n     ------------------------------------- ---------------------------\n\
    \     Test Case                             test case number\n     Test Topology\
    \                         Test Topology Figure number\n     IGP              \
    \                     (IS-IS, OSPF, other)\n     Interface Type              \
    \          (GigE, POS, ATM, other)\n     Packet Size offered to DUT          \
    \  bytes\n     Offered Load                          packets per second\n    \
    \ IGP Routes Advertised to DUT          number of IGP routes\n     Nodes in Emulated\
    \ Network             number of nodes\n     Number of Parallel or ECMP links \
    \     number of links\n     Number of Routes Measured             number of routes\n\
    \     Packet Sampling Interval on Tester    seconds\n     Forwarding Delay Threshold\
    \            seconds\n     Timer Values configured on DUT:\n      Interface Failure\
    \ Indication Delay   seconds\n      IGP Hello Timer                      seconds\n\
    \      IGP Dead-Interval or Hold-Time       seconds\n      LSA/LSP Generation\
    \ Delay             seconds\n      LSA/LSP Flood Packet Pacing          seconds\n\
    \      LSA/LSP Retransmission Packet Pacing seconds\n      Route Calculation Delay\
    \              seconds\n   Test Details:\n      Describe the IGP extensions and\
    \ IGP security mechanisms that are\n      configured on the DUT.\n      Describe\
    \ how the various fields on the IP and contained headers\n      for the packets\
    \ for the Offered Load are generated (Section 5.6).\n      If the Offered Load\
    \ matches a subset of routes, describe how this\n      subset is selected.\n \
    \     Describe how the Convergence Event is applied; does it cause\n      instantaneous\
    \ traffic loss or not?\n   The table below should be completed for the initial\
    \ Convergence Event\n   and the reversion Convergence Event.\n    Parameter  \
    \                                 Units\n    -------------------------------------------\
    \ ----------------------\n    Convergence Event                           (initial\
    \ or reversion)\n    Traffic Forwarding Metrics:\n     Total number of packets\
    \ offered to DUT     number of packets\n     Total number of packets forwarded\
    \ by DUT   number of packets\n     Connectivity Packet Loss                  \
    \ number of packets\n     Convergence Packet Loss                    number of\
    \ packets\n     Out-of-Order Packets                       number of packets\n\
    \     Duplicate Packets                          number of packets\n     Excessive\
    \ Forwarding Delay Packets         number of packets\n    Convergence Benchmarks:\n\
    \     Rate-Derived Method:\n      First Route Convergence Time              seconds\n\
    \      Full Convergence Time                     seconds\n     Loss-Derived Method:\n\
    \      Loss-Derived Convergence Time             seconds\n     Route-Specific\
    \ Loss-Derived Method:\n      Route-Specific Convergence Time[n]        array\
    \ of seconds\n      Minimum Route-Specific Convergence Time   seconds\n      Maximum\
    \ Route-Specific Convergence Time   seconds\n      Median Route-Specific Convergence\
    \ Time    seconds\n      Average Route-Specific Convergence Time   seconds\n \
    \   Loss of Connectivity Benchmarks:\n     Loss-Derived Method:\n      Loss-Derived\
    \ Loss of Connectivity Period  seconds\n     Route-Specific Loss-Derived Method:\n\
    \      Route Loss of Connectivity Period[n]      array of seconds\n      Minimum\
    \ Route Loss of Connectivity Period seconds\n      Maximum Route Loss of Connectivity\
    \ Period seconds\n      Median Route Loss of Connectivity Period  seconds\n  \
    \    Average Route Loss of Connectivity Period seconds\n"
- title: 8.  Test Cases
  contents:
  - "8.  Test Cases\n   It is RECOMMENDED that all applicable test cases be performed\
    \ for\n   best characterization of the DUT.  The test cases follow a generic\n\
    \   procedure tailored to the specific DUT configuration and Convergence\n   Event\
    \ [Po11t].  This generic procedure is as follows:\n   1.   Establish DUT and Tester\
    \ configurations and advertise an IGP\n        topology from Tester to DUT.\n\
    \   2.   Send Offered Load from Tester to DUT on Ingress Interface.\n   3.   Verify\
    \ traffic is routed correctly.  Verify if traffic is\n        forwarded without\
    \ Impaired Packets [Po06].\n   4.   Introduce Convergence Event [Po11t].\n   5.\
    \   Measure First Route Convergence Time [Po11t].\n   6.   Measure Full Convergence\
    \ Time [Po11t].\n   7.   Stop Offered Load.\n   8.   Measure Route-Specific Convergence\
    \ Times, Loss-Derived\n        Convergence Time, Route Loss of Connectivity Periods,\
    \ and Loss-\n        Derived Loss of Connectivity Period [Po11t].  At the same\
    \ time,\n        measure number of Impaired Packets [Po11t].\n   9.   Wait sufficient\
    \ time for queues to drain.  The duration of this\n        time period MUST be\
    \ larger than or equal to the Forwarding Delay\n        Threshold.\n   10.  Restart\
    \ Offered Load.\n   11.  Reverse Convergence Event.\n   12.  Measure First Route\
    \ Convergence Time.\n   13.  Measure Full Convergence Time.\n   14.  Stop Offered\
    \ Load.\n   15.  Measure Route-Specific Convergence Times, Loss-Derived\n    \
    \    Convergence Time, Route Loss of Connectivity Periods, and Loss-\n       \
    \ Derived Loss of Connectivity Period.  At the same time, measure\n        number\
    \ of Impaired Packets [Po11t].\n"
- title: 8.1.  Interface Failure and Recovery
  contents:
  - '8.1.  Interface Failure and Recovery

    '
- title: 8.1.1.  Convergence Due to Local Interface Failure and Recovery
  contents:
  - "8.1.1.  Convergence Due to Local Interface Failure and Recovery\n   Objective:\n\
    \      To obtain the IGP convergence measurements for Local Interface\n      failure\
    \ and recovery events.  The Next-Best Egress Interface can\n      be a single\
    \ interface (Figure 1) or an ECMP set (Figure 2).  The\n      test with ECMP topology\
    \ (Figure 2) is OPTIONAL.\n   Procedure:\n   1.   Advertise an IGP topology from\
    \ Tester to DUT using the topology\n        shown in Figures 1 or 2.\n   2.  \
    \ Send Offered Load from Tester to DUT on Ingress Interface.\n   3.   Verify traffic\
    \ is forwarded over Preferred Egress Interface.\n   4.   Remove link on the Preferred\
    \ Egress Interface of the DUT.  This\n        is the Convergence Event.\n   5.\
    \   Measure First Route Convergence Time.\n   6.   Measure Full Convergence Time.\n\
    \   7.   Stop Offered Load.\n   8.   Measure Route-Specific Convergence Times\
    \ and Loss-Derived\n        Convergence Time.  At the same time, measure number\
    \ of Impaired\n        Packets.\n   9.   Wait sufficient time for queues to drain.\n\
    \   10.  Restart Offered Load.\n   11.  Restore link on the Preferred Egress Interface\
    \ of the DUT.\n   12.  Measure First Route Convergence Time.\n   13.  Measure\
    \ Full Convergence Time.\n   14.  Stop Offered Load.\n   15.  Measure Route-Specific\
    \ Convergence Times, Loss-Derived\n        Convergence Time, Route Loss of Connectivity\
    \ Periods, and Loss-\n        Derived Loss of Connectivity Period.  At the same\
    \ time, measure\n        number of Impaired Packets.\n"
- title: 8.1.2.  Convergence Due to Remote Interface Failure and Recovery
  contents:
  - "8.1.2.  Convergence Due to Remote Interface Failure and Recovery\n   Objective:\n\
    \      To obtain the IGP convergence measurements for Remote Interface\n     \
    \ failure and recovery events.  The Next-Best Egress Interface can\n      be a\
    \ single interface (Figure 3) or an ECMP set (Figure 4).  The\n      test with\
    \ ECMP topology (Figure 4) is OPTIONAL.\n   Procedure:\n   1.   Advertise an IGP\
    \ topology from Tester to SUT using the topology\n        shown in Figures 3 or\
    \ 4.\n   2.   Send Offered Load from Tester to SUT on Ingress Interface.\n   3.\
    \   Verify traffic is forwarded over Preferred Egress Interface.\n   4.   Remove\
    \ link on the interface of the Tester connected to the\n        Preferred Egress\
    \ Interface of the SUT.  This is the Convergence\n        Event.\n   5.   Measure\
    \ First Route Convergence Time.\n   6.   Measure Full Convergence Time.\n   7.\
    \   Stop Offered Load.\n   8.   Measure Route-Specific Convergence Times and Loss-Derived\n\
    \        Convergence Time.  At the same time, measure number of Impaired\n   \
    \     Packets.\n   9.   Wait sufficient time for queues to drain.\n   10.  Restart\
    \ Offered Load.\n   11.  Restore link on the interface of the Tester connected\
    \ to the\n        Preferred Egress Interface of the SUT.\n   12.  Measure First\
    \ Route Convergence Time.\n   13.  Measure Full Convergence Time.\n   14.  Stop\
    \ Offered Load.\n   15.  Measure Route-Specific Convergence Times, Loss-Derived\n\
    \        Convergence Time, Route Loss of Connectivity Periods, and Loss-\n   \
    \     Derived Loss of Connectivity Period.  At the same time, measure\n      \
    \  number of Impaired Packets.\n   Discussion:\n      In this test case, there\
    \ is a possibility of a packet-forwarding\n      loop that may occur transiently\
    \ between DUT1 and DUT2 during\n      convergence (micro-loop, see [Sh10]), which\
    \ may increase the\n      measured convergence times and loss of connectivity\
    \ periods.\n"
- title: 8.1.3.  Convergence Due to ECMP Member Local Interface Failure and
  contents:
  - "8.1.3.  Convergence Due to ECMP Member Local Interface Failure and\n        Recovery\n\
    \   Objective:\n      To obtain the IGP convergence measurements for Local Interface\n\
    \      link failure and recovery events of an ECMP Member.\n   Procedure:\n  \
    \ 1.   Advertise an IGP topology from Tester to DUT using the test\n        setup\
    \ shown in Figure 5.\n   2.   Send Offered Load from Tester to DUT on Ingress\
    \ Interface.\n   3.   Verify traffic is forwarded over the ECMP member interface\
    \ of\n        the DUT that will be failed in the next step.\n   4.   Remove link\
    \ on one of the ECMP member interfaces of the DUT.\n        This is the Convergence\
    \ Event.\n   5.   Measure First Route Convergence Time.\n   6.   Measure Full\
    \ Convergence Time.\n   7.   Stop Offered Load.\n   8.   Measure Route-Specific\
    \ Convergence Times and Loss-Derived\n        Convergence Time.  At the same time,\
    \ measure number of Impaired\n        Packets.\n   9.   Wait sufficient time for\
    \ queues to drain.\n   10.  Restart Offered Load.\n   11.  Restore link on the\
    \ ECMP member interface of the DUT.\n   12.  Measure First Route Convergence Time.\n\
    \   13.  Measure Full Convergence Time.\n   14.  Stop Offered Load.\n   15.  Measure\
    \ Route-Specific Convergence Times, Loss-Derived\n        Convergence Time, Route\
    \ Loss of Connectivity Periods, and Loss-\n        Derived Loss of Connectivity\
    \ Period.  At the same time, measure\n        number of Impaired Packets.\n"
- title: 8.1.4.  Convergence Due to ECMP Member Remote Interface Failure and
  contents:
  - "8.1.4.  Convergence Due to ECMP Member Remote Interface Failure and\n       \
    \ Recovery\n   Objective:\n      To obtain the IGP convergence measurements for\
    \ Remote Interface\n      link failure and recovery events for an ECMP Member.\n\
    \   Procedure:\n   1.   Advertise an IGP topology from Tester to DUT using the\
    \ test\n        setup shown in Figure 6.\n   2.   Send Offered Load from Tester\
    \ to DUT on Ingress Interface.\n   3.   Verify traffic is forwarded over the ECMP\
    \ member interface of\n        the DUT that will be failed in the next step.\n\
    \   4.   Remove link on the interface of the Tester to R2.  This is the\n    \
    \    Convergence Event Trigger.\n   5.   Measure First Route Convergence Time.\n\
    \   6.   Measure Full Convergence Time.\n   7.   Stop Offered Load.\n   8.   Measure\
    \ Route-Specific Convergence Times and Loss-Derived\n        Convergence Time.\
    \  At the same time, measure number of Impaired\n        Packets.\n   9.   Wait\
    \ sufficient time for queues to drain.\n   10.  Restart Offered Load.\n   11.\
    \  Restore link on the interface of the Tester to R2.\n   12.  Measure First Route\
    \ Convergence Time.\n   13.  Measure Full Convergence Time.\n   14.  Stop Offered\
    \ Load.\n   15.  Measure Route-Specific Convergence Times, Loss-Derived\n    \
    \    Convergence Time, Route Loss of Connectivity Periods, and Loss-\n       \
    \ Derived Loss of Connectivity Period.  At the same time, measure\n        number\
    \ of Impaired Packets.\n   Discussion:\n      In this test case, there is a possibility\
    \ of a packet-forwarding\n      loop that may occur temporarily between DUT1 and\
    \ DUT2 during\n      convergence (micro-loop, see [Sh10]), which may increase\
    \ the\n      measured convergence times and loss of connectivity periods.\n"
- title: 8.1.5.  Convergence Due to Parallel Link Interface Failure and Recovery
  contents:
  - "8.1.5.  Convergence Due to Parallel Link Interface Failure and Recovery\n   Objective:\n\
    \      To obtain the IGP convergence measurements for local link failure\n   \
    \   and recovery events for a member of a parallel link.  The links\n      can\
    \ be used for data load-balancing\n   Procedure:\n   1.   Advertise an IGP topology\
    \ from Tester to DUT using the test\n        setup shown in Figure 7.\n   2. \
    \  Send Offered Load from Tester to DUT on Ingress Interface.\n   3.   Verify\
    \ traffic is forwarded over the parallel link member that\n        will be failed\
    \ in the next step.\n   4.   Remove link on one of the parallel link member interfaces\
    \ of the\n        DUT.  This is the Convergence Event.\n   5.   Measure First\
    \ Route Convergence Time.\n   6.   Measure Full Convergence Time.\n   7.   Stop\
    \ Offered Load.\n   8.   Measure Route-Specific Convergence Times and Loss-Derived\n\
    \        Convergence Time.  At the same time, measure number of Impaired\n   \
    \     Packets.\n   9.   Wait sufficient time for queues to drain.\n   10.  Restart\
    \ Offered Load.\n   11.  Restore link on the Parallel Link member interface of\
    \ the DUT.\n   12.  Measure First Route Convergence Time.\n   13.  Measure Full\
    \ Convergence Time.\n   14.  Stop Offered Load.\n   15.  Measure Route-Specific\
    \ Convergence Times, Loss-Derived\n        Convergence Time, Route Loss of Connectivity\
    \ Periods, and Loss-\n        Derived Loss of Connectivity Period.  At the same\
    \ time, measure\n        number of Impaired Packets.\n"
- title: 8.2.  Other Failures and Recoveries
  contents:
  - '8.2.  Other Failures and Recoveries

    '
- title: 8.2.1.  Convergence Due to Layer 2 Session Loss and Recovery
  contents:
  - "8.2.1.  Convergence Due to Layer 2 Session Loss and Recovery\n   Objective:\n\
    \      To obtain the IGP convergence measurements for a local Layer 2\n      loss\
    \ and recovery.\n   Procedure:\n   1.   Advertise an IGP topology from Tester\
    \ to DUT using the topology\n        shown in Figure 1.\n   2.   Send Offered\
    \ Load from Tester to DUT on Ingress Interface.\n   3.   Verify traffic is routed\
    \ over Preferred Egress Interface.\n   4.   Remove Layer 2 session from Preferred\
    \ Egress Interface of the\n        DUT.  This is the Convergence Event.\n   5.\
    \   Measure First Route Convergence Time.\n   6.   Measure Full Convergence Time.\n\
    \   7.   Stop Offered Load.\n   8.   Measure Route-Specific Convergence Times,\
    \ Loss-Derived\n        Convergence Time, Route Loss of Connectivity Periods,\
    \ and Loss-\n        Derived Loss of Connectivity Period.  At the same time, measure\n\
    \        number of Impaired Packets.\n   9.   Wait sufficient time for queues\
    \ to drain.\n   10.  Restart Offered Load.\n   11.  Restore Layer 2 session on\
    \ Preferred Egress Interface of the\n        DUT.\n   12.  Measure First Route\
    \ Convergence Time.\n   13.  Measure Full Convergence Time.\n   14.  Stop Offered\
    \ Load.\n   15.  Measure Route-Specific Convergence Times, Loss-Derived\n    \
    \    Convergence Time, Route Loss of Connectivity Periods, and Loss-\n       \
    \ Derived Loss of Connectivity Period.  At the same time, measure\n        number\
    \ of Impaired Packets.\n   Discussion:\n      When removing the Layer 2 session,\
    \ the physical layer must stay\n      up.  Configure IGP timers such that the\
    \ IGP adjacency does not\n      time out before Layer 2 failure is detected.\n\
    \      To measure convergence time, traffic SHOULD start dropping on the\n   \
    \   Preferred Egress Interface on the instant the Layer 2 session is\n      removed.\
    \  Alternatively, the Tester SHOULD record the time the\n      instant Layer 2\
    \ session is removed, and traffic loss SHOULD only\n      be measured on the Next-Best\
    \ Egress Interface.  For loss-derived\n      benchmarks, the time of the Start\
    \ Traffic Instant SHOULD be\n      recorded as well.  See Section 4.1.\n"
- title: 8.2.2.  Convergence Due to Loss and Recovery of IGP Adjacency
  contents:
  - "8.2.2.  Convergence Due to Loss and Recovery of IGP Adjacency\n   Objective:\n\
    \      To obtain the IGP convergence measurements for loss and recovery\n    \
    \  of an IGP Adjacency.  The IGP adjacency is removed on the Tester\n      by\
    \ disabling processing of IGP routing protocol packets on the\n      Tester.\n\
    \   Procedure:\n   1.   Advertise an IGP topology from Tester to DUT using the\
    \ topology\n        shown in Figure 1.\n   2.   Send Offered Load from Tester\
    \ to DUT on Ingress Interface.\n   3.   Verify traffic is routed over Preferred\
    \ Egress Interface.\n   4.   Remove IGP adjacency from the Preferred Egress Interface\
    \ while\n        the Layer 2 session MUST be maintained.  This is the Convergence\n\
    \        Event.\n   5.   Measure First Route Convergence Time.\n   6.   Measure\
    \ Full Convergence Time.\n   7.   Stop Offered Load.\n   8.   Measure Route-Specific\
    \ Convergence Times, Loss-Derived\n        Convergence Time, Route Loss of Connectivity\
    \ Periods, and Loss-\n        Derived Loss of Connectivity Period.  At the same\
    \ time, measure\n        number of Impaired Packets.\n   9.   Wait sufficient\
    \ time for queues to drain.\n   10.  Restart Offered Load.\n   11.  Restore IGP\
    \ session on Preferred Egress Interface of the DUT.\n   12.  Measure First Route\
    \ Convergence Time.\n   13.  Measure Full Convergence Time.\n   14.  Stop Offered\
    \ Load.\n   15.  Measure Route-Specific Convergence Times, Loss-Derived\n    \
    \    Convergence Time, Route Loss of Connectivity Periods, and Loss-\n       \
    \ Derived Loss of Connectivity Period.  At the same time, measure\n        number\
    \ of Impaired Packets.\n   Discussion:\n      Configure Layer 2 such that Layer\
    \ 2 does not time out before IGP\n      adjacency failure is detected.\n     \
    \ To measure convergence time, traffic SHOULD start dropping on the\n      Preferred\
    \ Egress Interface on the instant the IGP adjacency is\n      removed.  Alternatively,\
    \ the Tester SHOULD record the time the\n      instant the IGP adjacency is removed\
    \ and traffic loss SHOULD only\n      be measured on the Next-Best Egress Interface.\
    \  For loss-derived\n      benchmarks, the time of the Start Traffic Instant SHOULD\
    \ be\n      recorded as well.  See Section 4.1.\n"
- title: 8.2.3.  Convergence Due to Route Withdrawal and Re-Advertisement
  contents:
  - "8.2.3.  Convergence Due to Route Withdrawal and Re-Advertisement\n   Objective:\n\
    \      To obtain the IGP convergence measurements for route withdrawal\n     \
    \ and re-advertisement.\n   Procedure:\n   1.   Advertise an IGP topology from\
    \ Tester to DUT using the topology\n        shown in Figure 1.  The routes that\
    \ will be withdrawn MUST be a\n        set of leaf routes advertised by at least\
    \ two nodes in the\n        emulated topology.  The topology SHOULD be such that\
    \ before the\n        withdrawal the DUT prefers the leaf routes advertised by\
    \ a node\n        \"nodeA\" via the Preferred Egress Interface, and after the\n\
    \        withdrawal the DUT prefers the leaf routes advertised by a node\n   \
    \     \"nodeB\" via the Next-Best Egress Interface.\n   2.   Send Offered Load\
    \ from Tester to DUT on Ingress Interface.\n   3.   Verify traffic is routed over\
    \ Preferred Egress Interface.\n   4.   The Tester withdraws the set of IGP leaf\
    \ routes from nodeA.\n        This is the Convergence Event.  The withdrawal update\
    \ message\n        SHOULD be a single unfragmented packet.  If the routes cannot\
    \ be\n        withdrawn by a single packet, the messages SHOULD be sent using\n\
    \        the same pacing characteristics as the DUT.  The Tester MAY\n       \
    \ record the time it sends the withdrawal message(s).\n   5.   Measure First Route\
    \ Convergence Time.\n   6.   Measure Full Convergence Time.\n   7.   Stop Offered\
    \ Load.\n   8.   Measure Route-Specific Convergence Times, Loss-Derived\n    \
    \    Convergence Time, Route Loss of Connectivity Periods, and Loss-\n       \
    \ Derived Loss of Connectivity Period.  At the same time, measure\n        number\
    \ of Impaired Packets.\n   9.   Wait sufficient time for queues to drain.\n  \
    \ 10.  Restart Offered Load.\n   11.  Re-advertise the set of withdrawn IGP leaf\
    \ routes from nodeA\n        emulated by the Tester.  The update message SHOULD\
    \ be a single\n        unfragmented packet.  If the routes cannot be advertised\
    \ by a\n        single packet, the messages SHOULD be sent using the same pacing\n\
    \        characteristics as the DUT.  The Tester MAY record the time it\n    \
    \    sends the update message(s).\n   12.  Measure First Route Convergence Time.\n\
    \   13.  Measure Full Convergence Time.\n   14.  Stop Offered Load.\n   15.  Measure\
    \ Route-Specific Convergence Times, Loss-Derived\n        Convergence Time, Route\
    \ Loss of Connectivity Periods, and Loss-\n        Derived Loss of Connectivity\
    \ Period.  At the same time, measure\n        number of Impaired Packets.\n  \
    \ Discussion:\n      To measure convergence time, traffic SHOULD start dropping\
    \ on the\n      Preferred Egress Interface on the instant the routes are withdrawn\n\
    \      by the Tester.  Alternatively, the Tester SHOULD record the time\n    \
    \  the instant the routes are withdrawn, and traffic loss SHOULD only\n      be\
    \ measured on the Next-Best Egress Interface.  For loss-derived\n      benchmarks,\
    \ the time of the Start Traffic Instant SHOULD be\n      recorded as well.  See\
    \ Section 4.1.\n"
- title: 8.3.  Administrative Changes
  contents:
  - '8.3.  Administrative Changes

    '
- title: 8.3.1.  Convergence Due to Local Interface Administrative Changes
  contents:
  - "8.3.1.  Convergence Due to Local Interface Administrative Changes\n   Objective:\n\
    \      To obtain the IGP convergence measurements for administratively\n     \
    \ disabling and enabling a Local Interface.\n   Procedure:\n   1.   Advertise\
    \ an IGP topology from Tester to DUT using the topology\n        shown in Figure\
    \ 1.\n   2.   Send Offered Load from Tester to DUT on Ingress Interface.\n   3.\
    \   Verify traffic is routed over Preferred Egress Interface.\n   4.   Administratively\
    \ disable the Preferred Egress Interface of the\n        DUT.  This is the Convergence\
    \ Event.\n   5.   Measure First Route Convergence Time.\n   6.   Measure Full\
    \ Convergence Time.\n   7.   Stop Offered Load.\n   8.   Measure Route-Specific\
    \ Convergence Times, Loss-Derived\n        Convergence Time, Route Loss of Connectivity\
    \ Periods, and Loss-\n        Derived Loss of Connectivity Period.  At the same\
    \ time, measure\n        number of Impaired Packets.\n   9.   Wait sufficient\
    \ time for queues to drain.\n   10.  Restart Offered Load.\n   11.  Administratively\
    \ enable the Preferred Egress Interface of the\n        DUT.\n   12.  Measure\
    \ First Route Convergence Time.\n   13.  Measure Full Convergence Time.\n   14.\
    \  Stop Offered Load.\n   15.  Measure Route-Specific Convergence Times, Loss-Derived\n\
    \        Convergence Time, Route Loss of Connectivity Periods, and Loss-\n   \
    \     Derived Loss of Connectivity Period.  At the same time, measure\n      \
    \  number of Impaired Packets.\n"
- title: 8.3.2.  Convergence Due to Cost Change
  contents:
  - "8.3.2.  Convergence Due to Cost Change\n   Objective:\n      To obtain the IGP\
    \ convergence measurements for route cost change.\n   Procedure:\n   1.   Advertise\
    \ an IGP topology from Tester to DUT using the topology\n        shown in Figure\
    \ 1.\n   2.   Send Offered Load from Tester to DUT on Ingress Interface.\n   3.\
    \   Verify traffic is routed over Preferred Egress Interface.\n   4.   The Tester,\
    \ emulating the neighbor node, increases the cost for\n        all IGP routes\
    \ at the Preferred Egress Interface of the DUT so\n        that the Next-Best\
    \ Egress Interface becomes the preferred path.\n        The update message advertising\
    \ the higher cost MUST be a single\n        unfragmented packet.  This is the\
    \ Convergence Event.  The Tester\n        MAY record the time it sends the update\
    \ message advertising the\n        higher cost on the Preferred Egress Interface.\n\
    \   5.   Measure First Route Convergence Time.\n   6.   Measure Full Convergence\
    \ Time.\n   7.   Stop Offered Load.\n   8.   Measure Route-Specific Convergence\
    \ Times, Loss-Derived\n        Convergence Time, Route Loss of Connectivity Periods,\
    \ and Loss-\n        Derived Loss of Connectivity Period.  At the same time, measure\n\
    \        number of Impaired Packets.\n   9.   Wait sufficient time for queues\
    \ to drain.\n   10.  Restart Offered Load.\n   11.  The Tester, emulating the\
    \ neighbor node, decreases the cost for\n        all IGP routes at the Preferred\
    \ Egress Interface of the DUT so\n        that the Preferred Egress Interface\
    \ becomes the preferred path.\n        The update message advertising the lower\
    \ cost MUST be a single\n        unfragmented packet.\n   12.  Measure First Route\
    \ Convergence Time.\n   13.  Measure Full Convergence Time.\n   14.  Stop Offered\
    \ Load.\n   15.  Measure Route-Specific Convergence Times, Loss-Derived\n    \
    \    Convergence Time, Route Loss of Connectivity Periods, and Loss-\n       \
    \ Derived Loss of Connectivity Period.  At the same time, measure\n        number\
    \ of Impaired Packets.\n   Discussion:\n      To measure convergence time, traffic\
    \ SHOULD start dropping on the\n      Preferred Egress Interface on the instant\
    \ the cost is changed by\n      the Tester.  Alternatively, the Tester SHOULD\
    \ record the time the\n      instant the cost is changed, and traffic loss SHOULD\
    \ only be\n      measured on the Next-Best Egress Interface.  For loss-derived\n\
    \      benchmarks, the time of the Start Traffic Instant SHOULD be\n      recorded\
    \ as well.  See Section 4.1.\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   Benchmarking activities as described in this\
    \ memo are limited to\n   technology characterization using controlled stimuli\
    \ in a laboratory\n   environment, with dedicated address space and the constraints\n\
    \   specified in the sections above.\n   The benchmarking network topology will\
    \ be an independent test setup\n   and MUST NOT be connected to devices that may\
    \ forward the test\n   traffic into a production network or misroute traffic to\
    \ the test\n   management network.\n   Further, benchmarking is performed on a\
    \ \"black-box\" basis, relying\n   solely on measurements observable external\
    \ to the DUT/SUT.\n   Special capabilities SHOULD NOT exist in the DUT/SUT specifically\
    \ for\n   benchmarking purposes.  Any implications for network security arising\n\
    \   from the DUT/SUT SHOULD be identical in the lab and in production\n   networks.\n"
- title: 10.  Acknowledgements
  contents:
  - "10.  Acknowledgements\n   Thanks to Sue Hares, Al Morton, Kevin Dubray, Ron Bonica,\
    \ David Ward,\n   Peter De Vriendt, Anuj Dewagan, Julien Meuric, Adrian Farrel,\
    \ Stewart\n   Bryant, and the Benchmarking Methodology Working Group for their\n\
    \   contributions to this work.\n"
- title: 11.  References
  contents:
  - '11.  References

    '
- title: 11.1.  Normative References
  contents:
  - "11.1.  Normative References\n   [Br91]   Bradner, S., \"Benchmarking terminology\
    \ for network\n            interconnection devices\", RFC 1242, July 1991.\n \
    \  [Br97]   Bradner, S., \"Key words for use in RFCs to Indicate\n           \
    \ Requirement Levels\", BCP 14, RFC 2119, March 1997.\n   [Br99]   Bradner, S.\
    \ and J. McQuaid, \"Benchmarking Methodology for\n            Network Interconnect\
    \ Devices\", RFC 2544, March 1999.\n   [Ca90]   Callon, R., \"Use of OSI IS-IS\
    \ for routing in TCP/IP and dual\n            environments\", RFC 1195, December\
    \ 1990.\n   [Co08]   Coltun, R., Ferguson, D., Moy, J., and A. Lindem, \"OSPF\
    \ for\n            IPv6\", RFC 5340, July 2008.\n   [De02]   Demichelis, C. and\
    \ P. Chimento, \"IP Packet Delay Variation\n            Metric for IP Performance\
    \ Metrics (IPPM)\", RFC 3393,\n            November 2002.\n   [Ho08]   Hopps,\
    \ C., \"Routing IPv6 with IS-IS\", RFC 5308,\n            October 2008.\n   [Ko02]\
    \   Koodli, R. and R. Ravikanth, \"One-way Loss Pattern Sample\n            Metrics\"\
    , RFC 3357, August 2002.\n   [Ma05]   Manral, V., White, R., and A. Shaikh, \"\
    Benchmarking Basic\n            OSPF Single Router Control Plane Convergence\"\
    , RFC 4061,\n            April 2005.\n   [Ma05c]  Manral, V., White, R., and A.\
    \ Shaikh, \"Considerations When\n            Using Basic OSPF Convergence Benchmarks\"\
    , RFC 4063,\n            April 2005.\n   [Ma05t]  Manral, V., White, R., and A.\
    \ Shaikh, \"OSPF Benchmarking\n            Terminology and Concepts\", RFC 4062,\
    \ April 2005.\n   [Ma98]   Mandeville, R., \"Benchmarking Terminology for LAN\
    \ Switching\n            Devices\", RFC 2285, February 1998.\n   [Mo98]   Moy,\
    \ J., \"OSPF Version 2\", STD 54, RFC 2328, April 1998.\n   [Ne07]   Newman, D.\
    \ and T. Player, \"Hash and Stuffing: Overlooked\n            Factors in Network\
    \ Device Benchmarking\", RFC 4814,\n            March 2007.\n   [Pa05]   Pan,\
    \ P., Swallow, G., and A. Atlas, \"Fast Reroute Extensions\n            to RSVP-TE\
    \ for LSP Tunnels\", RFC 4090, May 2005.\n   [Po06]   Poretsky, S., Perser, J.,\
    \ Erramilli, S., and S. Khurana,\n            \"Terminology for Benchmarking Network-layer\
    \ Traffic Control\n            Mechanisms\", RFC 4689, October 2006.\n   [Po11t]\
    \  Poretsky, S., Imhoff, B., and K. Michielsen, \"Terminology\n            for\
    \ Benchmarking Link-State IGP Data-Plane Route\n            Convergence\", RFC\
    \ 6412, November 2011.\n   [Sh10]   Shand, M. and S. Bryant, \"A Framework for\
    \ Loop-Free\n            Convergence\", RFC 5715, January 2010.\n   [Sh10i]  Shand,\
    \ M. and S. Bryant, \"IP Fast Reroute Framework\",\n            RFC 5714, January\
    \ 2010.\n   [Th00]   Thaler, D. and C. Hopps, \"Multipath Issues in Unicast and\n\
    \            Multicast Next-Hop Selection\", RFC 2991, November 2000.\n"
- title: 11.2.  Informative References
  contents:
  - "11.2.  Informative References\n   [Al00]   Alaettinoglu, C., Jacobson, V., and\
    \ H. Yu, \"Towards\n            Millisecond IGP Convergence\", NANOG 20, October\
    \ 2000.\n   [Al02]   Alaettinoglu, C. and S. Casner, \"ISIS Routing on the Qwest\n\
    \            Backbone: a Recipe for Subsecond ISIS Convergence\",\n          \
    \  NANOG 24, February 2002.\n   [Fi02]   Filsfils, C., \"Tutorial: Deploying Tight-SLA\
    \ Services on an\n            Internet Backbone: ISIS Fast Convergence and Differentiated\n\
    \            Services Design\", NANOG 25, June 2002.\n   [Fr05]   Francois, P.,\
    \ Filsfils, C., Evans, J., and O. Bonaventure,\n            \"Achieving SubSecond\
    \ IGP Convergence in Large IP Networks\",\n            ACM SIGCOMM Computer Communication\
    \ Review v.35 n.3,\n            July 2005.\n   [Ka02]   Katz, D., \"Why are we\
    \ scared of SPF? IGP Scaling and\n            Stability\", NANOG 25, June 2002.\n\
    \   [Vi02]   Villamizar, C., \"Convergence and Restoration Techniques for\n  \
    \          ISP Interior Routing\", NANOG 25, June 2002.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Scott Poretsky\n   Allot Communications\n   300 TradeCenter\n\
    \   Woburn, MA  01801\n   USA\n   Phone: + 1 508 309 2179\n   EMail: sporetsky@allot.com\n\
    \   Brent Imhoff\n   Juniper Networks\n   1194 North Mathilda Ave\n   Sunnyvale,\
    \ CA  94089\n   USA\n   Phone: + 1 314 378 2571\n   EMail: bimhoff@planetspork.com\n\
    \   Kris Michielsen\n   Cisco Systems\n   6A De Kleetlaan\n   Diegem, BRABANT\
    \  1831\n   Belgium\n   EMail: kmichiel@cisco.com\n"
