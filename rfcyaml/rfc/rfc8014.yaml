- title: __initial_text__
  contents:
  - "                          An Architecture for\n         Data-Center Network Virtualization\
    \ over Layer 3 (NVO3)\n"
- title: Abstract
  contents:
  - "Abstract\n   This document presents a high-level overview architecture for\n\
    \   building data-center Network Virtualization over Layer 3 (NVO3)\n   networks.\
    \  The architecture is given at a high level, showing the\n   major components\
    \ of an overall system.  An important goal is to\n   divide the space into individual\
    \ smaller components that can be\n   implemented independently with clear inter-component\
    \ interfaces and\n   interactions.  It should be possible to build and implement\n\
    \   individual components in isolation and have them interoperate with\n   other\
    \ independently implemented components.  That way, implementers\n   have flexibility\
    \ in implementing individual components and can\n   optimize and innovate within\
    \ their respective components without\n   requiring changes to other components.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc8014.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2016 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   4\n   2.  Terminology . . . . . . . . . . . . . . . . . . . . .\
    \ . . . .   4\n   3.  Background  . . . . . . . . . . . . . . . . . . . . . .\
    \ . . .   5\n     3.1.  VN Service (L2 and L3)  . . . . . . . . . . . . . . .\
    \ . .   7\n       3.1.1.  VLAN Tags in L2 Service . . . . . . . . . . . . . .\
    \ .   8\n       3.1.2.  Packet Lifetime Considerations  . . . . . . . . . . .\
    \   8\n     3.2.  Network Virtualization Edge (NVE) Background  . . . . . .  \
    \ 9\n     3.3.  Network Virtualization Authority (NVA) Background . . . .  10\n\
    \     3.4.  VM Orchestration Systems  . . . . . . . . . . . . . . . .  11\n  \
    \ 4.  Network Virtualization Edge (NVE) . . . . . . . . . . . . . .  12\n    \
    \ 4.1.  NVE Co-located with Server Hypervisor . . . . . . . . . .  12\n     4.2.\
    \  Split-NVE . . . . . . . . . . . . . . . . . . . . . . . .  13\n       4.2.1.\
    \  Tenant VLAN Handling in Split-NVE Case  . . . . . . .  14\n     4.3.  NVE State\
    \ . . . . . . . . . . . . . . . . . . . . . . . .  14\n     4.4.  Multihoming\
    \ of NVEs . . . . . . . . . . . . . . . . . . .  15\n     4.5.  Virtual Access\
    \ Point (VAP)  . . . . . . . . . . . . . . .  16\n   5.  Tenant System Types .\
    \ . . . . . . . . . . . . . . . . . . . .  16\n     5.1.  Overlay-Aware Network\
    \ Service Appliances  . . . . . . . .  16\n     5.2.  Bare Metal Servers  . .\
    \ . . . . . . . . . . . . . . . . .  17\n     5.3.  Gateways  . . . . . . . .\
    \ . . . . . . . . . . . . . . . .  17\n       5.3.1.  Gateway Taxonomy  . . .\
    \ . . . . . . . . . . . . . . .  18\n         5.3.1.1.  L2 Gateways (Bridging)\
    \  . . . . . . . . . . . . .  18\n         5.3.1.2.  L3 Gateways (Only IP Packets)\
    \ . . . . . . . . . .  18\n     5.4.  Distributed Inter-VN Gateways . . . . .\
    \ . . . . . . . . .  19\n     5.5.  ARP and Neighbor Discovery  . . . . . . .\
    \ . . . . . . . .  20\n   6.  NVE-NVE Interaction . . . . . . . . . . . . . .\
    \ . . . . . . .  20\n   7.  Network Virtualization Authority (NVA)  . . . . .\
    \ . . . . . .  21\n     7.1.  How an NVA Obtains Information  . . . . . . . .\
    \ . . . . .  21\n     7.2.  Internal NVA Architecture . . . . . . . . . . . .\
    \ . . . .  22\n     7.3.  NVA External Interface  . . . . . . . . . . . . . .\
    \ . . .  22\n   8.  NVE-NVA Protocol  . . . . . . . . . . . . . . . . . . . .\
    \ . .  24\n     8.1.  NVE-NVA Interaction Models  . . . . . . . . . . . . . .\
    \ .  24\n     8.2.  Direct NVE-NVA Protocol . . . . . . . . . . . . . . . . .\
    \  25\n     8.3.  Propagating Information Between NVEs and NVAs . . . . . .  25\n\
    \   9.  Federated NVAs  . . . . . . . . . . . . . . . . . . . . . . .  26\n  \
    \   9.1.  Inter-NVA Peering . . . . . . . . . . . . . . . . . . . .  29\n   10.\
    \ Control Protocol Work Areas . . . . . . . . . . . . . . . . .  29\n   11. NVO3\
    \ Data-Plane Encapsulation . . . . . . . . . . . . . . . .  29\n   12. Operations,\
    \ Administration, and Maintenance (OAM) . . . . . .  30\n   13. Summary . . .\
    \ . . . . . . . . . . . . . . . . . . . . . . . .  31\n   14. Security Considerations\
    \ . . . . . . . . . . . . . . . . . . .  31\n   15. Informative References  .\
    \ . . . . . . . . . . . . . . . . . .  32\n   Acknowledgements  . . . . . . .\
    \ . . . . . . . . . . . . . . . . .  34\n   Authors' Addresses  . . . . . . .\
    \ . . . . . . . . . . . . . . . .  35\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   This document presents a high-level architecture for building\
    \ data-\n   center Network Virtualization over Layer 3 (NVO3) networks.  The\n\
    \   architecture is given at a high level, which shows the major\n   components\
    \ of an overall system.  An important goal is to divide the\n   space into smaller\
    \ individual components that can be implemented\n   independently with clear inter-component\
    \ interfaces and interactions.\n   It should be possible to build and implement\
    \ individual components in\n   isolation and have them interoperate with other\
    \ independently\n   implemented components.  That way, implementers have flexibility\
    \ in\n   implementing individual components and can optimize and innovate\n  \
    \ within their respective components without requiring changes to other\n   components.\n\
    \   The motivation for overlay networks is given in \"Problem Statement:\n   Overlays\
    \ for Network Virtualization\" [RFC7364].  \"Framework for Data\n   Center (DC)\
    \ Network Virtualization\" [RFC7365] provides a framework\n   for discussing overlay\
    \ networks generally and the various components\n   that must work together in\
    \ building such systems.  This document\n   differs from the framework document\
    \ in that it doesn't attempt to\n   cover all possible approaches within the general\
    \ design space.\n   Rather, it describes one particular approach that the NVO3\
    \ WG has\n   focused on.\n"
- title: 2.  Terminology
  contents:
  - "2.  Terminology\n   This document uses the same terminology as [RFC7365].  In\
    \ addition,\n   the following terms are used:\n   NV Domain:  A Network Virtualization\
    \ Domain is an administrative\n      construct that defines a Network Virtualization\
    \ Authority (NVA),\n      the set of Network Virtualization Edges (NVEs) associated\
    \ with\n      that NVA, and the set of virtual networks the NVA manages and\n\
    \      supports.  NVEs are associated with a (logically centralized) NVA,\n  \
    \    and an NVE supports communication for any of the virtual networks\n     \
    \ in the domain.\n   NV Region:  A region over which information about a set of\
    \ virtual\n      networks is shared.  The degenerate case of a single NV Domain\n\
    \      corresponds to an NV Region corresponding to that domain.  The\n      more\
    \ interesting case occurs when two or more NV Domains share\n      information\
    \ about part or all of a set of virtual networks that\n      they manage.  Two\
    \ NVAs share information about particular virtual\n      networks for the purpose\
    \ of supporting connectivity between\n      tenants located in different NV Domains.\
    \  NVAs can share\n      information about an entire NV Domain, or just individual\
    \ virtual\n      networks.\n   Tenant System Interface (TSI):  The interface to\
    \ a Virtual Network\n      (VN) as presented to a Tenant System (TS, see [RFC7365]).\
    \  The TSI\n      logically connects to the NVE via a Virtual Access Point (VAP).\n\
    \      To the Tenant System, the TSI is like a Network Interface Card\n      (NIC);\
    \ the TSI presents itself to a Tenant System as a normal\n      network interface.\n\
    \   VLAN:  Unless stated otherwise, the terms \"VLAN\" and \"VLAN Tag\" are\n\
    \      used in this document to denote a Customer VLAN (C-VLAN)\n      [IEEE.802.1Q];\
    \ the terms are used interchangeably to improve\n      readability.\n"
- title: 3.  Background
  contents:
  - "3.  Background\n   Overlay networks are an approach for providing network virtualization\n\
    \   services to a set of Tenant Systems (TSs) [RFC7365].  With overlays,\n   data\
    \ traffic between tenants is tunneled across the underlying data\n   center's\
    \ IP network.  The use of tunnels provides a number of\n   benefits by decoupling\
    \ the network as viewed by tenants from the\n   underlying physical network across\
    \ which they communicate.\n   Additional discussion of some NVO3 use cases can\
    \ be found in\n   [USECASES].\n   Tenant Systems connect to Virtual Networks (VNs),\
    \ with each VN having\n   associated attributes defining properties of the network\
    \ (such as the\n   set of members that connect to it).  Tenant Systems connected\
    \ to a\n   virtual network typically communicate freely with other Tenant\n  \
    \ Systems on the same VN, but communication between Tenant Systems on\n   one\
    \ VN and those external to the VN (whether on another VN or\n   connected to the\
    \ Internet) is carefully controlled and governed by\n   policy.  The NVO3 architecture\
    \ does not impose any restrictions to\n   the application of policy controls even\
    \ within a VN.\n   A Network Virtualization Edge (NVE) [RFC7365] is the entity\
    \ that\n   implements the overlay functionality.  An NVE resides at the boundary\n\
    \   between a Tenant System and the overlay network as shown in Figure 1.\n  \
    \ An NVE creates and maintains local state about each VN for which it\n   is providing\
    \ service on behalf of a Tenant System.\n       +--------+                   \
    \                          +--------+\n       | Tenant +--+                  \
    \                   +----| Tenant |\n       | System |  |                    \
    \                (')   | System |\n       +--------+  |          ................\
    \         (   )  +--------+\n                   |  +-+--+  .              .  +--+-+\
    \  (_)\n                   |  | NVE|--.              .--| NVE|   |\n         \
    \          +--|    |  .              .  |    |---+\n                      +-+--+\
    \  .              .   +--+-+\n                      /       .              .\n\
    \                     /        .  L3 Overlay  .   +--+-++--------+\n       +--------+\
    \   /         .    Network   .   | NVE|| Tenant |\n       | Tenant +--+      \
    \    .              .- -|    || System |\n       | System |             .    \
    \          .   +--+-++--------+\n       +--------+             ................\n\
    \                                     |\n                                   +----+\n\
    \                                   | NVE|\n                                 \
    \  |    |\n                                   +----+\n                       \
    \              |\n                                     |\n                   \
    \        =====================\n                             |               |\n\
    \                         +--------+      +--------+\n                       \
    \  | Tenant |      | Tenant |\n                         | System |      | System\
    \ |\n                         +--------+      +--------+\n                  Figure\
    \ 1: NVO3 Generic Reference Model\n   The following subsections describe key aspects\
    \ of an overlay system\n   in more detail.  Section 3.1 describes the service\
    \ model (Ethernet\n   vs. IP) provided to Tenant Systems.  Section 3.2 describes\
    \ NVEs in\n   more detail.  Section 3.3 introduces the Network Virtualization\n\
    \   Authority, from which NVEs obtain information about virtual networks.\n  \
    \ Section 3.4 provides background on Virtual Machine (VM) orchestration\n   systems\
    \ and their use of virtual networks.\n"
- title: 3.1.  VN Service (L2 and L3)
  contents:
  - "3.1.  VN Service (L2 and L3)\n   A VN provides either Layer 2 (L2) or Layer 3\
    \ (L3) service to\n   connected tenants.  For L2 service, VNs transport Ethernet\
    \ frames,\n   and a Tenant System is provided with a service that is analogous\
    \ to\n   being connected to a specific L2 C-VLAN.  L2 broadcast frames are\n \
    \  generally delivered to all (and multicast frames delivered to a\n   subset\
    \ of) the other Tenant Systems on the VN.  To a Tenant System,\n   it appears\
    \ as if they are connected to a regular L2 Ethernet link.\n   Within the NVO3\
    \ architecture, tenant frames are tunneled to remote\n   NVEs based on the Media\
    \ Access Control (MAC) addresses of the frame\n   headers as originated by the\
    \ Tenant System.  On the underlay, NVO3\n   packets are forwarded between NVEs\
    \ based on the outer addresses of\n   tunneled packets.\n   For L3 service, VNs\
    \ are routed networks that transport IP datagrams,\n   and a Tenant System is\
    \ provided with a service that supports only IP\n   traffic.  Within the NVO3\
    \ architecture, tenant frames are tunneled to\n   remote NVEs based on the IP\
    \ addresses of the packet originated by the\n   Tenant System; any L2 destination\
    \ addresses provided by Tenant\n   Systems are effectively ignored by the NVEs\
    \ and overlay network.  For\n   L3 service, the Tenant System will be configured\
    \ with an IP subnet\n   that is effectively a point-to-point link, i.e., having\
    \ only the\n   Tenant System and a next-hop router address on it.\n   L2 service\
    \ is intended for systems that need native L2 Ethernet\n   service and the ability\
    \ to run protocols directly over Ethernet\n   (i.e., not based on IP).  L3 service\
    \ is intended for systems in which\n   all the traffic can safely be assumed to\
    \ be IP.  It is important to\n   note that whether or not an NVO3 network provides\
    \ L2 or L3 service to\n   a Tenant System, the Tenant System does not generally\
    \ need to be\n   aware of the distinction.  In both cases, the virtual network\n\
    \   presents itself to the Tenant System as an L2 Ethernet interface.  An\n  \
    \ Ethernet interface is used in both cases simply as a widely supported\n   interface\
    \ type that essentially all Tenant Systems already support.\n   Consequently,\
    \ no special software is needed on Tenant Systems to use\n   an L3 vs. an L2 overlay\
    \ service.\n   NVO3 can also provide a combined L2 and L3 service to tenants.\
    \  A\n   combined service provides L2 service for intra-VN communication but\n\
    \   also provides L3 service for L3 traffic entering or leaving the VN.\n   Architecturally,\
    \ the handling of a combined L2/L3 service within the\n   NVO3 architecture is\
    \ intended to match what is commonly done today in\n   non-overlay environments\
    \ by devices providing a combined bridge/\n   router service.  With combined service,\
    \ the virtual network itself\n   retains the semantics of L2 service, and all\
    \ traffic is processed\n   according to its L2 semantics.  In addition, however,\
    \ traffic\n   requiring IP processing is also processed at the IP level.\n   The\
    \ IP processing for a combined service can be implemented on a\n   standalone\
    \ device attached to the virtual network (e.g., an IP\n   router) or implemented\
    \ locally on the NVE (see Section 5.4 on\n   Distributed Inter-VN Gateways). \
    \ For unicast traffic, NVE\n   implementation of a combined service may result\
    \ in a packet being\n   delivered to another Tenant System attached to the same\
    \ NVE (on\n   either the same or a different VN), tunneled to a remote NVE, or\
    \ even\n   forwarded outside the NV Domain.  For multicast or broadcast packets,\n\
    \   the combination of NVE L2 and L3 processing may result in copies of\n   the\
    \ packet receiving both L2 and L3 treatments to realize delivery to\n   all of\
    \ the destinations involved.  This distributed NVE\n   implementation of IP routing\
    \ results in the same network delivery\n   behavior as if the L2 processing of\
    \ the packet included delivery of\n   the packet to an IP router attached to the\
    \ L2 VN as a Tenant System,\n   with the router having additional network attachments\
    \ to other\n   networks, either virtual or not.\n"
- title: 3.1.1.  VLAN Tags in L2 Service
  contents:
  - "3.1.1.  VLAN Tags in L2 Service\n   An NVO3 L2 virtual network service may include\
    \ encapsulated L2 VLAN\n   tags provided by a Tenant System but does not use encapsulated\
    \ tags\n   in deciding where and how to forward traffic.  Such VLAN tags can be\n\
    \   passed through so that Tenant Systems that send or expect to receive\n   them\
    \ can be supported as appropriate.\n   The processing of VLAN tags that an NVE\
    \ receives from a TS is\n   controlled by settings associated with the VAP.  Just\
    \ as in the case\n   with ports on Ethernet switches, a number of settings are\
    \ possible.\n   For example, Customer VLAN Tags (C-TAGs) can be passed through\n\
    \   transparently, could always be stripped upon receipt from a Tenant\n   System,\
    \ could be compared against a list of explicitly configured\n   tags, etc.\n \
    \  Note that there are additional considerations when VLAN tags are used\n   to\
    \ identify both the VN and a Tenant System VLAN within that VN, as\n   described\
    \ in Section 4.2.1.\n"
- title: 3.1.2.  Packet Lifetime Considerations
  contents:
  - "3.1.2.  Packet Lifetime Considerations\n   For L3 service, Tenant Systems should\
    \ expect the IPv4 Time to Live\n   (TTL) or IPv6 Hop Limit in the packets they\
    \ send to be decremented by\n   at least 1.  For L2 service, neither the TTL nor\
    \ the Hop Limit (when\n   the packet is IP) is modified.  The underlay network\
    \ manages TTLs and\n   Hop Limits in the outer IP encapsulation -- the values\
    \ in these\n   fields could be independent from or related to the values in the\
    \ same\n   fields of tenant IP packets.\n"
- title: 3.2.  Network Virtualization Edge (NVE) Background
  contents:
  - "3.2.  Network Virtualization Edge (NVE) Background\n   Tenant Systems connect\
    \ to NVEs via a Tenant System Interface (TSI).\n   The TSI logically connects\
    \ to the NVE via a Virtual Access Point\n   (VAP), and each VAP is associated\
    \ with one VN as shown in Figure 2.\n   To the Tenant System, the TSI is like\
    \ a NIC; the TSI presents itself\n   to a Tenant System as a normal network interface.\
    \  On the NVE side, a\n   VAP is a logical network port (virtual or physical)\
    \ into a specific\n   virtual network.  Note that two different Tenant Systems\
    \ (and TSIs)\n   attached to a common NVE can share a VAP (e.g., TS1 and TS2 in\n\
    \   Figure 2) so long as they connect to the same VN.\n                    | \
    \        Data-Center Network (IP)        |\n                    |            \
    \                             |\n                    +-----------------------------------------+\n\
    \                         |                           |\n                    \
    \     |       Tunnel Overlay      |\n            +------------+---------+    \
    \   +---------+------------+\n            | +----------+-------+ |       | +-------+----------+\
    \ |\n            | |  Overlay Module  | |       | |  Overlay Module  | |\n   \
    \         | +---------+--------+ |       | +---------+--------+ |\n          \
    \  |           |          |       |           |          |\n     NVE1   |    \
    \       |          |       |           |          | NVE2\n            |  +--------+-------+\
    \  |       |  +--------+-------+  |\n            |  | VNI1      VNI2 |  |    \
    \   |  | VNI1      VNI2 |  |\n            |  +-+----------+---+  |       |  +-+-----------+--+\
    \  |\n            |    | VAP1     | VAP2 |       |    | VAP1      | VAP2|\n  \
    \          +----+----------+------+       +----+-----------+-----+\n         \
    \        |          |                   |           |\n                 |\\  \
    \       |                   |           |\n                 | \\        |    \
    \               |          /|\n          -------+--\\-------+-------------------+---------/-+-------\n\
    \                 |   \\      |     Tenant        |        /  |\n            TSI1\
    \ |TSI2\\     | TSI3            TSI1  TSI2/   TSI3\n                +---+ +---+\
    \ +---+             +---+ +---+   +---+\n                |TS1| |TS2| |TS3|   \
    \          |TS4| |TS5|   |TS6|\n                +---+ +---+ +---+            \
    \ +---+ +---+   +---+\n                       Figure 2: NVE Reference Model\n\
    \   The Overlay Module performs the actual encapsulation and\n   decapsulation\
    \ of tunneled packets.  The NVE maintains state about the\n   virtual networks\
    \ it is a part of so that it can provide the Overlay\n   Module with information\
    \ such as the destination address of the NVE to\n   tunnel a packet to and the\
    \ Context ID that should be placed in the\n   encapsulation header to identify\
    \ the virtual network that a tunneled\n   packet belongs to.\n   On the side facing\
    \ the data-center network, the NVE sends and\n   receives native IP traffic. \
    \ When ingressing traffic from a Tenant\n   System, the NVE identifies the egress\
    \ NVE to which the packet should\n   be sent, adds an overlay encapsulation header,\
    \ and sends the packet\n   on the underlay network.  When receiving traffic from\
    \ a remote NVE,\n   an NVE strips off the encapsulation header and delivers the\n\
    \   (original) packet to the appropriate Tenant System.  When the source\n   and\
    \ destination Tenant System are on the same NVE, no encapsulation\n   is needed\
    \ and the NVE forwards traffic directly.\n   Conceptually, the NVE is a single\
    \ entity implementing the NVO3\n   functionality.  In practice, there are a number\
    \ of different\n   implementation scenarios, as described in detail in Section\
    \ 4.\n"
- title: 3.3.  Network Virtualization Authority (NVA) Background
  contents:
  - "3.3.  Network Virtualization Authority (NVA) Background\n   Address dissemination\
    \ refers to the process of learning, building,\n   and distributing the mapping/forwarding\
    \ information that NVEs need in\n   order to tunnel traffic to each other on behalf\
    \ of communicating\n   Tenant Systems.  For example, in order to send traffic\
    \ to a remote\n   Tenant System, the sending NVE must know the destination NVE\
    \ for that\n   Tenant System.\n   One way to build and maintain mapping tables\
    \ is to use learning, as\n   802.1 bridges do [IEEE.802.1Q].  When forwarding\
    \ traffic to multicast\n   or unknown unicast destinations, an NVE could simply\
    \ flood traffic.\n   While flooding works, it can lead to traffic hot spots and\
    \ to\n   problems in larger networks (e.g., excessive amounts of flooded\n   traffic).\n\
    \   Alternatively, to reduce the scope of where flooding must take place,\n  \
    \ or to eliminate it all together, NVEs can make use of a Network\n   Virtualization\
    \ Authority (NVA).  An NVA is the entity that provides\n   address mapping and\
    \ other information to NVEs.  NVEs interact with an\n   NVA to obtain any required\
    \ address-mapping information they need in\n   order to properly forward traffic\
    \ on behalf of tenants.  The term\n   \"NVA\" refers to the overall system, without\
    \ regard to its scope or\n   how it is implemented.  NVAs provide a service, and\
    \ NVEs access that\n   service via an NVE-NVA protocol as discussed in Section\
    \ 8.\n   Even when an NVA is present, Ethernet bridge MAC address learning\n \
    \  could be used as a fallback mechanism, should the NVA be unable to\n   provide\
    \ an answer or for other reasons.  This document does not\n   consider flooding\
    \ approaches in detail, as there are a number of\n   benefits in using an approach\
    \ that depends on the presence of an NVA.\n   For the rest of this document, it\
    \ is assumed that an NVA exists and\n   will be used.  NVAs are discussed in more\
    \ detail in Section 7.\n"
- title: 3.4.  VM Orchestration Systems
  contents:
  - "3.4.  VM Orchestration Systems\n   VM orchestration systems manage server virtualization\
    \ across a set of\n   servers.  Although VM management is a separate topic from\
    \ network\n   virtualization, the two areas are closely related.  Managing the\n\
    \   creation, placement, and movement of VMs also involves creating,\n   attaching\
    \ to, and detaching from virtual networks.  A number of\n   existing VM orchestration\
    \ systems have incorporated aspects of\n   virtual network management into their\
    \ systems.\n   Note also that although this section uses the terms \"VM\" and\n\
    \   \"hypervisor\" throughout, the same issues apply to other\n   virtualization\
    \ approaches, including Linux Containers (LXC), BSD\n   Jails, Network Service\
    \ Appliances as discussed in Section 5.1, etc.\n   From an NVO3 perspective, it\
    \ should be assumed that where the\n   document uses the term \"VM\" and \"hypervisor\"\
    , the intention is that\n   the discussion also applies to other systems, where,\
    \ e.g., the host\n   operating system plays the role of the hypervisor in supporting\n\
    \   virtualization, and a container plays the equivalent role as a VM.\n   When\
    \ a new VM image is started, the VM orchestration system\n   determines where\
    \ the VM should be placed, interacts with the\n   hypervisor on the target server\
    \ to load and start the VM, and\n   controls when a VM should be shut down or\
    \ migrated elsewhere.  VM\n   orchestration systems also have knowledge about\
    \ how a VM should\n   connect to a network, possibly including the name of the\
    \ virtual\n   network to which a VM is to connect.  The VM orchestration system\
    \ can\n   pass such information to the hypervisor when a VM is instantiated.\n\
    \   VM orchestration systems have significant (and sometimes global)\n   knowledge\
    \ over the domain they manage.  They typically know on what\n   servers a VM is\
    \ running, and metadata associated with VM images can\n   be useful from a network\
    \ virtualization perspective.  For example,\n   the metadata may include the addresses\
    \ (MAC and IP) the VMs will use\n   and the name(s) of the virtual network(s)\
    \ they connect to.\n   VM orchestration systems run a protocol with an agent running\
    \ on the\n   hypervisor of the servers they manage.  That protocol can also carry\n\
    \   information about what virtual network a VM is associated with.  When\n  \
    \ the orchestrator instantiates a VM on a hypervisor, the hypervisor\n   interacts\
    \ with the NVE in order to attach the VM to the virtual\n   networks it has access\
    \ to.  In general, the hypervisor will need to\n   communicate significant VM\
    \ state changes to the NVE.  In the reverse\n   direction, the NVE may need to\
    \ communicate network connectivity\n   information back to the hypervisor.  Examples\
    \ of deployed VM\n   orchestration systems include VMware's vCenter Server, Microsoft's\n\
    \   System Center Virtual Machine Manager, and systems based on OpenStack\n  \
    \ and its associated plugins (e.g., Nova and Neutron).  Each can pass\n   information\
    \ about what virtual networks a VM connects to down to the\n   hypervisor.  The\
    \ protocol used between the VM orchestration system\n   and hypervisors is generally\
    \ proprietary.\n   It should be noted that VM orchestration systems may not have\
    \ direct\n   access to all networking-related information a VM uses.  For example,\n\
    \   a VM may make use of additional IP or MAC addresses that the VM\n   management\
    \ system is not aware of.\n"
- title: 4.  Network Virtualization Edge (NVE)
  contents:
  - "4.  Network Virtualization Edge (NVE)\n   As introduced in Section 3.2, an NVE\
    \ is the entity that implements\n   the overlay functionality.  This section describes\
    \ NVEs in more\n   detail.  An NVE will have two external interfaces:\n   Facing\
    \ the Tenant System:  On the side facing the Tenant System, an\n      NVE interacts\
    \ with the hypervisor (or equivalent entity) to\n      provide the NVO3 service.\
    \  An NVE will need to be notified when a\n      Tenant System \"attaches\" to\
    \ a virtual network (so it can validate\n      the request and set up any state\
    \ needed to send and receive\n      traffic on behalf of the Tenant System on\
    \ that VN).  Likewise, an\n      NVE will need to be informed when the Tenant\
    \ System \"detaches\"\n      from the virtual network so that it can reclaim state\
    \ and\n      resources appropriately.\n   Facing the Data-Center Network:  On\
    \ the side facing the data-center\n      network, an NVE interfaces with the data-center\
    \ underlay network,\n      sending and receiving tunneled packets to and from\
    \ the underlay.\n      The NVE may also run a control protocol with other entities\
    \ on the\n      network, such as the Network Virtualization Authority.\n"
- title: 4.1.  NVE Co-located with Server Hypervisor
  contents:
  - "4.1.  NVE Co-located with Server Hypervisor\n   When server virtualization is\
    \ used, the entire NVE functionality will\n   typically be implemented as part\
    \ of the hypervisor and/or virtual\n   switch on the server.  In such cases, the\
    \ Tenant System interacts\n   with the hypervisor, and the hypervisor interacts\
    \ with the NVE.\n   Because the interaction between the hypervisor and NVE is\
    \ implemented\n   entirely in software on the server, there is no \"on-the-wire\"\
    \n   protocol between Tenant Systems (or the hypervisor) and the NVE that\n  \
    \ needs to be standardized.  While there may be APIs between the NVE\n   and hypervisor\
    \ to support necessary interaction, the details of such\n   APIs are not in scope\
    \ for the NVO3 WG at the time of publication of\n   this memo.\n   Implementing\
    \ NVE functionality entirely on a server has the\n   disadvantage that server\
    \ CPU resources must be spent implementing the\n   NVO3 functionality.  Experimentation\
    \ with overlay approaches and\n   previous experience with TCP and checksum adapter\
    \ offloads suggest\n   that offloading certain NVE operations (e.g., encapsulation\
    \ and\n   decapsulation operations) onto the physical network adapter can\n  \
    \ produce performance advantages.  As has been done with checksum and/\n   or\
    \ TCP server offload and other optimization approaches, there may be\n   benefits\
    \ to offloading common operations onto adapters where\n   possible.  Just as important,\
    \ the addition of an overlay header can\n   disable existing adapter offload capabilities\
    \ that are generally not\n   prepared to handle the addition of a new header or\
    \ other operations\n   associated with an NVE.\n   While the exact details of\
    \ how to split the implementation of\n   specific NVE functionality between a\
    \ server and its network adapters\n   are an implementation matter and outside\
    \ the scope of IETF\n   standardization, the NVO3 architecture should be cognizant\
    \ of and\n   support such separation.  Ideally, it may even be possible to bypass\n\
    \   the hypervisor completely on critical data-path operations so that\n   packets\
    \ between a Tenant System and its VN can be sent and received\n   without having\
    \ the hypervisor involved in each individual packet\n   operation.\n"
- title: 4.2.  Split-NVE
  contents:
  - "4.2.  Split-NVE\n   Another possible scenario leads to the need for a split-NVE\n\
    \   implementation.  An NVE running on a server (e.g., within a\n   hypervisor)\
    \ could support NVO3 service towards the tenant but not\n   perform all NVE functions\
    \ (e.g., encapsulation) directly on the\n   server; some of the actual NVO3 functionality\
    \ could be implemented on\n   (i.e., offloaded to) an adjacent switch to which\
    \ the server is\n   attached.  While one could imagine a number of link types\
    \ between a\n   server and the NVE, one simple deployment scenario would involve\
    \ a\n   server and NVE separated by a simple L2 Ethernet link.  A more\n   complicated\
    \ scenario would have the server and NVE separated by a\n   bridged access network,\
    \ such as when the NVE resides on a Top of Rack\n   (ToR) switch, with an embedded\
    \ switch residing between servers and\n   the ToR switch.\n   For the split-NVE\
    \ case, protocols will be needed that allow the\n   hypervisor and NVE to negotiate\
    \ and set up the necessary state so\n   that traffic sent across the access link\
    \ between a server and the NVE\n   can be associated with the correct virtual\
    \ network instance.\n   Specifically, on the access link, traffic belonging to\
    \ a specific\n   Tenant System would be tagged with a specific VLAN C-TAG that\n\
    \   identifies which specific NVO3 virtual network instance it connects\n   to.\
    \  The hypervisor-NVE protocol would negotiate which VLAN C-TAG to\n   use for\
    \ a particular virtual network instance.  More details of the\n   protocol requirements\
    \ for functionality between hypervisors and NVEs\n   can be found in [NVE-NVA].\n"
- title: 4.2.1.  Tenant VLAN Handling in Split-NVE Case
  contents:
  - "4.2.1.  Tenant VLAN Handling in Split-NVE Case\n   Preserving tenant VLAN tags\
    \ across an NVO3 VN, as described in\n   Section 3.1.1, poses additional complications\
    \ in the split-NVE case.\n   The portion of the NVE that performs the encapsulation\
    \ function needs\n   access to the specific VLAN tags that the Tenant System is\
    \ using in\n   order to include them in the encapsulated packet.  When an NVE\
    \ is\n   implemented entirely within the hypervisor, the NVE has access to the\n\
    \   complete original packet (including any VLAN tags) sent by the\n   tenant.\
    \  In the split-NVE case, however, the VLAN tag used between\n   the hypervisor\
    \ and offloaded portions of the NVE normally only\n   identifies the specific\
    \ VN that traffic belongs to.  In order to\n   allow a tenant to preserve VLAN\
    \ information from end to end between\n   Tenant Systems in the split-NVE case,\
    \ additional mechanisms would be\n   needed (e.g., carry an additional VLAN tag\
    \ by carrying both a C-TAG\n   and a Service VLAN Tag (S-TAG) as specified in\
    \ [IEEE.802.1Q] where\n   the C-TAG identifies the tenant VLAN end to end and\
    \ the S-TAG\n   identifies the VN locally between each Tenant System and the\n\
    \   corresponding NVE).\n"
- title: 4.3.  NVE State
  contents:
  - "4.3.  NVE State\n   NVEs maintain internal data structures and state to support\
    \ the\n   sending and receiving of tenant traffic.  An NVE may need some or all\n\
    \   of the following information:\n   1.  An NVE keeps track of which attached\
    \ Tenant Systems are connected\n       to which virtual networks.  When a Tenant\
    \ System attaches to a\n       virtual network, the NVE will need to create or\
    \ update the local\n       state for that virtual network.  When the last Tenant\
    \ System\n       detaches from a given VN, the NVE can reclaim state associated\n\
    \       with that VN.\n   2.  For tenant unicast traffic, an NVE maintains a per-VN\
    \ table of\n       mappings from Tenant System (inner) addresses to remote NVE\n\
    \       (outer) addresses.\n   3.  For tenant multicast (or broadcast) traffic,\
    \ an NVE maintains a\n       per-VN table of mappings and other information on\
    \ how to deliver\n       tenant multicast (or broadcast) traffic.  If the underlying\n\
    \       network supports IP multicast, the NVE could use IP multicast to\n   \
    \    deliver tenant traffic.  In such a case, the NVE would need to\n       know\
    \ what IP underlay multicast address to use for a given VN.\n       Alternatively,\
    \ if the underlying network does not support\n       multicast, a source NVE could\
    \ use unicast replication to deliver\n       traffic.  In such a case, an NVE\
    \ would need to know which remote\n       NVEs are participating in the VN.  An\
    \ NVE could use both\n       approaches, switching from one mode to the other\
    \ depending on\n       factors such as bandwidth efficiency and group membership\n\
    \       sparseness.  [FRAMEWORK-MCAST] discusses the subject of multicast\n  \
    \     handling in NVO3 in further detail.\n   4.  An NVE maintains necessary information\
    \ to encapsulate outgoing\n       traffic, including what type of encapsulation\
    \ and what value to\n       use for a Context ID to identify the VN within the\
    \ encapsulation\n       header.\n   5.  In order to deliver incoming encapsulated\
    \ packets to the correct\n       Tenant Systems, an NVE maintains the necessary\
    \ information to map\n       incoming traffic to the appropriate VAP (i.e., TSI).\n\
    \   6.  An NVE may find it convenient to maintain additional per-VN\n       information\
    \ such as QoS settings, Path MTU information, Access\n       Control Lists (ACLs),\
    \ etc.\n"
- title: 4.4.  Multihoming of NVEs
  contents:
  - "4.4.  Multihoming of NVEs\n   NVEs may be multihomed.  That is, an NVE may have\
    \ more than one IP\n   address associated with it on the underlay network.  Multihoming\n\
    \   happens in two different scenarios.  First, an NVE may have multiple\n   interfaces\
    \ connecting it to the underlay.  Each of those interfaces\n   will typically\
    \ have a different IP address, resulting in a specific\n   Tenant Address (on\
    \ a specific VN) being reachable through the same\n   NVE but through more than\
    \ one underlay IP address.  Second, a\n   specific Tenant System may be reachable\
    \ through more than one NVE,\n   each having one or more underlay addresses. \
    \ In both cases, NVE\n   address-mapping functionality needs to support one-to-many\
    \ mappings\n   and enable a sending NVE to (at a minimum) be able to fail over\
    \ from\n   one IP address to another, e.g., should a specific NVE underlay\n \
    \  address become unreachable.\n   Finally, multihomed NVEs introduce complexities\
    \ when source unicast\n   replication is used to implement tenant multicast as\
    \ described in\n   Section 4.3.  Specifically, an NVE should only receive one\
    \ copy of a\n   replicated packet.\n   Multihoming is needed to support important\
    \ use cases.  First, a bare\n   metal server may have multiple uplink connections\
    \ to either the same\n   or different NVEs.  Having only a single physical path\
    \ to an upstream\n   NVE, or indeed, having all traffic flow through a single\
    \ NVE would be\n   considered unacceptable in highly resilient deployment scenarios\
    \ that\n   seek to avoid single points of failure.  Moreover, in today's\n   networks,\
    \ the availability of multiple paths would require that they\n   be usable in\
    \ an active-active fashion (e.g., for load balancing).\n"
- title: 4.5.  Virtual Access Point (VAP)
  contents:
  - "4.5.  Virtual Access Point (VAP)\n   The VAP is the NVE side of the interface\
    \ between the NVE and the TS.\n   Traffic to and from the tenant flows through\
    \ the VAP.  If an NVE runs\n   into difficulties sending traffic received on the\
    \ VAP, it may need to\n   signal such errors back to the VAP.  Because the VAP\
    \ is an emulation\n   of a physical port, its ability to signal NVE errors is\
    \ limited and\n   lacks sufficient granularity to reflect all possible errors\
    \ an NVE\n   may encounter (e.g., inability to reach a particular destination).\n\
    \   Some errors, such as an NVE losing all of its connections to the\n   underlay,\
    \ could be reflected back to the VAP by effectively disabling\n   it.  This state\
    \ change would reflect itself on the TS as an interface\n   going down, allowing\
    \ the TS to implement interface error handling\n   (e.g., failover) in the same\
    \ manner as when a physical interface\n   becomes disabled.\n"
- title: 5.  Tenant System Types
  contents:
  - "5.  Tenant System Types\n   This section describes a number of special Tenant\
    \ System types and\n   how they fit into an NVO3 system.\n"
- title: 5.1.  Overlay-Aware Network Service Appliances
  contents:
  - "5.1.  Overlay-Aware Network Service Appliances\n   Some Network Service Appliances\
    \ [NVE-NVA] (virtual or physical)\n   provide tenant-aware services.  That is,\
    \ the specific service they\n   provide depends on the identity of the tenant\
    \ making use of the\n   service.  For example, firewalls are now becoming available\
    \ that\n   support multitenancy where a single firewall provides virtual\n   firewall\
    \ service on a per-tenant basis, using per-tenant\n   configuration rules and\
    \ maintaining per-tenant state.  Such\n   appliances will be aware of the VN an\
    \ activity corresponds to while\n   processing requests.  Unlike server virtualization,\
    \ which shields VMs\n   from needing to know about multitenancy, a Network Service\
    \ Appliance\n   may explicitly support multitenancy.  In such cases, the Network\n\
    \   Service Appliance itself will be aware of network virtualization and\n   either\
    \ embed an NVE directly or implement a split-NVE as described in\n   Section 4.2.\
    \  Unlike server virtualization, however, the Network\n   Service Appliance may\
    \ not be running a hypervisor, and the VM\n   orchestration system may not interact\
    \ with the Network Service\n   Appliance.  The NVE on such appliances will need\
    \ to support a control\n   plane to obtain the necessary information needed to\
    \ fully participate\n   in an NV Domain.\n"
- title: 5.2.  Bare Metal Servers
  contents:
  - "5.2.  Bare Metal Servers\n   Many data centers will continue to have at least\
    \ some servers\n   operating as non-virtualized (or \"bare metal\") machines running\
    \ a\n   traditional operating system and workload.  In such systems, there\n \
    \  will be no NVE functionality on the server, and the server will have\n   no\
    \ knowledge of NVO3 (including whether overlays are even in use).\n   In such\
    \ environments, the NVE functionality can reside on the first-\n   hop physical\
    \ switch.  In such a case, the network administrator would\n   (manually) configure\
    \ the switch to enable the appropriate NVO3\n   functionality on the switch port\
    \ connecting the server and associate\n   that port with a specific virtual network.\
    \  Such configuration would\n   typically be static, since the server is not virtualized\
    \ and, once\n   configured, is unlikely to change frequently.  Consequently, this\n\
    \   scenario does not require any protocol or standards work.\n"
- title: 5.3.  Gateways
  contents:
  - "5.3.  Gateways\n   Gateways on VNs relay traffic onto and off of a virtual network.\n\
    \   Tenant Systems use gateways to reach destinations outside of the\n   local\
    \ VN.  Gateways receive encapsulated traffic from one VN, remove\n   the encapsulation\
    \ header, and send the native packet out onto the\n   data-center network for\
    \ delivery.  Outside traffic enters a VN in a\n   reverse manner.\n   Gateways\
    \ can be either virtual (i.e., implemented as a VM) or\n   physical (i.e., a standalone\
    \ physical device).  For performance\n   reasons, standalone hardware gateways\
    \ may be desirable in some cases.\n   Such gateways could consist of a simple\
    \ switch forwarding traffic\n   from a VN onto the local data-center network or\
    \ could embed router\n   functionality.  On such gateways, network interfaces\
    \ connecting to\n   virtual networks will (at least conceptually) embed NVE (or\
    \ split-\n   NVE) functionality within them.  As in the case with Network Service\n\
    \   Appliances, gateways may not support a hypervisor and will need an\n   appropriate\
    \ control-plane protocol to obtain the information needed\n   to provide NVO3\
    \ service.\n   Gateways handle several different use cases.  For example, one\
    \ use\n   case consists of systems supporting overlays together with systems\n\
    \   that do not (e.g., bare metal servers).  Gateways could be used to\n   connect\
    \ legacy systems supporting, e.g., L2 VLANs, to specific\n   virtual networks,\
    \ effectively making them part of the same virtual\n   network.  Gateways could\
    \ also forward traffic between a virtual\n   network and other hosts on the data-center\
    \ network or relay traffic\n   between different VNs.  Finally, gateways can provide\
    \ external\n   connectivity such as Internet or VPN access.\n"
- title: 5.3.1.  Gateway Taxonomy
  contents:
  - "5.3.1.  Gateway Taxonomy\n   As can be seen from the discussion above, there\
    \ are several types of\n   gateways that can exist in an NVO3 environment.  This\
    \ section breaks\n   them down into the various types that could be supported.\
    \  Note that\n   each of the types below could be either implemented in a centralized\n\
    \   manner or distributed to coexist with the NVEs.\n"
- title: 5.3.1.1.  L2 Gateways (Bridging)
  contents:
  - "5.3.1.1.  L2 Gateways (Bridging)\n   L2 Gateways act as Layer 2 bridges to forward\
    \ Ethernet frames based\n   on the MAC addresses present in them.\n   L2 VN to\
    \ Legacy L2:  This type of gateway bridges traffic between L2\n      VNs and other\
    \ legacy L2 networks such as VLANs or L2 VPNs.\n   L2 VN to L2 VN:  The main motivation\
    \ for this type of gateway is to\n      create separate groups of Tenant Systems\
    \ using L2 VNs such that\n      the gateway can enforce network policies between\
    \ each L2 VN.\n"
- title: 5.3.1.2.  L3 Gateways (Only IP Packets)
  contents:
  - "5.3.1.2.  L3 Gateways (Only IP Packets)\n   L3 Gateways forward IP packets based\
    \ on the IP addresses present in\n   the packets.\n   L3 VN to Legacy L2:  This\
    \ type of gateway forwards packets between L3\n      VNs and legacy L2 networks\
    \ such as VLANs or L2 VPNs.  The original\n      sender's destination MAC address\
    \ in any frames that the gateway\n      forwards from a legacy L2 network would\
    \ be the MAC address of the\n      gateway.\n   L3 VN to Legacy L3:  This type\
    \ of gateway forwards packets between L3\n      VNs and legacy L3 networks.  These\
    \ legacy L3 networks could be\n      local to the data center, be in the WAN,\
    \ or be an L3 VPN.\n   L3 VN to L2 VN:  This type of gateway forwards packets\
    \ between L3 VNs\n      and L2 VNs.  The original sender's destination MAC address\
    \ in any\n      frames that the gateway forwards from a L2 VN would be the MAC\n\
    \      address of the gateway.\n   L2 VN to L2 VN:  This type of gateway acts\
    \ similar to a traditional\n      router that forwards between L2 interfaces.\
    \  The original sender's\n      destination MAC address in any frames that the\
    \ gateway forwards\n      from any of the L2 VNs would be the MAC address of the\
    \ gateway.\n   L3 VN to L3 VN:  The main motivation for this type of gateway is\
    \ to\n      create separate groups of Tenant Systems using L3 VNs such that\n\
    \      the gateway can enforce network policies between each L3 VN.\n"
- title: 5.4.  Distributed Inter-VN Gateways
  contents:
  - "5.4.  Distributed Inter-VN Gateways\n   The relaying of traffic from one VN to\
    \ another deserves special\n   consideration.  Whether traffic is permitted to\
    \ flow from one VN to\n   another is a matter of policy and would not (by default)\
    \ be allowed\n   unless explicitly enabled.  In addition, NVAs are the logical\
    \ place\n   to maintain policy information about allowed inter-VN communication.\n\
    \   Policy enforcement for inter-VN communication can be handled in (at\n   least)\
    \ two different ways.  Explicit gateways could be the central\n   point for such\
    \ enforcement, with all inter-VN traffic forwarded to\n   such gateways for processing.\
    \  Alternatively, the NVA can provide\n   such information directly to NVEs by\
    \ either providing a mapping for a\n   target Tenant System (TS) on another VN\
    \ or indicating that such\n   communication is disallowed by policy.\n   When\
    \ inter-VN gateways are centralized, traffic between TSs on\n   different VNs\
    \ can take suboptimal paths, i.e., triangular routing\n   results in paths that\
    \ always traverse the gateway.  In the worst\n   case, traffic between two TSs\
    \ connected to the same NVE can be hair-\n   pinned through an external gateway.\
    \  As an optimization, individual\n   NVEs can be part of a distributed gateway\
    \ that performs such\n   relaying, reducing or completely eliminating triangular\
    \ routing.  In\n   a distributed gateway, each ingress NVE can perform such relaying\n\
    \   activity directly so long as it has access to the policy information\n   needed\
    \ to determine whether cross-VN communication is allowed.\n   Having individual\
    \ NVEs be part of a distributed gateway allows them\n   to tunnel traffic directly\
    \ to the destination NVE without the need to\n   take suboptimal paths.\n   The\
    \ NVO3 architecture supports distributed gateways for the case of\n   inter-VN\
    \ communication.  Such support requires that NVO3 control\n   protocols include\
    \ mechanisms for the maintenance and distribution of\n   policy information about\
    \ what type of cross-VN communication is\n   allowed so that NVEs acting as distributed\
    \ gateways can tunnel\n   traffic from one VN to another as appropriate.\n   Distributed\
    \ gateways could also be used to distribute other\n   traditional router services\
    \ to individual NVEs.  The NVO3\n   architecture does not preclude such implementations\
    \ but does not\n   define or require them as they are outside the scope of the\
    \ NVO3\n   architecture.\n"
- title: 5.5.  ARP and Neighbor Discovery
  contents:
  - "5.5.  ARP and Neighbor Discovery\n   Strictly speaking, for an L2 service, special\
    \ processing of the\n   Address Resolution Protocol (ARP) [RFC826] and IPv6 Neighbor\n\
    \   Discovery (ND) [RFC4861] is not required.  ARP requests are\n   broadcast,\
    \ and an NVO3 can deliver ARP requests to all members of a\n   given L2 virtual\
    \ network just as it does for any packet sent to an L2\n   broadcast address.\
    \  Similarly, ND requests are sent via IP multicast,\n   which NVO3 can support\
    \ by delivering via L2 multicast.  However, as a\n   performance optimization,\
    \ an NVE can intercept ARP (or ND) requests\n   from its attached TSs and respond\
    \ to them directly using information\n   in its mapping tables.  Since an NVE\
    \ will have mechanisms for\n   determining the NVE address associated with a given\
    \ TS, the NVE can\n   leverage the same mechanisms to suppress sending ARP and\
    \ ND requests\n   for a given TS to other members of the VN.  The NVO3 architecture\n\
    \   supports such a capability.\n"
- title: 6.  NVE-NVE Interaction
  contents:
  - "6.  NVE-NVE Interaction\n   Individual NVEs will interact with each other for\
    \ the purposes of\n   tunneling and delivering traffic to remote TSs.  At a minimum,\
    \ a\n   control protocol may be needed for tunnel setup and maintenance.  For\n\
    \   example, tunneled traffic may need to be encrypted or integrity\n   protected,\
    \ in which case it will be necessary to set up appropriate\n   security associations\
    \ between NVE peers.  It may also be desirable to\n   perform tunnel maintenance\
    \ (e.g., continuity checks) on a tunnel in\n   order to detect when a remote NVE\
    \ becomes unreachable.  Such generic\n   tunnel setup and maintenance functions\
    \ are not generally\n   NVO3-specific.  Hence, the NVO3 architecture expects to\
    \ leverage\n   existing tunnel maintenance protocols rather than defining new\
    \ ones.\n   Some NVE-NVE interactions may be specific to NVO3 (in particular,\
    \ be\n   related to information kept in mapping tables) and agnostic to the\n\
    \   specific tunnel type being used.  For example, when tunneling traffic\n  \
    \ for TS-X to a remote NVE, it is possible that TS-X is not presently\n   associated\
    \ with the remote NVE.  Normally, this should not happen,\n   but there could\
    \ be race conditions where the information an NVE has\n   learned from the NVA\
    \ is out of date relative to actual conditions.\n   In such cases, the remote\
    \ NVE could return an error or warning\n   indication, allowing the sending NVE\
    \ to attempt a recovery or\n   otherwise attempt to mitigate the situation.\n\
    \   The NVE-NVE interaction could signal a range of indications, for\n   example:\n\
    \   o  \"No such TS here\", upon a receipt of a tunneled packet for an\n     \
    \ unknown TS\n   o  \"TS-X not here, try the following NVE instead\" (i.e., a\
    \ redirect)\n   o  \"Delivered to correct NVE but could not deliver packet to\
    \ TS-X\"\n   When an NVE receives information from a remote NVE that conflicts\n\
    \   with the information it has in its own mapping tables, it should\n   consult\
    \ with the NVA to resolve those conflicts.  In particular, it\n   should confirm\
    \ that the information it has is up to date, and it\n   might indicate the error\
    \ to the NVA so as to nudge the NVA into\n   following up (as appropriate).  While\
    \ it might make sense for an NVE\n   to update its mapping table temporarily in\
    \ response to an error from\n   a remote NVE, any changes must be handled carefully\
    \ as doing so can\n   raise security considerations if the received information\
    \ cannot be\n   authenticated.  That said, a sending NVE might still take steps\
    \ to\n   mitigate a problem, such as applying rate limiting to data traffic\n\
    \   towards a particular NVE or TS.\n"
- title: 7.  Network Virtualization Authority (NVA)
  contents:
  - "7.  Network Virtualization Authority (NVA)\n   Before sending traffic to and\
    \ receiving traffic from a virtual\n   network, an NVE must obtain the information\
    \ needed to build its\n   internal forwarding tables and state as listed in Section\
    \ 4.3.  An\n   NVE can obtain such information from a Network Virtualization\n\
    \   Authority (NVA).\n   The NVA is the entity that is expected to provide address\
    \ mapping and\n   other information to NVEs.  NVEs can interact with an NVA to\
    \ obtain\n   any required information they need in order to properly forward\n\
    \   traffic on behalf of tenants.  The term \"NVA\" refers to the overall\n  \
    \ system, without regard to its scope or how it is implemented.\n"
- title: 7.1.  How an NVA Obtains Information
  contents:
  - "7.1.  How an NVA Obtains Information\n   There are two primary ways in which\
    \ an NVA can obtain the address\n   dissemination information it manages: from\
    \ the VM orchestration\n   system and/or directly from the NVEs themselves.\n\
    \   On virtualized systems, the NVA may be able to obtain the address-\n   mapping\
    \ information associated with VMs from the VM orchestration\n   system itself.\
    \  If the VM orchestration system contains a master\n   database for all the virtualization\
    \ information, having the NVA\n   obtain information directly from the orchestration\
    \ system would be a\n   natural approach.  Indeed, the NVA could effectively be\
    \ co-located\n   with the VM orchestration system itself.  In such systems, the\
    \ VM\n   orchestration system communicates with the NVE indirectly through the\n\
    \   hypervisor.\n   However, as described in Section 4, not all NVEs are associated\
    \ with\n   hypervisors.  In such cases, NVAs cannot leverage VM orchestration\n\
    \   protocols to interact with an NVE and will instead need to peer\n   directly\
    \ with them.  By peering directly with an NVE, NVAs can obtain\n   information\
    \ about the TSs connected to that NVE and can distribute\n   information to the\
    \ NVE about the VNs those TSs are associated with.\n   For example, whenever a\
    \ Tenant System attaches to an NVE, that NVE\n   would notify the NVA that the\
    \ TS is now associated with that NVE.\n   Likewise, when a TS detaches from an\
    \ NVE, that NVE would inform the\n   NVA.  By communicating directly with NVEs,\
    \ both the NVA and the NVE\n   are able to maintain up-to-date information about\
    \ all active tenants\n   and the NVEs to which they are attached.\n"
- title: 7.2.  Internal NVA Architecture
  contents:
  - "7.2.  Internal NVA Architecture\n   For reliability and fault tolerance reasons,\
    \ an NVA would be\n   implemented in a distributed or replicated manner without\
    \ single\n   points of failure.  How the NVA is implemented, however, is not\n\
    \   important to an NVE so long as the NVA provides a consistent and\n   well-defined\
    \ interface to the NVE.  For example, an NVA could be\n   implemented via database\
    \ techniques whereby a server stores address-\n   mapping information in a traditional\
    \ (possibly replicated) database.\n   Alternatively, an NVA could be implemented\
    \ in a distributed fashion\n   using an existing (or modified) routing protocol\
    \ to maintain and\n   distribute mappings.  So long as there is a clear interface\
    \ between\n   the NVE and NVA, how an NVA is architected and implemented is not\n\
    \   important to an NVE.\n   A number of architectural approaches could be used\
    \ to implement NVAs\n   themselves.  NVAs manage address bindings and distribute\
    \ them to\n   where they need to go.  One approach would be to use the Border\n\
    \   Gateway Protocol (BGP) [RFC4364] (possibly with extensions) and route\n  \
    \ reflectors.  Another approach could use a transaction-based database\n   model\
    \ with replicated servers.  Because the implementation details\n   are local to\
    \ an NVA, there is no need to pick exactly one solution\n   technology, so long\
    \ as the external interfaces to the NVEs (and\n   remote NVAs) are sufficiently\
    \ well defined to achieve\n   interoperability.\n"
- title: 7.3.  NVA External Interface
  contents:
  - "7.3.  NVA External Interface\n   Conceptually, from the perspective of an NVE,\
    \ an NVA is a single\n   entity.  An NVE interacts with the NVA, and it is the\
    \ NVA's\n   responsibility to ensure that interactions between the NVE and NVA\n\
    \   result in consistent behavior across the NVA and all other NVEs using\n  \
    \ the same NVA.  Because an NVA is built from multiple internal\n   components,\
    \ an NVA will have to ensure that information flows to all\n   internal NVA components\
    \ appropriately.\n   One architectural question is how the NVA presents itself\
    \ to the NVE.\n   For example, an NVA could be required to provide access via\
    \ a single\n   IP address.  If NVEs only have one IP address to interact with,\
    \ it\n   would be the responsibility of the NVA to handle NVA component\n   failures,\
    \ e.g., by using a \"floating IP address\" that migrates among\n   NVA components\
    \ to ensure that the NVA can always be reached via the\n   one address.  Having\
    \ all NVA accesses through a single IP address,\n   however, adds constraints\
    \ to implementing robust failover, load\n   balancing, etc.\n   In the NVO3 architecture,\
    \ an NVA is accessed through one or more IP\n   addresses (or an IP address/port\
    \ combination).  If multiple IP\n   addresses are used, each IP address provides\
    \ equivalent\n   functionality, meaning that an NVE can use any of the provided\n\
    \   addresses to interact with the NVA.  Should one address stop working,\n  \
    \ an NVE is expected to failover to another.  While the different\n   addresses\
    \ result in equivalent functionality, one address may respond\n   more quickly\
    \ than another, e.g., due to network conditions, load on\n   the server, etc.\n\
    \   To provide some control over load balancing, NVA addresses may have\n   an\
    \ associated priority.  Addresses are used in order of priority,\n   with no explicit\
    \ preference among NVA addresses having the same\n   priority.  To provide basic\
    \ load balancing among NVAs of equal\n   priorities, NVEs could use some randomization\
    \ input to select among\n   equal-priority NVAs.  Such a priority scheme facilitates\
    \ failover and\n   load balancing, for example, by allowing a network operator\
    \ to\n   specify a set of primary and backup NVAs.\n   It may be desirable to\
    \ have individual NVA addresses responsible for\n   a subset of information about\
    \ an NV Domain.  In such a case, NVEs\n   would use different NVA addresses for\
    \ obtaining or updating\n   information about particular VNs or TS bindings. \
    \ Key questions with\n   such an approach are how information would be partitioned\
    \ and how an\n   NVE could determine which address to use to get the information\
    \ it\n   needs.\n   Another possibility is to treat the information on which NVA\n\
    \   addresses to use as cached (soft-state) information at the NVEs, so\n   that\
    \ any NVA address can be used to obtain any information, but NVEs\n   are informed\
    \ of preferences for which addresses to use for particular\n   information on\
    \ VNs or TS bindings.  That preference information would\n   be cached for future\
    \ use to improve behavior, e.g., if all requests\n   for a specific subset of\
    \ VNs are forwarded to a specific NVA\n   component, the NVE can optimize future\
    \ requests within that subset by\n   sending them directly to that NVA component\
    \ via its address.\n"
- title: 8.  NVE-NVA Protocol
  contents:
  - "8.  NVE-NVA Protocol\n   As outlined in Section 4.3, an NVE needs certain information\
    \ in order\n   to perform its functions.  To obtain such information from an NVA,\
    \ an\n   NVE-NVA protocol is needed.  The NVE-NVA protocol provides two\n   functions.\
    \  First, it allows an NVE to obtain information about the\n   location and status\
    \ of other TSs with which it needs to communicate.\n   Second, the NVE-NVA protocol\
    \ provides a way for NVEs to provide\n   updates to the NVA about the TSs attached\
    \ to that NVE (e.g., when a\n   TS attaches or detaches from the NVE) or about\
    \ communication errors\n   encountered when sending traffic to remote NVEs.  For\
    \ example, an NVE\n   could indicate that a destination it is trying to reach\
    \ at a\n   destination NVE is unreachable for some reason.\n   While having a\
    \ direct NVE-NVA protocol might seem straightforward,\n   the existence of existing\
    \ VM orchestration systems complicates the\n   choices an NVE has for interacting\
    \ with the NVA.\n"
- title: 8.1.  NVE-NVA Interaction Models
  contents:
  - "8.1.  NVE-NVA Interaction Models\n   An NVE interacts with an NVA in at least\
    \ two (quite different) ways:\n   o  NVEs embedded within the same server as the\
    \ hypervisor can obtain\n      necessary information entirely through the hypervisor-facing\
    \ side\n      of the NVE.  Such an approach is a natural extension to existing\n\
    \      VM orchestration systems supporting server virtualization because\n   \
    \   an existing protocol between the hypervisor and VM orchestration\n      system\
    \ already exists and can be leveraged to obtain any needed\n      information.\
    \  Specifically, VM orchestration systems used to\n      create, terminate, and\
    \ migrate VMs already use well-defined\n      (though typically proprietary) protocols\
    \ to handle the\n      interactions between the hypervisor and VM orchestration\
    \ system.\n      For such systems, it is a natural extension to leverage the\n\
    \      existing orchestration protocol as a sort of proxy protocol for\n     \
    \ handling the interactions between an NVE and the NVA.  Indeed,\n      existing\
    \ implementations can already do this.\n   o  Alternatively, an NVE can obtain\
    \ needed information by interacting\n      directly with an NVA via a protocol\
    \ operating over the data-center\n      underlay network.  Such an approach is\
    \ needed to support NVEs that\n      are not associated with systems performing\
    \ server virtualization\n      (e.g., as in the case of a standalone gateway)\
    \ or where the NVE\n      needs to communicate directly with the NVA for other\
    \ reasons.\n   The NVO3 architecture will focus on support for the second model\n\
    \   above.  Existing virtualization environments are already using the\n   first\
    \ model, but they are not sufficient to cover the case of\n   standalone gateways\
    \ -- such gateways may not support virtualization\n   and do not interface with\
    \ existing VM orchestration systems.\n"
- title: 8.2.  Direct NVE-NVA Protocol
  contents:
  - "8.2.  Direct NVE-NVA Protocol\n   An NVE can interact directly with an NVA via\
    \ an NVE-NVA protocol.\n   Such a protocol can be either independent of the NVA\
    \ internal\n   protocol or an extension of it.  Using a purpose-specific protocol\n\
    \   would provide architectural separation and independence between the\n   NVE\
    \ and NVA.  The NVE and NVA interact in a well-defined way, and\n   changes in\
    \ the NVA (or NVE) do not need to impact each other.  Using\n   a dedicated protocol\
    \ also ensures that both NVE and NVA\n   implementations can evolve independently\
    \ and without dependencies on\n   each other.  Such independence is important\
    \ because the upgrade path\n   for NVEs and NVAs is quite different.  Upgrading\
    \ all the NVEs at a\n   site will likely be more difficult in practice than upgrading\
    \ NVAs\n   because of their large number -- one on each end device.  In\n   practice,\
    \ it would be prudent to assume that once an NVE has been\n   implemented and\
    \ deployed, it may be challenging to get subsequent NVE\n   extensions and changes\
    \ implemented and deployed, whereas an NVA (and\n   its associated internal protocols)\
    \ is more likely to evolve over time\n   as experience is gained from usage and\
    \ upgrades will involve fewer\n   nodes.\n   Requirements for a direct NVE-NVA\
    \ protocol can be found in [NVE-NVA].\n"
- title: 8.3.  Propagating Information Between NVEs and NVAs
  contents:
  - "8.3.  Propagating Information Between NVEs and NVAs\n   Information flows between\
    \ NVEs and NVAs in both directions.  The NVA\n   maintains information about all\
    \ VNs in the NV Domain so that NVEs do\n   not need to do so themselves.  NVEs\
    \ obtain information from the NVA\n   about where a given remote TS destination\
    \ resides.  NVAs, in turn,\n   obtain information from NVEs about the individual\
    \ TSs attached to\n   those NVEs.\n   While the NVA could push information relevant\
    \ to every virtual\n   network to every NVE, such an approach scales poorly and\
    \ is\n   unnecessary.  In practice, a given NVE will only need and want to\n \
    \  know about VNs to which it is attached.  Thus, an NVE should be able\n   to\
    \ subscribe to updates only for the virtual networks it is\n   interested in receiving\
    \ updates for.  The NVO3 architecture supports\n   a model where an NVE is not\
    \ required to have full mapping tables for\n   all virtual networks in an NV Domain.\n\
    \   Before sending unicast traffic to a remote TS (or TSs for broadcast\n   or\
    \ multicast traffic), an NVE must know where the remote TS(s)\n   currently reside.\
    \  When a TS attaches to a virtual network, the NVE\n   obtains information about\
    \ that VN from the NVA.  The NVA can provide\n   that information to the NVE at\
    \ the time the TS attaches to the VN,\n   either because the NVE requests the\
    \ information when the attach\n   operation occurs or because the VM orchestration\
    \ system has initiated\n   the attach operation and provides associated mapping\
    \ information to\n   the NVE at the same time.\n   There are scenarios where an\
    \ NVE may wish to query the NVA about\n   individual mappings within a VN.  For\
    \ example, when sending traffic\n   to a remote TS on a remote NVE, that TS may\
    \ become unavailable (e.g.,\n   because it has migrated elsewhere or has been\
    \ shut down, in which\n   case the remote NVE may return an error indication).\
    \  In such\n   situations, the NVE may need to query the NVA to obtain updated\n\
    \   mapping information for a specific TS or to verify that the\n   information\
    \ is still correct despite the error condition.  Note that\n   such a query could\
    \ also be used by the NVA as an indication that\n   there may be an inconsistency\
    \ in the network and that it should take\n   steps to verify that the information\
    \ it has about the current state\n   and location of a specific TS is still correct.\n\
    \   For very large virtual networks, the amount of state an NVE needs to\n   maintain\
    \ for a given virtual network could be significant.  Moreover,\n   an NVE may\
    \ only be communicating with a small subset of the TSs on\n   such a virtual network.\
    \  In such cases, the NVE may find it desirable\n   to maintain state only for\
    \ those destinations it is actively\n   communicating with.  In such scenarios,\
    \ an NVE may not want to\n   maintain full mapping information about all destinations\
    \ on a VN.\n   However, if it needs to communicate with a destination for which\
    \ it\n   does not have mapping information, it will need to be able to query\n\
    \   the NVA on demand for the missing information on a per-destination\n   basis.\n\
    \   The NVO3 architecture will need to support a range of operations\n   between\
    \ the NVE and NVA.  Requirements for those operations can be\n   found in [NVE-NVA].\n"
- title: 9.  Federated NVAs
  contents:
  - "9.  Federated NVAs\n   An NVA provides service to the set of NVEs in its NV Domain.\
    \  Each\n   NVA manages network virtualization information for the virtual\n \
    \  networks within its NV Domain.  An NV Domain is administered by a\n   single\
    \ entity.\n   In some cases, it will be necessary to expand the scope of a specific\n\
    \   VN or even an entire NV Domain beyond a single NVA.  For example, an\n   administrator\
    \ managing multiple data centers may wish to operate all\n   of its data centers\
    \ as a single NV Region.  Such cases are handled by\n   having different NVAs\
    \ peer with each other to exchange mapping\n   information about specific VNs.\
    \  NVAs operate in a federated manner\n   with a set of NVAs operating as a loosely\
    \ coupled federation of\n   individual NVAs.  If a virtual network spans multiple\
    \ NVAs (e.g.,\n   located at different data centers), and an NVE needs to deliver\n\
    \   tenant traffic to an NVE that is part of a different NV Domain, it\n   still\
    \ interacts only with its NVA, even when obtaining mappings for\n   NVEs associated\
    \ with a different NV Domain.\n   Figure 3 shows a scenario where two separate\
    \ NV Domains (A and B)\n   share information about a VN.  VM1 and VM2 both connect\
    \ to the same\n   VN, even though the two VMs are in separate NV Domains.  There\
    \ are\n   two cases to consider.  In the first case, NV Domain B does not allow\n\
    \   NVE-A to tunnel traffic directly to NVE-B.  There could be a number\n   of\
    \ reasons for this.  For example, NV Domains A and B may not share a\n   common\
    \ address space (i.e., traversal through a NAT device is\n   required), or for\
    \ policy reasons, a domain might require that all\n   traffic between separate\
    \ NV Domains be funneled through a particular\n   device (e.g., a firewall). \
    \ In such cases, NVA-2 will advertise to\n   NVA-1 that VM1 on the VN is available\
    \ and direct that traffic between\n   the two nodes be forwarded via IP-G (an\
    \ IP Gateway).  IP-G would then\n   decapsulate received traffic from one NV Domain,\
    \ translate it\n   appropriately for the other domain, and re-encapsulate the\
    \ packet for\n   delivery.\n                    xxxxxx                       \
    \   xxxx        +-----+\n   +-----+     xxxxxx    xxxxxx               xxxxxx\
    \    xxxxx   | VM2 |\n   | VM1 |    xx              xx            xxx        \
    \     xx  |-----|\n   |-----|   xx                x          xx              \
    \   x  |NVE-B|\n   |NVE-A|   x                 x  +----+  x                  \
    \ x +-----+\n   +--+--+   x   NV Domain A   x  |IP-G|--x                    x\
    \    |\n      +-------x               xx--+    | x                     xx   |\n\
    \              x              x    +----+ x     NV Domain B      x   |\n     \
    \      +---x           xx            xx                     x---+\n          \
    \ |    xxxx      xx           +->xx                   xx\n           |       xxxxxxxx\
    \            |   xx                 xx\n       +---+-+                       \
    \  |     xx              xx\n       |NVA-1|                      +--+--+    xx\
    \         xxx\n       +-----+                      |NVA-2|     xxxx   xxxx\n \
    \                                   +-----+        xxxxx\n               Figure\
    \ 3: VM1 and VM2 in Different NV Domains\n   NVAs at one site share information\
    \ and interact with NVAs at other\n   sites, but only in a controlled manner.\
    \  It is expected that policy\n   and access control will be applied at the boundaries\
    \ between\n   different sites (and NVAs) so as to minimize dependencies on external\n\
    \   NVAs that could negatively impact the operation within a site.  It is\n  \
    \ an architectural principle that operations involving NVAs at one site\n   not\
    \ be immediately impacted by failures or errors at another site.\n   (Of course,\
    \ communication between NVEs in different NV Domains may be\n   impacted by such\
    \ failures or errors.)  It is a strong requirement\n   that an NVA continue to\
    \ operate properly for local NVEs even if\n   external communication is interrupted\
    \ (e.g., should communication\n   between a local and remote NVA fail).\n   At\
    \ a high level, a federation of interconnected NVAs has some\n   analogies to\
    \ BGP and Autonomous Systems.  Like an Autonomous System,\n   NVAs at one site\
    \ are managed by a single administrative entity and do\n   not interact with external\
    \ NVAs except as allowed by policy.\n   Likewise, the interface between NVAs at\
    \ different sites is well\n   defined so that the internal details of operations\
    \ at one site are\n   largely hidden to other sites.  Finally, an NVA only peers\
    \ with other\n   NVAs that it has a trusted relationship with, i.e., where a VN\
    \ is\n   intended to span multiple NVAs.\n   Reasons for using a federated model\
    \ include:\n   o  Provide isolation among NVAs operating at different sites at\n\
    \      different geographic locations.\n   o  Control the quantity and rate of\
    \ information updates that flow\n      (and must be processed) between different\
    \ NVAs in different data\n      centers.\n   o  Control the set of external NVAs\
    \ (and external sites) a site peers\n      with.  A site will only peer with other\
    \ sites that are cooperating\n      in providing an overlay service.\n   o  Allow\
    \ policy to be applied between sites.  A site will want to\n      carefully control\
    \ what information it exports (and to whom) as\n      well as what information\
    \ it is willing to import (and from whom).\n   o  Allow different protocols and\
    \ architectures to be used for intra-\n      NVA vs. inter-NVA communication.\
    \  For example, within a single\n      data center, a replicated transaction server\
    \ using database\n      techniques might be an attractive implementation option\
    \ for an\n      NVA, and protocols optimized for intra-NVA communication would\n\
    \      likely be different from protocols involving inter-NVA\n      communication\
    \ between different sites.\n   o  Allow for optimized protocols rather than using\
    \ a one-size-fits-\n      all approach.  Within a data center, networks tend to\
    \ have lower\n      latency, higher speed, and higher redundancy when compared\
    \ with\n      WAN links interconnecting data centers.  The design constraints\n\
    \      and trade-offs for a protocol operating within a data-center\n      network\
    \ are different from those operating over WAN links.  While\n      a single protocol\
    \ could be used for both cases, there could be\n      advantages to using different\
    \ and more specialized protocols for\n      the intra- and inter-NVA case.\n"
- title: 9.1.  Inter-NVA Peering
  contents:
  - "9.1.  Inter-NVA Peering\n   To support peering between different NVAs, an inter-NVA\
    \ protocol is\n   needed.  The inter-NVA protocol defines what information is\
    \ exchanged\n   between NVAs.  It is assumed that the protocol will be used to\
    \ share\n   addressing information between data centers and must scale well over\n\
    \   WAN links.\n"
- title: 10.  Control Protocol Work Areas
  contents:
  - "10.  Control Protocol Work Areas\n   The NVO3 architecture consists of two major\
    \ distinct entities: NVEs\n   and NVAs.  In order to provide isolation and independence\
    \ between\n   these two entities, the NVO3 architecture calls for well-defined\n\
    \   protocols for interfacing between them.  For an individual NVA, the\n   architecture\
    \ calls for a logically centralized entity that could be\n   implemented in a\
    \ distributed or replicated fashion.  While the IETF\n   may choose to define\
    \ one or more specific architectural approaches to\n   building individual NVAs,\
    \ there is little need to pick exactly one\n   approach to the exclusion of others.\
    \  An NVA for a single domain will\n   likely be deployed as a single vendor product;\
    \ thus, there is little\n   benefit in standardizing the internal structure of\
    \ an NVA.\n   Individual NVAs peer with each other in a federated manner.  The\
    \ NVO3\n   architecture calls for a well-defined interface between NVAs.\n   Finally,\
    \ a hypervisor-NVE protocol is needed to cover the split-NVE\n   scenario described\
    \ in Section 4.2.\n"
- title: 11.  NVO3 Data-Plane Encapsulation
  contents:
  - "11.  NVO3 Data-Plane Encapsulation\n   When tunneling tenant traffic, NVEs add\
    \ an encapsulation header to\n   the original tenant packet.  The exact encapsulation\
    \ to use for NVO3\n   does not seem to be critical.  The main requirement is that\
    \ the\n   encapsulation support a Context ID of sufficient size.  A number of\n\
    \   encapsulations already exist that provide a VN Context of sufficient\n   size\
    \ for NVO3.  For example, Virtual eXtensible Local Area Network\n   (VXLAN) [RFC7348]\
    \ has a 24-bit VXLAN Network Identifier (VNI).\n   Network Virtualization using\
    \ Generic Routing Encapsulation (NVGRE)\n   [RFC7637] has a 24-bit Tenant Network\
    \ ID (TNI).  MPLS-over-GRE\n   provides a 20-bit label field.  While there is\
    \ widespread recognition\n   that a 12-bit VN Context would be too small (only\
    \ 4096 distinct\n   values), it is generally agreed that 20 bits (1 million distinct\n\
    \   values) and 24 bits (16.8 million distinct values) are sufficient for\n  \
    \ a wide variety of deployment scenarios.\n"
- title: 12.  Operations, Administration, and Maintenance (OAM)
  contents:
  - "12.  Operations, Administration, and Maintenance (OAM)\n   The simplicity of\
    \ operating and debugging overlay networks will be\n   critical for successful\
    \ deployment.\n   Overlay networks are based on tunnels between NVEs, so the\n\
    \   Operations, Administration, and Maintenance (OAM) [RFC6291] framework\n  \
    \ for overlay networks can draw from prior IETF OAM work for tunnel-\n   based\
    \ networks, specifically L2VPN OAM [RFC6136].  RFC 6136 focuses\n   on Fault Management\
    \ and Performance Management as fundamental to\n   L2VPN service delivery, leaving\
    \ the Configuration Management,\n   Accounting Management, and Security Management\
    \ components of the Open\n   Systems Interconnection (OSI) Fault, Configuration,\
    \ Accounting,\n   Performance, and Security (FCAPS) taxonomy [M.3400] for further\n\
    \   study.  This section does likewise for NVO3 OAM, but those three\n   areas\
    \ continue to be important parts of complete OAM functionality\n   for NVO3.\n\
    \   The relationship between the overlay and underlay networks is a\n   consideration\
    \ for fault and performance management -- a fault in the\n   underlay may manifest\
    \ as fault and/or performance issues in the\n   overlay.  Diagnosing and fixing\
    \ such issues are complicated by NVO3\n   abstracting the underlay network away\
    \ from the overlay network (e.g.,\n   intermediate nodes on the underlay network\
    \ path between NVEs are\n   hidden from overlay VNs).\n   NVO3-specific OAM techniques,\
    \ protocol constructs, and tools are\n   needed to provide visibility beyond this\
    \ abstraction to diagnose and\n   correct problems that appear in the overlay.\
    \  Two examples are\n   underlay-aware traceroute [TRACEROUTE-VXLAN] and ping\
    \ protocol\n   constructs for overlay networks [VXLAN-FAILURE] [NVO3-OVERLAY].\n\
    \   NVO3-specific tools and techniques are best viewed as complements to\n   (i.e.,\
    \ not as replacements for) single-network tools that apply to\n   the overlay\
    \ and/or underlay networks.  Coordination among the\n   individual network tools\
    \ (for the overlay and underlay networks) and\n   NVO3-aware, dual-network tools\
    \ is required to achieve effective\n   monitoring and fault diagnosis.  For example,\
    \ the defect detection\n   intervals and performance measurement intervals ought\
    \ to be\n   coordinated among all tools involved in order to provide consistency\n\
    \   and comparability of results.\n   For further discussion of NVO3 OAM requirements,\
    \ see [NVO3-OAM].\n"
- title: 13.  Summary
  contents:
  - "13.  Summary\n   This document presents the overall architecture for NVO3.  The\n\
    \   architecture calls for three main areas of protocol work:\n   1.  A hypervisor-NVE\
    \ protocol to support split-NVEs as discussed in\n       Section 4.2\n   2.  An\
    \ NVE-NVA protocol for disseminating VN information (e.g., inner\n       to outer\
    \ address mappings)\n   3.  An NVA-NVA protocol for exchange of information about\
    \ specific\n       virtual networks between federated NVAs\n   It should be noted\
    \ that existing protocols or extensions of existing\n   protocols are applicable.\n"
- title: 14.  Security Considerations
  contents:
  - "14.  Security Considerations\n   The data plane and control plane described in\
    \ this architecture will\n   need to address potential security threats.\n   For\
    \ the data plane, tunneled application traffic may need protection\n   against\
    \ being misdelivered, being modified, or having its content\n   exposed to an\
    \ inappropriate third party.  In all cases, encryption\n   between authenticated\
    \ tunnel endpoints (e.g., via use of IPsec\n   [RFC4301]) and enforcing policies\
    \ that control which endpoints and\n   VNs are permitted to exchange traffic can\
    \ be used to mitigate risks.\n   For the control plane, a combination of authentication\
    \ and encryption\n   can be used between NVAs, between the NVA and NVE, as well\
    \ as between\n   different components of the split-NVE approach.  All entities\
    \ will\n   need to properly authenticate with each other and enable encryption\n\
    \   for their interactions as appropriate to protect sensitive\n   information.\n\
    \   Leakage of sensitive information about users or other entities\n   associated\
    \ with VMs whose traffic is virtualized can also be covered\n   by using encryption\
    \ for the control-plane protocols and enforcing\n   policies that control which\
    \ NVO3 components are permitted to exchange\n   control-plane traffic.\n   Control-plane\
    \ elements such as NVEs and NVAs need to collect\n   performance and other data\
    \ in order to carry out their functions.\n   This data can sometimes be unexpectedly\
    \ sensitive, for example,\n   allowing non-obvious inferences of activity within\
    \ a VM.  This\n   provides a reason to minimize the data collected in some environments\n\
    \   in order to limit potential exposure of sensitive information.  As\n   noted\
    \ briefly in RFC 6973 [RFC6973] and RFC 7258 [RFC7258], there is\n   an inevitable\
    \ tension between being privacy sensitive and taking into\n   account network\
    \ operations in NVO3 protocol development.\n   See the NVO3 framework security\
    \ considerations in RFC 7365 [RFC7365]\n   for further discussion.\n"
- title: 15.  Informative References
  contents:
  - "15.  Informative References\n   [FRAMEWORK-MCAST]\n              Ghanwani, A.,\
    \ Dunbar, L., McBride, M., Bannai, V., and R.\n              Krishnan, \"A Framework\
    \ for Multicast in Network\n              Virtualization Overlays\", Work in Progress,\n\
    \              draft-ietf-nvo3-mcast-framework-05, May 2016.\n   [IEEE.802.1Q]\n\
    \              IEEE, \"IEEE Standard for Local and metropolitan area\n       \
    \       networks--Bridges and Bridged Networks\", IEEE 802.1Q-2014,\n        \
    \      DOI 10.1109/ieeestd.2014.6991462,\n              <http://ieeexplore.ieee.org/servlet/\n\
    \              opac?punumber=6991460>.\n   [M.3400]   ITU-T, \"TMN management\
    \ functions\", ITU-T\n              Recommendation M.3400, February 2000,\n  \
    \            <https://www.itu.int/rec/T-REC-M.3400-200002-I/>.\n   [NVE-NVA] \
    \ Kreeger, L., Dutt, D., Narten, T., and D. Black, \"Network\n              Virtualization\
    \ NVE to NVA Control Protocol Requirements\",\n              Work in Progress,\
    \ draft-ietf-nvo3-nve-nva-cp-req-05, March\n              2016.\n   [NVO3-OAM]\
    \ Chen, H., Ed., Ashwood-Smith, P., Xia, L., Iyengar, R.,\n              Tsou,\
    \ T., Sajassi, A., Boucadair, M., Jacquenet, C.,\n              Daikoku, M., Ghanwani,\
    \ A., and R. Krishnan, \"NVO3\n              Operations, Administration, and Maintenance\
    \ Requirements\",\n              Work in Progress, draft-ashwood-nvo3-oam-requirements-04,\n\
    \              October 2015.\n   [NVO3-OVERLAY]\n              Kumar, N., Pignataro,\
    \ C., Rao, D., and S. Aldrin,\n              \"Detecting NVO3 Overlay Data Plane\
    \ failures\", Work in\n              Progress, draft-kumar-nvo3-overlay-ping-01,\
    \ January 2014.\n   [RFC826]  Plummer, D., \"Ethernet Address Resolution Protocol:\
    \ Or\n              Converting Network Protocol Addresses to 48.bit Ethernet\n\
    \              Address for Transmission on Ethernet Hardware\", STD 37,\n    \
    \          RFC 826, DOI 10.17487/RFC0826, November 1982,\n              <http://www.rfc-editor.org/info/rfc826>.\n\
    \   [RFC4301]  Kent, S. and K. Seo, \"Security Architecture for the\n        \
    \      Internet Protocol\", RFC 4301, DOI 10.17487/RFC4301,\n              December\
    \ 2005, <http://www.rfc-editor.org/info/rfc4301>.\n   [RFC4364]  Rosen, E. and\
    \ Y. Rekhter, \"BGP/MPLS IP Virtual Private\n              Networks (VPNs)\",\
    \ RFC 4364, DOI 10.17487/RFC4364, February\n              2006, <http://www.rfc-editor.org/info/rfc4364>.\n\
    \   [RFC4861]  Narten, T., Nordmark, E., Simpson, W., and H. Soliman,\n      \
    \        \"Neighbor Discovery for IP version 6 (IPv6)\", RFC 4861,\n         \
    \     DOI 10.17487/RFC4861, September 2007,\n              <http://www.rfc-editor.org/info/rfc4861>.\n\
    \   [RFC6136]  Sajassi, A., Ed. and D. Mohan, Ed., \"Layer 2 Virtual\n       \
    \       Private Network (L2VPN) Operations, Administration, and\n            \
    \  Maintenance (OAM) Requirements and Framework\", RFC 6136,\n              DOI\
    \ 10.17487/RFC6136, March 2011,\n              <http://www.rfc-editor.org/info/rfc6136>.\n\
    \   [RFC6291]  Andersson, L., van Helvoort, H., Bonica, R., Romascanu,\n     \
    \         D., and S. Mansfield, \"Guidelines for the Use of the \"OAM\"\n    \
    \          Acronym in the IETF\", BCP 161, RFC 6291,\n              DOI 10.17487/RFC6291,\
    \ June 2011,\n              <http://www.rfc-editor.org/info/rfc6291>.\n   [RFC6973]\
    \  Cooper, A., Tschofenig, H., Aboba, B., Peterson, J.,\n              Morris,\
    \ J., Hansen, M., and R. Smith, \"Privacy\n              Considerations for Internet\
    \ Protocols\", RFC 6973,\n              DOI 10.17487/RFC6973, July 2013,\n   \
    \           <http://www.rfc-editor.org/info/rfc6973>.\n   [RFC7258]  Farrell,\
    \ S. and H. Tschofenig, \"Pervasive Monitoring Is an\n              Attack\",\
    \ BCP 188, RFC 7258, DOI 10.17487/RFC7258, May\n              2014, <http://www.rfc-editor.org/info/rfc7258>.\n\
    \   [RFC7348]  Mahalingam, M., Dutt, D., Duda, K., Agarwal, P., Kreeger,\n   \
    \           L., Sridhar, T., Bursell, M., and C. Wright, \"Virtual\n         \
    \     eXtensible Local Area Network (VXLAN): A Framework for\n              Overlaying\
    \ Virtualized Layer 2 Networks over Layer 3\n              Networks\", RFC 7348,\
    \ DOI 10.17487/RFC7348, August 2014,\n              <http://www.rfc-editor.org/info/rfc7348>.\n\
    \   [RFC7364]  Narten, T., Ed., Gray, E., Ed., Black, D., Fang, L.,\n        \
    \      Kreeger, L., and M. Napierala, \"Problem Statement:\n              Overlays\
    \ for Network Virtualization\", RFC 7364,\n              DOI 10.17487/RFC7364,\
    \ October 2014,\n              <http://www.rfc-editor.org/info/rfc7364>.\n   [RFC7365]\
    \  Lasserre, M., Balus, F., Morin, T., Bitar, N., and Y.\n              Rekhter,\
    \ \"Framework for Data Center (DC) Network\n              Virtualization\", RFC\
    \ 7365, DOI 10.17487/RFC7365, October\n              2014, <http://www.rfc-editor.org/info/rfc7365>.\n\
    \   [RFC7637]  Garg, P., Ed. and Y. Wang, Ed., \"NVGRE: Network\n            \
    \  Virtualization Using Generic Routing Encapsulation\",\n              RFC 7637,\
    \ DOI 10.17487/RFC7637, September 2015,\n              <http://www.rfc-editor.org/info/rfc7637>.\n\
    \   [TRACEROUTE-VXLAN]\n              Nordmark, E., Appanna, C., Lo, A., Boutros,\
    \ S., and A.\n              Dubey, \"Layer-Transcending Traceroute for Overlay\
    \ Networks\n              like VXLAN\", Work in Progress, draft-nordmark-nvo3-\n\
    \              transcending-traceroute-03, July 2016.\n   [USECASES]\n       \
    \       Yong, L., Dunbar, L., Toy, M., Isaac, A., and V. Manral,\n           \
    \   \"Use Cases for Data Center Network Virtualization Overlay\n             \
    \ Networks\", Work in Progress, draft-ietf-nvo3-use-case-15,\n              December\
    \ 2016.\n   [VXLAN-FAILURE]\n              Jain, P., Singh, K., Balus, F., Henderickx,\
    \ W., and V.\n              Bannai, \"Detecting VXLAN Segment Failure\", Work\
    \ in\n              Progress, draft-jain-nvo3-vxlan-ping-00, June 2013.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   Helpful comments and improvements to this document have\
    \ come from\n   Alia Atlas, Abdussalam Baryun, Spencer Dawkins, Linda Dunbar,\
    \ Stephen\n   Farrell, Anton Ivanov, Lizhong Jin, Suresh Krishnan, Mirja Kuehlwind,\n\
    \   Greg Mirsky, Carlos Pignataro, Dennis (Xiaohong) Qin, Erik Smith,\n   Takeshi\
    \ Takahashi, Ziye Yang, and Lucy Yong.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   David Black\n   Dell EMC\n   Email: david.black@dell.com\n\
    \   Jon Hudson\n   Independent\n   Email: jon.hudson@gmail.com\n   Lawrence Kreeger\n\
    \   Independent\n   Email: lkreeger@gmail.com\n   Marc Lasserre\n   Independent\n\
    \   Email: mmlasserre@gmail.com\n   Thomas Narten\n   IBM\n   Email: narten@us.ibm.com\n"
