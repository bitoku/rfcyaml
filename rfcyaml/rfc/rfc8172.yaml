- title: __initial_text__
  contents:
  - "       Considerations for Benchmarking Virtual Network Functions\n          \
    \              and Their Infrastructure\n"
- title: Abstract
  contents:
  - "Abstract\n   The Benchmarking Methodology Working Group has traditionally\n \
    \  conducted laboratory characterization of dedicated physical\n   implementations\
    \ of internetworking functions.  This memo investigates\n   additional considerations\
    \ when network functions are virtualized and\n   performed in general-purpose\
    \ hardware.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Engineering Task Force\n   (IETF).  It represents the consensus\
    \ of the IETF community.  It has\n   received public review and has been approved\
    \ for publication by the\n   Internet Engineering Steering Group (IESG).  Not\
    \ all documents\n   approved by the IESG are a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 7841.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc8172.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2017 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .\
    \ . . . . .   2\n     1.1.  Requirements Language . . . . . . . . . . . . . .\
    \ . . . .   3\n   2.  Scope . . . . . . . . . . . . . . . . . . . . . . . . .\
    \ . . .   4\n   3.  Considerations for Hardware and Testing . . . . . . . . .\
    \ . .   4\n     3.1.  Hardware Components . . . . . . . . . . . . . . . . . .\
    \ .   4\n     3.2.  Configuration Parameters  . . . . . . . . . . . . . . . .\
    \   5\n     3.3.  Testing Strategies  . . . . . . . . . . . . . . . . . . .  \
    \ 6\n     3.4.  Attention to Shared Resources . . . . . . . . . . . . . .   7\n\
    \   4.  Benchmarking Considerations . . . . . . . . . . . . . . . . .   8\n  \
    \   4.1.  Comparison with Physical Network Functions  . . . . . . .   8\n    \
    \ 4.2.  Continued Emphasis on Black-Box Benchmarks  . . . . . . .   8\n     4.3.\
    \  New Benchmarks and Related Metrics  . . . . . . . . . . .   9\n     4.4.  Assessment\
    \ of Benchmark Coverage  . . . . . . . . . . . .  10\n     4.5.  Power Consumption\
    \ . . . . . . . . . . . . . . . . . . . .  12\n   5.  Security Considerations\
    \ . . . . . . . . . . . . . . . . . . .  12\n   6.  IANA Considerations . . .\
    \ . . . . . . . . . . . . . . . . . .  13\n   7.  References  . . . . . . . .\
    \ . . . . . . . . . . . . . . . . .  13\n     7.1.  Normative References  . .\
    \ . . . . . . . . . . . . . . . .  13\n     7.2.  Informative References  . .\
    \ . . . . . . . . . . . . . . .  14\n   Acknowledgements  . . . . . . . . . .\
    \ . . . . . . . . . . . . . .  15\n   Author's Address  . . . . . . . . . . .\
    \ . . . . . . . . . . . . .  15\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   The Benchmarking Methodology Working Group (BMWG) has traditionally\n\
    \   conducted laboratory characterization of dedicated physical\n   implementations\
    \ of internetworking functions (or physical network\n   functions (PNFs)).  The\
    \ black-box benchmarks of throughput, latency,\n   forwarding rates, and others\
    \ have served our industry for many years.\n   [RFC1242] and [RFC2544] are the\
    \ cornerstones of the work.\n   A set of service provider and vendor development\
    \ goals has emerged:\n   reduce costs while increasing flexibility of network\
    \ devices and\n   drastically reduce deployment time.  Network Function Virtualization\n\
    \   (NFV) has the promise to achieve these goals and therefore has\n   garnered\
    \ much attention.  It now seems certain that some network\n   functions will be\
    \ virtualized following the success of cloud\n   computing and virtual desktops\
    \ supported by sufficient network path\n   capacity, performance, and widespread\
    \ deployment; many of the same\n   techniques will help achieve NFV.\n   In the\
    \ context of Virtual Network Functions (VNFs), the supporting\n   Infrastructure\
    \ requires general-purpose computing systems, storage\n   systems, networking\
    \ systems, virtualization support systems (such as\n   hypervisors), and management\
    \ systems for the virtual and physical\n   resources.  There will be many potential\
    \ suppliers of Infrastructure\n   systems and significant flexibility in configuring\
    \ the systems for\n   best performance.  There are also many potential suppliers\
    \ of VNFs,\n   adding to the combinations possible in this environment.  The\n\
    \   separation of hardware and software suppliers has a profound\n   implication\
    \ on benchmarking activities: much more of the internal\n   configuration of the\
    \ black-box Device Under Test (DUT) must now be\n   specified and reported with\
    \ the results, to foster both repeatability\n   and comparison testing at a later\
    \ time.\n   Consider the following user story as further background and\n   motivation:\n\
    \      I'm designing and building my NFV Infrastructure platform.  The\n     \
    \ first steps were easy because I had a small number of categories\n      of VNFs\
    \ to support and the VNF vendor gave hardware\n      recommendations that I followed.\
    \  Now I need to deploy more VNFs\n      from new vendors, and there are different\
    \ hardware\n      recommendations.  How well will the new VNFs perform on my\n\
    \      existing hardware?  Which among several new VNFs in a given\n      category\
    \ are most efficient in terms of capacity they deliver?\n      And, when I operate\
    \ multiple categories of VNFs (and PNFs)\n      *concurrently* on a hardware platform\
    \ such that they share\n      resources, what are the new performance limits,\
    \ and what are the\n      software design choices I can make to optimize my chosen\
    \ hardware\n      platform?  Conversely, what hardware platform upgrades should\
    \ I\n      pursue to increase the capacity of these concurrently operating\n \
    \     VNFs?\n   See <http://www.etsi.org/technologies-clusters/technologies/nfv>\
    \ for\n   more background; the white papers there may be a useful starting\n \
    \  place.  The \"NFV Performance & Portability Best Practices\" document\n   [NFV.PER001]\
    \ is particularly relevant to BMWG.  There are also\n   documents available among\
    \ the Approved ETSI NFV Specifications\n   [Approved_ETSI_NFV], including documents\
    \ describing Infrastructure\n   performance aspects and service quality metrics,\
    \ and drafts in the\n   ETSI NFV Open Area [Draft_ETSI_NFV], which may also have\
    \ relevance to\n   benchmarking.\n"
- title: 1.1.  Requirements Language
  contents:
  - "1.1.  Requirements Language\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\"\
    , \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"\
    NOT RECOMMENDED\", \"MAY\", and\n   \"OPTIONAL\" in this document are to be interpreted\
    \ as described in BCP\n   14 [RFC2119] [RFC8174] when, and only when, they appear\
    \ in all\n   capitals, as shown here.\n"
- title: 2.  Scope
  contents:
  - "2.  Scope\n   At the time of this writing, BMWG is considering the new topic\
    \ of\n   Virtual Network Functions and related Infrastructure to ensure that\n\
    \   common issues are recognized from the start; background materials\n   from\
    \ respective standards development organizations and Open Source\n   development\
    \ projects (e.g., IETF, ETSI NFV, and the Open Platform for\n   Network Function\
    \ Virtualization (OPNFV)) are being used.\n   This memo investigates additional\
    \ methodological considerations\n   necessary when benchmarking VNFs instantiated\
    \ and hosted in general-\n   purpose hardware, using bare metal hypervisors [BareMetal]\
    \ or other\n   isolation environments such as Linux containers.  An essential\n\
    \   consideration is benchmarking physical and Virtual Network Functions\n   in\
    \ the same way when possible, thereby allowing direct comparison.\n   Benchmarking\
    \ combinations of physical and virtual devices and\n   functions in a System Under\
    \ Test (SUT) is another topic of keen\n   interest.\n   A clearly related goal\
    \ is investigating benchmarks for the capacity\n   of a general-purpose platform\
    \ to host a plurality of VNF instances.\n   Existing networking technology benchmarks\
    \ will also be considered for\n   adaptation to NFV and closely associated technologies.\n\
    \   A non-goal is any overlap with traditional computer benchmark\n   development\
    \ and their specific metrics (e.g., SPECmark suites such as\n   SPEC CPU).\n \
    \  A continued non-goal is any form of architecture development related\n   to\
    \ NFV and associated technologies in BMWG, consistent with all\n   chartered work\
    \ since BMWG began in 1989.\n"
- title: 3.  Considerations for Hardware and Testing
  contents:
  - "3.  Considerations for Hardware and Testing\n   This section lists the new considerations\
    \ that must be addressed to\n   benchmark VNF(s) and their supporting Infrastructure.\
    \  The SUT is\n   composed of the hardware platform components, the VNFs installed,\
    \ and\n   many other supporting systems.  It is critical to document all\n   aspects\
    \ of the SUT to foster repeatability.\n"
- title: 3.1.  Hardware Components
  contents:
  - "3.1.  Hardware Components\n   The following new hardware components will become\
    \ part of the test\n   setup:\n   1.  High-volume server platforms (general-purpose,\
    \ possibly with\n       virtual technology enhancements)\n   2.  Storage systems\
    \ with large capacity, high speed, and high\n       reliability\n   3.  Network\
    \ interface ports specially designed for efficient service\n       of many virtual\
    \ Network Interface Cards (NICs)\n   4.  High-capacity Ethernet switches\n   The\
    \ components above are subjects for development of specialized\n   benchmarks\
    \ that focus on the special demands of network function\n   deployment.\n   Labs\
    \ conducting comparisons of different VNFs may be able to use the\n   same hardware\
    \ platform over many studies, until the steady march of\n   innovations overtakes\
    \ their capabilities (as happens with the lab's\n   traffic generation and testing\
    \ devices today).\n"
- title: 3.2.  Configuration Parameters
  contents:
  - "3.2.  Configuration Parameters\n   It will be necessary to configure and document\
    \ the settings for the\n   entire general-purpose platform to ensure repeatability\
    \ and foster\n   future comparisons, including, but clearly not limited to, the\n\
    \   following:\n   o  number of server blades (shelf occupation)\n   o  CPUs\n\
    \   o  caches\n   o  memory\n   o  storage system\n   o  I/O\n   as well as configurations\
    \ that support the devices that host the VNF\n   itself:\n   o  Hypervisor (or\
    \ other forms of virtual function hosting)\n   o  Virtual Machine (VM)\n   o \
    \ Infrastructure virtual network (which interconnects virtual\n      machines\
    \ with physical network interfaces or with each other\n      through virtual switches,\
    \ for example)\n   and finally, the VNF itself, with items such as:\n   o  specific\
    \ function being implemented in VNF\n   o  reserved resources for each function\
    \ (e.g., CPU pinning and Non-\n      Uniform Memory Access (NUMA) node assignment)\n\
    \   o  number of VNFs (or sub-VNF components, each with its own VM) in\n     \
    \ the service function chain (see Section 1.1 of [RFC7498] for a\n      definition\
    \ of service function chain)\n   o  number of physical interfaces and links transited\
    \ in the service\n      function chain\n   In the physical device benchmarking\
    \ context, most of the\n   corresponding Infrastructure configuration choices\
    \ were determined by\n   the vendor.  Although the platform itself is now one\
    \ of the\n   configuration variables, it is important to maintain emphasis on\
    \ the\n   networking benchmarks and capture the platform variables as input\n\
    \   factors.\n"
- title: 3.3.  Testing Strategies
  contents:
  - "3.3.  Testing Strategies\n   The concept of characterizing performance at capacity\
    \ limits may\n   change.  For example:\n   1.  It may be more representative of\
    \ system capacity to characterize\n       the case where the VMs hosting the VNFs\
    \ are operating at 50%\n       utilization and therefore sharing the \"real\"\
    \ processing power\n       across many VMs.\n   2.  Another important test case\
    \ stems from the need to partition (or\n       isolate) network functions.  A\
    \ noisy neighbor (VM hosting a VNF\n       in an infinite loop) would ideally\
    \ be isolated; the performance\n       of other VMs would continue according to\
    \ their specifications,\n       and tests would evaluate the degree of isolation.\n\
    \   3.  System errors will likely occur as transients, implying a\n       distribution\
    \ of performance characteristics with a long tail\n       (like latency) and leading\
    \ to the need for longer-term tests of\n       each set of configuration and test\
    \ parameters.\n   4.  The desire for elasticity and flexibility among network\
    \ functions\n       will include tests where there is constant flux in the number\
    \ of\n       VM instances, the resources the VMs require, and the setup/\n   \
    \    teardown of network paths that support VM connectivity.  Requests\n     \
    \  for and instantiation of new VMs, along with releases for VMs\n       hosting\
    \ VNFs that are no longer needed, would be a normal\n       operational condition.\
    \  In other words, benchmarking should\n       include scenarios with production\
    \ life-cycle management of VMs\n       and their VNFs and network connectivity\
    \ in progress, including\n       VNF scaling up/down operations, as well as static\
    \ configurations.\n   5.  All physical things can fail, and benchmarking efforts\
    \ can also\n       examine recovery aided by the virtual architecture with different\n\
    \       approaches to resiliency.\n   6.  The sheer number of test conditions\
    \ and configuration\n       combinations encourage increased efficiency, including\
    \ automated\n       testing arrangements, combination sub-sampling through an\n\
    \       understanding of inter-relationships, and machine-readable test\n    \
    \   results.\n"
- title: 3.4.  Attention to Shared Resources
  contents:
  - "3.4.  Attention to Shared Resources\n   Since many components of the new NFV\
    \ Infrastructure are virtual, test\n   setup design must have prior knowledge\
    \ of interactions/dependencies\n   within the various resource domains in the\
    \ SUT.  For example, a\n   virtual machine performing the role of a traditional\
    \ tester function,\n   such as generating and/or receiving traffic, should avoid\
    \ sharing any\n   SUT resources with the DUT.  Otherwise, the results will have\n\
    \   unexpected dependencies not encountered in physical device\n   benchmarking.\n\
    \   Note that the term \"tester\" has traditionally referred to devices\n   dedicated\
    \ to testing in BMWG literature.  In this new context,\n   \"tester\" additionally\
    \ refers to functions dedicated to testing, which\n   may be either virtual or\
    \ physical.  \"Tester\" has never referred to\n   the individuals performing the\
    \ tests.\n   The possibility to use shared resources in test design while\n  \
    \ producing useful results remains one of the critical challenges to\n   overcome.\
    \  Benchmarking setups may designate isolated resources for\n   the DUT and other\
    \ critical support components (such as the host/\n   kernel) as the first baseline\
    \ step and add other loading processes.\n   The added complexity of each setup\
    \ leads to shared-resource testing\n   scenarios, where the characteristics of\
    \ the competing load (in terms\n   of memory, storage, and CPU utilization) will\
    \ directly affect the\n   benchmarking results (and variability of the results),\
    \ but the\n   results should reconcile with the baseline.\n   The physical test\
    \ device remains a solid foundation to compare with\n   results using combinations\
    \ of physical and virtual test functions or\n   results using only virtual testers\
    \ when necessary to assess virtual\n   interfaces and other virtual functions.\n"
- title: 4.  Benchmarking Considerations
  contents:
  - "4.  Benchmarking Considerations\n   This section discusses considerations related\
    \ to benchmarks\n   applicable to VNFs and their associated technologies.\n"
- title: 4.1.  Comparison with Physical Network Functions
  contents:
  - "4.1.  Comparison with Physical Network Functions\n   In order to compare the\
    \ performance of VNFs and system\n   implementations with their physical counterparts,\
    \ identical\n   benchmarks must be used.  Since BMWG has already developed\n \
    \  specifications for many network functions, there will be re-use of\n   existing\
    \ benchmarks through references, while allowing for the\n   possibility of benchmark\
    \ curation during development of new\n   methodologies.  Consideration should\
    \ be given to quantifying the\n   number of parallel VNFs required to achieve\
    \ comparable scale/capacity\n   with a given physical device or whether some limit\
    \ of scale was\n   reached before the VNFs could achieve the comparable level.\
    \  Again,\n   implementation based on different hypervisors or other virtual\n\
    \   function hosting remain as critical factors in performance\n   assessment.\n"
- title: 4.2.  Continued Emphasis on Black-Box Benchmarks
  contents:
  - "4.2.  Continued Emphasis on Black-Box Benchmarks\n   When the network functions\
    \ under test are based on open-source code,\n   there may be a tendency to rely\
    \ on internal measurements to some\n   extent, especially when the externally\
    \ observable phenomena only\n   support an inference of internal events (such\
    \ as routing protocol\n   convergence observed in the data plane).  Examples include\
    \ CPU/Core\n   utilization, network utilization, storage utilization, and memory\n\
    \   committed/used.  These \"white-box\" metrics provide one view of the\n   resource\
    \ footprint of a VNF.  Note that the resource utilization\n   metrics do not easily\
    \ match the 3x4 Matrix, described in Section 4.4.\n   However, external observations\
    \ remain essential as the basis for\n   benchmarks.  Internal observations with\
    \ fixed specification and\n   interpretation may be provided in parallel (as auxiliary\
    \ metrics), to\n   assist the development of operations procedures when the technology\n\
    \   is deployed, for example.  Internal metrics and measurements from\n   open-source\
    \ implementations may be the only direct source of\n   performance results in\
    \ a desired dimension, but corroborating\n   external observations are still required\
    \ to assure the integrity of\n   measurement discipline was maintained for all\
    \ reported results.\n   A related aspect of benchmark development is where the\
    \ scope includes\n   multiple approaches to a common function under the same benchmark.\n\
    \   For example, there are many ways to arrange for activation of a\n   network\
    \ path between interface points, and the activation times can\n   be compared\
    \ if the start-to-stop activation interval has a generic\n   and unambiguous definition.\
    \  Thus, generic benchmark definitions are\n   preferred over technology/protocol-specific\
    \ definitions where\n   possible.\n"
- title: 4.3.  New Benchmarks and Related Metrics
  contents:
  - "4.3.  New Benchmarks and Related Metrics\n   There will be new classes of benchmarks\
    \ needed for network design and\n   assistance when developing operational practices\
    \ (possibly automated\n   management and orchestration of deployment scale). \
    \ Examples follow\n   in the paragraphs below, many of which are prompted by the\
    \ goals of\n   increased elasticity and flexibility of the network functions,\
    \ along\n   with reduced deployment times.\n   o  Time to deploy VNFs: In cases\
    \ where the general-purpose hardware\n      is already deployed and ready for\
    \ service, it is valuable to know\n      the response time when a management system\
    \ is tasked with\n      \"standing up\" 100s of virtual machines and the VNFs\
    \ they will\n      host.\n   o  Time to migrate VNFs: In cases where a rack or\
    \ shelf of hardware\n      must be removed from active service, it is valuable\
    \ to know the\n      response time when a management system is tasked with \"\
    migrating\"\n      some number of virtual machines and the VNFs they currently\
    \ host\n      to alternate hardware that will remain in service.\n   o  Time to\
    \ create a virtual network in the general-purpose\n      Infrastructure: This\
    \ is a somewhat simplified version of existing\n      benchmarks for convergence\
    \ time, in that the process is initiated\n      by a request from (centralized\
    \ or distributed) control, rather\n      than inferred from network events (link\
    \ failure).  The successful\n      response time would remain dependent on data-plane\
    \ observations to\n      confirm that the network is ready to perform.\n   o \
    \ Effect of verification measurements on performance: A complete\n      VNF, or\
    \ something as simple as a new policy to implement in a VNF,\n      is implemented.\
    \  The action to verify instantiation of the VNF or\n      policy could affect\
    \ performance during normal operation.\n   Also, it appears to be valuable to\
    \ measure traditional packet\n   transfer performance metrics during the assessment\
    \ of traditional and\n   new benchmarks, including metrics that may be used to\
    \ support service\n   engineering such as the spatial composition metrics found\
    \ in\n   [RFC6049].  Examples include mean one-way delay in Section 4.1 of\n \
    \  [RFC6049], Packet Delay Variation (PDV) in [RFC5481], and Packet\n   Reordering\
    \ [RFC4737] [RFC4689].\n"
- title: 4.4.  Assessment of Benchmark Coverage
  contents:
  - "4.4.  Assessment of Benchmark Coverage\n   It can be useful to organize benchmarks\
    \ according to their applicable\n   life-cycle stage and the performance criteria\
    \ they were designed to\n   assess.  The table below (derived from [X3.102]) provides\
    \ a way to\n   organize benchmarks such that there is a clear indication of coverage\n\
    \   for the intersection of life-cycle stages and performance criteria.\n   |----------------------------------------------------------|\n\
    \   |               |             |            |               |\n   |       \
    \        |   SPEED     |  ACCURACY  |  RELIABILITY  |\n   |               |  \
    \           |            |               |\n   |----------------------------------------------------------|\n\
    \   |               |             |            |               |\n   |  Activation\
    \   |             |            |               |\n   |               |       \
    \      |            |               |\n   |----------------------------------------------------------|\n\
    \   |               |             |            |               |\n   |  Operation\
    \    |             |            |               |\n   |               |      \
    \       |            |               |\n   |----------------------------------------------------------|\n\
    \   |               |             |            |               |\n   | De-activation\
    \ |             |            |               |\n   |               |         \
    \    |            |               |\n   |----------------------------------------------------------|\n\
    \   For example, the \"Time to deploy VNFs\" benchmark described above\n   would\
    \ be placed in the intersection of Activation and Speed, making\n   it clear that\
    \ there are other potential performance criteria to\n   benchmark, such as the\
    \ \"percentage of unsuccessful VM/VNF stand-ups\"\n   in a set of 100 attempts.\
    \  This example emphasizes that the\n   Activation and De-activation life-cycle\
    \ stages are key areas for NFV\n   and related Infrastructure and encourages expansion\
    \ beyond\n   traditional benchmarks for normal operation.  Thus, reviewing the\n\
    \   benchmark coverage using this table (sometimes called the 3x3 Matrix)\n  \
    \ can be a worthwhile exercise in BMWG.\n   In one of the first applications of\
    \ the 3x3 Matrix in BMWG\n   [SDN-BENCHMARK], we discovered that metrics on measured\
    \ size,\n   capacity, or scale do not easily match one of the three columns\n\
    \   above.  Following discussion, this was resolved in two ways:\n   o  Add a\
    \ column, Scale, for use when categorizing and assessing the\n      coverage of\
    \ benchmarks (without measured results).  An example of\n      this use is found\
    \ in [OPNFV-BENCHMARK] (and a variation may be\n      found in [SDN-BENCHMARK]).\
    \  This is the 3x4 Matrix.\n   o  If using the matrix to report results in an\
    \ organized way, keep\n      size, capacity, and scale metrics separate from the\
    \ 3x3 Matrix and\n      incorporate them in the report with other qualifications\
    \ of the\n      results.\n   Note that the resource utilization (e.g., CPU) metrics\
    \ do not fit in\n   the matrix.  They are not benchmarks, and omitting them confirms\n\
    \   their status as auxiliary metrics.  Resource assignments are\n   configuration\
    \ parameters, and these are reported separately.\n   This approach encourages\
    \ use of the 3x3 Matrix to organize reports of\n   results, where the capacity\
    \ at which the various metrics were\n   measured could be included in the title\
    \ of the matrix (and results\n   for multiple capacities would result in separate\
    \ 3x3 Matrices, if\n   there were sufficient measurements/results to organize\
    \ in that way).\n   For example, results for each VM and VNF could appear in the\
    \ 3x3\n   Matrix, organized to illustrate resource occupation (CPU Cores) in a\n\
    \   particular physical computing system, as shown below.\n                 VNF#1\n\
    \             .-----------.\n             |__|__|__|__|\n   Core 1    |__|__|__|__|\n\
    \             |__|__|__|__|\n             |  |  |  |  |\n             '-----------'\n\
    \                 VNF#2\n             .-----------.\n             |__|__|__|__|\n\
    \   Cores 2-5 |__|__|__|__|\n             |__|__|__|__|\n             |  |  |\
    \  |  |\n             '-----------'\n                 VNF#3             VNF#4\
    \             VNF#5\n             .-----------.    .-----------.     .-----------.\n\
    \             |__|__|__|__|    |__|__|__|__|     |__|__|__|__|\n   Core 6    |__|__|__|__|\
    \    |__|__|__|__|     |__|__|__|__|\n             |__|__|__|__|    |__|__|__|__|\
    \     |__|__|__|__|\n             |  |  |  |  |    |  |  |  |  |     |  |  | \
    \ |  |\n             '-----------'    '-----------'     '-----------'\n      \
    \            VNF#6\n             .-----------.\n             |__|__|__|__|\n \
    \  Core 7    |__|__|__|__|\n             |__|__|__|__|\n             |  |  | \
    \ |  |\n             '-----------'\n   The combination of tables above could be\
    \ built incrementally,\n   beginning with VNF#1 and one Core, then adding VNFs\
    \ according to\n   their supporting Core assignments.  X-Y plots of critical benchmarks\n\
    \   would also provide insight to the effect of increased hardware\n   utilization.\
    \  All VNFs might be of the same type, or to match a\n   production environment,\
    \ there could be VNFs of multiple types and\n   categories.  In this figure, VNFs\
    \ #3-#5 are assumed to require small\n   CPU resources, while VNF#2 requires four\
    \ Cores to perform its\n   function.\n"
- title: 4.5.  Power Consumption
  contents:
  - "4.5.  Power Consumption\n   Although there is incomplete work to benchmark physical\
    \ network\n   function power consumption in a meaningful way, the desire to measure\n\
    \   the physical Infrastructure supporting the virtual functions only\n   adds\
    \ to the need.  Both maximum power consumption and dynamic power\n   consumption\
    \ (with varying load) would be useful.  The Intelligent\n   Platform Management\
    \ Interface (IPMI) standard [IPMI2.0] has been\n   implemented by many manufacturers\
    \ and supports measurement of\n   instantaneous energy consumption.\n   To assess\
    \ the instantaneous energy consumption of virtual resources,\n   it may be possible\
    \ to estimate the value using an overall metric\n   based on utilization readings,\
    \ according to [NFVIaas-FRAMEWORK].\n"
- title: 5.  Security Considerations
  contents:
  - "5.  Security Considerations\n   Benchmarking activities as described in this\
    \ memo are limited to\n   technology characterization of a DUT/SUT using controlled\
    \ stimuli in\n   a laboratory environment, with dedicated address space and the\n\
    \   constraints specified in the sections above.\n   The benchmarking network\
    \ topology will be an independent test setup\n   and MUST NOT be connected to\
    \ devices that may forward the test\n   traffic into a production network or misroute\
    \ traffic to the test\n   management network.\n   Further, benchmarking is performed\
    \ on a \"black-box\" basis, relying\n   solely on measurements observable external\
    \ to the DUT/SUT.\n   Special capabilities SHOULD NOT exist in the DUT/SUT specifically\
    \ for\n   benchmarking purposes.  Any implications for network security arising\n\
    \   from the DUT/SUT SHOULD be identical in the lab and in production\n   networks.\n"
- title: 6.  IANA Considerations
  contents:
  - "6.  IANA Considerations\n   This document does not require any IANA actions.\n"
- title: 7.  References
  contents:
  - '7.  References

    '
- title: 7.1.  Normative References
  contents:
  - "7.1.  Normative References\n   [NFV.PER001]\n              ETSI, \"Network Function\
    \ Virtualization: Performance &\n              Portability Best Practices\", ETSI\
    \ GS NFV-PER 001, V1.1.2,\n              December 2014.\n   [RFC2119]  Bradner,\
    \ S., \"Key words for use in RFCs to Indicate\n              Requirement Levels\"\
    , BCP 14, RFC 2119,\n              DOI 10.17487/RFC2119, March 1997,\n       \
    \       <http://www.rfc-editor.org/info/rfc2119>.\n   [RFC2544]  Bradner, S. and\
    \ J. McQuaid, \"Benchmarking Methodology for\n              Network Interconnect\
    \ Devices\", RFC 2544,\n              DOI 10.17487/RFC2544, March 1999,\n    \
    \          <http://www.rfc-editor.org/info/rfc2544>.\n   [RFC4689]  Poretsky,\
    \ S., Perser, J., Erramilli, S., and S. Khurana,\n              \"Terminology\
    \ for Benchmarking Network-layer Traffic\n              Control Mechanisms\",\
    \ RFC 4689, DOI 10.17487/RFC4689,\n              October 2006, <http://www.rfc-editor.org/info/rfc4689>.\n\
    \   [RFC4737]  Morton, A., Ciavattone, L., Ramachandran, G., Shalunov,\n     \
    \         S., and J. Perser, \"Packet Reordering Metrics\", RFC 4737,\n      \
    \        DOI 10.17487/RFC4737, November 2006,\n              <http://www.rfc-editor.org/info/rfc4737>.\n\
    \   [RFC7498]  Quinn, P., Ed. and T. Nadeau, Ed., \"Problem Statement for\n  \
    \            Service Function Chaining\", RFC 7498,\n              DOI 10.17487/RFC7498,\
    \ April 2015,\n              <http://www.rfc-editor.org/info/rfc7498>.\n   [RFC8174]\
    \  Leiba, B., \"Ambiguity of Uppercase vs Lowercase in RFC\n              2119\
    \ Key Words\", BCP 14, RFC 8174, DOI 10.17487/RFC8174,\n              May 2017,\
    \ <http://www.rfc-editor.org/info/rfc8174>.\n"
- title: 7.2.  Informative References
  contents:
  - "7.2.  Informative References\n   [Approved_ETSI_NFV]\n              ETSI, Network\
    \ Functions Virtualisation Technical\n              Committee, \"ETSI NFV\",\n\
    \              <http://www.etsi.org/standards-search>.\n   [BareMetal]\n     \
    \         Popek, G. and R. Goldberg, \"Formal requirements for\n             \
    \ virtualizable third generation architectures\",\n              Communications\
    \ of the ACM, Volume 17, Issue 7, Pages\n              412-421, DOI 10.1145/361011.361073,\
    \ July 1974.\n   [Draft_ETSI_NFV]\n              ETSI, \"Network Functions Virtualisation:\
    \ Specifications\",\n              <http://www.etsi.org/technologies-clusters/technologies/\n\
    \              nfv>.\n   [IPMI2.0]  Intel Corporation, Hewlett-Packard Company,\
    \ NEC\n              Corporation, and Dell Inc., \"Intelligent Platform\n    \
    \          Management Interface Specification v2.0 (with latest\n            \
    \  errata)\", April 2015,\n              <http://www.intel.com/content/dam/www/public/us/en/\n\
    \              documents/specification-updates/ipmi-intelligent-platform-\n  \
    \            mgt-interface-spec-2nd-gen-v2-0-spec-update.pdf>.\n   [NFVIaas-FRAMEWORK]\n\
    \              Krishnan, R., Figueira, N., Krishnaswamy, D., Lopez, D.,\n    \
    \          Wright, S., Hinrichs, T., Krishnaswamy, R., and A. Yerra,\n       \
    \       \"NFVIaaS Architectural Framework for Policy Based Resource\n        \
    \      Placement and Scheduling\", Work in Progress,\n              draft-krishnan-nfvrg-policy-based-rm-nfviaas-06,\
    \ March\n              2016.\n   [OPNFV-BENCHMARK]\n              Tahhan, M.,\
    \ O'Mahony, B., and A. Morton, \"Benchmarking\n              Virtual Switches\
    \ in OPNFV\", Work in Progress,\n              draft-ietf-bmwg-vswitch-opnfv-04,\
    \ June 2017.\n   [RFC1242]  Bradner, S., \"Benchmarking Terminology for Network\n\
    \              Interconnection Devices\", RFC 1242, DOI 10.17487/RFC1242,\n  \
    \            July 1991, <http://www.rfc-editor.org/info/rfc1242>.\n   [RFC5481]\
    \  Morton, A. and B. Claise, \"Packet Delay Variation\n              Applicability\
    \ Statement\", RFC 5481, DOI 10.17487/RFC5481,\n              March 2009, <http://www.rfc-editor.org/info/rfc5481>.\n\
    \   [RFC6049]  Morton, A. and E. Stephan, \"Spatial Composition of\n         \
    \     Metrics\", RFC 6049, DOI 10.17487/RFC6049, January 2011,\n             \
    \ <http://www.rfc-editor.org/info/rfc6049>.\n   [SDN-BENCHMARK]\n            \
    \  Vengainathan, B., Basil, A., Tassinari, M., Manral, V.,\n              and\
    \ S. Banks, \"Terminology for Benchmarking SDN Controller\n              Performance\"\
    , Work in Progress, draft-ietf-bmwg-sdn-\n              controller-benchmark-term-04,\
    \ June 2017.\n   [X3.102]   ANSI, \"Information Systems - Data Communication Systems\n\
    \              and Services - User-Oriented Performance Parameters\n         \
    \     Communications Framework\", ANSI X3.102, 1983.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   The author acknowledges an encouraging conversation on this\
    \ topic\n   with Mukhtiar Shaikh and Ramki Krishnan in November 2013.  Bhavani\n\
    \   Parise and Ilya Varlashkin have provided useful suggestions to expand\n  \
    \ these considerations.  Bhuvaneswaran Vengainathan has already tried\n   the\
    \ 3x3 Matrix with the SDN controller document and contributed to\n   many discussions.\
    \  Scott Bradner quickly pointed out shared resource\n   dependencies in an early\
    \ vSwitch measurement proposal, and the topic\n   was included here as a key consideration.\
    \  Further development was\n   encouraged by Barry Constantine's comments following\
    \ the BMWG session\n   at IETF 92: the session itself was an affirmation for this\
    \ memo.\n   There have been many interesting contributions from Maryam Tahhan,\n\
    \   Marius Georgescu, Jacob Rapp, Saurabh Chattopadhyay, and others.\n"
- title: Author's Address
  contents:
  - "Author's Address\n   Al Morton\n   AT&T Labs\n   200 Laurel Avenue South\n  \
    \ Middletown, NJ  07748\n   United States of America\n   Phone: +1 732 420 1571\n\
    \   Fax:   +1 732 368 1192\n   Email: acmorton@att.com\n"
