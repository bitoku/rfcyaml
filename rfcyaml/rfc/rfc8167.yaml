- contents:
  - '    Bidirectional Remote Procedure Call on RPC-over-RDMA Transports

    '
  title: __initial_text__
- contents:
  - "Abstract\n   Minor versions of Network File System (NFS) version 4 newer than\n
    \  minor version 0 work best when Remote Procedure Call (RPC) transports\n   can
    send RPC transactions in both directions on the same connection.\n   This document
    describes how RPC transport endpoints capable of Remote\n   Direct Memory Access
    (RDMA) convey RPCs in both directions on a\n   single connection.\n"
  title: Abstract
- contents:
  - "Status of This Memo\n   This is an Internet Standards Track document.\n   This
    document is a product of the Internet Engineering Task Force\n   (IETF).  It represents
    the consensus of the IETF community.  It has\n   received public review and has
    been approved for publication by the\n   Internet Engineering Steering Group (IESG).
    \ Further information on\n   Internet Standards is available in Section 2 of RFC
    7841.\n   Information about the current status of this document, any errata,\n
    \  and how to provide feedback on it may be obtained at\n   http://www.rfc-editor.org/info/rfc8167.\n"
  title: Status of This Memo
- contents:
  - "Copyright Notice\n   Copyright (c) 2017 IETF Trust and the persons identified
    as the\n   document authors.  All rights reserved.\n   This document is subject
    to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n
    \  (http://trustee.ietf.org/license-info) in effect on the date of\n   publication
    of this document.  Please review these documents\n   carefully, as they describe
    your rights and restrictions with respect\n   to this document.  Code Components
    extracted from this document must\n   include Simplified BSD License text as described
    in Section 4.e of\n   the Trust Legal Provisions and are provided without warranty
    as\n   described in the Simplified BSD License.\n"
  title: Copyright Notice
- contents:
  - "Table of Contents\n   1.  Introduction  . . . . . . . . . . . . . . . . . . .
    . . . . .   2\n   2.  Understanding RPC Direction . . . . . . . . . . . . . .
    . . .   3\n   3.  Immediate Uses of Bidirectional RPC-over-RDMA . . . . . . .
    .   5\n   4.  Flow Control  . . . . . . . . . . . . . . . . . . . . . . . .   6\n
    \  5.  Sending and Receiving Operations in the Reverse Direction . .   8\n   6.
    \ In the Absence of Support for Reverse-Direction Operation . .  11\n   7.  Considerations
    for ULBs . . . . . . . . . . . . . . . . . . .  11\n   8.  Security Considerations
    . . . . . . . . . . . . . . . . . . .  12\n   9.  IANA Considerations . . . .
    . . . . . . . . . . . . . . . . .  12\n   10. Normative References  . . . . .
    . . . . . . . . . . . . . . .  12\n   Acknowledgements  . . . . . . . . . . .
    . . . . . . . . . . . . .  13\n   Author's Address  . . . . . . . . . . . . .
    . . . . . . . . . . .  13\n"
  title: Table of Contents
- contents:
  - "1.  Introduction\n   RPC-over-RDMA transports, introduced in [RFC8166], efficiently
    convey\n   Remote Procedure Call (RPC) transactions on transport layers capable\n
    \  of Remote Direct Memory Access (RDMA).  The purpose of this document\n   is
    to enable concurrent operation in both directions on a single\n   transport connection
    using RPC-over-RDMA protocol versions that do\n   not have specific facilities
    for reverse-direction operation.\n   Reverse-direction RPC transactions are necessary
    for the operation of\n   version 4.1 of the Network File System (NFS), and in
    particular, of\n   Parallel NFS (pNFS) [RFC5661], though any Upper-Layer Protocol
    (ULP)\n   implementation may make use of them.  An Upper-Layer Binding (ULB)\n
    \  for NFS version 4.x callback operation is additionally required (see\n   Section
    7) but is not provided in this document.\n   For example, using the approach described
    herein, RPC transactions\n   can be conveyed in both directions on the same RPC-over-RDMA
    version\n   1 connection without changes to the RPC-over-RDMA version 1 protocol.\n
    \  This document does not update the protocol specified in [RFC8166].\n   The
    remainder of this document assumes familiarity with the\n   terminology and concepts
    contained in [RFC8166], especially Sections\n   2 and 3.\n   The key words \"MUST\",
    \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD
    NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and\n   \"OPTIONAL\" in
    this document are to be interpreted as described in\n   BCP 14 [RFC2119] [RFC8174]
    when, and only when, they appear in all\n   capitals, as shown here.\n"
  title: 1.  Introduction
- contents:
  - "2.  Understanding RPC Direction\n   The Open Network Computing Remote Procedure
    Call (ONC RPC) protocol\n   as described in [RFC5531] is architected as a message-passing\n
    \  protocol between one server and one or more clients.  ONC RPC\n   transactions
    are made up of two types of messages.\n   A CALL message, or \"Call\", requests
    work.  A Call is designated by\n   the value CALL in the message's msg_type field.
    \ An arbitrary unique\n   value is placed in the message's Transaction ID (XID)
    field.  A host\n   that originates a Call is referred to in this document as a\n
    \  \"Requester\".\n   A REPLY message, or \"Reply\", reports the results of work
    requested by\n   a Call.  A Reply is designated by the value REPLY in the message's\n
    \  msg_type field.  The value contained in the message's XID field is\n   copied
    from the Call whose results are being returned.  A host that\n   emits a Reply
    is referred to as a \"Responder\".\n   Typically, a Call results in a corresponding
    Reply.  A Reply is never\n   sent without a corresponding Call.\n   RPC-over-RDMA
    is a connection-oriented RPC transport.  In all cases,\n   when a connection-oriented
    transport is used, ONC RPC client\n   endpoints are responsible for initiating
    transport connections, while\n   ONC RPC service endpoints passively await incoming
    connection\n   requests.\n   RPC direction on connectionless RPC transports is
    not addressed in\n   this document.\n"
  - contents:
    - "2.1.  Forward Direction\n   Traditionally, an ONC RPC client acts as a Requester,
      while an ONC\n   RPC service acts as a Responder.  This form of message passing
      is\n   referred to as \"forward-direction\" operation.\n"
    title: 2.1.  Forward Direction
  - contents:
    - "2.2.  Reverse Direction\n   The ONC RPC specification [RFC5531] does not forbid
      passing messages\n   in the other direction.  An ONC RPC service endpoint can
      act as a\n   Requester, in which case, an ONC RPC client endpoint acts as a\n
      \  Responder.  This form of message passing is referred to as \"reverse-\n   direction\"
      operation.\n   During reverse-direction operation, the ONC RPC client is responsible\n
      \  for establishing transport connections, even though RPC Call messages\n   come
      from the ONC RPC server.\n   ONC RPC clients and servers are optimized to perform
      and scale well\n   while handling traffic in the forward direction and might
      not be\n   prepared to handle operation in the reverse direction.  Not until
      NFS\n   version 4.1 [RFC5661] has there been a strong need to handle reverse-\n
      \  direction operation.\n"
    title: 2.2.  Reverse Direction
  - contents:
    - "2.3.  Bidirectional Operation\n   A pair of connected RPC endpoints may choose
      to use only forward-\n   direction or only reverse-direction operations on a
      particular\n   transport connection.  Or, these endpoints may send Calls in
      both\n   directions concurrently on the same transport connection.\n   \"Bidirectional
      operation\" occurs when both transport endpoints act as\n   a Requester and
      a Responder at the same time.\n   Bidirectionality is an extension of RPC transport
      connection sharing.\n   Two RPC endpoints wish to exchange independent RPC messages
      over a\n   shared connection, but in opposite directions.  These messages may
      or\n   may not be related to the same workloads or RPC Programs.\n"
    title: 2.3.  Bidirectional Operation
  - contents:
    - "2.4.  XID Values\n   Section 9 of [RFC5531] introduces the ONC RPC transaction
      identifier,\n   or \"XID\" for short.  The value of an XID is interpreted in
      the\n   context of the message's msg_type field.\n   o  The XID of a Call is
      arbitrary but is unique among outstanding\n      Calls from that Requester.\n
      \  o  The XID of a Reply always matches that of the initiating Call.\n   When
      receiving a Reply, a Requester matches the XID value in the\n   Reply with a
      Call it previously sent.\n"
    - contents:
      - "2.4.1.  XID Generation\n   During bidirectional operation, forward- and reverse-direction
        XIDs\n   are typically generated on distinct hosts by possibly different\n
        \  algorithms.  There is no coordination between forward- and reverse-\n   direction
        XID generation.\n   Therefore, a forward-direction Requester MAY use the same
        XID value\n   at the same time as a reverse-direction Requester on the same\n
        \  transport connection.  Though such concurrent requests use the same\n   XID
        value, they represent distinct ONC RPC transactions.\n"
      title: 2.4.1.  XID Generation
    title: 2.4.  XID Values
  title: 2.  Understanding RPC Direction
- contents:
  - '3.  Immediate Uses of Bidirectional RPC-over-RDMA

    '
  - contents:
    - "3.1.  NFS Version 4.0 Callback Operation\n   An NFS version 4.0 client employs
      a traditional ONC RPC client to\n   send NFS requests to an NFS version 4.0
      server's traditional ONC RPC\n   service [RFC7530].  NFS version 4.0 requests
      flow in the forward\n   direction on a connection established by the client.
      \ This connection\n   is referred to as a \"forechannel\" connection.\n   An
      NFS version 4.x \"delegation\" is simply a promise made by a server\n   that
      it will notify a client before another client or program running\n   on the
      server is allowed access to a file.  With this guarantee, that\n   client can
      operate as sole accessor of the file.  In particular, it\n   can manage the
      file's data and metadata caches aggressively.\n   To administer file delegations,
      NFS version 4.0 introduces the use of\n   callback operations, or \"callbacks\",
      in Section 10.2 of [RFC7530].\n   An NFS version 4.0 server sets up a forward-direction
      ONC RPC client,\n   and an NFS version 4.0 client sets up a forward-direction
      ONC RPC\n   service.  Callbacks flow in the forward direction on a connection\n
      \  established between the server's callback client and the client's\n   callback
      service.  This connection is distinct from connections being\n   used as forechannels
      and is referred to as a \"backchannel\n   connection\".\n   When an RDMA transport
      is used as a forechannel, an NFS version 4.0\n   client typically provides a
      TCP-based callback service.  The client's\n   SETCLIENTID operation advertises
      the callback service endpoint with a\n   \"tcp\" or \"tcp6\" netid.  The server
      then connects to this service\n   using a TCP socket.\n   NFS version 4.0 implementations
      can function without a backchannel in\n   place.  In this case, the NFS server
      does not grant file delegations.\n   This might result in a negative performance
      effect, but correctness\n   is not affected.\n"
    title: 3.1.  NFS Version 4.0 Callback Operation
  - contents:
    - "3.2.  NFS Version 4.1 Callback Operation\n   NFS version 4.1 supports file
      delegation in a similar fashion to NFS\n   version 4.0 and extends the callback
      mechanism to manage pNFS\n   layouts, as discussed in Section 12 of [RFC5661].\n
      \  NFS version 4.1 transport connections are initiated by NFS version\n   4.1
      clients.  Therefore, NFS version 4.1 servers send callbacks to\n   clients in
      the reverse direction on connections established by NFS\n   version 4.1 clients.\n
      \  NFS version 4.1 clients and servers indicate to their peers that a\n   backchannel
      capability is available on a given transport connection\n   in the arguments
      and results of the NFS CREATE_SESSION or\n   BIND_CONN_TO_SESSION operations.\n
      \  NFS version 4.1 clients may establish distinct transport connections\n   for
      forechannel and backchannel operation, or they may combine\n   forechannel and
      backchannel operation on one transport connection\n   using bidirectional operation.\n
      \  Without a reverse-direction RPC-over-RDMA capability, an NFS version\n   4.1
      client additionally connects using a transport with reverse-\n   direction capability
      to use as a backchannel.  Opening an independent\n   TCP socket is the only
      choice for an NFS version 4.1 backchannel\n   connection in this case.\n   Implementations
      often find it more convenient to use a single\n   combined transport (i.e.,
      a transport that is capable of\n   bidirectional operation).  This simplifies
      connection establishment\n   and recovery during network partitions or when
      one endpoint restarts.\n   This can also enable better scaling by using fewer
      transport\n   connections to perform the same work.\n   As with NFS version
      4.0, if a backchannel is not in use, an NFS\n   version 4.1 server does not
      grant delegations.  Because NFS version\n   4.1 relies on callbacks to manage
      pNFS layout state, pNFS operation\n   is not possible without a backchannel.\n"
    title: 3.2.  NFS Version 4.1 Callback Operation
  title: 3.  Immediate Uses of Bidirectional RPC-over-RDMA
- contents:
  - "4.  Flow Control\n   For an RDMA Send operation to work properly, the receiving
    peer has\n   to have already posted a Receive buffer in which to accept the\n
    \  incoming message.  If a receiver hasn't posted enough buffers to\n   accommodate
    each incoming Send operation, the receiving RDMA provider\n   is allowed to terminate
    the RDMA connection.\n   RPC-over-RDMA transport protocols provide built-in send
    flow control\n   to prevent overrunning the number of pre-posted Receive buffers
    on a\n   connection's receive endpoint using a \"credit grant\" mechanism.  The\n
    \  use of credits in RPC-over-RDMA version 1 is described in\n   Section 3.3.1
    of [RFC8166].\n"
  - contents:
    - "4.1.  Reverse-Direction Credits\n   RPC-over-RDMA credits work the same way
      in the reverse direction as\n   they do in the forward direction.  However,
      forward-direction credits\n   and reverse-direction credits on the same connection
      are accounted\n   separately.  Direction-independent credit accounting prevents
      head-\n   of-line blocking in one direction from impacting operation in the\n
      \  other direction.\n   The forward-direction credit value retains the same
      meaning whether\n   or not there are reverse-direction resources associated
      with an RPC-\n   over-RDMA transport connection.  This is the number of RPC
      requests\n   the forward-direction Responder (the ONC RPC server) is prepared
      to\n   receive concurrently.\n   The reverse-direction credit value is the number
      of RPC requests the\n   reverse-direction Responder (the ONC RPC client) is
      prepared to\n   receive concurrently.  The reverse-direction credit value MAY
      be\n   different than the forward-direction credit value.\n   During bidirectional
      operation, each receiver has to decide whether\n   an incoming message contains
      a credit request (the receiver is acting\n   as a Responder) or a credit grant
      (the receiver is acting as a\n   requester) and apply the credit value accordingly.\n
      \  When message direction is not fully determined by context (e.g.,\n   suggested
      by the definition of the RPC-over-RDMA version that is in\n   use) or by an
      accompanying RPC message payload with a call direction\n   field, it is not
      possible for the receiver to tell with certainty\n   whether the header credit
      value is a request or grant.  In such\n   cases, the receiver MUST ignore the
      header's credit value.\n"
    title: 4.1.  Reverse-Direction Credits
  - contents:
    - "4.2.  Inline Thresholds\n   Forward- and reverse-direction operation on the
      same connection share\n   the same Receive buffers.  Therefore, the inline threshold
      values for\n   the forward direction and the reverse direction are the same.
      \ The\n   call inline threshold for the reverse direction is the same as the\n
      \  reply inline threshold for the forward direction, and vice versa.\n   For
      more information, see Section 3.3.2 of [RFC8166].\n"
    title: 4.2.  Inline Thresholds
  - contents:
    - "4.3.  Managing Receive Buffers\n   An RPC-over-RDMA transport endpoint posts
      Receive buffers before it\n   can receive and process incoming RPC-over-RDMA
      messages.  If a sender\n   transmits a message for a receiver that has no posted
      Receive buffer,\n   the RDMA provider is allowed to drop the RDMA connection.\n"
    - contents:
      - "4.3.1.  Client Receive Buffers\n   Typically, an RPC-over-RDMA Requester
        posts only as many Receive\n   buffers as there are outstanding RPC Calls.
        \ Therefore, a client\n   endpoint without reverse-direction support might,
        at times, have no\n   available Receive buffers.\n   To receive incoming reverse-direction
        Calls, an RPC-over-RDMA client\n   endpoint posts enough additional Receive
        buffers to match its\n   advertised reverse-direction credit value.  Each
        outstanding forward-\n   direction RPC requires an additional Receive buffer
        above this\n   minimum.\n   When an RDMA transport connection is lost, all
        active Receive buffers\n   are flushed and are no longer available to receive
        incoming messages.\n   When a fresh transport connection is established, a
        client endpoint\n   posts a Receive buffer to handle the Reply for each retransmitted\n
        \  forward-direction Call, and it posts enough Receive buffers to handle\n
        \  reverse-direction Calls.\n"
      title: 4.3.1.  Client Receive Buffers
    - contents:
      - "4.3.2.  Server Receive Buffers\n   A forward-direction RPC-over-RDMA service
        endpoint posts as many\n   Receive buffers as it expects incoming forward-direction
        Calls.  That\n   is, it posts no fewer buffers than the number of credits
        granted in\n   the rdma_credit field of forward-direction RPC replies.\n   To
        receive incoming reverse-direction replies, an RPC-over-RDMA\n   server endpoint
        posts enough additional Receive buffers to handle\n   replies for each reverse-direction
        Call it sends.\n   When the existing transport connection is lost, all active
        Receive\n   buffers are flushed and are no longer available to receive incoming\n
        \  messages.  When a fresh transport connection is established, a server\n
        \  endpoint posts a Receive buffer to handle the Reply for each\n   retransmitted
        reverse-direction Call, and it posts enough Receive\n   buffers to handle
        incoming forward-direction Calls.\n"
      title: 4.3.2.  Server Receive Buffers
    title: 4.3.  Managing Receive Buffers
  title: 4.  Flow Control
- contents:
  - "5.  Sending and Receiving Operations in the Reverse Direction\n   The operation
    of RPC-over-RDMA transports in the forward direction is\n   defined in [RFC5531]
    and [RFC8166].  In this section, a mechanism for\n   reverse-direction operation
    on RPC-over-RDMA is defined.  Reverse-\n   direction operation used in combination
    with forward-direction\n   operation enables bidirectional communication on a
    common RPC-over-\n   RDMA transport connection.\n   Certain fields in the RPC-over-RDMA
    header have a fixed position in\n   all versions of RPC-over-RDMA.  The normative
    specification of these\n   fields is contained in Section 4 of [RFC8166].\n"
  - contents:
    - "5.1.  Sending a Call in the Reverse Direction\n   To form a reverse-direction
      RPC-over-RDMA Call message, an ONC RPC\n   service endpoint constructs an RPC-over-RDMA
      header containing a\n   fresh RPC XID in the rdma_xid field (see Section 2.4
      for full\n   requirements).\n   The rdma_vers field MUST contain the same value
      in reverse- and\n   forward-direction Call messages on the same connection.\n
      \  The number of requested reverse-direction credits is placed in the\n   rdma_credit
      field (see Section 4).\n   Whether presented inline or as a separate chunk,
      the ONC RPC Call\n   header MUST start with the same XID value that is present
      in the RPC-\n   over-RDMA header, and the RPC header's msg_type field MUST contain\n
      \  the value CALL.\n"
    title: 5.1.  Sending a Call in the Reverse Direction
  - contents:
    - "5.2.  Sending a Reply in the Reverse Direction\n   To form a reverse-direction
      RPC-over-RDMA Reply message, an ONC RPC\n   client endpoint constructs an RPC-over-RDMA
      header containing a copy\n   of the matching ONC RPC Call's RPC XID in the rdma_xid
      field (see\n   Section 2.4 for full requirements).\n   The rdma_vers field MUST
      contain the same value in a reverse-\n   direction Reply message as in the matching
      Call message.\n   The number of granted reverse-direction credits is placed
      in the\n   rdma_credit field (see Section 4).\n   Whether presented inline or
      as a separate chunk, the ONC RPC Reply\n   header MUST start with the same XID
      value that is present in the RPC-\n   over-RDMA header, and the RPC header's
      msg_type field MUST contain\n   the value REPLY.\n"
    title: 5.2.  Sending a Reply in the Reverse Direction
  - contents:
    - "5.3.  Using Chunks in Reverse-Direction Operations\n   A \"chunk\" refers to
      a portion of a message's Payload stream that is\n   DDP-eligible and that is
      placed directly in the receiver's memory by\n   the transport.  Chunk data may
      be moved by an explicit RDMA\n   operation, for example.  Chunks are defined
      in Section 3.4.4 and DDP-\n   eligibility is covered in Section 6.1 of [RFC8166].\n
      \  Chunks MAY be used in the reverse direction.  They operate the same\n   way
      as in the forward direction.\n   An implementation might support only ULPs that
      have no DDP-eligible\n   data items.  Such ULPs may use only small messages,
      or they may have\n   a native mechanism for restricting the size of reverse-direction
      RPC\n   messages, obviating the need to handle Long Messages in the reverse\n
      \  direction.\n   When there is no ULP requirement for chunks in the reverse
      direction,\n   implementers can choose not to provide support for chunks in
      the\n   reverse direction.  This avoids the complexity of adding support for\n
      \  performing RDMA Reads and Writes in the reverse direction.\n   When chunks
      are not implemented, RPC messages in the reverse\n   direction are always sent
      using a Short Message; therefore, they can\n   be no larger than what can be
      sent inline (that is, without chunks).\n   Sending an inline message larger
      than the inline threshold can result\n   in loss of connection.\n   If a reverse-direction
      requester provides a non-empty chunk list to a\n   Responder that does not support
      chunks, the Responder MUST reply with\n   an RDMA_ERROR message with rdma_err
      field set to ERR_CHUNK.\n"
    title: 5.3.  Using Chunks in Reverse-Direction Operations
  - contents:
    - "5.4.  Reverse-Direction Retransmission\n   In rare cases, an ONC RPC service
      cannot complete an RPC transaction\n   and then send a reply.  This can be because
      the transport connection\n   was lost, because the Call or Reply message was
      dropped, or because\n   the ULP delayed or dropped the ONC RPC request.  Typically,
      the\n   Requester sends the RPC transaction again, reusing the same RPC XID.\n
      \  This is known as an \"RPC retransmission\".\n   In the forward direction,
      the Requester is the ONC RPC client.  The\n   client is always responsible for
      establishing a transport connection\n   before sending again.\n   With reverse-direction
      operation, the Requester is the ONC RPC\n   server.  Because an ONC RPC server
      does not establish transport\n   connections with clients, it cannot retransmit
      if there is no\n   transport connection.  It is forced to wait for the ONC RPC
      client to\n   re-establish a transport connection before it can retransmit ONC
      RPC\n   transactions in the reverse direction.\n   If the ONC RPC client peer
      has no work to do, it can be some time\n   before it re-establishes a transport
      connection.  A waiting reverse-\n   direction ONC RPC Call may time out to avoid
      waiting indefinitely for\n   a connection to be established.\n   Therefore,
      forward-direction Requesters SHOULD maintain a transport\n   connection as long
      as there is the possibility that the connection\n   peer can send reverse-direction
      requests.  For example, while an NFS\n   version 4.1 client has open delegated
      files or active pNFS layouts,\n   it maintains one or more transport connections
      to enable the NFS\n   server to perform callback operations.\n"
    title: 5.4.  Reverse-Direction Retransmission
  title: 5.  Sending and Receiving Operations in the Reverse Direction
- contents:
  - "6.  In the Absence of Support for Reverse-Direction Operation\n   An RPC-over-RDMA
    transport endpoint might not support reverse-\n   direction operation (and thus
    it does not support bidirectional\n   operation).  There might be no mechanism
    in the transport\n   implementation to do so.  Or in an implementation that can
    support\n   operation in the reverse direction, the ULP might not yet have\n   configured
    or enabled the transport to handle reverse-direction\n   traffic.\n   If an endpoint
    is not prepared to receive an incoming reverse-\n   direction message, loss of
    the RDMA connection might result.  Thus,\n   denial of service could result if
    a sender continues to send reverse-\n   direction messages after every transport
    reconnect to an endpoint\n   that is not prepared to receive them.\n   When dealing
    with the possibility that the remote peer has no\n   transport-level support for
    reverse-direction operation, the ULP\n   becomes responsible for informing peers
    when reverse-direction\n   operation is supported.  Otherwise, even a simple reverse-direction\n
    \  RPC NULL procedure from a peer could result in a lost connection.\n   Therefore,
    a ULP MUST NOT perform reverse-direction ONC RPC\n   operations until the peer
    has indicated it is prepared to handle\n   them.  A description of ULP mechanisms
    used for this indication is\n   outside the scope of this document.\n   For example,
    an NFS version 4.1 server does not send backchannel\n   messages to an NFS version
    4.1 client before the NFS version 4.1\n   client has sent a CREATE_SESSION or
    a BIND_CONN_TO_SESSION operation.\n   As long as an NFS version 4.1 client has
    prepared appropriate\n   resources to receive reverse-direction operations before
    sending one\n   of these NFS operations, denial of service is avoided.\n"
  title: 6.  In the Absence of Support for Reverse-Direction Operation
- contents:
  - "7.  Considerations for ULBs\n   A ULP that operates on RPC-over-RDMA transports
    may have procedures\n   that include DDP-eligible data items.  DDP-eligibility
    is specified\n   in an Upper-Layer Binding (ULB).  Direction of operation does
    not\n   obviate the need for DDP-eligibility statements.\n   Reverse-direction-only
    operation requires the client endpoint to\n   establish a fresh connection.  The
    ULB can specify appropriate RPC\n   binding parameters for such connections.\n
    \  Bidirectional operation occurs on an already-established connection.\n   Specification
    of RPC binding parameters is usually not necessary in\n   this case.\n   For bidirectional
    operation, other considerations may apply when\n   distinct RPC Programs share
    an RPC-over-RDMA transport connection\n   concurrently.  Consult Section 6 of
    [RFC8166] for details about what\n   else may be contained in a ULB.\n"
  title: 7.  Considerations for ULBs
- contents:
  - "8.  Security Considerations\n   RPC security is handled in the RPC layer, which
    is above the\n   transport layer where RPC-over-RDMA operates.\n   Reverse-direction
    operations make use of an authentication mechanism\n   and credentials that are
    independent of forward-direction operation\n   but otherwise operate in the same
    fashion as outlined in Section 8.2\n   of [RFC8166].\n"
  title: 8.  Security Considerations
- contents:
  - "9.  IANA Considerations\n   This document does not require any IANA actions.\n"
  title: 9.  IANA Considerations
- contents:
  - "10.  Normative References\n   [RFC2119]  Bradner, S., \"Key words for use in
    RFCs to Indicate\n              Requirement Levels\", BCP 14, RFC 2119,\n              DOI
    10.17487/RFC2119, March 1997,\n              <http://www.rfc-editor.org/info/rfc2119>.\n
    \  [RFC5531]  Thurlow, R., \"RPC: Remote Procedure Call Protocol\n              Specification
    Version 2\", RFC 5531, DOI 10.17487/RFC5531,\n              May 2009, <http://www.rfc-editor.org/info/rfc5531>.\n
    \  [RFC5661]  Shepler, S., Ed., Eisler, M., Ed., and D. Noveck, Ed.,\n              \"Network
    File System (NFS) Version 4 Minor Version 1\n              Protocol\", RFC 5661,
    DOI 10.17487/RFC5661, January 2010,\n              <http://www.rfc-editor.org/info/rfc5661>.\n
    \  [RFC7530]  Haynes, T., Ed. and D. Noveck, Ed., \"Network File System\n              (NFS)
    Version 4 Protocol\", RFC 7530, DOI 10.17487/RFC7530,\n              March 2015,
    <http://www.rfc-editor.org/info/rfc7530>.\n   [RFC8166]  Lever, C., Ed., Simpson,
    W., and T. Talpey, \"Remote Direct\n              Memory Access Transport for
    Remote Procedure Call Version\n              1\", RFC 8166, DOI 10.17487/RFC8166,
    June 2017,\n              <http://www.rfc-editor.org/info/rfc8166>.\n   [RFC8174]
    \ Leiba, B., \"Ambiguity of Uppercase vs Lowercase in RFC\n              2119
    Key Words\", BCP 14, RFC 8174, DOI 10.17487/RFC8174,\n              May 2017,
    <http://www.rfc-editor.org/info/rfc8174>.\n"
  title: 10.  Normative References
- contents:
  - "Acknowledgements\n   Tom Talpey was an indispensable resource, in addition to
    creating the\n   foundation upon which this work is based.  The author's warmest\n
    \  regards go to him for his help and support.\n   Dave Noveck provided excellent
    review, constructive suggestions, and\n   navigational guidance throughout the
    process of drafting this\n   document.\n   Dai Ngo was a solid partner and collaborator.
    \ Together we\n   constructed and tested independent prototypes of the changes\n
    \  described in this document.\n   The author wishes to thank Bill Baker and Greg
    Marsden for their\n   unwavering support of this work.  In addition, the author
    gratefully\n   acknowledges the expert contributions of Karen Deitke, Chunli Zhang,\n
    \  Mahesh Siddheshwar, Steve Wise, and Tom Tucker.\n   Special thanks go to Transport
    Area Director Spencer Dawkins, NFSV4\n   Working Group Chair and Document Shepherd
    Spencer Shepler, and NFSV4\n   Working Group Secretary Tom Haynes for their support.\n"
  title: Acknowledgements
- contents:
  - "Author's Address\n   Charles Lever\n   Oracle Corporation\n   1015 Granger Avenue\n
    \  Ann Arbor, MI  48104\n   United States of America\n   Phone: +1 248 816 6463\n
    \  Email: chuck.lever@oracle.com\n"
  title: Author's Address
