- title: __initial_text__
  contents:
  - '                   Issues affecting MARS Cluster Size

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This memo provides information for the Internet community.\
    \  This memo\n   does not specify an Internet standard of any kind.  Distribution\
    \ of\n   this memo is unlimited.\n"
- title: Abstract
  contents:
  - "Abstract\n   IP multicast over ATM currently uses the MARS model [1] to manage\
    \ the\n   use of ATM pt-mpt SVCs for IP multicast packet forwarding. The scope\n\
    \   of any given MARS services is the MARS Cluster - typically the same\n   as\
    \ an IPv4 Logical IP Subnet (LIS). Current IP/ATM networks are\n   usually architected\
    \ with unicast routing and forwarding issues\n   dictating the sizes of individual\
    \ LISes. However, as IP multicast is\n   deployed as a service, the size of a\
    \ LIS will only be as big as a\n   MARS Cluster can be. This document provides\
    \ a qualitative look at the\n   issues constraining a MARS Cluster's size, including\
    \ the impact of VC\n   limits in switches and NICs, geographical distribution\
    \ of cluster\n   members, and the use of VC Mesh or MCS modes to support multicast\n\
    \   groups.\n"
- title: 1. Introduction
  contents:
  - "1. Introduction\n   A MARS Cluster is the set of IP/ATM interfaces that are willing\
    \ to\n   engage in direct, ATM level pt-mpt SVCs to perform IP multicast\n   packet\
    \ forwarding [1]. Each IP/ATM interface (a MARS Client) must\n   keep state information\
    \ regarding the ATM addresses of each leaf node\n   (recipient) of each  pt-mpt\
    \ SVC it has open. In  addition, each MARS\n   Client receives MARS_JOIN and MARS_LEAVE\
    \ messages from the MARS\n   whenever there is a requirement that Clients around\
    \ the Cluster need\n   to update their pt-mpt SVCs for a given IP multicast group.\n\
    \   The definition of Cluster 'size' can mean two things - the number of\n   MARS\
    \ Clients using a given MARS, and the geographic distribution of\n   MARS Clients.\
    \ The number of MARS Clients in a Cluster impacts on the\n   amount of state information\
    \ any given client may need to store while\n   managing  outgoing  pt- mpt SVCs.\
    \ It also impacts on the average rate\n   of JOIN/LEAVE traffic that is propagated\
    \ by the MARS on\n   ClusterControlVC, and the number of pt-mpt VCs that may need\n\
    \   modification each time a MARS_JOIN or MARS_LEAVE appears on\n   ClusterControlVC.\n\
    \   The geographic distribution of clients affects the latency between a\n   client\
    \ issuing a MARS_JOIN, and it finally being added onto the pt-\n   mpt VCs of\
    \ the other MARS Clients transmitting to the specified\n   multicast group. (This\
    \ latency is made up of both the time to\n   propagate the MARS_JOIN, and the\
    \ delay in the underlying ATM cloud's\n   reaction to the subsequent ADD_PARTY\
    \ messages.)\n   When architecting an IP/ATM network it is important to understand\
    \ the\n   worst case scaling limits applicable to your Clusters. This document\n\
    \   provides a primarily qualitative look at the design choices that\n   impose\
    \ the most dramatic constraints on Cluster size. Since the focus\n   is on worst-case\
    \ scenarios, most of the analysis will assume\n   multicast groups that are VC\
    \ Mesh based and have all cluster members\n   as sources and receivers. Engineering\
    \ using the worst-case boundary\n   conditions, then applying optimisations such\
    \ as Multicast Servers\n   (MCS), provides the Cluster with a margin of safety.\
    \  It is hoped\n   that more detailed quantitative analysis of Cluster sizing\
    \ limits\n   will be prompted by this document.\n   Section 2 comments on the\
    \ VC state requirements of the MARS model,\n   while Sections 3 and 4 identify\
    \ the group change processing load and\n   latency characteristics of a cluster\
    \ as a function of its size.\n   Section 5 looks at how Multicast Routers (both\
    \ conventional and\n   combination router/switch architectures) increase the scale\
    \ of a\n   multicast capable IP/ATM network. Finally, Section 6 discusses how\n\
    \   the use of Multicast Servers (MCS) might impact on the worst case\n   Cluster\
    \ size limits.\n"
- title: 2. VC state limitations.
  contents:
  - "2. VC state limitations.\n   Two characteristics of ATM NICs and switches will\
    \ limit the number of\n   members a Cluster may contain. They are:\n      The\
    \ maximum number of VCs that can be originated from, or\n      terminate on, a\
    \ port (VCmax).\n      The maximum number of leaf nodes supportable by a root\
    \ node\n      (LEAFmax).\n   We'll assume that the MARS node has similar VCmax\
    \ and LEAFmax values\n   as Cluster members.  VCmax affects the Cluster size because\
    \ of the\n   following:\n      The MARS terminates a pt-pt control VC from each\
    \ cluster member,\n      and originates a VC for ClusterControlVC and ServerControlVC.\n\
    \      When a multicast group is VC Mesh based, a group member terminates\n  \
    \    a VC from every sender to the group, per group.\n      When a multicast group\
    \ is MCS based, the MCS terminates a VC from\n      every sender to the group.\n\
    \   LEAFmax affects the Cluster size because of the following:\n      ClusterControlVC\
    \ from the MARS. It has a leaf node per cluster\n      member (MARS Client).\n\
    \      Packet forwarding SVCs out of each MARS Client for each IP\n      multicast\
    \ group being sent to. It has a leaf node for each group\n      member when a\
    \ group is VC Mesh based.\n      Packet forwarding SVCs out of each MCS for each\
    \ IP multicast group\n      being sent to. It has a leaf node for each group member\
    \ when a\n      group is MCS based.\n   If we have N cluster members, and M multicast\
    \ groups active (using VC\n   Mesh mode, and densely populated - all receivers\
    \ are senders), the\n   following observations may be made:\n      ClusterControlVC\
    \ has N leaf nodes, so\n            N <= LEAFmax.\n      The MARS terminates a\
    \ pt-pt VC from each cluster member, and\n      originates ClusterControlVC and\
    \ ServerControlVC, so\n            (N+2) <= VCmax.\n      Each Cluster Member\
    \ sources 1 VC per group, terminates (N-1) VC\n      per group, originates a pt-pt\
    \ VC to the MARS, and terminates 1 VC\n      as a leaf on ClusterControlVC, so\n\
    \            (M*N) + 2 <= VCmax.\n      The VC sourced by each Cluster member\
    \ per group goes to all other\n      cluster members, so\n            (N-1) <=\
    \ LEAFmax.\n   Since all the above conditions must be simultaneously true, we\
    \ can\n   see that the most constraining requirement is either:\n      (M*N) +\
    \ 2 <= VCmax.\n   or\n      N <= LEAFmax.\n   The limit involving VCmax is fundamentally\
    \ controlled by the VC\n   consumption of group members using a VC Mesh for data\
    \ forwarding,\n   rather than the termination of pt-pt control VCs on the MARS.\
    \ (It is\n   in practice going to be very dependent on the multicast group\n \
    \  membership distributions within the cluster.)\n   The LEAFmax limit comes from\
    \ ClusterControlVC, and is independent of\n   the density of group members (or\
    \ the ratios of senders to receivers)\n   for active multicast groups within the\
    \ cluster.\n   Under UNI 3.0/3.1 the most obvious limit on LEAFmax is 2^15 (the\
    \ leaf\n   node ID is 15 bits wide).  However, the signaling driver software for\n\
    \   most ATM NICs may impose a limit much lower than this - a function of\n  \
    \ how much per-leaf node state information they need to store (and are\n   capable\
    \ of storing) for pt-mpt SVCs.\n   VCmax is constrained by the ATM NIC hardware\
    \ (for available\n   segmentation or reassembly instances), or by the VC capacity\
    \ of the\n   switch port that the NIC is attached to.  VCmax will be the smaller\n\
    \   of the two.\n   A MARS Client may impose its own state storage limitations,\
    \ such that\n   the combined memory consumption of a MARS Client and the ATM NIC's\n\
    \   driver in a given host limits both LEAFmax and VCmax to values lower\n   than\
    \ the ATM NIC alone might have been able to support.\n   It may be possible to\
    \ work around LEAFmax limits by distributing the\n   leaf nodes across multiple\
    \ pt-mpt SVCs operating in parallel.\n   However, such an approach requires further\
    \ study, and doesn't solve\n   the VCmax limitation associated with a node terminating\
    \ too many VCs.\n   A related observation can also be made that the number of\
    \ MARS\n   Clients in a Cluster may be limited by the memory constraints of the\n\
    \   MARS itself. It is required to keep state on all the groups that\n   every\
    \ one of its MARS Clients have joined. For a given memory limit,\n   the maximum\
    \ number of MARS Clients must drop if the average number of\n   groups joined\
    \ per Client rises. Depending on the level of group\n   memberships, this limitation\
    \ may be more severe than LEAFmax.\n"
- title: 3. Signaling load.
  contents:
  - "3. Signaling load.\n   In any given cluster there will be an 'ambient' level\
    \ of\n   MARS_JOIN/LEAVE activity. The dynamic characteristics of this\n   activity\
    \ will depend on the types of multicast applications running\n   within the cluster.\
    \ For a constant relative distribution of multicast\n   applications we can assume\
    \ that, as the number of MARS Clients in a\n   given cluster rises, so does the\
    \ ambient level of MARS_JOIN/LEAVE\n   activity. This increases the average frequency\
    \ with which the MARS\n   processes and propagates MARS_JOIN/LEAVE messages.\n\
    \   The existence of MARS_JOIN/LEAVE traffic also has a consequential\n   impact\
    \ on signaling activity at the ATM level (across the UNI and\n   {P}NNI boundaries).\
    \ For groups that are VC Mesh supported, each\n   MARS_JOIN or MARS_LEAVE propagated\
    \ on ClusterControlVC will result in\n   an ADD_PARTY or DROP_PARTY message sent\
    \ across the UNIs of all MARS\n   Clients that are transmitting to a given group.\
    \ As a cluster's\n   membership increases, so does the average number of MARS\
    \ Clients that\n   trigger ATM signaling activity in response to MARS_JOIN/LEAVEs.\n\
    \   The size of a cluster needs to be chosen to provide some level of\n   containment\
    \ to this ambient level of MARS and UNI/NNI signaling.\n   Some refinements to\
    \ the MARS Client behaviour may also be explored to\n   smooth out UNI signaling\
    \ transients. MARS Clients are currently\n   required to initiate revalidation\
    \ of group memberships only when the\n   Client next sends a packet to an invalidated\
    \ group SVC. A Client\n   could apply a similar algorithm to decide when it should\
    \ issue\n   ADD_PARTYs. For example, after seeing a MARS_JOIN, wait until it\n\
    \   actually has a packet to send, send the packet, then initiate the\n   ADD_PARTY.\
    \ As a result actively transmitting Clients would update\n   their SVCs sooner\
    \ than intermittently transmitting Clients.\n"
- title: 4. Group change latencies
  contents:
  - "4. Group change latencies\n   The group change latency can be defined as the\
    \ time it takes for all\n   the senders to a group to have correctly updated their\
    \ forwarding\n   SVCs after a MARS_JOIN or MARS_LEAVE is received from the MARS.\
    \ This\n   is affected by both the number of Cluster members and the\n   geographical\
    \ distribution of Cluster members. (Groups that are MCS\n   based create the lowest\
    \ impact when new members join or leave, since\n   only the MCS needs to update\
    \ its forwarding SVC.) Under some\n   circumstances, especially modelling or simulation\
    \ environments, group\n   change latencies within a cluster may be an important\
    \ characteristic\n   to control.\n   As noted in the previous section, the ADD_PARTY/DROP_PARTY\
    \ signaling\n   load created by membership changes in VC Mesh based groups goes\
    \ up as\n   the number of cluster members rises (assuming worst case scenario\
    \ of\n   each cluster member being a sender to the group). As the UNI load\n \
    \  rises, the ATM network itself may start delivering slower processing\n   of\
    \ the requested events.\n   Wide geographic distribution of Cluster members also\
    \ delays the\n   propagation of MARS_JOIN/LEAVE and ATM UNI/NNI messages. The\
    \ further\n   apart various members are, the longer it takes for them to receive\n\
    \   MARS_JOIN/LEAVE traffic on ClusterControlVC, and the longer it takes\n   for\
    \ the ATM network to react to ADD_PARTY and DROP_PARTY requests. If\n   the long\
    \ distance paths are populated by many ATM switches,\n   propagation delays due\
    \ to per-switch processing will add\n   substantially to delays due to the speed\
    \ of light.\n   (Unfortunately, mechanisms for smoothing out the transient ATM\n\
    \   signaling load described in section 3 have a consequence of\n   increasing\
    \ the group change latency, since the goal is for some of\n   the senders to deliberately\
    \ delay updating their forwarding SVCs.\n   This is an area where the system architect\
    \ needs to make a\n   situation-specific trade-off.)\n   It is not clear what\
    \ affect the internal processing of the MARS\n   itself has on group change latency,\
    \ and how this might be impacted by\n   cluster size.  A component of the MARS\
    \ processing latency will depend\n   on the specific database implementation and\
    \ search algorithms as much\n   as on the number of group members for the group\
    \ being modified at any\n   instant. Since the maximum number of group members\
    \ for a given group\n   is equal to the number of cluster members, there will\
    \ be an indirect\n   (even if small) relationship between worst case MARS processing\n\
    \   latencies and cluster size.\n"
- title: 5. Large IP/ATM networks using Mrouters
  contents:
  - "5. Large IP/ATM networks using Mrouters\n   Building a large scale, multicast\
    \ capable IP over ATM network is a\n   tradeoff between Cluster sizes and numbers\
    \ of Mrouters. For a given\n   number of hosts, the number of clusters goes up\
    \ as individual\n   clusters shrink. Since Mrouters are the topological intersections\n\
    \   between clusters, the number of Mrouters rises as the size of\n   individual\
    \ clusters shrinks. (The actual number of Mrouters depends\n   largely on the\
    \ logical IP topology you choose to implement, since a\n   single physical Mrouter\
    \ may interconnect more than two Clusters at\n   once.) It is a local deployment\
    \ question as to what the optimal mix\n   of Clusters and Mrouters will be.\n\
    \   Currently two broad classes of Mrouters may be identified:\n      Those that\
    \ originate unique VCs into target Clusters, and\n      forward/interleave data\
    \ at the IP packet level (the Conventional\n      Mrouter).\n      Those that\
    \ originate unique VCs into target Clusters, but create\n      internal, cell\
    \ level 'cut through' paths between VCs from\n      different Clusters (e.g. the\
    \ Cell Switch Router).\n   How these Mrouters establish and manage the associations\
    \ of VCs to IP\n   traffic flows is beyond the scope of this document.  However,\
    \ it is\n   worth looking briefly at their impact on VC consumption and ATM\n\
    \   signaling load.\n"
- title: 5.1 Impact of the Conventional Mrouter
  contents:
  - "5.1 Impact of the Conventional Mrouter\n   A conventional Mrouter acts as an\
    \ aggregation point for both\n   signaling and data plane loads. It hides host\
    \ specific group\n   membership changes in one cluster from senders within other\
    \ clusters,\n   and protects group members (receivers) in one cluster from having\
    \ to\n   be leaf nodes on SVCs from senders in other Clusters.\n   When acting\
    \ as an ingress point into a cluster, a conventional\n   Mrouter establishes a\
    \ single forwarding SVC for IP packets.  This\n   single SVC carries data from\
    \ other clusters interleaved at the IP\n   packet level. Only this single SVC\
    \ needs to be modified in response\n   to group memberships changes within the\
    \ target cluster.  As a\n   consequence, there is no need for sources in other\
    \ clusters to be\n   aware of, or react to, MARS_JOIN/LEAVE traffic in the target\
    \ cluster.\n   (The consequential UNI signaling load identified in section 3 is\
    \ also\n   localized within the target Cluster.)\n   MARS Clients within the target\
    \ cluster also benefit from this data\n   path aggregation because they terminate\
    \ only one SVC from the Mrouter\n   (per group), rather than multiple SVCs originating\
    \ from actual\n   senders in other Clusters.\n   Conventional Mrouters help control\
    \ the limiting factors described in\n   sections 2, 3, and 4.  A hypothetical\
    \ 10000 node Cluster could be\n   broken into two 5000 node Clusters, or four\
    \ 2500 node Clusters, etc,\n   to reduce VC consumption. Or you might have 200\
    \ nodes of the overall\n   10000 that are known to join and leave groups rapidly,\
    \ whilst the\n   other 9800 are fairly steady - so you deploy clusters of 200,\
    \ 2500,\n   2500, 2500, 2300 hosts respectively.\n"
- title: 5.2. Impact of the Cell Switch Router (CSR).
  contents:
  - "5.2. Impact of the Cell Switch Router (CSR).\n   Another class of Mrouter, the\
    \ Cell Switch Router (CSR) attempts to\n   utilize IP level flow information to\
    \ dynamically manage the switching\n   of data through the device below the IP\
    \ level.  Once the CSR has\n   identified a flow of IP traffic, and associated\
    \ it with an inbound\n   and outbound SVC, it begins to function as an ATM cell\
    \ level device\n   rather than a packet level device.\n   Even when operating\
    \ in this mode the CSR isolates attached Clusters\n   from each other's MARS_JOIN/LEAVE\
    \ activities, in the same manner as a\n   conventional Mrouter. This occurs because\
    \ the CSR manages its\n   forwarding SVCs just like a normal MARS Client - responding\
    \ to\n   MARS_JOIN/LEAVE messages within the target cluster by updating the\n\
    \   pt-mpt trees rooted on its own ATM ports.\n   However, since AAL5 AAL_SDUs\
    \ cannot be interleaved at the cell level\n   on a single SVC, a CSR cannot simultaneously\
    \ perform cell level cut-\n   through and aggregate the IP packet flows from multiple\
    \ senders onto\n   a single SVC into a target Cluster. As a result, the CSR must\n\
    \   construct a separate forwarding SVC into a target cluster for each\n   SVC\
    \ it is a leaf of in a source Cluster (to to ensure that cells from\n   individual\
    \ sources are not interleaved prior to reaching the re-\n   assembly engines of\
    \ the group members in the target cluster).\n   Interestingly, the UNI signaling\
    \ load offered within the target\n   Cluster by the CSR is potentially greater\
    \ than that of a conventional\n   Mrouter. If there are N senders in the source\
    \ Cluster, the CSR will\n   have built N identical pt-mpt SVCs out to the group\
    \ members within\n   the target Cluster. If a new MARS_JOIN is issued within the\
    \ target\n   Cluster, the CSR must issue N ADD_PARTYs to update the N SVCs into\n\
    \   the target Cluster. (Under similar circumstances a conventional\n   Mrouter\
    \ would have issued only one ADD_PARTY for its single SVC into\n   the target\
    \ Cluster.)\n   Thus, without the ability to provide internal cut-through forwarding\n\
    \   with AAL_SDU boundaries intact, the CSR only provides for the\n   isolation\
    \ of MARS_JOIN/LEAVE traffic within clusters. It cannot\n   provide the data path\
    \ aggregation of a conventional Mrouter.\n"
- title: 6. The impact of Multicast Servers (MCSs)
  contents:
  - "6. The impact of Multicast Servers (MCSs)\n   Since the focus of this document\
    \ is on worst-case scenarios, most of\n   the analysis has assumed multicast groups\
    \ that are VC Mesh based and\n   have all cluster members as sources and receivers.\
    \ The impact of\n   using an MCS to support a multicast group can be dramatic\
    \ in the\n   context of the group's resource consumption, but less so in the\n\
    \   over-all context of cluster size limits.\n   The intra-cluster, per group\
    \ impact of an MCS is somewhat analogous\n   to the inter-cluster impact of a\
    \ conventional Mrouter. The MCS\n   aggregates the data flows (only 1 SVC terminates\
    \ on each group\n   member, independent of the number of senders), and isolates\n\
    \   MARS_JOIN/LEAVE traffic (which is shifted to ServerControlVC rather\n   than\
    \ ClusterControlVC). The resulting UNI signaling traffic and load\n   is reduced\
    \ too, as only the forwarding SVC out of the MCS needs to be\n   modified for\
    \ every membership change in the MCS supported group.\n   Deploying a mixture\
    \ of MCS and VC Mesh based groups will certainly\n   improve resource utilization.\
    \ However, the actual extent of the\n   improvements (and consequently how large\
    \ the cluster can be made)\n   will depend greatly on the dynamics of your typical\
    \ applications and\n   which characteristics from sections 2, 3, and 4 are your\
    \ primary\n   limitations.\n   For example, if VCmax or LEAFmax (section 2) are\
    \ primary limitations,\n   one must keep in mind that each MCS itself suffers\
    \ the same NIC\n   limits as the MARS and MARS Clients. Even though using an MCS\n\
    \   dramatically reduces the number of VCs per MARS Client per group,\n   each\
    \ MCS still needs to terminate 1 SVC per sender - potentially up\n   to 1 SVC\
    \ from each Cluster member.  (This may become 1 SVC per member\n   per group if\
    \ the MCS supports multiple groups simultaneously.)\n   Assume we have a Cluster\
    \ where every group is MCS based, each MCS\n   supports only one group, and both\
    \ VCmax and LEAFmax apply equally to\n   MCS nodes as MARS and MARS Clients nodes.\
    \  If we have N cluster\n   members, M groups, and all receivers are senders for\
    \ a given MCS\n   supported group, the following observations may be made:\n \
    \     Each MCS forwarding SVC has N leaf nodes, so\n            N <= LEAFmax.\n\
    \      Each MCS terminates an SVC from N senders, originates 1 SVC\n      forwarding\
    \ path, originates a pt-pt control SVC to the MARS, and\n      terminates 1 SVC\
    \ as a leaf on ServerControlVC, so\n            N + 3 <= VCmax.\n      MARS ClusterControlVC\
    \ has N leaf nodes, so\n            N <= LEAFmax.\n      MARS ServerControlVC\
    \ has M leaf nodes, so\n            M <= LEAFmax.\n      The MARS terminates a\
    \ pt-pt VC from each cluster member, a pt-pt\n      VC from each MCS, originates\
    \ ClusterControlVC, and originates\n      ServerControlVC, so\n            N +\
    \ M + 2 <= VCmax.\n      Each Cluster Member sources 1 VC per group, terminates\
    \ 1 VC per\n      group, originates a pt-pt VC to the MARS, and terminates 1 VC\
    \ as a\n      leaf on ClusterControlVC, so\n            2*M + 2 <= VCmax.\n  \
    \ Since all the above conditions must be simultaneously true, we can\n   see that\
    \ the most constraining requirements are:\n      N + M + 2 <= VCmax (if M <= N)\n\
    \      2*M + 2 <= VCmax (if M >= N)\n   or\n      N <= LEAFmax.\n   (Assuming\
    \ that in general M+2 > 3, so the VCmax constraint at each\n   MCS is not a limiting\
    \ factor.)\n   We can get a feel for the relative impacts of VC Mesh groups vs\
    \ MCS\n   based groups by considering a cluster where M1 represents the number\n\
    \   of VC Mesh based groups, and M2 represents the number of MCS based\n   groups.\
    \ Again we assume worst case group density (all N cluster\n   members are group\
    \ members, all receivers are also senders).\n   As noted in section 2, the VCmax\
    \ constraint in VC Mesh mode comes\n   from each MARS Client, and is:\n      N*M1\
    \ <= VCmax - 2\n   For the MCS case we have two scenarios, M2 <= N and M2 >= N.\n\
    \   If M2 <= N we can see the VC consumption by VC Mesh based groups will\n  \
    \ become the applicable constraint on cluster size N when:\n      N + M2 <= N*M1\n\
    \   i.e.\n      M1 >= 1 + (M2/N)\n   Thus, if there is more than 1 VC Mesh based\
    \ group, and less MCS based\n   groups than cluster members (M2 < N), the constraint\
    \ on cluster size\n   is dictated by the VC Mesh characteristics: N*M1 <= VCmax\
    \ - 2. (If M2\n   == N, then there may be 2 VC Mesh based groups before the VC\
    \ Mesh\n   characteristics are the dictating factor.)\n   Now, if M2 > N (more\
    \ MCS based groups, and hence MCSes, than cluster\n   members) the calculation\
    \ is more complex since in this case VCmax at\n   the MARS Client is the limiting\
    \ parameter for both VC Mesh and MCS\n   cases. The limit becomes:\n      N*M1\
    \ + 2*M2 <= VCmax - 2\n   However, on face value this is an odd situation anyway,\
    \ since it\n   implies more MCS entities than hosts or router interfaces into\
    \ the\n   cluster (given the assumption of one group per MCS).\n   The impact\
    \ of MCS entities that simultaneously support multiple\n   groups is left for\
    \ future study.\n"
- title: 7. Open Issues
  contents:
  - "7. Open Issues\n   There is a wide range of qualitative analysis that can be\
    \ extracted\n   from typical MARS deployment scenarios. This document does not\n\
    \   attempt to develop any numerical models for VC consumptions, end to\n   end\
    \ latencies, etc.\n"
- title: 8. Conclusion
  contents:
  - "8. Conclusion\n   This document has provided a high level, qualitative overview\
    \ of the\n   parameters affecting the size of MARS Clusters.  Limitations on the\n\
    \   number of leaf nodes a pt-mpt SVC may support, sizes of the MARS\n   database,\
    \ propagation delays of MARS and UNI messages, and the\n   frequency of MARS and\
    \ UNI control messages are all identified as\n   issues that will constrain Clusters.\
    \  Conventional Mrouters are\n   identified as useful aggregators of IP multicast\
    \ traffic and\n   signaling information.  Cell Switch Routers are noted to offer\
    \ only\n   some of the aggregation attributes of conventional Mrouters.  Large\n\
    \   scale IP multicasting over ATM requires a combination of Mrouters and\n  \
    \ appropriately sized MARS Clusters. Finally, it has been shown that in\n   a\
    \ simple cluster where there are less MCS based groups than cluster\n   members,\
    \ two or more VC Mesh based groups are sufficient to render\n   the use of Multicast\
    \ Servers irrelevant to the worst case cluster\n   size limit.\n"
- title: Security Considerations
  contents:
  - "Security Considerations\n   Security issues are not discussed in this memo.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   Thanks must go to Rajesh Talpade (Georgia Tech) for specific\
    \ input on\n   aspects of the VC Mesh vs MCS tradeoffs, and Joel Halpern (Newbridge)\n\
    \   for general input on the document's focus.\n"
- title: Author's Address
  contents:
  - "Author's Address\n   Grenville Armitage\n   Bellcore, 445 South Street\n   Morristown,\
    \ NJ, 07960\n   USA\n   EMail: gja@thumper.bellcore.com\n   Phone +1 201 829 2635\n"
- title: References
  contents:
  - "References\n   [1] Armitage, G., \"Support for Multicast over UNI 3.0/3.1 based\
    \ ATM\n   Networks.\", Bellcore, RFC 2022, November 1996.\n"
