- title: __initial_text__
  contents:
  - "           Label Switched Path (LSP) Preemption Policies for\n              \
    \          MPLS Traffic Engineering\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This memo provides information for the Internet community.\
    \  It does\n   not specify an Internet standard of any kind.  Distribution of\
    \ this\n   memo is unlimited.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (C) The IETF Trust (2007).\n"
- title: IESG Note
  contents:
  - "IESG Note\n   This RFC is not a candidate for any level of Internet Standard.\
    \  The\n   IETF disclaims any knowledge of the fitness of this RFC for any\n \
    \  purpose and, in particular, notes that the decision to publish is not\n   based\
    \ on IETF review for such things as security, congestion control,\n   or inappropriate\
    \ interaction with deployed protocols.  The RFC Editor\n   has chosen to publish\
    \ this document at its discretion.  Readers of\n   this document should exercise\
    \ caution in evaluating its value for\n   implementation and deployment.  See\
    \ RFC 3932 for more information.\n"
- title: Abstract
  contents:
  - "Abstract\n   When the establishment of a higher priority (Traffic Engineering\n\
    \   Label Switched Path) TE LSP requires the preemption of a set of lower\n  \
    \ priority TE LSPs, a node has to make a local decision to select which\n   TE\
    \ LSPs will be preempted.  The preempted LSPs are then rerouted by\n   their respective\
    \ Head-end Label Switch Router (LSR).  This document\n   presents a flexible policy\
    \ that can be used to achieve different\n   objectives: preempt the lowest priority\
    \ LSPs; preempt the minimum\n   number of LSPs; preempt the set of TE LSPs that\
    \ provide the closest\n   amount of bandwidth to the required bandwidth for the\
    \ preempting TE\n   LSPs (to minimize bandwidth wastage); preempt the LSPs that\
    \ will have\n   the maximum chance to get rerouted.  Simulation results are given\
    \ and\n   a comparison among several different policies, with respect to\n   preemption\
    \ cascading, number of preempted LSPs, priority, wasted\n   bandwidth and blocking\
    \ probability is also included.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1.  Motivation . . . . . . . . . . . . . . . . . . . .\
    \ . . . . . .  3\n   2.  Introduction . . . . . . . . . . . . . . . . . . . .\
    \ . . . . .  3\n   3.  LSP Setup Procedure and Preemption . . . . . . . . . .\
    \ . . . .  5\n   4.  Preemption Cascading . . . . . . . . . . . . . . . . . .\
    \ . . .  6\n   5.  Preemption Heuristic . . . . . . . . . . . . . . . . . . .\
    \ . .  7\n     5.1.  Preempting Resources on a Path . . . . . . . . . . . . .\
    \ .  7\n     5.2.  Preemption Heuristic Algorithm . . . . . . . . . . . . . .\
    \  8\n   6.  Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\
    \     6.1.  Simple Case: Single Link . . . . . . . . . . . . . . . . . 10\n  \
    \   6.2.  Network Case . . . . . . . . . . . . . . . . . . . . . . . 12\n   7.\
    \  Security Considerations  . . . . . . . . . . . . . . . . . . . 16\n   8.  Acknowledgements\
    \ . . . . . . . . . . . . . . . . . . . . . . . 16\n   9.  Informative References\
    \ . . . . . . . . . . . . . . . . . . . . 17\n"
- title: 1.  Motivation
  contents:
  - "1.  Motivation\n   The IETF Traffic Engineering Working Group has defined the\n\
    \   requirements and protocol extensions for DiffServ-aware MPLS Traffic\n   Engineering\
    \ (DS-TE) [RFC3564] [RFC4124].  Several Bandwidth\n   Constraint models for use\
    \ with DS-TE have been proposed [RFC4127]\n   [RFC4128] [RFC4126] and their performance\
    \ was analyzed with respect\n   to the use of preemption.\n   Preemption can be\
    \ used as a tool to help ensure that high priority\n   LSPs can always be routed\
    \ through relatively favorable paths.\n   Preemption can also be used to implement\
    \ various prioritized access\n   policies as well as restoration policies following\
    \ fault events\n   [RFC2702].\n   Although not a mandatory attribute in the traditional\
    \ IP world,\n   preemption becomes important in networks using online, distributed\n\
    \   Constrained Shortest Path First (CSPF) strategies for their Traffic\n   Engineering\
    \ Label Switched Path (TE LSP) path computation to limit\n   the impact of bandwidth\
    \ fragmentation.  Moreover, preemption is an\n   attractive strategy in an MPLS\
    \ network in which traffic is treated in\n   a differentiated manner and high-importance\
    \ traffic may be given\n   special treatment over lower-importance traffic [DEC-PREP,\
    \ ATM-PREP].\n   Nevertheless, in the DS-TE approach, whose issues and requirements\n\
    \   are discussed in [RFC3564], the preemption policy is considered an\n   important\
    \ piece on the bandwidth reservation and management puzzle,\n   but no preemption\
    \ strategy is defined.  Note that preemption also\n   plays an important role\
    \ in regular MPLS Traffic Engineering\n   environments (with a single pool of\
    \ bandwidth).\n   This document proposes a flexible preemption policy that can\
    \ be\n   adjusted in order to give different weight to various preemption\n  \
    \ criteria: priority of LSPs to be preempted, number of LSPs to be\n   preempted,\
    \ amount of bandwidth preempted, blocking probability.  The\n   implications (cascading\
    \ effect, bandwidth wastage, priority of\n   preempted LSPs) of selecting a certain\
    \ order of importance for the\n   criteria are discussed for the examples given.\n"
- title: 2.  Introduction
  contents:
  - "2.  Introduction\n   In [RFC2702], issues and requirements for Traffic Engineering\
    \ in an\n   MPLS network are highlighted.  In order to address both traffic-\n\
    \   oriented and resource-oriented performance objectives, the authors\n   point\
    \ out the need for priority and preemption parameters as Traffic\n   Engineering\
    \ attributes of traffic trunks.  The notion of preemption\n   and preemption priority\
    \ is defined in [RFC3272], and preemption\n   attributes are defined in [RFC2702]\
    \ and [RFC3209].\n   A traffic trunk is defined as an aggregate of traffic flows\
    \ belonging\n   to the same class that are placed inside an LSP [RFC3564].  In\
    \ this\n   context, preemption is the act of selecting an LSP that will be\n \
    \  removed from a given path in order to give room to another LSP with a\n   higher\
    \ priority (lower preemption number).  More specifically, the\n   preemption attributes\
    \ determine whether an LSP with a certain setup\n   preemption priority can preempt\
    \ another LSP with a lower holding\n   preemption priority from a given path,\
    \ when there is competition for\n   available resources.  Note that competing\
    \ for resources is one\n   situation in which preemption can be triggered, but\
    \ other situations\n   may exist, themselves controlled by a policy.\n   For readability,\
    \ a number of definitions from [RFC3564] are repeated\n   here:\n   Class-Type\
    \ (CT): The set of Traffic Trunks crossing a link that is\n   governed by a specific\
    \ set of Bandwidth constraints.  CT is used for\n   the purposes of link bandwidth\
    \ allocation, constraint-based routing,\n   and admission control.  A given Traffic\
    \ Trunk belongs to the same CT\n   on all links.\n   TE-Class: A pair of:\n  \
    \ i.  A Class-Type.\n   ii.  A preemption priority allowed for that Class-Type.\
    \  This means\n   that an LSP transporting a Traffic Trunk from that Class-Type\
    \ can use\n   that preemption priority as the set-up priority, as the holding\n\
    \   priority, or both.\n   By definition, there may be more than one TE-Class\
    \ using the same CT,\n   as long as each TE-Class uses a different preemption\
    \ priority.  Also,\n   there may be more than one TE-Class with the same preemption\n\
    \   priority, provided that each TE-Class uses a different CT.  The\n   network\
    \ administrator may define the TE-Classes in order to support\n   preemption across\
    \ CTs, to avoid preemption within a certain CT, or to\n   avoid preemption completely,\
    \ when so desired.  To ensure coherent\n   operation, the same TE-Classes must\
    \ be configured in every Label\n   Switched Router (LSR) in the DS-TE domain.\n\
    \   As a consequence of a per-TE-Class treatment, the Interior Gateway\n   Protocol\
    \ (IGP) needs to advertise separate Traffic Engineering\n   information for each\
    \ TE-Class, which consists of the Unreserved\n   Bandwidth (UB) information [RFC4124].\
    \  The UB information will be\n   used by the routers, checking against the bandwidth\
    \ constraint model\n   parameters, to decide whether preemption is needed.  Details\
    \ on how\n   to calculate the UB are given in [RFC4124].\n"
- title: 3.  LSP Setup Procedure and Preemption
  contents:
  - "3.  LSP Setup Procedure and Preemption\n   A new LSP setup request has two important\
    \ parameters: bandwidth and\n   preemption priority.  The set of LSPs to be preempted\
    \ can be selected\n   by optimizing an objective function that represents these\
    \ two\n   parameters, and the number of LSPs to be preempted.  More\n   specifically,\
    \ the objective function could be any, or a combination,\n   of the following\
    \ [DEC-PREP, ATM-PREP]:\n   * Preempt the LSPs that have the least priority (preemption\n\
    \     priority).  The Quality of Service (QoS) of high priority traffic\n    \
    \ would be better satisfied, and the cascading effect described below\n     can\
    \ be limited.\n   * Preempt the least number of LSPs.  The number of LSPs that\
    \ need to\n     be rerouted would be lower.\n   * Preempt the least amount of\
    \ bandwidth that still satisfies the\n     request.  Resource utilization could\
    \ be improved.  The preemption\n     of larger TE LSPs (more than requested) by\
    \ the newly signaled TE\n     LSP implies a larger amount of bandwidth to be rerouted,\
    \ which is\n     likely to increase the probability of blocking (inability to\
    \ find a\n     path for some TE LSPs).\n   * Preempt LSPs that minimize the blocking\
    \ probability (risk that\n     preempted TE LSP cannot be rerouted).\n   After\
    \ the preemption selection phase is finished, the selected LSPs\n   are signaled\
    \ as preempted and the new LSP is established (if a new\n   path satisfying the\
    \ constraints can be found).  The UB information is\n   then updated via flooding\
    \ of an IGP-TE update and/or simply pruning\n   the link where preemption occurred.\
    \  Figure 1 shows a flowchart that\n   summarizes how each LSP setup request is\
    \ treated in a preemption-\n   enabled scenario.\n      LSP Setup Request\n  \
    \   (TE-Class i, bw=r)\n               |\n               |\n               v \
    \              NO\n     UB[TE-Class i] >= r ? -------> Reject LSP\n          \
    \                          Setup and flood an updated IGP-TE\n               |\
    \                    LSA/LSP\n               |YES\n               v          \
    \    NO\n      Preemption Needed ? -------> Setup LSP/Update UB if a threshold\
    \ is\n               |                   crossed\n               | YES\n     \
    \          v\n           Preemption   ---->    Setup LSP/Reroute Preempted LSPs\n\
    \           Algorithm             Update UB\n   Figure 1: Flowchart for LSP setup\
    \ procedure.\n   In [DEC-PREP], the authors propose connection preemption policies\n\
    \   that optimize the discussed criteria in a given order of importance:\n   number\
    \ of LSPs, bandwidth, and priority; bandwidth, priority, and\n   number of LSPs.\
    \  The novelty in our approach is the use of an\n   objective function that can\
    \ be adjusted by the service provider in\n   order to stress the desired criteria.\
    \  No particular criteria order\n   is enforced.  Moreover, a new criterion is\
    \ added to the objective\n   function: optimize the blocking probability (to minimize\
    \ the risk\n   that an LSP is not rerouted after being preempted).\n"
- title: 4.  Preemption Cascading
  contents:
  - "4.  Preemption Cascading\n   The decision of preempting an LSP may cause other\
    \ preemptions in the\n   network.  This is called preemption cascading effect\
    \ and different\n   cascading levels may be achieved by the preemption of a single\
    \ LSP.\n   The cascading levels are defined in the following manner: when an LSP\n\
    \   is preempted and rerouted without causing any further preemption, the\n  \
    \ cascading is said to be of level zero.  However, when a preempted LSP\n   is\
    \ rerouted, and in order to be established in the new route it also\n   causes\
    \ the preemption of other LSPs, the cascading is said to be of\n   level 1, and\
    \ so on.\n   Preemption cascading is not desirable and therefore policies that\n\
    \   minimize it are of interest.  Typically, this can result in severe\n   network\
    \ instabilities.  In Section 5, a new versatile preemption\n   heuristic will\
    \ be presented.  In Section 6, preemption simulation\n   results will be discussed\
    \ and the cascading effect will be analyzed.\n"
- title: 5.  Preemption Heuristic
  contents:
  - '5.  Preemption Heuristic

    '
- title: 5.1.  Preempting Resources on a Path
  contents:
  - "5.1.  Preempting Resources on a Path\n   It is important to note that once a\
    \ request for an LSP setup arrives,\n   each LSR along the TE LSP path checks\
    \ the available bandwidth on its\n   outgoing link.  For the links in which the\
    \ available bandwidth is not\n   enough, the preemption policy needs to be activated\
    \ in order to\n   guarantee the end-to-end bandwidth reservation for the new LSP.\
    \  This\n   is a distributed approach, in which every node on the path is\n  \
    \ responsible for running the preemption algorithm and determining\n   which LSPs\
    \ would be preempted in order to fit the new request.  A\n   distributed approach\
    \ may not lead to an optimal solution.\n   Alternatively, in a centralized approach,\
    \ a manager entity runs the\n   preemption policy and determines the best LSPs\
    \ to be preempted in\n   order to free the required bandwidth in all the links\
    \ that compose\n   the path.  The preemption policy would try to select LSPs that\n\
    \   overlap with the path being considered (preempt a single LSP that\n   overlaps\
    \ with the route versus preempt a single LSP on every link\n   that belongs to\
    \ the route).\n   Both centralized and distributed approaches have advantages\
    \ and\n   drawbacks.  A centralized approach would be more precise, but require\n\
    \   that the whole network state be stored and updated accordingly, which\n  \
    \ raises scalability issues.  In a network where LSPs are mostly\n   static, an\
    \ offline decision can be made to reroute LSPs and the\n   centralized approach\
    \ could be appropriate.  However, in a dynamic\n   network in which LSPs are set\
    \ up and torn down in a frequent manner\n   because of new TE LSPs, bandwidth\
    \ increase, reroute due to failure,\n   etc., the correctness of the stored network\
    \ state could be\n   questionable.  Moreover, the setup time is generally increased\
    \ when\n   compared to a distributed solution.  In this scenario, the\n   distributed\
    \ approach would bring more benefits, even when resulting\n   in a non-optimal\
    \ solution (The gain in optimality of a centralized\n   approach compared to a\
    \ distributed approach depends on many factors:\n   network topology, traffic\
    \ matrix, TE strategy, etc.).  A distributed\n   approach is also easier to be\
    \ implemented due to the distributed\n   nature of the current Internet protocols.\n\
    \   Since the current Internet routing protocols are essentially\n   distributed,\
    \ a decentralized approach was selected for the LSP\n   preemption policy.  The\
    \ parameters required by the new preemption\n   policies are currently available\
    \ for OSPF and Intermediate System to\n   Intermediate System (IS-IS).\n"
- title: 5.2.  Preemption Heuristic Algorithm
  contents:
  - "5.2.  Preemption Heuristic Algorithm\n   Consider a request for a new LSP setup\
    \ with bandwidth b and setup\n   preemption priority p.  When preemption is needed,\
    \ due to lack of\n   available resources, the preemptable LSPs will be chosen\
    \ among the\n   ones with lower holding preemption priority (higher numerical\
    \ value)\n   in order to fit r=b-Abw(l).  The variable r represents the actual\n\
    \   bandwidth that needs to be preempted (the requested, b, minus the\n   available\
    \ bandwidth on link l: Abw(l)).\n   L is the set of active LSPs having a holding\
    \ preemption priority\n   lower (numerically higher) than p.  So L is the set\
    \ of candidates for\n   preemption. b(l) is the bandwidth reserved by LSP l in\
    \ L, expressed\n   in bandwidth units, and p(l) is the holding preemption priority\
    \ of\n   LSP l.\n   In order to represent a cost for each preemption priority,\
    \ an\n   associated cost y(l) inversely related to the holding preemption\n  \
    \ priority p(l) is defined.  For simplicity, a linear relation\n   y(l)=8-p(l)\
    \ is chosen. y is a cost vector with L components, y(l). b\n   is a reserved bandwidth\
    \ vector with dimension L, and components b(l).\n   Concerning the objective function,\
    \ four main objectives can be\n   reached in the selection of preempted LSPs:\n\
    \   * minimize the priority of preempted LSPs,\n   * minimize the number of preempted\
    \ LSPs,\n   * minimize the preempted bandwidth,\n   * minimize the blocking probability.\n\
    \   To have the widest choice on the overall objective that each service\n   provider\
    \ needs to achieve, the following equation was defined (for\n   simplicity chosen\
    \ as a weighted sum of the above mentioned criteria):\n   H(l)= alpha y(l) + beta\
    \ 1/b(l) + gamma (b(l)-r)^2 + theta b(l)\n   In this equation:\n   - alpha y(l)\
    \ captures the cost of preempting high priority LSPs.\n   - beta 1/b(l) penalizes\
    \ the preemption of low bandwidth LSPs,\n     capturing the cost of preempting\
    \ a large number of LSPs.\n   - gamma (b(l)-r)^2 captures the cost of preemption\
    \ of LSPs that are\n     much larger or much smaller than r.\n   - theta b(l)\
    \ captures the cost of preempting large LSPs.\n   Coefficients alpha, beta, gamma,\
    \ and theta can be chosen to emphasize\n   one or more components of H.\n   The\
    \ coefficient theta is defined such that theta = 0 if gamma > 0.\n   This is because\
    \ when trying to minimize the blocking probability of\n   preempted LSPs, the\
    \ heuristic gives preference to preempting several\n   small LSPs (therefore gamma,\
    \ which is the weight for minimizing the\n   preempted bandwidth enforcing the\
    \ selection of LSPs with similar\n   amount of bandwidth as the requested, needs\
    \ to be set as zero).  The\n   selection of several small LSPs in a normally loaded\
    \ portion of the\n   network will increase the chance that such LSPs are successfully\n\
    \   rerouted.  Moreover, the selection of several small LSPs may not\n   imply\
    \ preempting much more than the required bandwidth (resulting in\n   low-bandwidth\
    \ wastage), as it will be seen in the discussed examples.\n   When preemption\
    \ is to happen in a heavy loaded portion of the\n   network, to minimize blocking\
    \ probability, the heuristic will select\n   fewer LSPs for preemption in order\
    \ to increase the chance of\n   rerouting.\n   H is calculated for each LSP in\
    \ L. The LSPs to be preempted are\n   chosen as the ones with smaller H that add\
    \ enough bandwidth to\n   accommodate r.  When sorting LSPs by H, LSPs with the\
    \ same value for\n   H are ordered by bandwidth b, in increasing order.  For each\
    \ LSP with\n   repeated H, the algorithm checks whether the bandwidth b assigned\
    \ to\n   only that LSP is enough to satisfy r.  If there is no such LSP, it\n\
    \   checks whether the bandwidth of each of those LSPs added to the\n   previously\
    \ preempted LSPs' bandwidth is enough to satisfy r.  If that\n   is not true for\
    \ any LSP in that repeated H-value sequence, the\n   algorithm preempts the LSP\
    \ that has the larger amount of bandwidth in\n   the sequence, and keeps preempting\
    \ in decreasing order of b until r\n   is satisfied or the sequence is finished.\
    \  If the sequence is\n   finished and r is not satisfied, the algorithm again\
    \ selects LSPs to\n   be preempted based on an increasing order of H. More details\
    \ on the\n   algorithm are given in [PREEMPTION].\n   When the objective is to\
    \ minimize blocking, the heuristic will follow\n   two options on how to calculate\
    \ H:\n   * If the link in which preemption is to happen is normally loaded,\n\
    \     several small LSPs will be selected for preemption using H(l)=\n     alpha\
    \ y(l) + theta b(l).\n   * If the link is overloaded, few LSPs are selected using\
    \ H(l)= alpha\n     y(l) + beta 1/b(l).\n"
- title: 6.  Examples
  contents:
  - '6.  Examples

    '
- title: '6.1.  Simple Case: Single Link'
  contents:
  - "6.1.  Simple Case: Single Link\n   We first consider a very simple case, in which\
    \ the path considered\n   for preemption is composed by a single hop.  The objective\
    \ of this\n   example is to illustrate how the heuristic works.  In the next\n\
    \   section, we will study a more complex case in which the preemption\n   policies\
    \ are being tested on a network.\n   Consider a link with 16 LSPs with reserved\
    \ bandwidth b in Mbps,\n   preemption holding priority p, and cost y, as shown\
    \ in Table 1.  In\n   this example, 8 TE-Classes are active.  The preemption here\
    \ is being\n   performed on a single link as an illustrative example.\n      ------------------------------------------------------------------\n\
    \      LSP                      L1   L2   L3   L4   L5   L6   L7   L8\n      ------------------------------------------------------------------\n\
    \      Bandwidth (b)            20   10   60   25   20    1   75   45\n      Priority\
    \  (p)             1    2    3    4    5    6    7    5\n      Cost      (y) \
    \            7    6    5    4    3    2    1    3\n      ------------------------------------------------------------------\n\
    \      LSP                      L9   L10  L11  L12  L13  L14  L15  L16\n     \
    \ ------------------------------------------------------------------\n      Bandwidth\
    \ (b)           100     5   40   85   50   20   70   25\n      Priority  (p) \
    \            3     6    4    5    2    3    4    7\n      Cost      (y)      \
    \       5     2    4    3    6    5    4    1\n      ------------------------------------------------------------------\n\
    \      Table 1: LSPs in the considered link.\n   A request for an LSP establishment\
    \ arrives with r=175 Mbps and p=0\n   (highest possible priority, which implies\
    \ that all LSPs with p>0 in\n   Table 1 will be considered when running the algorithm).\
    \  Assume\n   Abw(l)=0.\n   If priority is the only important criterion, the network\
    \ operator\n   configures alpha=1, beta=gamma=theta=0.  In this case, LSPs L6,\
    \ L7,\n   L10, L12, and L16 are selected for preemption, freeing 191 bandwidth\n\
    \   units to establish the high-priority LSP.  Note that 5 LSPs were\n   preempted,\
    \ but all with a priority level between 5 and 7.\n   In a network in which rerouting\
    \ is an expensive task to perform (and\n   the number of rerouted TE LSPs should\
    \ be as small as possible), one\n   might prefer to set beta=1 and alpha=gamma=theta=0.\
    \  LSPs L9 and L12\n   would then be selected for preemption, adding up to 185\
    \ bandwidth\n   units (less wastage than the previous case).  The priorities of\
    \ the\n   selected LSPs are 3 and 5 (which means that they might themselves\n\
    \   preempt some other LSPs when rerouted).\n   Suppose the network operator decides\
    \ that it is more appropriate to\n   configure alpha=1, beta=10, gamma=0, theta=0\
    \ (the parameters were set\n   to values that would balance the weight of each\
    \ component, namely\n   priority and number, in the cost function), because in\
    \ this network\n   rerouting is very expensive, LSP priority is important, but\
    \ bandwidth\n   is not a critical issue.  In this case, LSPs L7, L12, and L16\
    \ are\n   selected for preemption.  This configuration results in a smaller\n\
    \   number of preempted LSPs when compared to the first case, and the\n   priority\
    \ levels are kept between 5 and 7.\n   To take into account the number of LSPs\
    \ preempted, the preemption\n   priority, and the amount of bandwidth preempted,\
    \ the network operator\n   may set alpha > 0, beta > 0, and gamma > 0.  To achieve\
    \ a balance\n   among the three components, the parameters need to be normalized.\n\
    \   Aiming for a balance, the parameters could be set as alpha=1, beta=10\n  \
    \ (bringing the term 1/b(l) closer to the other parameters), and\n   gamma=0.001\
    \ (bringing the value of the term (b(l)-r)^2 closer to the\n   other parameters).\
    \  LSPs L7 and L9 are selected for preemption,\n   resulting in exactly 175 bandwidth\
    \ units and with priorities 3 and 7\n   (note that less LSP are preempted but\
    \ they have a higher priority\n   which may result in a cascading effect).\n \
    \  If the minimization of the blocking probability is the criterion of\n   most\
    \ interest, the cost function could be configured with theta=1,\n   alpha=beta=gamma=0.\
    \  In that case, several small LSPs are selected\n   for preemption: LSPs L2,\
    \ L4, L5, L6, L7, L10, L14, and L16.  Their\n   preemption will free 181 Mbps\
    \ in this link, and because the selected\n   LSPs have small bandwidth requirement\
    \ there is a good chance that\n   each of them will find a new route in the network.\n\
    \   From the above example, it can be observed that when the priority was\n  \
    \ the highest concern and the number of preempted LSPs was not an\n   issue, 5\
    \ LSPs with the lowest priority were selected for preemption.\n   When only the\
    \ number of LSPs was an issue, the minimum number of LSPs\n   was selected for\
    \ preemption: 2, but the priority was higher than in\n   the previous case.  When\
    \ priority and number were important factors\n   and a possible waste of bandwidth\
    \ was not an issue, 3 LSPs were\n   selected, adding more bandwidth than requested,\
    \ but still with low\n   preemption priority.  When considering all the parameters\
    \ but the\n   blocking probability, the smallest set of LSP was selected, 2, adding\n\
    \   just enough bandwidth, 175 Mbps, and with priority levels 3 and 7.\n   When\
    \ the blocking probability was the criterion of interest, several\n   (8) small\
    \ LSPs were preempted.  The bandwidth wastage is low, but the\n   number of rerouting\
    \ events will increase.  Given the bandwidth\n   requirement of the preempted\
    \ LSPs, it is expected that the chances of\n   finding a new route for each LSP\
    \ will be high.\n"
- title: 6.2.  Network Case
  contents:
  - "6.2.  Network Case\n   For these experiments, we consider a 150 nodes topology\
    \ with an\n   average network connectivity of 3. 10% of the nodes in the topology\n\
    \   have a degree of connectivity of 6. 10% of the links are OC3, 70% are\n  \
    \ OC48, and 20% are OC192.\n   Two classes of TE LSPs are in use: Voice LSPs and\
    \ Data Internet/VPN\n   LSPs.  For each class of TE LSP, the set of preemptions\
    \ (and the\n   proportion of LSPs for each preemption) and the size distributions\n\
    \   are as follows (a total of T LSPs is considered):\n   T: total number of TE\
    \ LSPs in the network (T = 18,306 LSPs)\n   Voice:\n   Number: 20% of T\n   Preemption:\
    \ 0, 1 and 2\n   Size: uniform distribution between 30M and 50M\n   Internet/VPN\
    \ TE:\n   Number: 4% of T\n   Preemption: 3\n   Size: uniform distribution between\
    \ 20M and 50M\n   Number: 8% of T\n   Preemption 4\n   Size: uniform distribution\
    \ between 15M and 40M\n   Number: 8% of T\n   Preemption 5\n   Size: uniform distribution\
    \ between 10M and 20M\n   Number: 20% of T\n   Preemption 6\n   Size: uniform\
    \ distribution between 1M and 20M\n   Number: 40% of T\n   Preemption 7\n   Size:\
    \ uniform distribution between 1K and 1M\n   LSPs are set up mainly due to network\
    \ failure: a link or a node\n   failed and LSPs are rerouted.\n   The network\
    \ failure events were simulated with two functions:\n   - Constant: 1 failure\
    \ chosen randomly among the set of links every 1\n     hour.\n   - Poisson process\
    \ with interarrival average = 1 hour.\n   Table 2 shows the results for simulations\
    \ with constant failure.  The\n   simulations were run with the preemption heuristic\
    \ configured to\n   balance different criteria (left side of the table), and then\
    \ with\n   different preemption policies that consider the criteria in a given\n\
    \   order of importance rather than balancing them (right side of the\n   table).\n\
    \   The proposed heuristic was configured to balance the following\n   criteria:\n\
    \   HPB: The heuristic with priority and bandwidth wastage as the most\n   important\
    \ criteria (alpha=10, beta=0, gamma=0.001, theta=0).\n   HBlock: The heuristic\
    \ considering the minimization of blocking\n   probability (normal load links:\
    \ alpha=1, beta=0, gamma=0, theta=0.01)\n   (heavy load links: alpha=1, beta=10).\n\
    \   HNB: The heuristic with number of preemptions and wasted bandwidth in\n  \
    \ consideration (alpha=0, beta=10, gamma=0.001, theta=0).\n   Other algorithms\
    \ that consider the criteria in a given order of\n   importance:\n   P: Sorts\
    \ candidate LSPs by priority only.\n   PN: Sorts the LSPs by priority, and for\
    \ cases in which the priority\n   is the same, orders those LSPs by decreasing\
    \ bandwidth (selects\n   larger LSPs for preemption in order to minimize number\
    \ of preempted\n   LSPs).\n   PB: Sorts the LSPs by priority, and for LSPs with\
    \ the same priority,\n   sorts those by increasing bandwidth (select smaller LSPs\
    \ in order to\n   reduce bandwidth wastage).\n                      -------------------------------------------------\n\
    \                      |       Heuristic       |   Other algorithms    |\n   \
    \                   -------------------------------------------------\n      \
    \                |  HPB  | HBlock|  HNB  |   P   |  PN   |  PB   |\n      -----------------------------------------------------------------\n\
    \      Need to be      |  532  |  532  |  532  |  532  |  532  |  532  |\n   \
    \   Rerouted        |       |       |       |       |       |       |\n      -----------------------------------------------------------------\n\
    \      Preempted       |  612  |  483  |  619  |  504  |  477  |  598  |\n   \
    \   -----------------------------------------------------------------\n      Rerouted\
    \        |467|76%|341|73%|475|77%|347|69%|335|70%|436|73%|\n      Blocked    \
    \     |145|24%|130|27%|144|23%|157|31%|142|30%|162|27%|\n      -----------------------------------------------------------------\n\
    \      Max Cascading   |  4.5  |   2   |   5   |  2.75 |   2   | 2.75  |\n   \
    \   -----------------------------------------------------------------\n      Wasted\
    \ Bandwidth|       |       |       |       |       |       |\n      AVR (Mbps)\
    \      | 6638  |  532  | 6479  |  8247 | 8955  |  6832 |\n      Worst Case(Mbps)|\
    \ 35321 |26010  |36809  | 28501 | 31406 | 23449 |\n      -----------------------------------------------------------------\n\
    \      Priority        |       |       |       |       |       |       |\n   \
    \   Average         |   6   |  6.5  |  5.8  |  6.6  |  6.6  |  6.6  |\n      Worst\
    \ Case      |  1.5  |  3.8  |  1.2  |  3.8  |  3.8  |  3.8  |\n      -----------------------------------------------------------------\n\
    \      Extra Hops      |       |       |       |       |       |       |\n   \
    \   Average         |  0.23 | 0.25  | 0.22  | 0.25  | 0.25  | 0.23  |\n      Worst\
    \ Case      |  3.25 |  3    | 3.25  |  3    |   3   | 2.75  |\n      -----------------------------------------------------------------\n\
    \      Table 2: Simulation results for constant network failure:\n           \
    \    1 random failure every hour.\n   From Table 2, we can conclude that among\
    \ the heuristic (HPB, HBlock,\n   HNB) results, HBlock resulted in the smaller\
    \ number of LSPs being\n   preempted.  More importantly, it also resulted in an\
    \ overall smaller\n   rejection rate and smaller average wasted bandwidth (and\
    \ second\n   overall smaller worst-case wasted bandwidth.)\n   Although HBlock\
    \ does not try to minimize the number of preempted\n   LSPs, it ends up doing\
    \ so, because it preempts LSPs with lower\n   priority mostly, and therefore it\
    \ does not propagate cascading much\n   further.  Cascading was the overall lowest\
    \ (preemption caused at most\n   two levels of preemption, which was also the\
    \ case for the policy PN).\n   The average and worst preemption priority was very\
    \ satisfactory\n   (preempting mostly lowest-priority LSPs, like the other algorithms\
    \ P,\n   PN, and PB).\n   When HPB was in use, more LSPs were preempted as a consequence\
    \ of the\n   higher cascading effect.  That is due to the heuristic's choice of\n\
    \   preempting LSPs that are very similar in bandwidth size to the\n   bandwidth\
    \ size of the preemptor LSP (which can result in preempting a\n   higher priority\
    \ LSP and therefore causing cascading).  The wasted\n   bandwidth was reduced\
    \ when compared to the other algorithms (P, PN,\n   PB).\n   When HNB was used,\
    \ cascading was higher than the other cases, due to\n   the fact that LSPs with\
    \ higher priority could be preempted.  When\n   compared to P, PN, or PB, the\
    \ heuristic HNB preempted more LSPs (in\n   fact, it preempted the largest number\
    \ of LSPs overall, clearly\n   showing the cascading effect), but the average\
    \ wasted bandwidth was\n   smaller, although not as small as HBlock's (the HNB\
    \ heuristic tries\n   to preempt a single LSP, meaning it will preempt LSPs that\
    \ have a\n   reserved bandwidth similar to the actual bandwidth needed.  The\n\
    \   algorithm is not always successful, because such a match may not\n   exist,\
    \ and in that case, the wasted bandwidth could be high).  The\n   preempted priority\
    \ was the highest on average and worse case, which\n   also shows why the cascading\
    \ level was also the highest (the\n   heuristic tries to select LSPs for preemption\
    \ without looking at\n   their priority levels).  In summary, this policy resulted\
    \ in a poor\n   performance.\n   Policy PN resulted in the small number of preempted\
    \ LSPs overall and\n   small number of LSPs not successfully rerouted.  Cascading\
    \ is low,\n   but bandwidth wastage is very high (overall highest bandwidth\n\
    \   wastage).  Moreover, in several cases in which rerouting happened on\n   portions\
    \ of the network that were underloaded, the heuristic HBlock\n   preempted a smaller\
    \ number of LSPs than PN.\n   Policy P selects a larger number of LSPs (when compared\
    \ to PN) with\n   low priority for preemption, and therefore it is able to successfully\n\
    \   reroute less LSPs when compared to HBlock, HPB, HNB, or PN.  The\n   bandwidth\
    \ wastage is also higher when compared to any of the\n   heuristic results or\
    \ to PB, and it could be worse if the network had\n   LSPs with a low priority\
    \ and large bandwidth, which is not the case.\n   Policy PB, when compared to\
    \ PN, resulted in a larger number of\n   preempted LSPs and an overall larger\
    \ number of blocked LSPs (not\n   rerouted), due to preemption.  Cascading was\
    \ slightly higher.  Since\n   the selected LSPs have low priority, they are not\
    \ able to preempt\n   much further and are blocked when the links are congested.\
    \  Bandwidth\n   wastage was smaller since the policy tries to minimize wastage,\
    \ and\n   preempted priority was about the same on average and worst case.\n \
    \  The simulation results show that when preemption is based on\n   priority,\
    \ cascading is not critical since the preempted LSPs will not\n   be able to propagate\
    \ preemption much further.  When the number of\n   LSPs is considered, fewer LSPs\
    \ are preempted and the chances of\n   rerouting increases.  When bandwidth wastage\
    \ is considered, smaller\n   LSPs are preempted in each link and the wasted bandwidth\
    \ is low.  The\n   heuristic seems to combine these features, yielding the best\
    \ results,\n   especially in the case of blocking probability.  When the heuristic\n\
    \   was configured to minimize blocking probability (HBlock), small LSPs\n   with\
    \ low priority were selected for preemption on normally loaded\n   links and fewer\
    \ (larger) LSPs with low priority were selected on\n   congested links.  Due to\
    \ their low priority, cascading was not an\n   issue.  Several LSPs were selected\
    \ for preemption, but the rate of\n   LSPs that were not successfully rerouted\
    \ was the lowest.  Since the\n   LSPs are small, it is easier to find a new route\
    \ in the network.\n   When selecting LSPs on a congested link, fewer larger LSPs\
    \ are\n   selected improving load balance.  Moreover, the bandwidth wastage was\n\
    \   the overall lowest.  In summary, the heuristic is very flexible and\n   can\
    \ be configured according to the network provider's best interest\n   regarding\
    \ the considered criteria.\n   For several cases, the failure of a link resulted\
    \ in no preemption at\n   all (all LSPs were able to find an alternate path in\
    \ the network) or\n   resulted in preemption of very few LSPs and subsequent successfully\n\
    \   rerouting of the same with no cascading effect.\n   It is also important to\
    \ note that for all policies in use, the number\n   of extra hops when LSPs are\
    \ rerouted was not critical, showing that\n   preempted LSPs can be rerouted on\
    \ a path with the same length or a\n   path that is slightly longer in number\
    \ of hops.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   The practice described in this document does\
    \ not raise specific\n   security issues beyond those of existing TE.\n"
- title: 8.  Acknowledgements
  contents:
  - "8.  Acknowledgements\n   We would like to acknowledge the input and helpful comments\
    \ from\n   Francois Le Faucheur (Cisco Systems) and George Uhl (Swales\n   Aerospace).\n"
- title: 9.  Informative References
  contents:
  - "9.  Informative References\n   [ATM-PREP]    Poretsky, S. and Gannon, T., \"\
    An Algorithm for\n                 Connection Precedence and Preemption in Asynchronous\n\
    \                 Transfer Mode (ATM) Networks\", Proceedings of IEEE ICC\n  \
    \               1998.\n   [DEC-PREP]    Peyravian, M. and Kshemkalyani, A. D.\
    \ , \"Decentralized\n                 Network Connection Preemption Algorithms\"\
    , Computer\n                 Networks and ISDN Systems, vol. 30 (11), pp. 1029-1043,\n\
    \                 June 1998.\n   [PREEMPTION]  de Oliveira, J. C. et al., \"A\
    \ New Preemption Policy for\n                 DiffServ-Aware Traffic Engineering\
    \ to Minimize\n                 Rerouting\", Proceedings of IEEE INFOCOM 2002.\n\
    \   [RFC2702]     Awduche, D., Malcolm, J., Agogbua, J., O'Dell, M., and\n   \
    \              J. McManus, \"Requirements for Traffic Engineering Over\n     \
    \            MPLS\", RFC 2702, September 1999.\n   [RFC3209]     Awduche, D.,\
    \ Berger, L., Gan, D., Li, T., Srinivasan,\n                 V., and G. Swallow,\
    \ \"RSVP-TE: Extensions to RSVP for\n                 LSP Tunnels\", RFC 3209,\
    \ December 2001.\n   [RFC3272]     Awduche, D., Chiu, A., Elwalid, A., Widjaja,\
    \ I., and X.\n                 Xiao, \"Overview and Principles of Internet Traffic\n\
    \                 Engineering\", RFC 3272, May 2002.\n   [RFC3564]     Le Faucheur,\
    \ F. and W. Lai, \"Requirements for Support\n                 of Differentiated\
    \ Services-aware MPLS Traffic\n                 Engineering\", RFC 3564, July\
    \ 2003.\n   [RFC4124]     Le Faucheur, F., \"Protocol Extensions for Support of\n\
    \                 Diffserv-aware MPLS Traffic Engineering\", RFC 4124,\n     \
    \            June 2005.\n   [RFC4126]     Ash, J., \"Max Allocation with Reservation\
    \ Bandwidth\n                 Constraints Model for Diffserv-aware MPLS Traffic\n\
    \                 Engineering & Performance Comparisons\", RFC 4126,\n       \
    \          June 2005.\n   [RFC4127]     Le Faucheur, F., \"Russian Dolls Bandwidth\
    \ Constraints\n                 Model for Diffserv-aware MPLS Traffic Engineering\"\
    ,\n                 RFC 4127, June 2005.\n   [RFC4128]     Lai, W., \"Bandwidth\
    \ Constraints Models for\n                 Differentiated Services (Diffserv)-aware\
    \ MPLS Traffic\n                 Engineering: Performance Evaluation\", RFC 4128,\n\
    \                 June 2005.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Jaudelice C. de Oliveira (editor)\n   Drexel University\n\
    \   3141 Chestnut Street (ECE Dept.)\n   Philadelphia, PA  19104\n   USA\n   EMail:\
    \ jau@ece.drexel.edu\n   JP Vasseur (editor)\n   Cisco Systems, Inc.\n   1414\
    \ Massachusetts Avenue\n   Boxborough, MA  01719\n   USA\n   EMail: jpv@cisco.com\n\
    \   Leonardo Chen\n   Verizon Laboratories\n   40 Sylvan Rd. LA0MS55\n   Waltham,\
    \ MA  02451\n   USA\n   EMail: leonardo.c.chen@verizon.com\n   Caterina Scoglio\n\
    \   Kansas State University\n   2061 Rathbone Hall\n   Manhattan, Kansas  66506-5204\n\
    \   USA\n   EMail: caterina@eece.ksu.edu\n"
- title: Full Copyright Statement
  contents:
  - "Full Copyright Statement\n   Copyright (C) The IETF Trust (2007).\n   This document\
    \ is subject to the rights, licenses and restrictions\n   contained in BCP 78\
    \ and at www.rfc-editor.org/copyright.html, and\n   except as set forth therein,\
    \ the authors retain all their rights.\n   This document and the information contained\
    \ herein are provided on an\n   \"AS IS\" basis and THE CONTRIBUTOR, THE ORGANIZATION\
    \ HE/SHE REPRESENTS\n   OR IS SPONSORED BY (IF ANY), THE INTERNET SOCIETY, THE\
    \ IETF TRUST AND\n   THE INTERNET ENGINEERING TASK FORCE DISCLAIM ALL WARRANTIES,\
    \ EXPRESS\n   OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTY THAT THE USE\
    \ OF\n   THE INFORMATION HEREIN WILL NOT INFRINGE ANY RIGHTS OR ANY IMPLIED\n\
    \   WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.\n"
- title: Intellectual Property
  contents:
  - "Intellectual Property\n   The IETF takes no position regarding the validity or\
    \ scope of any\n   Intellectual Property Rights or other rights that might be\
    \ claimed to\n   pertain to the implementation or use of the technology described\
    \ in\n   this document or the extent to which any license under such rights\n\
    \   might or might not be available; nor does it represent that it has\n   made\
    \ any independent effort to identify any such rights.  Information\n   on the\
    \ procedures with respect to rights in RFC documents can be\n   found in BCP 78\
    \ and BCP 79.\n   Copies of IPR disclosures made to the IETF Secretariat and any\n\
    \   assurances of licenses to be made available, or the result of an\n   attempt\
    \ made to obtain a general license or permission for the use of\n   such proprietary\
    \ rights by implementers or users of this\n   specification can be obtained from\
    \ the IETF on-line IPR repository at\n   http://www.ietf.org/ipr.\n   The IETF\
    \ invites any interested party to bring to its attention any\n   copyrights, patents\
    \ or patent applications, or other proprietary\n   rights that may cover technology\
    \ that may be required to implement\n   this standard.  Please address the information\
    \ to the IETF at\n   ietf-ipr@ietf.org.\n"
- title: Acknowledgement
  contents:
  - "Acknowledgement\n   Funding for the RFC Editor function is currently provided\
    \ by the\n   Internet Society.\n"
