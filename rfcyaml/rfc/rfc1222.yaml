- title: __initial_text__
  contents:
  - '               Advancing the NSFNET Routing Architecture

    '
- title: Status of this Memo
  contents:
  - "Status of this Memo\n   This RFC suggests improvements in the NSFNET routing\
    \ architecture to\n   accommodate a more flexible interface to the Backbone clients.\
    \  This\n   memo provides information for the Internet community.  It does not\n\
    \   specify an Internet standard.  Distribution of this memo is\n   unlimited.\n"
- title: Introduction
  contents:
  - "Introduction\n   This memo describes the history of NSFNET Backbone routing and\n\
    \   outlines two suggested phases for further evolution of the Backbone's\n  \
    \ routing interface.  The intent is to provide a more flexible\n   interface for\
    \ NSFNET Backbone service subscribers, by providing an\n   attachment option that\
    \ is simpler and lower-cost than the current\n   one.\n"
- title: Acknowledgements
  contents:
  - "Acknowledgements\n   The authors would like to thank Scott Brim (Cornell University),\n\
    \   Bilal Chinoy (Merit), Elise Gerich (Merit), Paul Love (SDSC), Steve\n   Wolff\
    \ (NSF), Bob Braden (ISI), and Joyce K. Reynolds (ISI) for their\n   review and\
    \ constructive comments.\n"
- title: 1. NSFNET Phase 1 Routing Architecture
  contents:
  - "1. NSFNET Phase 1 Routing Architecture\n   In the first phase of the NSFNET Backbone,\
    \ a 56Kbps infrastructure\n   utilized routers based on Fuzzball software [2].\
    \  The Phase 1\n   Backbone used the Hello Protocol for interior routing.  At\
    \ the\n   periphery of the Backbone, the client networks were typically\n   connected\
    \ by using a gatedaemon (\"gated\") interface to translate\n   between the Backbone's\
    \ Hello Protocol and the interior gateway\n   protocol (IGP) of the mid-level\
    \ network.\n   Mid-level networks primarily used the Routing Information Protocol\n\
    \   (RIP) [3] for their IGP.  The gatedaemon system acted as an interface\n  \
    \ between the Hello and RIP environments.  The overall appearance was\n   that\
    \ the Backbone, mid-level networks, and the campus networks formed\n   a single\
    \ routing system in which information was freely exchanged.\n   Network metrics\
    \ were translated among the three network levels\n   (backbone, mid-level networks,\
    \ and campuses).\n   With the development of the gatedaemon, sites were able to\
    \ introduce\n   filtering based on IP network numbers.  This process was controlled\n\
    \   by the staff at each individual site.\n   Once specific network routes were\
    \ learned, the infrastructure\n   forwarded metric changes throughout the interconnected\
    \ network. The\n   end-result was that a metric fluctuation on one end of the\n\
    \   interconnected network could permeate all the way to the other end,\n   crossing\
    \ multiple network administrations.  The frequency of metric\n   fluctuations\
    \ within the Backbone itself was further increased when\n   event-driven updates\
    \ (e.g., metric changes) were introduced.  Later,\n   damping of the event driven\
    \ updates lessened their frequency, but the\n   overall routing environment still\
    \ appeared to be quite unstable.\n   Given that only limited tools and protocols\
    \ were available to\n   engineer the flow of dynamic routing information, it was\
    \ fairly easy\n   for routing loops to form.  This was amplified as the topology\
    \ became\n   more fully connected without insulation of routing components from\n\
    \   each other.\n   All six nodes of the Phase 1 Backbone were located at client\
    \ sites,\n   specifically NSF funded supercomputer centers.\n"
- title: 2. NSFNET Phase 2 Routing Architecture
  contents:
  - "2. NSFNET Phase 2 Routing Architecture\n   The routing architecture for the second\
    \ phase of the NSFNET Backbone,\n   implemented on T1 (1.5Mbps) lines, focused\
    \ on the lessons learned in\n   the first NSFNET phase.  This resulted in a strong\
    \ decoupling of the\n   IGP environments of the backbone network and its attached\
    \ clients\n   [5].  Specifically, each of the administrative entities was able\
    \ to\n   use its own IGP in any way appropriate for the specific network.  The\n\
    \   interface between the backbone network and its attached client was\n   built\
    \ by means of exterior routing, initially via the Exterior\n   Gateway Protocol\
    \ (EGP) [1,4].\n   EGP improved provided routing isolation in two ways.  First,\
    \ EGP\n   signals only up/down transitions for individual network numbers, not\n\
    \   the fluctuations of metrics (with the exception of metric acceptance\n   of\
    \ local relevance to a single Nodal Switching System (NSS) only for\n   inbound\
    \ routing information, in the case of multiple EGP peers at a\n   NSS).  Second,\
    \ it allowed engineering of the dynamic distribution of\n   routing information.\
    \  That is, primary, secondary, etc., paths can be\n   determined, as long as\
    \ dynamic externally learned routing information\n   is available.  This allows\
    \ creation of a spanning tree routing\n   topology, satisfying the constraints\
    \ of EGP.\n   The pre-engineering of routes is accomplished by means of a routing\n\
    \   configuration database that is centrally controlled and created, with\n  \
    \ a subsequent distribution of individual configuration information to\n   all\
    \ the NSFNET Backbone nodes.  A computer controlled central system\n   ensures\
    \ the correctness of the database prior to its distribution to\n   the nodes.\n\
    \   All nodes of the 1.5Mbps NSFNET Backbone (currently fourteen) are\n   located\
    \ at client sites, such as NSF funded supercomputer centers and\n   mid-level\
    \ network attachment points.\n"
- title: 3. T3 Phase of the NSFNET Backbone
  contents:
  - "3. T3 Phase of the NSFNET Backbone\n   The T3 (45Mbps) phase of the NSFNET Backbone\
    \ is implemented by means\n   of a new architectural model, in which the principal\
    \ communication\n   nodes (core nodes) are co-located with major phone company\
    \ switching\n   facilities.  Those co-located nodes then form a two-dimensional\n\
    \   networking infrastructure \"cloud\".  Individual sites are connected\n   via\
    \ exterior nodes (E-NSS) and typically have a single T3 access line\n   to a core\
    \ node (C-NSS).  That is, an exterior node is physically at\n   the service subscriber\
    \ site.\n   With respect to routing, this structure is invisible to client sites,\n\
    \   as the routing interface uses the same techniques as the T1 NSFNET\n   Backbone.\
    \  The two backbones will remain independent infrastructures,\n   overlaying each\
    \ other and interconnected by exterior routing, and the\n   T1 Backbone will eventually\
    \ be phased out as a separate network.\n"
- title: 4. A Near-term Routing Alternative
  contents:
  - "4. A Near-term Routing Alternative\n   The experience with the T1/T3 NSFNET routing\
    \ demonstrated clear\n   advantages of this routing architecture in which the\
    \ whole\n   infrastructure is strongly compartmentalized.  Previous experience\n\
    \   also showed that the architecture imposes certain obligations upon\n   the\
    \ attached client networks.  Among them is the requirement that a\n   service\
    \ subscriber must deploy its own routing protocol peer,\n   participating in the\
    \ IGP of the service subscriber and connected via\n   a common subnet to the subscriber-site\
    \ NSFNET node.  The router and\n   the NSFNET Backbone exchange routing information\
    \ via an EGP or BGP\n   [7] session.\n   The drawbacks imposed by this requirement\
    \ will become more obvious\n   with the transition to the new architecture that\
    \ is employed by the\n   T3 phase of the NSFNET Backbone.  This will allow rapid\
    \ expansion to\n   many and smaller sites for which a very simple routing interface\
    \ may\n   be needed.\n   We strongly believe that separating the routing of the\
    \ service\n   subscriber from the NSFNET Backbone routing via some kind of EGP\
    \ is\n   the correct routing architecture.  However, it should not be\n   necessary\
    \ to translate this architecture into a requirement for each\n   service subscriber\
    \ to install and maintain additional equipment, or\n   for the subscriber to deal\
    \ with more complicated routing\n   environments.  In other words, while maintaining\
    \ that the concept of\n   routing isolation is correct, we view the present implementation\
    \ of\n   the concept as more restrictive than necessary.\n   An alternative implementation\
    \ of this concept may be realized by\n   separating the requirement for an EGP/BGP\
    \ session, as the mechanism\n   for exchanging routing information between the\
    \ service subscriber\n   network and the backbone, from the actual equipment that\
    \ has to be\n   deployed and maintained to support such a requirement.  The only\n\
    \   essential requirement for routing isolation is the presence of two\n   logical\
    \ routing entities.  The first logical entity participates in\n   the service\
    \ subscriber's IGP, the second logical entity participates\n   in the NSFNET Backbone\
    \ IGP, and the two logical entities exchange\n   information with each other by\
    \ means of inter-domain mechanisms.  We\n   suggest that these two logical entities\
    \ could exist within a single\n   physical entity.\n   In terms of implementation,\
    \ this would be no different from a\n   gatedaemon system interfacing with the\
    \ previous 56Kbps NSFNET\n   Backbone from the regional clients, except that we\
    \ want to continue\n   the strong routing and administrative control that decouple\
    \ the two\n   IGP domains.  Retaining an inter-domain mechanism (e.g., BGP) to\n\
    \   connect the two IGP domains within the single physical entity allows\n   the\
    \ use of a well defined and understood interface.  At the same\n   time, care\
    \ must be taken in the implementation that the two daemons\n   will not simultaneously\
    \ interact with the system kernel in unwanted\n   ways.\n   The possibility of\
    \ interfacing two IGP domains within a single router\n   has also been noted in\
    \ [8].  For the NSFNET Backbone case, we propose\n   in addition to retain strong\
    \ firewalls between the IGP domains.  The\n   IGP information would need to be\
    \ tagged with exterior domain\n   information at its entry into the other IGP.\
    \  It would also be\n   important to allow distributed control of the configuration.\
    \  The\n   NSFNET Backbone organization and the provider of the attached client\n\
    \   network are each responsible for the integrity of their own routing\n   information.\n\
    \   An example implementation might be a single routing engine that\n   executed\
    \ two instances of routing daemons.  In the NSFNET Backbone\n   case, one of the\
    \ daemons would participate in the service\n   subscriber's IGP, and the other\
    \ would participate in the NSFNET\n   Backbone IGP.  These two instances could\
    \ converse with each other by\n   running EGP/BGP via a local loopback mechanism\
    \ or internal IPC.  In\n   the NSFNET Backbone implementation, the NSFNET T1 E-PSP\
    \ or T3 E-NSS\n   are UNIX machines, so the local loopback interface (lo0) of\
    \ the UNIX\n   operating system may be used.\n   Putting both entities into the\
    \ same physical machine means that the\n   E-PSP/E-NSS would participate in the\
    \ regional IGP on its exterior\n   interface.  We would still envision the Ethernet\
    \ attachment to be the\n   demarcation point for the administrative control and\
    \ operational\n   responsibility.  However, the regional client could provide\
    \ the\n   configuration information for the routing daemon that interfaced to\n\
    \   the regional IGP, allowing the regional to continue to exercise\n   control\
    \ over the introduction of routing information into its IGP.\n"
- title: 5. Long-Term Alternatives
  contents:
  - "5. Long-Term Alternatives\n   As technology employed by the NSFNET Backbone evolves,\
    \ one may\n   envision the demarcation line between the Backbone and the service\n\
    \   subscribers moving in the direction of the \"C-NSS cloud\", so that the\n\
    \   NSFNET IGP will be confined to the C-NSS, while the E-NSS will be a\n   full\
    \ participant in the IGP of the service subscriber.\n   Clearly, one of the major\
    \ prerequisites for such an evolution is the\n   ability for operational management\
    \ of the physical medium connecting\n   a C-NSS with an E-NSS by two different\
    \ administrative entities (i.e.,\n   the NSFNET Backbone provider as well as the\
    \ service subscriber).  It\n   will also have to be manageable enough to be comparable\
    \ in ease of\n   use to an Ethernet interface, as a well-defined demarcation point.\n\
    \   The evolution of the Point-to-Point Protocol, as well as a\n   significantly\
    \ enhanced capability for managing serial lines via\n   standard network management\
    \ protocols, will clearly help.  This may\n   not be the complete answer, as a\
    \ variety of equipment is used on\n   serial lines, making it difficult to isolate\
    \ a hardware problem.\n   Similar issues may arise for future demarcation interfaces\
    \ to\n   Internet infrastructure (e.g., SMDS interfaces).\n   In summary, there\
    \ is an opportunity to simplify the management,\n   administration, and exchange\
    \ of routing information by collapsing the\n   number of physical entities involved.\n"
- title: 6. References
  contents:
  - "6. References\n   [1] Mills, D., \"Exterior Gateway Protocol Formal Specification\"\
    , RFC\n       904, BBN, April 1984.\n   [2] Mills, D., and H-W. Braun, \"The NSFNET\
    \ Backbone Network\", SIGCOMM\n       1987, August 1987.\n   [3] Hedrick, C.,\
    \ \"Routing Information Protocol\", RFC 1058, Rutgers\n       University, June\
    \ 1988.\n   [4] Rekhter, Y., \"EGP and Policy Based Routing in the New NSFNET\n\
    \       Backbone\", RFC 1092, IBM T.J. Watson Research Center, February\n    \
    \   1989.\n   [5] Braun, H-W., \"The NSFNET Routing Architecture\", RFC 1093,\n\
    \       Merit/NSFNET, February 1989.\n   [6] Braun, H-W., \"Models of Policy Based\
    \ Routing\", RFC 1104,\n       Merit/NSFNET, June 1989.\n   [7] Lougheed, K.,\
    \ and Y. Rekhter, \"A Border Gateway Protocol (BGP)\",\n       RFC 1163, cisco\
    \ Systems, IBM T.J. Watson Research Center, June\n       1990.\n   [8] Almquist,\
    \ P., \"Requirements for Internet IP Routers\", to be\n       published as a RFC.\n"
- title: 7.  Security Considerations
  contents:
  - "7.  Security Considerations\n   Security issues are not discussed in this memo.\n"
- title: 8. Authors' Addresses
  contents:
  - "8. Authors' Addresses\n   Hans-Werner Braun\n   San Diego Supercomputer Center\n\
    \   P.O. Box 85608\n   La Jolla, CA 92186-9784\n   Phone: (619) 534-5035\n   Fax:\
    \   (619) 534-5113\n   EMail: HWB@SDSC.EDU\n   Yakov Rekhter\n   T.J. Watson Research\
    \ Center\n   IBM Corporation\n   P.O. Box 218\n   Yorktown Heights, NY  10598\n\
    \   Phone: (914) 945-3896\n   EMail: Yakov@Watson.IBM.COM\n"
