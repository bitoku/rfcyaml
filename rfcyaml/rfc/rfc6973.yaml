- title: __initial_text__
  contents:
  - ''
- title: Internet Architecture Board (IAB)                              A. Cooper
  contents:
  - "Internet Architecture Board (IAB)                              A. Cooper\n  \
    \           Privacy Considerations for Internet Protocols\n"
- title: Abstract
  contents:
  - "Abstract\n   This document offers guidance for developing privacy considerations\n\
    \   for inclusion in protocol specifications.  It aims to make designers,\n  \
    \ implementers, and users of Internet protocols aware of privacy-\n   related\
    \ design choices.  It suggests that whether any individual RFC\n   warrants a\
    \ specific privacy considerations section will depend on the\n   document's content.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for informational purposes.\n   This document is a product\
    \ of the Internet Architecture Board (IAB)\n   and represents information that\
    \ the IAB has deemed valuable to\n   provide for permanent record.  It represents\
    \ the consensus of the\n   Internet Architecture Board (IAB).  Documents approved\
    \ for\n   publication by the IAB are not a candidate for any level of Internet\n\
    \   Standard; see Section 2 of RFC 5741.\n   Information about the current status\
    \ of this document, any errata,\n   and how to provide feedback on it may be obtained\
    \ at\n   http://www.rfc-editor.org/info/rfc6973.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2013 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.  This document is subject\
    \ to\n   BCP 78 and the IETF Trust's Legal Provisions Relating to IETF\n   Documents\n\
    \   (http://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................4\n\
    \   2. Scope of Privacy Implications of Internet Protocols .............5\n  \
    \ 3. Terminology .....................................................6\n    \
    \  3.1. Entities ...................................................7\n      3.2.\
    \ Data and Analysis ..........................................8\n      3.3. Identifiability\
    \ ............................................9\n   4. Communications Model ...........................................10\n\
    \   5. Privacy Threats ................................................12\n  \
    \    5.1. Combined Security-Privacy Threats .........................13\n    \
    \       5.1.1. Surveillance .......................................13\n      \
    \     5.1.2. Stored Data Compromise .............................14\n        \
    \   5.1.3. Intrusion ..........................................14\n          \
    \ 5.1.4. Misattribution .....................................14\n      5.2. Privacy-Specific\
    \ Threats ..................................15\n           5.2.1. Correlation\
    \ ........................................15\n           5.2.2. Identification\
    \ .....................................16\n           5.2.3. Secondary Use ......................................16\n\
    \           5.2.4. Disclosure .........................................17\n  \
    \         5.2.5. Exclusion ..........................................17\n   6.\
    \ Threat Mitigations .............................................18\n      6.1.\
    \ Data Minimization .........................................18\n           6.1.1.\
    \ Anonymity ..........................................19\n           6.1.2. Pseudonymity\
    \ .......................................20\n           6.1.3. Identity Confidentiality\
    \ ...........................20\n           6.1.4. Data Minimization within Identity\
    \ Management .......21\n      6.2. User Participation ........................................21\n\
    \      6.3. Security ..................................................22\n  \
    \ 7. Guidelines .....................................................23\n    \
    \  7.1. Data Minimization .........................................24\n      7.2.\
    \ User Participation ........................................25\n      7.3. Security\
    \ ..................................................25\n      7.4. General ...................................................26\n\
    \   8. Example ........................................................26\n  \
    \ 9. Security Considerations ........................................31\n   10.\
    \ Acknowledgements ..............................................31\n   11. IAB\
    \ Members at the Time of Approval ...........................32\n   12. Informative\
    \ References ........................................32\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   [RFC3552] provides detailed guidance to protocol designers\
    \ about both\n   how to consider security as part of protocol design and how to\
    \ inform\n   readers of protocol specifications about security issues.  This\n\
    \   document intends to provide a similar set of guidelines for\n   considering\
    \ privacy in protocol design.\n   Privacy is a complicated concept with a rich\
    \ history that spans many\n   disciplines.  With regard to data, often it is a\
    \ concept applied to\n   \"personal data\", commonly defined as information relating\
    \ to an\n   identified or identifiable individual.  Many sets of privacy\n   principles\
    \ and privacy design frameworks have been developed in\n   different forums over\
    \ the years.  These include the Fair Information\n   Practices [FIPs], a baseline\
    \ set of privacy protections pertaining to\n   the collection and use of personal\
    \ data (often based on the\n   principles established in [OECD], for example),\
    \ and the Privacy by\n   Design concept, which provides high-level privacy guidance\
    \ for\n   systems design (see [PbD] for one example).  The guidance provided in\n\
    \   this document is inspired by this prior work, but it aims to be more\n   concrete,\
    \ pointing protocol designers to specific engineering choices\n   that can impact\
    \ the privacy of the individuals that make use of\n   Internet protocols.\n  \
    \ Different people have radically different conceptions of what privacy\n   means,\
    \ both in general and as it relates to them personally [Westin].\n   Furthermore,\
    \ privacy as a legal concept is understood differently in\n   different jurisdictions.\
    \  The guidance provided in this document is\n   generic and can be used to inform\
    \ the design of any protocol to be\n   used anywhere in the world, without reference\
    \ to specific legal\n   frameworks.\n   Whether any individual document warrants\
    \ a specific privacy\n   considerations section will depend on the document's\
    \ content.\n   Documents whose entire focus is privacy may not merit a separate\n\
    \   section (for example, \"Private Extensions to the Session Initiation\n   Protocol\
    \ (SIP) for Asserted Identity within Trusted Networks\"\n   [RFC3325]).  For certain\
    \ specifications, privacy considerations are a\n   subset of security considerations\
    \ and can be discussed explicitly in\n   the security considerations section.\
    \  Some documents will not require\n   discussion of privacy considerations (for\
    \ example, \"Definition of the\n   Opus Audio Codec\" [RFC6716]).  The guidance\
    \ provided here can and\n   should be used to assess the privacy considerations\
    \ of protocol,\n   architectural, and operational specifications and to decide\
    \ whether\n   those considerations are to be documented in a stand-alone section,\n\
    \   within the security considerations section, or throughout the\n   document.\
    \  The guidance provided here is meant to help the thought\n   process of privacy\
    \ analysis; it does not provide specific directions\n   for how to write a privacy\
    \ considerations section.\n   This document is organized as follows.  Section\
    \ 2 describes the\n   extent to which the guidance offered here is applicable\
    \ within the\n   IETF and within the larger Internet community.  Section 3 explains\n\
    \   the terminology used in this document.  Section 4 reviews typical\n   communications\
    \ architectures to understand at which points there may\n   be privacy threats.\
    \  Section 5 discusses threats to privacy as they\n   apply to Internet protocols.\
    \  Section 6 outlines mitigations of those\n   threats.  Section 7 provides the\
    \ guidelines for analyzing and\n   documenting privacy considerations within IETF\
    \ specifications.\n   Section 8 examines the privacy characteristics of an IETF\
    \ protocol to\n   demonstrate the use of the guidance framework.\n"
- title: 2.  Scope of Privacy Implications of Internet Protocols
  contents:
  - "2.  Scope of Privacy Implications of Internet Protocols\n   Internet protocols\
    \ are often built flexibly, making them useful in a\n   variety of architectures,\
    \ contexts, and deployment scenarios without\n   requiring significant interdependency\
    \ between disparately designed\n   components.  Although protocol designers often\
    \ have a particular\n   target architecture or set of architectures in mind at\
    \ design time,\n   it is not uncommon for architectural frameworks to develop\
    \ later,\n   after implementations exist and have been deployed in combination\n\
    \   with other protocols or components to form complete systems.\n   As a consequence,\
    \ the extent to which protocol designers can foresee\n   all of the privacy implications\
    \ of a particular protocol at design\n   time is limited.  An individual protocol\
    \ may be relatively benign on\n   its own, and it may make use of privacy and\
    \ security features at\n   lower layers of the protocol stack (Internet Protocol\
    \ Security,\n   Transport Layer Security, and so forth) to mitigate the risk of\n\
    \   attack.  But when deployed within a larger system or used in a way\n   not\
    \ envisioned at design time, its use may create new privacy risks.\n   Protocols\
    \ are often implemented and deployed long after design time\n   by different people\
    \ than those who did the protocol design.  The\n   guidelines in Section 7 ask\
    \ protocol designers to consider how their\n   protocols are expected to interact\
    \ with systems and information that\n   exist outside the protocol bounds, but\
    \ not to imagine every possible\n   deployment scenario.\n   Furthermore, in many\
    \ cases the privacy properties of a system are\n   dependent upon the complete\
    \ system design where various protocols are\n   combined together to form a product\
    \ solution; the implementation,\n   which includes the user interface design;\
    \ and operational deployment\n   practices, including default privacy settings\
    \ and security processes\n   of the company doing the deployment.  These details\
    \ are specific to\n   particular instantiations and generally outside the scope\
    \ of the work\n   conducted in the IETF.  The guidance provided here may be useful\
    \ in\n   making choices about these details, but its primary aim is to assist\n\
    \   with the design, implementation, and operation of protocols.\n   Transparency\
    \ of data collection and use -- often effectuated through\n   user interface design\
    \ -- is normally relied on (whether rightly or\n   wrongly) as a key factor in\
    \ determining the privacy impact of a\n   system.  Although most IETF activities\
    \ do not involve standardizing\n   user interfaces or user-facing communications,\
    \ in some cases,\n   understanding expected user interactions can be important\
    \ for\n   protocol design.  Unexpected user behavior may have an adverse impact\n\
    \   on security and/or privacy.\n   In sum, privacy issues, even those related\
    \ to protocol development,\n   go beyond the technical guidance discussed herein.\
    \  As an example,\n   consider HTTP [RFC2616], which was designed to allow the\
    \ exchange of\n   arbitrary data.  A complete analysis of the privacy considerations\n\
    \   for uses of HTTP might include what type of data is exchanged, how\n   this\
    \ data is stored, and how it is processed.  Hence the analysis for\n   an individual's\
    \ static personal web page would be different than the\n   use of HTTP for exchanging\
    \ health records.  A protocol designer\n   working on HTTP extensions (such as\
    \ Web Distributed Authoring and\n   Versioning (WebDAV) [RFC4918]) is not expected\
    \ to describe the\n   privacy risks derived from all possible usage scenarios,\
    \ but rather\n   the privacy properties specific to the extensions and any particular\n\
    \   uses of the extensions that are expected and foreseen at design time.\n"
- title: 3.  Terminology
  contents:
  - "3.  Terminology\n   This section defines basic terms used in this document, with\n\
    \   references to pre-existing definitions as appropriate.  As in\n   [RFC4949],\
    \ each entry is preceded by a dollar sign ($) and a space\n   for automated searching.\
    \  Note that this document does not try to\n   attempt to define the term 'privacy'\
    \ with a brief definition.\n   Instead, privacy is the sum of what is contained\
    \ in this document.\n   We therefore follow the approach taken by [RFC3552]. \
    \ Examples of\n   several different brief definitions are provided in [RFC4949].\n"
- title: 3.1.  Entities
  contents:
  - "3.1.  Entities\n   Several of these terms are further elaborated in Section 4.\n\
    \   $ Attacker:  An entity that works against one or more privacy\n      protection\
    \ goals.  Unlike observers, attackers' behavior is\n      unauthorized.\n   $\
    \ Eavesdropper:  A type of attacker that passively observes an\n      initiator's\
    \ communications without the initiator's knowledge or\n      authorization.  See\
    \ [RFC4949].\n   $ Enabler:  A protocol entity that facilitates communication\
    \ between\n      an initiator and a recipient without being directly in the\n\
    \      communications path.\n   $ Individual:  A human being.\n   $ Initiator:\
    \  A protocol entity that initiates communications with a\n      recipient.\n\
    \   $ Intermediary:  A protocol entity that sits between the initiator\n     \
    \ and the recipient and is necessary for the initiator and recipient\n      to\
    \ communicate.  Unlike an eavesdropper, an intermediary is an\n      entity that\
    \ is part of the communication architecture and\n      therefore at least tacitly\
    \ authorized.  For example, a SIP\n      [RFC3261] proxy is an intermediary in\
    \ the SIP architecture.\n   $ Observer:  An entity that is able to observe and\
    \ collect\n      information from communications, potentially posing privacy\n\
    \      threats, depending on the context.  As defined in this document,\n    \
    \  initiators, recipients, intermediaries, and enablers can all be\n      observers.\
    \  Observers are distinguished from eavesdroppers by\n      being at least tacitly\
    \ authorized.\n   $ Recipient:  A protocol entity that receives communications\
    \ from an\n      initiator.\n"
- title: 3.2.  Data and Analysis
  contents:
  - "3.2.  Data and Analysis\n   $ Attack:  An intentional act by which an entity\
    \ attempts to violate\n      an individual's privacy.  See [RFC4949].\n   $ Correlation:\
    \  The combination of various pieces of information that\n      relate to an individual\
    \ or that obtain that characteristic when\n      combined.\n   $ Fingerprint:\
    \  A set of information elements that identifies a\n      device or application\
    \ instance.\n   $ Fingerprinting:  The process of an observer or attacker uniquely\n\
    \      identifying (with a sufficiently high probability) a device or\n      application\
    \ instance based on multiple information elements\n      communicated to the observer\
    \ or attacker.  See [EFF].\n   $ Item of Interest (IOI):  Any data item that an\
    \ observer or attacker\n      might be interested in.  This includes attributes,\
    \ identifiers,\n      identities, communications content, and the fact that a\n\
    \      communication interaction has taken place.\n   $ Personal Data:  Any information\
    \ relating to an individual who can\n      be identified, directly or indirectly.\n\
    \   $ (Protocol) Interaction:  A unit of communication within a\n      particular\
    \ protocol.  A single interaction may be comprised of a\n      single message\
    \ between an initiator and recipient or multiple\n      messages, depending on\
    \ the protocol.\n   $ Traffic Analysis:  The inference of information from observation\
    \ of\n      traffic flows (presence, absence, amount, direction, timing,\n   \
    \   packet size, packet composition, and/or frequency), even if flows\n      are\
    \ encrypted.  See [RFC4949].\n   $ Undetectability:  The inability of an observer\
    \ or attacker to\n      sufficiently distinguish whether an item of interest exists\n\
    \      or not.\n   $ Unlinkability:  Within a particular set of information, the\n\
    \      inability of an observer or attacker to distinguish whether two\n     \
    \ items of interest are related or not (with a high enough degree of\n      probability\
    \ to be useful to the observer or attacker).\n"
- title: 3.3.  Identifiability
  contents:
  - "3.3.  Identifiability\n   $ Anonymity:  The state of being anonymous.\n   $ Anonymity\
    \ Set:  A set of individuals that have the same attributes,\n      making them\
    \ indistinguishable from each other from the perspective\n      of a particular\
    \ attacker or observer.\n   $ Anonymous:  A state of an individual in which an\
    \ observer or\n      attacker cannot identify the individual within a set of other\n\
    \      individuals (the anonymity set).\n   $ Attribute:  A property of an individual.\n\
    \   $ Identifiability:  The extent to which an individual is\n      identifiable.\n\
    \   $ Identifiable:  A property in which an individual's identity is\n      capable\
    \ of being known to an observer or attacker.\n   $ Identification:  The linking\
    \ of information to a particular\n      individual to infer an individual's identity\
    \ or to allow the\n      inference of an individual's identity in some context.\n\
    \   $ Identified:  A state in which an individual's identity is known.\n   $ Identifier:\
    \  A data object uniquely referring to a specific\n      identity of a protocol\
    \ entity or individual in some context.  See\n      [RFC4949].  Identifiers can\
    \ be based upon natural names --\n      official names, personal names, and/or\
    \ nicknames -- or can be\n      artificial (for example, x9z32vb).  However, identifiers\
    \ are by\n      definition unique within their context of use, while natural names\n\
    \      are often not unique.\n   $ Identity:  Any subset of an individual's attributes,\
    \ including\n      names, that identifies the individual within a given context.\n\
    \      Individuals usually have multiple identities for use in different\n   \
    \   contexts.\n   $ Identity Confidentiality:  A property of an individual where\
    \ only\n      the recipient can sufficiently identify the individual within a\n\
    \      set of other individuals.  This can be a desirable property of\n      authentication\
    \ protocols.\n   $ Identity Provider:  An entity (usually an organization) that\
    \ is\n      responsible for establishing, maintaining, securing, and vouching\n\
    \      for the identities associated with individuals.\n   $ Official Name:  A\
    \ personal name for an individual that is\n      registered in some official context\
    \ (for example, the name on an\n      individual's birth certificate).  Official\
    \ names are often not\n      unique.\n   $ Personal Name:  A natural name for\
    \ an individual.  Personal names\n      are often not unique and often comprise\
    \ given names in combination\n      with a family name.  An individual may have\
    \ multiple personal\n      names at any time and over a lifetime, including official\
    \ names.\n      From a technological perspective, it cannot always be determined\n\
    \      whether a given reference to an individual is, or is based upon,\n    \
    \  the individual's personal name(s) (see Pseudonym).\n   $ Pseudonym:  A name\
    \ assumed by an individual in some context,\n      unrelated to the individual's\
    \ personal names known by others in\n      that context, with an intent of not\
    \ revealing the individual's\n      identities associated with his or her other\
    \ names.  Pseudonyms are\n      often not unique.\n   $ Pseudonymity:  The state\
    \ of being pseudonymous.\n   $ Pseudonymous:  A property of an individual in which\
    \ the individual\n      is identified by a pseudonym.\n   $ Real Name:  See Personal\
    \ Name and Official Name.\n   $ Relying Party:  An entity that relies on assertions\
    \ of individuals'\n      identities from identity providers in order to provide\
    \ services to\n      individuals.  In effect, the relying party delegates aspects\
    \ of\n      identity management to the identity provider(s).  Such delegation\n\
    \      requires protocol exchanges, trust, and a common understanding of\n   \
    \   semantics of information exchanged between the relying party and\n      the\
    \ identity provider.\n"
- title: 4.  Communications Model
  contents:
  - "4.  Communications Model\n   To understand attacks in the privacy-harm sense,\
    \ it is helpful to\n   consider the overall communication architecture and different\
    \ actors'\n   roles within it.  Consider a protocol entity, the \"initiator\"\
    , that\n   initiates communication with some recipient.  Privacy analysis is\n\
    \   most relevant for protocols with use cases in which the initiator\n   acts\
    \ on behalf of an individual (or different individuals at\n   different times).\
    \  It is this individual whose privacy is potentially\n   threatened (although\
    \ in some instances an initiator communicates\n   information about another individual,\
    \ in which case both of their\n   privacy interests will be implicated).\n   Communications\
    \ may be direct between the initiator and the recipient,\n   or they may involve\
    \ an application-layer intermediary (such as a\n   proxy, cache, or relay) that\
    \ is necessary for the two parties to\n   communicate.  In some cases, this intermediary\
    \ stays in the\n   communication path for the entire duration of the communication;\n\
    \   sometimes it is only used for communication establishment, for either\n  \
    \ inbound or outbound communication.  In some cases, there may be a\n   series\
    \ of intermediaries that are traversed.  At lower layers,\n   additional entities\
    \ involved in packet forwarding may interfere with\n   privacy protection goals\
    \ as well.\n   Some communications tasks require multiple protocol interactions\
    \ with\n   different entities.  For example, a request to an HTTP server may be\n\
    \   preceded by an interaction between the initiator and an\n   Authentication,\
    \ Authorization, and Accounting (AAA) server for\n   network access and to a Domain\
    \ Name System (DNS) server for name\n   resolution.  In this case, the HTTP server\
    \ is the recipient and the\n   other entities are enablers of the initiator-to-recipient\n\
    \   communication.  Similarly, a single communication with the recipient\n   might\
    \ generate further protocol interactions between either the\n   initiator or the\
    \ recipient and other entities, and the roles of the\n   entities might change\
    \ with each interaction.  For example, an HTTP\n   request might trigger interactions\
    \ with an authentication server or\n   with other resource servers wherein the\
    \ recipient becomes an\n   initiator in those later interactions.\n   Thus, when\
    \ conducting privacy analysis of an architecture that\n   involves multiple communications\
    \ phases, the entities involved may\n   take on different -- or opposing -- roles\
    \ from a privacy\n   considerations perspective in each phase.  Understanding\
    \ the privacy\n   implications of the architecture as a whole may require a separate\n\
    \   analysis of each phase.\n   Protocol design is often predicated on the notion\
    \ that recipients,\n   intermediaries, and enablers are assumed to be authorized\
    \ to receive\n   and handle data from initiators.  As [RFC3552] explains, \"we\
    \ assume\n   that the end systems engaging in a protocol exchange have not\n \
    \  themselves been compromised\".  However, privacy analysis requires\n   questioning\
    \ this assumption, since systems are often compromised for\n   the purpose of\
    \ obtaining personal data.\n   Although recipients, intermediaries, and enablers\
    \ may not generally\n   be considered as attackers, they may all pose privacy\
    \ threats\n   (depending on the context) because they are able to observe, collect,\n\
    \   process, and transfer privacy-relevant data.  These entities are\n   collectively\
    \ described below as \"observers\" to distinguish them from\n   traditional attackers.\
    \  From a privacy perspective, one important\n   type of attacker is an eavesdropper:\
    \ an entity that passively\n   observes the initiator's communications without\
    \ the initiator's\n   knowledge or authorization.\n   The threat descriptions\
    \ in the next section explain how observers and\n   attackers might act to harm\
    \ individuals' privacy.  Different kinds of\n   attacks may be feasible at different\
    \ points in the communications\n   path.  For example, an observer could mount\
    \ surveillance or\n   identification attacks between the initiator and intermediary,\
    \ or\n   instead could surveil an enabler (e.g., by observing DNS queries from\n\
    \   the initiator).\n"
- title: 5.  Privacy Threats
  contents:
  - "5.  Privacy Threats\n   Privacy harms come in a number of forms, including harms\
    \ to financial\n   standing, reputation, solitude, autonomy, and safety.  A victim\
    \ of\n   identity theft or blackmail, for example, may suffer a financial loss\n\
    \   as a result.  Reputational harm can occur when disclosure of\n   information\
    \ about an individual, whether true or false, subjects that\n   individual to\
    \ stigma, embarrassment, or loss of personal dignity.\n   Intrusion or interruption\
    \ of an individual's life or activities can\n   harm the individual's ability\
    \ to be left alone.  When individuals or\n   their activities are monitored, exposed,\
    \ or at risk of exposure,\n   those individuals may be stifled from expressing\
    \ themselves,\n   associating with others, and generally conducting their lives\
    \ freely.\n   They may also feel a general sense of unease, in that it is \"creepy\"\
    \n   to be monitored or to have data collected about them.  In cases where\n \
    \  such monitoring is for the purpose of stalking or violence (for\n   example,\
    \ monitoring communications to or from a domestic abuse\n   shelter), it can put\
    \ individuals in physical danger.\n   This section lists common privacy threats\
    \ (drawing liberally from\n   [Solove], as well as [CoE]), showing how each of\
    \ them may cause\n   individuals to incur privacy harms and providing examples\
    \ of how\n   these threats can exist on the Internet.  This threat modeling is\n\
    \   inspired by security threat analysis.  Although it is not a perfect\n   fit\
    \ for assessing privacy risks in Internet protocols and systems, no\n   better\
    \ methodology has been developed to date.\n   Some privacy threats are already\
    \ considered in Internet protocols as\n   a matter of routine security analysis.\
    \  Others are more pure privacy\n   threats that existing security considerations\
    \ do not usually address.\n   The threats described here are divided into those\
    \ that may also be\n   considered security threats and those that are primarily\
    \ privacy\n   threats.\n   Note that an individual's awareness of and consent\
    \ to the practices\n   described below may change an individual's perception of\
    \ and concern\n   for the extent to which they threaten privacy.  If an individual\n\
    \   authorizes surveillance of his own activities, for example, the\n   individual\
    \ may be able to take actions to mitigate the harms\n   associated with it or\
    \ may consider the risk of harm to be tolerable.\n"
- title: 5.1.  Combined Security-Privacy Threats
  contents:
  - '5.1.  Combined Security-Privacy Threats

    '
- title: 5.1.1.  Surveillance
  contents:
  - "5.1.1.  Surveillance\n   Surveillance is the observation or monitoring of an\
    \ individual's\n   communications or activities.  The effects of surveillance\
    \ on the\n   individual can range from anxiety and discomfort to behavioral\n\
    \   changes such as inhibition and self-censorship, and even to the\n   perpetration\
    \ of violence against the individual.  The individual need\n   not be aware of\
    \ the surveillance for it to impact his or her privacy\n   -- the possibility\
    \ of surveillance may be enough to harm individual\n   autonomy.\n   Surveillance\
    \ can impact privacy, even if the individuals being\n   surveilled are not identifiable\
    \ or if their communications are\n   encrypted.  For example, an observer or eavesdropper\
    \ that conducts\n   traffic analysis may be able to determine what type of traffic\
    \ is\n   present (real-time communications or bulk file transfers, for\n   example)\
    \ or which protocols are in use, even if the observed\n   communications are encrypted\
    \ or the communicants are unidentifiable.\n   This kind of surveillance can adversely\
    \ impact the individuals\n   involved by causing them to become targets for further\
    \ investigation\n   or enforcement activities.  It may also enable attacks that\
    \ are\n   specific to the protocol, such as redirection to a specialized\n   interception\
    \ point or protocol-specific denials of service.\n   Protocols that use predictable\
    \ packet sizes or timing or include\n   fixed tokens at predictable offsets within\
    \ a packet can facilitate\n   this kind of surveillance.\n   Surveillance can\
    \ be conducted by observers or eavesdroppers at any\n   point along the communications\
    \ path.  Confidentiality protections (as\n   discussed in Section 3 of [RFC3552])\
    \ are necessary to prevent\n   surveillance of the content of communications.\
    \  To prevent traffic\n   analysis or other surveillance of communications patterns,\
    \ other\n   measures may be necessary, such as [Tor].\n"
- title: 5.1.2.  Stored Data Compromise
  contents:
  - "5.1.2.  Stored Data Compromise\n   End systems that do not take adequate measures\
    \ to secure stored data\n   from unauthorized or inappropriate access expose individuals\
    \ to\n   potential financial, reputational, or physical harm.\n   Protecting against\
    \ stored data compromise is typically outside the\n   scope of IETF protocols.\
    \  However, a number of common protocol\n   functions -- key management, access\
    \ control, or operational logging,\n   for example -- require the storage of data\
    \ about initiators of\n   communications.  When requiring or recommending that\
    \ information\n   about initiators or their communications be stored or logged\
    \ by end\n   systems (see, e.g., RFC 6302 [RFC6302]), it is important to recognize\n\
    \   the potential for that information to be compromised and for that\n   potential\
    \ to be weighed against the benefits of data storage.  Any\n   recipient, intermediary,\
    \ or enabler that stores data may be\n   vulnerable to compromise.  (Note that\
    \ stored data compromise is\n   distinct from purposeful disclosure, which is\
    \ discussed in\n   Section 5.2.4.)\n"
- title: 5.1.3.  Intrusion
  contents:
  - "5.1.3.  Intrusion\n   Intrusion consists of invasive acts that disturb or interrupt\
    \ one's\n   life or activities.  Intrusion can thwart individuals' desires to\
    \ be\n   left alone, sap their time or attention, or interrupt their\n   activities.\
    \  This threat is focused on intrusion into one's life\n   rather than direct\
    \ intrusion into one's communications.  The latter\n   is captured in Section\
    \ 5.1.1.\n   Unsolicited messages and denial-of-service attacks are the most\n\
    \   common types of intrusion on the Internet.  Intrusion can be\n   perpetrated\
    \ by any attacker that is capable of sending unwanted\n   traffic to the initiator.\n"
- title: 5.1.4.  Misattribution
  contents:
  - "5.1.4.  Misattribution\n   Misattribution occurs when data or communications\
    \ related to one\n   individual are attributed to another.  Misattribution can\
    \ result in\n   adverse reputational, financial, or other consequences for\n \
    \  individuals that are misidentified.\n   Misattribution in the protocol context\
    \ comes as a result of using\n   inadequate or insecure forms of identity or authentication,\
    \ and is\n   sometimes related to spoofing.  For example, as [RFC6269] notes,\n\
    \   abuse mitigation is often conducted on the basis of the source IP\n   address,\
    \ such that connections from individual IP addresses may be\n   prevented or temporarily\
    \ blacklisted if abusive activity is\n   determined to be sourced from those addresses.\
    \  However, in the case\n   where a single IP address is shared by multiple individuals,\
    \ those\n   penalties may be suffered by all individuals sharing the address,\n\
    \   even if they were not involved in the abuse.  This threat can be\n   mitigated\
    \ by using identity management mechanisms with proper forms\n   of authentication\
    \ (ideally with cryptographic properties) so that\n   actions can be attributed\
    \ uniquely to an individual to provide the\n   basis for accountability without\
    \ generating false positives.\n"
- title: 5.2.  Privacy-Specific Threats
  contents:
  - '5.2.  Privacy-Specific Threats

    '
- title: 5.2.1.  Correlation
  contents:
  - "5.2.1.  Correlation\n   Correlation is the combination of various pieces of information\n\
    \   related to an individual or that obtain that characteristic when\n   combined.\
    \  Correlation can defy people's expectations of the limits\n   of what others\
    \ know about them.  It can increase the power that those\n   doing the correlating\
    \ have over individuals as well as correlators'\n   ability to pass judgment,\
    \ threatening individual autonomy and\n   reputation.\n   Correlation is closely\
    \ related to identification.  Internet protocols\n   can facilitate correlation\
    \ by allowing individuals' activities to be\n   tracked and combined over time.\
    \  The use of persistent or\n   infrequently replaced identifiers at any layer\
    \ of the stack can\n   facilitate correlation.  For example, an initiator's persistent\
    \ use\n   of the same device ID, certificate, or email address across multiple\n\
    \   interactions could allow recipients (and observers) to correlate all\n   of\
    \ the initiator's communications over time.\n   As an example, consider Transport\
    \ Layer Security (TLS) session\n   resumption [RFC5246] or TLS session resumption\
    \ without server-side\n   state [RFC5077].  In RFC 5246 [RFC5246], a server provides\
    \ the client\n   with a session_id in the ServerHello message and caches the\n\
    \   master_secret for later exchanges.  When the client initiates a new\n   connection\
    \ with the server, it re-uses the previously obtained\n   session_id in its ClientHello\
    \ message.  The server agrees to resume\n   the session by using the same session_id\
    \ and the previously stored\n   master_secret for the generation of the TLS Record\
    \ Layer security\n   association.  RFC 5077 [RFC5077] borrows from the session\
    \ resumption\n   design idea, but the server encapsulates all state information\
    \ into a\n   ticket instead of caching it.  An attacker who is able to observe\
    \ the\n   protocol exchanges between the TLS client and the TLS server is able\n\
    \   to link the initial exchange to subsequently resumed TLS sessions\n   when\
    \ the session_id and the ticket are exchanged in the clear (which\n   is the case\
    \ with data exchanged in the initial handshake messages).\n   In theory, any observer\
    \ or attacker that receives an initiator's\n   communications can engage in correlation.\
    \  The extent of the\n   potential for correlation will depend on what data the\
    \ entity\n   receives from the initiator and has access to otherwise.  Often,\n\
    \   intermediaries only require a small amount of information for message\n  \
    \ routing and/or security.  In theory, protocol mechanisms could ensure\n   that\
    \ end-to-end information is not made accessible to these entities,\n   but in\
    \ practice the difficulty of deploying end-to-end security\n   procedures, additional\
    \ messaging or computational overhead, and other\n   business or legal requirements\
    \ often slow or prevent the deployment\n   of end-to-end security mechanisms,\
    \ giving intermediaries greater\n   exposure to initiators' data than is strictly\
    \ necessary from a\n   technical point of view.\n"
- title: 5.2.2.  Identification
  contents:
  - "5.2.2.  Identification\n   Identification is the linking of information to a\
    \ particular\n   individual to infer an individual's identity or to allow the\n\
    \   inference of an individual's identity.  In some contexts, it is\n   perfectly\
    \ legitimate to identify individuals, whereas in others,\n   identification may\
    \ potentially stifle individuals' activities or\n   expression by inhibiting their\
    \ ability to be anonymous or\n   pseudonymous.  Identification also makes it easier\
    \ for individuals to\n   be explicitly controlled by others (e.g., governments)\
    \ and to be\n   treated differentially compared to other individuals.\n   Many\
    \ protocols provide functionality to convey the idea that some\n   means has been\
    \ provided to validate that entities are who they claim\n   to be.  Often, this\
    \ is accomplished with cryptographic\n   authentication.  Furthermore, many protocol\
    \ identifiers, such as\n   those used in SIP or the Extensible Messaging and Presence\
    \ Protocol\n   (XMPP), may allow for the direct identification of individuals.\n\
    \   Protocol identifiers may also contribute indirectly to identification\n  \
    \ via correlation.  For example, a web site that does not directly\n   authenticate\
    \ users may be able to match its HTTP header logs with\n   logs from another site\
    \ that does authenticate users, rendering users\n   on the first site identifiable.\n\
    \   As with correlation, any observer or attacker may be able to engage\n   in\
    \ identification, depending on the information about the initiator\n   that is\
    \ available via the protocol mechanism or other channels.\n"
- title: 5.2.3.  Secondary Use
  contents:
  - "5.2.3.  Secondary Use\n   Secondary use is the use of collected information about\
    \ an individual\n   without the individual's consent for a purpose different from\
    \ that\n   for which the information was collected.  Secondary use may violate\n\
    \   people's expectations or desires.  The potential for secondary use\n   can\
    \ generate uncertainty as to how one's information will be used in\n   the future,\
    \ potentially discouraging information exchange in the\n   first place.  Secondary\
    \ use encompasses any use of data, including\n   disclosure.\n   One example of\
    \ secondary use would be an authentication server that\n   uses a network access\
    \ server's Access-Requests to track an\n   initiator's location.  Any observer\
    \ or attacker could potentially\n   make unwanted secondary uses of initiators'\
    \ data.  Protecting against\n   secondary use is typically outside the scope of\
    \ IETF protocols.\n"
- title: 5.2.4.  Disclosure
  contents:
  - "5.2.4.  Disclosure\n   Disclosure is the revelation of information about an individual\
    \ that\n   affects the way others judge the individual.  Disclosure can violate\n\
    \   individuals' expectations of the confidentiality of the data they\n   share.\
    \  The threat of disclosure may deter people from engaging in\n   certain activities\
    \ for fear of reputational harm, or simply because\n   they do not wish to be\
    \ observed.\n   Any observer or attacker that receives data about an initiator\
    \ may\n   engage in disclosure.  Sometimes disclosure is unintentional because\n\
    \   system designers do not realize that information being exchanged\n   relates\
    \ to individuals.  The most common way for protocols to limit\n   disclosure is\
    \ by providing access control mechanisms (discussed in\n   Section 5.2.5).  A\
    \ further example is provided by the IETF\n   geolocation privacy architecture\
    \ [RFC6280], which supports a way for\n   users to express a preference that their\
    \ location information not be\n   disclosed beyond the intended recipient.\n"
- title: 5.2.5.  Exclusion
  contents:
  - "5.2.5.  Exclusion\n   Exclusion is the failure to allow individuals to know about\
    \ the data\n   that others have about them and to participate in its handling\
    \ and\n   use.  Exclusion reduces accountability on the part of entities that\n\
    \   maintain information about people and creates a sense of\n   vulnerability\
    \ in relation to individuals' ability to control how\n   information about them\
    \ is collected and used.\n   The most common way for Internet protocols to be\
    \ involved in\n   enforcing exclusion is through access control mechanisms.  The\n\
    \   presence architecture developed in the IETF is a good example where\n   individuals\
    \ are included in the control of information about them.\n   Using a rules expression\
    \ language (e.g., presence authorization rules\n   [RFC5025]), presence clients\
    \ can authorize the specific conditions\n   under which their presence information\
    \ may be shared.\n   Exclusion is primarily considered problematic when the recipient\n\
    \   fails to involve the initiator in decisions about data collection,\n   handling,\
    \ and use.  Eavesdroppers engage in exclusion by their very\n   nature, since\
    \ their data collection and handling practices are\n   covert.\n"
- title: 6.  Threat Mitigations
  contents:
  - "6.  Threat Mitigations\n   Privacy is notoriously difficult to measure and quantify.\
    \  The extent\n   to which a particular protocol, system, or architecture \"protects\"\
    \ or\n   \"enhances\" privacy is dependent on a large number of factors relating\n\
    \   to its design, use, and potential misuse.  However, there are certain\n  \
    \ widely recognized classes of mitigations against the threats\n   discussed in\
    \ Section 5.  This section describes three categories of\n   relevant mitigations:\
    \ (1) data minimization, (2) user participation,\n   and (3) security.  The privacy\
    \ mitigations described in this section\n   can loosely be mapped to existing\
    \ privacy principles, such as the\n   Fair Information Practices, but they have\
    \ been adapted to fit the\n   target audience of this document.\n"
- title: 6.1.  Data Minimization
  contents:
  - "6.1.  Data Minimization\n   Data minimization refers to collecting, using, disclosing,\
    \ and\n   storing the minimal data necessary to perform a task.  Reducing the\n\
    \   amount of data exchanged reduces the amount of data that can be\n   misused\
    \ or leaked.\n   Data minimization can be effectuated in a number of different\
    \ ways,\n   including by limiting collection, use, disclosure, retention,\n  \
    \ identifiability, sensitivity, and access to personal data.  Limiting\n   the\
    \ data collected by protocol elements to only what is necessary\n   (collection\
    \ limitation) is the most straightforward way to help\n   reduce privacy risks\
    \ associated with the use of the protocol.  In\n   some cases, protocol designers\
    \ may also be able to recommend limits\n   to the use or retention of data, although\
    \ protocols themselves are\n   not often capable of controlling these properties.\n\
    \   However, the most direct application of data minimization to protocol\n  \
    \ design is limiting identifiability.  Reducing the identifiability of\n   data\
    \ by using pseudonyms or no identifiers at all helps to weaken the\n   link between\
    \ an individual and his or her communications.  Allowing\n   for the periodic\
    \ creation of new or randomized identifiers reduces\n   the possibility that multiple\
    \ protocol interactions or communications\n   can be correlated back to the same\
    \ individual.  The following\n   sections explore a number of different properties\
    \ related to\n   identifiability that protocol designers may seek to achieve.\n\
    \   Data minimization mitigates the following threats: surveillance,\n   stored\
    \ data compromise, correlation, identification, secondary use,\n   and disclosure.\n"
- title: 6.1.1.  Anonymity
  contents:
  - "6.1.1.  Anonymity\n   To enable anonymity of an individual, there must exist\
    \ a set of\n   individuals that appear to have the same attribute(s) as the\n\
    \   individual.  To the attacker or the observer, these individuals must\n   appear\
    \ indistinguishable from each other.  The set of all such\n   individuals is known\
    \ as the anonymity set, and membership of this set\n   may vary over time.\n \
    \  The composition of the anonymity set depends on the knowledge of the\n   observer\
    \ or attacker.  Thus, anonymity is relative with respect to\n   the observer or\
    \ attacker.  An initiator may be anonymous only within\n   a set of potential\
    \ initiators -- its initiator anonymity set -- which\n   itself may be a subset\
    \ of all individuals that may initiate\n   communications.  Conversely, a recipient\
    \ may be anonymous only within\n   a set of potential recipients -- its recipient\
    \ anonymity set.  Both\n   anonymity sets may be disjoint, may overlap, or may\
    \ be the same.\n   As an example, consider RFC 3325 (P-Asserted-Identity (PAI))\n\
    \   [RFC3325], an extension for the Session Initiation Protocol (SIP)\n   that\
    \ allows an individual, such as a Voice over IP (VoIP) caller, to\n   instruct\
    \ an intermediary that he or she trusts not to populate the\n   SIP From header\
    \ field with the individual's authenticated and\n   verified identity.  The recipient\
    \ of the call, as well as any other\n   entity outside of the individual's trust\
    \ domain, would therefore only\n   learn that the SIP message (typically a SIP\
    \ INVITE) was sent with a\n   header field 'From: \"Anonymous\" <sip:anonymous@anonymous.invalid>'\n\
    \   rather than the individual's address-of-record, which is typically\n   thought\
    \ of as the \"public address\" of the user.  When PAI is used,\n   the individual\
    \ becomes anonymous within the initiator anonymity set\n   that is populated by\
    \ every individual making use of that specific\n   intermediary.\n   Note that\
    \ this example ignores the fact that the recipient may infer\n   or obtain personal\
    \ data from the other SIP payloads (e.g., SIP Via\n   and Contact headers, the\
    \ Session Description Protocol (SDP)).  The\n   implication is that PAI only attempts\
    \ to address a particular threat,\n   namely the disclosure of identity (in the\
    \ From header) with respect\n   to the recipient.  This caveat makes the analysis\
    \ of the specific\n   protocol extension easier but cannot be assumed when conducting\n\
    \   analysis of an entire architecture.\n"
- title: 6.1.2.  Pseudonymity
  contents:
  - "6.1.2.  Pseudonymity\n   In the context of Internet protocols, almost all identifiers\
    \ can be\n   nicknames or pseudonyms, since there is typically no requirement\
    \ to\n   use personal names in protocols.  However, in certain scenarios it is\n\
    \   reasonable to assume that personal names will be used (with vCard\n   [RFC6350],\
    \ for example).\n   Pseudonymity is strengthened when less personal data can be\
    \ linked to\n   the pseudonym; when the same pseudonym is used less often and\
    \ across\n   fewer contexts; and when independently chosen pseudonyms are more\n\
    \   frequently used for new actions (making them, from an observer's or\n   attacker's\
    \ perspective, unlinkable).\n   For Internet protocols, the following are important\
    \ considerations:\n   whether protocols allow pseudonyms to be changed without\
    \ human\n   interaction, the default length of pseudonym lifetimes, to whom\n\
    \   pseudonyms are exposed, how individuals are able to control\n   disclosure,\
    \ how often pseudonyms can be changed, and the consequences\n   of changing them.\n"
- title: 6.1.3.  Identity Confidentiality
  contents:
  - "6.1.3.  Identity Confidentiality\n   An initiator has identity confidentiality\
    \ when any party other than\n   the recipient cannot sufficiently identify the\
    \ initiator within the\n   anonymity set.  The size of the anonymity set has a\
    \ direct impact on\n   identity confidentiality, since the smaller the set is,\
    \ the easier it\n   is to identify the initiator.  Identity confidentiality aims\
    \ to\n   provide a protection against eavesdroppers and intermediaries rather\n\
    \   than against the intended communication endpoints.\n   As an example, consider\
    \ the network access authentication procedures\n   utilizing the Extensible Authentication\
    \ Protocol (EAP) [RFC3748].\n   EAP includes an identity exchange where the Identity\
    \ Response is\n   primarily used for routing purposes and selecting which EAP\
    \ method to\n   use.  Since EAP Identity Requests and Identity Responses are sent\
    \ in\n   cleartext, eavesdroppers and intermediaries along the communication\n\
    \   path between the EAP peer and the EAP server can snoop on the\n   identity,\
    \ which is encoded in the form of the Network Access\n   Identifier (NAI) as defined\
    \ in RFC 4282 [RFC4282].  To address this\n   threat, as discussed in RFC 4282\
    \ [RFC4282], the username part of the\n   NAI (but not the realm part) can be\
    \ hidden from these eavesdroppers\n   and intermediaries with the cryptographic\
    \ support offered by EAP\n   methods.  Identity confidentiality has become a recommended\
    \ design\n   criteria for EAP (see [RFC4017]).  The EAP method for 3rd Generation\n\
    \   Authentication and Key Agreement (EAP-AKA) [RFC4187], for example,\n   protects\
    \ the EAP peer's identity against passive adversaries by\n   utilizing temporal\
    \ identities.  The EAP-Internet Key Exchange\n   Protocol version 2 (EAP-IKEv2)\
    \ method [RFC5106] is an example of an\n   EAP method that offers protection against\
    \ active attackers with\n   regard to the individual's identity.\n"
- title: 6.1.4.  Data Minimization within Identity Management
  contents:
  - "6.1.4.  Data Minimization within Identity Management\n   Modern systems are increasingly\
    \ relying on multi-party transactions\n   to authenticate individuals.  Many of\
    \ these systems make use of an\n   identity provider that is responsible for providing\
    \ AAA functionality\n   to relying parties that offer some protected resources.\
    \  To\n   facilitate these functions, an identity provider will usually go\n \
    \  through a process of verifying the individual's identity and issuing\n   credentials\
    \ to the individual.  When an individual seeks to make use\n   of a service provided\
    \ by the relying party, the relying party relies\n   on the authentication assertions\
    \ provided by its identity provider.\n   Note that in more sophisticated scenarios\
    \ the authentication\n   assertions are traits that demonstrate the individual's\
    \ capabilities\n   and roles.  The authorization responsibility may also be shared\n\
    \   between the identity provider and the relying party and does not\n   necessarily\
    \ need to reside only with the identity provider.\n   Such systems have the ability\
    \ to support a number of properties that\n   minimize data collection in different\
    \ ways:\n      In certain use cases, relying parties do not need to know the real\n\
    \      name or date of birth of an individual (for example, when the\n      individual's\
    \ age is the only attribute that needs to be\n      authenticated).\n      Relying\
    \ parties that collude can be prevented from using an\n      individual's credentials\
    \ to track the individual.  That is, two\n      different relying parties can\
    \ be prevented from determining that\n      the same individual has authenticated\
    \ to both of them.  This\n      typically requires identity management protocol\
    \ support as well as\n      support by both the relying party and the identity\
    \ provider.\n      The identity provider can be prevented from knowing which relying\n\
    \      parties an individual interacted with.  This requires, at a\n      minimum,\
    \ avoiding direct communication between the identity\n      provider and the relying\
    \ party at the time when access to a\n      resource by the initiator is made.\n"
- title: 6.2.  User Participation
  contents:
  - "6.2.  User Participation\n   As explained in Section 5.2.5, data collection and\
    \ use that happen\n   \"in secret\", without the individual's knowledge, are apt\
    \ to violate\n   the individual's expectation of privacy and may create incentives\
    \ for\n   misuse of data.  As a result, privacy regimes tend to include\n   provisions\
    \ to require informing individuals about data collection and\n   use and involving\
    \ them in decisions about the treatment of their\n   data.  In an engineering\
    \ context, supporting the goal of user\n   participation usually means providing\
    \ ways for users to control the\n   data that is shared about them.  It may also\
    \ mean providing ways for\n   users to signal how they expect their data to be\
    \ used and shared.\n   Different protocol and architectural designs can make supporting\
    \ user\n   participation (for example, the ability to support a dialog box for\n\
    \   user interaction) easier or harder; for example, OAuth-based services\n  \
    \ may have more natural hooks for user input than AAA services.\n   User participation\
    \ mitigates the following threats: surveillance,\n   secondary use, disclosure,\
    \ and exclusion.\n"
- title: 6.3.  Security
  contents:
  - "6.3.  Security\n   Keeping data secure at rest and in transit is another important\n\
    \   component of privacy protection.  As they are described in Section 2\n   of\
    \ [RFC3552], a number of security goals also serve to enhance\n   privacy:\n \
    \  o  Confidentiality: Keeping data secret from unintended listeners.\n   o  Peer\
    \ entity authentication: Ensuring that the endpoint of a\n      communication\
    \ is the one that is intended (in support of\n      maintaining confidentiality).\n\
    \   o  Unauthorized usage: Limiting data access to only those users who\n    \
    \  are authorized.  (Note that this goal also falls within data\n      minimization.)\n\
    \   o  Inappropriate usage: Limiting how authorized users can use data.\n    \
    \  (Note that this goal also falls within data minimization.)\n   Note that even\
    \ when these goals are achieved, the existence of items\n   of interest -- attributes,\
    \ identifiers, identities, communications,\n   actions (such as the sending or\
    \ receiving of a communication), or\n   anything else an attacker or observer\
    \ might be interested in -- may\n   still be detectable, even if they are not\
    \ readable.  Thus,\n   undetectability, in which an observer or attacker cannot\
    \ sufficiently\n   distinguish whether an item of interest exists or not, may\
    \ be\n   considered as a further security goal (albeit one that can be\n   extremely\
    \ difficult to accomplish).\n   Detection of the protocols or applications in\
    \ use via traffic\n   analysis may be particularly difficult to defend against.\
    \  As with\n   the anonymity of individuals, achieving \"protocol anonymity\"\
    \ requires\n   that multiple protocols or applications exist that appear to have\
    \ the\n   same attributes -- packet sizes, content, token locations, or\n   inter-packet\
    \ timing, for example.  An attacker or observer will not\n   be able to use traffic\
    \ analysis to identify which protocol or\n   application is in use if multiple\
    \ protocols or applications are\n   indistinguishable.\n   Defending against the\
    \ threat of traffic analysis will be possible to\n   different extents for different\
    \ protocols, may depend on\n   implementation- or use-specific details, and may\
    \ depend on which\n   other protocols already exist and whether they share similar\
    \ traffic\n   characteristics.  The defenses will also vary relative to what the\n\
    \   protocol is designed to do; for example, in some situations\n   randomizing\
    \ packet sizes, timing, or token locations will reduce the\n   threat of traffic\
    \ analysis, whereas in other situations (real-time\n   communications, for example)\
    \ holding some or all of those factors\n   constant is a more appropriate defense.\
    \  See \"Guidelines for the Use\n   of Variable Bit Rate Audio with Secure RTP\"\
    \ [RFC6562] for an example\n   of how these kinds of trade-offs should be evaluated.\n\
    \   By providing proper security protection, the following threats can be\n  \
    \ mitigated: surveillance, stored data compromise, misattribution,\n   secondary\
    \ use, disclosure, and intrusion.\n"
- title: 7.  Guidelines
  contents:
  - "7.  Guidelines\n   This section provides guidance for document authors in the\
    \ form of a\n   questionnaire about a protocol being designed.  The questionnaire\
    \ may\n   be useful at any point in the design process, particularly after\n \
    \  document authors have developed a high-level protocol model as\n   described\
    \ in [RFC4101].\n   Note that the guidance provided in this section does not recommend\n\
    \   specific practices.  The range of protocols developed in the IETF is\n   too\
    \ broad to make recommendations about particular uses of data or\n   how privacy\
    \ might be balanced against other design goals.  However,\n   by carefully considering\
    \ the answers to each question, document\n   authors should be able to produce\
    \ a comprehensive analysis that can\n   serve as the basis for discussion of whether\
    \ the protocol adequately\n   protects against privacy threats.  This guidance\
    \ is meant to help the\n   thought process of privacy analysis; it does not provide\
    \ specific\n   directions for how to write a privacy considerations section.\n\
    \   The framework is divided into four sections: three sections that\n   address\
    \ each of the mitigation classes from Section 6, plus a general\n   section. \
    \ Security is not fully elaborated, since substantial\n   guidance already exists\
    \ in [RFC3552].\n"
- title: 7.1.  Data Minimization
  contents:
  - "7.1.  Data Minimization\n   a.  Identifiers.  What identifiers does the protocol\
    \ use for\n       distinguishing initiators of communications?  Does the protocol\n\
    \       use identifiers that allow different protocol interactions to be\n   \
    \    correlated?  What identifiers could be omitted or be made less\n       identifying\
    \ while still fulfilling the protocol's goals?\n   b.  Data.  What information\
    \ does the protocol expose about\n       individuals, their devices, and/or their\
    \ device usage (other than\n       the identifiers discussed in (a))?  To what\
    \ extent is this\n       information linked to the identities of the individuals?\
    \  How\n       does the protocol combine personal data with the identifiers\n\
    \       discussed in (a)?\n   c.  Observers.  Which information discussed in (a)\
    \ and (b) is exposed\n       to each other protocol entity (i.e., recipients,\
    \ intermediaries,\n       and enablers)?  Are there ways for protocol implementers\
    \ to\n       choose to limit the information shared with each entity?  Are\n \
    \      there operational controls available to limit the information\n       shared\
    \ with each entity?\n   d.  Fingerprinting.  In many cases, the specific ordering\
    \ and/or\n       occurrences of information elements in a protocol allow users,\n\
    \       devices, or software using the protocol to be fingerprinted.  Is\n   \
    \    this protocol vulnerable to fingerprinting?  If so, how?  Can it\n      \
    \ be designed to reduce or eliminate the vulnerability?  If not,\n       why not?\n\
    \   e.  Persistence of identifiers.  What assumptions are made in the\n      \
    \ protocol design about the lifetime of the identifiers discussed\n       in (a)?\
    \  Does the protocol allow implementers or users to delete\n       or replace\
    \ identifiers?  How often does the specification\n       recommend deleting or\
    \ replacing identifiers by default?  Can the\n       identifiers, along with other\
    \ state information, be set to\n       automatically expire?\n   f.  Correlation.\
    \  Does the protocol allow for correlation of\n       identifiers?  Are there\
    \ expected ways that information exposed by\n       the protocol will be combined\
    \ or correlated with information\n       obtained outside the protocol?  How will\
    \ such combination or\n       correlation facilitate fingerprinting of a user,\
    \ device, or\n       application?  Are there expected combinations or correlations\n\
    \       with outside data that will make users of the protocol more\n       identifiable?\n\
    \   g.  Retention.  Does the protocol or its anticipated uses require\n      \
    \ that the information discussed in (a) or (b) be retained by\n       recipients,\
    \ intermediaries, or enablers?  If so, why?  Is the\n       retention expected\
    \ to be persistent or temporary?\n"
- title: 7.2.  User Participation
  contents:
  - "7.2.  User Participation\n   a.  User control.  What controls or consent mechanisms\
    \ does the\n       protocol define or require before personal data or identifiers\n\
    \       are shared or exposed via the protocol?  If no such mechanisms or\n  \
    \     controls are specified, is it expected that control and consent\n      \
    \ will be handled outside of the protocol?\n   b.  Control over sharing with individual\
    \ recipients.  Does the\n       protocol provide ways for initiators to share\
    \ different\n       information with different recipients?  If not, are there\n\
    \       mechanisms that exist outside of the protocol to provide\n       initiators\
    \ with such control?\n   c.  Control over sharing with intermediaries.  Does the\
    \ protocol\n       provide ways for initiators to limit which information is shared\n\
    \       with intermediaries?  If not, are there mechanisms that exist\n      \
    \ outside of the protocol to provide users with such control?  Is\n       it expected\
    \ that users will have relationships that govern the\n       use of the information\
    \ (contractual or otherwise) with those who\n       operate these intermediaries?\n\
    \   d.  Preference expression.  Does the protocol provide ways for\n       initiators\
    \ to express individuals' preferences to recipients or\n       intermediaries\
    \ with regard to the collection, use, or disclosure\n       of their personal\
    \ data?\n"
- title: 7.3.  Security
  contents:
  - "7.3.  Security\n   a.  Surveillance.  How do the protocol's security considerations\n\
    \       prevent surveillance, including eavesdropping and traffic\n       analysis?\
    \  Does the protocol leak information that can be\n       observed through traffic\
    \ analysis, such as by using a fixed token\n       at fixed offsets, or packet\
    \ sizes or timing that allow observers\n       to determine characteristics of\
    \ the traffic (e.g., which protocol\n       is in use or whether the traffic is\
    \ part of a real-time flow)?\n   b.  Stored data compromise.  How do the protocol's\
    \ security\n       considerations prevent or mitigate stored data compromise?\n\
    \   c.  Intrusion.  How do the protocol's security considerations prevent\n  \
    \     or mitigate intrusion, including denial-of-service attacks and\n       unsolicited\
    \ communications more generally?\n   d.  Misattribution.  How do the protocol's\
    \ mechanisms for identifying\n       and/or authenticating individuals prevent\
    \ misattribution?\n"
- title: 7.4.  General
  contents:
  - "7.4.  General\n   a.  Trade-offs.  Does the protocol make trade-offs between\
    \ privacy\n       and usability, privacy and efficiency, privacy and\n       implementability,\
    \ or privacy and other design goals?  Describe\n       the trade-offs and the\
    \ rationale for the design chosen.\n   b.  Defaults.  If the protocol can be operated\
    \ in multiple modes or\n       with multiple configurable options, does the default\
    \ mode or\n       option minimize the amount, identifiability, and persistence\
    \ of\n       the data and identifiers exposed by the protocol?  Does the\n   \
    \    default mode or option maximize the opportunity for user\n       participation?\
    \  Does it provide the strictest security features\n       of all the modes/options?\
    \  If the answer to any of these\n       questions is no, explain why less protective\
    \ defaults were\n       chosen.\n"
- title: 8.  Example
  contents:
  - "8.  Example\n   The following section gives an example of the threat analysis\
    \ and\n   threat mitigations recommended by this document.  It covers a\n   particularly\
    \ difficult application protocol, presence, to try to\n   demonstrate these principles\
    \ on an architecture that is vulnerable to\n   many of the threats described above.\
    \  This text is not intended as an\n   example of a privacy considerations section\
    \ that might appear in an\n   IETF specification, but rather as an example of\
    \ the thinking that\n   should go into the design of a protocol when considering\
    \ privacy as a\n   first principle.\n   A presence service, as defined in the\
    \ abstract in [RFC2778], allows\n   users of a communications service to monitor\
    \ one another's\n   availability and disposition in order to make decisions about\n\
    \   communicating.  Presence information is highly dynamic and generally\n   characterizes\
    \ whether a user is online or offline, busy or idle, away\n   from communications\
    \ devices or nearby, and the like.  Necessarily,\n   this information has certain\
    \ privacy implications, and from the start\n   the IETF approached this work with\
    \ the aim of providing users with\n   the controls to determine how their presence\
    \ information would be\n   shared.  The Common Profile for Presence (CPP) [RFC3859]\
    \ defines a\n   set of logical operations for delivery of presence information.\
    \  This\n   abstract model is applicable to multiple presence systems.  The SIP\n\
    \   for Instant Messaging and Presence Leveraging Extensions (SIMPLE)\n   presence\
    \ system [RFC3856] uses CPP as its baseline architecture, and\n   the presence\
    \ operations in the Extensible Messaging and Presence\n   Protocol (XMPP) have\
    \ also been mapped to CPP [RFC3922].\n   The fundamental architecture defined\
    \ in RFC 2778 and RFC 3859 is a\n   mediated one.  Clients (presentities in RFC\
    \ 2778 terms) publish their\n   presence information to presence servers, which\
    \ in turn distribute\n   information to authorized watchers.  Presence servers\
    \ thus retain\n   presence information for an interval of time, until it either\
    \ changes\n   or expires, so that it can be revealed to authorized watchers upon\n\
    \   request.  This architecture mirrors existing pre-standard deployment\n   models.\
    \  The integration of an explicit authorization mechanism into\n   the presence\
    \ architecture has been widely successful in involving the\n   end users in the\
    \ decision-making process before sharing information.\n   Nearly all presence\
    \ systems deployed today provide such a mechanism,\n   typically through a reciprocal\
    \ authorization system by which a pair\n   of users, when they agree to be \"\
    buddies\", consent to divulge their\n   presence information to one another. \
    \ Buddylists are managed by\n   servers but controlled by end users.  Users can\
    \ also explicitly block\n   one another through a similar interface, and in some\
    \ deployments it\n   is desirable to provide \"polite blocking\" of various kinds.\n\
    \   From a perspective of privacy design, however, the classical presence\n  \
    \ architecture represents nearly a worst-case scenario.  In terms of\n   data\
    \ minimization, presentities share their sensitive information\n   with presence\
    \ services, and while services only share this presence\n   information with watchers\
    \ authorized by the user, no technical\n   mechanism constrains those watchers\
    \ from relaying presence to further\n   third parties.  Any of these entities\
    \ could conceivably log or retain\n   presence information indefinitely.  The\
    \ sensitivity cannot be\n   mitigated by rendering the user anonymous, as it is\
    \ indeed the\n   purpose of the system to facilitate communications between users\
    \ who\n   know one another.  The identifiers employed by users are long-lived\n\
    \   and often contain personal information, including personal names and\n   the\
    \ domains of service providers.  While users do participate in the\n   construction\
    \ of buddylists and blacklists, they do so with little\n   prospect for accountability:\
    \ the user effectively throws their\n   presence information over the wall to\
    \ a presence server that in turn\n   distributes the information to watchers.\
    \  Users typically have no way\n   to verify that presence is being distributed\
    \ only to authorized\n   watchers, especially as it is the server that authenticates\
    \ watchers,\n   not the end user.  Moreover, connections between the server and\
    \ all\n   publishers and consumers of presence data are an attractive target\n\
    \   for eavesdroppers and require strong confidentiality mechanisms,\n   though\
    \ again the end user has no way to verify what mechanisms are in\n   place between\
    \ the presence server and a watcher.\n   Additionally, the sensitivity of presence\
    \ information is not limited\n   to the disposition and capability to communicate.\
    \  Capabilities can\n   reveal the type of device that a user employs, for example,\
    \ and since\n   multiple devices can publish the same user's presence, there are\n\
    \   significant risks of allowing attackers to correlate user devices.\n   An\
    \ important extension to presence was developed to enable the\n   support for\
    \ location sharing.  The effort to standardize protocols\n   for systems sharing\
    \ geolocation was started in the GEOPRIV working\n   group.  During the initial\
    \ requirements and privacy threat analysis\n   in the process of chartering the\
    \ working group, it became clear that\n   the system would require an underlying\
    \ communication mechanism\n   supporting user consent to share location information.\
    \  The\n   resemblance of these requirements to the presence framework was\n \
    \  quickly recognized, and this design decision was documented in\n   [RFC4079].\
    \  Location information thus mingles with other presence\n   information available\
    \ through the system to intermediaries and to\n   authorized watchers.\n   Privacy\
    \ concerns about presence information largely arise due to the\n   built-in mediation\
    \ of the presence architecture.  The need for a\n   presence server is motivated\
    \ by two primary design requirements of\n   presence: in the first place, the\
    \ server can respond with an\n   \"offline\" indication when the user is not online;\
    \ in the second\n   place, the server can compose presence information published\
    \ by\n   different devices under the user's control.  Additionally, to\n   facilitate\
    \ the use of URIs as identifiers for entities, some service\n   must operate a\
    \ host with the domain name appearing in a presence URI,\n   and in practical\
    \ terms no commercial presence architecture would\n   force end users to own and\
    \ operate their own domain names.  Many end\n   users of applications like presence\
    \ are behind NATs or firewalls and\n   effectively cannot receive direct connections\
    \ from the Internet --\n   the persistent bidirectional channel these clients\
    \ open and maintain\n   with a presence server is essential to the operation of\
    \ the protocol.\n   One must first ask if the trade-off of mediation for presence\
    \ is\n   worthwhile.  Does a server need to be in the middle of all\n   publications\
    \ of presence information?  It might seem that end-to-end\n   encryption of the\
    \ presence information could solve many of these\n   problems.  A presentity could\
    \ encrypt the presence information with\n   the public key of a watcher and only\
    \ then send the presence\n   information through the server.  The IETF defined\
    \ an object format\n   for presence information called the Presence Information\
    \ Data Format\n   (PIDF), which for the purposes of conveying location information\
    \ was\n   extended to the PIDF Location Object (PIDF-LO) -- these XML objects\n\
    \   were designed to accommodate an encrypted wrapper.  Encrypting this\n   data\
    \ would have the added benefit of preventing stored cleartext\n   presence information\
    \ from being seized by an attacker who manages to\n   compromise a presence server.\
    \  This proposal, however, quickly runs\n   into usability problems.  Discovering\
    \ the public keys of watchers is\n   the first difficulty, one that few Internet\
    \ protocols have addressed\n   successfully.  This solution would then require\
    \ the presentity to\n   publish one encrypted copy of its presence information\
    \ per authorized\n   watcher to the presence service, regardless of whether or\
    \ not a\n   watcher is actively seeking presence information -- for a presentity\n\
    \   with many watchers, this may place an unacceptable burden on the\n   presence\
    \ server, especially given the dynamism of presence\n   information.  Finally,\
    \ it prevents the server from composing presence\n   information reported by multiple\
    \ devices under the same user's\n   control.  On the whole, these difficulties\
    \ render object encryption\n   of presence information a doubtful prospect.\n\
    \   Some protocols that support presence information, such as SIP, can\n   operate\
    \ intermediaries in a redirecting mode rather than a publishing\n   or proxying\
    \ mode.  Instead of sending presence information through\n   the server, in other\
    \ words, these protocols can merely redirect\n   watchers to the presentity, and\
    \ then presence information could pass\n   directly and securely from the presentity\
    \ to the watcher.  It is\n   worth noting that this would disclose the IP address\
    \ of the\n   presentity to the watcher, which has its own set of risks.  In that\n\
    \   case, the presentity can decide exactly what information it would\n   like\
    \ to share with the watcher in question, it can authenticate the\n   watcher itself\
    \ with whatever strength of credential it chooses, and\n   with end-to-end encryption\
    \ it can reduce the likelihood of any\n   eavesdropping.  In a redirection architecture,\
    \ a presence server\n   could still provide the necessary \"offline\" indication\
    \ without\n   requiring the presence server to observe and forward all information\n\
    \   itself.  This mechanism is more promising than encryption but also\n   suffers\
    \ from significant difficulties.  It too does not provide for\n   composition\
    \ of presence information from multiple devices -- it in\n   fact forces the watcher\
    \ to perform this composition itself.  The\n   largest single impediment to this\
    \ approach is, however, the\n   difficulty of creating end-to-end connections\
    \ between the\n   presentity's device(s) and a watcher, as some or all of these\n\
    \   endpoints may be behind NATs or firewalls that prevent peer-to-peer\n   connections.\
    \  While there are potential solutions for this problem,\n   like Session Traversal\
    \ Utilities for NAT (STUN) and Traversal Using\n   Relays around NAT (TURN), they\
    \ add complexity to the overall system.\n   Consequently, mediation is a difficult\
    \ feature of the presence\n   architecture to remove.  It is hard to minimize\
    \ the data shared with\n   intermediaries, especially due to the requirement for\
    \ composition.\n   Control over sharing with intermediaries must therefore come\
    \ from\n   some other explicit component of the architecture.  As such, the\n\
    \   presence work in the IETF focused on improving user participation in\n   the\
    \ activities of the presence server.  This work began in the\n   GEOPRIV working\
    \ group, with controls on location privacy, as location\n   of users is perceived\
    \ as having especially sensitive properties.\n   With the aim of meeting the privacy\
    \ requirements defined in\n   [RFC2779], a set of usage indications, such as whether\
    \ retransmission\n   is allowed or when the retention period expires, have been\
    \ added to\n   the PIDF-LO such that they always travel with the location\n  \
    \ information itself.  These privacy preferences apply not only to the\n   intermediaries\
    \ that store and forward presence information but also\n   to the watchers who\
    \ consume it.\n   This approach very much follows the spirit of Creative Commons\
    \ [CC],\n   namely the usage of a limited number of conditions (such as 'Share\n\
    \   Alike' [CC-SA]).  Unlike Creative Commons, the GEOPRIV working group\n   did\
    \ not, however, initiate work to produce legal language or design\n   graphical\
    \ icons, since this would fall outside the scope of the IETF.\n   In particular,\
    \ the GEOPRIV rules state a preference on the retention\n   and retransmission\
    \ of location information; while GEOPRIV cannot\n   force any entity receiving\
    \ a PIDF-LO object to abide by those\n   preferences, if users lack the ability\
    \ to express them at all, we can\n   guarantee their preferences will not be honored.\
    \  The GEOPRIV rules\n   can provide a means to establish accountability.\n  \
    \ The retention and retransmission elements were envisioned as the most\n   essential\
    \ examples of preference expression in sharing presence.  The\n   PIDF object\
    \ was designed for extensibility, and the rulesets created\n   for the PIDF-LO\
    \ can also be extended to provide new expressions of\n   user preference.  Not\
    \ all user preference information should be bound\n   into a particular PIDF object,\
    \ however; many forms of access control\n   policy assumed by the presence architecture\
    \ need to be provisioned in\n   the presence server by some interface with the\
    \ user.  This\n   requirement eventually triggered the standardization of a general\n\
    \   access control policy language called the common policy framework\n   (defined\
    \ in [RFC4745]).  This language allows one to express ways to\n   control the\
    \ distribution of information as simple conditions,\n   actions, and transformation\
    \ rules expressed in an XML format.  Common\n   Policy itself is an abstract format\
    \ that needs to be instantiated:\n   two examples can be found with the presence\
    \ authorization rules\n   [RFC5025] and the Geolocation Policy [RFC6772].  The\
    \ former provides\n   additional expressiveness for presence-based systems, while\
    \ the\n   latter defines syntax and semantics for location-based conditions and\n\
    \   transformations.\n   Ultimately, the privacy work on presence represents a\
    \ compromise\n   between privacy principles and the needs of the architecture\
    \ and\n   marketplace.  While it was not feasible to remove intermediaries from\n\
    \   the architecture entirely or prevent their access to presence\n   information,\
    \ the IETF did provide a way for users to express their\n   preferences and provision\
    \ their controls at the presence service.  We\n   have not had great successes\
    \ in the implementation space with privacy\n   mechanisms thus far, but by documenting\
    \ and acknowledging the\n   limitations of these mechanisms, the designers were\
    \ able to provide\n   implementers, and end users, with an informed perspective\
    \ on the\n   privacy properties of the IETF's presence protocols.\n"
- title: 9.  Security Considerations
  contents:
  - "9.  Security Considerations\n   This document describes privacy aspects that\
    \ protocol designers\n   should consider in addition to regular security analysis.\n"
- title: 10.  Acknowledgements
  contents:
  - "10.  Acknowledgements\n   We would like to thank Christine Runnegar for her extensive\
    \ helpful\n   review comments.\n   We would like to thank Scott Brim, Kasey Chappelle,\
    \ Marc Linsner,\n   Bryan McLaughlin, Nick Mathewson, Eric Rescorla, Scott Bradner,\
    \ Nat\n   Sakimura, Bjoern Hoehrmann, David Singer, Dean Willis, Lucy Lynch,\n\
    \   Trent Adams, Mark Lizar, Martin Thomson, Josh Howlett, Mischa\n   Tuffield,\
    \ S. Moonesamy, Zhou Sujing, Claudia Diaz, Leif Johansson,\n   Jeff Hodges, Stephen\
    \ Farrell, Steven Johnston, Cullen Jennings, Ted\n   Hardie, Dave Thaler, Klaas\
    \ Wierenga, Adrian Farrel, Stephane\n   Bortzmeyer, Dave Crocker, and Hector Santos\
    \ for their useful feedback\n   on this document.\n   Finally, we would like to\
    \ thank the participants for the feedback\n   they provided during the December\
    \ 2010 Internet Privacy workshop\n   co-organized by MIT, ISOC, W3C, and the IAB.\n\
    \   Although John Morris is currently employed by the U.S. Government, he\n  \
    \ participated in the development of this document in his personal\n   capacity,\
    \ and the views expressed in the document may not reflect\n   those of his employer.\n"
- title: 11.  IAB Members at the Time of Approval
  contents:
  - "11.  IAB Members at the Time of Approval\n   Bernard Aboba\n   Jari Arkko\n \
    \  Marc Blanchet\n   Ross Callon\n   Alissa Cooper\n   Spencer Dawkins\n   Joel\
    \ Halpern\n   Russ Housley\n   Eliot Lear\n   Xing Li\n   Andrew Sullivan\n  \
    \ Dave Thaler\n   Hannes Tschofenig\n"
- title: 12.  Informative References
  contents:
  - "12.  Informative References\n   [CC-SA]    Creative Commons, \"Share Alike\"\
    , 2012,\n              <http://wiki.creativecommons.org/Share_Alike>.\n   [CC]\
    \       Creative Commons, \"Creative Commons\", 2012,\n              <http://creativecommons.org/>.\n\
    \   [CoE]      Council of Europe, \"Recommendation CM/Rec(2010)13 of the\n   \
    \           Committee of Ministers to member states on the protection\n      \
    \        of individuals with regard to automatic processing of\n             \
    \ personal data in the context of profiling\", November 2010,\n              <https://wcd.coe.int/ViewDoc.jsp?Ref=CM/Rec%282010%2913>.\n\
    \   [EFF]      Electronic Frontier Foundation, \"Panopticlick\", 2013,\n     \
    \         <http://panopticlick.eff.org>.\n   [FIPs]     Gellman, B., \"Fair Information\
    \ Practices: A Basic\n              History\", 2012,\n              <http://bobgellman.com/rg-docs/rg-FIPShistory.pdf>.\n\
    \   [OECD]     Organisation for Economic Co-operation and Development,\n     \
    \         \"OECD Guidelines on the Protection of Privacy and\n              Transborder\
    \ Flows of Personal Data\", (adopted 1980),\n              September 2010, <http://www.oecd.org/>.\n\
    \   [PbD]      Office of the Information and Privacy Commissioner,\n         \
    \     Ontario, Canada, \"Privacy by Design\", 2013,\n              <http://privacybydesign.ca/>.\n\
    \   [RFC2616]  Fielding, R., Gettys, J., Mogul, J., Frystyk, H.,\n           \
    \   Masinter, L., Leach, P., and T. Berners-Lee, \"Hypertext\n              Transfer\
    \ Protocol -- HTTP/1.1\", RFC 2616, June 1999.\n   [RFC2778]  Day, M., Rosenberg,\
    \ J., and H. Sugano, \"A Model for\n              Presence and Instant Messaging\"\
    , RFC 2778, February 2000.\n   [RFC2779]  Day, M., Aggarwal, S., Mohr, G., and\
    \ J. Vincent, \"Instant\n              Messaging / Presence Protocol Requirements\"\
    , RFC 2779,\n              February 2000.\n   [RFC3261]  Rosenberg, J., Schulzrinne,\
    \ H., Camarillo, G., Johnston,\n              A., Peterson, J., Sparks, R., Handley,\
    \ M., and E.\n              Schooler, \"SIP: Session Initiation Protocol\", RFC\
    \ 3261,\n              June 2002.\n   [RFC3325]  Jennings, C., Peterson, J., and\
    \ M. Watson, \"Private\n              Extensions to the Session Initiation Protocol\
    \ (SIP) for\n              Asserted Identity within Trusted Networks\", RFC 3325,\n\
    \              November 2002.\n   [RFC3552]  Rescorla, E. and B. Korver, \"Guidelines\
    \ for Writing RFC\n              Text on Security Considerations\", BCP 72, RFC\
    \ 3552,\n              July 2003.\n   [RFC3748]  Aboba, B., Blunk, L., Vollbrecht,\
    \ J., Carlson, J., and H.\n              Levkowetz, \"Extensible Authentication\
    \ Protocol (EAP)\",\n              RFC 3748, June 2004.\n   [RFC3856]  Rosenberg,\
    \ J., \"A Presence Event Package for the Session\n              Initiation Protocol\
    \ (SIP)\", RFC 3856, August 2004.\n   [RFC3859]  Peterson, J., \"Common Profile\
    \ for Presence (CPP)\",\n              RFC 3859, August 2004.\n   [RFC3922]  Saint-Andre,\
    \ P., \"Mapping the Extensible Messaging and\n              Presence Protocol\
    \ (XMPP) to Common Presence and Instant\n              Messaging (CPIM)\", RFC\
    \ 3922, October 2004.\n   [RFC4017]  Stanley, D., Walker, J., and B. Aboba, \"\
    Extensible\n              Authentication Protocol (EAP) Method Requirements for\n\
    \              Wireless LANs\", RFC 4017, March 2005.\n   [RFC4079]  Peterson,\
    \ J., \"A Presence Architecture for the\n              Distribution of GEOPRIV\
    \ Location Objects\", RFC 4079,\n              July 2005.\n   [RFC4101]  Rescorla,\
    \ E. and IAB, \"Writing Protocol Models\", RFC 4101,\n              June 2005.\n\
    \   [RFC4187]  Arkko, J. and H. Haverinen, \"Extensible Authentication\n     \
    \         Protocol Method for 3rd Generation Authentication and Key\n        \
    \      Agreement (EAP-AKA)\", RFC 4187, January 2006.\n   [RFC4282]  Aboba, B.,\
    \ Beadles, M., Arkko, J., and P. Eronen, \"The\n              Network Access Identifier\"\
    , RFC 4282, December 2005.\n   [RFC4745]  Schulzrinne, H., Tschofenig, H., Morris,\
    \ J., Cuellar, J.,\n              Polk, J., and J. Rosenberg, \"Common Policy:\
    \ A Document\n              Format for Expressing Privacy Preferences\", RFC 4745,\n\
    \              February 2007.\n   [RFC4918]  Dusseault, L., \"HTTP Extensions\
    \ for Web Distributed\n              Authoring and Versioning (WebDAV)\", RFC\
    \ 4918, June 2007.\n   [RFC4949]  Shirey, R., \"Internet Security Glossary, Version\
    \ 2\",\n              RFC 4949, August 2007.\n   [RFC5025]  Rosenberg, J., \"\
    Presence Authorization Rules\", RFC 5025,\n              December 2007.\n   [RFC5077]\
    \  Salowey, J., Zhou, H., Eronen, P., and H. Tschofenig,\n              \"Transport\
    \ Layer Security (TLS) Session Resumption without\n              Server-Side State\"\
    , RFC 5077, January 2008.\n   [RFC5106]  Tschofenig, H., Kroeselberg, D., Pashalidis,\
    \ A., Ohba, Y.,\n              and F. Bersani, \"The Extensible Authentication\
    \ Protocol-\n              Internet Key Exchange Protocol version 2 (EAP-IKEv2)\n\
    \              Method\", RFC 5106, February 2008.\n   [RFC5246]  Dierks, T. and\
    \ E. Rescorla, \"The Transport Layer Security\n              (TLS) Protocol Version\
    \ 1.2\", RFC 5246, August 2008.\n   [RFC6269]  Ford, M., Boucadair, M., Durand,\
    \ A., Levis, P., and P.\n              Roberts, \"Issues with IP Address Sharing\"\
    , RFC 6269,\n              June 2011.\n   [RFC6280]  Barnes, R., Lepinski, M.,\
    \ Cooper, A., Morris, J.,\n              Tschofenig, H., and H. Schulzrinne, \"\
    An Architecture for\n              Location and Location Privacy in Internet Applications\"\
    ,\n              BCP 160, RFC 6280, July 2011.\n   [RFC6302]  Durand, A., Gashinsky,\
    \ I., Lee, D., and S. Sheppard,\n              \"Logging Recommendations for Internet-Facing\
    \ Servers\",\n              BCP 162, RFC 6302, June 2011.\n   [RFC6350]  Perreault,\
    \ S., \"vCard Format Specification\", RFC 6350,\n              August 2011.\n\
    \   [RFC6562]  Perkins, C. and JM. Valin, \"Guidelines for the Use of\n      \
    \        Variable Bit Rate Audio with Secure RTP\", RFC 6562,\n              March\
    \ 2012.\n   [RFC6716]  Valin, JM., Vos, K., and T. Terriberry, \"Definition of\
    \ the\n              Opus Audio Codec\", RFC 6716, September 2012.\n   [RFC6772]\
    \  Schulzrinne, H., Tschofenig, H., Cuellar, J., Polk, J.,\n              Morris,\
    \ J., and M. Thomson, \"Geolocation Policy: A\n              Document Format for\
    \ Expressing Privacy Preferences for\n              Location Information\", RFC\
    \ 6772, January 2013.\n   [Solove]   Solove, D., \"Understanding Privacy\", March\
    \ 2010.\n   [Tor]      The Tor Project, Inc., \"Tor\", 2013,\n              <https://www.torproject.org/>.\n\
    \   [Westin]   Kumaraguru, P. and L. Cranor, \"Privacy Indexes: A Survey\n   \
    \           of Westin's Studies\", December 2005,\n              <http://reports-archive.adm.cs.cmu.edu/anon/isri2005/\n\
    \              CMU-ISRI-05-138.pdf>.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Alissa Cooper\n   CDT\n   1634 Eye St. NW, Suite 1100\n\
    \   Washington, DC  20006\n   US\n   Phone: +1-202-637-9800\n   EMail: acooper@cdt.org\n\
    \   URI:   http://www.cdt.org/\n   Hannes Tschofenig\n   Nokia Siemens Networks\n\
    \   Linnoitustie 6\n   Espoo  02600\n   Finland\n   Phone: +358 (50) 4871445\n\
    \   EMail: Hannes.Tschofenig@gmx.net\n   URI:   http://www.tschofenig.priv.at\n\
    \   Bernard Aboba\n   Skype\n   EMail: bernard_aboba@hotmail.com\n   Jon Peterson\n\
    \   NeuStar, Inc.\n   1800 Sutter St. Suite 570\n   Concord, CA  94520\n   US\n\
    \   EMail: jon.peterson@neustar.biz\n   John B. Morris, Jr.\n   EMail: ietf@jmorris.org\n\
    \   Marit Hansen\n   ULD\n   EMail: marit.hansen@datenschutzzentrum.de\n   Rhys\
    \ Smith\n   Janet\n   EMail: rhys.smith@ja.net\n"
