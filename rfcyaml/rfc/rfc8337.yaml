- title: __initial_text__
  contents:
  - '            Model-Based Metrics for Bulk Transport Capacity

    '
- title: Abstract
  contents:
  - "Abstract\n   This document introduces a new class of Model-Based Metrics designed\n\
    \   to assess if a complete Internet path can be expected to meet a\n   predefined\
    \ Target Transport Performance by applying a suite of IP\n   diagnostic tests\
    \ to successive subpaths.  The subpath-at-a-time tests\n   can be robustly applied\
    \ to critical infrastructure, such as network\n   interconnections or even individual\
    \ devices, to accurately detect if\n   any part of the infrastructure will prevent\
    \ paths traversing it from\n   meeting the Target Transport Performance.\n   Model-Based\
    \ Metrics rely on mathematical models to specify a Targeted\n   IP Diagnostic\
    \ Suite, a set of IP diagnostic tests designed to assess\n   whether common transport\
    \ protocols can be expected to meet a\n   predetermined Target Transport Performance\
    \ over an Internet path.\n   For Bulk Transport Capacity, the IP diagnostics are\
    \ built using test\n   streams and statistical criteria for evaluating the packet\
    \ transfer\n   that mimic TCP over the complete path.  The temporal structure\
    \ of the\n   test stream (e.g., bursts) mimics TCP or other transport protocols\n\
    \   carrying bulk data over a long path.  However, they are constructed\n   to\
    \ be independent of the details of the subpath under test, end\n   systems, or\
    \ applications.  Likewise, the success criteria evaluates\n   the packet transfer\
    \ statistics of the subpath against criteria\n   determined by protocol performance\
    \ models applied to the Target\n   Transport Performance of the complete path.\
    \  The success criteria\n   also does not depend on the details of the subpath,\
    \ end systems, or\n   applications.\n"
- title: Status of This Memo
  contents:
  - "Status of This Memo\n   This document is not an Internet Standards Track specification;\
    \ it is\n   published for examination, experimental implementation, and\n   evaluation.\n\
    \   This document defines an Experimental Protocol for the Internet\n   community.\
    \  This document is a product of the Internet Engineering\n   Task Force (IETF).\
    \  It represents the consensus of the IETF\n   community.  It has received public\
    \ review and has been approved for\n   publication by the Internet Engineering\
    \ Steering Group (IESG).  Not\n   all documents approved by the IESG are candidates\
    \ for any level of\n   Internet Standard; see Section 2 of RFC 7841.\n   Information\
    \ about the current status of this document, any errata,\n   and how to provide\
    \ feedback on it may be obtained at\n   https://www.rfc-editor.org/info/rfc8337.\n"
- title: Copyright Notice
  contents:
  - "Copyright Notice\n   Copyright (c) 2018 IETF Trust and the persons identified\
    \ as the\n   document authors.  All rights reserved.\n   This document is subject\
    \ to BCP 78 and the IETF Trust's Legal\n   Provisions Relating to IETF Documents\n\
    \   (https://trustee.ietf.org/license-info) in effect on the date of\n   publication\
    \ of this document.  Please review these documents\n   carefully, as they describe\
    \ your rights and restrictions with respect\n   to this document.  Code Components\
    \ extracted from this document must\n   include Simplified BSD License text as\
    \ described in Section 4.e of\n   the Trust Legal Provisions and are provided\
    \ without warranty as\n   described in the Simplified BSD License.\n"
- title: Table of Contents
  contents:
  - "Table of Contents\n   1. Introduction ....................................................4\n\
    \   2. Overview ........................................................5\n  \
    \ 3. Terminology .....................................................8\n    \
    \  3.1. General Terminology ........................................8\n      3.2.\
    \ Terminology about Paths ...................................10\n      3.3. Properties\
    \ ................................................11\n      3.4. Basic Parameters\
    \ ..........................................12\n      3.5. Ancillary Parameters\
    \ ......................................13\n      3.6. Temporal Patterns for Test\
    \ Streams ........................14\n      3.7. Tests .....................................................15\n\
    \   4. Background .....................................................16\n  \
    \    4.1. TCP Properties ............................................18\n    \
    \  4.2. Diagnostic Approach .......................................20\n      4.3.\
    \ New Requirements Relative to RFC 2330 .....................21\n   5. Common\
    \ Models and Parameters ...................................22\n      5.1. Target\
    \ End-to-End Parameters ..............................22\n      5.2. Common Model\
    \ Calculations .................................22\n      5.3. Parameter Derating\
    \ ........................................23\n      5.4. Test Preconditions ........................................24\n\
    \   6. Generating Test Streams ........................................24\n  \
    \    6.1. Mimicking Slowstart .......................................25\n    \
    \  6.2. Constant Window Pseudo CBR ................................27\n      6.3.\
    \ Scanned Window Pseudo CBR .................................28\n      6.4. Concurrent\
    \ or Channelized Testing .........................28\n   7. Interpreting the Results\
    \ .......................................29\n      7.1. Test Outcomes .............................................29\n\
    \      7.2. Statistical Criteria for Estimating run_length ............31\n  \
    \    7.3. Reordering Tolerance ......................................33\n   8.\
    \ IP Diagnostic Tests ............................................34\n      8.1.\
    \ Basic Data Rate and Packet Transfer Tests .................34\n           8.1.1.\
    \ Delivery Statistics at Paced Full Data Rate ........35\n           8.1.2. Delivery\
    \ Statistics at Full Data Windowed Rate .....35\n           8.1.3. Background\
    \ Packet Transfer Statistics Tests ........35\n      8.2. Standing Queue Tests\
    \ ......................................36\n           8.2.1. Congestion Avoidance\
    \ ...............................37\n           8.2.2. Bufferbloat ........................................37\n\
    \           8.2.3. Non-excessive Loss .................................38\n  \
    \         8.2.4. Duplex Self-Interference ...........................38\n    \
    \  8.3. Slowstart Tests ...........................................39\n      \
    \     8.3.1. Full Window Slowstart Test .........................39\n        \
    \   8.3.2. Slowstart AQM Test .................................39\n      8.4.\
    \ Sender Rate Burst Tests ...................................40\n      8.5. Combined\
    \ and Implicit Tests ...............................41\n           8.5.1. Sustained\
    \ Full-Rate Bursts Test ....................41\n           8.5.2. Passive Measurements\
    \ ...............................42\n   9. Example ........................................................43\n\
    \      9.1. Observations about Applicability ..........................44\n  \
    \ 10. Validation ....................................................45\n   11.\
    \ Security Considerations .......................................46\n   12. IANA\
    \ Considerations ...........................................47\n   13. Informative\
    \ References ........................................47\n   Appendix A.  Model\
    \ Derivations ....................................52\n     A.1.  Queueless Reno\
    \ ............................................52\n   Appendix B.  The Effects\
    \ of ACK Scheduling ........................53\n   Acknowledgments ...................................................55\n\
    \   Authors' Addresses ................................................55\n"
- title: 1.  Introduction
  contents:
  - "1.  Introduction\n   Model-Based Metrics (MBM) rely on peer-reviewed mathematical\
    \ models\n   to specify a Targeted IP Diagnostic Suite (TIDS), a set of IP\n \
    \  diagnostic tests designed to assess whether common transport\n   protocols\
    \ can be expected to meet a predetermined Target Transport\n   Performance over\
    \ an Internet path.  This document describes the\n   modeling framework to derive\
    \ the test parameters for assessing an\n   Internet path's ability to support\
    \ a predetermined Bulk Transport\n   Capacity.\n   Each test in TIDS measures\
    \ some aspect of IP packet transfer needed\n   to meet the Target Transport Performance.\
    \  For Bulk Transport\n   Capacity, the TIDS includes IP diagnostic tests to verify\
    \ that there\n   is sufficient IP capacity (data rate), sufficient queue space\
    \ at\n   bottlenecks to absorb and deliver typical transport bursts, low\n   enough\
    \ background packet loss ratio to not interfere with congestion\n   control, and\
    \ other properties described below.  Unlike typical IP\n   Performance Metrics\
    \ (IPPM) that yield measures of network properties,\n   Model-Based Metrics nominally\
    \ yield pass/fail evaluations of the\n   ability of standard transport protocols\
    \ to meet the specific\n   performance objective over some network path.\n   In\
    \ most cases, the IP diagnostic tests can be implemented by\n   combining existing\
    \ IPPM metrics with additional controls for\n   generating test streams having\
    \ a specified temporal structure (bursts\n   or standing queues caused by constant\
    \ bit rate streams, etc.) and\n   statistical criteria for evaluating packet transfer.\
    \  The temporal\n   structure of the test streams mimics transport protocol behavior\
    \ over\n   the complete path; the statistical criteria models the transport\n\
    \   protocol's response to less-than-ideal IP packet transfer.  In\n   control\
    \ theory terms, the tests are \"open loop\".  Note that running a\n   test requires\
    \ the coordinated activity of sending and receiving\n   measurement points.\n\
    \   This document addresses Bulk Transport Capacity.  It describes an\n   alternative\
    \ to the approach presented in \"A Framework for Defining\n   Empirical Bulk Transfer\
    \ Capacity Metrics\" [RFC3148].  Other Model-\n   Based Metrics may cover other\
    \ applications and transports, such as\n   Voice over IP (VoIP) over UDP, RTP,\
    \ and new transport protocols.\n   This document assumes a traditional Reno TCP-style,\
    \ self-clocked,\n   window-controlled transport protocol that uses packet loss\
    \ and\n   Explicit Congestion Notification (ECN) Congestion Experienced (CE)\n\
    \   marks for congestion feedback.  There are currently some experimental\n  \
    \ protocols and congestion control algorithms that are rate based or\n   otherwise\
    \ fall outside of these assumptions.  In the future, these\n   new protocols and\
    \ algorithms may call for revised models.\n   The MBM approach, i.e., mapping\
    \ Target Transport Performance to a\n   Targeted IP Diagnostic Suite (TIDS) of\
    \ IP tests, solves some\n   intrinsic problems with using TCP or other throughput-maximizing\n\
    \   protocols for measurement.  In particular, all throughput-maximizing\n   protocols\
    \ (especially TCP congestion control) cause some level of\n   congestion in order\
    \ to detect when they have reached the available\n   capacity limitation of the\
    \ network.  This self-inflicted congestion\n   obscures the network properties\
    \ of interest and introduces non-linear\n   dynamic equilibrium behaviors that\
    \ make any resulting measurements\n   useless as metrics because they have no\
    \ predictive value for\n   conditions or paths different from that of the measurement\
    \ itself.\n   In order to prevent these effects, it is necessary to avoid the\n\
    \   effects of TCP congestion control in the measurement method.  These\n   issues\
    \ are discussed at length in Section 4.  Readers who are\n   unfamiliar with basic\
    \ properties of TCP and TCP-like congestion\n   control may find it easier to\
    \ start at Section 4 or 4.1.\n   A Targeted IP Diagnostic Suite does not have\
    \ such difficulties.  IP\n   diagnostics can be constructed such that they make\
    \ strong statistical\n   statements about path properties that are independent\
    \ of measurement\n   details, such as vantage and choice of measurement points.\n"
- title: 2.  Overview
  contents:
  - "2.  Overview\n   This document describes a modeling framework for deriving a\
    \ Targeted\n   IP Diagnostic Suite from a predetermined Target Transport\n   Performance.\
    \  It is not a complete specification and relies on other\n   standards documents\
    \ to define important details such as packet type-P\n   selection, sampling techniques,\
    \ vantage selection, etc.  Fully\n   Specified Targeted IP Diagnostic Suites (FSTIDSs)\
    \ define all of these\n   details.  A Targeted IP Diagnostic Suite (TIDS) refers\
    \ to the subset\n   of such a specification that is in scope for this document.\
    \  This\n   terminology is further defined in Section 3.\n   Section 4 describes\
    \ some key aspects of TCP behavior and what they\n   imply about the requirements\
    \ for IP packet transfer.  Most of the IP\n   diagnostic tests needed to confirm\
    \ that the path meets these\n   properties can be built on existing IPPM metrics,\
    \ with the addition\n   of statistical criteria for evaluating packet transfer\
    \ and, in a few\n   cases, new mechanisms to implement the required temporal structure.\n\
    \   (One group of tests, the standing queue tests described in\n   Section 8.2,\
    \ don't correspond to existing IPPM metrics, but suitable\n   new IPPM metrics\
    \ can be patterned after the existing definitions.)\n   Figure 1 shows the MBM\
    \ modeling and measurement framework.  The\n   Target Transport Performance at\
    \ the top of the figure is determined\n   by the needs of the user or application,\
    \ which are outside the scope\n   of this document.  For Bulk Transport Capacity,\
    \ the main performance\n   parameter of interest is the Target Data Rate.  However,\
    \ since TCP's\n   ability to compensate for less-than-ideal network conditions\
    \ is\n   fundamentally affected by the Round-Trip Time (RTT) and the Maximum\n\
    \   Transmission Unit (MTU) of the complete path, these parameters must\n   also\
    \ be specified in advance based on knowledge about the intended\n   application\
    \ setting.  They may reflect a specific application over a\n   real path through\
    \ the Internet or an idealized application and\n   hypothetical path representing\
    \ a typical user community.  Section 5\n   describes the common parameters and\
    \ models derived from the Target\n   Transport Performance.\n                \
    \      Target Transport Performance\n            (Target Data Rate, Target RTT,\
    \ and Target MTU)\n                                   |\n                    \
    \       ________V_________\n                           |  mathematical  |\n  \
    \                         |     models     |\n                           |   \
    \             |\n                           ------------------\n          Traffic\
    \ parameters |            | Statistical criteria\n                           \
    \  |            |\n                      _______V____________V____Targeted IP____\n\
    \                     |       |   * * *    | Diagnostic Suite  |\n           \
    \     _____|_______V____________V________________   |\n              __|____________V____________V______________\
    \  |  |\n              |           IP diagnostic tests            | |  |\n   \
    \           |              |            |              | |  |\n              |\
    \ _____________V__        __V____________  | |  |\n              | |   traffic\
    \    |        |   Delivery  |  | |  |\n              | |   pattern    |      \
    \  |  Evaluation |  | |  |\n              | |  generation  |        |        \
    \     |  | |  |\n              | -------v--------        ------^--------  | |\
    \  |\n              |   |    v    test stream via   ^      |   | |--\n       \
    \       |   |  -->======================>--    |   | |\n              |   |  \
    \     subpath under test         |   |-\n              ----V----------------------------------V---\
    \ |\n                  | |  |                             | |  |\n           \
    \       V V  V                             V V  V\n              fail/inconclusive\
    \            pass/fail/inconclusive\n          (traffic generation status)   \
    \        (test result)\n                   Figure 1: Overall Modeling Framework\n\
    \   Mathematical TCP models are used to determine traffic parameters and\n   subsequently\
    \ to design traffic patterns that mimic TCP (which has\n   burst characteristics\
    \ at multiple time scales) or other transport\n   protocols delivering bulk data\
    \ and operating at the Target Data Rate,\n   MTU, and RTT over a full range of\
    \ conditions.  Using the techniques\n   described in Section 6, the traffic patterns\
    \ are generated based on\n   the three Target parameters of the complete path\
    \ (Target Data Rate,\n   Target RTT, and Target MTU), independent of the properties\
    \ of\n   individual subpaths.  As much as possible, the test streams are\n   generated\
    \ deterministically (precomputed) to minimize the extent to\n   which test methodology,\
    \ measurement points, measurement vantage, or\n   path partitioning affect the\
    \ details of the measurement traffic.\n   Section 7 describes packet transfer\
    \ statistics and methods to test\n   against the statistical criteria provided\
    \ by the mathematical models.\n   Since the statistical criteria typically apply\
    \ to the complete path\n   (a composition of subpaths) [RFC6049], in situ testing\
    \ requires that\n   the end-to-end statistical criteria be apportioned as separate\n\
    \   criteria for each subpath.  Subpaths that are expected to be\n   bottlenecks\
    \ would then be permitted to contribute a larger fraction\n   of the end-to-end\
    \ packet loss budget.  In compensation, subpaths that\n   are not expected to\
    \ exhibit bottlenecks must be constrained to\n   contribute less packet loss.\
    \  Thus, the statistical criteria for each\n   subpath in each test of a TIDS\
    \ is an apportioned share of the end-to-\n   end statistical criteria for the\
    \ complete path that was determined by\n   the mathematical model.\n   Section\
    \ 8 describes the suite of individual tests needed to verify\n   all of the required\
    \ IP delivery properties.  A subpath passes if and\n   only if all of the individual\
    \ IP diagnostic tests pass.  Any subpath\n   that fails any test indicates that\
    \ some users are likely to fail to\n   attain their Target Transport Performance\
    \ under some conditions.  In\n   addition to passing or failing, a test can be\
    \ deemed inconclusive for\n   a number of reasons, including the following: the\
    \ precomputed traffic\n   pattern was not accurately generated, the measurement\
    \ results were\n   not statistically significant, the test failed to meet some\
    \ required\n   test preconditions, etc.  If all tests pass but some are\n   inconclusive,\
    \ then the entire suite is deemed to be inconclusive.\n   In Section 9, we present\
    \ an example TIDS that might be representative\n   of High Definition (HD) video\
    \ and illustrate how Model-Based Metrics\n   can be used to address difficult\
    \ measurement situations, such as\n   confirming that inter-carrier exchanges\
    \ have sufficient performance\n   and capacity to deliver HD video between ISPs.\n\
    \   Since there is some uncertainty in the modeling process, Section 10\n   describes\
    \ a validation procedure to diagnose and minimize false\n   positive and false\
    \ negative results.\n"
- title: 3.  Terminology
  contents:
  - "3.  Terminology\n   Terms containing underscores (rather than spaces) appear\
    \ in equations\n   and typically have algorithmic definitions.\n"
- title: 3.1.  General Terminology
  contents:
  - "3.1.  General Terminology\n   Target:  A general term for any parameter specified\
    \ by or derived\n      from the user's application or transport performance requirements.\n\
    \   Target Transport Performance:  Application or transport performance\n    \
    \  target values for the complete path.  For Bulk Transport Capacity\n      defined\
    \ in this document, the Target Transport Performance\n      includes the Target\
    \ Data Rate, Target RTT, and Target MTU as\n      described below.\n   Target\
    \ Data Rate:  The specified application data rate required for\n      an application's\
    \ proper operation.  Conventional Bulk Transport\n      Capacity (BTC) metrics\
    \ are focused on the Target Data Rate;\n      however, these metrics have little\
    \ or no predictive value because\n      they do not consider the effects of the\
    \ other two parameters of\n      the Target Transport Performance -- the RTT and\
    \ MTU of the\n      complete paths.\n   Target RTT (Round-Trip Time):  The specified\
    \ baseline (minimum) RTT\n      of the longest complete path over which the user\
    \ expects to be\n      able to meet the target performance.  TCP and other transport\n\
    \      protocol's ability to compensate for path problems is generally\n     \
    \ proportional to the number of round trips per second.  The Target\n      RTT\
    \ determines both key parameters of the traffic patterns (e.g.,\n      burst sizes)\
    \ and the thresholds on acceptable IP packet transfer\n      statistics.  The\
    \ Target RTT must be specified considering\n      appropriate packets sizes: MTU-sized\
    \ packets on the forward path\n      and ACK-sized packets (typically, header_overhead)\
    \ on the return\n      path.  Note that Target RTT is specified and not measured;\
    \ MBM\n      measurements derived for a given target_RTT will be applicable to\n\
    \      any path with a smaller RTT.\n   Target MTU (Maximum Transmission Unit):\
    \  The specified maximum MTU\n      supported by the complete path over which\
    \ the application expects\n      to meet the target performance.  In this document,\
    \ we assume a\n      1500-byte MTU unless otherwise specified.  If a subpath has\
    \ a\n      smaller MTU, then it becomes the Target MTU for the complete path,\n\
    \      and all model calculations and subpath tests must use the same\n      smaller\
    \ MTU.\n   Targeted IP Diagnostic Suite (TIDS):  A set of IP diagnostic tests\n\
    \      designed to determine if an otherwise ideal complete path\n      containing\
    \ the subpath under test can sustain flows at a specific\n      target_data_rate\
    \ using packets with a size of target_MTU when the\n      RTT of the complete\
    \ path is target_RTT.\n   Fully Specified Targeted IP Diagnostic Suite (FSTIDS):\
    \  A TIDS\n      together with additional specifications such as measurement packet\n\
    \      type (\"type-p\" [RFC2330]) that are out of scope for this document\n \
    \     and need to be drawn from other standards documents.\n   Bulk Transport\
    \ Capacity (BTC):  Bulk Transport Capacity metrics\n      evaluate an Internet\
    \ path's ability to carry bulk data, such as\n      large files, streaming (non-real-time)\
    \ video, and, under some\n      conditions, web images and other content.  Prior\
    \ efforts to define\n      BTC metrics have been based on [RFC3148], which predates\
    \ our\n      understanding of TCP and the requirements described in Section 4.\n\
    \      In general, \"Bulk Transport\" indicates that performance is\n      determined\
    \ by the interplay between the network, cross traffic,\n      and congestion control\
    \ in the transport protocol.  It excludes\n      situations where performance\
    \ is dominated by the RTT alone (e.g.,\n      transactions) or bottlenecks elsewhere,\
    \ such as in the application\n      itself.\n   IP diagnostic tests:  Measurements\
    \ or diagnostics to determine if\n      packet transfer statistics meet some precomputed\
    \ target.\n   traffic patterns:  The temporal patterns or burstiness of traffic\n\
    \      generated by applications over transport protocols such as TCP.\n     \
    \ There are several mechanisms that cause bursts at various\n      timescales\
    \ as described in Section 4.1.  Our goal here is to mimic\n      the range of\
    \ common patterns (burst sizes, rates, etc.), without\n      tying our applicability\
    \ to specific applications, implementations,\n      or technologies, which are\
    \ sure to become stale.\n   Explicit Congestion Notification (ECN):  See [RFC3168].\n\
    \   packet transfer statistics:  Raw, detailed, or summary statistics\n      about\
    \ packet transfer properties of the IP layer including packet\n      losses, ECN\
    \ Congestion Experienced (CE) marks, reordering, or any\n      other properties\
    \ that may be germane to transport performance.\n   packet loss ratio:  As defined\
    \ in [RFC7680].\n   apportioned:  To divide and allocate, for example, budgeting\
    \ packet\n      loss across multiple subpaths such that the losses will accumulate\n\
    \      to less than a specified end-to-end loss ratio.  Apportioning\n      metrics\
    \ is essentially the inverse of the process described in\n      [RFC5835].\n \
    \  open loop:  A control theory term used to describe a class of\n      techniques\
    \ where systems that naturally exhibit circular\n      dependencies can be analyzed\
    \ by suppressing some of the\n      dependencies, such that the resulting dependency\
    \ graph is acyclic.\n"
- title: 3.2.  Terminology about Paths
  contents:
  - "3.2.  Terminology about Paths\n   See [RFC2330] and [RFC7398] for existing terms\
    \ and definitions.\n   data sender:  Host sending data and receiving ACKs.\n \
    \  data receiver:  Host receiving data and sending ACKs.\n   complete path:  The\
    \ end-to-end path from the data sender to the data\n      receiver.\n   subpath:\
    \  A portion of the complete path.  Note that there is no\n      requirement that\
    \ subpaths be non-overlapping.  A subpath can be as\n      small as a single device,\
    \ link, or interface.\n   measurement point:  Measurement points as described\
    \ in [RFC7398].\n   test path:  A path between two measurement points that includes\
    \ a\n      subpath of the complete path under test.  If the measurement\n    \
    \  points are off path, the test path may include \"test leads\"\n      between\
    \ the measurement points and the subpath.\n   dominant bottleneck:  The bottleneck\
    \ that generally determines most\n      packet transfer statistics for the entire\
    \ path.  It typically\n      determines a flow's self-clock timing, packet loss,\
    \ and ECN CE\n      marking rate, with other potential bottlenecks having less\
    \ effect\n      on the packet transfer statistics.  See Section 4.1 on TCP\n \
    \     properties.\n   front path:  The subpath from the data sender to the dominant\n\
    \      bottleneck.\n   back path:  The subpath from the dominant bottleneck to\
    \ the receiver.\n   return path:  The path taken by the ACKs from the data receiver\
    \ to\n      the data sender.\n   cross traffic:  Other, potentially interfering,\
    \ traffic competing for\n      network resources (such as bandwidth and/or queue\
    \ capacity).\n"
- title: 3.3.  Properties
  contents:
  - "3.3.  Properties\n   The following properties are determined by the complete\
    \ path and\n   application.  These are described in more detail in Section 5.1.\n\
    \   Application Data Rate:  General term for the data rate as seen by the\n  \
    \    application above the transport layer in bytes per second.  This\n      is\
    \ the payload data rate and explicitly excludes transport-level\n      and lower-level\
    \ headers (TCP/IP or other protocols),\n      retransmissions, and other overhead\
    \ that is not part of the total\n      quantity of data delivered to the application.\n\
    \   IP rate:  The actual number of IP-layer bytes delivered through a\n      subpath,\
    \ per unit time, including TCP and IP headers, retransmits,\n      and other TCP/IP\
    \ overhead.  This is the same as IP-type-P Link\n      Usage in [RFC5136].\n \
    \  IP capacity:  The maximum number of IP-layer bytes that can be\n      transmitted\
    \ through a subpath, per unit time, including TCP and IP\n      headers, retransmits,\
    \ and other TCP/IP overhead.  This is the same\n      as IP-type-P Link Capacity\
    \ in [RFC5136].\n   bottleneck IP capacity:  The IP capacity of the dominant bottleneck\n\
    \      in the forward path.  All throughput-maximizing protocols estimate\n  \
    \    this capacity by observing the IP rate delivered through the\n      bottleneck.\
    \  Most protocols derive their self-clocks from the\n      timing of this data.\
    \  See Section 4.1 and Appendix B for more\n      details.\n   implied bottleneck\
    \ IP capacity:  The bottleneck IP capacity implied\n      by the ACKs returning\
    \ from the receiver.  It is determined by\n      looking at how much application\
    \ data the ACK stream at the sender\n      reports as delivered to the data receiver\
    \ per unit time at various\n      timescales.  If the return path is thinning,\
    \ batching, or\n      otherwise altering the ACK timing, the implied bottleneck\
    \ IP\n      capacity over short timescales might be substantially larger than\n\
    \      the bottleneck IP capacity averaged over a full RTT.  Since TCP\n     \
    \ derives its clock from the data delivered through the bottleneck,\n      the\
    \ front path must have sufficient buffering to absorb any data\n      bursts at\
    \ the dimensions (size and IP rate) implied by the ACK\n      stream, which are\
    \ potentially doubled during slowstart.  If the\n      return path is not altering\
    \ the ACK stream, then the implied\n      bottleneck IP capacity will be the same\
    \ as the bottleneck IP\n      capacity.  See Section 4.1 and Appendix B for more\
    \ details.\n   sender interface rate:  The IP rate that corresponds to the IP\n\
    \      capacity of the data sender's interface.  Due to sender efficiency\n  \
    \    algorithms, including technologies such as TCP segmentation\n      offload\
    \ (TSO), nearly all modern servers deliver data in bursts at\n      full interface\
    \ link rate.  Today, 1 or 10 Gb/s are typical.\n   header_overhead:  The IP and\
    \ TCP header sizes, which are the portion\n      of each MTU not available for\
    \ carrying application payload.\n      Without loss of generality, this is assumed\
    \ to be the size for\n      returning acknowledgments (ACKs).  For TCP, the Maximum\
    \ Segment\n      Size (MSS) is the Target MTU minus the header_overhead.\n"
- title: 3.4.  Basic Parameters
  contents:
  - "3.4.  Basic Parameters\n   Basic parameters common to models and subpath tests\
    \ are defined here.\n   Formulas for target_window_size and target_run_length\
    \ appear in\n   Section 5.2.  Note that these are mixed between application transport\n\
    \   performance (excludes headers) and IP performance (includes TCP\n   headers\
    \ and retransmissions as part of the IP payload).\n   Network power:  The observed\
    \ data rate divided by the observed RTT.\n      Network power indicates how effectively\
    \ a transport protocol is\n      filling a network.\n   Window [size]:  The total\
    \ quantity of data carried by packets\n      in-flight plus the data represented\
    \ by ACKs circulating in the\n      network is referred to as the window.  See\
    \ Section 4.1.  Sometimes\n      used with other qualifiers (congestion window\
    \ (cwnd) or receiver\n      window) to indicate which mechanism is controlling\
    \ the window.\n   pipe size:  A general term for the number of packets needed\
    \ in flight\n      (the window size) to exactly fill a network path or subpath.\
    \  It\n      corresponds to the window size, which maximizes network power.  It\n\
    \      is often used with additional qualifiers to specify which path,\n     \
    \ under what conditions, etc.\n   target_window_size:  The average number of packets\
    \ in flight (the\n      window size) needed to meet the Target Data Rate for the\
    \ specified\n      Target RTT and Target MTU.  It implies the scale of the bursts\n\
    \      that the network might experience.\n   run length:  A general term for\
    \ the observed, measured, or specified\n      number of packets that are (expected\
    \ to be) delivered between\n      losses or ECN CE marks.  Nominally, it is one\
    \ over the sum of the\n      loss and ECN CE marking probabilities, if they are\
    \ independently\n      and identically distributed.\n   target_run_length:  The\
    \ target_run_length is an estimate of the\n      minimum number of non-congestion\
    \ marked packets needed between\n      losses or ECN CE marks necessary to attain\
    \ the target_data_rate\n      over a path with the specified target_RTT and target_MTU,\
    \ as\n      computed by a mathematical model of TCP congestion control.  A\n \
    \     reference calculation is shown in Section 5.2 and alternatives in\n    \
    \  Appendix A.\n   reference target_run_length:  target_run_length computed precisely\
    \ by\n      the method in Section 5.2.  This is likely to be slightly more\n \
    \     conservative than required by modern TCP implementations.\n"
- title: 3.5.  Ancillary Parameters
  contents:
  - "3.5.  Ancillary Parameters\n   The following ancillary parameters are used for\
    \ some tests:\n   derating:  Under some conditions, the standard models are too\n\
    \      conservative.  The modeling framework permits some latitude in\n      relaxing\
    \ or \"derating\" some test parameters, as described in\n      Section 5.3, in\
    \ exchange for a more stringent TIDS validation\n      procedures, described in\
    \ Section 10.  Models can be derated by\n      including a multiplicative derating\
    \ factor to make tests less\n      stringent.\n   subpath_IP_capacity:  The IP\
    \ capacity of a specific subpath.\n   test path:  A subpath of a complete path\
    \ under test.\n   test_path_RTT:  The RTT observed between two measurement points\
    \ using\n      packet sizes that are consistent with the transport protocol.\n\
    \      This is generally MTU-sized packets of the forward path and\n      packets\
    \ with a size of header_overhead on the return path.\n   test_path_pipe:  The\
    \ pipe size of a test path.  Nominally, it is the\n      test_path_RTT times the\
    \ test path IP_capacity.\n   test_window:  The smallest window sufficient to meet\
    \ or exceed the\n      target_rate when operating with a pure self-clock over\
    \ a test\n      path.  The test_window is typically calculated as follows (but\
    \ see\n      the discussion in Appendix B about the effects of channel\n     \
    \ scheduling on RTT):\n      ceiling(target_data_rate * test_path_RTT / (target_MTU\
    \ -\n      header_overhead))\n      On some test paths, the test_window may need\
    \ to be adjusted\n      slightly to compensate for the RTT being inflated by the\
    \ devices\n      that schedule packets.\n"
- title: 3.6.  Temporal Patterns for Test Streams
  contents:
  - "3.6.  Temporal Patterns for Test Streams\n   The terminology below is used to\
    \ define temporal patterns for test\n   streams.  These patterns are designed\
    \ to mimic TCP behavior, as\n   described in Section 4.1.\n   packet headway:\
    \  Time interval between packets, specified from the\n      start of one to the\
    \ start of the next.  For example, if packets\n      are sent with a 1 ms headway,\
    \ there will be exactly 1000 packets\n      per second.\n   burst headway:  Time\
    \ interval between bursts, specified from the\n      start of the first packet\
    \ of one burst to the start of the first\n      packet of the next burst.  For\
    \ example, if 4 packet bursts are\n      sent with a 1 ms burst headway, there\
    \ will be exactly 4000 packets\n      per second.\n   paced single packets:  Individual\
    \ packets sent at the specified rate\n      or packet headway.\n   paced bursts:\
    \  Bursts on a timer.  Specify any 3 of the following:\n      average data rate,\
    \ packet size, burst size (number of packets),\n      and burst headway (burst\
    \ start to start).  By default, the bursts\n      are assumed to occur at full\
    \ sender interface rate, such that the\n      packet headway within each burst\
    \ is the minimum supported by the\n      sender's interface.  Under some conditions,\
    \ it is useful to\n      explicitly specify the packet headway within each burst.\n\
    \   slowstart rate:  Paced bursts of four packets each at an average data\n  \
    \    rate equal to twice the implied bottleneck IP capacity (but not\n      more\
    \ than the sender interface rate).  This mimics TCP slowstart.\n      This is\
    \ a two-level burst pattern described in more detail in\n      Section 6.1.  If\
    \ the implied bottleneck IP capacity is more than\n      half of the sender interface\
    \ rate, the slowstart rate becomes the\n      sender interface rate.\n   slowstart\
    \ burst:  A specified number of packets in a two-level burst\n      pattern that\
    \ resembles slowstart.  This mimics one round of TCP\n      slowstart.\n   repeated\
    \ slowstart bursts:  Slowstart bursts repeated once per\n      target_RTT.  For\
    \ TCP, each burst would be twice as large as the\n      prior burst, and the sequence\
    \ would end at the first ECN CE mark\n      or lost packet.  For measurement,\
    \ all slowstart bursts would be\n      the same size (nominally, target_window_size\
    \ but other sizes might\n      be specified), and the ECN CE marks and lost packets\
    \ are counted.\n"
- title: 3.7.  Tests
  contents:
  - "3.7.  Tests\n   The tests described in this document can be grouped according\
    \ to\n   their applicability.\n   Capacity tests:  Capacity tests determine if\
    \ a network subpath has\n      sufficient capacity to deliver the Target Transport\
    \ Performance.\n      As long as the test stream is within the proper envelope\
    \ for the\n      Target Transport Performance, the average packet losses or ECN\
    \ CE\n      marks must be below the statistical criteria computed by the\n   \
    \   model.  As such, capacity tests reflect parameters that can\n      transition\
    \ from passing to failing as a consequence of cross\n      traffic, additional\
    \ presented load, or the actions of other\n      network users.  By definition,\
    \ capacity tests also consume\n      significant network resources (data capacity\
    \ and/or queue buffer\n      space), and the test schedules must be balanced by\
    \ their cost.\n   Monitoring tests:  Monitoring tests are designed to capture\
    \ the most\n      important aspects of a capacity test without presenting excessive\n\
    \      ongoing load themselves.  As such, they may miss some details of\n    \
    \  the network's performance but can serve as a useful reduced-cost\n      proxy\
    \ for a capacity test, for example, to support continuous\n      production network\
    \ monitoring.\n   Engineering tests:  Engineering tests evaluate how network algorithms\n\
    \      (such as Active Queue Management (AQM) and channel allocation)\n      interact\
    \ with TCP-style self-clocked protocols and adaptive\n      congestion control\
    \ based on packet loss and ECN CE marks.  These\n      tests are likely to have\
    \ complicated interactions with cross\n      traffic and, under some conditions,\
    \ can be inversely sensitive to\n      load.  For example, a test to verify that\
    \ an AQM algorithm causes\n      ECN CE marks or packet drops early enough to\
    \ limit queue occupancy\n      may experience a false pass result in the presence\
    \ of cross\n      traffic.  It is important that engineering tests be performed\n\
    \      under a wide range of conditions, including both in situ and bench\n  \
    \    testing, and over a wide variety of load conditions.  Ongoing\n      monitoring\
    \ is less likely to be useful for engineering tests,\n      although sparse in\
    \ situ testing might be appropriate.\n"
- title: 4.  Background
  contents:
  - "4.  Background\n   When \"Framework for IP Performance Metrics\" [RFC2330] was\
    \ published\n   in 1998, sound Bulk Transport Capacity (BTC) measurement was known\
    \ to\n   be well beyond our capabilities.  Even when \"A Framework for Defining\n\
    \   Empirical Bulk Transfer Capacity Metrics\" [RFC3148] was published, we\n \
    \  knew that we didn't really understand the problem.  Now, in\n   hindsight,\
    \ we understand why assessing BTC is such a difficult\n   problem:\n   o  TCP\
    \ is a control system with circular dependencies -- everything\n      affects\
    \ performance, including components that are explicitly not\n      part of the\
    \ test (for example, the host processing power is not\n      in-scope of path\
    \ performance tests).\n   o  Congestion control is a dynamic equilibrium process,\
    \ similar to\n      processes observed in chemistry and other fields.  The network\
    \ and\n      transport protocols find an operating point that balances opposing\n\
    \      forces: the transport protocol pushing harder (raising the data\n     \
    \ rate and/or window) while the network pushes back (raising packet\n      loss\
    \ ratio, RTT, and/or ECN CE marks).  By design, TCP congestion\n      control\
    \ keeps raising the data rate until the network gives some\n      indication that\
    \ its capacity has been exceeded by dropping packets\n      or adding ECN CE marks.\
    \  If a TCP sender accurately fills a path\n      to its IP capacity (e.g., the\
    \ bottleneck is 100% utilized), then\n      packet losses and ECN CE marks are\
    \ mostly determined by the TCP\n      sender and how aggressively it seeks additional\
    \ capacity; they are\n      not determined by the network itself, because the\
    \ network must\n      send exactly the signals that TCP needs to set its rate.\n\
    \   o  TCP's ability to compensate for network impairments (such as loss,\n  \
    \    delay, and delay variation, outside of those caused by TCP itself)\n    \
    \  is directly proportional to the number of send-ACK round-trip\n      exchanges\
    \ per second (i.e., inversely proportional to the RTT).\n      As a consequence,\
    \ an impaired subpath may pass a short RTT local\n      test even though it fails\
    \ when the subpath is extended by an\n      effectively perfect network to some\
    \ larger RTT.\n   o  TCP has an extreme form of the Observer Effect (colloquially\
    \ known\n      as the \"Heisenberg Effect\").  Measurement and cross traffic\n\
    \      interact in unknown and ill-defined ways.  The situation is\n      actually\
    \ worse than the traditional physics problem where you can\n      at least estimate\
    \ bounds on the relative momentum of the\n      measurement and measured particles.\
    \  In general, for network\n      measurement, you cannot determine even the order\
    \ of magnitude of\n      the effect.  It is possible to construct measurement\
    \ scenarios\n      where the measurement traffic starves real user traffic, yielding\n\
    \      an overly inflated measurement.  The inverse is also possible: the\n  \
    \    user traffic can fill the network, such that the measurement\n      traffic\
    \ detects only minimal available capacity.  In general, you\n      cannot determine\
    \ which scenario might be in effect, so you cannot\n      gauge the relative magnitude\
    \ of the uncertainty introduced by\n      interactions with other network traffic.\n\
    \   o  As a consequence of the properties listed above, it is difficult,\n   \
    \   if not impossible, for two independent implementations (hardware\n      or\
    \ software) of TCP congestion control to produce equivalent\n      performance\
    \ results [RFC6576] under the same network conditions.\n   These properties are\
    \ a consequence of the dynamic equilibrium\n   behavior intrinsic to how all throughput-maximizing\
    \ protocols\n   interact with the Internet.  These protocols rely on control systems\n\
    \   based on estimated network metrics to regulate the quantity of data\n   to\
    \ send into the network.  The packet-sending characteristics in turn\n   alter\
    \ the network properties estimated by the control system metrics,\n   such that\
    \ there are circular dependencies between every transmission\n   characteristic\
    \ and every estimated metric.  Since some of these\n   dependencies are nonlinear,\
    \ the entire system is nonlinear, and any\n   change anywhere causes a difficult-to-predict\
    \ response in network\n   metrics.  As a consequence, Bulk Transport Capacity\
    \ metrics have not\n   fulfilled the analytic framework envisioned in [RFC2330].\n\
    \   Model-Based Metrics overcome these problems by making the measurement\n  \
    \ system open loop: the packet transfer statistics (akin to the network\n   estimators)\
    \ do not affect the traffic or traffic patterns (bursts),\n   which are computed\
    \ on the basis of the Target Transport Performance.\n   A path or subpath meeting\
    \ the Target Transfer Performance\n   requirements would exhibit packet transfer\
    \ statistics and estimated\n   metrics that would not cause the control system\
    \ to slow the traffic\n   below the Target Data Rate.\n"
- title: 4.1.  TCP Properties
  contents:
  - "4.1.  TCP Properties\n   TCP and other self-clocked protocols (e.g., the Stream\
    \ Control\n   Transmission Protocol (SCTP)) carry the vast majority of all Internet\n\
    \   data.  Their dominant bulk data transport behavior is to have an\n   approximately\
    \ fixed quantity of data and acknowledgments (ACKs)\n   circulating in the network.\
    \  The data receiver reports arriving data\n   by returning ACKs to the data sender,\
    \ and the data sender typically\n   responds by sending approximately the same\
    \ quantity of data back into\n   the network.  The total quantity of data plus\
    \ the data represented by\n   ACKs circulating in the network is referred to as\
    \ the \"window\".  The\n   mandatory congestion control algorithms incrementally\
    \ adjust the\n   window by sending slightly more or less data in response to each\
    \ ACK.\n   The fundamentally important property of this system is that it is\n\
    \   self-clocked: the data transmissions are a reflection of the ACKs\n   that\
    \ were delivered by the network, and the ACKs are a reflection of\n   the data\
    \ arriving from the network.\n   A number of protocol features cause bursts of\
    \ data, even in idealized\n   networks that can be modeled as simple queuing systems.\n\
    \   During slowstart, the IP rate is doubled on each RTT by sending twice\n  \
    \ as much data as was delivered to the receiver during the prior RTT.\n   Each\
    \ returning ACK causes the sender to transmit twice the data the\n   ACK reported\
    \ arriving at the receiver.  For slowstart to be able to\n   fill the pipe, the\
    \ network must be able to tolerate slowstart bursts\n   up to the full pipe size\
    \ inflated by the anticipated window reduction\n   on the first loss or ECN CE\
    \ mark.  For example, with classic Reno\n   congestion control, an optimal slowstart\
    \ has to end with a burst that\n   is twice the bottleneck rate for one RTT in\
    \ duration.  This burst\n   causes a queue that is equal to the pipe size (i.e.,\
    \ the window is\n   twice the pipe size), so when the window is halved in response\
    \ to the\n   first packet loss, the new window will be the pipe size.\n   Note\
    \ that if the bottleneck IP rate is less than half of the capacity\n   of the\
    \ front path (which is almost always the case), the slowstart\n   bursts will\
    \ not by themselves cause significant queues anywhere else\n   along the front\
    \ path; they primarily exercise the queue at the\n   dominant bottleneck.\n  \
    \ Several common efficiency algorithms also cause bursts.  The self-\n   clock\
    \ is typically applied to groups of packets: the receiver's\n   delayed ACK algorithm\
    \ generally sends only one ACK per two data\n   segments.  Furthermore, modern\
    \ senders use TCP segmentation offload\n   (TSO) to reduce CPU overhead.  The\
    \ sender's software stack builds\n   super-sized TCP segments that the TSO hardware\
    \ splits into MTU-sized\n   segments on the wire.  The net effect of TSO, delayed\
    \ ACK, and other\n   efficiency algorithms is to send bursts of segments at full\
    \ sender\n   interface rate.\n   Note that these efficiency algorithms are almost\
    \ always in effect,\n   including during slowstart, such that slowstart typically\
    \ has a two-\n   level burst structure.  Section 6.1 describes slowstart in more\n\
    \   detail.\n   Additional sources of bursts include TCP's initial window [RFC6928],\n\
    \   application pauses, channel allocation mechanisms, and network\n   devices\
    \ that schedule ACKs.  Appendix B describes these last two\n   items.  If the\
    \ application pauses (e.g., stops reading or writing\n   data) for some fraction\
    \ of an RTT, many TCP implementations catch up\n   to their earlier window size\
    \ by sending a burst of data at the full\n   sender interface rate.  To fill a\
    \ network with a realistic\n   application, the network has to be able to tolerate\
    \ sender interface\n   rate bursts large enough to restore the prior window following\n\
    \   application pauses.\n   Although the sender interface rate bursts are typically\
    \ smaller than\n   the last burst of a slowstart, they are at a higher IP rate\
    \ so they\n   potentially exercise queues at arbitrary points along the front\
    \ path\n   from the data sender up to and including the queue at the dominant\n\
    \   bottleneck.  It is known that these bursts can hurt network\n   performance,\
    \ especially in conjunction with other queue pressure;\n   however, we are not\
    \ aware of any models for estimating the impact or\n   prescribing limits on the\
    \ size or frequency of sender rate bursts.\n   In conclusion, to verify that a\
    \ path can meet a Target Transport\n   Performance, it is necessary to independently\
    \ confirm that the path\n   can tolerate bursts at the scales that can be caused\
    \ by the above\n   mechanisms.  Three cases are believed to be sufficient:\n \
    \  o  Two-level slowstart bursts sufficient to get connections started\n     \
    \ properly.\n   o  Ubiquitous sender interface rate bursts caused by efficiency\n\
    \      algorithms.  We assume four packet bursts to be the most common\n     \
    \ case, since it matches the effects of delayed ACK during\n      slowstart. \
    \ These bursts should be assumed not to significantly\n      affect packet transfer\
    \ statistics.\n   o  Infrequent sender interface rate bursts that are the maximum\
    \ of\n      the full target_window_size and the initial window size (10\n    \
    \  segments in [RFC6928]).  The target_run_length may be derated for\n      these\
    \ large fast bursts.\n   If a subpath can meet the required packet loss ratio\
    \ for bursts at\n   all of these scales, then it has sufficient buffering at all\n\
    \   potential bottlenecks to tolerate any of the bursts that are likely\n   introduced\
    \ by TCP or other transport protocols.\n"
- title: 4.2.  Diagnostic Approach
  contents:
  - "4.2.  Diagnostic Approach\n   A complete path is expected to be able to attain\
    \ a specified Bulk\n   Transport Capacity if the path's RTT is equal to or smaller\
    \ than the\n   Target RTT, the path's MTU is equal to or larger than the Target\
    \ MTU,\n   and all of the following conditions are met:\n   1.  The IP capacity\
    \ is above the Target Data Rate by a sufficient\n       margin to cover all TCP/IP\
    \ overheads.  This can be confirmed by\n       the tests described in Section\
    \ 8.1 or any number of IP capacity\n       tests adapted to implement MBM.\n \
    \  2.  The observed packet transfer statistics are better than required\n    \
    \   by a suitable TCP performance model (e.g., fewer packet losses or\n      \
    \ ECN CE marks).  See Section 8.1 or any number of low- or fixed-\n       rate\
    \ packet loss tests outside of MBM.\n   3.  There is sufficient buffering at the\
    \ dominant bottleneck to\n       absorb a slowstart burst large enough to get\
    \ the flow out of\n       slowstart at a suitable window size.  See Section 8.3.\n\
    \   4.  There is sufficient buffering in the front path to absorb and\n      \
    \ smooth sender interface rate bursts at all scales that are likely\n       to\
    \ be generated by the application, any channel arbitration in\n       the ACK\
    \ path, or any other mechanisms.  See Section 8.4.\n   5.  When there is a slowly\
    \ rising standing queue at the bottleneck,\n       then the onset of packet loss\
    \ has to be at an appropriate point\n       (in time or in queue depth) and has\
    \ to be progressive, for\n       example, by use of Active Queue Management [RFC7567].\
    \  See\n       Section 8.2.\n   6.  When there is a standing queue at a bottleneck\
    \ for a shared media\n       subpath (e.g., a half-duplex link), there must be\
    \ a suitable\n       bound on the interaction between ACKs and data, for example,\
    \ due\n       to the channel arbitration mechanism.  See Section 8.2.4.\n   Note\
    \ that conditions 1 through 4 require capacity tests for\n   validation and thus\
    \ may need to be monitored on an ongoing basis.\n   Conditions 5 and 6 require\
    \ engineering tests, which are best\n   performed in controlled environments (e.g.,\
    \ bench tests).  They won't\n   generally fail due to load but may fail in the\
    \ field (e.g., due to\n   configuration errors, etc.) and thus should be spot\
    \ checked.\n   A tool that can perform many of the tests is available from\n \
    \  [MBMSource].\n"
- title: 4.3.  New Requirements Relative to RFC 2330
  contents:
  - "4.3.  New Requirements Relative to RFC 2330\n   Model-Based Metrics are designed\
    \ to fulfill some additional\n   requirements that were not recognized at the\
    \ time RFC 2330 [RFC2330]\n   was published.  These missing requirements may have\
    \ significantly\n   contributed to policy difficulties in the IP measurement space.\
    \  Some\n   additional requirements are:\n   o  IP metrics must be actionable\
    \ by the ISP -- they have to be\n      interpreted in terms of behaviors or properties\
    \ at the IP or lower\n      layers that an ISP can test, repair, and verify.\n\
    \   o  Metrics should be spatially composable, such that measures of\n      concatenated\
    \ paths should be predictable from subpaths.\n   o  Metrics must be vantage point\
    \ invariant over a significant range\n      of measurement point choices, including\
    \ off-path measurement\n      points.  The only requirements for Measurement Point\
    \ (MP)\n      selection should be that the RTT between the MPs is below some\n\
    \      reasonable bound and that the effects of the \"test leads\"\n      connecting\
    \ MPs to the subpath under test can be calibrated out of\n      the measurements.\
    \  The latter might be accomplished if the test\n      leads are effectively ideal\
    \ or their properties can be deducted\n      from the measurements between the\
    \ MPs.  While many tests require\n      that the test leads have at least as much\
    \ IP capacity as the\n      subpath under test, some do not, for example, the\
    \ Background\n      Packet Transfer Statistics Tests described in Section 8.1.3.\n\
    \   o  Metric measurements should be repeatable by multiple parties with\n   \
    \   no specialized access to MPs or diagnostic infrastructure.  It\n      should\
    \ be possible for different parties to make the same\n      measurement and observe\
    \ the same results.  In particular, it is\n      important that both a consumer\
    \ (or the consumer's delegate) and\n      ISP be able to perform the same measurement\
    \ and get the same\n      result.  Note that vantage independence is key to meeting\
    \ this\n      requirement.\n"
- title: 5.  Common Models and Parameters
  contents:
  - '5.  Common Models and Parameters

    '
- title: 5.1.  Target End-to-End Parameters
  contents:
  - "5.1.  Target End-to-End Parameters\n   The target end-to-end parameters are the\
    \ Target Data Rate, Target\n   RTT, and Target MTU as defined in Section 3.  These\
    \ parameters are\n   determined by the needs of the application or the ultimate\
    \ end user\n   and the complete Internet path over which the application is expected\n\
    \   to operate.  The target parameters are in units that make sense to\n   layers\
    \ above the TCP layer: payload bytes delivered to the\n   application.  They exclude\
    \ overheads associated with TCP and IP\n   headers, retransmits and other protocols\
    \ (e.g., DNS).  Note that\n   IP-based network services include TCP headers and\
    \ retransmissions as\n   part of delivered payload; this difference (header_overhead)\
    \ is\n   recognized in calculations below.\n   Other end-to-end parameters defined\
    \ in Section 3 include the\n   effective bottleneck data rate, the sender interface\
    \ data rate, and\n   the TCP and IP header sizes.\n   The target_data_rate must\
    \ be smaller than all subpath IP capacities\n   by enough headroom to carry the\
    \ transport protocol overhead,\n   explicitly including retransmissions and an\
    \ allowance for\n   fluctuations in TCP's actual data rate.  Specifying a\n  \
    \ target_data_rate with insufficient headroom is likely to result in\n   brittle\
    \ measurements that have little predictive value.\n   Note that the target parameters\
    \ can be specified for a hypothetical\n   path (for example, to construct TIDS\
    \ designed for bench testing in\n   the absence of a real application) or for\
    \ a live in situ test of\n   production infrastructure.\n   The number of concurrent\
    \ connections is explicitly not a parameter in\n   this model.  If a subpath requires\
    \ multiple connections in order to\n   meet the specified performance, that must\
    \ be stated explicitly, and\n   the procedure described in Section 6.4 applies.\n"
- title: 5.2.  Common Model Calculations
  contents:
  - "5.2.  Common Model Calculations\n   The Target Transport Performance is used\
    \ to derive the\n   target_window_size and the reference target_run_length.\n\
    \   The target_window_size is the average window size in packets needed\n   to\
    \ meet the target_rate, for the specified target_RTT and target_MTU.\n   To calculate\
    \ target_window_size:\n   target_window_size = ceiling(target_rate * target_RTT\
    \ / (target_MTU -\n   header_overhead))\n   The target_run_length is an estimate\
    \ of the minimum required number\n   of unmarked packets that must be delivered\
    \ between losses or ECN CE\n   marks, as computed by a mathematical model of TCP\
    \ congestion control.\n   The derivation here is parallel to the derivation in\
    \ [MSMO97] and, by\n   design, is quite conservative.\n   The reference target_run_length\
    \ is derived as follows.  Assume the\n   subpath_IP_capacity is infinitesimally\
    \ larger than the\n   target_data_rate plus the required header_overhead.  Then,\n\
    \   target_window_size also predicts the onset of queuing.  A larger\n   window\
    \ will cause a standing queue at the bottleneck.\n   Assume the transport protocol\
    \ is using standard Reno-style Additive\n   Increase Multiplicative Decrease (AIMD)\
    \ congestion control [RFC5681]\n   (but not Appropriate Byte Counting [RFC3465])\
    \ and the receiver is\n   using standard delayed ACKs.  Reno increases the window\
    \ by one packet\n   every pipe size worth of ACKs.  With delayed ACKs, this takes\
    \ two\n   RTTs per increase.  To exactly fill the pipe, the spacing of losses\n\
    \   must be no closer than when the peak of the AIMD sawtooth reached\n   exactly\
    \ twice the target_window_size.  Otherwise, the multiplicative\n   window reduction\
    \ triggered by the loss would cause the network to be\n   underfilled.  Per [MSMO97]\
    \ the number of packets between losses must\n   be the area under the AIMD sawtooth.\
    \  They must be no more frequent\n   than every 1 in ((3/2)*target_window_size)*(2*target_window_size)\n\
    \   packets, which simplifies to:\n   target_run_length = 3*(target_window_size^2)\n\
    \   Note that this calculation is very conservative and is based on a\n   number\
    \ of assumptions that may not apply.  Appendix A discusses these\n   assumptions\
    \ and provides some alternative models.  If a different\n   model is used, an\
    \ FSTIDS must document the actual method for\n   computing target_run_length and\
    \ the ratio between alternate\n   target_run_length and the reference target_run_length\
    \ calculated\n   above, along with a discussion of the rationale for the underlying\n\
    \   assumptions.\n   Most of the individual parameters for the tests in Section\
    \ 8 are\n   derived from target_window_size and target_run_length.\n"
- title: 5.3.  Parameter Derating
  contents:
  - "5.3.  Parameter Derating\n   Since some aspects of the models are very conservative,\
    \ the MBM\n   framework permits some latitude in derating test parameters.  Rather\n\
    \   than trying to formalize more complicated models, we permit some test\n  \
    \ parameters to be relaxed as long as they meet some additional\n   procedural\
    \ constraints:\n   o  The FSTIDS must document and justify the actual method used\
    \ to\n      compute the derated metric parameters.\n   o  The validation procedures\
    \ described in Section 10 must be used to\n      demonstrate the feasibility of\
    \ meeting the Target Transport\n      Performance with infrastructure that just\
    \ barely passes the\n      derated tests.\n   o  The validation process for an\
    \ FSTIDS itself must be documented in\n      such a way that other researchers\
    \ can duplicate the validation\n      experiments.\n   Except as noted, all tests\
    \ below assume no derating.  Tests for which\n   there is not currently a well-established\
    \ model for the required\n   parameters explicitly include derating as a way to\
    \ indicate\n   flexibility in the parameters.\n"
- title: 5.4.  Test Preconditions
  contents:
  - "5.4.  Test Preconditions\n   Many tests have preconditions that are required\
    \ to assure their\n   validity.  Examples include the presence or non-presence\
    \ of cross\n   traffic on specific subpaths; negotiating ECN; and a test stream\n\
    \   preamble of appropriate length to achieve stable access to network\n   resources\
    \ in the presence of reactive network elements (as defined in\n   Section 1.1\
    \ of [RFC7312]).  If preconditions are not properly\n   satisfied for some reason,\
    \ the tests should be considered to be\n   inconclusive.  In general, it is useful\
    \ to preserve diagnostic\n   information as to why the preconditions were not\
    \ met and any test\n   data that was collected even if it is not useful for the\
    \ intended\n   test.  Such diagnostic information and partial test data may be\n\
    \   useful for improving the test or test procedures themselves.\n   It is important\
    \ to preserve the record that a test was scheduled;\n   otherwise, precondition\
    \ enforcement mechanisms can introduce sampling\n   bias.  For example, canceling\
    \ tests due to cross traffic on\n   subscriber access links might introduce sampling\
    \ bias in tests of the\n   rest of the network by reducing the number of tests\
    \ during peak\n   network load.\n   Test preconditions and failure actions must\
    \ be specified in an\n   FSTIDS.\n"
- title: 6.  Generating Test Streams
  contents:
  - "6.  Generating Test Streams\n   Many important properties of Model-Based Metrics,\
    \ such as vantage\n   independence, are a consequence of using test streams that\
    \ have\n   temporal structures that mimic TCP or other transport protocols\n \
    \  running over a complete path.  As described in Section 4.1, self-\n   clocked\
    \ protocols naturally have burst structures related to the RTT\n   and pipe size\
    \ of the complete path.  These bursts naturally get\n   larger (contain more packets)\
    \ as either the Target RTT or Target Data\n   Rate get larger or the Target MTU\
    \ gets smaller.  An implication of\n   these relationships is that test streams\
    \ generated by running self-\n   clocked protocols over short subpaths may not\
    \ adequately exercise the\n   queuing at any bottleneck to determine if the subpath\
    \ can support the\n   full Target Transport Performance over the complete path.\n\
    \   Failing to authentically mimic TCP's temporal structure is part of\n   the\
    \ reason why simple performance tools such as iPerf, netperf, nc,\n   etc., have\
    \ the reputation for yielding false pass results over short\n   test paths, even\
    \ when a subpath has a flaw.\n   The definitions in Section 3 are sufficient for\
    \ most test streams.\n   We describe the slowstart and standing queue test streams\
    \ in more\n   detail.\n   In conventional measurement practice, stochastic processes\
    \ are used\n   to eliminate many unintended correlations and sample biases.\n\
    \   However, MBM tests are designed to explicitly mimic temporal\n   correlations\
    \ caused by network or protocol elements themselves.  Some\n   portions of these\
    \ systems, such as traffic arrival (e.g., test\n   scheduling), are naturally\
    \ stochastic.  Other behaviors, such as\n   back-to-back packet transmissions,\
    \ are dominated by implementation-\n   specific deterministic effects.  Although\
    \ these behaviors always\n   contain non-deterministic elements and might be modeled\n\
    \   stochastically, these details typically do not contribute\n   significantly\
    \ to the overall system behavior.  Furthermore, it is\n   known that real protocols\
    \ are subject to failures caused by network\n   property estimators suffering\
    \ from bias due to correlation in their\n   own traffic.  For example, TCP's RTT\
    \ estimator used to determine the\n   Retransmit Timeout (RTO), can be fooled\
    \ by periodic cross traffic or\n   start-stop applications.  For these reasons,\
    \ many details of the test\n   streams are specified deterministically.\n   It\
    \ may prove useful to introduce fine-grained noise sources into the\n   models\
    \ used for generating test streams in an update of Model-Based\n   Metrics, but\
    \ the complexity is not warranted at the time this\n   document was written.\n"
- title: 6.1.  Mimicking Slowstart
  contents:
  - "6.1.  Mimicking Slowstart\n   TCP slowstart has a two-level burst structure as\
    \ shown in Figure 2.\n   The fine time structure is caused by efficiency algorithms\
    \ that\n   deliberately batch work (CPU, channel allocation, etc.) to better\n\
    \   amortize certain network and host overheads.  ACKs passing through\n   the\
    \ return path typically cause the sender to transmit small bursts\n   of data\
    \ at the full sender interface rate.  For example, TCP\n   Segmentation Offload\
    \ (TSO) and Delayed Acknowledgment both contribute\n   to this effect.  During\
    \ slowstart, these bursts are at the same\n   headway as the returning ACKs but\
    \ are typically twice as large (e.g.,\n   have twice as much data) as the ACK\
    \ reported was delivered to the\n   receiver.  Due to variations in delayed ACK\
    \ and algorithms such as\n   Appropriate Byte Counting [RFC3465], different pairs\
    \ of senders and\n   receivers produce slightly different burst patterns.  Without\
    \ loss of\n   generality, we assume each ACK causes four packet sender interface\n\
    \   rate bursts at an average headway equal to the ACK headway; this\n   corresponds\
    \ to sending at an average rate equal to twice the\n   effective bottleneck IP\
    \ rate.  Each slowstart burst consists of a\n   series of four packet sender interface\
    \ rate bursts such that the\n   total number of packets is the current window\
    \ size (as of the last\n   packet in the burst).\n   The coarse time structure\
    \ is due to each RTT being a reflection of\n   the prior RTT.  For real transport\
    \ protocols, each slowstart burst is\n   twice as large (twice the window) as\
    \ the previous burst but is spread\n   out in time by the network bottleneck,\
    \ such that each successive RTT\n   exhibits the same effective bottleneck IP\
    \ rate.  The slowstart phase\n   ends on the first lost packet or ECN mark, which\
    \ is intended to\n   happen after successive slowstart bursts merge in time: the\
    \ next\n   burst starts before the bottleneck queue is fully drained and the\n\
    \   prior burst is complete.\n   For the diagnostic tests described below, we\
    \ preserve the fine time\n   structure but manipulate the coarse structure of\
    \ the slowstart bursts\n   (burst size and headway) to measure the ability of\
    \ the dominant\n   bottleneck to absorb and smooth slowstart bursts.\n   Note\
    \ that a stream of repeated slowstart bursts has three different\n   average rates,\
    \ depending on the averaging time interval.  At the\n   finest timescale (a few\
    \ packet times at the sender interface), the\n   peak of the average IP rate is\
    \ the same as the sender interface rate;\n   at a medium timescale (a few ACK\
    \ times at the dominant bottleneck),\n   the peak of the average IP rate is twice\
    \ the implied bottleneck IP\n   capacity; and at timescales longer than the target_RTT\
    \ and when the\n   burst size is equal to the target_window_size, the average\
    \ rate is\n   equal to the target_data_rate.  This pattern corresponds to repeating\n\
    \   the last RTT of TCP slowstart when delayed ACK and sender-side byte\n   counting\
    \ are present but without the limits specified in Appropriate\n   Byte Counting\
    \ [RFC3465].\n   time ==>    ( - equals one packet)\n   Fine time structure of\
    \ the packet stream:\n   ----  ----  ----  ----  ----\n   |<>| sender interface\
    \ rate bursts (typically 3 or 4 packets)\n   |<===>| burst headway (from the ACK\
    \ headway)\n   \\____repeating sender______/\n          rate bursts\n   Coarse\
    \ (RTT-level) time structure of the packet stream:\n   ----  ----  ----  ----\
    \  ----                     ----  ---- ...\n   |<========================>| slowstart\
    \ burst size (from the window)\n   |<==============================================>|\
    \ slowstart headway\n                                                       (from\
    \ the RTT)\n   \\__________________________/                     \\_________ ...\n\
    \       one slowstart burst                     Repeated slowstart bursts\n  \
    \             Figure 2: Multiple Levels of Slowstart Bursts\n"
- title: 6.2.  Constant Window Pseudo CBR
  contents:
  - "6.2.  Constant Window Pseudo CBR\n   Pseudo constant bit rate (CBR) is implemented\
    \ by running a standard\n   self-clocked protocol such as TCP with a fixed window\
    \ size.  If that\n   window size is test_window, the data rate will be slightly\
    \ above the\n   target_rate.\n   Since the test_window is constrained to be an\
    \ integer number of\n   packets, for small RTTs or low data rates, there may not\
    \ be\n   sufficiently precise control over the data rate.  Rounding the\n   test_window\
    \ up (as defined above) is likely to result in data rates\n   that are higher\
    \ than the target rate, but reducing the window by one\n   packet may result in\
    \ data rates that are too small.  Also, cross\n   traffic potentially raises the\
    \ RTT, implicitly reducing the rate.\n   Cross traffic that raises the RTT nearly\
    \ always makes the test more\n   strenuous (i.e., more demanding for the network\
    \ path).\n   Note that Constant Window Pseudo CBR (and Scanned Window Pseudo CBR\n\
    \   in the next section) both rely on a self-clock that is at least\n   partially\
    \ derived from the properties of the subnet under test.  This\n   introduces the\
    \ possibility that the subnet under test exhibits\n   behaviors such as extreme\
    \ RTT fluctuations that prevent these\n   algorithms from accurately controlling\
    \ data rates.\n   An FSTIDS specifying a Constant Window Pseudo CBR test must\n\
    \   explicitly indicate under what conditions errors in the data rate\n   cause\
    \ tests to be inconclusive.  Conventional paced measurement\n   traffic may be\
    \ more appropriate for these environments.\n"
- title: 6.3.  Scanned Window Pseudo CBR
  contents:
  - "6.3.  Scanned Window Pseudo CBR\n   Scanned Window Pseudo CBR is similar to the\
    \ Constant Window Pseudo\n   CBR described above, except the window is scanned\
    \ across a range of\n   sizes designed to include two key events: the onset of\
    \ queuing and\n   the onset of packet loss or ECN CE marks.  The window is scanned\
    \ by\n   incrementing it by one packet every 2*target_window_size delivered\n\
    \   packets.  This mimics the additive increase phase of standard Reno\n   TCP\
    \ congestion avoidance when delayed ACKs are in effect.  Normally,\n   the window\
    \ increases are separated by intervals slightly longer than\n   twice the target_RTT.\n\
    \   There are two ways to implement this test: 1) applying a window clamp\n  \
    \ to standard congestion control in a standard protocol such as TCP and\n   2)\
    \ stiffening a non-standard transport protocol.  When standard\n   congestion\
    \ control is in effect, any losses or ECN CE marks cause the\n   transport to\
    \ revert to a window smaller than the clamp, such that the\n   scanning clamp\
    \ loses control of the window size.  The NPAD (Network\n   Path and Application\
    \ Diagnostics) pathdiag tool is an example of this\n   class of algorithms [Pathdiag].\n\
    \   Alternatively, a non-standard congestion control algorithm can\n   respond\
    \ to losses by transmitting extra data, such that it maintains\n   the specified\
    \ window size independent of losses or ECN CE marks.\n   Such a stiffened transport\
    \ explicitly violates mandatory Internet\n   congestion control [RFC5681] and\
    \ is not suitable for in situ testing.\n   It is only appropriate for engineering\
    \ testing under laboratory\n   conditions.  The Windowed Ping tool implements\
    \ such a test [WPING].\n   This tool has been updated (see [mpingSource]).\n \
    \  The test procedures in Section 8.2 describe how to the partition the\n   scans\
    \ into regions and how to interpret the results.\n"
- title: 6.4.  Concurrent or Channelized Testing
  contents:
  - "6.4.  Concurrent or Channelized Testing\n   The procedures described in this\
    \ document are only directly\n   applicable to single-stream measurement, e.g.,\
    \ one TCP connection or\n   measurement stream.  In an ideal world, we would disallow\
    \ all\n   performance claims based on multiple concurrent streams, but this is\n\
    \   not practical due to at least two issues.  First, many very high-rate\n  \
    \ link technologies are channelized and at last partially pin the flow-\n   to-channel\
    \ mapping to minimize packet reordering within flows.\n   Second, TCP itself has\
    \ scaling limits.  Although the former problem\n   might be overcome through different\
    \ design decisions, the latter\n   problem is more deeply rooted.\n   All congestion\
    \ control algorithms that are philosophically aligned\n   with [RFC5681] (e.g.,\
    \ claim some level of TCP compatibility,\n   friendliness, or fairness) have scaling\
    \ limits; that is, as a long\n   fat network (LFN) with a fixed RTT and MTU gets\
    \ faster, these\n   congestion control algorithms get less accurate and, as a\n\
    \   consequence, have difficulty filling the network [CCscaling].  These\n   properties\
    \ are a consequence of the original Reno AIMD congestion\n   control design and\
    \ the requirement in [RFC5681] that all transport\n   protocols have similar responses\
    \ to congestion.\n   There are a number of reasons to want to specify performance\
    \ in terms\n   of multiple concurrent flows; however, this approach is not\n \
    \  recommended for data rates below several megabits per second, which\n   can\
    \ be attained with run lengths under 10000 packets on many paths.\n   Since the\
    \ required run length is proportional to the square of the\n   data rate, at higher\
    \ rates, the run lengths can be unreasonably\n   large, and multiple flows might\
    \ be the only feasible approach.\n   If multiple flows are deemed necessary to\
    \ meet aggregate performance\n   targets, then this must be stated both in the\
    \ design of the TIDS and\n   in any claims about network performance.  The IP\
    \ diagnostic tests\n   must be performed concurrently with the specified number\
    \ of\n   connections.  For the tests that use bursty test streams, the bursts\n\
    \   should be synchronized across streams unless there is a priori\n   knowledge\
    \ that the applications have some explicit mechanism to\n   stagger their own\
    \ bursts.  In the absence of an explicit mechanism to\n   stagger bursts, many\
    \ network and application artifacts will sometimes\n   implicitly synchronize\
    \ bursts.  A test that does not control burst\n   synchronization may be prone\
    \ to false pass results for some\n   applications.\n"
- title: 7.  Interpreting the Results
  contents:
  - '7.  Interpreting the Results

    '
- title: 7.1.  Test Outcomes
  contents:
  - "7.1.  Test Outcomes\n   To perform an exhaustive test of a complete network path,\
    \ each test\n   of the TIDS is applied to each subpath of the complete path. \
    \ If any\n   subpath fails any test, then a standard transport protocol running\n\
    \   over the complete path can also be expected to fail to attain the\n   Target\
    \ Transport Performance under some conditions.\n   In addition to passing or failing,\
    \ a test can be deemed to be\n   inconclusive for a number of reasons.  Proper\
    \ instrumentation and\n   treatment of inconclusive outcomes is critical to the\
    \ accuracy and\n   robustness of Model-Based Metrics.  Tests can be inconclusive\
    \ if the\n   precomputed traffic pattern or data rates were not accurately\n \
    \  generated; the measurement results were not statistically\n   significant;\
    \ the required preconditions for the test were not met; or\n   other causes. \
    \ See Section 5.4.\n   For example, consider a test that implements Constant Window\
    \ Pseudo\n   CBR (Section 6.2) by adding rate controls and detailed IP packet\n\
    \   transfer instrumentation to TCP (e.g., using the extended performance\n  \
    \ statistics for TCP as described in [RFC4898]).  TCP includes built-in\n   control\
    \ systems that might interfere with the sending data rate.  If\n   such a test\
    \ meets the required packet transfer statistics (e.g., run\n   length) while failing\
    \ to attain the specified data rate, it must be\n   treated as an inconclusive\
    \ result, because we cannot a priori\n   determine if the reduced data rate was\
    \ caused by a TCP problem or a\n   network problem or if the reduced data rate\
    \ had a material effect on\n   the observed packet transfer statistics.\n   Note\
    \ that for capacity tests, if the observed packet transfer\n   statistics meet\
    \ the statistical criteria for failing (based on\n   acceptance of hypothesis\
    \ H1 in Section 7.2), the test can be\n   considered to have failed because it\
    \ doesn't really matter that the\n   test didn't attain the required data rate.\n\
    \   The important new properties of MBM, such as vantage independence,\n   are\
    \ a direct consequence of opening the control loops in the\n   protocols, such\
    \ that the test stream does not depend on network\n   conditions or IP packets\
    \ received.  Any mechanism that introduces\n   feedback between the path's measurements\
    \ and the test stream\n   generation is at risk of introducing nonlinearities\
    \ that spoil these\n   properties.  Any exceptional event that indicates that\
    \ such feedback\n   has happened should cause the test to be considered inconclusive.\n\
    \   Inconclusive tests may be caused by situations in which a test\n   outcome\
    \ is ambiguous because of network limitations or an unknown\n   limitation on\
    \ the IP diagnostic test itself, which may have been\n   caused by some uncontrolled\
    \ feedback from the network.\n   Note that procedures that attempt to search the\
    \ target parameter\n   space to find the limits on a parameter such as target_data_rate\
    \ are\n   at risk of breaking the location-independent properties of Model-\n\
    \   Based Metrics if any part of the boundary between passing,\n   inconclusive,\
    \ or failing results is sensitive to RTT (which is\n   normally the case).  For\
    \ example, the maximum data rate for a\n   marginal link (e.g., exhibiting excess\
    \ errors) is likely to be\n   sensitive to the test_path_RTT.  The maximum observed\
    \ data rate over\n   the test path has very little value for predicting the maximum\
    \ rate\n   over a different path.\n   One of the goals for evolving TIDS designs\
    \ will be to keep sharpening\n   the distinctions between inconclusive, passing,\
    \ and failing tests.\n   The criteria for inconclusive, passing, and failing tests\
    \ must be\n   explicitly stated for every test in the TIDS or FSTIDS.\n   One\
    \ of the goals for evolving the testing process, procedures, tools,\n   and measurement\
    \ point selection should be to minimize the number of\n   inconclusive tests.\n\
    \   It may be useful to keep raw packet transfer statistics and ancillary\n  \
    \ metrics [RFC3148] for deeper study of the behavior of the network\n   path and\
    \ to measure the tools themselves.  Raw packet transfer\n   statistics can help\
    \ to drive tool evolution.  Under some conditions,\n   it might be possible to\
    \ re-evaluate the raw data for satisfying\n   alternate Target Transport Performance.\
    \  However, it is important to\n   guard against sampling bias and other implicit\
    \ feedback that can\n   cause false results and exhibit measurement point vantage\n\
    \   sensitivity.  Simply applying different delivery criteria based on a\n   different\
    \ Target Transport Performance is insufficient if the test\n   traffic patterns\
    \ (bursts, etc.) do not match the alternate Target\n   Transport Performance.\n"
- title: 7.2.  Statistical Criteria for Estimating run_length
  contents:
  - "7.2.  Statistical Criteria for Estimating run_length\n   When evaluating the\
    \ observed run_length, we need to determine\n   appropriate packet stream sizes\
    \ and acceptable error levels for\n   efficient measurement.  In practice, can\
    \ we compare the empirically\n   estimated packet loss and ECN CE marking ratios\
    \ with the targets as\n   the sample size grows?  How large a sample is needed\
    \ to say that the\n   measurements of packet transfer indicate a particular run\
    \ length is\n   present?\n   The generalized measurement can be described as recursive\
    \ testing:\n   send packets (individually or in patterns) and observe the packet\n\
    \   transfer performance (packet loss ratio, other metric, or any marking\n  \
    \ we define).\n   As each packet is sent and measured, we have an ongoing estimate\
    \ of\n   the performance in terms of the ratio of packet loss or ECN CE marks\n\
    \   to total packets (i.e., an empirical probability).  We continue to\n   send\
    \ until conditions support a conclusion or a maximum sending limit\n   has been\
    \ reached.\n   We have a target_mark_probability, one mark per target_run_length,\n\
    \   where a \"mark\" is defined as a lost packet, a packet with ECN CE\n   mark,\
    \ or other signal.  This constitutes the null hypothesis:\n   H0:  no more than\
    \ one mark in target_run_length =\n      3*(target_window_size)^2 packets\n  \
    \ We can stop sending packets if ongoing measurements support accepting\n   H0\
    \ with the specified Type I error = alpha (= 0.05, for example).\n   We also have\
    \ an alternative hypothesis to evaluate: is performance\n   significantly lower\
    \ than the target_mark_probability?  Based on\n   analysis of typical values and\
    \ practical limits on measurement\n   duration, we choose four times the H0 probability:\n\
    \   H1:  one or more marks in (target_run_length/4) packets\n   and we can stop\
    \ sending packets if measurements support rejecting H0\n   with the specified\
    \ Type II error = beta (= 0.05, for example), thus\n   preferring the alternate\
    \ hypothesis H1.\n   H0 and H1 constitute the success and failure outcomes described\n\
    \   elsewhere in this document; while the ongoing measurements do not\n   support\
    \ either hypothesis, the current status of measurements is\n   inconclusive.\n\
    \   The problem above is formulated to match the Sequential Probability\n   Ratio\
    \ Test (SPRT) [Wald45] [Montgomery90].  Note that as originally\n   framed, the\
    \ events under consideration were all manufacturing\n   defects.  In networking,\
    \ ECN CE marks and lost packets are not\n   defects but signals, indicating that\
    \ the transport protocol should\n   slow down.\n   The Sequential Probability\
    \ Ratio Test also starts with a pair of\n   hypotheses specified as above:\n \
    \  H0:  p0 = one defect in target_run_length\n   H1:  p1 = one defect in target_run_length/4\n\
    \   As packets are sent and measurements collected, the tester evaluates\n   the\
    \ cumulative defect count against two boundaries representing H0\n   Acceptance\
    \ or Rejection (and acceptance of H1):\n   Acceptance line:  Xa = -h1 + s*n\n\
    \   Rejection line:  Xr = h2 + s*n\n   where n increases linearly for each packet\
    \ sent and\n   h1 =  { log((1-alpha)/beta) }/k\n   h2 =  { log((1-beta)/alpha)\
    \ }/k\n   k  =  log{ (p1(1-p0)) / (p0(1-p1)) }\n   s  =  [ log{ (1-p0)/(1-p1)\
    \ } ]/k\n   for p0 and p1 as defined in the null and alternative hypotheses\n\
    \   statements above, and alpha and beta as the Type I and Type II\n   errors.\n\
    \   The SPRT specifies simple stopping rules:\n   o  Xa < defect_count(n) < Xr:\
    \ continue testing\n   o  defect_count(n) <= Xa: Accept H0\n   o  defect_count(n)\
    \ >= Xr: Accept H1\n   The calculations above are implemented in the R-tool for\
    \ Statistical\n   Analysis [Rtool], in the add-on package for Cross-Validation\
    \ via\n   Sequential Testing (CVST) [CVST].\n   Using the equations above, we\
    \ can calculate the minimum number of\n   packets (n) needed to accept H0 when\
    \ x defects are observed.  For\n   example, when x = 0:\n   Xa = 0  = -h1 + s*n\n\
    \   and  n = h1 / s\n   Note that the derivations in [Wald45] and [Montgomery90]\
    \ differ.\n   Montgomery's simplified derivation of SPRT may assume a Bernoulli\n\
    \   processes, where the packet loss probabilities are independent and\n   identically\
    \ distributed, making the SPRT more accessible.  Wald's\n   seminal paper showed\
    \ that this assumption is not necessary.  It helps\n   to remember that the goal\
    \ of SPRT is not to estimate the value of the\n   packet loss rate but only whether\
    \ or not the packet loss ratio is\n   likely (1) low enough (when we accept the\
    \ H0 null hypothesis),\n   yielding success or (2) too high (when we accept the\
    \ H1 alternate\n   hypothesis), yielding failure.\n"
- title: 7.3.  Reordering Tolerance
  contents:
  - "7.3.  Reordering Tolerance\n   All tests must be instrumented for packet-level\
    \ reordering [RFC4737].\n   However, there is no consensus for how much reordering\
    \ should be\n   acceptable.  Over the last two decades, the general trend has\
    \ been to\n   make protocols and applications more tolerant to reordering (for\n\
    \   example, see [RFC5827]), in response to the gradual increase in\n   reordering\
    \ in the network.  This increase has been due to the\n   deployment of technologies\
    \ such as multithreaded routing lookups and\n   Equal-Cost Multipath (ECMP) routing.\
    \  These techniques increase\n   parallelism in the network and are critical to\
    \ enabling overall\n   Internet growth to exceed Moore's Law.\n   With transport\
    \ retransmission strategies, there are fundamental\n   trade-offs among reordering\
    \ tolerance, how quickly losses can be\n   repaired, and overhead from spurious\
    \ retransmissions.  In advance of\n   new retransmission strategies, we propose\
    \ the following strawman:\n   transport protocols should be able to adapt to reordering\
    \ as long as\n   the reordering extent is not more than the maximum of one quarter\n\
    \   window or 1 ms, whichever is larger.  (These values come from\n   experience\
    \ prototyping Early Retransmit [RFC5827] and related\n   algorithms.  They agree\
    \ with the values being proposed for \"RACK: a\n   time-based fast loss detection\
    \ algorithm\" [RACK].)  Within this limit\n   on reorder extent, there should\
    \ be no bound on reordering density.\n   By implication, recording that is less\
    \ than these bounds should not\n   be treated as a network impairment.  However,\
    \ [RFC4737] still\n   applies: reordering should be instrumented, and the maximum\n\
    \   reordering that can be properly characterized by the test (because of\n  \
    \ the bound on history buffers) should be recorded with the measurement\n   results.\n\
    \   Reordering tolerance and diagnostic limitations, such as the size of\n   the\
    \ history buffer used to diagnose packets that are way out of\n   order, must\
    \ be specified in an FSTIDS.\n"
- title: 8.  IP Diagnostic Tests
  contents:
  - "8.  IP Diagnostic Tests\n   The IP diagnostic tests below are organized according\
    \ to the\n   technique used to generate the test stream as described in Section\
    \ 6.\n   All of the results are evaluated in accordance with Section 7,\n   possibly\
    \ with additional test-specific criteria.\n   We also introduce some combined\
    \ tests that are more efficient when\n   networks are expected to pass but conflate\
    \ diagnostic signatures when\n   they fail.\n"
- title: 8.1.  Basic Data Rate and Packet Transfer Tests
  contents:
  - "8.1.  Basic Data Rate and Packet Transfer Tests\n   We propose several versions\
    \ of the basic data rate and packet\n   transfer statistics test that differ in\
    \ how the data rate is\n   controlled.  The data can be paced on a timer or window\
    \ controlled\n   (and self-clocked).  The first two tests implicitly confirm that\n\
    \   sub_path has sufficient raw capacity to carry the target_data_rate.\n   They\
    \ are recommended for relatively infrequent testing, such as an\n   installation\
    \ or periodic auditing process.  The third test,\n   Background Packet Transfer\
    \ Statistics, is a low-rate test designed\n   for ongoing monitoring for changes\
    \ in subpath quality.\n"
- title: 8.1.1.  Delivery Statistics at Paced Full Data Rate
  contents:
  - "8.1.1.  Delivery Statistics at Paced Full Data Rate\n   This test confirms that\
    \ the observed run length is at least the\n   target_run_length while relying\
    \ on timer to send data at the\n   target_rate using the procedure described in\
    \ Section 6.1 with a burst\n   size of 1 (single packets) or 2 (packet pairs).\n\
    \   The test is considered to be inconclusive if the packet transmission\n   cannot\
    \ be accurately controlled for any reason.\n   RFC 6673 [RFC6673] is appropriate\
    \ for measuring packet transfer\n   statistics at full data rate.\n"
- title: 8.1.2.  Delivery Statistics at Full Data Windowed Rate
  contents:
  - "8.1.2.  Delivery Statistics at Full Data Windowed Rate\n   This test confirms\
    \ that the observed run length is at least the\n   target_run_length while sending\
    \ at an average rate approximately\n   equal to the target_data_rate, by controlling\
    \ (or clamping) the\n   window size of a conventional transport protocol to test_window.\n\
    \   Since losses and ECN CE marks cause transport protocols to reduce\n   their\
    \ data rates, this test is expected to be less precise about\n   controlling its\
    \ data rate.  It should not be considered inconclusive\n   as long as at least\
    \ some of the round trips reached the full\n   target_data_rate without incurring\
    \ losses or ECN CE marks.  To pass\n   this test, the network must deliver target_window_size\
    \ packets in\n   target_RTT time without any losses or ECN CE marks at least once\
    \ per\n   two target_window_size round trips, in addition to meeting the run\n\
    \   length statistical test.\n"
- title: 8.1.3.  Background Packet Transfer Statistics Tests
  contents:
  - "8.1.3.  Background Packet Transfer Statistics Tests\n   The Background Packet\
    \ Transfer Statistics Test is a low-rate version\n   of the target rate test above,\
    \ designed for ongoing lightweight\n   monitoring for changes in the observed\
    \ subpath run length without\n   disrupting users.  It should be used in conjunction\
    \ with one of the\n   above full-rate tests because it does not confirm that the\
    \ subpath\n   can support raw data rate.\n   RFC 6673 [RFC6673] is appropriate\
    \ for measuring background packet\n   transfer statistics.\n"
- title: 8.2.  Standing Queue Tests
  contents:
  - "8.2.  Standing Queue Tests\n   These engineering tests confirm that the bottleneck\
    \ is well behaved\n   across the onset of packet loss, which typically follows\
    \ after the\n   onset of queuing.  Well behaved generally means lossless for\n\
    \   transient queues, but once the queue has been sustained for a\n   sufficient\
    \ period of time (or reaches a sufficient queue depth),\n   there should be a\
    \ small number of losses or ECN CE marks to signal to\n   the transport protocol\
    \ that it should reduce its window or data rate.\n   Losses that are too early\
    \ can prevent the transport from averaging at\n   the target_data_rate.  Losses\
    \ that are too late indicate that the\n   queue might not have an appropriate\
    \ AQM [RFC7567] and, as a\n   consequence, be subject to bufferbloat [wikiBloat].\
    \  Queues without\n   AQM have the potential to inflict excess delays on all flows\
    \ sharing\n   the bottleneck.  Excess losses (more than half of the window) at\
    \ the\n   onset of loss make loss recovery problematic for the transport\n   protocol.\
    \  Non-linear, erratic, or excessive RTT increases suggest\n   poor interactions\
    \ between the channel acquisition algorithms and the\n   transport self-clock.\
    \  All of the tests in this section use the same\n   basic scanning algorithm,\
    \ described here, but score the link or\n   subpath on the basis of how well it\
    \ avoids each of these problems.\n   Some network technologies rely on virtual\
    \ queues or other techniques\n   to meter traffic without adding any queuing delay,\
    \ in which case the\n   data rate will vary with the window size all the way up\
    \ to the onset\n   of load-induced packet loss or ECN CE marks.  For these technologies,\n\
    \   the discussion of queuing in Section 6.3 does not apply, but it is\n   still\
    \ necessary to confirm that the onset of losses or ECN CE marks\n   be at an appropriate\
    \ point and progressive.  If the network\n   bottleneck does not introduce significant\
    \ queuing delay, modify the\n   procedure described in Section 6.3 to start the\
    \ scan at a window\n   equal to or slightly smaller than the test_window.\n  \
    \ Use the procedure in Section 6.3 to sweep the window across the onset\n   of\
    \ queuing and the onset of loss.  The tests below all assume that\n   the scan\
    \ emulates standard additive increase and delayed ACK by\n   incrementing the\
    \ window by one packet for every 2*target_window_size\n   packets delivered. \
    \ A scan can typically be divided into three\n   regions: below the onset of queuing,\
    \ a standing queue, and at or\n   beyond the onset of loss.\n   Below the onset\
    \ of queuing, the RTT is typically fairly constant, and\n   the data rate varies\
    \ in proportion to the window size.  Once the data\n   rate reaches the subpath\
    \ IP rate, the data rate becomes fairly\n   constant, and the RTT increases in\
    \ proportion to the increase in\n   window size.  The precise transition across\
    \ the start of queuing can\n   be identified by the maximum network power, defined\
    \ to be the ratio\n   data rate over the RTT.  The network power can be computed\
    \ at each\n   window size, and the window with the maximum is taken as the start\
    \ of\n   the queuing region.\n   If there is random background loss (e.g., bit\
    \ errors), precise\n   determination of the onset of queue-induced packet loss\
    \ may require\n   multiple scans.  At window sizes large enough to cause loss\
    \ in\n   queues, all transport protocols are expected to experience periodic\n\
    \   losses determined by the interaction between the congestion control\n   and\
    \ AQM algorithms.  For standard congestion control algorithms, the\n   periodic\
    \ losses are likely to be relatively widely spaced, and the\n   details are typically\
    \ dominated by the behavior of the transport\n   protocol itself.  For the case\
    \ of stiffened transport protocols (with\n   non-standard, aggressive congestion\
    \ control algorithms), the details\n   of periodic losses will be dominated by\
    \ how the window increase\n   function responds to loss.\n"
- title: 8.2.1.  Congestion Avoidance
  contents:
  - "8.2.1.  Congestion Avoidance\n   A subpath passes the congestion avoidance standing\
    \ queue test if more\n   than target_run_length packets are delivered between\
    \ the onset of\n   queuing (as determined by the window with the maximum network\
    \ power\n   as described above) and the first loss or ECN CE mark.  If this test\n\
    \   is implemented using a standard congestion control algorithm with a\n   clamp,\
    \ it can be performed in situ in the production internet as a\n   capacity test.\
    \  For an example of such a test, see [Pathdiag].\n   For technologies that do\
    \ not have conventional queues, use the\n   test_window in place of the onset\
    \ of queuing.  That is, a subpath\n   passes the congestion avoidance standing\
    \ queue test if more than\n   target_run_length packets are delivered between\
    \ the start of the scan\n   at test_window and the first loss or ECN CE mark.\n"
- title: 8.2.2.  Bufferbloat
  contents:
  - "8.2.2.  Bufferbloat\n   This test confirms that there is some mechanism to limit\
    \ buffer\n   occupancy (e.g., that prevents bufferbloat).  Note that this is not\n\
    \   strictly a requirement for single-stream bulk transport capacity;\n   however,\
    \ if there is no mechanism to limit buffer queue occupancy,\n   then a single\
    \ stream with sufficient data to deliver is likely to\n   cause the problems described\
    \ in [RFC7567] and [wikiBloat].  This may\n   cause only minor symptoms for the\
    \ dominant flow but has the potential\n   to make the subpath unusable for other\
    \ flows and applications.\n   The test will pass if the onset of loss occurs before\
    \ a standing\n   queue has introduced delay greater than twice the target_RTT\
    \ or\n   another well-defined and specified limit.  Note that there is not yet\n\
    \   a model for how much standing queue is acceptable.  The factor of two\n  \
    \ chosen here reflects a rule of thumb.  In conjunction with the\n   previous\
    \ test, this test implies that the first loss should occur at\n   a queuing delay\
    \ that is between one and two times the target_RTT.\n   Specified RTT limits that\
    \ are larger than twice the target_RTT must\n   be fully justified in the FSTIDS.\n"
- title: 8.2.3.  Non-excessive Loss
  contents:
  - "8.2.3.  Non-excessive Loss\n   This test confirms that the onset of loss is not\
    \ excessive.  The test\n   will pass if losses are equal to or less than the increase\
    \ in the\n   cross traffic plus the test stream window increase since the previous\n\
    \   RTT.  This could be restated as non-decreasing total throughput of\n   the\
    \ subpath at the onset of loss.  (Note that when there is a\n   transient drop\
    \ in subpath throughput and there is not already a\n   standing queue, a subpath\
    \ that passes other queue tests in this\n   document will have sufficient queue\
    \ space to hold one full RTT worth\n   of data).\n   Note that token bucket policers\
    \ will not pass this test, which is as\n   intended.  TCP often stumbles badly\
    \ if more than a small fraction of\n   the packets are dropped in one RTT.  Many\
    \ TCP implementations will\n   require a timeout and slowstart to recover their\
    \ self-clock.  Even if\n   they can recover from the massive losses, the sudden\
    \ change in\n   available capacity at the bottleneck wastes serving and front-path\n\
    \   capacity until TCP can adapt to the new rate [Policing].\n"
- title: 8.2.4.  Duplex Self-Interference
  contents:
  - "8.2.4.  Duplex Self-Interference\n   This engineering test confirms a bound on\
    \ the interactions between\n   the forward data path and the ACK return path when\
    \ they share a half-\n   duplex link.\n   Some historical half-duplex technologies\
    \ had the property that each\n   direction held the channel until it completely\
    \ drained its queue.\n   When a self-clocked transport protocol, such as TCP,\
    \ has data and\n   ACKs passing in opposite directions through such a link, the\
    \ behavior\n   often reverts to stop-and-wait.  Each additional packet added to\
    \ the\n   window raises the observed RTT by two packet times, once as the\n  \
    \ additional packet passes through the data path and once for the\n   additional\
    \ delay incurred by the ACK waiting on the return path.\n   The Duplex Self-Interference\
    \ Test fails if the RTT rises by more than\n   a fixed bound above the expected\
    \ queuing time computed from the\n   excess window divided by the subpath IP capacity.\
    \  This bound must be\n   smaller than target_RTT/2 to avoid reverting to stop-and-wait\n\
    \   behavior (e.g., data packets and ACKs both have to be released at\n   least\
    \ twice per RTT).\n"
- title: 8.3.  Slowstart Tests
  contents:
  - "8.3.  Slowstart Tests\n   These tests mimic slowstart: data is sent at twice\
    \ the effective\n   bottleneck rate to exercise the queue at the dominant bottleneck.\n"
- title: 8.3.1.  Full Window Slowstart Test
  contents:
  - "8.3.1.  Full Window Slowstart Test\n   This capacity test confirms that slowstart\
    \ is not likely to exit\n   prematurely.  To perform this test, send slowstart\
    \ bursts that are\n   target_window_size total packets and accumulate packet transfer\n\
    \   statistics as described in Section 7.2 to score the outcome.  The\n   test\
    \ will pass if it is statistically significant that the observed\n   number of\
    \ good packets delivered between losses or ECN CE marks is\n   larger than the\
    \ target_run_length.  The test will fail if it is\n   statistically significant\
    \ that the observed interval between losses\n   or ECN CE marks is smaller than\
    \ the target_run_length.\n   The test is deemed inconclusive if the elapsed time\
    \ to send the data\n   burst is not less than half of the time to receive the\
    \ ACKs.  (That\n   is, it is acceptable to send data too fast, but sending it\
    \ slower\n   than twice the actual bottleneck rate as indicated by the ACKs is\n\
    \   deemed inconclusive).  The headway for the slowstart bursts should be\n  \
    \ the target_RTT.\n   Note that these are the same parameters that are used for\
    \ the\n   Sustained Full-Rate Bursts Test, except the burst rate is at\n   slowstart\
    \ rate rather than sender interface rate.\n"
- title: 8.3.2.  Slowstart AQM Test
  contents:
  - "8.3.2.  Slowstart AQM Test\n   To perform this test, do a continuous slowstart\
    \ (send data\n   continuously at twice the implied IP bottleneck capacity) until\
    \ the\n   first loss; stop and allow the network to drain and repeat; gather\n\
    \   statistics on how many packets were delivered before the loss, the\n   pattern\
    \ of losses, maximum observed RTT, and window size; and justify\n   the results.\
    \  There is not currently sufficient theory to justify\n   requiring any particular\
    \ result; however, design decisions that\n   affect the outcome of this tests\
    \ also affect how the network balances\n   between long and short flows (the \"\
    mice vs. elephants\" problem).  The\n   queue sojourn time for the first packet\
    \ delivered after the first\n   loss should be at least one half of the target_RTT.\n\
    \   This engineering test should be performed on a quiescent network or\n   testbed,\
    \ since cross traffic has the potential to change the results\n   in ill-defined\
    \ ways.\n"
- title: 8.4.  Sender Rate Burst Tests
  contents:
  - "8.4.  Sender Rate Burst Tests\n   These tests determine how well the network\
    \ can deliver bursts sent at\n   the sender's interface rate.  Note that this\
    \ test most heavily\n   exercises the front path and is likely to include infrastructure\
    \ that\n   may be out of scope for an access ISP, even though the bursts might\n\
    \   be caused by ACK compression, thinning, or channel arbitration in the\n  \
    \ access ISP.  See Appendix B.\n   Also, there are a several details about sender\
    \ interface rate bursts\n   that are not fully defined here.  These details, such\
    \ as the assumed\n   sender interface rate, should be explicitly stated in an\
    \ FSTIDS.\n   Current standards permit TCP to send full window bursts following\
    \ an\n   application pause.  (Congestion Window Validation [RFC2861] and\n   updates\
    \ to support Rate-Limited Traffic [RFC7661] are not required).\n   Since full\
    \ window bursts are consistent with standard behavior, it is\n   desirable that\
    \ the network be able to deliver such bursts; otherwise,\n   application pauses\
    \ will cause unwarranted losses.  Note that the AIMD\n   sawtooth requires a peak\
    \ window that is twice target_window_size, so\n   the worst-case burst may be\
    \ 2*target_window_size.\n   It is also understood in the application and serving\
    \ community that\n   interface rate bursts have a cost to the network that has\
    \ to be\n   balanced against other costs in the servers themselves.  For example,\n\
    \   TCP Segmentation Offload (TSO) reduces server CPU in exchange for\n   larger\
    \ network bursts, which increase the stress on network buffer\n   memory.  Some\
    \ newer TCP implementations can pace traffic at scale\n   [TSO_pacing] [TSO_fq_pacing].\
    \  It remains to be determined if and how\n   quickly these changes will be deployed.\n\
    \   There is not yet theory to unify these costs or to provide a\n   framework\
    \ for trying to optimize global efficiency.  We do not yet\n   have a model for\
    \ how many server rate bursts should be tolerated by\n   the network.  Some bursts\
    \ must be tolerated by the network, but it is\n   probably unreasonable to expect\
    \ the network to be able to efficiently\n   deliver all data as a series of bursts.\n\
    \   For this reason, this is the only test for which we encourage\n   derating.\
    \  A TIDS could include a table containing pairs of derating\n   parameters: burst\
    \ sizes and how much each burst size is permitted to\n   reduce the run length,\
    \ relative to the target_run_length.\n"
- title: 8.5.  Combined and Implicit Tests
  contents:
  - "8.5.  Combined and Implicit Tests\n   Combined tests efficiently confirm multiple\
    \ network properties in a\n   single test, possibly as a side effect of normal\
    \ content delivery.\n   They require less measurement traffic than other testing\
    \ strategies\n   at the cost of conflating diagnostic signatures when they fail.\n\
    \   These are by far the most efficient for monitoring networks that are\n   nominally\
    \ expected to pass all tests.\n"
- title: 8.5.1.  Sustained Full-Rate Bursts Test
  contents:
  - "8.5.1.  Sustained Full-Rate Bursts Test\n   The Sustained Full-Rate Bursts Test\
    \ implements a combined worst-case\n   version of all of the capacity tests above.\
    \  To perform this test,\n   send target_window_size bursts of packets at server\
    \ interface rate\n   with target_RTT burst headway (burst start to next burst\
    \ start), and\n   verify that the observed packet transfer statistics meets the\n\
    \   target_run_length.\n   Key observations:\n   o  The subpath under test is\
    \ expected to go idle for some fraction of\n      the time, determined by the\
    \ difference between the time to drain\n      the queue at the subpath_IP_capacity\
    \ and the target_RTT.  If the\n      queue does not drain completely, it may be\
    \ an indication that the\n      subpath has insufficient IP capacity or that there\
    \ is some other\n      problem with the test (e.g., it is inconclusive).\n   o\
    \  The burst sensitivity can be derated by sending smaller bursts\n      more\
    \ frequently (e.g., by sending target_window_size*derate packet\n      bursts\
    \ every target_RTT*derate, where \"derate\" is less than one).\n   o  When not\
    \ derated, this test is the most strenuous capacity test.\n   o  A subpath that\
    \ passes this test is likely to be able to sustain\n      higher rates (close\
    \ to subpath_IP_capacity) for paths with RTTs\n      significantly smaller than\
    \ the target_RTT.\n   o  This test can be implemented with instrumented TCP [RFC4898],\n\
    \      using a specialized measurement application at one end (e.g.,\n      [MBMSource])\
    \ and a minimal service at the other end (e.g.,\n      [RFC863] and [RFC864]).\n\
    \   o  This test is efficient to implement, since it does not require\n      per-packet\
    \ timers, and can make use of TSO in modern network\n      interfaces.\n   o \
    \ If a subpath is known to pass the standing queue engineering tests\n      (particularly\
    \ that it has a progressive onset of loss at an\n      appropriate queue depth),\
    \ then the Sustained Full-Rate Bursts Test\n      is sufficient to assure that\
    \ the subpath under test will not\n      impair Bulk Transport Capacity at the\
    \ target performance under all\n      conditions.  See Section 8.2 for a discussion\
    \ of the standing\n      queue tests.\n   Note that this test is clearly independent\
    \ of the subpath RTT or\n   other details of the measurement infrastructure, as\
    \ long as the\n   measurement infrastructure can accurately and reliably deliver\
    \ the\n   required bursts to the subpath under test.\n"
- title: 8.5.2.  Passive Measurements
  contents:
  - "8.5.2.  Passive Measurements\n   Any non-throughput-maximizing application, such\
    \ as fixed-rate\n   streaming media, can be used to implement passive or hybrid\
    \ (defined\n   in [RFC7799]) versions of Model-Based Metrics with some additional\n\
    \   instrumentation and possibly a traffic shaper or other controls in\n   the\
    \ servers.  The essential requirement is that the data transmission\n   be constrained\
    \ such that even with arbitrary application pauses and\n   bursts, the data rate\
    \ and burst sizes stay within the envelope\n   defined by the individual tests\
    \ described above.\n   If the application's serving data rate can be constrained\
    \ to be less\n   than or equal to the target_data_rate and the serving_RTT (the\
    \ RTT\n   between the sender and client) is less than the target_RTT, this\n \
    \  constraint is most easily implemented by clamping the transport\n   window\
    \ size to serving_window_clamp (which is set to the test_window\n   and computed\
    \ for the actual serving path).\n   Under the above constraints, the serving_window_clamp\
    \ will limit both\n   the serving data rate and burst sizes to be no larger than\
    \ the\n   parameters specified by the procedures in Section 8.1.2, 8.4, or\n \
    \  8.5.1.  Since the serving RTT is smaller than the target_RTT, the\n   worst-case\
    \ bursts that might be generated under these conditions will\n   be smaller than\
    \ called for by Section 8.4, and the sender rate burst\n   sizes are implicitly\
    \ derated by the serving_window_clamp divided by\n   the target_window_size at\
    \ the very least.  (Depending on the\n   application behavior, the data might\
    \ be significantly smoother than\n   specified by any of the burst tests.)\n \
    \  In an alternative implementation, the data rate and bursts might be\n   explicitly\
    \ controlled by a programmable traffic shaper or by pacing\n   at the sender.\
    \  This would provide better control over transmissions\n   but is more complicated\
    \ to implement, although the required\n   technology is available [TSO_pacing]\
    \ [TSO_fq_pacing].\n   Note that these techniques can be applied to any content\
    \ delivery\n   that can be operated at a constrained data rate to inhibit TCP\n\
    \   equilibrium behavior.\n   Furthermore, note that Dynamic Adaptive Streaming\
    \ over HTTP (DASH) is\n   generally in conflict with passive Model-Based Metrics\
    \ measurement,\n   because it is a rate-maximizing protocol.  It can still meet\
    \ the\n   requirement here if the rate can be capped, for example, by knowing\
    \ a\n   priori the maximum rate needed to deliver a particular piece of\n   content.\n"
- title: 9.  Example
  contents:
  - "9.  Example\n   In this section, we illustrate a TIDS designed to confirm that\
    \ an\n   access ISP can reliably deliver HD video from multiple content\n   providers\
    \ to all of its customers.  With modern codecs, minimal HD\n   video (720p) generally\
    \ fits in 2.5 Mb/s.  Due to the ISP's\n   geographical size, network topology,\
    \ and modem characteristics, the\n   ISP determines that most content is within\
    \ a 50 ms RTT of its users.\n   (This example RTT is sufficient to cover the propagation\
    \ delay to\n   continental Europe or to either coast of the United States with\
    \ low-\n   delay modems; it is sufficient to cover somewhat smaller geographical\n\
    \   regions if the modems require additional delay to implement advanced\n   compression\
    \ and error recovery.)\n                +----------------------+-------+---------+\n\
    \                | End-to-End Parameter | value | units   |\n                +----------------------+-------+---------+\n\
    \                | target_rate          | 2.5   | Mb/s    |\n                |\
    \ target_RTT           | 50    | ms      |\n                | target_MTU     \
    \      | 1500  | bytes   |\n                | header_overhead      | 64    | bytes\
    \   |\n                |                      |       |         |\n          \
    \      | target_window_size   | 11    | packets |\n                | target_run_length\
    \    | 363   | packets |\n                +----------------------+-------+---------+\n\
    \                    Table 1: 2.5 Mb/s over a 50 ms Path\n   Table 1 shows the\
    \ default TCP model with no derating and, as such, is\n   quite conservative.\
    \  The simplest TIDS would be to use the Sustained\n   Full-Rate Bursts Test,\
    \ described in Section 8.5.1.  Such a test would\n   send 11 packet bursts every\
    \ 50 ms and confirm that there was no more\n   than 1 packet loss per 33 bursts\
    \ (363 total packets in 1.650\n   seconds).\n   Since this number represents the\
    \ entire end-to-end loss budget,\n   independent subpath tests could be implemented\
    \ by apportioning the\n   packet loss ratio across subpaths.  For example, 50%\
    \ of the losses\n   might be allocated to the access or last mile link to the\
    \ user, 40%\n   to the network interconnections with other ISPs, and 1% to each\n\
    \   internal hop (assuming no more than 10 internal hops).  Then, all of\n   the\
    \ subpaths can be tested independently, and the spatial composition\n   of passing\
    \ subpaths would be expected to be within the end-to-end\n   loss budget.\n"
- title: 9.1.  Observations about Applicability
  contents:
  - "9.1.  Observations about Applicability\n   Guidance on deploying and using MBM\
    \ belong in a future document.\n   However, the example above illustrates some\
    \ of the issues that may\n   need to be considered.\n   Note that another ISP,\
    \ with different geographical coverage,\n   topology, or modem technology may\
    \ need to assume a different\n   target_RTT and, as a consequence, a different\
    \ target_window_size and\n   target_run_length, even for the same target_data\
    \ rate.  One of the\n   implications of this is that infrastructure shared by\
    \ multiple ISPs,\n   such as Internet Exchange Points (IXPs) and other interconnects\
    \ may\n   need to be evaluated on the basis of the most stringent\n   target_window_size\
    \ and target_run_length of any participating ISP.\n   One way to do this might\
    \ be to choose target parameters for\n   evaluating such shared infrastructure\
    \ on the basis of a hypothetical\n   reference path that does not necessarily\
    \ match any actual paths.\n   Testing interconnects has generally been problematic:\
    \ conventional\n   performance tests run between measurement points adjacent to\
    \ either\n   side of the interconnect are not generally useful.  Unconstrained\
    \ TCP\n   tests, such as iPerf [iPerf], are usually overly aggressive due to\n\
    \   the small RTT (often less than 1 ms).  With a short RTT, these tools\n   are\
    \ likely to report inflated data rates because on a short RTT,\n   these tools\
    \ can tolerate very high packet loss ratios and can push\n   other cross traffic\
    \ off of the network.  As a consequence, these\n   measurements are useless for\
    \ predicting actual user performance over\n   longer paths and may themselves\
    \ be quite disruptive.  Model-Based\n   Metrics solves this problem.  The interconnect\
    \ can be evaluated with\n   the same TIDS as other subpaths.  Continuing our example,\
    \ if the\n   interconnect is apportioned 40% of the losses, 11 packet bursts sent\n\
    \   every 50 ms should have fewer than one loss per 82 bursts (902\n   packets).\n"
- title: 10.  Validation
  contents:
  - "10.  Validation\n   Since some aspects of the models are likely to be too conservative,\n\
    \   Section 5.2 permits alternate protocol models, and Section 5.3\n   permits\
    \ test parameter derating.  If either of these techniques is\n   used, we require\
    \ demonstrations that such a TIDS can robustly detect\n   subpaths that will prevent\
    \ authentic applications using state-of-the-\n   art protocol implementations\
    \ from meeting the specified Target\n   Transport Performance.  This correctness\
    \ criteria is potentially\n   difficult to prove, because it implicitly requires\
    \ validating a TIDS\n   against all possible paths and subpaths.  The procedures\
    \ described\n   here are still experimental.\n   We suggest two approaches, both\
    \ of which should be applied.  First,\n   publish a fully open description of\
    \ the TIDS, including what\n   assumptions were used and how it was derived, such\
    \ that the research\n   community can evaluate the design decisions, test them,\
    \ and comment\n   on their applicability.  Second, demonstrate that applications\
    \ do\n   meet the Target Transport Performance when running over a network\n \
    \  testbed that has the tightest possible constraints that still allow\n   the\
    \ tests in the TIDS to pass.\n   This procedure resembles an epsilon-delta proof\
    \ in calculus.\n   Construct a test network such that all of the individual tests\
    \ of the\n   TIDS pass by only small (infinitesimal) margins, and demonstrate\
    \ that\n   a variety of authentic applications running over real TCP\n   implementations\
    \ (or other protocols as appropriate) meets the Target\n   Transport Performance\
    \ over such a network.  The workloads should\n   include multiple types of streaming\
    \ media and transaction-oriented\n   short flows (e.g., synthetic web traffic).\n\
    \   For example, for the HD streaming video TIDS described in Section 9,\n   the\
    \ IP capacity should be exactly the header_overhead above 2.5 Mb/s,\n   the per\
    \ packet random background loss ratio should be 1/363 (for a\n   run length of\
    \ 363 packets), the bottleneck queue should be 11\n   packets, and the front path\
    \ should have just enough buffering to\n   withstand 11 packet interface rate\
    \ bursts.  We want every one of the\n   TIDS tests to fail if we slightly increase\
    \ the relevant test\n   parameter, so, for example, sending a 12-packet burst\
    \ should cause\n   excess (possibly deterministic) packet drops at the dominant\
    \ queue at\n   the bottleneck.  This network has the tightest possible constraints\n\
    \   that can be expected to pass the TIDS, yet it should be possible for\n   a\
    \ real application using a stock TCP implementation in the vendor's\n   default\
    \ configuration to attain 2.5 Mb/s over a 50 ms path.\n   The most difficult part\
    \ of setting up such a testbed is arranging for\n   it to have the tightest possible\
    \ constraints that still allow it to\n   pass the individual tests.  Two approaches\
    \ are suggested:\n   o  constraining (configuring) the network devices not to\
    \ use all\n      available resources (e.g., by limiting available buffer space\
    \ or\n      data rate)\n   o  pre-loading subpaths with cross traffic\n   Note\
    \ that it is important that a single tightly constrained\n   environment just\
    \ barely passes all tests; otherwise, there is a\n   chance that TCP can exploit\
    \ extra latitude in some parameters (such\n   as data rate) to partially compensate\
    \ for constraints in other\n   parameters (e.g., queue space).  This effect is\
    \ potentially\n   bidirectional: extra latitude in the queue space tests has the\n\
    \   potential to enable TCP to compensate for insufficient data-rate\n   headroom.\n\
    \   To the extent that a TIDS is used to inform public dialog, it should\n   be\
    \ fully documented publicly, including the details of the tests,\n   what assumptions\
    \ were used, and how it was derived.  All of the\n   details of the validation\
    \ experiment should also be published with\n   sufficient detail for the experiments\
    \ to be replicated by other\n   researchers.  All components should be either\
    \ open source or fully\n   described proprietary implementations that are available\
    \ to the\n   research community.\n"
- title: 11.  Security Considerations
  contents:
  - "11.  Security Considerations\n   Measurement is often used to inform business\
    \ and policy decisions\n   and, as a consequence, is potentially subject to manipulation.\n\
    \   Model-Based Metrics are expected to be a huge step forward because\n   equivalent\
    \ measurements can be performed from multiple vantage\n   points, such that performance\
    \ claims can be independently validated\n   by multiple parties.\n   Much of the\
    \ acrimony in the Net Neutrality debate is due to the\n   historical lack of any\
    \ effective vantage-independent tools to\n   characterize network performance.\
    \  Traditional methods for measuring\n   Bulk Transport Capacity are sensitive\
    \ to RTT and as a consequence\n   often yield very different results when run\
    \ local to an ISP or\n   interconnect and when run over a customer's complete\
    \ path.  Neither\n   the ISP nor customer can repeat the other's measurements,\
    \ leading to\n   high levels of distrust and acrimony.  Model-Based Metrics are\n\
    \   expected to greatly improve this situation.\n   Note that in situ measurements\
    \ sometimes require sending synthetic\n   measurement traffic between arbitrary\
    \ locations in the network and,\n   as such, are potentially attractive platforms\
    \ for launching DDoS\n   attacks.  All active measurement tools and protocols\
    \ must be designed\n   to minimize the opportunities for these misuses.  See the\
    \ discussion\n   in Section 7 of [RFC7594].\n   Some of the tests described in\
    \ this document are not intended for\n   frequent network monitoring since they\
    \ have the potential to cause\n   high network loads and might adversely affect\
    \ other traffic.\n   This document only describes a framework for designing a\
    \ Fully\n   Specified Targeted IP Diagnostic Suite.  Each FSTIDS must include\
    \ its\n   own security section.\n"
- title: 12.  IANA Considerations
  contents:
  - "12.  IANA Considerations\n   This document has no IANA actions.\n"
- title: 13.  Informative References
  contents:
  - "13.  Informative References\n   [RFC863]   Postel, J., \"Discard Protocol\",\
    \ STD 21, RFC 863,\n              DOI 10.17487/RFC0863, May 1983,\n          \
    \    <https://www.rfc-editor.org/info/rfc863>.\n   [RFC864]   Postel, J., \"Character\
    \ Generator Protocol\", STD 22,\n              RFC 864, DOI 10.17487/RFC0864,\
    \ May 1983,\n              <https://www.rfc-editor.org/info/rfc864>.\n   [RFC2330]\
    \  Paxson, V., Almes, G., Mahdavi, J., and M. Mathis,\n              \"Framework\
    \ for IP Performance Metrics\", RFC 2330,\n              DOI 10.17487/RFC2330,\
    \ May 1998,\n              <https://www.rfc-editor.org/info/rfc2330>.\n   [RFC2861]\
    \  Handley, M., Padhye, J., and S. Floyd, \"TCP Congestion\n              Window\
    \ Validation\", RFC 2861, DOI 10.17487/RFC2861, June\n              2000, <https://www.rfc-editor.org/info/rfc2861>.\n\
    \   [RFC3148]  Mathis, M. and M. Allman, \"A Framework for Defining\n        \
    \      Empirical Bulk Transfer Capacity Metrics\", RFC 3148,\n              DOI\
    \ 10.17487/RFC3148, July 2001,\n              <https://www.rfc-editor.org/info/rfc3148>.\n\
    \   [RFC3168]  Ramakrishnan, K., Floyd, S., and D. Black, \"The Addition\n   \
    \           of Explicit Congestion Notification (ECN) to IP\",\n             \
    \ RFC 3168, DOI 10.17487/RFC3168, September 2001,\n              <https://www.rfc-editor.org/info/rfc3168>.\n\
    \   [RFC3465]  Allman, M., \"TCP Congestion Control with Appropriate Byte\n  \
    \            Counting (ABC)\", RFC 3465, DOI 10.17487/RFC3465, February\n    \
    \          2003, <https://www.rfc-editor.org/info/rfc3465>.\n   [RFC4737]  Morton,\
    \ A., Ciavattone, L., Ramachandran, G., Shalunov,\n              S., and J. Perser,\
    \ \"Packet Reordering Metrics\", RFC 4737,\n              DOI 10.17487/RFC4737,\
    \ November 2006,\n              <https://www.rfc-editor.org/info/rfc4737>.\n \
    \  [RFC4898]  Mathis, M., Heffner, J., and R. Raghunarayan, \"TCP\n          \
    \    Extended Statistics MIB\", RFC 4898, DOI 10.17487/RFC4898,\n            \
    \  May 2007, <https://www.rfc-editor.org/info/rfc4898>.\n   [RFC5136]  Chimento,\
    \ P. and J. Ishac, \"Defining Network Capacity\",\n              RFC 5136, DOI\
    \ 10.17487/RFC5136, February 2008,\n              <https://www.rfc-editor.org/info/rfc5136>.\n\
    \   [RFC5681]  Allman, M., Paxson, V., and E. Blanton, \"TCP Congestion\n    \
    \          Control\", RFC 5681, DOI 10.17487/RFC5681, September 2009,\n      \
    \        <https://www.rfc-editor.org/info/rfc5681>.\n   [RFC5827]  Allman, M.,\
    \ Avrachenkov, K., Ayesta, U., Blanton, J., and\n              P. Hurtig, \"Early\
    \ Retransmit for TCP and Stream Control\n              Transmission Protocol (SCTP)\"\
    , RFC 5827,\n              DOI 10.17487/RFC5827, May 2010,\n              <https://www.rfc-editor.org/info/rfc5827>.\n\
    \   [RFC5835]  Morton, A., Ed. and S. Van den Berghe, Ed., \"Framework for\n \
    \             Metric Composition\", RFC 5835, DOI 10.17487/RFC5835, April\n  \
    \            2010, <https://www.rfc-editor.org/info/rfc5835>.\n   [RFC6049]  Morton,\
    \ A. and E. Stephan, \"Spatial Composition of\n              Metrics\", RFC 6049,\
    \ DOI 10.17487/RFC6049, January 2011,\n              <https://www.rfc-editor.org/info/rfc6049>.\n\
    \   [RFC6576]  Geib, R., Ed., Morton, A., Fardid, R., and A. Steinmitz,\n    \
    \          \"IP Performance Metrics (IPPM) Standard Advancement\n            \
    \  Testing\", BCP 176, RFC 6576, DOI 10.17487/RFC6576, March\n              2012,\
    \ <https://www.rfc-editor.org/info/rfc6576>.\n   [RFC6673]  Morton, A., \"Round-Trip\
    \ Packet Loss Metrics\", RFC 6673,\n              DOI 10.17487/RFC6673, August\
    \ 2012,\n              <https://www.rfc-editor.org/info/rfc6673>.\n   [RFC6928]\
    \  Chu, J., Dukkipati, N., Cheng, Y., and M. Mathis,\n              \"Increasing\
    \ TCP's Initial Window\", RFC 6928,\n              DOI 10.17487/RFC6928, April\
    \ 2013,\n              <https://www.rfc-editor.org/info/rfc6928>.\n   [RFC7312]\
    \  Fabini, J. and A. Morton, \"Advanced Stream and Sampling\n              Framework\
    \ for IP Performance Metrics (IPPM)\", RFC 7312,\n              DOI 10.17487/RFC7312,\
    \ August 2014,\n              <https://www.rfc-editor.org/info/rfc7312>.\n   [RFC7398]\
    \  Bagnulo, M., Burbridge, T., Crawford, S., Eardley, P., and\n              A.\
    \ Morton, \"A Reference Path and Measurement Points for\n              Large-Scale\
    \ Measurement of Broadband Performance\",\n              RFC 7398, DOI 10.17487/RFC7398,\
    \ February 2015,\n              <https://www.rfc-editor.org/info/rfc7398>.\n \
    \  [RFC7567]  Baker, F., Ed. and G. Fairhurst, Ed., \"IETF\n              Recommendations\
    \ Regarding Active Queue Management\",\n              BCP 197, RFC 7567, DOI 10.17487/RFC7567,\
    \ July 2015,\n              <https://www.rfc-editor.org/info/rfc7567>.\n   [RFC7594]\
    \  Eardley, P., Morton, A., Bagnulo, M., Burbridge, T.,\n              Aitken,\
    \ P., and A. Akhter, \"A Framework for Large-Scale\n              Measurement\
    \ of Broadband Performance (LMAP)\", RFC 7594,\n              DOI 10.17487/RFC7594,\
    \ September 2015,\n              <https://www.rfc-editor.org/info/rfc7594>.\n\
    \   [RFC7661]  Fairhurst, G., Sathiaseelan, A., and R. Secchi, \"Updating\n  \
    \            TCP to Support Rate-Limited Traffic\", RFC 7661,\n              DOI\
    \ 10.17487/RFC7661, October 2015,\n              <https://www.rfc-editor.org/info/rfc7661>.\n\
    \   [RFC7680]  Almes, G., Kalidindi, S., Zekauskas, M., and A. Morton,\n     \
    \         Ed., \"A One-Way Loss Metric for IP Performance Metrics\n          \
    \    (IPPM)\", STD 82, RFC 7680, DOI 10.17487/RFC7680, January\n             \
    \ 2016, <https://www.rfc-editor.org/info/rfc7680>.\n   [RFC7799]  Morton, A.,\
    \ \"Active and Passive Metrics and Methods (with\n              Hybrid Types In-Between)\"\
    , RFC 7799, DOI 10.17487/RFC7799,\n              May 2016, <https://www.rfc-editor.org/info/rfc7799>.\n\
    \   [AFD]      Pan, R., Breslau, L., Prabhakar, B., and S. Shenker,\n        \
    \      \"Approximate fairness through differential dropping\", ACM\n         \
    \     SIGCOMM Computer Communication Review, Volume 33, Issue 2,\n           \
    \   DOI 10.1145/956981.956985, April 2003.\n   [CCscaling]\n              Paganini,\
    \ F., Doyle, J., and S. Low, \"Scalable laws for\n              stable network\
    \ congestion control\", Proceedings of IEEE\n              Conference on Decision\
    \ and Control,,\n              DOI 10.1109/CDC.2001.980095, December 2001.\n \
    \  [CVST]     Krueger, T. and M. Braun, \"R package: Fast Cross-\n           \
    \   Validation via Sequential Testing\", version 0.1, 11 2012.\n   [iPerf]   \
    \ Wikipedia, \"iPerf\", November 2017,\n              <https://en.wikipedia.org/w/\n\
    \              index.php?title=Iperf&oldid=810583885>.\n   [MBMSource]\n     \
    \         \"mbm\", July 2016, <https://github.com/m-lab/MBM>.\n   [Montgomery90]\n\
    \              Montgomery, D., \"Introduction to Statistical Quality\n       \
    \       Control\", 2nd Edition, ISBN 0-471-51988-X, 1990.\n   [mpingSource]\n\
    \              \"mping\", July 2016, <https://github.com/m-lab/mping>.\n   [MSMO97]\
    \   Mathis, M., Semke, J., Mahdavi, J., and T. Ott, \"The\n              Macroscopic\
    \ Behavior of the TCP Congestion Avoidance\n              Algorithm\", Computer\
    \ Communications Review, Volume 27,\n              Issue 3, DOI 10.1145/263932.264023,\
    \ July 1997.\n   [Pathdiag] Mathis, M., Heffner, J., O'Neil, P., and P. Siemsen,\n\
    \              \"Pathdiag: Automated TCP Diagnosis\", Passive and Active\n   \
    \           Network Measurement, Lecture Notes in Computer Science,\n        \
    \      Volume 4979, DOI 10.1007/978-3-540-79232-1_16, 2008.\n   [Policing] Flach,\
    \ T., Papageorge, P., Terzis, A., Pedrosa, L., Cheng,\n              Y., Karim,\
    \ T., Katz-Bassett, E., and R. Govindan, \"An\n              Internet-Wide Analysis\
    \ of Traffic Policing\", Proceedings\n              of ACM SIGCOMM, DOI 10.1145/2934872.2934873,\
    \ August 2016.\n   [RACK]     Cheng, Y., Cardwell, N., Dukkipati, N., and P. Jha,\
    \ \"RACK:\n              a time-based fast loss detection algorithm for TCP\"\
    , Work\n              in Progress, draft-ietf-tcpm-rack-03, March 2018.\n   [Rtool]\
    \    R Development Core Team, \"R: A language and environment\n              for\
    \ statistical computing\", R Foundation for Statistical\n              Computing,\
    \ Vienna, Austria, ISBN 3-900051-07-0, 2011,\n              <http://www.R-project.org/>.\n\
    \   [TSO_fq_pacing]\n              Dumazet, E. and Y. Chen, \"TSO, fair queuing,\
    \ pacing:\n              three's a charm\", Proceedings of IETF 88, TCPM WG,\n\
    \              November 2013,\n              <https://www.ietf.org/proceedings/88/slides/\n\
    \              slides-88-tcpm-9.pdf>.\n   [TSO_pacing]\n              Corbet,\
    \ J., \"TSO sizing and the FQ scheduler\", August\n              2013, <https://lwn.net/Articles/564978/>.\n\
    \   [Wald45]   Wald, A., \"Sequential Tests of Statistical Hypotheses\",\n   \
    \           The Annals of Mathematical Statistics, Volume 16, Number\n       \
    \       2, pp. 117-186, June 1945,\n              <http://www.jstor.org/stable/2235829>.\n\
    \   [wikiBloat]\n              Wikipedia, \"Bufferbloat\", January 2018,\n   \
    \           <https://en.wikipedia.org/w/\n              index.php?title=Bufferbloat&oldid=819293377>.\n\
    \   [WPING]    Mathis, M., \"Windowed Ping: An IP Level Performance\n        \
    \      Diagnostic\", Computer Networks and ISDN Systems, Volume\n            \
    \  27, Issue 3, DOI 10.1016/0169-7552(94)90119-8, June 1994.\n"
- title: Appendix A.  Model Derivations
  contents:
  - "Appendix A.  Model Derivations\n   The reference target_run_length described\
    \ in Section 5.2 is based on\n   very conservative assumptions: that all excess\
    \ data in flight (i.e.,\n   the window size) above the target_window_size contributes\
    \ to a\n   standing queue that raises the RTT and that classic Reno congestion\n\
    \   control with delayed ACKs is in effect.  In this section we provide\n   two\
    \ alternative calculations using different assumptions.\n   It may seem out of\
    \ place to allow such latitude in a measurement\n   method, but this section provides\
    \ offsetting requirements.\n   The estimates provided by these models make the\
    \ most sense if network\n   performance is viewed logarithmically.  In the operational\
    \ Internet,\n   data rates span more than eight orders of magnitude, RTT spans\
    \ more\n   than three orders of magnitude, and packet loss ratio spans at least\n\
    \   eight orders of magnitude if not more.  When viewed logarithmically\n   (as\
    \ in decibels), these correspond to 80 dB of dynamic range.  On an\n   80 dB scale,\
    \ a 3 dB error is less than 4% of the scale, even though\n   it represents a factor\
    \ of 2 in untransformed parameter.\n   This document gives a lot of latitude for\
    \ calculating\n   target_run_length; however, people designing a TIDS should consider\n\
    \   the effect of their choices on the ongoing tussle about the relevance\n  \
    \ of \"TCP friendliness\" as an appropriate model for Internet capacity\n   allocation.\
    \  Choosing a target_run_length that is substantially\n   smaller than the reference\
    \ target_run_length specified in Section 5.2\n   strengthens the argument that\
    \ it may be appropriate to abandon \"TCP\n   friendliness\" as the Internet fairness\
    \ model.  This gives developers\n   incentive and permission to develop even more\
    \ aggressive applications\n   and protocols, for example, by increasing the number\
    \ of connections\n   that they open concurrently.\n"
- title: A.1.  Queueless Reno
  contents:
  - "A.1.  Queueless Reno\n   In Section 5.2, models were derived based on the assumption\
    \ that the\n   subpath IP rate matches the target rate plus overhead, such that\
    \ the\n   excess window needed for the AIMD sawtooth causes a fluctuating queue\n\
    \   at the bottleneck.\n   An alternate situation would be a bottleneck where\
    \ there is no\n   significant queue and losses are caused by some mechanism that\
    \ does\n   not involve extra delay, for example, by the use of a virtual queue\n\
    \   as done in Approximate Fair Dropping [AFD].  A flow controlled by\n   such\
    \ a bottleneck would have a constant RTT and a data rate that\n   fluctuates in\
    \ a sawtooth due to AIMD congestion control.  Assume the\n   losses are being\
    \ controlled to make the average data rate meet some\n   goal that is equal to\
    \ or greater than the target_rate.  The necessary\n   run length to meet the target_rate\
    \ can be computed as follows:\n   For some value of Wmin, the window will sweep\
    \ from Wmin packets to\n   2*Wmin packets in 2*Wmin RTT (due to delayed ACK).\
    \  Unlike the\n   queuing case where Wmin = target_window_size, we want the average\
    \ of\n   Wmin and 2*Wmin to be the target_window_size, so the average data\n \
    \  rate is the target rate.  Thus, we want Wmin =\n   (2/3)*target_window_size.\n\
    \   Between losses, each sawtooth delivers (1/2)(Wmin+2*Wmin)(2Wmin)\n   packets\
    \ in 2*Wmin RTTs.\n   Substituting these together, we get:\n   target_run_length\
    \ = (4/3)(target_window_size^2)\n   Note that this is 44% of the reference_run_length\
    \ computed earlier.\n   This makes sense because under the assumptions in Section\
    \ 5.2, the\n   AMID sawtooth caused a queue at the bottleneck, which raised the\n\
    \   effective RTT by 50%.\n"
- title: Appendix B.  The Effects of ACK Scheduling
  contents:
  - "Appendix B.  The Effects of ACK Scheduling\n   For many network technologies,\
    \ simple queuing models don't apply: the\n   network schedules, thins, or otherwise\
    \ alters the timing of ACKs and\n   data, generally to raise the efficiency of\
    \ the channel allocation\n   algorithms when confronted with relatively widely\
    \ spaced small ACKs.\n   These efficiency strategies are ubiquitous for half-duplex,\
    \ wireless,\n   and broadcast media.\n   Altering the ACK stream by holding or\
    \ thinning ACKs typically has two\n   consequences: it raises the implied bottleneck\
    \ IP capacity, making\n   the fine-grained slowstart bursts either faster or larger,\
    \ and it\n   raises the effective RTT by the average time that the ACKs and data\n\
    \   are delayed.  The first effect can be partially mitigated by\n   re-clocking\
    \ ACKs once they are beyond the bottleneck on the return\n   path to the sender;\
    \ however, this further raises the effective RTT.\n   The most extreme example\
    \ of this sort of behavior would be a half-\n   duplex channel that is not released\
    \ as long as the endpoint currently\n   holding the channel has more traffic (data\
    \ or ACKs) to send.  Such\n   environments cause self-clocked protocols under\
    \ full load to revert\n   to extremely inefficient stop-and-wait behavior.  The\
    \ channel\n   constrains the protocol to send an entire window of data as a single\n\
    \   contiguous burst on the forward path, followed by the entire window\n   of\
    \ ACKs on the return path.  (A channel with this behavior would fail\n   the Duplex\
    \ Self-Interference Test described in Section 8.2.4).\n   If a particular return\
    \ path contains a subpath or device that alters\n   the timing of the ACK stream,\
    \ then the entire front path from the\n   sender up to the bottleneck must be\
    \ tested at the burst parameters\n   implied by the ACK scheduling algorithm.\
    \  The most important\n   parameter is the implied bottleneck IP capacity, which\
    \ is the average\n   rate at which the ACKs advance snd.una.  Note that thinning\
    \ the ACK\n   stream (relying on the cumulative nature of seg.ack to permit\n\
    \   discarding some ACKs) causes most TCP implementations to send\n   interface\
    \ rate bursts to offset the longer times between ACKs in\n   order to maintain\
    \ the average data rate.\n   Note that due to ubiquitous self-clocking in Internet\
    \ protocols,\n   ill-conceived channel allocation mechanisms are likely to increases\n\
    \   the queuing stress on the front path because they cause larger full\n   sender\
    \ rate data bursts.\n   Holding data or ACKs for channel allocation or other reasons\
    \ (such as\n   forward error correction) always raises the effective RTT relative\
    \ to\n   the minimum delay for the path.  Therefore, it may be necessary to\n\
    \   replace target_RTT in the calculation in Section 5.2 by an\n   effective_RTT,\
    \ which includes the target_RTT plus a term to account\n   for the extra delays\
    \ introduced by these mechanisms.\n"
- title: Acknowledgments
  contents:
  - "Acknowledgments\n   Ganga Maguluri suggested the statistical test for measuring\
    \ loss\n   probability in the target run length.  Alex Gilgur and Merry Mou\n\
    \   helped with the statistics.\n   Meredith Whittaker improved the clarity of\
    \ the communications.\n   Ruediger Geib provided feedback that greatly improved\
    \ the document.\n   This work was inspired by Measurement Lab: open tools running\
    \ on an\n   open platform, using open tools to collect open data.  See\n   <http://www.measurementlab.net/>.\n"
- title: Authors' Addresses
  contents:
  - "Authors' Addresses\n   Matt Mathis\n   Google, Inc\n   1600 Amphitheatre Parkway\n\
    \   Mountain View, CA  94043\n   United States of America\n   Email: mattmathis@google.com\n\
    \   Al Morton\n   AT&T Labs\n   200 Laurel Avenue South\n   Middletown, NJ  07748\n\
    \   United States of America\n   Phone: +1 732 420 1571\n   Email: acmorton@att.com\n"
