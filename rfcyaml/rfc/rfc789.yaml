- title: __initial_text__
  contents:
  - "    Vulnerabilities of Network Control Protocols: An Example\n              \
    \            Eric C. Rosen\n     This paper has appeared in the January 1981 edition\
    \  of  the\nSIGSOFT  Software  Engineering Notes, and will soon appear in the\n\
    SIGCOMM Computer Communications Review.  It is  being  circulated\nas  an  RFC\
    \ because it is thought that it may be of interest to a\nwider audience, particularly\
    \ to the internet community.  It is  a\ncase  study  of  a  particular  kind of\
    \ problem that can arise in\nlarge distributed systems,  and  of  the  approach\
    \  used  in  the\nARPANET to deal with one such problem.\n     On  October 27,\
    \ 1980, there was an unusual occurrence on the\nARPANET.  For a period of several\
    \ hours, the network appeared  to\nbe  unusable,  due to what was later diagnosed\
    \ as a high priority\nsoftware  process   running   out   of   control.    Network-wide\n\
    disturbances  are  extremely  unusual  in  the  ARPANET (none has\noccurred in\
    \ several years), and as a  result,  many  people  have\nexpressed  interest \
    \ in  learning more about the etiology of this\nparticular incident.  The purpose\
    \ of this note is to explain what\nthe symptoms of the problem  were,  what  the\
    \  underlying  causes\nwere,  and  what  lessons  can  be  drawn.   As we shall\
    \ see, the\nimmediate cause of the problem was  a  rather  freakish  hardware\n\
    malfunction  (which is not likely to recur) which caused a faulty\nsequence of\
    \ network control packets to be generated.  This faulty\nsequence of control packets\
    \ in turn affected the apportionment of\nsoftware resources in the IMPs, causing\
    \ one of the IMP  processes\nto  use  an  excessive  amount  of resources, to\
    \ the detriment of\nother  IMP  processes.   Restoring  the  network  to  operational\n\
    condition  was  a  relatively straightforward task.  There was no\ndamage other\
    \ than the outage itself,  and  no  residual  problems\nonce  the  network  was\
    \  restored.   Nevertheless,  it  is  quite\ninteresting to see the way  in  which\
    \  unusual  (indeed,  unique)\ncircumstances  can  bring  out vulnerabilities\
    \ in network control\nprotocols, and that shall be the focus of this paper.\n\
    \     The problem began suddenly when  we  discovered  that,  with\nvery few exceptions,\
    \ no IMP was able to communicate reliably with\nany other IMP.  Attempts to go\
    \ from a TIP to a host on some other\nIMP   only   brought  forth  the  \"net\
    \  trouble\"  error  message,\nindicating that no physical path  existed  between\
    \  the  pair  of\nIMPs.   Connections  which already existed were summarily broken.\n\
    A flood of phone calls to the Network Control Center  (NCC)  from\nall  around\
    \  the  country  indicated  that  the  problem  was not\nlocalized, but rather\
    \ seemed to be affecting virtually every IMP.\n     As a first step towards trying\
    \ to find out what the state of\nthe network actually was, we dialed up a number\
    \  of  TIPs  around\nthe  country.  What we generally found was that the TIPs\
    \ were up,\nbut  that  their  lines  were  down.   That  is,  the  TIPs  were\n\
    communicating  properly  with the user over the dial-up line, but\nno connections\
    \ to other IMPs were possible.\n     We tried manually restarting a number of\
    \ IMPs which  are  in\nour own building (after taking dumps, of course).  This\
    \ procedure\ninitializes  all  of  the IMPs' dynamic data structures, and will\n\
    often clear up problems which arise when, as sometimes happens in\nmost complex\
    \ software systems, the IMPs'  software  gets  into  a\n\"funny\"  state.   The\
    \ IMPs which were restarted worked well until\nthey were connected to the rest\
    \ of  the  net,  after  which  they\nexhibited  the same complex of symptoms as\
    \ the IMPs which had not\nbeen restarted.\n     From the facts so far presented,\
    \ we  were  able  to  draw  a\nnumber  of  conclusions.   Any  problem  which\
    \  affects  all IMPs\nthroughout the network is usually a routing problem.   Restarting\n\
    an  IMP  re-initializes  the routing data structures, so the fact\nthat restarting\
    \ an IMP did not alleviate the problem in that  IMP\nsuggested  that  the problem\
    \ was due to one or more \"bad\" routing\nupdates circulating in the network.\
    \  IMPs  which  were  restarted\nwould  just receive the bad updates from those\
    \ of their neighbors\nwhich were not restarted.  The fact that IMPs  seemed  unable\
    \  to\nkeep  their lines up was also a significant clue as to the nature\nof the\
    \ problem.  Each  pair  of  neighboring  IMPs  runs  a  line\nup/down protocol\
    \ to determine whether the line connecting them is\nof  sufficient  quality  to\
    \ be put into operation.  This protocol\ninvolves the sending of HELLO and I-HEARD-YOU\
    \ messages.  We  have\nnoted  in  the  past that under conditions of extremely\
    \ heavy CPU\nutilization, so many buffers can pile up waiting to be served  by\n\
    the  bottleneck  CPU process, that the IMPs are unable to acquire\nthe  buffers\
    \  needed  for  receiving  the  HELLO  or  I-HEARD-YOU\nmessages.  If a condition\
    \ like this lasts for any length of time,\nthe  IMPs  may  not be able to run\
    \ the line up/down protocol, and\nlines will be declared down by the IMPs' software.\
    \  On the  basis\nof  all  these  facts,  our  tentative  conclusion  was that\
    \ some\nmalformed update was causing the routing process in the  IMPs  to\nuse\
    \  an excessive amount of CPU time, possibly even to be running\nin an infinite\
    \ loop.  (This would be  quite  a  surprise  though,\nsince  we  tried very hard\
    \ to protect ourselves against malformed\nupdates when we designed the routing\
    \ process.)  As we shall  see,\nthis  tentative  conclusion, although on the right\
    \ track, was not\nquite correct, and the actual situation turned  out  to  be\
    \  much\nmore complex.\n     When we examined core dumps from several IMPs, we\
    \ noted that\nmost,  in  some cases all, of the IMPs' buffers contained routing\n\
    updates  waiting  to  be  processed.   Before   describing   this\nsituation further,\
    \ it is necessary to explain some of the details\nof  the  routing  algorithm's\
    \  updating  scheme.   (The following\nexplanation will of course be very brief\
    \ and incomplete.  Readers\nwith a greater  level  of  interest  are  urged  to\
    \  consult  the\nreferences.)  Every so often, each IMP generates a routing update\n\
    indicating  which  other  IMPs  are  its immediate neighbors over\noperational\
    \  lines,  and  the  average   per-packet   delay   (in\nmilliseconds)  over that\
    \ line.  Every IMP is required to generate\nsuch an update at least once per minute,\
    \ and no IMP is  permitted\nto  generate  more than a dozen such updates over\
    \ the course of a\nminute.  Each  update  has  a  6-bit  sequence  number  which\
    \  is\nadvanced by 1 (modulo 64) for each successive update generated by\na  particular\
    \ IMP.  If two updates generated by the same IMP have\nsequence numbers n and\
    \ m, update n  is  considered  to  be  LATER\n(i.e.,  more recently generated)\
    \ than update m if and only if one\nof the following two conditions hold:\n  \
    \       (a) n > m, and n - m <= 32\n         (b) n < m, and m - n > 32\n(where\
    \ the comparisons and subtractions treat n and m as unsigned\n6-bit numbers, with\
    \  no  modulus).   When  an  IMP  generates  an\nupdate,  it sends a copy of the\
    \ update to each neighbor.  When an\nIMP A receives an update u1 which was generated\
    \  by  a  different\nIMP  B,  it  first  compares  the  sequence number of u1\
    \ with the\nsequence number of the last update, u2, that it accepted from  B.\n\
    If  this  comparison  indicates  that  u2 is LATER than u1, u1 is\nsimply discarded.\
    \  If, on the other hand, u1 appears  to  be  the\nLATER  update, IMP A will send\
    \ u1 to all its neighbors (including\nthe one from which it was received).  The\
    \ sequence number  of  u1\nwill be retained in A's tables as the LATEST received\
    \ update from\nB.   Of  course,  u1 is always accepted if A has seen no previous\n\
    update from B.  Note that this procedure is  designed  to  ensure\nthat  an  update\
    \  generated  by  a  particular  IMP  is received,\nunchanged, by all other  IMPs\
    \  in  the  network,  IN  THE  PROPER\nSEQUENCE.    Each routing update is broadcast\
    \ (or flooded) to all\nIMPs, not just to immediate neighbors of the IMP which\
    \  generated\nthe update (as in some other routing algorithms).  The purpose of\n\
    the  sequence numbers is to ensure that all IMPs will agree as to\nwhich update\
    \ from a given IMP  is  the  most  recently  generated\nupdate from that IMP.\n\
    \     For  reliability,  there  is  a  protocol for retransmitting\nupdates over\
    \ individual links.  Let X and Y be neighboring  IMPs,\nand let A be a third IMP.\
    \  Suppose X receives an update which was\ngenerated by A, and transmits it to\
    \ Y.  Now if in the next 100 ms\nor  so, X does not receive from Y an update which\
    \ originated at A\nand whose sequence number is at least as recent as  that  of\
    \  the\nupdate  X  sent  to  Y,  X concludes that its transmission of the\nupdate\
    \ did not get through to Y, and  that  a  retransmission  is\nrequired.   (This\
    \  conclusion is warranted, since an update which\nis  received  and  adjudged\
    \  to  be  the  most  recent  from  its\noriginating  IMP is sent to all neighbors,\
    \ including the one from\nwhich it was received.)  The IMPs do not keep the original\
    \ update\npackets  buffered  pending  retransmission.   Rather,   all   the\n\
    information  in  the  update  packet  is  kept in tables, and the\npacket  is\
    \  re-created  from  the  tables  if  necessary  for   a\nretransmission.\n  \
    \   This  transmission  protocol  (\"flooding\")  distributes  the\nrouting updates\
    \  in a  very  rapid  and  reliable  manner.   Once\ngenerated by an IMP, an update\
    \ will almost always reach all other\nIMPs  in  a time period on the order of\
    \ 100 ms.  Since an IMP can\n64  possible sequence numbers, sequence number wrap-around\
    \ is not\na problem.  There is only one exception  to  this.   Suppose  two\n\
    IMPs  A  and  B  are  out  of  communication for a period of time\nbecause there\
    \ is no physical path between them.  (This may be due\neither to a network partition,\
    \ or to a more  mundane  occurrence,\nsuch  as  one  of  the  IMPs  being down.)\
    \  When communication is\nre-established, A and B have no way of knowing how long\
    \ they have\nbeen out of communication, or how many times the other's sequence\n\
    numbers may have wrapped around.  Comparing the  sequence  number\nof  a newly\
    \ received update with the sequence number of an update\nreceived before the outage\
    \ may give an incorrect result.  To deal\nwith this problem, the following scheme\
    \ is adopted.   Let  t0  be\nthe time at which IMP A receives update number n\
    \ generated by IMP\nB.   Let  t1 be t0 plus 1 minute.  If by t1, A receives no\
    \ update\ngenerated by B with a LATER sequence number than n, A will accept\n\
    any update from B as being more recent than n.  So  if  two  IMPs\nare  out  of\
    \  communication  for  a  period of time which is long\nenough for the sequence\
    \ numbers  to  have  wrapped  around,  this\nprocedure  ensures  that  proper\
    \  resynchronization  of  sequence\nnumbers is effected when communication is\
    \ re-established.\n     There is just one more facet of the updating  process\
    \  which\nneeds  to  be  discussed.   Because  of  the way the line up/down\n\
    protocol works, a line cannot be  brought  up  until  60  seconds\nafter  its\
    \ performance becomes good enough to warrant operational\nuse.  (Roughly speaking,\
    \ this is the time it takes  to  determine\nthat  the  line's  performance  is\
    \  good  enough.)   During  this\n60-second period, no data is sent  over  the\
    \  line,  but  routing\nupdates are transmitted.  Remember that every node is\
    \ required to\ngenerate  a  routing update at least once per minute.  Therefore,\n\
    this procedure ensures that if two IMPs are out of  communication\nbecause  of\
    \  the  failure  of some line, each has the most recent\nupdate  from   the  \
    \ other   by   the   time   communication   is\nre-established.\n     This  very\
    \  short  introduction  to  the routing algorithm's\nupdating protocol should\
    \ provide enough background to enable  the\nreader  to  understand  the  particular\
    \ problem under discussion;\nfurther justification and detail can be found in\
    \ the  references.\n     Let  us  return now to the discussion of the network\
    \ outage.\nI have already mentioned that the core dumps  showed  almost  all\n\
    buffers   holding  routing  updates  which  were  waiting  to  be\nprocessed.\
    \  Close inspection showed that  all  the  updates  were\nfrom  a  single  IMP,\
    \ IMP 50.  By a strange \"coincidence,\" IMP 50\nhad been  malfunctioning  just\
    \  before  the  network-wide  outage\noccurred,  and  was  off the net during\
    \ the period of the outage.\nHence it was not generating any updates during the\
    \ period of  the\noutage.   In  addition,  IMP 29, an immediate neighbor of IMP\
    \ 50,\nwas also suffering hardware malfunctions (in particular, dropping\nbits),\
    \ but was up (though somewhat flakey) while the network  was\nin  bad  shape.\
    \  Furthermore, the malfunction in IMP 50 had to do\n29.  Although we did not\
    \ yet understand how it was  possible  for\nso  many updates from one IMP to be\
    \ extant simultaneously, we did\nunderstand enough to be able to get the network\
    \ to recover.   All\nthat was necessary was to patch the IMPs to disregard any\
    \ updates\nfrom  IMP  50, which after all was down anyway.  When the network\n\
    is operating normally, broadcasting a patch to all  IMPs  can  be\ndone  in  a\
    \  matter of minutes.  With the network operating as it\nwas during the period\
    \ of the outage, this can take as much  as  3\nor  4 hours.  (Remember that the\
    \ IMPs are generally unmanned, and\nthat the only way of controlling them from\
    \ the  NCC  is  via  the\nnetwork  itself.   This  is perfectly satisfactory when\
    \ an outage\naffects only a small group of IMPs, but  is  an  obvious  problem\n\
    when  the  outage  has network-wide effects.)  This procedure was\nfully successful\
    \ in bringing the network back up.\n     When we looked closely at the dumps,\
    \ we saw  that  not  only\nwere  all  the updates on the queue from IMP 50, but\
    \ they all had\none of three sequence numbers (either 8, 40,  or  44),  and  were\n\
    ordered        in        the        queue       as       follows:\n8, 40, 44,\
    \ 8, 40, 44, 8, 40, 44, ...  Note that by the definition\nof LATER, 44 is LATER\
    \ than 40 (44 > 40 and 44 - 40 <= 32), 40  is\nLATER  than  8  (40 > 8 and 40\
    \ - 8 <= 32), and 8 is LATER than 44\n(8 < 44 and 44 - 8 > 32).  Given the presence\
    \  of  three  updates\nfrom the same IMP with these three sequence numbers, this\
    \ is what\nwould  be  expected.   Since each update is LATER than one of the\n\
    others, a cycle is formed which keeps the three updates  floating\naround  the\
    \  network  indefinitely.   Thus the IMPs spend most of\ntheir CPU time and buffer\
    \ space in processing these updates.  The\nproblem was to figure out how these\
    \ three updates could  possibly\nhave  existed at the same time.  After all, getting\
    \ from update 8\nto update 40  should  require  2  or  3  full  minutes,  plus\
    \  31\nintervening  sequence  numbers.   So  how could 8 still be around\nwhen\
    \  40  was  generated,  especially  since  no   updates   with\nintervening sequence\
    \ numbers were present?\n     Our  first thought was that maybe the real-time\
    \ clock in IMP\n50 was running one or two orders of magnitude faster than normal,\n\
    invalidating our assumptions about the maximum number of  updates\nwhich  could\
    \  be  generated  in  a  given  time.   An alternative\nhypothesis suggested itself\
    \ however when we looked at the  binary\nrepresentations of the three sequence\
    \ numbers:\n          8 - 001000\n         40 - 101000\n         44 - 101100\n\
    Note  that  44  has only one more bit than 40, which has only one\nmore bit than\
    \ 8.  Furthermore, the three different  updates  were\ncompletely  identical,\
    \  except  for their sequence numbers.  This\nsuggests that  there  was  really\
    \  only  one  update,  44,  whose\nsequence number was twice corrupted by dropped\
    \ bits.  (Of course,\nit's  also  possible  that  the  \"real\"  update  was \
    \ 8,  and was\ncorrupted by added bits.  However, bit-dropping has proven itself\n\
    to be a much  more  common  sort  of  hardware  malfunction  than\nbit-adding,\
    \  although  spontaneously  dropped  bits may sometimes\ncome back on spontaneously.)\n\
    \     Surely, the reader will object,  there  must  be  protection\nagainst  dropped\
    \  bits.   Yes there is protection, but apparently\nnot enough.  The update packets\
    \ themselves are checksummed, so  a\ndropped  bit  in  an update packet is readily\
    \ detected.  Remember\nthough that if  an  update  needs  to  be  retransmitted,\
    \  it  is\nrecreated  from tabled information.  For maximal reliability, the\n\
    tables must  be  checksummed  also,  and  the  checksum  must  be\nrecomputed\
    \ every time the table is accessed.  However, this would\nrequire  either  a \
    \ large  number  of  CPU  cycles  (for frequent\nchecksumming of a large area\
    \ of memory)  or  a  large  amount  of\nmemory  (to store the checksums for a\
    \ lot of small areas).  Since\nCPU cycles and memory are both potentially scarce\
    \ resources, this\ndid not seem to us to  be  a  cost-effective  way  to  deal\
    \  with\nproblems  that  arise, say, once per year (this is the first such\nproblem\
    \ encountered in a year and a half of running this  routing\nalgorithm).   Time\
    \  and  space  can  be  saved by recomputing the\nchecksum at  a  somewhat  slower\
    \  frequency,  but  this  is  less\nreliable,  in  that it allows a certain number\
    \ of dropped bits to\n\"fall between the cracks.\"  It seems likely then that\
    \ one of  the\nmalfunctioning  IMPs  had to retransmit update 44 at least twice,\n\
    (recreating it each time from tabled information), retransmitting\nit at least\
    \ once with the corrupted sequence number  40,  and  at\nleast  once  with  the\
    \  corrupted  sequence number 8.  This would\ncause those three sequence numbers\
    \ to be extant  in  the  network\nsimultaneously,  even  though protocol is supposed\
    \ to ensure that\nthis is impossible.\n     Actually, the detection of dropped\
    \ bits is most  properly  a\nhardware function.  The next generation of IMP hardware\
    \ (the \"C30\nIMP\")  will  be able to detect and correct all single-bit errors,\n\
    and will detect all other bit errors.  Uncorrectable  bit  errors\nwill  cause\
    \  the  IMP to go into its \"loader/dumper.\"  (An IMP in\nits loader/dumper is\
    \ not usable for  transferring  data,  and  is\nofficially   in  the  \"down\"\
    \  state.   However,  an  IMP  in  its\nloader/dumper is easily controllable from\
    \ the  NCC,  and  can  be\nrestarted  or  reloaded  without  on-site intervention.)\
    \  Current\nhardware does have parity checking (which  should  detect  single\n\
    dropped  bits),  but  this feature has had to be turned off since\n(a) there are\
    \ too many spurious parity \"errors,\"  i.e.,  most  of\nthe  time when the machines\
    \ complain of parity errors there don't\nreally seem to be any, and (b) parity\
    \ errors cause  the  machines\nto  simply  halt, rather than go into their loader/dumpers,\
    \ which\nmeans that on-site intervention is required to restart them.\n     Pending\
    \ the introduction of improved hardware, what  can  be\ndone  to prevent problems\
    \ like this from recurring in the future?\nIt is easy to think of many  ways \
    \ of  avoiding  this  particular\nproblem,  especially  if  one does not consider\
    \ the problems that\navoid  this  sort of problem by spending a lot more CPU cycles\
    \ on\nchecksumming, but this may be too expensive because of  the  side\neffects\
    \  it  would  introduce.   (Also,  it is not clear that any\nmemory checksumming\
    \ strategy can be totally free of \"cracks.\")  A\nvery  simple  and  conservative\
    \  fix  to  prevent this particular\nproblem from recurring is to modify clause\
    \ (a) of the  definition\nof  LATER  so  that  the  \"<=\"  is replaced by \"\
    <\" (strictly less\nthan).  We will implement this fix, but it cannot  be  guaranteed\n\
    that no related problems will ever arise.\n     What  is  really  needed  is \
    \ not some particular fix to the\nrouting algorithm, but a more general fix. \
    \ In  some  sense,  the\nproblem  we  saw  was  not really a routing problem.\
    \  The routing\ncode was working correctly, and the routes  that  were  generated\n\
    were correct and consistent.  The real problem is that a freakish\nhardware  malfunction\
    \ caused a high priority process to run wild,\ndevouring resources needed by other\
    \ processes, thereby making the\nnetwork unusable.  The fact that the wild process\
    \ was the routing\nprocess is incidental.  In  designing  the  routing  process,\
    \  we\ncarefully  considered the amount of resource utilization it would\nrequire.\
    \  By strictly controlling and limiting the rate at  which\nupdates  can  be \
    \ generated, we tried to prevent any situation in\nwhich the routing process would\
    \ make  excessive  demands  on  the\nsystem.   As  we  have  seen  though, even\
    \ our carefully designed\nmechanisms were unable to protect against every possible\
    \ sort  of\nhardware  failure.  We need a better means of detecting that some\n\
    high priority process in the IMP, despite all the  safeguards  we\nhave  built\
    \ in, is still consuming too many resources.  Once this\nis  detected,  the  IMP\
    \  can  be  automatically  placed  in   its\nloader/dumper.  In the case under\
    \ discussion, we would have liked\nto  have  all  the  IMPs  go  into  their loader/dumpers\
    \ when the\nproblem arose.  This would have enabled us to  re-initialize  and\n\
    restart  all  the  IMPs  much more quickly.  (Although restarting\nindividual\
    \  IMPs  did  little  good,  restarting  all  the   IMPs\nsimultaneously would\
    \ have cleared up the problem instantly, since\nall  routing  tables  in  all\
    \  IMPs  would  have been initialized\nsimultaneously.)  It took us no more than\
    \ an hour to  figure  out\nhow  to  restore  the  network;  several  additional\
    \  hours  were\nrequired because it took so long for us to gain  control  of \
    \ the\nmisbehaving  IMPs  and  get  them  back  to  normal.   A built-in\nsoftware\
    \ alarm system (assuming,  of  course,  that  it  was  not\nsubject  to  false\
    \  alarms)  might have enabled us to restore the\nnetwork more quickly, significantly\
    \ reducing the duration of  the\noutage.   This  is  not  to  say  that a better\
    \ alarm and control\nsystem could ever be a replacement for careful study  and\
    \  design\nwhich   attempts   to  properly  distribute  the  utilization  of\n\
    important resources, but only that it is a necessary adjunct,  to\nhandle  the\
    \ cases that will inevitably fall between the cracks of\neven the most careful\
    \ design.\n                           REFERENCES\n\"The New Routing Algorithm\
    \ for the ARPANET,\" IEEE TRANSACTIONS ON\nCOMMUNICATIONS, May 1980, J.M. McQuillan,\
    \ I. Richer, E.C.  Rosen.\n\"The  Updating  Protocol  of  ARPANET's  New  Routing\
    \ Algorithm,\"\nCOMPUTER NETWORKS, February 1980, E.C. Rosen.\n"
